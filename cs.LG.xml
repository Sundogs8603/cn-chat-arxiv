<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Aria&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#20010;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20854;&#23427;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#27809;&#26377;&#30340;&#39640;&#31934;&#24230;&#12289;&#29031;&#29255;&#36924;&#30495;&#21644;&#35814;&#23613;&#30340;&#30495;&#23454;&#20449;&#24687;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#35780;&#20272;&#30340;&#26032;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.06362</link><description>&lt;p&gt;
Aria&#25968;&#23383;&#23402;&#29983;&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#29992;&#20110;&#33258;&#25105;&#20013;&#24515;&#30340;3D&#26426;&#22120;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception. (arXiv:2306.06362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06362
&lt;/p&gt;
&lt;p&gt;
Aria&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#20010;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20854;&#23427;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#27809;&#26377;&#30340;&#39640;&#31934;&#24230;&#12289;&#29031;&#29255;&#36924;&#30495;&#21644;&#35814;&#23613;&#30340;&#30495;&#23454;&#20449;&#24687;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#35780;&#20272;&#30340;&#26032;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#20986;&#20102;Aria&#25968;&#23383;&#23402;&#29983;&#65288;ADT&#65289;-&#19968;&#20010;&#20351;&#29992;Aria&#30524;&#38236;&#25429;&#33719;&#30340;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23545;&#35937;&#65292;&#29615;&#22659;&#21644;&#20154;&#31867;&#32423;&#21035;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#35813;ADT&#25968;&#25454;&#38598;&#21253;&#25324;200&#20010;&#30001;&#31359;&#25140;Aria&#35774;&#22791;&#30340;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#30495;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#30340;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#24207;&#21015;&#65292;&#21253;&#21547;398&#20010;&#23545;&#35937;&#23454;&#20363;&#65288;324&#20010;&#38745;&#24577;&#21644;74&#20010;&#21160;&#24577;&#65289;&#12290;&#27599;&#20010;&#24207;&#21015;&#21253;&#25324;&#65306;a&#65289;&#20004;&#20010;&#21333;&#33394;&#30456;&#26426;&#27969;&#65292;&#19968;&#20010;RGB&#30456;&#26426;&#27969;&#65292;&#20004;&#20010;IMU&#27969;&#30340;&#21407;&#22987;&#25968;&#25454;&#65307;b&#65289;&#23436;&#25972;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#65307;c&#65289;&#30495;&#23454;&#25968;&#25454;&#65292;&#21253;&#25324;Aria&#35774;&#22791;&#30340;&#36830;&#32493;6&#33258;&#30001;&#24230;&#65288;6DoF&#65289;&#23039;&#24577;&#65292;&#23545;&#35937;6DoF&#23039;&#24577;&#65292;3D&#27880;&#35270;&#30690;&#37327;&#65292;3D&#20154;&#20307;&#23039;&#24577;&#65292;2D&#22270;&#20687;&#20998;&#21106;&#65292;&#22270;&#20687;&#28145;&#24230;&#22270;&#65307;d&#65289;&#29031;&#29255;&#33324;&#30495;&#23454;&#30340;&#21512;&#25104;&#28210;&#26579;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#33021;&#22815;&#19982;ADT&#30340;&#20934;&#30830;&#24615;&#12289;&#36924;&#30495;&#24230;&#21644;&#20840;&#38754;&#24615;&#30456;&#23218;&#32654;&#12290;&#36890;&#36807;&#21521;&#30740;&#31350;&#31038;&#21306;&#36129;&#29486;ADT&#65292;&#25105;&#20204;&#30340;&#20351;&#21629;&#26159;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#30340;&#35780;&#20272;&#35774;&#31435;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities conducted by Aria wearers in two real indoor scenes with 398 object instances (324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome camera streams, one RGB camera stream, two IMU streams; b) complete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to ADT. By contributing ADT to the research community, our mission is to set a new standard for evaluation in the egocentric machine perce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#30340;&#32467;&#26500;&#30340;&#19977;&#32500;&#37325;&#24314;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#23545;HDR&#29031;&#30456;&#26426;&#21644;&#23460;&#20869;&#31227;&#21160;&#22320;&#38754;&#26426;&#22120;&#20154;&#36827;&#34892;&#25429;&#25417;&#21644;&#31639;&#27861;&#25512;&#31639;&#65292;&#23454;&#29616;&#23460;&#20869;&#31354;&#38388;&#30340;&#28145;&#24230;&#22270;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.06360</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#30340;&#32467;&#26500;&#30340;&#19977;&#32500;&#37325;&#24314;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
3D reconstruction using Structure for Motion. (arXiv:2306.06360v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#30340;&#32467;&#26500;&#30340;&#19977;&#32500;&#37325;&#24314;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#23545;HDR&#29031;&#30456;&#26426;&#21644;&#23460;&#20869;&#31227;&#21160;&#22320;&#38754;&#26426;&#22120;&#20154;&#36827;&#34892;&#25429;&#25417;&#21644;&#31639;&#27861;&#25512;&#31639;&#65292;&#23454;&#29616;&#23460;&#20869;&#31354;&#38388;&#30340;&#28145;&#24230;&#22270;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#19968;&#23545;HDR&#29031;&#30456;&#26426;&#36827;&#34892;&#23460;&#20869;&#31354;&#38388;&#30340;&#19977;&#32500;&#37325;&#24314;&#65292;&#35813;&#29031;&#30456;&#26426;&#20197;&#31435;&#20307;&#35270;&#35273;&#37197;&#32622;&#23433;&#35013;&#22312;&#23460;&#20869;&#31227;&#21160;&#22320;&#38754;&#26426;&#22120;&#20154;&#19978;&#65292;&#25429;&#25417;&#21508;&#31181;&#32441;&#29702;&#21644;&#31354;&#38388;&#29305;&#24449;&#24182;&#23558;&#20854;&#20316;&#20026;2D&#22270;&#20687;&#20256;&#32473;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#36827;&#32780;&#23454;&#29616;&#28145;&#24230;&#22270;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;</title><link>http://arxiv.org/abs/2306.06344</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#22330;&#26223;&#32423;&#20132;&#36890;&#20223;&#30495;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Language-Guided Traffic Simulation via Scene-Level Diffusion. (arXiv:2306.06344v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#20223;&#30495;&#26159;&#21152;&#36895;&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#21457;&#23637;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#23398;&#20064;&#30340;&#20132;&#36890;&#27169;&#22411;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#23545;&#20110;&#20174;&#19994;&#32773;&#26469;&#35828;&#24456;&#38590;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CTG++&#65292;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#65306;&#38656;&#35201;&#19968;&#20010;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#27169;&#22411;&#39592;&#24178;&#32467;&#26500;&#65292;&#24182;&#19988;&#35201;&#26377;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#35821;&#35328;&#19982;&#20132;&#36890;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#26102;&#31354;&#36716;&#25442;&#22120;&#39592;&#24178;&#32467;&#26500;&#30340;&#22330;&#26223;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#29983;&#25104;&#20102;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#25442;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#26397;&#30528;&#26597;&#35810;&#21512;&#35268;&#30340;&#29983;&#25104;&#26041;&#21521;&#21069;&#36827;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effect
&lt;/p&gt;</description></item><item><title>ECGBERT&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#23545;ECG&#30340;&#28508;&#22312;&#35821;&#35328;&#36827;&#34892;&#35299;&#38145;&#65292;&#20943;&#36731;&#20102;&#21307;&#23398;&#25968;&#25454;&#32570;&#20047;&#33391;&#22909;&#26631;&#27880;&#21644;&#31934;&#36873;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#22312;&#22810;&#20010;&#22522;&#20110;ECG&#30340;&#20219;&#21153;&#20013;&#65292;ECGBERT&#34920;&#29616;&#20986;&#20102;&#23454;&#29616;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.06340</link><description>&lt;p&gt;
ECGBERT: &#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25581;&#31034;ECG&#30340;&#38544;&#34255;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
ECGBERT: Understanding Hidden Language of ECGs with Self-Supervised Representation Learning. (arXiv:2306.06340v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06340
&lt;/p&gt;
&lt;p&gt;
ECGBERT&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#23545;ECG&#30340;&#28508;&#22312;&#35821;&#35328;&#36827;&#34892;&#35299;&#38145;&#65292;&#20943;&#36731;&#20102;&#21307;&#23398;&#25968;&#25454;&#32570;&#20047;&#33391;&#22909;&#26631;&#27880;&#21644;&#31934;&#36873;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#22312;&#22810;&#20010;&#22522;&#20110;ECG&#30340;&#20219;&#21153;&#20013;&#65292;ECGBERT&#34920;&#29616;&#20986;&#20102;&#23454;&#29616;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#30446;&#21069;&#30340;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#20998;&#26512;&#26041;&#27861;&#20381;&#36182;&#20110;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;ECGBERT&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#35299;&#38145;ECG&#30340;&#28508;&#22312;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#32531;&#35299;&#20102;&#21307;&#23398;&#25968;&#25454;&#32570;&#20047;&#33391;&#22909;&#26631;&#27880;&#21644;&#31934;&#36873;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;ECGBERT&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#30340;&#39069;&#22806;&#23618;&#36827;&#34892;&#24494;&#35843;&#65292;&#24212;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;ECG&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22235;&#39033;&#20219;&#21153;&#65292;&#21253;&#25324;&#25151;&#39076;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12289;&#24515;&#36339;&#20998;&#31867;&#12289;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#26816;&#27979;&#21644;&#29992;&#25143;&#35748;&#35777;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;ECGBERT&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the medical field, current ECG signal analysis approaches rely on supervised deep neural networks trained for specific tasks that require substantial amounts of labeled data. However, our paper introduces ECGBERT, a self-supervised representation learning approach that unlocks the underlying language of ECGs. By unsupervised pre-training of the model, we mitigate challenges posed by the lack of well-labeled and curated medical data. ECGBERT, inspired by advances in the area of natural language processing and large language models, can be fine-tuned with minimal additional layers for various ECG-based problems. Through four tasks, including Atrial Fibrillation arrhythmia detection, heartbeat classification, sleep apnea detection, and user authentication, we demonstrate ECGBERT's potential to achieve state-of-the-art results on a wide variety of tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#21644;&#25554;&#34917;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#29992;ECOC&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#20010;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06338</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#32570;&#22833;&#20540;&#25554;&#34917;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Based Missing Values Imputation in Categorical Datasets. (arXiv:2306.06338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#21644;&#25554;&#34917;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#29992;ECOC&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#20010;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#39044;&#27979;&#21644;&#25554;&#34917;&#32570;&#22833;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#20351;&#29992;&#35823;&#24046;&#32416;&#27491;&#36755;&#20986;&#30721;(ECOC)&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;SVM&#21644;KNN&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#20197;&#21450;&#32467;&#21512;&#20102;SVM&#12289;KNN&#21644;MLP&#27169;&#22411;&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#25968;&#25454;&#38598;: CPU&#25968;&#25454;&#38598;&#12289;&#30002;&#29366;&#33146;&#21151;&#33021;&#20943;&#36864;&#25968;&#25454;&#38598;&#21644;&#20083;&#33146;&#30284;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#22312;&#39044;&#27979;&#21644;&#25554;&#34917;&#32570;&#22833;&#20540;&#26041;&#38754;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20855;&#20307;&#32467;&#26524;&#22240;&#25968;&#25454;&#38598;&#21644;&#32570;&#22833;&#20540;&#27169;&#24335;&#32780;&#24322;&#12290;&#37319;&#29992;&#35823;&#24046;&#32416;&#27491;&#36755;&#20986;&#30721;(ECOC)&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#21333;&#20010;&#27169;&#22411;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#32570;&#22833;&#20540;&#25554;&#34917;&#20063;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explored the use of machine learning algorithms for predicting and imputing missing values in categorical datasets. We focused on ensemble models that use the error correction output codes (ECOC) framework, including SVM-based and KNN-based ensemble models, as well as an ensemble classifier that combines SVM, KNN, and MLP models. We applied these algorithms to three datasets: the CPU dataset, the hypothyroid dataset, and the Breast Cancer dataset. Our experiments showed that the machine learning algorithms were able to achieve good performance in predicting and imputing the missing values, with some variations depending on the specific dataset and missing value pattern. The ensemble models using the error correction output codes (ECOC) framework were particularly effective in improving the accuracy and robustness of the predictions, compared to individual models. However, there are also challenges and limitations to using deep learning for missing value imputation, including
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#25511;&#21046;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#29289;&#29702;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#22312;&#35774;&#35745;&#20013;&#21033;&#29992;&#36317;&#31163;&#24863;&#30693;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#21487;&#29992;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#27169;&#22411;&#22522;&#30784;&#22686;&#24378;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.06335</link><description>&lt;p&gt;
&#22914;&#20309;&#20174;&#30701;&#30701;&#19977;&#20998;&#38047;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#25512;&#24191;&#65306;&#21463;&#38480;&#20110;&#29289;&#29702;&#23398;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations. (arXiv:2306.06335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#25511;&#21046;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#29289;&#29702;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#22312;&#35774;&#35745;&#20013;&#21033;&#29992;&#36317;&#31163;&#24863;&#30693;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#21487;&#29992;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#27169;&#22411;&#22522;&#30784;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#26469;&#23398;&#20064;&#21463;&#25511;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#26500;&#24314;&#28418;&#31227;&#39033;&#65292;&#21033;&#29992;&#20808;&#39564;&#30340;&#29289;&#29702;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#35774;&#35745;&#25193;&#25955;&#39033;&#26469;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#36317;&#31163;&#24863;&#30693;&#20272;&#35745;&#8212;&#8212;&#24403;&#22312;&#25509;&#36817;&#35757;&#32451;&#25968;&#25454;&#38598;&#29366;&#24577;&#30340;&#29366;&#24577;&#19979;&#35780;&#20272;&#26102;&#65292;&#23427;&#21305;&#37197;&#31995;&#32479;&#30340;&#22522;&#26412;&#38543;&#26426;&#24615;&#65292;&#24182;&#19988;&#24403;&#22312;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#29366;&#24577;&#19979;&#35780;&#20272;&#26102;&#65292;&#23427;&#20250;&#39044;&#27979;&#39640;&#24230;&#38543;&#26426;&#30340;&#21160;&#24577;&#12290;&#25552;&#20986;&#30340;&#31070;&#32463;SDEs&#21487;&#20197;&#24555;&#36895;&#35780;&#20272;&#65292;&#20197;&#29992;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#65292;&#20063;&#21487;&#20197;&#29992;&#20316;&#27169;&#22411;&#22522;&#30784;&#22686;&#24378;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#35206;&#30422;&#29366;&#24577;&#31354;&#38388;&#26377;&#38480;&#21306;&#22495;&#30340;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#20063;&#21487;&#20197;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#23567;&#22411;&#26080;&#20154;&#26426;&#21644;&#27169;&#25311;&#26426;&#22120;&#33151;&#30340;&#36816;&#21160;&#65288;&#32771;&#34385;&#21040;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#20223;&#30495;&#21040;&#29616;&#23454;&#30340;&#24046;&#24322;&#65289;&#20197;&#21450;&#27169;&#25311;&#24314;&#31569;&#30340;&#28909;&#21147;&#23398;&#21160;&#24577;&#65288;&#32771;&#34385;&#21040;&#32570;&#22833;&#25968;&#25454;&#24182;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework and algorithms to learn controlled dynamics models using neural stochastic differential equations (SDEs) -- SDEs whose drift and diffusion terms are both parametrized by neural networks. We construct the drift term to leverage a priori physics knowledge as inductive bias, and we design the diffusion term to represent a distance-aware estimate of the uncertainty in the learned model's predictions -- it matches the system's underlying stochasticity when evaluated on states near those from the training dataset, and it predicts highly stochastic dynamics when evaluated on states beyond the training regime. The proposed neural SDEs can be evaluated quickly enough for use in model predictive control algorithms, or they can be used as simulators for model-based reinforcement learning. Furthermore, they make accurate predictions over long time horizons, even when trained on small datasets that cover limited regions of the state space. We demonstrate these capabilities th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#31070;&#32463;-ExpTanh&#21442;&#25968;&#21270;&#30340;&#26032;&#22411;&#36718;&#32974;&#21147;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#29616;&#26377;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26694;&#26550;&#20013;&#35299;&#26512;&#21047;&#24335;&#36718;&#32974;&#27169;&#22411;&#30340;&#26367;&#20195;&#21697;,&#25104;&#21151;&#23454;&#29616;&#20102;&#20165;&#21033;&#29992;&#23569;&#20110;&#19977;&#20998;&#38047;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33258;&#20027;&#28418;&#31227;</title><link>http://arxiv.org/abs/2306.06330</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#36718;&#32974;&#27169;&#22411;&#30340;&#19977;&#20998;&#38047;&#25968;&#25454;&#33258;&#20027;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Autonomous Drifting with 3 Minutes of Data via Learned Tire Models. (arXiv:2306.06330v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#31070;&#32463;-ExpTanh&#21442;&#25968;&#21270;&#30340;&#26032;&#22411;&#36718;&#32974;&#21147;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#29616;&#26377;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26694;&#26550;&#20013;&#35299;&#26512;&#21047;&#24335;&#36718;&#32974;&#27169;&#22411;&#30340;&#26367;&#20195;&#21697;,&#25104;&#21151;&#23454;&#29616;&#20102;&#20165;&#21033;&#29992;&#23569;&#20110;&#19977;&#20998;&#38047;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33258;&#20027;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38468;&#30528;&#26497;&#38480;&#38468;&#36817;&#65292;&#36718;&#32974;&#25152;&#20135;&#29983;&#30340;&#21147;&#26159;&#38750;&#32447;&#24615;&#21644;&#38169;&#32508;&#22797;&#26434;&#30340;&#12290;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#39640;&#25928;&#12289;&#20934;&#30830;&#30340;&#24314;&#27169;&#21487;&#20197;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#21147;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#31070;&#32463;-ExpTanh&#21442;&#25968;&#21270;&#30340;&#26032;&#22411;&#36718;&#32974;&#21147;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#28385;&#36275;&#29289;&#29702;&#27934;&#23519;&#21147;&#20551;&#35774;&#65292;&#21516;&#26102;&#20855;&#26377;&#36275;&#22815;&#30340;&#20445;&#30495;&#24230;&#65292;&#20197;&#30452;&#25509;&#20174;&#36710;&#36742;&#29366;&#24577;&#27979;&#37327;&#20013;&#25429;&#25417;&#39640;&#38454;&#25928;&#24212;&#12290;&#23427;&#20204;&#34987;&#29992;&#20316;&#29616;&#26377;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26694;&#26550;&#20013;&#35299;&#26512;&#21047;&#24335;&#36718;&#32974;&#27169;&#22411;&#30340;&#26367;&#20195;&#21697;&#12290;&#36890;&#36807;&#20351;&#29992;Toyota Supra&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23569;&#20110;&#19977;&#20998;&#38047;&#30340;&#39550;&#39542;&#25968;&#25454;&#36275;&#20197;&#22312;&#21508;&#31181;&#36712;&#36857;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33258;&#20027;&#28418;&#31227;&#65292;&#26368;&#39640;&#36895;&#24230;&#21487;&#36798;45mph&#12290;&#19982;&#22522;&#20934;&#27169;&#22411;&#30340;&#27604;&#36739;&#26174;&#31034;&#36712;&#36857;&#25552;&#39640;&#20102;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Near the limits of adhesion, the forces generated by a tire are nonlinear and intricately coupled. Efficient and accurate modelling in this region could improve safety, especially in emergency situations where high forces are required. To this end, we propose a novel family of tire force models based on neural ordinary differential equations and a neural-ExpTanh parameterization. These models are designed to satisfy physically insightful assumptions while also having sufficient fidelity to capture higher-order effects directly from vehicle state measurements. They are used as drop-in replacements for an analytical brush tire model in an existing nonlinear model predictive control framework. Experiments with a customized Toyota Supra show that scarce amounts of driving data -- less than three minutes -- is sufficient to achieve high-performance autonomous drifting on various trajectories with speeds up to 45mph. Comparisons with the benchmark model show a $4 \times$ improvement in track
&lt;/p&gt;</description></item><item><title>HIPODE&#26159;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#25509;&#36817;&#25968;&#25454;&#38598;&#20998;&#24067;&#20013;&#28508;&#22312;&#39640;&#20215;&#20540;&#29366;&#24577;&#30340;&#20505;&#36873;&#29366;&#24577;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;ORL&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06329</link><description>&lt;p&gt;
HIPODE: &#20174;&#31574;&#30053;&#35299;&#32806;&#35282;&#24230;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
HIPODE: Enhancing Offline Reinforcement Learning with High-Quality Synthetic Data from a Policy-Decoupled Approach. (arXiv:2306.06329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06329
&lt;/p&gt;
&lt;p&gt;
HIPODE&#26159;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#25509;&#36817;&#25968;&#25454;&#38598;&#20998;&#24067;&#20013;&#28508;&#22312;&#39640;&#20215;&#20540;&#29366;&#24577;&#30340;&#20505;&#36873;&#29366;&#24577;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;ORL&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;ORL&#65289;&#20316;&#20026;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#38745;&#24577;&#25968;&#25454;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24050;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#30340;&#38382;&#39064;&#24182;&#25913;&#21892;&#19979;&#28216;ORL&#30340;&#24615;&#33021;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25193;&#23637;&#25968;&#25454;&#38598;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#19982;&#29305;&#23450;&#31574;&#30053;&#65288;&#31574;&#30053;&#30456;&#20851;&#65289;&#30456;&#20851;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21482;&#33021;&#20445;&#35777;&#25903;&#25345;&#24403;&#21069;&#19979;&#28216;ORL&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#20854;&#20182;&#19979;&#28216;&#31574;&#30053;&#19978;&#30340;&#20351;&#29992;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#36890;&#24120;&#19981;&#33021;&#24456;&#22909;&#22320;&#25511;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#19979;&#28216;&#31574;&#30053;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ORL&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;&#39640;&#36136;&#37327;&#31574;&#30053;&#35299;&#32806;&#65288;HIPODE&#65289;&#12290;&#19968;&#26041;&#38754;&#65292;HIPODE&#36890;&#36807;&#36873;&#25321;&#28508;&#22312;&#39640;&#20215;&#20540;&#29366;&#24577;&#30340;&#20505;&#36873;&#29366;&#24577;&#20013;&#25509;&#36817;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#29366;&#24577;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;&#36127;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (ORL) has gained attention as a means of training reinforcement learning models using pre-collected static data. To address the issue of limited data and improve downstream ORL performance, recent work has attempted to expand the dataset's coverage through data augmentation. However, most of these methods are tied to a specific policy (policy-dependent), where the generated data can only guarantee to support the current downstream ORL policy, limiting its usage scope on other downstream policies. Moreover, the quality of synthetic data is often not well-controlled, which limits the potential for further improving the downstream policy. To tackle these issues, we propose \textbf{HI}gh-quality \textbf{PO}licy-\textbf{DE}coupled~(HIPODE), a novel data augmentation method for ORL. On the one hand, HIPODE generates high-quality synthetic data by selecting states near the dataset distribution with potentially high value among candidate states using the negative
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20195;&#25968;&#25299;&#25169;&#20013;&#30340;&#34920;&#31034;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#23450;&#20041;&#20986;&#19968;&#20010;&#21487;&#20197;&#20197;&#20219;&#24847;&#32500;&#24230;&#20026;&#36755;&#20837;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#26041;&#20415;&#65292;&#21482;&#38656;&#25351;&#23450;&#32593;&#32476;&#26550;&#26500;&#21644;&#31561;&#21464;&#24615;&#30340;&#32452;&#65292;&#19988;&#22312;&#20219;&#20309;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21487;&#20197;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.06327</link><description>&lt;p&gt;
&#20219;&#24847;&#32500;&#24230;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Any-dimensional equivariant neural networks. (arXiv:2306.06327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20195;&#25968;&#25299;&#25169;&#20013;&#30340;&#34920;&#31034;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#23450;&#20041;&#20986;&#19968;&#20010;&#21487;&#20197;&#20197;&#20219;&#24847;&#32500;&#24230;&#20026;&#36755;&#20837;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#26041;&#20415;&#65292;&#21482;&#38656;&#25351;&#23450;&#32593;&#32476;&#26550;&#26500;&#21644;&#31561;&#21464;&#24615;&#30340;&#32452;&#65292;&#19988;&#22312;&#20219;&#20309;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21487;&#20197;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23558;&#20989;&#25968;&#25311;&#21512;&#21040;&#19968;&#32452;&#20855;&#26377;&#22266;&#23450;&#32500;&#24230;&#30340;&#36755;&#20837;/&#36755;&#20986;&#23545;&#26469;&#23398;&#20064;&#26410;&#30693;&#26144;&#23556;&#12290;&#28982;&#21518;&#65292;&#22312;&#30456;&#21516;&#32500;&#24230;&#30340;&#36755;&#20837;&#19978;&#23450;&#20041;&#25311;&#21512;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26410;&#30693;&#26144;&#23556;&#20197;&#20219;&#24847;&#32500;&#24230;&#30340;&#36755;&#20837;&#20316;&#20026;&#36755;&#20837;&#65307;&#20363;&#22914;&#65292;&#23450;&#20041;&#22312;&#20219;&#24847;&#22823;&#23567;&#30340;&#22270;&#24418;&#19978;&#30340;&#22270;&#24418;&#21442;&#25968;&#21644;&#23450;&#20041;&#22312;&#20219;&#24847;&#25968;&#37327;&#31890;&#23376;&#19978;&#30340;&#29289;&#29702;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#20195;&#25968;&#25299;&#25169;&#20013;&#30340;&#26032;&#29616;&#35937;&#8212;&#8212;&#34920;&#31034;&#31283;&#23450;&#24615;&#65292;&#26469;&#23450;&#20041;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20351;&#29992;&#22266;&#23450;&#32500;&#24230;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#25193;&#23637;&#25509;&#21463;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#20351;&#29992;&#65292;&#21482;&#38656;&#35201;&#32593;&#32476;&#26550;&#26500;&#21644;&#31561;&#21464;&#24615;&#30340;&#32452;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#35757;&#32451;&#36807;&#31243;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#31616;&#21333;&#24320;&#28304;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional supervised learning aims to learn an unknown mapping by fitting a function to a set of input-output pairs with a fixed dimension. The fitted function is then defined on inputs of the same dimension. However, in many settings, the unknown mapping takes inputs in any dimension; examples include graph parameters defined on graphs of any size and physics quantities defined on an arbitrary number of particles. We leverage a newly-discovered phenomenon in algebraic topology, called representation stability, to define equivariant neural networks that can be trained with data in a fixed dimension and then extended to accept inputs in any dimension. Our approach is user-friendly, requiring only the network architecture and the groups for equivariance, and can be combined with any training procedure. We provide a simple open-source implementation of our methods and offer preliminary numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#26377;&#29992;&#30340;&#29616;&#23454;&#26102;&#38388;&#24207;&#21015;&#21453;&#20107;&#23454;&#35770;&#35777;(CFs)&#65292;&#20197;&#24110;&#21161;&#21307;&#29983;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.06325</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#35770;&#35777;&#21521;&#21307;&#29983;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Explaining a machine learning decision to physicians via counterfactuals. (arXiv:2306.06325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#26377;&#29992;&#30340;&#29616;&#23454;&#26102;&#38388;&#24207;&#21015;&#21453;&#20107;&#23454;&#35770;&#35777;(CFs)&#65292;&#20197;&#24110;&#21161;&#21307;&#29983;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#20010;&#21307;&#30103;&#20445;&#20581;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#26159;&#23427;&#20204;&#22312;&#21307;&#38498;&#20013;&#34987;&#37319;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#32771;&#34385;&#29992;&#21453;&#20107;&#23454;&#35770;&#35777;(CFs)&#26469;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#21363;&#20551;&#35774;&#23558;&#32467;&#26524;&#32763;&#36716;&#30340;&#24773;&#20917;&#12290;&#29305;&#21035;&#22320;&#65292;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#30340;CFs&#65292;&#21463;&#21040;&#21307;&#29983;&#35752;&#35770;&#21644;&#25512;&#29702;&#20915;&#31574;&#30340;&#21551;&#21457;: &#8220;&#22914;&#26524;&#24739;&#32773;&#30340;&#34880;&#21387;&#26356;&#20302;&#19988;&#19979;&#38477;&#65292;&#25105;&#20250;&#32473;&#20182;&#19968;&#20010;&#34880;&#31649;&#32039;&#24352;&#32032;&#12290;&#8221; &#35752;&#35770;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#29305;&#21035;&#26377;&#24847;&#20041;&#30340;CFs&#30340;&#20851;&#38190;&#23646;&#24615;: &#29983;&#29702;&#21512;&#29702;&#24615;&#12289;&#20219;&#21153;&#30456;&#20851;&#24615;&#21644;&#31232;&#30095;&#25200;&#21160;&#12290;&#36807;&#21435;CF&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#19981;&#33021;&#28385;&#36275;&#36825;&#20123;&#23646;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#29616;&#23454;&#26102;&#38388;&#24207;&#21015;CFs&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#29616;&#23454;&#30340;&#26102;&#38388;&#24207;&#21015;CFs&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models perform well on several healthcare tasks and can help reduce the burden on the healthcare system. However, the lack of explainability is a major roadblock to their adoption in hospitals. \textit{How can the decision of an ML model be explained to a physician?} The explanations considered in this paper are counterfactuals (CFs), hypothetical scenarios that would have resulted in the opposite outcome. Specifically, time-series CFs are investigated, inspired by the way physicians converse and reason out decisions `I would have given the patient a vasopressor if their blood pressure was lower and falling'. Key properties of CFs that are particularly meaningful in clinical settings are outlined: physiological plausibility, relevance to the task and sparse perturbations. Past work on CF generation does not satisfy these properties, specifically plausibility in that realistic time-series CFs are not generated. A variational autoencoder (VAE)-based approach is proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22810;&#23618;&#29983;&#25104;&#22120;&#27169;&#22411;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#22810;&#23618;&#29983;&#25104;&#22120;&#20026;&#39592;&#24178;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;EBM&#20808;&#39564;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20998;&#23618;&#34920;&#31034;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20462;&#22797;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06323</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#23618;&#29983;&#25104;&#22120;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;EBM&#20808;&#39564;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Joint Latent Space EBM Prior Model for Multi-layer Generator. (arXiv:2306.06323v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22810;&#23618;&#29983;&#25104;&#22120;&#27169;&#22411;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#22810;&#23618;&#29983;&#25104;&#22120;&#20026;&#39592;&#24178;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;EBM&#20808;&#39564;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20998;&#23618;&#34920;&#31034;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20462;&#22797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22810;&#23618;&#29983;&#25104;&#22120;&#27169;&#22411;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#22810;&#23618;&#29983;&#25104;&#22120;&#27169;&#22411;&#22312;&#29983;&#25104;&#22120;&#20043;&#19978;&#26500;&#24314;&#22810;&#23618;&#28508;&#22312;&#21464;&#37327;&#20316;&#20026;&#20808;&#39564;&#27169;&#22411;&#65292;&#26377;&#21033;&#20110;&#23398;&#20064;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20998;&#23618;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20808;&#39564;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20551;&#35774;&#38750;&#20449;&#24687;&#65288;&#26465;&#20214;&#65289;&#39640;&#26031;&#20998;&#24067;&#26469;&#19987;&#27880;&#20110;&#24314;&#27169;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#23618;&#38388;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#23398;&#20064;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#20808;&#39564;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#22810;&#23618;&#29983;&#25104;&#22120;&#20026;&#39592;&#24178;&#30340;&#33021;&#37327;&#22522;&#27169;&#22411;&#65288;EBM&#65289;&#30340;&#20808;&#39564;&#27169;&#22411;&#12290;&#36825;&#31181;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;EBM &#20808;&#39564;&#27169;&#22411;&#36890;&#36807;&#23618;&#38388;&#33021;&#37327;&#39033;&#25429;&#33719;&#27599;&#23618;&#20869;&#30340;&#20869;&#37096;&#20851;&#31995;&#65292;&#24182;&#23545;&#19981;&#21516;&#23618;&#30340;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#32852;&#21512;&#20462;&#27491;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#24320;&#21457;&#20102;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#26041;&#26696;&#65292;&#20854;&#20013;&#21516;&#26102;&#23398;&#20064;&#29983;&#25104;&#22120;&#21644;EBM&#20808;&#39564;&#27169;&#22411;&#12290;&#22312;&#29983;&#25104;&#22270;&#20687;&#24314;&#27169;&#21644;&#20462;&#22797;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20960;&#20010;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20808;&#39564;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20462;&#22797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the fundamental problem of learning multi-layer generator models. The multi-layer generator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learning complex data distribution and hierarchical representations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distributions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we propose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer generator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estimation (MLE), which involves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;DL-based MSA&#39046;&#22495;&#32570;&#20047;&#26631;&#20934;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#21644;&#21333;&#35789;&#23545;&#40784;&#25216;&#26415;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#26469;&#35774;&#35745;&#31649;&#36947;&#27969;&#31243;&#65292;&#26500;&#24314;&#20102;&#38463;&#25289;&#20271;&#35821;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.06322</link><description>&lt;p&gt;
&#38754;&#21521;&#38463;&#25289;&#20271;&#35821;&#24773;&#24863;&#20998;&#26512;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Towards Arabic Multimodal Dataset for Sentiment Analysis. (arXiv:2306.06322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;DL-based MSA&#39046;&#22495;&#32570;&#20047;&#26631;&#20934;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#21644;&#21333;&#35789;&#23545;&#40784;&#25216;&#26415;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#26469;&#35774;&#35745;&#31649;&#36947;&#27969;&#31243;&#65292;&#26500;&#24314;&#20102;&#38463;&#25289;&#20271;&#35821;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;(MSA)&#24050;&#25104;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#20013;&#24515;&#30740;&#31350;&#26041;&#21521;&#12290;&#36825;&#31181;&#26222;&#21450;&#24471;&#30410;&#20110;&#24847;&#35265;&#23545;&#20960;&#20046;&#25152;&#26377;&#20154;&#31867;&#27963;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#25104;&#20026;&#25105;&#20204;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;(DL)&#27169;&#22411;&#30340;&#39640;&#25928;&#24615;&#20063;&#24050;&#22312;&#22810;&#31181;&#35199;&#26041;&#35821;&#35328;&#20013;&#24471;&#21040;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#38463;&#25289;&#20271;&#35821;DL-based MSA&#20173;&#22788;&#20110;&#21021;&#22987;&#38454;&#27573;&#12290;&#26412;&#25991;&#30340;&#25506;&#31350;&#30446;&#26631;&#26377;&#20004;&#20010;&#26041;&#38754;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31649;&#36947;&#27969;&#31243;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#20197;&#21450;&#21333;&#35789;&#23545;&#40784;&#25216;&#26415;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#24110;&#21161;&#26500;&#24314;&#25105;&#20204;&#30340;&#38463;&#25289;&#20271;&#35821;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#36755;&#20986;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#36739;&#23567;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38463;&#25289;&#20271;&#35821;&#22810;&#27169;&#24577;&#20173;&#28982;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis (MSA) has recently become a centric research direction for many real-world applications. This proliferation is due to the fact that opinions are central to almost all human activities and are key influencers of our behaviors. In addition, the recent deployment of Deep Learning-based (DL) models has proven their high efficiency for a wide range of Western languages. In contrast, Arabic DL-based multimodal sentiment analysis (MSA) is still in its infantile stage due, mainly, to the lack of standard datasets. In this paper, our investigation is twofold. First, we design a pipeline that helps building our Arabic Multimodal dataset leveraging both state-of-the-art transformers and feature extraction tools within word alignment techniques. Thereafter, we validate our dataset using state-of-the-art transformer-based model dealing with multimodality. Despite the small size of the outcome dataset, experiments show that Arabic multimodality is very promising
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#39046;&#22495;&#20132;&#20114;&#20449;&#24687;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#26469;&#36827;&#34892;&#26032;&#39046;&#22495;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20010;AI&#21161;&#25163;&#24212;&#29992;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06302</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#30693;&#35782;&#22686;&#24378;&#22312;AI&#21161;&#25163;&#24212;&#29992;&#20013;&#30340;&#38646;&#26679;&#26412;&#21644;&#22810;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Knowledge Enhancement for Zero-Shot and Multi-Domain Recommendation in an AI Assistant Application. (arXiv:2306.06302v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#39046;&#22495;&#20132;&#20114;&#20449;&#24687;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#26469;&#36827;&#34892;&#26032;&#39046;&#22495;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20010;AI&#21161;&#25163;&#24212;&#29992;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#21830;&#19994;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#23558;&#26032;&#29992;&#25143;&#25972;&#21512;&#36827;&#21435;&#12290;&#30001;&#20110;&#29992;&#25143;&#32463;&#24120;&#22312;&#19981;&#21516;&#39046;&#22495;&#19982;&#20869;&#23481;&#36827;&#34892;&#20132;&#20114;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#29992;&#25143;&#22312;&#20043;&#21069;&#30340;&#39046;&#22495;&#20013;&#30340;&#20132;&#20114;&#26469;&#25913;&#21892;&#20854;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#25512;&#33616;&#65288;&#22810;&#39046;&#22495;&#25512;&#33616;&#65289;&#12290;&#30693;&#35782;&#22270;&#22686;&#24378;&#30340;&#21333;&#19968;&#39046;&#22495;&#25512;&#33616;&#65288;&#30693;&#35782;&#22270;&#22686;&#24378;&#65289;&#30340;&#30740;&#31350;&#32447;&#31243;&#29420;&#31435;&#20110;&#27492;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#26469;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#23558;&#36825;&#20123;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65306;&#21033;&#29992;&#20854;&#20182;&#39046;&#22495;&#20013;&#30340;&#20132;&#20114;&#20449;&#24687;&#20197;&#21450;&#22806;&#37096;&#30693;&#35782;&#22270;&#26469;&#36827;&#34892;&#26032;&#39046;&#22495;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24819;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#20174;&#25968;&#30334;&#19975;&#29992;&#25143;&#35831;&#27714;&#30340;&#35270;&#39057;&#12289;&#38899;&#20048;&#21644;&#20070;&#31821;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#19968;&#20010;AI&#21161;&#25163;&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have found significant commercial success but still struggle with integrating new users. Since users often interact with content in different domains, it is possible to leverage a user's interactions in previous domains to improve that user's recommendations in a new one (multi-domain recommendation). A separate research thread on knowledge graph enhancement uses external knowledge graphs to improve single domain recommendations (knowledge graph enhancement). Both research threads incorporate related information to improve predictions in a new domain. We propose in this work to unify these approaches: Using information from interactions in other domains as well as external knowledge graphs to make predictions in a new domain that would be impossible with either information source alone. We apply these ideas to a dataset derived from millions of users' requests for content across three domains (videos, music, and books) in a live virtual assistant application. We dem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#21709;&#24212;&#26102;&#38388;&#32771;&#34385;&#36827;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36873;&#25321;&#39044;&#27979;&#21644;&#20989;&#25968;&#20272;&#35745;&#20013;&#65292;&#20351;&#24471;&#26356;&#23481;&#26131;&#22312;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#36924;&#36817;&#27169;&#22411;&#19979;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#21644;&#20989;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06296</link><description>&lt;p&gt;
&#21709;&#24212;&#26102;&#38388;&#25913;&#21892;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#24863;&#30693;&#21644;&#20559;&#22909;&#36873;&#25321;&#39044;&#27979;&#19982;&#20989;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Response Time Improves Choice Prediction and Function Estimation for Gaussian Process Models of Perception and Preferences. (arXiv:2306.06296v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#21709;&#24212;&#26102;&#38388;&#32771;&#34385;&#36827;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36873;&#25321;&#39044;&#27979;&#21644;&#20989;&#25968;&#20272;&#35745;&#20013;&#65292;&#20351;&#24471;&#26356;&#23481;&#26131;&#22312;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#36924;&#36817;&#27169;&#22411;&#19979;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#21644;&#20989;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;&#21644;&#24515;&#29702;&#29289;&#29702;&#23398;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#20165;&#32771;&#34385;&#20108;&#36827;&#21046;&#21709;&#24212;&#25968;&#25454;&#65292;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#25165;&#33021;&#20934;&#30830;&#23398;&#20064;&#20559;&#22909;&#25110;&#24863;&#30693;&#26816;&#27979;&#38408;&#20540;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#20570;&#20986;&#27599;&#20010;&#36873;&#25321;&#30340;&#21709;&#24212;&#26102;&#38388;&#65288;RT&#65289;&#25429;&#33719;&#20102;&#20851;&#20110;&#20915;&#31574;&#36807;&#31243;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#20294;&#26159;&#29616;&#26377;&#27169;&#22411;&#23558;RT&#29992;&#20110;&#36873;&#25321;&#39044;&#27979;&#30340;&#35774;&#32622;&#26159;&#20805;&#20998;&#21442;&#25968;&#21270;&#30340;&#25110;&#31163;&#25955;&#30340;&#21050;&#28608;&#35774;&#32622;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#36873;&#25321;RT&#30340;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#27169;&#22411; - &#25193;&#25955;&#21028;&#23450;&#27169;&#22411;&#65288;DDM&#65289;&#65292;&#19981;&#20801;&#35768;&#21487;&#22788;&#29702;&#65292;&#21487;&#24494;&#20998;&#30340;&#25512;&#26029;&#12290;&#22240;&#27492;&#65292;DDM&#19981;&#33021;&#36731;&#26494;&#22320;&#19982;&#28789;&#27963;&#30340;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#36924;&#36817;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#38598;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#36817;&#20284;DDM&#20284;&#28982;&#20989;&#25968;&#65292;&#20351;&#29992;&#24050;&#30693;&#30340;&#24102;&#20559;&#19977;&#21442;&#25968;&#20998;&#24067;&#26063;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26032;&#30340;&#20284;&#28982;&#20989;&#25968;&#23558;RT&#32435;&#20837;&#20108;&#36827;&#21046;&#36873;&#25321;&#30340;GP&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;RT&#36873;&#25321;GP&#20351;&#24471;&#26356;&#23481;&#26131;&#22312;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#36924;&#36817;&#27169;&#22411;&#19979;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#21644;&#20989;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for human choice prediction in preference learning and psychophysics often consider only binary response data, requiring many samples to accurately learn preferences or perceptual detection thresholds. The response time (RT) to make each choice captures additional information about the decision process, however existing models incorporating RTs for choice prediction do so in fully parametric settings or over discrete stimulus sets. This is in part because the de-facto standard model for choice RTs, the diffusion decision model (DDM), does not admit tractable, differentiable inference. The DDM thus cannot be easily integrated with flexible models for continuous, multivariate function approximation, particularly Gaussian process (GP) models. We propose a novel differentiable approximation to the DDM likelihood using a family of known, skewed three-parameter distributions. We then use this new likelihood to incorporate RTs into GP models for binary choices. Our RT-choice GPs enable
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;PLPCA&#65292;&#29992;&#20110;&#35299;&#20915;PCA&#22312;&#24494;&#38453;&#21015;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#25345;&#32493;&#35889;&#22270;&#29702;&#35770;&#32467;&#21512;&#25289;&#26222;&#25289;&#26031;&#22686;&#24378;&#26469;&#23454;&#29616;&#22810;&#23610;&#24230;&#20998;&#26512;&#21644;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.06292</link><description>&lt;p&gt;
PLPCA&#65306;&#29992;&#20110;&#24494;&#38453;&#21015;&#25968;&#25454;&#20998;&#26512;&#30340;&#25345;&#32493;&#25289;&#26222;&#25289;&#26031;&#22686;&#24378;PCA
&lt;/p&gt;
&lt;p&gt;
PLPCA: Persistent Laplacian Enhanced-PCA for Microarray Data Analysis. (arXiv:2306.06292v1 [math.AT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;PLPCA&#65292;&#29992;&#20110;&#35299;&#20915;PCA&#22312;&#24494;&#38453;&#21015;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#25345;&#32493;&#35889;&#22270;&#29702;&#35770;&#32467;&#21512;&#25289;&#26222;&#25289;&#26031;&#22686;&#24378;&#26469;&#23454;&#29616;&#22810;&#23610;&#24230;&#20998;&#26512;&#21644;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#19968;&#30452;&#26159;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#38477;&#32500;&#12290;&#23427;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20174;&#25104;&#21315;&#19978;&#19975;&#30340;&#22522;&#22240;&#20013;&#35782;&#21035;&#20986;&#19968;&#32452;&#30142;&#30149;&#30456;&#20851;&#30340;&#22522;&#22240;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;PCA&#20855;&#26377;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#38459;&#30861;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#20837;&#20102;&#20998;&#31867;&#27495;&#20041;&#65292;&#24182;&#26410;&#25429;&#25417;&#21040;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#25289;&#26222;&#25289;&#26031;&#22686;&#24378;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PLPCA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, Principal Component Analysis (PCA) has served as the baseline approach for dimensionality reduction in gene expression data analysis. It primary objective is to identify a subset of disease-causing genes from a vast pool of thousands of genes. However, PCA possesses inherent limitations that hinder its interpretability, introduce classification ambiguity, and fail to capture complex geometric structures in the data. Although these limitations have been partially addressed in the literature by incorporating various regularizers such as graph Laplacian regularization, existing improved PCA methods still face challenges related to multiscale analysis and capturing higher-order interactions in the data. To address these challenges, we propose a novel approach called Persistent Laplacian-enhanced Principal Component Analysis (PLPCA). PLPCA amalgamates the advantages of earlier regularized PCA methods with persistent spectral graph theory, specifically persistent Laplacians d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;MOLAR&#65292;&#23427;&#21033;&#29992;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24322;&#36136;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#29420;&#31435;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.06291</link><description>&lt;p&gt;
&#26368;&#20248;&#24322;&#26500;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Heterogeneous Collaborative Linear Regression and Contextual Bandits. (arXiv:2306.06291v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;MOLAR&#65292;&#23427;&#21033;&#29992;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24322;&#36136;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#29420;&#31435;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#26469;&#33258;&#20110;&#20960;&#20010;&#21487;&#33021;&#26159;&#24322;&#26500;&#30340;&#26469;&#28304;&#12290;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20849;&#24615;&#25552;&#39640;&#25928;&#29575;&#65292;&#21516;&#26102;&#32771;&#34385;&#21487;&#33021;&#20986;&#29616;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20363;&#30340;&#30456;&#20851;&#21442;&#25968;&#31561;&#20110;&#20840;&#23616;&#21442;&#25968;&#21152;&#19978;&#19968;&#20010;&#31232;&#30095;&#30340;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MOLAR&#30340;&#26032;&#22411;&#20108;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#23427;&#36890;&#36807;&#39318;&#20808;&#26500;&#24314;&#23454;&#20363;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#30340;&#36880;&#39033;&#20013;&#20301;&#25968;&#65292;&#28982;&#21518;&#23558;&#23454;&#20363;&#29305;&#23450;&#20272;&#35745;&#20540;&#25910;&#32553;&#21040;&#20013;&#20301;&#25968;&#38468;&#36817;&#26469;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#12290;&#19982;&#29420;&#31435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30456;&#27604;&#65292;MOLAR&#25552;&#39640;&#20102;&#20272;&#35745;&#35823;&#24046;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;MOLAR&#24212;&#29992;&#20110;&#24320;&#21457;&#29992;&#20110;&#31232;&#30095;&#24322;&#26500;&#21327;&#21516;&#19978;&#19979;&#25991;&#33218;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#29420;&#31435;&#33218;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#36129;&#29486;&#20248;&#20110;&#20808;&#21069;&#22312;&#25991;&#29486;&#20013;&#25253;&#36947;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large and complex datasets are often collected from several, possibly heterogeneous sources. Collaborative learning methods improve efficiency by leveraging commonalities across datasets while accounting for possible differences among them. Here we study collaborative linear regression and contextual bandits, where each instance's associated parameters are equal to a global parameter plus a sparse instance-specific term. We propose a novel two-stage estimator called MOLAR that leverages this structure by first constructing an entry-wise median of the instances' linear regression estimates, and then shrinking the instance-specific estimates towards the median. MOLAR improves the dependence of the estimation error on the data dimension, compared to independent least squares estimates. We then apply MOLAR to develop methods for sparsely heterogeneous collaborative contextual bandits, which lead to improved regret guarantees compared to independent bandit methods. We further show that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35760;&#24405;&#20102;&#19968;&#27425;&#40657;&#23458;&#26494;&#27963;&#21160;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#29305;&#24615;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.06283</link><description>&lt;p&gt;
LLMs&#22914;&#20309;&#25913;&#21464;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#40657;&#23458;&#39532;&#25289;&#26494;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. (arXiv:2306.06283v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35760;&#24405;&#20102;&#19968;&#27425;&#40657;&#23458;&#26494;&#27963;&#21160;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#29305;&#24615;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#38750;&#24120;&#22797;&#26434;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#25110;&#35745;&#31639;&#25216;&#26415;&#35299;&#20915;&#20102;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#20013;&#26377;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36755;&#20837;&#38656;&#35201;&#38750;&#24120;&#29305;&#23450;&#24418;&#24335;&#30340;&#32467;&#26500;&#20197;&#21450;&#24037;&#20855;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#25152;&#24102;&#26469;&#21487;&#29992;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;&#25361;&#25112;&#12290;&#21152;&#19978;&#36825;&#20123;&#23398;&#31185;&#20013;&#30340;&#22823;&#22810;&#25968;&#25968;&#25454;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#20107;&#23454;&#65292;&#20351;&#24471;&#36825;&#20123;&#24037;&#20855;&#30340;&#25928;&#29575;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35760;&#24405;&#20102;&#20851;&#20110;LLMs&#30340;&#40657;&#23458;&#26494;&#27963;&#21160;&#20013;&#26500;&#24314;&#30340;&#39033;&#30446;&#12290;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#29305;&#24615;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#21508;&#31181;&#21508;&#26679;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#36825;&#20123;&#27169;&#22411;&#25913;&#21464;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemistry and materials science are complex. Recently, there have been great successes in addressing this complexity using data-driven or computational techniques. Yet, the necessity of input structured in very specific forms and the fact that there is an ever-growing number of tools creates usability and accessibility challenges. Coupled with the reality that much data in these disciplines is unstructured, the effectiveness of these tools is limited.  Motivated by recent works that indicated that large language models (LLMs) might help address some of these issues, we organized a hackathon event on the applications of LLMs in chemistry, materials science, and beyond. This article chronicles the projects built as part of this hackathon. Participants employed LLMs for various applications, including predicting properties of molecules and materials, designing novel interfaces for tools, extracting knowledge from unstructured data, and developing new educational applications.  The diverse
&lt;/p&gt;</description></item><item><title>&#33021;&#37327;&#32791;&#25955;&#36827;&#21270;&#28145;&#24230;&#31639;&#23376;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#36816;&#31639;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20026;&#19968;&#31867;&#20559;&#24494;&#20998;&#26041;&#31243;&#25552;&#20379;&#25968;&#20540;&#35299;&#24182;&#20445;&#30041;&#20854;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#25903;&#36335;&#32593;&#32476;&#32534;&#30721;&#19981;&#21516;&#30340;&#36755;&#20837;&#20989;&#25968;&#65292;&#24178;&#32447;&#32593;&#32476;&#35780;&#20272;&#36755;&#20986;&#20989;&#25968;&#65292;&#32463;&#36807;&#35757;&#32451;&#21487;&#29983;&#25104;&#36816;&#31639;&#31526;&#36817;&#20284;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.06281</link><description>&lt;p&gt;
&#33021;&#37327;&#32791;&#25955;&#36827;&#21270;&#28145;&#24230;&#31639;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Energy-Dissipative Evolutionary Deep Operator Neural Networks. (arXiv:2306.06281v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06281
&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#32791;&#25955;&#36827;&#21270;&#28145;&#24230;&#31639;&#23376;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#36816;&#31639;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20026;&#19968;&#31867;&#20559;&#24494;&#20998;&#26041;&#31243;&#25552;&#20379;&#25968;&#20540;&#35299;&#24182;&#20445;&#30041;&#20854;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#25903;&#36335;&#32593;&#32476;&#32534;&#30721;&#19981;&#21516;&#30340;&#36755;&#20837;&#20989;&#25968;&#65292;&#24178;&#32447;&#32593;&#32476;&#35780;&#20272;&#36755;&#20986;&#20989;&#25968;&#65292;&#32463;&#36807;&#35757;&#32451;&#21487;&#29983;&#25104;&#36816;&#31639;&#31526;&#36817;&#20284;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#32791;&#25955;&#36827;&#21270;&#28145;&#24230;&#31639;&#23376;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#36816;&#31639;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20026;&#19968;&#31867;&#20559;&#24494;&#20998;&#26041;&#31243;&#31181;&#23376;&#25552;&#20379;&#25968;&#20540;&#35299;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#20363;&#22914;&#24102;&#26377;&#19981;&#21516;&#21442;&#25968;&#25110;&#19981;&#21516;&#21021;&#22987;&#26465;&#20214;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#32593;&#32476;&#30001;&#20004;&#20010;&#23376;&#32593;&#32476;&#32452;&#25104;&#65306;&#25903;&#36335;&#32593;&#32476;&#21644;&#24178;&#32447;&#32593;&#32476;&#12290;&#23545;&#20110;&#30446;&#26631;&#36816;&#31639;&#31526; G&#65292;&#25903;&#36335;&#32593;&#32476;&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#20256;&#24863;&#22120;&#19978;&#32534;&#30721;&#19981;&#21516;&#30340;&#36755;&#20837;&#20989;&#25968; u&#65292;&#32780;&#24178;&#32447;&#32593;&#32476;&#35780;&#20272;&#20219;&#20309;&#20301;&#32622;&#30340;&#36755;&#20986;&#20989;&#25968;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#35780;&#20272;&#30340;&#36755;&#20986; q &#21644;&#39044;&#26399;&#36755;&#20986; G(u)(y) &#20043;&#38388;&#30340;&#35823;&#24046;&#65292;DeepONet &#29983;&#25104; G &#36816;&#31639;&#31526;&#30340;&#33391;&#22909;&#36817;&#20284;&#12290;&#20026;&#20102;&#20445;&#30041;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#37325;&#35201;&#29289;&#29702;&#29305;&#24615;&#65292;&#22914;&#33021;&#37327;&#32791;&#25955;&#23450;&#24459;&#65292;&#25105;&#20204;&#37319;&#29992;&#26631;&#37327;&#36741;&#21161;&#21464;&#37327;&#27861;&#29983;&#25104;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#23427;&#24341;&#20837;&#20102;&#20462;&#25913;&#21518;&#30340;&#33021;&#37327;&#24182;&#23454;&#29616;&#20102;&#26080;&#26465;&#20214;&#30340;&#33021;&#37327;&#32791;&#25955;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Dissipative Evolutionary Deep Operator Neural Network is an operator learning neural network. It is designed to seed numerical solutions for a class of partial differential equations instead of a single partial differential equation, such as partial differential equations with different parameters or different initial conditions. The network consists of two sub-networks, the Branch net and the Trunk net. For an objective operator G, the Branch net encodes different input functions u at the same number of sensors, and the Trunk net evaluates the output function at any location. By minimizing the error between the evaluated output q and the expected output G(u)(y), DeepONet generates a good approximation of the operator G. In order to preserve essential physical properties of PDEs, such as the Energy Dissipation Law, we adopt a scalar auxiliary variable approach to generate the minimization problem. It introduces a modified energy and enables unconditional energy dissipation law a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;Cox&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06276</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#30340;&#22522;&#22240;&#34920;&#36798;&#20540;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values. (arXiv:2306.06276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;Cox&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#20102;&#22810;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#20316;&#20026;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#65292;&#20197;&#22522;&#20110;&#32959;&#30244;&#36716;&#24405;&#32452;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#27809;&#26377;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26377;&#35268;&#21017;&#21270;&#30340;Cox&#22238;&#24402;&#26174;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#21644;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;ANN&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22270;&#20687;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#33391;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#32959;&#30244;&#22522;&#22240;&#34920;&#36798;&#21644;&#20020;&#24202;&#25968;&#25454;&#65292;&#20197;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26469;&#35757;&#32451;Cox&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#12290;&#20351;&#29992;&#26469;&#33258;The Cancer Genome Atlas&#65288;TCGA&#65289;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;Cox&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#24449;&#65292;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several artificial neural networks (ANNs) have recently been developed as the Cox proportional hazard model for predicting cancer prognosis based on tumor transcriptome. However, they have not demonstrated significantly better performance than the traditional Cox regression with regularization. Training an ANN with high prediction power is challenging in the presence of a limited number of data samples and a high-dimensional feature space. Recent advancements in image classification have shown that contrastive learning can facilitate further learning tasks by learning good feature representation from a limited number of data samples. In this paper, we applied supervised contrastive learning to tumor gene expression and clinical data to learn feature representations in a low-dimensional space. We then used these learned features to train the Cox model for predicting cancer prognosis. Using data from The Cancer Genome Atlas (TCGA), we demonstrated that our contrastive learning-based Cox 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21517;&#20026;DeepLCZChange&#65292;&#32467;&#21512;&#20102;&#31354;&#20013;LiDAR&#25968;&#25454;&#21644;Landsat 8&#21355;&#26143;&#30340;&#22320;&#34920;&#28201;&#24230;&#20135;&#21697;&#65292;&#29992;&#20110;&#30740;&#31350;&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#19982;&#24403;&#22320;&#27668;&#20505;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#32445;&#32422;&#24066;&#30340;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#22478;&#24066;&#26862;&#26519;&#30340;&#38477;&#28201;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.06269</link><description>&lt;p&gt;
DeepLCZChange&#65306;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#27668;&#20505;&#38887;&#24615;&#30340;&#36965;&#24863;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DeepLCZChange: A Remote Sensing Deep Learning Model Architecture for Urban Climate Resilience. (arXiv:2306.06269v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21517;&#20026;DeepLCZChange&#65292;&#32467;&#21512;&#20102;&#31354;&#20013;LiDAR&#25968;&#25454;&#21644;Landsat 8&#21355;&#26143;&#30340;&#22320;&#34920;&#28201;&#24230;&#20135;&#21697;&#65292;&#29992;&#20110;&#30740;&#31350;&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#19982;&#24403;&#22320;&#27668;&#20505;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#32445;&#32422;&#24066;&#30340;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#22478;&#24066;&#26862;&#26519;&#30340;&#38477;&#28201;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#32467;&#26500;&#20250;&#24433;&#21709;&#37117;&#24066;&#21306;&#30340;&#23616;&#37096;&#27668;&#20505;&#26465;&#20214;&#12290;&#20026;&#20102;&#25581;&#31034;&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#19982;&#24403;&#22320;&#27668;&#20505;&#20043;&#38388;&#30340;&#26426;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#27969;&#31243;&#65292;DeepLCZChange&#65292;&#20197;&#30456;&#20851;&#31354;&#20013;LiDAR&#25968;&#25454;&#32479;&#35745;&#25968;&#25454;&#19982;Landsat 8&#21355;&#26143;&#30340;&#22320;&#34920;&#28201;&#24230;&#20135;&#21697;&#30456;&#20851;&#32852;&#12290;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#25968;&#20540;&#23454;&#39564;&#21033;&#29992;&#32445;&#32422;&#24066;&#30340;&#30456;&#24212;&#36965;&#24863;&#25968;&#25454;&#39564;&#35777;&#20102;&#22478;&#24066;&#26862;&#26519;&#30340;&#38477;&#28201;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban land use structures impact local climate conditions of metropolitan areas. To shed light on the mechanism of local climate wrt. urban land use, we present a novel, data-driven deep learning architecture and pipeline, DeepLCZChange, to correlate airborne LiDAR data statistics with the Landsat 8 satellite's surface temperature product. A proof-of-concept numerical experiment utilizes corresponding remote sensing data for the city of New York to verify the cooling effect of urban forests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AS-GAN&#30340;&#25216;&#26415;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#20013;AS-GAN&#26377;&#25928;&#22320;&#23398;&#20064;&#24322;&#24120;&#29366;&#24577;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;&#22312;&#32447;&#21046;&#36896;&#31995;&#32479;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.06268</link><description>&lt;p&gt;
AS-GAN&#22686;&#24378;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#21046;&#36896;&#31995;&#32479;&#22312;&#32447;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention-stacked Generative Adversarial Network (AS-GAN)-empowered Sensor Data Augmentation for Online Monitoring of Manufacturing System. (arXiv:2306.06268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AS-GAN&#30340;&#25216;&#26415;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#20013;AS-GAN&#26377;&#25928;&#22320;&#23398;&#20064;&#24322;&#24120;&#29366;&#24577;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;&#22312;&#32447;&#21046;&#36896;&#31995;&#32479;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20808;&#36827;&#21046;&#36896;&#31995;&#32479;&#30340;&#22312;&#32447;&#24863;&#30693;&#30417;&#27979;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#24120;&#26465;&#20214;&#19979;&#25910;&#38598;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#20805;&#20998;&#30340;&#65292;&#36825;&#20250;&#23548;&#33268;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20005;&#37325;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#26469;&#22686;&#21152;&#21487;&#29992;&#30340;&#24322;&#24120;&#29366;&#24577;&#25968;&#25454;&#65288;&#21363;&#23569;&#25968;&#26679;&#26412;&#65289;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23569;&#25968;&#26679;&#26412;&#65292;&#23398;&#20064;&#24322;&#24120;&#29366;&#24577;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#25104;&#20026;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#20197;&#21450;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#22522;&#20110;GAN&#30340;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#20256;&#24863;&#22120;&#20449;&#21495;&#26159;&#25353;&#26102;&#38388;&#20174;&#21046;&#36896;&#31995;&#32479;&#20013;&#39034;&#24207;&#25910;&#38598;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#32771;&#34385;&#21040;&#26102;&#38388;&#30340;&#39034;&#24207;&#24615;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has been extensively adopted for the online sensing-based monitoring in advanced manufacturing systems. However, the sensor data collected under abnormal states are usually insufficient, leading to significant data imbalanced issue for supervised machine learning. A common solution for this issue is to incorporate data augmentation technique, i.e., augmenting the available abnormal states data (i.e., minority samples) via synthetic generation. To generate the high-quality minority samples effectively, it is vital to learn the underlying distribution of the abnormal states data. In recent years, the generative adversarial network (GAN)-based approaches become popular to learn data distribution as well as perform data augmentation. However, in practice, the quality of generated samples from GAN-based data augmentation may vary drastically. In addition, the sensor signals are collected sequentially by time from the manufacturing systems, which means the consideration
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20445;&#23432;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;StepMix&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#24179;&#34913;&#24320;&#21457;&#21644;&#25506;&#32034;&#65292;&#21516;&#26102;&#20445;&#35777;&#27599;&#20010;&#22238;&#21512;&#19981;&#36829;&#21453;&#20445;&#23432;&#38480;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2306.06265</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#22238;&#21512;&#38480;&#21046;&#30340;&#36817;&#20248;&#20445;&#23432;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Near-optimal Conservative Exploration in Reinforcement Learning under Episode-wise Constraints. (arXiv:2306.06265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20445;&#23432;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;StepMix&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#24179;&#34913;&#24320;&#21457;&#21644;&#25506;&#32034;&#65292;&#21516;&#26102;&#20445;&#35777;&#27599;&#20010;&#22238;&#21512;&#19981;&#36829;&#21453;&#20445;&#23432;&#38480;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20445;&#23432;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#20195;&#29702;&#30340;&#24615;&#33021;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#35777;&#39640;&#20110;&#26576;&#20010;&#29305;&#23450;&#38408;&#20540;&#12290;&#30740;&#31350;&#38024;&#23545;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#26631;&#31614;&#24335;&#22238;&#21512;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StepMix&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#22312;&#20445;&#35777;&#27599;&#20010;&#22238;&#21512;&#19981;&#36829;&#21453;&#20445;&#23432;&#38480;&#21046;&#30340;&#21069;&#25552;&#19979;&#65292;&#24179;&#34913;&#24320;&#21457;&#21644;&#25506;&#32034;&#12290;StepMix&#20855;&#26377;&#29420;&#29305;&#30340;&#28151;&#21512;&#31574;&#30053;&#35774;&#35745;&#65292;&#33258;&#36866;&#24212;&#22320;&#12289;&#24179;&#28369;&#22320;&#25554;&#20540;&#22522;&#32447;&#31574;&#30053;&#21644;&#20048;&#35266;&#31574;&#30053;&#20043;&#38388;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;StepMix&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#37327;&#32423;&#65292;&#35828;&#26126;&#36981;&#23432;&#20005;&#26684;&#30340;&#22238;&#21512;&#38480;&#21046;&#19981;&#20250;&#25439;&#23475;&#23398;&#20064;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;EpsMix&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates conservative exploration in reinforcement learning where the performance of the learning agent is guaranteed to be above a certain threshold throughout the learning process. It focuses on the tabular episodic Markov Decision Process (MDP) setting that has finite states and actions. With the knowledge of an existing safe baseline policy, an algorithm termed as StepMix is proposed to balance the exploitation and exploration while ensuring that the conservative constraint is never violated in each episode with high probability. StepMix features a unique design of a mixture policy that adaptively and smoothly interpolates between the baseline policy and the optimistic policy. Theoretical analysis shows that StepMix achieves near-optimal regret order as in the constraint-free setting, indicating that obeying the stringent episode-wise conservative constraint does not compromise the learning performance. Besides, a randomization-based EpsMix algorithm is also proposed
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#27979;&#37327;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29109;&#21450;KL&#25955;&#24230;&#31561;&#24230;&#37327;&#25351;&#26631;&#36827;&#34892;&#30693;&#35782;&#20462;&#25913;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#27979;&#37327;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.06264</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#21644;&#20462;&#25913;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Measuring and Modifying Factual Knowledge in Large Language Models. (arXiv:2306.06264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06264
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#27979;&#37327;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29109;&#21450;KL&#25955;&#24230;&#31561;&#24230;&#37327;&#25351;&#26631;&#36827;&#34892;&#30693;&#35782;&#20462;&#25913;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#27979;&#37327;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#20648;&#30528;&#20174;&#22823;&#37327;&#25991;&#26412;&#20013;&#33719;&#21462;&#30340;&#24191;&#27867;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#65292;&#26377;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#23427;&#20204;&#30340;&#30693;&#35782;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#26576;&#20123;&#38480;&#21046;&#65292;&#23613;&#31649;&#26368;&#36817;&#26377;&#19981;&#23569;&#21162;&#21147;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#25552;&#20379;&#20934;&#30830;&#30340;&#27979;&#37327;&#21644;&#20462;&#25913;LLMs&#20013;&#25152;&#38656;&#30340;&#27934;&#23519;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#27979;&#37327;&#26041;&#27861;&#26469;&#25552;&#20379;&#19968;&#20010;&#26694;&#26550;&#26469;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21253;&#21547;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;LLM&#22312;&#27880;&#20837;&#30446;&#26631;&#30693;&#35782;&#21069;&#21518;&#30340;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#26469;&#34913;&#37327;&#30693;&#35782;&#65292;&#20351;&#29992;&#29109;&#21644;KL-&#25955;&#24230;&#31561;&#24230;&#37327;&#26631;&#20934;&#12290;&#39318;&#20808;&#20171;&#32461;&#25105;&#20204;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#21512;&#25104;&#23454;&#39564;&#65292;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;&#20197;&#21069;&#30340;&#25490;&#21517;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#36229;&#36807;&#20102;&#23427;&#20204;35&#65285;&#20197;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#20123;&#25351;&#26631;&#22914;&#20309;&#29992;&#20110;&#30693;&#35782;&#20462;&#25913;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#38469;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#27979;&#37327;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) store an extensive amount of factual knowledge obtained from vast collections of text. To effectively utilize these models for downstream tasks, it is crucial to have reliable methods for measuring their knowledge. However, existing approaches for knowledge measurement have certain limitations, and despite recent efforts, they fail to provide accurate measurements and the necessary insights for modifying the knowledge within LLMs. In this work, we employ information theory-based measurements to provide a framework estimating the factual knowledge contained within large language models. More specifically, we measure knowledge by analyzing the LLM's prediction probability distribution before and after instilling the target knowledge, employing metrics such as entropy and KL-divergence. Introducing our metrics, we first assess their accuracy in comparison to previous ranking-based methods, surpassing them by over $35\%$ in a synthetic experiment. Then, we expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30028;&#23450;&#20102;Poisson loss&#21644;&#21407;&#23376;&#33539;&#25968;&#26368;&#23567;&#21270;&#20004;&#31181;&#24352;&#37327;&#34917;&#20840;&#26041;&#27861;&#30340;&#35299;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#25552;&#20379;&#20102;&#26356;&#32039;&#30340;&#30028;&#38480;&#65292;&#38024;&#23545;$r$&#30340;&#20381;&#36182;&#24615;&#20174;&#20043;&#21069;&#30340;$r^{2(t-1)(t^2-t-1)}$&#25913;&#36827;&#20026;$r^{2(t-1)(3t-5)}$&#12290;&#26681;&#25454;&#37319;&#26679;&#31232;&#30095;&#27169;&#24335;&#30340;&#35889;&#38388;&#38548;&#25511;&#21046;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21407;&#23376;&#24352;&#37327;&#33539;&#25968;&#30340;&#20960;&#20010;&#26032;&#23646;&#24615;&#65292;&#20294;&#21407;&#23376;&#33539;&#25968;&#26368;&#23567;&#21270;&#23384;&#22312;&#35745;&#31639;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.06262</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#38388;&#38548;&#30340;&#30830;&#23450;&#24615;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Spectral gap-based deterministic tensor completion. (arXiv:2306.06262v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30028;&#23450;&#20102;Poisson loss&#21644;&#21407;&#23376;&#33539;&#25968;&#26368;&#23567;&#21270;&#20004;&#31181;&#24352;&#37327;&#34917;&#20840;&#26041;&#27861;&#30340;&#35299;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#25552;&#20379;&#20102;&#26356;&#32039;&#30340;&#30028;&#38480;&#65292;&#38024;&#23545;$r$&#30340;&#20381;&#36182;&#24615;&#20174;&#20043;&#21069;&#30340;$r^{2(t-1)(t^2-t-1)}$&#25913;&#36827;&#20026;$r^{2(t-1)(3t-5)}$&#12290;&#26681;&#25454;&#37319;&#26679;&#31232;&#30095;&#27169;&#24335;&#30340;&#35889;&#38388;&#38548;&#25511;&#21046;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21407;&#23376;&#24352;&#37327;&#33539;&#25968;&#30340;&#20960;&#20010;&#26032;&#23646;&#24615;&#65292;&#20294;&#21407;&#23376;&#33539;&#25968;&#26368;&#23567;&#21270;&#23384;&#22312;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#26159;&#19968;&#20010;&#26680;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#20854;&#20182;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#39046;&#22495;&#12290;&#34429;&#28982;&#23545;&#20110;&#30697;&#38453;&#24773;&#20917;&#24050;&#26377;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#38024;&#23545;&#24352;&#37327;&#38382;&#39064;&#30340;&#29702;&#35770;&#32467;&#26524;&#20173;&#28982;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#24403;&#37319;&#26679;&#27169;&#24335;&#26159;&#30830;&#23450;&#24615;&#30340;&#26102;&#20505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#24352;&#37327;&#34917;&#20840;&#26041;&#27861;&#30340;&#35299;&#30340;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#30028;&#23450;&#65292;&#20998;&#21035;&#26159;Poisson loss&#21644;&#21407;&#23376;&#33539;&#25968;&#26368;&#23567;&#21270;&#65292;&#29992;&#30446;&#26631;&#24352;&#37327;&#31209;&#20316;&#20026;&#21028;&#26029;&#20381;&#25454;&#65292;&#25552;&#20379;&#20102;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;&#22914;&#26524;&#30446;&#26631;&#24352;&#37327;&#30340;&#38454;&#25968;&#20026;$t$&#65292;CP&#31209;&#20026;$r$&#65292;&#21017;&#25105;&#20204;&#30340;&#30028;&#38480;&#20013;&#38024;&#23545;$r$&#30340;&#20381;&#36182;&#24615;&#20174;arXiv:1910.10692&#30340;$r^{2(t-1)(t^2-t-1)}$&#25913;&#36827;&#20026;$r^{2(t-1)(3t-5)}$&#12290;&#25105;&#20204;&#30340;&#35823;&#24046;&#30028;&#26159;&#30001;&#37319;&#26679;&#31232;&#30095;&#27169;&#24335;&#30340;&#35889;&#38388;&#38548;&#20915;&#23450;&#30340;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#21407;&#23376;&#24352;&#37327;&#33539;&#25968;&#30340;&#20960;&#20010;&#26032;&#23646;&#24615;&#65292;&#22312;&#38543;&#26426;&#37319;&#26679;&#26041;&#26696;&#19979;&#23558;&#31209;&#30340;&#20381;&#36182;&#24615;&#20174;arXiv:1711.04965&#30340;$r^{3t-3}$&#20943;&#23569;&#21040;$r^{3t-5}$&#12290;&#28982;&#32780;&#65292;&#21407;&#23376;&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#26159;&#65292;&#34429;&#28982;&#22312;&#29702;&#35770;&#19978;&#24456;&#26377;&#36259;&#65292;&#20294;&#20250;&#23548;&#33268;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor completion is a core machine learning algorithm used in recommender systems and other domains with missing data. While the matrix case is well-understood, theoretical results for tensor problems are limited, particularly when the sampling patterns are deterministic. Here we bound the generalization error of the solutions of two tensor completion methods, Poisson loss and atomic norm minimization, providing tighter bounds in terms of the target tensor rank. If the ground-truth tensor is order $t$ with CP-rank $r$, the dependence on $r$ is improved from $r^{2(t-1)(t^2-t-1)}$ in arXiv:1910.10692 to $r^{2(t-1)(3t-5)}$. The error in our bounds is deterministically controlled by the spectral gap of the sampling sparsity pattern. We also prove several new properties for the atomic tensor norm, reducing the rank dependence from $r^{3t-3}$ in arXiv:1711.04965 to $r^{3t-5}$ under random sampling schemes. A limitation is that atomic norm minimization, while theoretically interesting, leads
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;ResNets&#27169;&#22411;&#24182;&#23545;&#20854;&#36827;&#34892;&#22686;&#24191;&#65292;&#21457;&#29616;&#20351;&#29992;&#20004;&#20010;&#22270;&#20687;&#20449;&#24687;&#30340;&#22686;&#24191;&#23545;&#25152;&#23398;&#26435;&#37325;&#30340;&#24433;&#21709;&#26174;&#30528;&#22823;&#20110;&#21333;&#20010;&#22270;&#20687;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;ImageNet-1K&#26435;&#37325;&#21644;&#24494;&#35843;&#30340;ResNets&#28145;&#23618;&#27604;&#26089;&#26399;&#23618;&#21463;&#21040;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.06254</link><description>&lt;p&gt;
&#29702;&#35299;&#22270;&#20687;&#22686;&#24378;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Understanding the Benefits of Image Augmentations. (arXiv:2306.06254v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;ResNets&#27169;&#22411;&#24182;&#23545;&#20854;&#36827;&#34892;&#22686;&#24191;&#65292;&#21457;&#29616;&#20351;&#29992;&#20004;&#20010;&#22270;&#20687;&#20449;&#24687;&#30340;&#22686;&#24191;&#23545;&#25152;&#23398;&#26435;&#37325;&#30340;&#24433;&#21709;&#26174;&#30528;&#22823;&#20110;&#21333;&#20010;&#22270;&#20687;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;ImageNet-1K&#26435;&#37325;&#21644;&#24494;&#35843;&#30340;ResNets&#28145;&#23618;&#27604;&#26089;&#26399;&#23618;&#21463;&#21040;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#22686;&#24378;&#34987;&#24191;&#27867;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22909;&#22788;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#20351;&#29992;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#65288;ResNets&#65289;&#30340;&#21738;&#20123;&#23618;&#21463;&#21040;&#22686;&#24378;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#26435;&#37325;&#26159;&#38543;&#26426;&#21021;&#22987;&#21270;&#36824;&#26159;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23618;&#21463;&#24433;&#21709;&#30340;&#27169;&#24335;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#65292;&#24182;&#19988;&#20351;&#29992;&#26469;&#33258;&#20004;&#20010;&#22270;&#20687;&#30340;&#20449;&#24687;&#30340;&#22686;&#24378;&#23545;&#23398;&#20064;&#30340;&#26435;&#37325;&#20135;&#29983;&#30340;&#24433;&#21709;&#26174;&#33879;&#22823;&#20110;&#23545;&#21333;&#20010;&#22270;&#20687;&#36827;&#34892;&#25805;&#20316;&#30340;&#22686;&#24378;&#12290;&#20351;&#29992;ImageNet-1K&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#24494;&#35843;&#30340;ResNets&#30340;&#28145;&#23618;&#27604;&#26089;&#26399;&#23618;&#21463;&#21040;&#30340;&#22686;&#24378;&#24433;&#21709;&#26356;&#22823;&#12290;&#20102;&#35299;&#22270;&#20687;&#22686;&#24378;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#23558;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#30830;&#23450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#23545;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#20197;&#21450;&#24494;&#35843;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Augmentations are widely used to reduce overfitting in neural networks. However, the explainability of their benefits largely remains a mystery. We study which layers of residual neural networks (ResNets) are most affected by augmentations using Centered Kernel Alignment (CKA). We do so by analyzing models of varying widths and depths, as well as whether their weights are initialized randomly or through transfer learning. We find that the pattern of how the layers are affected depends on the model's depth, and that networks trained with augmentation that use information from two images affect the learned weights significantly more than augmentations that operate on a single image. Deeper layers of ResNets initialized with ImageNet-1K weights and fine-tuned receive more impact from the augmentations than early layers. Understanding the effects of image augmentations on CNNs will have a variety of applications, such as determining how far back one needs to fine-tune a network and w
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#22534;&#21472;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06253</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#28789;&#27963;&#30340;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#22534;&#21472;
&lt;/p&gt;
&lt;p&gt;
Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models. (arXiv:2306.06253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06253
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#22534;&#21472;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#29702;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#20960;&#20010;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25351;&#23450;&#22797;&#26434;&#30446;&#26631;&#12289;&#35268;&#21010;&#26410;&#26469;&#35266;&#23519;&#21644;&#34892;&#21160;&#65292;&#20197;&#21450;&#25209;&#35780;&#20854;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33021;&#21147;&#30340;&#32508;&#21512;&#38598;&#25104;&#22312;&#20445;&#25345;&#26368;&#22823;&#34920;&#36798;&#33021;&#21147;&#30340;&#21516;&#26102;&#20801;&#35768;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#36825;&#26500;&#25104;&#20102;&#31454;&#20105;&#24615;&#30340;&#31639;&#27861;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#22534;&#21472;&#65288;Decision Stacks&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#36890;&#36807;&#29420;&#31435;&#30340;&#29983;&#25104;&#27169;&#22411;&#27169;&#25311;&#20102;&#35266;&#27979;&#12289;&#22870;&#21169;&#21644;&#34892;&#21160;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20445;&#35777;&#20102;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#22534;&#21472;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities. However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed. Our empirical 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#32534;&#31243;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#39044;&#27979;&#29305;&#24449;&#65292;&#21516;&#26102;&#20801;&#35768;&#29992;&#25143;&#20197;&#26368;&#23567;&#30340;&#24037;&#20316;&#37327;&#32467;&#21512;&#20182;&#20204;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#32454;&#31890;&#24230;&#30340;&#36712;&#36857;&#22686;&#37327;&#65292;&#24341;&#20837;&#20102;&#26032;&#22411;&#33258;&#26059;&#27668;&#20307;&#21160;&#21147;&#23398;&#20234;&#36763;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#32422;&#30340;&#31639;&#23376;&#38598;&#26469;&#24635;&#32467;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2306.06252</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#32534;&#31243;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Feature Programming for Multivariate Time Series Prediction. (arXiv:2306.06252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#32534;&#31243;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#39044;&#27979;&#29305;&#24449;&#65292;&#21516;&#26102;&#20801;&#35768;&#29992;&#25143;&#20197;&#26368;&#23567;&#30340;&#24037;&#20316;&#37327;&#32467;&#21512;&#20182;&#20204;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#32454;&#31890;&#24230;&#30340;&#36712;&#36857;&#22686;&#37327;&#65292;&#24341;&#20837;&#20102;&#26032;&#22411;&#33258;&#26059;&#27668;&#20307;&#21160;&#21147;&#23398;&#20234;&#36763;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#32422;&#30340;&#31639;&#23376;&#38598;&#26469;&#24635;&#32467;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#32534;&#31243;&#29305;&#24449;&#24037;&#31243;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#32534;&#31243;&#26694;&#26550;&#65292;&#20026;&#22024;&#26434;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22823;&#37327;&#39044;&#27979;&#29305;&#24449;&#65292;&#21516;&#26102;&#20801;&#35768;&#29992;&#25143;&#20197;&#26368;&#23567;&#30340;&#24037;&#20316;&#37327;&#32467;&#21512;&#20182;&#20204;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#20851;&#38190;&#21160;&#26426;&#26159;&#23558;&#20219;&#20309;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#32454;&#31890;&#24230;&#36712;&#36857;&#22686;&#37327;&#30340;&#32047;&#31215;&#24635;&#21644;&#65292;&#20854;&#20013;&#27599;&#20010;&#22686;&#37327;&#30001;&#19968;&#31181;&#26032;&#22411;&#33258;&#26059;&#27668;&#20307;&#21160;&#21147;&#23398;&#20234;&#36763;&#27169;&#22411;&#25511;&#21046;&#12290;&#36825;&#31181;&#32454;&#31890;&#24230;&#30340;&#35270;&#35282;&#28608;&#21457;&#20102;&#21457;&#23637;&#19968;&#20010;&#31616;&#32422;&#30340;&#31639;&#23376;&#38598;&#65292;&#20197;&#25277;&#35937;&#30340;&#26041;&#24335;&#24635;&#32467;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65292;&#20026;&#22823;&#35268;&#27169;&#30340;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#25552;&#20379;&#22522;&#30784;&#12290;&#25968;&#20540;&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of programmable feature engineering for time series modeling and propose a feature programming framework. This framework generates large amounts of predictive features for noisy multivariate time series while allowing users to incorporate their inductive bias with minimal effort. The key motivation of our framework is to view any multivariate time series as a cumulative sum of fine-grained trajectory increments, with each increment governed by a novel spin-gas dynamical Ising model. This fine-grained perspective motivates the development of a parsimonious set of operators that summarize multivariate time series in an abstract fashion, serving as the foundation for large-scale automated feature engineering. Numerically, we validate the efficacy of our method on several synthetic and real-world noisy time series datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.06251</link><description>&lt;p&gt;
&#36890;&#20449;&#31995;&#32479;&#20013;AI&#36890;&#29992;&#24615;&#19982;&#21487;&#25193;&#23637;&#24615;&#30340;&#35774;&#35745;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Design Principles for Generalization and Scalability of AI in Communication Systems. (arXiv:2306.06251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#25104;&#20026;&#36890;&#20449;&#31995;&#32479;&#20013;&#35299;&#20915;&#22797;&#26434;&#21644;&#21160;&#24577;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31639;&#27861;&#24448;&#24448;&#26080;&#27861;&#32988;&#20219;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32593;&#32476;&#20219;&#21153;&#30340;AI&#24212;&#29992;&#37117;&#26159;&#38024;&#23545;&#29305;&#23450;&#21644;&#26377;&#38480;&#30340;&#26465;&#20214;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#65292;&#20351;&#24471;&#31639;&#27861;&#26080;&#27861;&#36866;&#24212;&#20110;&#24120;&#35265;&#30340;&#32593;&#32476;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#21487;&#20197;&#22312;&#32593;&#32476;&#29615;&#22659;&#12289;&#24847;&#22270;&#21644;&#25511;&#21046;&#20219;&#21153;&#19978;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#21644;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#25903;&#25345;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#25152;&#26377;AI&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26550;&#26500;&#23558;&#20013;&#22830;&#21270;&#23398;&#20064;&#21151;&#33021;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#25968;&#25454;&#37319;&#38598;&#21151;&#33021;&#20998;&#31163;&#65292;&#30830;&#20445;&#20102;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has emerged as a powerful tool for addressing complex and dynamic tasks in communication systems, where traditional rule-based algorithms often struggle. However, most AI applications to networking tasks are designed and trained for specific, limited conditions, hindering the algorithms from learning and adapting to generic situations, such as those met across radio access networks (RAN). This paper proposes design principles for sustainable and scalable AI integration in communication systems, focusing on creating AI algorithms that can generalize across network environments, intents, and control tasks. This approach enables a limited number of AI-driven RAN functions to tackle larger problems, improve system performance, and simplify lifecycle management. To achieve sustainability and automation, we introduce a scalable learning architecture that supports all deployed AI applications in the system. This architecture separates centralized learning function
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#33529;&#26524;&#21697;&#23581;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#20197;&#23454;&#29616;&#25112;&#30053;&#36951;&#25022;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#25112;&#30053;&#36951;&#25022;&#36817;&#20284;&#20110;&#26368;&#20339;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06250</link><description>&lt;p&gt;
&#25112;&#30053;&#24615;&#33529;&#26524;&#21697;&#23581;&#65306;&#24102;&#26377;&#19968;&#38754;&#24615;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Strategic Apple Tasting. (arXiv:2306.06250v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#33529;&#26524;&#21697;&#23581;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#20197;&#23454;&#29616;&#25112;&#30053;&#36951;&#25022;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#25112;&#30053;&#36951;&#25022;&#36817;&#20284;&#20110;&#26368;&#20339;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#65292;&#31639;&#27861;&#20915;&#31574;&#24448;&#24448;&#28041;&#21450;&#23558;&#20915;&#31574;&#20998;&#37197;&#32473;&#20855;&#26377;&#31574;&#30053;&#24615;&#20462;&#25913;&#20854;&#31639;&#27861;&#36755;&#20837;&#21160;&#26426;&#30340;&#20195;&#29702;&#12290;&#38500;&#20102;&#24212;&#23545;&#28608;&#21169;&#22240;&#32032;&#22806;&#65292;&#22312;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#36151;&#27454;&#21644;&#25307;&#32856;&#65289;&#20013;&#65292;&#20915;&#31574;&#32773;&#21482;&#35266;&#23519;&#21040;&#22312;&#20998;&#37197;&#31215;&#26497;&#20915;&#31574;&#32473;&#20195;&#29702;&#26102;&#30340;&#22238;&#39304;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#21453;&#39304;&#31216;&#20026;&#33529;&#26524;&#21697;&#23581;&#65288;&#25110;&#21333;&#21521;&#21453;&#39304;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#24773;&#22659;&#24418;&#24335;&#21270;&#20026;&#24102;&#26377;&#33529;&#26524;&#21697;&#23581;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#36127;&#36131;&#20154;&#20915;&#31574;&#19968;&#31995;&#21015; $T$ &#20010;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#30001;&#21487;&#34987;&#31574;&#30053;&#24615;&#20462;&#25913;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#20195;&#29702;&#25581;&#31034;&#20854;&#19978;&#19979;&#25991;&#26102;&#33719;&#24471;&#20122;&#32447;&#24615;&#30340;&#25112;&#30053;&#36951;&#25022;&#65292;&#21363;&#22914;&#26524;&#20195;&#29702;&#22312;&#25581;&#31034;&#20854;&#19978;&#19979;&#25991;&#26102;&#26159;&#30495;&#23454;&#30340;&#65292;&#21017;&#23558;&#36127;&#36131;&#20154;&#30340;&#34920;&#29616;&#19982;&#21518;&#35265;&#20043;&#26126;&#30340;&#26368;&#20339;&#22266;&#23450;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20135;&#29983; $\tilde{\mathcal{O}}(\sqrt{T})$ &#30340;&#25112;&#30053;&#36951;&#25022;&#65292;&#19982;&#26356;&#19968;&#33324;&#31867;&#22411;&#30340;&#21453;&#39304;&#25152;&#30693;&#30340;&#26368;&#20339;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic decision-making in high-stakes domains often involves assigning decisions to agents with incentives to strategically modify their input to the algorithm. In addition to dealing with incentives, in many domains of interest (e.g. lending and hiring) the decision-maker only observes feedback regarding their policy for rounds in which they assign a positive decision to the agent; this type of feedback is often referred to as apple tasting (or one-sided) feedback. We formalize this setting as an online learning problem with apple-tasting feedback where a principal makes decisions about a sequence of $T$ agents, each of which is represented by a context that may be strategically modified. Our goal is to achieve sublinear strategic regret, which compares the performance of the principal to that of the best fixed policy in hindsight, if the agents were truthful when revealing their contexts. Our main result is a learning algorithm which incurs $\tilde{\mathcal{O}}(\sqrt{T})$ strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#35770;&#25991;&#34920;&#26126;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#22312;&#23454;&#29616;&#35774;&#32622;&#19979;&#19981;&#31561;&#20215;&#65292;&#24182;&#23558;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#20316;&#20026;&#20854;&#29305;&#23450;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.06247</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning with Set-Valued Feedback. (arXiv:2306.06247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#35770;&#25991;&#34920;&#26126;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#22312;&#23454;&#29616;&#35774;&#32622;&#19979;&#19981;&#31561;&#20215;&#65292;&#24182;&#23558;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#20316;&#20026;&#20854;&#29305;&#23450;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#39044;&#27979;&#21333;&#20010;&#26631;&#31614;&#65292;&#20294;&#25509;&#25910;&#21040;&#19968;&#20010;&#26631;&#31614;&#30340;&#38598;&#21512;&#20316;&#20026;&#21453;&#39304;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#23398;&#20064;&#22120;&#27809;&#26377;&#36755;&#20986;&#21253;&#21547;&#22312;&#21453;&#39304;&#38598;&#21512;&#20013;&#30340;&#26631;&#31614;&#65292;&#21017;&#20250;&#21463;&#21040;&#24809;&#32602;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#20855;&#26377;&#21333;&#26631;&#31614;&#21453;&#39304;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#19981;&#21516;&#65292;&#22312;&#23454;&#29616;&#35774;&#32622;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#26102;&#65292;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21270;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;\textit{&#19981;&#31561;&#20215;}&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;&#38598;&#21512;&#23567;&#30707;&#21644;&#24230;&#37327;&#30772;&#35010;&#32500;&#24230;&#65292;&#20005;&#26684;&#25551;&#36848;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21270;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#24230;&#37327;&#30772;&#35010;&#32500;&#24230;&#22312;&#24735;&#24615;&#35774;&#32622;&#19979;&#20005;&#26684;&#25551;&#36848;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#26159;&#25105;&#20204;&#36890;&#29992;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a variant of online multiclass classification where the learner predicts a single label but receives a \textit{set of labels} as feedback. In this model, the learner is penalized for not outputting a label contained in the revealed set. We show that unlike online multiclass learning with single-label feedback, deterministic and randomized online learnability are \textit{not equivalent} even in the realizable setting with set-valued feedback. Accordingly, we give two new combinatorial dimensions, named the Set Littlestone and Measure Shattering dimension, that tightly characterize deterministic and randomized online learnability respectively in the realizable setting. In addition, we show that the Measure Shattering dimension tightly characterizes online learnability in the agnostic setting. Finally, we show that practical learning settings like online multilabel ranking and online multilabel classification are specific instances of our general online learning framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.06238</link><description>&lt;p&gt;
&#29702;&#35299;&#38271;&#23614;&#25928;&#24212;&#23545;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#21387;&#32553;&#29616;&#22312;&#26159;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#19968;&#20010;&#25104;&#29087;&#30340;&#23376;&#39046;&#22495;&#65292;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#21152;&#36895;&#25512;&#26029;&#20026;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#20165;&#20851;&#27880;&#24635;&#20307;&#20934;&#30830;&#24615;&#21487;&#33021;&#26159;&#35823;&#23548;&#30340;&#12290;&#20363;&#22914;&#65292;&#24050;&#32463;&#35777;&#26126;&#20840;&#27169;&#22411;&#21644;&#21387;&#32553;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#20302;&#39057;&#30340;&#31867;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#8220;&#25105;&#20204;&#33021;&#21542;&#22312;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#35821;&#20041;&#31561;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32593;&#32476;&#21387;&#32553;&#65311;&#8221;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;Feldman&#31561;&#20154;&#35266;&#23519;&#21040;&#30340;&#8220;&#38271;&#23614;&#8221;&#29616;&#35937;&#12290;&#20182;&#20204;&#35748;&#20026;&#65292;&#26576;&#20123;&#36755;&#20837;&#65288;&#36866;&#24403;&#23450;&#20041;&#65289;&#30340;&#35760;&#24518;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#30001;&#20110;&#21387;&#32553;&#38480;&#21046;&#20102;&#32593;&#32476;&#30340;&#23481;&#37327;&#65288;&#22240;&#27492;&#20063;&#38480;&#21046;&#20102;&#20854;&#35760;&#24518;&#33021;&#21147;&#65289;&#65292;&#25152;&#20197;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MARL&#31639;&#27861;&#30340;iPLAN&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#23494;&#24230;&#19988;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#19979;&#36827;&#34892;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#34892;&#20026;&#25110;&#30636;&#26102;&#28608;&#21169;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2306.06236</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24322;&#26500;&#20132;&#36890;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#31639;&#27861;iPLAN
&lt;/p&gt;
&lt;p&gt;
iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning. (arXiv:2306.06236v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06236
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MARL&#31639;&#27861;&#30340;iPLAN&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#23494;&#24230;&#19988;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#19979;&#36827;&#34892;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#34892;&#20026;&#25110;&#30636;&#26102;&#28608;&#21169;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#23494;&#24230;&#21644;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#20013;&#20445;&#38556;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AVs&#65289;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#38754;&#20020;&#36739;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#34892;&#20026;&#25110;&#24847;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36712;&#36857;&#21644;&#24847;&#22270;&#39044;&#27979;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#23494;&#24230;&#21644;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#20013;&#36827;&#34892;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;iPLAN&#26041;&#27861;&#20351;&#26234;&#33021;&#20307;&#20165;&#20174;&#20854;&#26412;&#22320;&#35266;&#27979;&#20013;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#28608;&#21169;&#22240;&#32032;&#65306;&#34892;&#20026;&#28608;&#21169;&#29992;&#20110;&#26234;&#33021;&#20307;&#30340;&#38271;&#26399;&#35268;&#21010;&#65292;&#22522;&#20110;&#23427;&#20204;&#30340;&#39550;&#39542;&#34892;&#20026;&#25110;&#20010;&#24615;&#65307;&#30636;&#26102;&#28608;&#21169;&#29992;&#20110;&#26234;&#33021;&#20307;&#30340;&#30701;&#26399;&#35268;&#21010;&#65292;&#20197;&#22522;&#20110;&#24403;&#21069;&#20132;&#36890;&#29366;&#24577;&#36827;&#34892;&#30896;&#25758;&#36991;&#20813;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27969;&#25512;&#29702;&#27169;&#22359;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#25512;&#26029;&#23545;&#25163;&#30340;&#28608;&#21169;&#24182;&#23558;&#20854;&#25512;&#26029;&#20449;&#24687;&#32435;&#20837;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we propose a distributed multi-agent reinforcement learning (MARL) algorithm with trajectory and intent prediction in dense and heterogeneous traffic scenarios. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers' intents solely from their local observations. We model two distinct incentives for agents' strategies: Behavioral incentives for agents' long-term planning based on their driving behavior or personality; Instant incentives for agents' short-term planning for collision avoidance based on the current traffic state. We design a two-stream inference module that allows agents to infer their opponents' incentives and incorporate their inferred information into decision-making. We perform experiments on two simulation environments, Non-C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;AVScan2Vec&#65292;&#19968;&#31181;&#38024;&#23545;&#26432;&#27602;&#25195;&#25551;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#20854;&#20316;&#20026;&#24694;&#24847;&#36719;&#20214;&#30340;&#21487;&#25193;&#23637;&#29305;&#24449;&#28304;&#65292;&#20197;&#35299;&#20915;&#29983;&#20135;&#35268;&#27169;&#24694;&#24847;&#36719;&#20214;&#24211;&#20013;&#25991;&#20214;&#25968;&#20247;&#22810;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06228</link><description>&lt;p&gt;
AVScan2Vec&#65306;&#38024;&#23545;&#29983;&#20135;&#35268;&#27169;&#24694;&#24847;&#36719;&#20214;&#24211;&#30340;&#26432;&#27602;&#25195;&#25551;&#25968;&#25454;&#30340;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AVScan2Vec: Feature Learning on Antivirus Scan Data for Production-Scale Malware Corpora. (arXiv:2306.06228v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;AVScan2Vec&#65292;&#19968;&#31181;&#38024;&#23545;&#26432;&#27602;&#25195;&#25551;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#20854;&#20316;&#20026;&#24694;&#24847;&#36719;&#20214;&#30340;&#21487;&#25193;&#23637;&#29305;&#24449;&#28304;&#65292;&#20197;&#35299;&#20915;&#29983;&#20135;&#35268;&#27169;&#24694;&#24847;&#36719;&#20214;&#24211;&#20013;&#25991;&#20214;&#25968;&#20247;&#22810;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35843;&#26597;&#24694;&#24847;&#25991;&#20214;&#26102;&#65292;&#25628;&#32034;&#30456;&#20851;&#25991;&#20214;&#26159;&#24694;&#24847;&#20998;&#26512;&#21592;&#24517;&#39035;&#25191;&#34892;&#30340;&#24120;&#35265;&#20219;&#21153;&#12290;&#30001;&#20110;&#29983;&#20135;&#24694;&#24847;&#36719;&#20214;&#24211;&#21487;&#33021;&#21253;&#21547;&#36229;&#36807;&#21313;&#20159;&#20010;&#25991;&#20214;&#24182;&#28040;&#32791;PB&#32423;&#30340;&#23384;&#20648;&#31354;&#38388;&#65292;&#35768;&#22810;&#29305;&#24449;&#25552;&#21462;&#21644;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#26432;&#27602;&#25195;&#25551;&#25968;&#25454;&#20316;&#20026;&#24694;&#24847;&#36719;&#20214;&#30340;&#21487;&#25193;&#23637;&#29305;&#24449;&#28304;&#30340;&#28508;&#21147;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;AV&#25195;&#25551;&#25253;&#21578;&#36890;&#36807;VirusTotal&#31561;&#26381;&#21153;&#24191;&#27867;&#21487;&#29992;&#65292;&#24182;&#19988;&#27604;&#24179;&#22343;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#23567;&#32422;100&#20493;&#12290; AV&#25195;&#25551;&#25253;&#21578;&#20013;&#30340;&#20449;&#24687;&#20805;&#28385;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#25351;&#31034;&#24694;&#24847;&#25991;&#20214;&#30340;&#23478;&#26063;&#12289;&#34892;&#20026;&#12289;&#30446;&#26631;&#25805;&#20316;&#31995;&#32479;&#21644;&#35768;&#22810;&#20854;&#20182;&#29305;&#24449;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;AVScan2Vec&#65292;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29702;&#35299;AV&#25195;&#25551;&#25968;&#25454;&#30340;&#35821;&#20041;&#12290;AVScan2Vec&#25668;&#21462;&#29992;&#20110;&#24694;&#24847;&#25991;&#20214;&#30340;AV&#25195;&#25551;&#25968;&#25454;&#65292;&#24182;&#36755;&#20986;&#26377;&#24847;&#20041;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;AVScan2Vec&#21521;&#37327;&#27604;&#27969;&#34892;&#30340;&#24694;&#24847;&#36719;&#20214;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#23567;&#32422;3&#21040;85&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
When investigating a malicious file, searching for related files is a common task that malware analysts must perform. Given that production malware corpora may contain over a billion files and consume petabytes of storage, many feature extraction and similarity search approaches are computationally infeasible. Our work explores the potential of antivirus (AV) scan data as a scalable source of features for malware. This is possible because AV scan reports are widely available through services such as VirusTotal and are ~100x smaller than the average malware sample. The information within an AV scan report is abundant with information and can indicate a malicious file's family, behavior, target operating system, and many other characteristics. We introduce AVScan2Vec, a language model trained to comprehend the semantics of AV scan data. AVScan2Vec ingests AV scan data for a malicious file and outputs a meaningful vector representation. AVScan2Vec vectors are ~3 to 85x smaller than popula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06213</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#22810;&#31867;&#20998;&#31867;&#40065;&#26834;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Twin Parametric Margin Support Vector Machine for Multiclass Classification. (arXiv:2306.06213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;(TPMSVM)&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290; &#23545;&#20110;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#23545;&#21106;&#24179;&#38754;&#30340;&#27169;&#24335;&#26500;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#12290;&#19968;&#26086;&#30830;&#23450;&#20102;&#25152;&#26377;&#20998;&#31867;&#22120;&#65292;&#21017;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#32508;&#21512;&#30340;&#20915;&#31574;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20869;&#26680;&#24341;&#36215;&#30340;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#22686;&#24378;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290; &#21021;&#27493;&#30340;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a Twin Parametric-Margin Support Vector Machine (TPMSVM) model to tackle the problem of multiclass classification. In the spirit of one-versus-all paradigm, for each class we construct a classifier by solving a TPMSVM-type model. Once all classifiers have been determined, they are combined into an aggregate decision function. We consider the cases of both linear and nonlinear kernel-induced classifiers. In addition, we robustify the proposed approach through robust optimization techniques. Indeed, in real-world applications observations are subject to measurement errors and noise, affecting the quality of the solutions. Consequently, data uncertainties need to be included within the model in order to prevent low accuracies in the classification process. Preliminary computational experiments on real-world datasets show the good performance of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06210</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#32456;&#23618;&#21453;&#28436;&#36827;&#34892;&#21333;&#27169;&#22411;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#24320;&#21019;&#24615;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#23454;&#29992;&#21333;&#27169;&#22411;&#24402;&#22240;&#30340;&#20852;&#36259;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#19968;&#20010;&#26679;&#26412;&#26159;&#30001;&#29305;&#23450;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#19981;&#26159;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#35777;&#26126;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#65292;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FLIPAD&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#21033;&#29992;&#30340;&#26368;&#32456;&#23618;&#21453;&#28436;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#20010;&#20984;&#30340; Lasso &#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21487;&#38752;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#29702;&#35770;&#32467;&#26524;&#36824;&#24471;&#21040;&#20102;&#23454;&#39564;&#30740;&#31350;&#30340;&#25903;&#25345;&#65292;&#35777;&#26126;&#26412;&#25991;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIBA&#30340;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#23384;&#22312;&#30340;&#21487;&#35265;&#25110;&#31232;&#30095;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06209</link><description>&lt;p&gt;
&#31232;&#30095;&#38544;&#24418;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack with Sparse and Invisible Trigger. (arXiv:2306.06209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIBA&#30340;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#23384;&#22312;&#30340;&#21487;&#35265;&#25110;&#31232;&#30095;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#22312;&#23567;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#24471;&#21463;&#23475;&#30340;&#27169;&#22411;&#23545;&#27491;&#24120;&#26679;&#26412;&#26377;&#27491;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#23558;&#24102;&#26377;&#35302;&#21457;&#22120;&#30340;&#26679;&#26412;&#24402;&#31867;&#20026;&#30446;&#26631;&#20998;&#31867;&#12290;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#20852;&#32780;&#21448;&#21361;&#38505;&#30340;&#35757;&#32451;&#38454;&#27573;&#23041;&#32961;&#65292;&#23545;DNN&#24212;&#29992;&#24102;&#26469;&#20005;&#37325;&#39118;&#38505;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#35265;&#25110;&#31232;&#30095;&#24615;&#19981;&#36275;&#65292;&#22240;&#27492;&#19981;&#22815;&#38544;&#31192;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#35774;&#35745;&#26377;&#25928;&#30340;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#26102;&#65292;&#19981;&#33021;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#35302;&#21457;&#22120;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#31232;&#30095;&#24615;&#21644;&#38544;&#31192;&#24615;&#32422;&#26463;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#65292;&#31216;&#20026;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#65288;SIBA&#65289;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to backdoor attacks, where the adversary manipulates a small portion of training data such that the victim model predicts normally on the benign samples but classifies the triggered samples as the target class. The backdoor attack is an emerging yet threatening training-phase threat, leading to serious risks in DNN-based applications. In this paper, we revisit the trigger patterns of existing backdoor attacks. We reveal that they are either visible or not sparse and therefore are not stealthy enough. More importantly, it is not feasible to simply combine existing methods to design an effective sparse and invisible backdoor attack. To address this problem, we formulate the trigger generation as a bi-level optimization problem with sparsity and invisibility constraints and propose an effective method to solve it. The proposed method is dubbed sparse and invisible backdoor attack (SIBA). We conduct extensive experiments on benchmark datasets unde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.06208</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#38656;&#35201;&#24040;&#22823;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;GPU&#21644;TPU&#31561;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#24555;&#36895;&#12289;&#21450;&#26102;&#30340;&#22788;&#29702;&#12290;&#22312;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#23376;&#20248;&#26144;&#23556;&#21487;&#33021;&#20250;&#23548;&#33268;&#23454;&#26102;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22833;&#36133;&#65292;&#20174;&#32780;&#23548;&#33268;&#26102;&#38388;&#19981;&#30830;&#23450;&#24615;&#21644;&#38169;&#35823;&#34892;&#20026;&#12290;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#26144;&#23556;&#26159;&#36890;&#36807;&#22810;&#20010;&#36719;&#20214;&#32452;&#20214;&#36827;&#34892;&#30340;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#12289;&#35774;&#22791;&#24211;&#31561;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#35745;&#31639;&#29615;&#22659;&#12290;&#38543;&#30528;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#30103;&#25104;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#22686;&#21152;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#30828;&#20214;&#35774;&#22791;&#31561;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#27491;&#30830;&#24615;&#30340;&#24433;&#21709;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26356;&#25913;&#36719;&#20214;&#32452;&#20214;&#26469;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#29983;&#25104;&#20855;&#26377;&#24050;&#30693;&#39044;&#27979;&#36755;&#20986;&#24046;&#24322;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#27604;&#36739;&#21407;&#22987;&#22270;&#20687;&#21644;&#25200;&#21160;&#22270;&#20687;&#30340;&#39044;&#27979;&#36755;&#20986;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#19977;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26469;&#35777;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#33258;&#21160;&#39532;&#38083;&#34223;&#23475;&#34411;&#35782;&#21035;&#31995;&#32479;PotatoPestNet&#65292;&#20351;&#29992;&#20102;&#20843;&#31181;&#39532;&#38083;&#34223;&#23475;&#34411;&#30340;&#25968;&#25454;&#38598;&#21644;&#20116;&#31181;&#39044;&#35757;&#32451;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#25628;&#32034;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#21516;&#31181;&#31867;&#30340;&#39532;&#38083;&#34223;&#23475;&#34411;&#12290;</title><link>http://arxiv.org/abs/2306.06206</link><description>&lt;p&gt;
&#8220;PotatoPestNet&#65306;&#19968;&#31181;&#22522;&#20110;CTInceptionV3-RS&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#39532;&#38083;&#34223;&#23475;&#34411;&#12290;&#8221;(arXiv&#65306;2306.06206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests. (arXiv:2306.06206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#33258;&#21160;&#39532;&#38083;&#34223;&#23475;&#34411;&#35782;&#21035;&#31995;&#32479;PotatoPestNet&#65292;&#20351;&#29992;&#20102;&#20843;&#31181;&#39532;&#38083;&#34223;&#23475;&#34411;&#30340;&#25968;&#25454;&#38598;&#21644;&#20116;&#31181;&#39044;&#35757;&#32451;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#25628;&#32034;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#21516;&#31181;&#31867;&#30340;&#39532;&#38083;&#34223;&#23475;&#34411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#38083;&#34223;&#26159;&#20840;&#29699;&#31532;&#19977;&#22823;&#39135;&#21697;&#20316;&#29289;&#65292;&#20294;&#30001;&#20110;&#20405;&#34989;&#24615;&#23475;&#34411;&#30340;&#22256;&#25200;&#65292;&#20854;&#20135;&#37327;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#35843;&#26597;&#36825;&#20123;&#23475;&#34411;&#30340;&#21508;&#31181;&#31867;&#22411;&#21644;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;AI&#30340;&#33258;&#21160;&#39532;&#38083;&#34223;&#23475;&#34411;&#35782;&#21035;&#31995;&#32479;PotatoPestNet&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31579;&#36873;&#20102;&#21253;&#25324;&#20843;&#31181;&#39532;&#38083;&#34223;&#23475;&#34411;&#30340;&#21487;&#38752;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#20116;&#20010;&#32463;&#36807;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65306;CMobileNetV2&#12289;CNASLargeNet&#12289;CXception&#12289;CDenseNet201&#21644;CInceptionV3&#30340;&#24378;&#22823;&#20043;&#22788;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;PotatoPestNet&#27169;&#22411;&#26469;&#20934;&#30830;&#20998;&#31867;&#39532;&#38083;&#34223;&#23475;&#34411;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21152;&#20837;&#19968;&#20010;&#20840;&#23616;&#22343;&#20540;&#27744;&#21270;&#23618;&#65292;&#24182;&#23454;&#26045;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#25628;&#32034;&#65288;RS&#65289;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Potatoes are the third-largest food crop globally, but their production frequently encounters difficulties because of aggressive pest infestations. The aim of this study is to investigate the various types and characteristics of these pests and propose an efficient PotatoPestNet AI-based automatic potato pest identification system. To accomplish this, we curated a reliable dataset consisting of eight types of potato pests. We leveraged the power of transfer learning by employing five customized, pre-trained transfer learning models: CMobileNetV2, CNASLargeNet, CXception, CDenseNet201, and CInceptionV3, in proposing a robust PotatoPestNet model to accurately classify potato pests. To improve the models' performance, we applied various augmentation techniques, incorporated a global average pooling layer, and implemented proper regularization methods. To further enhance the performance of the models, we utilized random search (RS) optimization for hyperparameter tuning. This optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;FLSL&#26041;&#27861;&#65292;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35821;&#20041;&#31867;&#31751;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.06203</link><description>&lt;p&gt;
&#29305;&#24449;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;FLSL
&lt;/p&gt;
&lt;p&gt;
FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;FLSL&#26041;&#27861;&#65292;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35821;&#20041;&#31867;&#31751;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;SimCLR&#12289;DINO&#12289;VICReg&#12289;MOCOv3&#65289;&#20027;&#35201;&#38024;&#23545;&#23454;&#20363;&#32423;&#21035;&#30340;&#34920;&#31034;&#65292;&#19981;&#36866;&#29992;&#20110;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;Vision Transformers&#65288;ViT&#65289;&#30340;&#22522;&#30784;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#36807;&#31243;&#33021;&#22815;&#33391;&#22909;&#22320;&#19982;&#33258;&#28982;&#22270;&#20687;&#35821;&#20041;&#65288;&#20363;&#22914;&#29289;&#20307;&#21644;&#22330;&#26223;&#65289;&#23545;&#40784;&#12290;&#36890;&#36807;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#32423;&#29305;&#24449;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;FLSL&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FLSL&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#20174;&#22343;&#20540;&#28418;&#31227;&#21644;k-means&#30340;&#35282;&#24230;&#26500;&#24314;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLSL&#20419;&#36827;&#20102;&#26174;&#33879;&#30340;&#35821;&#20041;&#31867;&#31751;&#34920;&#31034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#30340;&#23884;&#20837;&#26041;&#26696;&#12290;FLSL&#30340;&#36816;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.06202</link><description>&lt;p&gt;
NeuroGraph:&#38754;&#21521;&#33041;&#36830;&#25509;&#32452;&#23398;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#20998;&#26512;&#39640;&#32500;&#21151;&#33021;&#24615;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#39044;&#27979;&#21508;&#31181;&#31070;&#32463;&#30142;&#30149;&#12289;&#31934;&#31070;&#38556;&#30861;&#21644;&#35748;&#30693;&#27169;&#24335;&#26377;&#25928;&#12290;&#22312;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#30740;&#31350;&#20013;&#65292;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#12290;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#35777;&#23454;&#65292;&#26631;&#24535;&#30528;&#25968;&#25454;&#35299;&#37322;&#21644;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#36716;&#21464;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#22270;&#24418;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#24191;&#27867;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#21644;&#22823;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#20013;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#36716;&#25442;&#20173;&#28982;&#21463;&#21040;&#24847;&#22806;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroGraph(&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#38598;)&#65292;&#23427;&#28085;&#30422;&#20102;&#22810;&#20010;&#34892;&#20026;&#21644;&#35748;&#30693;&#29305;&#24449;&#31867;&#21035;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#23545;&#25935;&#24863;&#35805;&#39064;&#21644;&#25552;&#31034;&#25514;&#36766;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#20854;&#22312;&#38452;&#35851;&#35770;&#21644;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#26377;&#27491;&#30830;&#30340;&#21453;&#24212;&#65292;&#20294;&#22312;&#35823;&#35299;&#21644;&#20105;&#35758;&#26041;&#38754;&#23384;&#22312;&#38169;&#35823;&#65292;&#24182;&#20855;&#26377;&#19981;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06199</link><description>&lt;p&gt;
&#21487;&#38752;&#24615;&#26816;&#26597;&#65306;&#23545;GPT-3&#22312;&#25935;&#24863;&#35805;&#39064;&#21644;&#25552;&#31034;&#25514;&#36766;&#26041;&#38754;&#30340;&#21453;&#24212;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording. (arXiv:2306.06199v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#23545;&#25935;&#24863;&#35805;&#39064;&#21644;&#25552;&#31034;&#25514;&#36766;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#20854;&#22312;&#38452;&#35851;&#35770;&#21644;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#26377;&#27491;&#30830;&#30340;&#21453;&#24212;&#65292;&#20294;&#22312;&#35823;&#35299;&#21644;&#20105;&#35758;&#26041;&#38754;&#23384;&#22312;&#38169;&#35823;&#65292;&#24182;&#20855;&#26377;&#19981;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#20027;&#27969;&#25216;&#26415;&#65292;&#20855;&#26377;&#22810;&#31181;&#29992;&#36884;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#26377;&#26080;&#25968;&#30340;&#24212;&#29992;&#65292;&#20294;LLMs&#20173;&#28982;&#19981;&#26159;&#21487;&#38752;&#30340;&#12290;&#36890;&#36807;&#24494;&#35843;&#12289;&#25552;&#31034;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#31561;&#26041;&#27861;&#65292;&#27491;&#22312;&#36827;&#34892;&#22823;&#37327;&#24037;&#20316;&#26469;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36947;&#24503;&#26631;&#20934;&#65292;&#20294;&#32570;&#20047;&#23545;&#36825;&#20123;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#21477;&#31867;&#21035;&#30340;&#21453;&#24212;&#25110;&#22312;&#31616;&#21333;&#25552;&#31034;&#21464;&#21270;&#19979;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20160;&#20040;&#20250;&#35753;GPT-3&#22256;&#24785;&#65306;&#27169;&#22411;&#22914;&#20309;&#21709;&#24212;&#26576;&#20123;&#25935;&#24863;&#35805;&#39064;&#20197;&#21450;&#25552;&#31034;&#25514;&#36766;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3&#27491;&#30830;&#22320;&#21453;&#23545;&#26126;&#26174;&#30340;&#38452;&#35851;&#35770;&#21644;&#21051;&#26495;&#21360;&#35937;&#65292;&#20294;&#22312;&#26222;&#36941;&#30340;&#35823;&#35299;&#21644;&#20105;&#35758;&#20013;&#29359;&#20102;&#38169;&#35823;&#12290;&#27169;&#22411;&#21709;&#24212;&#22312;&#25552;&#31034;&#21644;&#35774;&#32622;&#19978;&#19981;&#19968;&#33268;&#65292;&#31361;&#26174;&#20986;GPT-3&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability. Dataset and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#32780;&#39640;&#25928;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;&#24515;&#30005;&#22270;(ECG)&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#27169;&#22411;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23454;&#29616;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#26032;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.06196</link><description>&lt;p&gt;
ElectroCardioGuard&#65306;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#38450;&#27490;&#24515;&#30005;&#22270;&#25968;&#25454;&#24211;&#20013;&#24739;&#32773;&#35823;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks. (arXiv:2306.06196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#32780;&#39640;&#25928;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;&#24515;&#30005;&#22270;(ECG)&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#27169;&#22411;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23454;&#29616;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#26032;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;(ECG)&#36890;&#24120;&#34987;&#24515;&#33039;&#30149;&#19987;&#23478;&#29992;&#20110;&#26816;&#27979;&#19982;&#24515;&#33039;&#30456;&#20851;&#30340;&#30149;&#29702;&#24773;&#20917;&#65292;&#32780;&#21487;&#38752;&#30340;ECG&#38598;&#21512;&#23545;&#20110;&#30830;&#35786;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#23558;&#35760;&#24405;&#30340;ECG&#20998;&#37197;&#32473;&#38169;&#35823;&#30340;&#24739;&#32773;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#21457;&#29983;&#12290;&#26412;&#25991;&#19982;&#19968;&#23478;&#20020;&#24202;&#21644;&#30740;&#31350;&#26426;&#26500;&#21512;&#20316;&#65292;&#35813;&#26426;&#26500;&#35748;&#35782;&#21040;&#36825;&#19968;&#25361;&#25112;&#24182;&#32852;&#31995;&#25105;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#24039;&#39640;&#25928;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;ECG&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#21033;&#29992;760&#20493;&#26356;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;PTB-XL&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#30011;&#24266;&#25506;&#38024;&#24739;&#32773;&#35782;&#21035;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#19968;&#20010;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#26032;&#25910;&#38598;&#30340;ECG&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiograms (ECGs) are commonly used by cardiologists to detect heart-related pathological conditions. Reliable collections of ECGs are crucial for precise diagnosis. However, in clinical practice, the assignment of captured ECG recordings to incorrect patients can occur inadvertently. In collaboration with a clinical and research facility which recognized this challenge and reached out to us, we present a study that addresses this issue. In this work, we propose a small and efficient neural-network based model for determining whether two ECGs originate from the same patient. Our model demonstrates great generalization capabilities and achieves state-of-the-art performance in gallery-probe patient identification on PTB-XL while utilizing 760x fewer parameters. Furthermore, we present a technique leveraging our model for detection of recording-assignment mistakes, showcasing its applicability in a realistic scenario. Finally, we evaluate our model on a newly collected ECG dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#26234;&#33021;&#21345;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21733;&#20262;&#27604;&#20122;&#27874;&#21733;&#22823;BRT&#31995;&#32479;&#38548;&#26085;&#38656;&#27714;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;&#24182;&#35780;&#20272;&#20102;&#20116;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;&#31283;&#23450;&#21644;&#39640;&#24230;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22810;&#25968;&#27979;&#35797;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;MAAPE&#20174;0.08&#21040;0.12&#19981;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.06194</link><description>&lt;p&gt;
&#39640;&#24230;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#20844;&#20849;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#65306;&#29616;&#26377;&#27169;&#22411;&#30340;&#20803;&#20998;&#26512;&#21644;&#24320;&#28304;&#22522;&#20934;&#35774;&#26045;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Public Transit Demand Prediction During Highly Dynamic Conditions: A Meta-Analysis of State-of-the-Art Models and Open-Source Benchmarking Infrastructure. (arXiv:2306.06194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#26234;&#33021;&#21345;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21733;&#20262;&#27604;&#20122;&#27874;&#21733;&#22823;BRT&#31995;&#32479;&#38548;&#26085;&#38656;&#27714;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;&#24182;&#35780;&#20272;&#20102;&#20116;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;&#31283;&#23450;&#21644;&#39640;&#24230;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22810;&#25968;&#27979;&#35797;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;MAAPE&#20174;0.08&#21040;0.12&#19981;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#38656;&#27714;&#39044;&#27979;&#26159;&#21160;&#24577;&#20844;&#20132;&#36335;&#32447;&#35268;&#21010;&#30340;&#20851;&#38190;&#36755;&#20837;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#22797;&#26434;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#30701;&#26399;&#20844;&#20849;&#20132;&#36890;&#38656;&#27714;&#65292;&#20294;&#24212;&#29992;&#33539;&#22260;&#20165;&#38480;&#20110;&#30701;&#26102;&#38388;&#12289;&#31283;&#23450;&#30340;&#26102;&#38388;&#33539;&#22260;&#21644;&#23569;&#25968;&#36710;&#31449;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#24230;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#36824;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#65292;&#20063;&#27809;&#26377;&#31995;&#32479;&#22320;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;&#65292;&#21253;&#25324;&#35745;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#31283;&#23450;&#21644;&#39640;&#24230;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#26234;&#33021;&#21345;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21733;&#20262;&#27604;&#20122;&#27874;&#21733;&#22823;BRT&#31995;&#32479;&#22312;&#38548;&#26085;&#30340;&#38656;&#27714;&#12290;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#26465;&#20214;&#21253;&#25324;&#19968;&#20010;&#38271;&#36798;&#19968;&#20010;&#26376;&#30340;&#25239;&#35758;&#21644;COVID-19&#22823;&#27969;&#34892;&#12290;&#36825;&#20004;&#20010;&#26465;&#20214;&#37117;&#24341;&#21457;&#20102;&#38656;&#27714;&#30340;&#24040;&#22823;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31283;&#23450;&#26465;&#20214;&#19979;&#65292;&#22823;&#22810;&#25968;&#27979;&#35797;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;MAAPE&#20174;0.08&#21040;0.12&#19981;&#31561;&#12290; &#22522;&#20934;&#35774;&#26045;&#30340;
&lt;/p&gt;
&lt;p&gt;
Real-time demand prediction is a critical input for dynamic bus routing. While many researchers have developed numerous complex methods to predict short-term transit demand, the applications have been limited to short, stable time frames and a few stations. How these methods perform in highly dynamic environments has not been studied, nor has their performance been systematically compared. We built an open-source infrastructure with five common methodologies, including econometric and deep learning approaches, and assessed their performance under stable and highly dynamic conditions. We used a time series from smartcard data to predict demand for the following day for the BRT system in Bogota, Colombia. The dynamic conditions in the time series include a month-long protest and the COVID-19 pandemic. Both conditions triggered drastic shifts in demand. The results reveal that most tested models perform similarly in stable conditions, with MAAPE varying from 0.08 to 0.12. The benchmark de
&lt;/p&gt;</description></item><item><title>Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06192</link><description>&lt;p&gt;
Ada-NAV&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06192
&lt;/p&gt;
&lt;p&gt;
Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#26041;&#38754;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#20854;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#65292;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#37096;&#20998;&#26469;&#33258;&#20110;&#26410;&#33021;&#36866;&#24403;&#22320;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#38750;&#38745;&#24577;&#26102;&#12290;&#20026;&#20102;&#21152;&#20837;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-NAV&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#65292;&#20854;&#20013;&#38271;&#24230;&#38543;&#30528;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#65288;&#29992;&#20854;Shannon&#25110;&#24046;&#20998;&#29109;&#34920;&#31034;&#65289;&#30340;&#20943;&#23567;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#30001;&#20110;&#26356;&#39057;&#32321;&#30340;&#26799;&#24230;&#26356;&#26032;&#24378;&#35843;&#20102;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#25506;&#32034;&#65292;&#21518;&#26469;&#21017;&#26356;&#24378;&#35843;&#21033;&#29992;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#65292;&#20223;&#30495;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#34920;&#29616;&#22312;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#24120;&#25968;&#21644;&#38543;&#26426;&#37319;&#26679;&#30340;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#39044;&#31639;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;Ada-NAV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;46&#65285;&#65292;&#37319;&#26679;&#25968;&#37327;&#20943;&#23569;&#20102;&#39640;&#36798;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
&lt;/p&gt;</description></item><item><title>GitHub&#26159;&#20840;&#29699;&#26368;&#22823;&#30340;&#21327;&#20316;&#36719;&#20214;&#24320;&#21457;&#24179;&#21488;&#20043;&#19968;&#65292;&#25176;&#31649;&#20102;&#36229;&#36807;8&#20159;&#20010;&#24320;&#25918;&#25968;&#25454;&#25991;&#20214;&#65292;&#20849;&#35745;142TB&#30340;&#25968;&#25454;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36807;&#21435;&#22235;&#24180;&#20013;&#20854;&#24320;&#25918;&#25968;&#25454;&#36164;&#20135;&#32463;&#21382;&#20102;&#21152;&#36895;&#22686;&#38271;&#65292;&#26377;&#21161;&#20110;&#21152;&#36895;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#65292;&#35299;&#20915;&#22797;&#26434;&#31038;&#20250;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06191</link><description>&lt;p&gt;
GitHub&#19978;&#30340;&#24320;&#25918;&#25968;&#25454;&#65306;&#37322;&#25918;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Open Data on GitHub: Unlocking the Potential of AI. (arXiv:2306.06191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06191
&lt;/p&gt;
&lt;p&gt;
GitHub&#26159;&#20840;&#29699;&#26368;&#22823;&#30340;&#21327;&#20316;&#36719;&#20214;&#24320;&#21457;&#24179;&#21488;&#20043;&#19968;&#65292;&#25176;&#31649;&#20102;&#36229;&#36807;8&#20159;&#20010;&#24320;&#25918;&#25968;&#25454;&#25991;&#20214;&#65292;&#20849;&#35745;142TB&#30340;&#25968;&#25454;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36807;&#21435;&#22235;&#24180;&#20013;&#20854;&#24320;&#25918;&#25968;&#25454;&#36164;&#20135;&#32463;&#21382;&#20102;&#21152;&#36895;&#22686;&#38271;&#65292;&#26377;&#21161;&#20110;&#21152;&#36895;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#65292;&#35299;&#20915;&#22797;&#26434;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GitHub&#26159;&#20840;&#29699;&#26368;&#22823;&#30340;&#21327;&#20316;&#36719;&#20214;&#24320;&#21457;&#24179;&#21488;&#20043;&#19968;&#65292;&#25317;&#26377;&#36229;&#36807;1&#20159;&#29992;&#25143;&#65292;&#21516;&#26102;&#20063;&#34987;&#24191;&#27867;&#29992;&#20110;&#24320;&#25918;&#25968;&#25454;&#21327;&#20316;&#65292;&#25176;&#31649;&#20102;&#36229;&#36807;8&#20159;&#20010;&#24320;&#25918;&#25968;&#25454;&#25991;&#20214;&#65292;&#20849;&#35745;142TB&#30340;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;GitHub&#19978;&#24320;&#25918;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21152;&#36895;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;GitHub&#19978;&#29616;&#26377;&#30340;&#24320;&#25918;&#25968;&#25454;&#21644;&#29992;&#25143;&#20998;&#20139;&#25968;&#25454;&#38598;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;GitHub&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#24320;&#25918;&#25968;&#25454;&#25176;&#31649;&#24179;&#21488;&#20043;&#19968;&#65292;&#24182;&#22312;&#36807;&#21435;&#22235;&#24180;&#20013;&#32463;&#21382;&#20102;&#24320;&#25918;&#25968;&#25454;&#36164;&#20135;&#30340;&#21152;&#36895;&#22686;&#38271;&#12290;&#36890;&#36807;&#23545;GitHub&#19978;&#24320;&#25918;&#25968;&#25454;&#30340;&#27010;&#36848;&#65292;&#25105;&#20204;&#26088;&#22312;&#36171;&#20104;&#29992;&#25143;&#21644;&#32452;&#32455;&#20351;&#29992;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#25968;&#25454;&#38598;&#24182;&#25552;&#39640;&#23427;&#20204;&#30340;&#21487;&#21457;&#29616;&#24615;&#65292;&#20174;&#32780;&#26368;&#32456;&#26377;&#21161;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#31038;&#20250;&#38382;&#39064;&#12290;&#25105;&#20204;&#20250;&#23558;&#25910;&#38598;&#21040;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#20316;&#20026;&#24320;&#25918;&#25968;&#25454;&#21457;&#24067;&#22312;&#20197;&#19979;&#38142;&#25509;&#65306;https://gith
&lt;/p&gt;
&lt;p&gt;
GitHub is the world's largest platform for collaborative software development, with over 100 million users. GitHub is also used extensively for open data collaboration, hosting more than 800 million open data files, totaling 142 terabytes of data. This study highlights the potential of open data on GitHub and demonstrates how it can accelerate AI research. We analyze the existing landscape of open data on GitHub and the patterns of how users share datasets. Our findings show that GitHub is one of the largest hosts of open data in the world and has experienced an accelerated growth of open data assets over the past four years. By examining the open data landscape on GitHub, we aim to empower users and organizations to leverage existing open datasets and improve their discoverability -- ultimately contributing to the ongoing AI revolution to help address complex societal issues. We release the three datasets that we have collected to support this analysis as open datasets at https://gith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$FPDM$&#65292;&#20351;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;$FPDM$&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#36755;&#20837;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#24494;&#35843;&#26102;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06190</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26723;&#32423;&#20803;&#25968;&#25454;&#30340;&#39046;&#22495;&#29305;&#23450;&#24555;&#36895;&#39044;&#35757;&#32451;&#25216;&#26415;$FPDM$
&lt;/p&gt;
&lt;p&gt;
$FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata. (arXiv:2306.06190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$FPDM$&#65292;&#20351;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;$FPDM$&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#36755;&#20837;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#24494;&#35843;&#26102;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#24050;&#26174;&#31034;&#20986;&#22312;&#24320;&#25918;&#39046;&#22495;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;transformers&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$FPDM$&#65288;Fast Pre-training Technique using Document Level Metadata&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#26368;&#20027;&#35201;&#30340;&#21019;&#26032;&#22312;&#20110;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#21477;&#23376;&#32423;&#21035;&#30340;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#65292;&#25345;&#32493;&#23545;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20197;&#36866;&#24212;&#38271;&#25991;&#26723;&#65289;&#65292;&#20294;&#22312;&#23545;&#35813;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#21017;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;$FPDM$&#22312;&#23458;&#25143;&#25903;&#25345;&#12289;&#31185;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#23383;&#31526;&#32423;F1&#20998;&#25968;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#22522;&#20110;transformer&#30340;&#22522;&#20934;&#65292;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#21518;&#24615;&#33021;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training Transformers has shown promising results on open-domain and domain-specific downstream tasks. However, state-of-the-art Transformers require an unreasonably large amount of pre-training data and compute. In this paper, we propose $FPDM$ (Fast Pre-training Technique using Document Level Metadata), a novel, compute-efficient framework that utilizes Document metadata and Domain-Specific Taxonomy as supervision signals to pre-train transformer encoder on a domain-specific corpus. The main innovation is that during domain-specific pretraining, an open-domain encoder is continually pre-trained using sentence-level embeddings as inputs (to accommodate long documents), however, fine-tuning is done with token-level embeddings as inputs to this encoder. We show that $FPDM$ outperforms several transformer-based baselines in terms of character-level F1 scores and other automated metrics in the Customer Support, Scientific, and Legal Domains, and shows a negligible drop in performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;FasterViT&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;HAT&#65292;&#23558;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#22810;&#32423;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#12290; FasterViT&#22312;&#31934;&#24230;&#21644;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;CV&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.06189</link><description>&lt;p&gt;
FasterViT&#65306;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#24555;&#36895;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
FasterViT: Fast Vision Transformers with Hierarchical Attention. (arXiv:2306.06189v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;FasterViT&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;HAT&#65292;&#23558;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#22810;&#32423;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#12290; FasterViT&#22312;&#31934;&#24230;&#21644;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;CV&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;&#65292;&#21629;&#21517;&#20026;FasterViT&#65292;&#19987;&#27880;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#39640;&#22270;&#20687;&#21534;&#21520;&#37327;&#12290;FasterViT&#23558;CNN&#20013;&#24555;&#36895;&#26412;&#22320;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#28857;&#19982;ViT&#20013;&#30340;&#20840;&#23616;&#24314;&#27169;&#29305;&#24615;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#27880;&#24847;&#21147;&#65288;HAT&#65289;&#26041;&#27861;&#65292;&#23558;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#30340;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#20855;&#26377;&#36739;&#23567;&#35745;&#31639;&#25104;&#26412;&#30340;&#22810;&#32423;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#21463;&#30410;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;&#27599;&#20010;&#31383;&#21475;&#37117;&#21487;&#20197;&#35775;&#38382;&#19987;&#38376;&#29992;&#20110;&#26412;&#22320;&#21644;&#20840;&#23616;&#34920;&#31034;&#23398;&#20064;&#30340;&#19987;&#29992;&#36733;&#20307;&#20196;&#29260;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36739;&#20302;&#25104;&#26412;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#65292;FasterViT&#22312;&#31934;&#24230;&#19982;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#21508;&#31181;CV&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#65292;&#21253;&#25324;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;HAT&#21487;&#20197;&#29992;&#20316;&#29616;&#26377;CNN&#26550;&#26500;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy \vs image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for exi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#20272;&#35745;&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#65292;&#24341;&#20837;&#32452;&#21512;&#24230;&#37327;&#24046;&#24322;&#32500;&#24230;&#26469;&#25429;&#25417;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#22810;&#39033;&#24335;&#30340;&#36951;&#25022;&#21644;PAC&#27867;&#21270;&#30028;&#38480;&#30340;&#31639;&#27861;&#65292;&#24182;&#32479;&#19968;&#20102;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#36172;&#21338;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06184</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#20272;&#35745;&#30340;&#32479;&#19968;&#27169;&#22411;&#21644;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Unified Model and Dimension for Interactive Estimation. (arXiv:2306.06184v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#20272;&#35745;&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#65292;&#24341;&#20837;&#32452;&#21512;&#24230;&#37327;&#24046;&#24322;&#32500;&#24230;&#26469;&#25429;&#25417;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#22810;&#39033;&#24335;&#30340;&#36951;&#25022;&#21644;PAC&#27867;&#21270;&#30028;&#38480;&#30340;&#31639;&#27861;&#65292;&#24182;&#32479;&#19968;&#20102;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#36172;&#21338;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;"&#20132;&#20114;&#24335;&#20272;&#35745;"&#30340;&#25277;&#35937;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#30446;&#26631;&#26159;&#26681;&#25454;&#23398;&#20064;&#32773;&#26597;&#35810;&#21040;&#30340;&#26679;&#26412;&#28857;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;"&#30456;&#20284;&#24615;"&#26469;&#20272;&#35745;&#30446;&#26631;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#24046;&#24322;&#32500;&#24230;&#30340;&#32452;&#21512;&#24230;&#37327;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25429;&#25417;&#20102;&#25105;&#20204;&#27169;&#22411;&#20013;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#19988;&#24191;&#27867;&#36866;&#29992;&#30340;&#31639;&#27861;&#65292;&#23545;&#20110;&#36825;&#20010;&#31639;&#27861;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#22312;&#26032;&#30340;&#32500;&#24230;&#19978;&#22810;&#39033;&#24335;&#30340;&#36951;&#25022;&#21644;PAC&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#24182;&#22240;&#27492;&#32479;&#19968;&#20102;&#20004;&#20010;&#32463;&#20856;&#30340;&#23398;&#20064;&#27169;&#22411;&#65306;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#36172;&#21338;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35828;&#26126;&#20102;&#24046;&#24322;&#32500;&#24230;&#22914;&#20309;&#19982;&#36825;&#20004;&#20010;&#26694;&#26550;&#20013;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#21442;&#25968;&#30456;&#20851;&#65292;&#26377;&#26102;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study an abstract framework for interactive learning called interactive estimation in which the goal is to estimate a target from its "similarity'' to points queried by the learner. We introduce a combinatorial measure called dissimilarity dimension which largely captures learnability in our model. We present a simple, general, and broadly-applicable algorithm, for which we obtain both regret and PAC generalization bounds that are polynomial in the new dimension. We show that our framework subsumes and thereby unifies two classic learning models: statistical-query learning and structured bandits. We also delineate how the dissimilarity dimension is related to well-known parameters for both frameworks, in some cases yielding significantly improved analyses.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#21069;&#39304;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#20309;&#22266;&#23450;&#26550;&#26500;&#65292;&#22312;&#27809;&#26377;&#27604;&#36755;&#20837;&#23618;&#26356;&#31364;&#30340;&#23618;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#27809;&#26377;&#38544;&#34255;&#23545;&#31216;&#24615;&#30340;&#21442;&#25968;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#27809;&#26377;&#38544;&#34255;&#23545;&#31216;&#24615;&#30340;&#27010;&#29575;&#36880;&#28176;&#38477;&#20302;&#21040;0&#12290;</title><link>http://arxiv.org/abs/2306.06179</link><description>&lt;p&gt;
ReLU&#32593;&#32476;&#30340;&#38544;&#34255;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hidden symmetries of ReLU networks. (arXiv:2306.06179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06179
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#21069;&#39304;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#20309;&#22266;&#23450;&#26550;&#26500;&#65292;&#22312;&#27809;&#26377;&#27604;&#36755;&#20837;&#23618;&#26356;&#31364;&#30340;&#23618;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#27809;&#26377;&#38544;&#34255;&#23545;&#31216;&#24615;&#30340;&#21442;&#25968;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#27809;&#26377;&#38544;&#34255;&#23545;&#31216;&#24615;&#30340;&#27010;&#29575;&#36880;&#28176;&#38477;&#20302;&#21040;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21069;&#39304;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#20309;&#22266;&#23450;&#26550;&#26500;&#65292;&#21442;&#25968;&#31354;&#38388;&#22312;&#35757;&#32451;&#26399;&#38388;&#29992;&#20316;&#30456;&#20851;&#20989;&#25968;&#31867;&#30340;&#20195;&#29702; - &#20294;&#26159;&#36825;&#31181;&#34920;&#31034;&#26377;&#22810;&#30495;&#23454;&#21602;&#65311;&#24050;&#30693;&#35768;&#22810;&#19981;&#21516;&#30340;&#21442;&#25968;&#35774;&#32622;&#21487;&#20197;&#30830;&#23450;&#30456;&#21516;&#30340;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#20887;&#20313;&#30340;&#31243;&#24230;&#26159;&#19981;&#22343;&#21248;&#30340;&#65306;&#23545;&#20110;&#26576;&#20123;&#32593;&#32476;&#65292;&#21807;&#19968;&#30340;&#23545;&#31216;&#24615;&#26159;&#23618;&#20013;&#31070;&#32463;&#20803;&#30340;&#25490;&#21015;&#21644;&#31070;&#32463;&#20803;&#21442;&#25968;&#30340;&#27491;&#27604;&#20363;&#32553;&#25918;&#65292;&#32780;&#20854;&#20182;&#32593;&#32476;&#21017;&#20855;&#26377;&#20854;&#20182;&#38544;&#34255;&#23545;&#31216;&#24615;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#27809;&#26377;&#27604;&#36755;&#20837;&#23618;&#26356;&#31364;&#30340;&#23618;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#23384;&#22312;&#27809;&#26377;&#38544;&#34255;&#23545;&#31216;&#24615;&#30340;&#21442;&#25968;&#35774;&#32622;&#12290; &#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20123;&#26426;&#21046;&#65292;&#36890;&#36807;&#36825;&#20123;&#26426;&#21046;&#38544;&#34255;&#30340;&#23545;&#31216;&#24615;&#20250;&#20986;&#29616;&#65292;&#24182;&#22312;&#21021;&#22987;&#21270;&#26102;&#32463;&#39564;&#22320;&#36817;&#20284;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#30340;&#21151;&#33021;&#32500;&#24230;&#12290;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#27809;&#26377;&#38544;&#34255;&#23545;&#31216;&#24615;&#30340;&#27010;&#29575;&#36880;&#28176;&#38477;&#20302;&#21040;0&#12290;
&lt;/p&gt;
&lt;p&gt;
The parameter space for any fixed architecture of feedforward ReLU neural networks serves as a proxy during training for the associated class of functions - but how faithful is this representation? It is known that many different parameter settings can determine the same function. Moreover, the degree of this redundancy is inhomogeneous: for some networks, the only symmetries are permutation of neurons in a layer and positive scaling of parameters at a neuron, while other networks admit additional hidden symmetries. In this work, we prove that, for any network architecture where no layer is narrower than the input, there exist parameter settings with no hidden symmetries. We also describe a number of mechanisms through which hidden symmetries can arise, and empirically approximate the functional dimension of different network architectures at initialization. These experiments indicate that the probability that a network has no hidden symmetries decreases towards 0 as depth increases, w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#20248;&#21270;&#20934;&#21017;&#26469;&#26377;&#25928;&#26500;&#24314;&#21442;&#25968;&#21270;&#38477;&#38454;&#20195;&#29702;&#27169;&#22411;&#65292;&#31216;&#20026;ActLearn-POD-KSNN&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06174</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#20195;&#29702;&#24314;&#27169;&#25216;&#26415;&#20248;&#21270;&#21442;&#25968;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#27169;&#25311;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Active-Learning-Driven Surrogate Modeling for Efficient Simulation of Parametric Nonlinear Systems. (arXiv:2306.06174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#20248;&#21270;&#20934;&#21017;&#26469;&#26377;&#25928;&#26500;&#24314;&#21442;&#25968;&#21270;&#38477;&#38454;&#20195;&#29702;&#27169;&#22411;&#65292;&#31216;&#20026;ActLearn-POD-KSNN&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38656;&#35201;&#23545;&#39640;&#20445;&#30495;&#29289;&#29702;&#27169;&#22411;&#30340;&#19981;&#21516;&#21442;&#25968;&#37197;&#32622;&#36827;&#34892;&#37325;&#22797;&#35780;&#20272;&#26102;&#65292;&#22522;&#20110;&#27169;&#22411;&#38454;&#27573;&#38477;&#20302;&#30340;&#20195;&#29702;&#24314;&#27169;&#25216;&#26415;&#26159;&#29702;&#24819;&#30340;&#36873;&#25321;&#12290;&#22312;&#32570;&#20047;&#25551;&#36848;&#21160;&#24577;&#30340;&#25511;&#21046;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#38656;&#35201;&#20197;&#38750;&#20405;&#20837;&#24335;&#30340;&#26041;&#24335;&#26500;&#24314;&#21442;&#25968;&#21270;&#38477;&#38454;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#27531;&#24046;&#30340;&#35823;&#24046;&#20272;&#35745;&#19982;&#21442;&#25968;&#37319;&#26679;&#26041;&#27861;&#19981;&#20877;&#36866;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#20248;&#21270;&#20934;&#21017;&#26469;&#26377;&#25928;&#22320;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#20998;&#31163;&#21442;&#25968;&#29305;&#23450;&#30340;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#23376;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#65292;&#31616;&#31216;ActLearn-POD-KSNN&#20195;&#29702;&#27169;&#22411;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
When repeated evaluations for varying parameter configurations of a high-fidelity physical model are required, surrogate modeling techniques based on model order reduction are desired. In absence of the governing equations describing the dynamics, we need to construct the parametric reduced-order surrogate model in a non-intrusive fashion. In this setting, the usual residual-based error estimate for optimal parameter sampling associated with the reduced basis method is not directly available. Our work provides a non-intrusive optimality criterion to efficiently populate the parameter snapshots, thereby, enabling us to effectively construct a parametric surrogate model. We consider separate parameter-specific proper orthogonal decomposition (POD) subspaces and propose an active-learning-driven surrogate model using kernel-based shallow neural networks, abbreviated as ActLearn-POD-KSNN surrogate model. To demonstrate the validity of our proposed ideas, we present numerical experiments us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.06157</link><description>&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#26694;&#26550;&#36716;&#25442;&#30340;&#25925;&#38556;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26102;&#65292;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#23558;&#27169;&#22411;&#20174;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#65288;&#20363;&#22914;&#65292;&#20174;TensorFlow&#21040;PyTorch&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#20986;&#38169;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#30446;&#26631;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#30830;&#23450;&#36825;&#31181;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;DNNs&#65288;MobileNetV2&#12289;ResNet101&#21644;InceptionV3&#65289;&#36827;&#34892;&#20102;&#19981;&#21516;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;PyTorch&#12289;Keras&#12289;TensorFlow&#65288;TF&#65289;&#21644;TFLite&#65289;&#20043;&#38388;&#36827;&#34892;&#20102;&#36716;&#25442;&#65292;&#24182;&#21457;&#29616;&#20102;&#35768;&#22810;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#39640;&#36798;100&#65285;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23450;&#20301;&#25925;&#38556;&#21644;&#20462;&#22797;&#26377;&#32570;&#38519;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#65292;&#37325;&#28857;&#25918;&#22312;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#20998;&#26512;&#38454;&#27573;&#65306;1&#65289;&#36716;&#25442;&#24037;&#20855;&#65292;2&#65289;&#27169;&#22411;&#21442;&#25968;&#65292;3&#65289;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;4&#65289;&#22270;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#38024;&#23545;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#36716;&#25442;&#24037;&#20855;&#30340;&#25512;&#33616;&#12289;&#35843;&#35797;&#25216;&#24039;&#20197;&#21450;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#25104;&#21151;&#20462;&#22797;&#25152;&#26377;&#27979;&#35797;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;MobileNetV2&#65292;ResNet101&#21644;InceptionV3 &#30340;&#26377;&#32570;&#38519;&#30340;&#36716;&#25442;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
&lt;/p&gt;</description></item><item><title>PoET&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#20309;&#34507;&#30333;&#36136;&#23478;&#26063;&#30340;&#19968;&#31995;&#21015;&#30456;&#20851;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#21487;&#20197;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#35780;&#20998;&#20219;&#20309;&#20462;&#25913;</title><link>http://arxiv.org/abs/2306.06156</link><description>&lt;p&gt;
PoET: &#19968;&#31181;&#23558;&#34507;&#30333;&#36136;&#23478;&#26063;&#30475;&#20316;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PoET: A generative model of protein families as sequences-of-sequences. (arXiv:2306.06156v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06156
&lt;/p&gt;
&lt;p&gt;
PoET&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#20309;&#34507;&#30333;&#36136;&#23478;&#26063;&#30340;&#19968;&#31995;&#21015;&#30456;&#20851;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#21487;&#20197;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#35780;&#20998;&#20219;&#20309;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#26159;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#21151;&#33021;&#30340;&#26032;&#34507;&#30333;&#36136;&#30340;&#33258;&#28982;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#35201;&#20040;&#38590;&#20197;&#25351;&#23548;&#20854;&#29983;&#25104;&#29305;&#23450;&#31867;&#22411;&#30340;&#34507;&#30333;&#36136;&#65292;&#35201;&#20040;&#24517;&#39035;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;&#34507;&#30333;&#36136;&#23478;&#26063;&#30340;&#22823;&#22411;&#22810;&#37325;&#24207;&#21015;&#27604;&#23545;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#20174;&#23478;&#26063;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34507;&#30333;&#36136;&#36827;&#21270;&#21464;&#25442;&#22120;&#65288;PoET&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20840;&#34507;&#30333;&#36136;&#23478;&#26063;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;&#22312;&#25968;&#21315;&#19975;&#20010;&#22825;&#28982;&#34507;&#30333;&#36136;&#24207;&#21015;&#31751;&#20043;&#38388;&#29983;&#25104;&#19968;&#31995;&#21015;&#30456;&#20851;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;PoET&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#34507;&#30333;&#36136;&#23478;&#26063;&#26465;&#20214;&#19979;&#29983;&#25104;&#21644;&#35780;&#20998;&#20219;&#24847;&#20462;&#25913;&#65292;&#32780;&#19988;&#21487;&#20197;&#20174;&#30701;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#22806;&#25512;&#65292;&#22312;&#23567;&#23478;&#26063;&#20013;&#20063;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#36825;&#26159;&#36890;&#36807;&#29420;&#29305;&#30340;Transformer&#23618;&#23454;&#29616;&#30340;&#65307;&#25105;&#20204;&#27169;&#25311;&#20102;&#20196;&#29260;s
&lt;/p&gt;
&lt;p&gt;
Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\textbf{P}$r$\textbf{o}$tein $\textbf{E}$volutionary $\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#28085;&#30422;&#26680;&#24179;&#28369;&#30340;&#24378;&#24230;&#20989;&#25968;&#20272;&#35745;&#12289;&#26368;&#23567;&#21270;&#24378;&#24230;&#37325;&#26500;&#35823;&#24046;&#30340;&#25237;&#24433;&#23398;&#20064;&#21644;&#24402;&#32435;&#26500;&#36896;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#20445;&#30041;&#20102;&#32593;&#32476;&#32467;&#26500;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06155</link><description>&lt;p&gt;
&#24378;&#24230;&#36718;&#24275;&#25237;&#24433;&#65306;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#30340;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks. (arXiv:2306.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#28085;&#30422;&#26680;&#24179;&#28369;&#30340;&#24378;&#24230;&#20989;&#25968;&#20272;&#35745;&#12289;&#26368;&#23567;&#21270;&#24378;&#24230;&#37325;&#26500;&#35823;&#24046;&#30340;&#25237;&#24433;&#23398;&#20064;&#21644;&#24402;&#32435;&#26500;&#36896;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#20445;&#30041;&#20102;&#32593;&#32476;&#32467;&#26500;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24378;&#24230;&#36718;&#24275;&#25237;&#24433;&#8221;&#30340;&#26032;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21160;&#24577;&#32593;&#32476;&#33410;&#28857;&#30340;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#65292;&#35813;&#21160;&#24577;&#32593;&#32476;&#30001;&#33410;&#28857;&#38598;&#21644;&#22312;&#36830;&#32493;&#26102;&#38388;&#20869;&#21457;&#29983;&#30340;&#30636;&#26102;&#20132;&#20114;&#20107;&#20214;&#30340;&#38598;&#21512;&#25152;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#36890;&#36807;&#26680;&#24179;&#28369;&#31561;&#26041;&#27861;&#20272;&#35745;&#33410;&#28857;&#23545;&#20043;&#38388;&#20132;&#20114;&#30340;&#24378;&#24230;&#20989;&#25968;&#65307;&#23398;&#20064;&#19968;&#20010;&#26368;&#23567;&#21270;&#26576;&#31181;&#24378;&#24230;&#37325;&#26500;&#35823;&#24046;&#30340;&#25237;&#24433;&#65307;&#36890;&#36807;&#23398;&#20064;&#30340;&#25237;&#24433;&#24402;&#32435;&#26500;&#36896;&#20986;&#19981;&#26029;&#21457;&#23637;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#34920;&#31034;&#20445;&#30041;&#20102;&#32593;&#32476;&#30340;&#22522;&#26412;&#32467;&#26500;&#65292;&#24182;&#20855;&#26377;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#33410;&#28857;&#34920;&#31034;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#28857;&#19978;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#26500;&#24314;&#20102;&#20272;&#35745;&#29702;&#35770;&#26469;&#38416;&#26126;&#24179;&#28369;&#20316;&#20026;&#20559;&#24046;&#26041;&#24046;&#25240;&#34935;&#30340;&#20316;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#38543;&#30528;&#20449;&#22122;&#27604;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#24179;&#28369;&#31243;&#24230;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithmic framework, Intensity Profile Projection, for learning continuous-time representations of the nodes of a dynamic network, characterised by a node set and a collection of instantaneous interaction events which occur in continuous time. Our framework consists of three stages: estimating the intensity functions underlying the interactions between pairs of nodes, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and inductively constructing evolving node representations via the learned projection. We show that our representations preserve the underlying structure of the network, and are temporally coherent, meaning that node representations can be meaningfully compared at different points in time. We develop estimation theory which elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce smoothing as the signal-to-noise ratio increases on account of the algorithm `borrow
&lt;/p&gt;</description></item><item><title>HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06154</link><description>&lt;p&gt;
HypLL: &#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06154
&lt;/p&gt;
&lt;p&gt;
HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#22810;&#23186;&#20307;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#65292;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#27491;&#36805;&#36895;&#24341;&#36215;&#20851;&#27880;&#12290;&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#25968;&#25454;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#26102;&#65292;&#24076;&#20122;&#20960;&#20309;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#24211;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HypLL, &#21363;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#20197;&#23558;&#24076;&#20122;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;HypLL&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#29305;&#21035;&#24378;&#35843;&#20854;&#26131;&#29992;&#24615;&#35774;&#35745;&#65292;&#20197;&#21560;&#24341;&#24191;&#27867;&#30340;&#21463;&#20247;&#20851;&#27880;&#36825;&#20010;&#26032;&#30340;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/maxvanspengler/hyperbolic_learning_library&#12290;&#21387;&#32553;&#25991;&#20214;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://d
&lt;/p&gt;
&lt;p&gt;
Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for easy-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library. The compressed archive is available at: https://d
&lt;/p&gt;</description></item><item><title>EfficientBioAI&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#21387;&#32553;&#29983;&#29289;&#25104;&#20687;AI&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#36816;&#34892;&#26102;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#33021;&#28304;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06152</link><description>&lt;p&gt;
EfficientBioAI&#65306;&#20351;&#29983;&#29289;&#25104;&#20687;AI&#27169;&#22411;&#22312;&#33021;&#37327;&#12289;&#24310;&#36831;&#21644;&#34920;&#31034;&#26041;&#38754;&#26356;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
EfficientBioAI: Making Bioimaging AI Models Efficient in Energy, Latency and Representation. (arXiv:2306.06152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06152
&lt;/p&gt;
&lt;p&gt;
EfficientBioAI&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#21387;&#32553;&#29983;&#29289;&#25104;&#20687;AI&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#36816;&#34892;&#26102;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#33021;&#28304;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#29983;&#29289;&#25104;&#20687;&#20998;&#26512;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#38271;&#20197;&#21450;&#29616;&#20195;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#38656;&#35201;&#24555;&#36895;&#22686;&#38271;&#30340;&#20998;&#26512;&#38656;&#27714;&#65292;AI&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#22914;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#65292;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#12290;&#23601;&#20687;&#25105;&#20204;&#21487;&#20197;&#21387;&#32553;&#22823;&#22411;&#22270;&#20687;&#20197;&#23454;&#29616;&#39640;&#25928;&#23384;&#20648;&#21644;&#20849;&#20139;&#19968;&#26679;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#21387;&#32553;AI&#27169;&#22411;&#20197;&#23454;&#29616;&#39640;&#25928;&#24212;&#29992;&#21644;&#37096;&#32626;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EfficientBioAI&#65292;&#36825;&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#21387;&#32553;&#32473;&#23450;&#30340;&#29983;&#29289;&#25104;&#20687;AI&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#36816;&#34892;&#26102;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#33021;&#28304;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21387;&#32553;&#21518;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#29978;&#33267;&#21487;&#33021;&#20250;&#22686;&#21152;&#65292;&#22240;&#20026;&#21387;&#32553;&#36807;&#31243;&#21487;&#20197;&#21435;&#38500;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#12290;&#20174;&#22235;&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;2-5&#20493;&#65292;&#22312;GPU&#19978;&#25552;&#39640;&#20102;&#32422;3-10&#20493;&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22987;&#32456;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has been widely used in bioimage image analysis nowadays, but the efficiency of AI models, like the energy consumption and latency is not ignorable due to the growing model size and complexity, as well as the fast-growing analysis needs in modern biomedical studies. Like we can compress large images for efficient storage and sharing, we can also compress the AI models for efficient applications and deployment. In this work, we present EfficientBioAI, a plug-and-play toolbox that can compress given bioimaging AI models for them to run with significantly reduced energy cost and inference time on both CPU and GPU, without compromise on accuracy. In some cases, the prediction accuracy could even increase after compression, since the compression procedure could remove redundant information in the model representation and therefore reduce over-fitting. From four different bioimage analysis applications, we observed around 2-5 times speed-up during inference and 3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36752;&#23556;&#38450;&#25252;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36752;&#23556;&#38450;&#25252;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;&#21644;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20419;&#36827;&#31185;&#23398;&#25216;&#26415;&#25104;&#26524;&#30340;&#21512;&#20316;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.06148</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#36752;&#23556;&#38450;&#25252;&#65306;&#38761;&#21629;&#24615;&#36824;&#26159;&#26356;&#26032;&#25442;&#20195;&#65311;(arXiv:2306.06148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence and radiation protection. A game changer or an update?. (arXiv:2306.06148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36752;&#23556;&#38450;&#25252;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36752;&#23556;&#38450;&#25252;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;&#21644;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20419;&#36827;&#31185;&#23398;&#25216;&#26415;&#25104;&#26524;&#30340;&#21512;&#20316;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#34987;&#35748;&#20026;&#26159;&#26412;&#19990;&#32426;&#26368;&#39072;&#35206;&#24615;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#20855;&#26377;&#26080;&#25968;&#30340;&#24212;&#29992;&#12290;&#37027;&#20040;&#23427;&#23545;&#36752;&#23556;&#38450;&#25252;&#24847;&#21619;&#30528;&#20160;&#20040;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#36752;&#23556;&#38450;&#25252;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#12290;&#39044;&#35745;&#20154;&#24037;&#26234;&#33021;&#22312;&#36752;&#23556;&#38450;&#25252;&#20013;&#30340;&#20351;&#29992;&#23558;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20123;&#20248;&#28857;&#21644;&#28508;&#22312;&#30340;&#38556;&#30861;&#21644;&#38382;&#39064;&#65292;&#21253;&#25324;&#20262;&#29702;&#38382;&#39064;&#12290;&#20316;&#32773;&#24314;&#35758;&#36752;&#23556;&#38450;&#25252;&#19987;&#19994;&#20154;&#21592;&#19982;&#25968;&#25454;&#31185;&#23398;&#23478;&#19987;&#23478;&#21512;&#20316;&#65292;&#21152;&#36895;&#24182;&#25351;&#23548;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#31185;&#23398;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is regarded as one of the most disruptive technology of the century and with countless applications. What does it mean for radiation protection? This article describes the fundamentals of machine learning (ML) based methods and presents the inaugural applications in different fields of radiation protection. It is foreseen that the usage of AI will increase in radiation protection. Consequently, this article explores some of the benefits and also the potential barriers and questions, including ethical ones, that can come out. The article proposes that collaboration between radiation protection professionals and data scientist experts can accelerate and guide the development of the algorithms for effective scientific and technological outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22521;&#35757;&#26041;&#27861;&#24433;&#21709;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06146</link><description>&lt;p&gt;
&#38544;&#34255;&#20998;&#31867;&#23618;&#65306;&#20851;&#20110;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#26356;&#39640;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes. (arXiv:2306.06146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22521;&#35757;&#26041;&#27861;&#24433;&#21709;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#20195;&#34920;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37117;&#22522;&#20110;&#26631;&#20934;&#30340;&#22810;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#31181;&#12290;&#36825;&#20123;&#20063;&#34987;&#31216;&#20026;&#28145;&#24230;&#32593;&#32476;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#27599;&#20010;&#38544;&#34255;&#31070;&#32463;&#23618;&#23436;&#25104;&#19968;&#31181;&#25968;&#25454;&#36716;&#25442;&#65292;&#39044;&#26399;&#20351;&#25968;&#25454;&#34920;&#31034;&#8220;&#27604;&#20043;&#21069;&#26356;&#32447;&#24615;&#21487;&#20998;&#8221;&#65292;&#20197;&#33719;&#24471;&#23613;&#21487;&#33021;&#32447;&#24615;&#21487;&#20998;&#30340;&#26368;&#32456;&#25968;&#25454;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#21487;&#20197;&#25191;&#34892;&#36825;&#20123;&#36716;&#25442;&#30340;&#36866;&#24403;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22521;&#35757;&#26041;&#27861;&#23545;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#38544;&#34255;&#23618;&#30340;&#25968;&#25454;&#34920;&#31034;&#20855;&#26377;&#26356;&#39640;&#30340;&#31867;&#20043;&#38388;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#24341;&#20837;&#20102;&#19968;&#20010;&#28041;&#21450;&#35823;&#24046;&#20989;&#25968;&#30340;&#26032;&#39062;&#22521;&#35757;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of classification problems, Deep Learning (DL) approaches represent state of art. Many DL approaches are based on variations of standard multi-layer feed-forward neural networks. These are also referred to as deep networks. The basic idea is that each hidden neural layer accomplishes a data transformation which is expected to make the data representation "somewhat more linearly separable" than the previous one to obtain a final data representation which is as linearly separable as possible. However, determining the appropriate neural network parameters that can perform these transformations is a critical problem. In this paper, we investigate the impact on deep network classifier performances of a training approach favouring solutions where data representations at the hidden layers have a higher degree of linear separability between the classes with respect to standard methods. To this aim, we propose a neural network architecture which induces an error function involvin
&lt;/p&gt;</description></item><item><title>LDMRes-Net&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#21452;&#37325;&#22810;&#37325;&#27531;&#24046;&#36830;&#25509;&#26469;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#35745;&#31639;&#25104;&#26412;&#65292;&#21487;&#29992;&#20110;&#23454;&#26102;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#24182;&#22312;8&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06145</link><description>&lt;p&gt;
LDMRes-Net&#65306;&#36890;&#36807;&#39640;&#25928;&#22270;&#20687;&#20998;&#21106;&#23454;&#29616;&#23454;&#26102;&#30142;&#30149;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
LDMRes-Net: Enabling Real-Time Disease Monitoring through Efficient Image Segmentation. (arXiv:2306.06145v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06145
&lt;/p&gt;
&lt;p&gt;
LDMRes-Net&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#21452;&#37325;&#22810;&#37325;&#27531;&#24046;&#36830;&#25509;&#26469;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#35745;&#31639;&#25104;&#26412;&#65292;&#21487;&#29992;&#20110;&#23454;&#26102;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#24182;&#22312;8&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#32593;&#33180;&#30524;&#30149;&#22914;&#26524;&#19981;&#21450;&#26089;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#21487;&#33021;&#23548;&#33268;&#21452;&#30524;&#19981;&#21487;&#36870;&#30340;&#35270;&#21147;&#25439;&#22833;&#12290;&#30001;&#20110;&#35270;&#32593;&#33180;&#30142;&#30149;&#30340;&#22797;&#26434;&#24615;&#65292;&#35270;&#32593;&#33180;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#20004;&#20010;&#25110;&#26356;&#22810;&#24322;&#24120;&#12290;&#30446;&#21069;&#29992;&#20110;&#20998;&#21106;&#24102;&#26377;&#22810;&#20010;&#26631;&#31614;&#21644;&#29305;&#24449;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#19981;&#36275;&#21644;&#27867;&#21270;&#24615;&#19981;&#24378;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#37319;&#29992;&#21452;&#37325;&#22810;&#37325;&#27531;&#24046;&#36830;&#25509;&#22686;&#24378;&#20998;&#21106;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#35745;&#31639;&#25104;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;8&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#23545;&#20110;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#39640;&#25928;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#23454;&#26102;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#24212;&#29992;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retinal eye diseases can lead to irreversible vision loss in both eyes if not diagnosed and treated earlier. Owing to the complexities of retinal diseases, the likelihood that retinal images would contain two or more abnormalities is very high. The current deep learning algorithms used for segmenting retinal images with multiple labels and features suffer from inadequate detection accuracy and a lack of generalizability. In this paper, we propose a lightweight and efficient network, featuring dual multi-residual connections to enhance segmentation performance while minimizing computational cost. The proposed network is evaluated on eight publicly available retinal image datasets and achieved promising segmentation results, which demonstrate the effectiveness of the proposed network for retinal image analysis tasks. The proposed network's lightweight and efficient design makes it a promising candidate for real-time retinal image analysis applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;NIR&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#20998;&#31867;&#31995;&#32479;&#30340;&#24615;&#33021;&#26159;&#21542;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06140</link><description>&lt;p&gt;
Null/No Information Rate (NIR)&#65306;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#38382;&#39064;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26159;&#21542;&#26174;&#33879;&#30340;&#32479;&#35745;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Null/No Information Rate (NIR): a statistical test to assess if a classification accuracy is significant for a given problem. (arXiv:2306.06140v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06140
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;NIR&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#20998;&#31867;&#31995;&#32479;&#30340;&#24615;&#33021;&#26159;&#21542;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#30740;&#31350;&#32972;&#26223;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#32463;&#36807;&#30740;&#31350;&#21644;&#24320;&#21457;&#20998;&#31867;&#31995;&#32479;&#21518;&#65292;&#33258;&#28982;&#20250;&#20135;&#29983;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#36825;&#20010;&#20934;&#30830;&#24230;&#26159;&#21542;&#36275;&#22815;&#39640;&#65311;&#8221;&#25110;&#32773;&#26356;&#22909;&#30340;&#38382;&#39064;&#26159;&#65292;&#8220;&#25105;&#20204;&#33021;&#21542;&#20197;&#32479;&#35745;&#23398;&#26174;&#30528;&#30340;&#32622;&#20449;&#24230;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#20998;&#31867;&#31995;&#32479;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26412;&#25991;&#25551;&#36848;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#26377;&#26102;&#31216;&#20026;NIR&#65288;&#26080;&#20449;&#24687;&#29575;&#25110;&#38646;&#20540;&#20449;&#24687;&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many research contexts, especially in the biomedical field, after studying and developing a classification system a natural question arises: "Is this accuracy enough high?", or better, "Can we say, with a statistically significant confidence, that our classification system is able to solve the problem"? To answer to this question, we can use the statistical test described in this paper, which is referred in some cases as NIR (No Information Rate or Null Information Rate).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#24335;&#26041;&#27861;&#36827;&#34892;&#21152;&#26435;&#24322;&#24120;&#20540;&#26816;&#27979;&#30340;&#22823;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#25581;&#31034;&#31995;&#32479;&#25925;&#38556;&#12289;&#27450;&#35784;&#27963;&#21160;&#21644;&#25968;&#25454;&#27169;&#24335;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.06139</link><description>&lt;p&gt;
WePaMaDM-Outlier Detection: &#20351;&#29992;&#27169;&#24335;&#26041;&#27861;&#36827;&#34892;&#21152;&#26435;&#24322;&#24120;&#20540;&#26816;&#27979;&#30340;&#22823;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WePaMaDM-Outlier Detection: Weighted Outlier Detection using Pattern Approaches for Mass Data Mining. (arXiv:2306.06139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#24335;&#26041;&#27861;&#36827;&#34892;&#21152;&#26435;&#24322;&#24120;&#20540;&#26816;&#27979;&#30340;&#22823;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#25581;&#31034;&#31995;&#32479;&#25925;&#38556;&#12289;&#27450;&#35784;&#27963;&#21160;&#21644;&#25968;&#25454;&#27169;&#24335;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#24322;&#24120;&#20540;&#26816;&#27979;&#26159;&#19968;&#31181;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#24322;&#24120;&#25110;&#24322;&#24120;&#25968;&#25454;&#28857;&#30340;&#26041;&#27861;&#65292;&#21487;&#33021;&#30001;&#21508;&#31181;&#22240;&#32032;&#22914;&#20154;&#20026;&#38169;&#35823;&#12289;&#27450;&#35784;&#25110;&#35774;&#22791;&#25925;&#38556;&#24341;&#36215;&#12290;&#26816;&#27979;&#24322;&#24120;&#20540;&#21487;&#20197;&#25581;&#31034;&#26377;&#20851;&#31995;&#32479;&#25925;&#38556;&#12289;&#27450;&#35784;&#27963;&#21160;&#21644;&#25968;&#25454;&#27169;&#24335;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#24110;&#21161;&#19987;&#23478;&#35299;&#20915;&#36825;&#20123;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#27491;&#24120;&#25968;&#25454;&#27169;&#24335;&#30340;&#27169;&#22411;&#20197;&#35782;&#21035;&#24322;&#24120;&#20540;&#21487;&#33021;&#30001;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#24615;&#36136;&#12289;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#38382;&#39064;&#30340;&#29305;&#23450;&#35201;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#29305;&#23450;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#25552;&#20986;&#20102;WePaMaDM-Outlier Detection&#26041;&#27861;&#65292;&#35777;&#26126;&#36825;&#26679;&#30340;&#25216;&#26415;&#26159;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#24182;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#21046;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#30340;&#39046;&#22495;&#21487;&#20197;&#36827;&#34892;&#20462;&#25913;&#20197;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#36824;&#30740;&#31350;&#20102;&#25968;&#25454;&#24314;&#27169;&#22312;&#30417;&#25511;&#12289;&#25925;&#38556;&#26816;&#27979;&#21644;&#36235;&#21183;&#20998;&#26512;&#20013;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted Outlier Detection is a method for identifying unusual or anomalous data points in a dataset, which can be caused by various factors like human error, fraud, or equipment malfunctions. Detecting outliers can reveal vital information about system faults, fraudulent activities, and patterns in the data, assisting experts in addressing the root causes of these anomalies. However,creating a model of normal data patterns to identify outliers can be challenging due to the nature of input data, labeled data availability, and specific requirements of the problem. This article proposed the WePaMaDM-Outlier Detection with distinct mass data mining domain, demonstrating that such techniques are domain-dependent and usually developed for specific problem formulations. Nevertheless, similar domains can adapt solutions with modifications. This work also investigates the significance of data modeling in outlier detection techniques in surveillance, fault detection, and trend analysis, also re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#21462;&#21644;&#24674;&#22797;&#28508;&#22312;&#21160;&#21147;&#23398;&#23545;&#40784;&#30340;&#26102;&#31354;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#28508;&#22312;&#21160;&#21147;&#23398;&#26102;&#31354;&#32467;&#26500;&#23548;&#33268;&#23545;&#40784;&#21518;&#24615;&#33021;&#36136;&#37327;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06138</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#21462;&#21644;&#24674;&#22797;&#28508;&#22312;&#21160;&#21147;&#23398;&#23545;&#40784;&#30340;&#26102;&#31354;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model. (arXiv:2306.06138v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#21462;&#21644;&#24674;&#22797;&#28508;&#22312;&#21160;&#21147;&#23398;&#23545;&#40784;&#30340;&#26102;&#31354;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#28508;&#22312;&#21160;&#21147;&#23398;&#26102;&#31354;&#32467;&#26500;&#23548;&#33268;&#23545;&#40784;&#21518;&#24615;&#33021;&#36136;&#37327;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34892;&#20026;&#30456;&#20851;&#30340;&#33041;&#35745;&#31639;&#39046;&#22495;&#65292;&#26377;&#24517;&#35201;&#23558;&#21407;&#22987;&#31070;&#32463;&#32676;&#20307;&#27963;&#21160;&#19982;&#23427;&#20204;&#20043;&#38388;&#30340;&#21095;&#28872;&#21464;&#21270;&#26377;&#24847;&#20041;&#22320;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#31070;&#32463;&#32676;&#20307;&#27963;&#21160;&#37117;&#20197;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#24335;&#20986;&#29616;&#65292;&#22240;&#27492;&#23545;&#40784;&#26159;&#38750;&#24120;&#26840;&#25163;&#30340;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#24037;&#20855;&#24615;&#26694;&#26550;&#35748;&#20026;&#65292;&#22522;&#20110;&#35797;&#39564;&#30340;&#31070;&#32463;&#32676;&#20307;&#27963;&#21160;&#20381;&#36182;&#20110;&#20302;&#32500;&#24230;&#28508;&#22312;&#21160;&#21147;&#23398;&#12290;&#20851;&#27880;&#36825;&#31181;&#28508;&#22312;&#21160;&#21147;&#23398;&#22823;&#22823;&#20419;&#36827;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#23613;&#31649;&#25105;&#20204;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#28508;&#22312;&#21160;&#21147;&#23398;&#20013;&#22266;&#26377;&#30340;&#26102;&#31354;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#23545;&#40784;&#21518;&#23548;&#33268;&#21160;&#21147;&#23398;&#32467;&#26500;&#21644;&#25972;&#20307;&#24615;&#33021;&#36136;&#37327;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#39318;&#20808;&#25552;&#21462;&#28304;&#22495;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#32467;&#26500;&#65292;&#28982;&#21518;&#22312;&#23545;&#40784;&#26102;&#36827;&#34892;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of behavior-related brain computation, it is necessary to meaningfully align raw neural population activities against the drastic shift between them. However, the alignment is non-trivial since most neural population activities are in a multivariate time-series manner. An instrumental framework within neuroscience research posits that trial-based neural population activities rely on low-dimensional latent dynamics. Focusing on such latent dynamics greatly facilitates the alignment procedure. Despite the considerable progress we have reached, existing methods usually ignore the intrinsic spatio-temporal structures within latent dynamics. Thus, those solutions lead to poor quality in dynamics structures and overall performance after alignment. To tackle this problem, we propose a method leveraging the expressiveness of diffusion model to relieve such issues. Specifically, the latent dynamics structures of the source domain are first extracted by the diffusion model. Then, su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#40065;&#26834;&#24615;&#27979;&#35797;&#26694;&#26550;RTCA&#65292;&#37319;&#29992;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#30340;&#20851;&#38190;&#26234;&#33021;&#20307;&#25915;&#20987;&#26041;&#27861;&#65292;&#20851;&#38190;&#26234;&#33021;&#20307;&#36890;&#36807;&#22242;&#38431;&#21512;&#20316;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#34987;&#36873;&#21462;&#20026;&#21463;&#23475;&#32773;&#12290;&#35813;&#26694;&#26550;&#22312;&#40065;&#26834;&#24615;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06136</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;&#65306;&#23545;&#20851;&#38190;&#26234;&#33021;&#20307;&#36827;&#34892;&#29366;&#24577;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents. (arXiv:2306.06136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#40065;&#26834;&#24615;&#27979;&#35797;&#26694;&#26550;RTCA&#65292;&#37319;&#29992;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#30340;&#20851;&#38190;&#26234;&#33021;&#20307;&#25915;&#20987;&#26041;&#27861;&#65292;&#20851;&#38190;&#26234;&#33021;&#20307;&#36890;&#36807;&#22242;&#38431;&#21512;&#20316;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#34987;&#36873;&#21462;&#20026;&#21463;&#23475;&#32773;&#12290;&#35813;&#26694;&#26550;&#22312;&#40065;&#26834;&#24615;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#26234;&#33021;&#20132;&#36890;&#21644;&#26080;&#20154;&#26426;&#31561;&#35768;&#22810;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;MARL&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#23545;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#26159;&#30830;&#35748;&#27169;&#22411;&#22312;&#38754;&#23545;&#24847;&#22806;&#25200;&#21160;&#26102;&#21487;&#20449;&#36182;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;MARL&#30340;&#26032;&#22411;&#40065;&#26834;&#24615;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#25915;&#20987;&#20851;&#38190;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#65288;RTCA&#65289;&#12290;RTCA&#26377;&#20004;&#20010;&#21019;&#26032;&#28857;&#65306;1&#65289;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#30340;&#26041;&#27861;&#36873;&#23450;&#20851;&#38190;&#30340;&#26234;&#33021;&#20307;&#20316;&#20026;&#21463;&#23475;&#32773;&#65292;&#24182;&#20026;&#23427;&#20204;&#25552;&#20379;&#26368;&#22351;&#24773;&#20917;&#30340;&#32852;&#21512;&#21160;&#20316;&#24314;&#35758;&#65307;2&#65289;&#37319;&#29992;&#22242;&#38431;&#21512;&#20316;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#20316;&#20026;DE&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#26368;&#22351;&#24773;&#20917;&#30340;&#32852;&#21512;&#21160;&#20316;&#29983;&#25104;&#20851;&#38190;&#26234;&#33021;&#20307;&#30340;&#23545;&#25239;&#24615;&#29366;&#24577;&#25200;&#21160;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#21463;&#23475;&#32773;&#26234;&#33021;&#20307;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;&#26694;&#26550;&#12290;RTCA&#22312;&#40065;&#26834;&#24615;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) has been widely applied in many fields such as smart traffic and unmanned aerial vehicles. However, most MARL algorithms are vulnerable to adversarial perturbations on agent states. Robustness testing for a trained model is an essential step for confirming the trustworthiness of the model against unexpected perturbations. This work proposes a novel Robustness Testing framework for MARL that attacks states of Critical Agents (RTCA). The RTCA has two innovations: 1) a Differential Evolution (DE) based method to select critical agents as victims and to advise the worst-case joint actions on them; and 2) a team cooperation policy evaluation method employed as the objective function for the optimization of DE. Then, adversarial state perturbations of the critical agents are generated based on the worst-case joint actions. This is the first robustness testing framework with varying victim agents. RTCA demonstrates outstanding performance in terms of 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27169;&#20223;&#26368;&#31967;&#31957;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#36807;&#28388;&#22120;&#23454;&#29616;&#36127;&#36131;&#20219;&#37096;&#32626;&#65307;&#36890;&#36807;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#25351;&#26631;&#20844;&#24179;&#30340;&#23450;&#20041;&#65292;&#21015;&#20030;&#20102;&#27599;&#20010;&#39046;&#22495;&#21487;&#33021;&#36935;&#21040;&#30340;&#20363;&#23376;&#25439;&#23475;&#65292;&#24182;&#25552;&#20379;&#20102;&#25439;&#23475;&#37327;&#21270;&#30340;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06135</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20869;&#23481;&#23457;&#26680;&#23433;&#20840;&#19982;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Safety and Fairness for Content Moderation in Generative Models. (arXiv:2306.06135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06135
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27169;&#20223;&#26368;&#31967;&#31957;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#36807;&#28388;&#22120;&#23454;&#29616;&#36127;&#36131;&#20219;&#37096;&#32626;&#65307;&#36890;&#36807;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#25351;&#26631;&#20844;&#24179;&#30340;&#23450;&#20041;&#65292;&#21015;&#20030;&#20102;&#27599;&#20010;&#39046;&#22495;&#21487;&#33021;&#36935;&#21040;&#30340;&#20363;&#23376;&#25439;&#23475;&#65292;&#24182;&#25552;&#20379;&#20102;&#25439;&#23475;&#37327;&#21270;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#27493;&#65292;&#26032;&#25216;&#26415;&#27491;&#36805;&#36895;&#37096;&#32626;&#21040;&#29983;&#25104;&#32452;&#20214;&#20013;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23548;&#33268;&#20854;&#34892;&#20026;&#21487;&#33021;&#27169;&#20223;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#31967;&#31957;&#30340;&#20869;&#23481;&#12290;&#36127;&#36131;&#20219;&#22320;&#37096;&#32626;&#29983;&#25104;&#25216;&#26415;&#38656;&#35201;&#20869;&#23481;&#23457;&#26680;&#31574;&#30053;&#65292;&#20363;&#22914;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#36807;&#28388;&#22120;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#24565;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#36127;&#36131;&#20219;&#20869;&#23481;&#23457;&#26680;&#65292;&#21253;&#25324;&#22914;&#20309;&#23454;&#35777;&#22320;&#34913;&#37327;&#25105;&#20204;&#21015;&#20030;&#30340;&#26500;&#36896;&#12290;&#25105;&#20204;&#23450;&#20041;&#21644;&#21306;&#20998;&#20102;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#25351;&#26631;&#20844;&#24179;&#30340;&#27010;&#24565;&#65292;&#24182;&#21015;&#20030;&#20102;&#27599;&#20010;&#39046;&#22495;&#21487;&#33021;&#20986;&#29616;&#30340;&#20363;&#23376;&#25439;&#23475;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22914;&#20309;&#37327;&#21270;&#23450;&#20041;&#30340;&#25439;&#23475;&#30340;&#28436;&#31034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#28436;&#31034;&#30340;&#25439;&#23475;&#37327;&#21270;&#39118;&#26684;&#22914;&#20309;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20869;&#23481;&#23457;&#26680;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22768;&#38899;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22240;&#26524;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#30340;&#39044;&#27979;&#65292;&#21453;&#23545;&#36890;&#36807;&#23558;&#20998;&#25968;&#24402;&#22240;&#20110;&#36755;&#20837;&#32452;&#20214;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#24815;&#20363;&#65292;&#24182;&#19988;&#25552;&#20986;&#24212;&#29992;&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#30284;&#30151;&#39044;&#27979;&#27169;&#22411;&#30340;&#22768;&#38899;&#35299;&#37322;&#65292;&#20197;&#24314;&#31435;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2306.06134</link><description>&lt;p&gt;
&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#22768;&#38899;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Sound Explanation for Trustworthy Machine Learning. (arXiv:2306.06134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22768;&#38899;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22240;&#26524;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#30340;&#39044;&#27979;&#65292;&#21453;&#23545;&#36890;&#36807;&#23558;&#20998;&#25968;&#24402;&#22240;&#20110;&#36755;&#20837;&#32452;&#20214;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#24815;&#20363;&#65292;&#24182;&#19988;&#25552;&#20986;&#24212;&#29992;&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#30284;&#30151;&#39044;&#27979;&#27169;&#22411;&#30340;&#22768;&#38899;&#35299;&#37322;&#65292;&#20197;&#24314;&#31435;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#27491;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#21453;&#23545;&#36890;&#36807;&#23558;&#20998;&#25968;&#24402;&#22240;&#20110;&#36755;&#20837;&#32452;&#20214;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#24815;&#20363;&#65292;&#22240;&#20026;&#36825;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#30446;&#26631;&#23384;&#22312;&#20869;&#22312;&#30340;&#20914;&#31361;&#12290;&#25105;&#20204;&#35777;&#26126;&#27809;&#26377;&#20219;&#20309;&#24402;&#22240;&#31639;&#27861;&#33021;&#22815;&#28385;&#36275;&#29305;&#24322;&#24615;&#12289;&#21487;&#21152;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#22522;&#32447;&#19981;&#21464;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#19981;&#27491;&#24335;&#37319;&#29992;&#30340;&#27010;&#24565;&#8212;&#8212;&#22768;&#38899;&#35299;&#37322;&#12290;&#19968;&#20010;&#22768;&#38899;&#30340;&#35299;&#37322;&#38656;&#35201;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22240;&#26524;&#35299;&#37322;&#31995;&#32479;&#25152;&#36827;&#34892;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24212;&#29992;&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#30284;&#30151;&#39044;&#27979;&#27169;&#22411;&#30340;&#22768;&#38899;&#35299;&#37322;&#65292;&#20197;&#24314;&#31435;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
We take a formal approach to the explainability problem of machine learning systems. We argue against the practice of interpreting black-box models via attributing scores to input components due to inherently conflicting goals of attribution-based interpretation. We prove that no attribution algorithm satisfies specificity, additivity, completeness, and baseline invariance. We then formalize the concept, sound explanation, that has been informally adopted in prior work. A sound explanation entails providing sufficient information to causally explain the predictions made by a system. Finally, we present the application of feature selection as a sound explanation for cancer prediction models to cultivate trust among clinicians.
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#29983;&#25104;AI&#21644;&#20114;&#32852;&#32593;&#20043;&#38388;&#20114;&#21160;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;AI&#21487;&#33021;&#25104;&#20026;&#25968;&#25454;&#20179;&#24211;&#36129;&#29486;&#32773;&#24182;&#24433;&#21709;&#21518;&#32493;&#35757;&#32451;&#65292;&#25552;&#20986;&#26410;&#26469;&#29256;&#26412;&#29983;&#25104;AI&#24037;&#20855;&#22312;&#20351;&#29992;&#28151;&#21512;&#25968;&#25454;&#35757;&#32451;&#26102;&#20250;&#20986;&#29616;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.06130</link><description>&lt;p&gt;
&#25506;&#31350;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20114;&#32852;&#32593;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet. (arXiv:2306.06130v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06130
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#29983;&#25104;AI&#21644;&#20114;&#32852;&#32593;&#20043;&#38388;&#20114;&#21160;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;AI&#21487;&#33021;&#25104;&#20026;&#25968;&#25454;&#20179;&#24211;&#36129;&#29486;&#32773;&#24182;&#24433;&#21709;&#21518;&#32493;&#35757;&#32451;&#65292;&#25552;&#20986;&#26410;&#26469;&#29256;&#26412;&#29983;&#25104;AI&#24037;&#20855;&#22312;&#20351;&#29992;&#28151;&#21512;&#25968;&#25454;&#35757;&#32451;&#26102;&#20250;&#20986;&#29616;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22914;DALL-E&#12289;MidJourney&#25110;ChatGPT&#31561;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#25110;&#25991;&#26412;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24341;&#21457;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#31038;&#20250;&#24433;&#21709;&#25104;&#20026;&#20844;&#20849;&#20105;&#35770;&#30340;&#26680;&#24515;&#12290;&#36825;&#20123;&#24037;&#20855;&#26159;&#21487;&#33021;&#30340;&#65292;&#26159;&#22240;&#20026;&#20114;&#32852;&#32593;&#19978;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#37327;&#25968;&#25454;&#65288;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#25104;&#20026;&#20869;&#23481;&#21019;&#20316;&#32773;&#65292;&#24050;&#32463;&#20026;&#26410;&#26469;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#25968;&#25454;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#26410;&#26469;&#29256;&#26412;&#23558;&#36890;&#36807;&#20154;&#24037;&#21019;&#24314;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#28151;&#21512;&#35757;&#32451;&#65292;&#24341;&#21457;&#28508;&#22312;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21644;&#20844;&#20849;&#25968;&#25454;&#20179;&#24211;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#36825;&#31181;&#20114;&#21160;&#24341;&#21457;&#20102;&#35768;&#22810;&#38382;&#39064;&#65306;&#26410;&#26469;&#29256;&#26412;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#28151;&#21512;&#30340;&#30495;&#23454;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#23558;&#22914;&#20309;&#34920;&#29616;&#65311;&#23427;&#20204;&#26159;&#21542;&#20250;&#38543;&#30528;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#21270;&#21644;&#25913;&#36827;&#36824;&#26159;&#30456;&#21453;&#21464;&#24471;&#26356;&#24046;&#65311;&#36827;&#21270;&#26159;&#21542;&#20250;&#24341;&#20837;&#20559;&#35823;&#25110;&#25910;&#32553;&#35270;&#37326;&#65311;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative Artificial Intelligence (AI) tools that can generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have put the societal impacts of these technologies at the center of public debate. These tools are possible due to the massive amount of data (text and images) that is publicly available through the Internet. At the same time, these generative AI tools become content creators that are already contributing to the data that is available to train future models. Therefore, future versions of generative AI tools will be trained with a mix of human-created and AI-generated content, causing a potential feedback loop between generative AI and public data repositories. This interaction raises many questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI generated data? Will they evolve and improve with the new data sets or on the contrary will they degrade? Will evolution introduce biases or red
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#26234;&#33021;&#25163;&#34920;&#21644;&#36830;&#25509;&#30340;&#26234;&#33021;&#25163;&#26426;&#26469;&#26368;&#22823;&#21270;&#24515;&#29575;&#36319;&#36394;&#30340;&#24615;&#33021;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#24310;&#38271;&#30005;&#27744;&#23551;&#21629;&#65292;&#24182;&#22312;&#33258;&#23450;&#20041;&#30340;&#26234;&#33021;&#25163;&#34920;&#21407;&#22411;&#19978;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;8.5\%&#30340;HR&#36319;&#36394;&#31934;&#24230;&#21644;&#38477;&#20302;37\%&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.06129</link><description>&lt;p&gt;
&#22522;&#20110;PPG&#30340;&#24515;&#29575;&#20272;&#35745;&#30340;&#33021;&#37327;&#39640;&#25928;&#21487;&#31359;&#25140;&#24335;&#21040;&#31227;&#21160;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient Wearable-to-Mobile Offload of ML Inference for PPG-based Heart-Rate Estimation. (arXiv:2306.06129v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#26234;&#33021;&#25163;&#34920;&#21644;&#36830;&#25509;&#30340;&#26234;&#33021;&#25163;&#26426;&#26469;&#26368;&#22823;&#21270;&#24515;&#29575;&#36319;&#36394;&#30340;&#24615;&#33021;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#24310;&#38271;&#30005;&#27744;&#23551;&#21629;&#65292;&#24182;&#22312;&#33258;&#23450;&#20041;&#30340;&#26234;&#33021;&#25163;&#34920;&#21407;&#22411;&#19978;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;8.5\%&#30340;HR&#36319;&#36394;&#31934;&#24230;&#21644;&#38477;&#20302;37\%&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26234;&#33021;&#25163;&#34920;&#36890;&#24120;&#21253;&#25324;&#20809;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20256;&#24863;&#22120;&#65292;&#36890;&#36807;&#23558;PPG&#25968;&#25454;&#19982;&#20854;&#20182;&#20449;&#21495;&#34701;&#21512;&#30340;&#22797;&#26434;&#31639;&#27861;&#26469;&#27979;&#37327;&#24515;&#36339;&#25110;&#34880;&#21387;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#26234;&#33021;&#25163;&#34920;&#21644;&#36830;&#25509;&#30340;&#26234;&#33021;&#25163;&#26426;&#26469;&#26368;&#22823;&#21270;&#24515;&#29575;&#65288;HR&#65289;&#36319;&#36394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#24310;&#38271;&#26234;&#33021;&#25163;&#34920;&#30340;&#30005;&#27744;&#23551;&#21629;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;HR&#36319;&#36394;&#25110;&#23558;&#24037;&#20316;&#21368;&#36733;&#21040;&#31227;&#21160;&#35774;&#22791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#39069;&#22806;&#30340;&#27493;&#39588;&#35780;&#20272;&#21363;&#23558;&#21040;&#26469;&#30340;HR&#39044;&#27979;&#30340;&#38590;&#24230;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#32874;&#26126;&#22320;&#31649;&#29702;&#26234;&#33021;&#25163;&#34920;&#21644;&#26234;&#33021;&#25163;&#26426;&#20043;&#38388;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#20445;&#25345;&#20302;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#21516;&#26102;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#12290;&#25105;&#20204;&#22312;&#33258;&#23450;&#20041;&#30340;&#26234;&#33021;&#25163;&#34920;&#21407;&#22411;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;STM32WB55 MCU&#21644;&#20302;&#21151;&#32791;&#34013;&#29273;&#65288;BLE&#65289;&#36890;&#20449;&#65292;&#20197;&#21450;&#26641;&#33683;&#27966;Pi3&#20316;&#20026;&#26234;&#33021;&#25163;&#26426;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#21327;&#20316;&#24515;&#29575;&#20272;&#35745;&#65288;CHRE&#65289;&#26694;&#26550;&#23558;HR&#36319;&#36394;&#31934;&#24230;&#25552;&#39640;&#20102;8.5&#65285;&#65292;&#21516;&#26102;&#23558;&#26234;&#33021;&#25163;&#34920;&#30340;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#20102;37&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern smartwatches often include photoplethysmographic (PPG) sensors to measure heartbeats or blood pressure through complex algorithms that fuse PPG data with other signals. In this work, we propose a collaborative inference approach that uses both a smartwatch and a connected smartphone to maximize the performance of heart rate (HR) tracking while also maximizing the smartwatch's battery life. In particular, we first analyze the trade-offs between running on-device HR tracking or offloading the work to the mobile. Then, thanks to an additional step to evaluate the difficulty of the upcoming HR prediction, we demonstrate that we can smartly manage the workload between smartwatch and smartphone, maintaining a low mean absolute error (MAE) while reducing energy consumption. We benchmark our approach on a custom smartwatch prototype, including the STM32WB55 MCU and Bluetooth Low-Energy (BLE) communication, and a Raspberry Pi3 as a proxy for the smartphone. With our Collaborative Heart R
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#26426;&#21046;&#30340;&#26032;&#22411;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#20256;&#24863;&#22120;&#35760;&#24405;&#20013;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#30446;&#26631;&#36319;&#36394;&#21644;&#36895;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#35760;&#24518;&#29366;&#24577;&#36827;&#34892;&#25237;&#24433;&#12290;</title><link>http://arxiv.org/abs/2306.06126</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#36319;&#36394;&#12289;&#36895;&#24230;&#20272;&#35745;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26102;&#38388;&#25237;&#24433;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Method for Object Tracking, Velocity Estimation and Projection of Sensor Data over Time. (arXiv:2306.06126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#26426;&#21046;&#30340;&#26032;&#22411;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#20256;&#24863;&#22120;&#35760;&#24405;&#20013;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#30446;&#26631;&#36319;&#36394;&#21644;&#36895;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#35760;&#24518;&#29366;&#24577;&#36827;&#34892;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29615;&#22659;&#20998;&#21106;&#21644;&#36895;&#24230;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21033;&#29992;&#25152;&#33719;&#21462;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#23558;&#26032;&#36755;&#20837;&#21644;&#35760;&#24518;&#25968;&#25454;&#30456;&#20851;&#32852;&#26469;&#38544;&#24335;&#22320;&#25512;&#23548;&#22330;&#26223;&#21160;&#24577;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#21457;&#29616;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23384;&#22312;&#26550;&#26500;&#38480;&#21046;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#21033;&#29992;Transformer&#26426;&#21046;&#30340;&#26032;&#22411;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#26469;&#35299;&#20915;&#21033;&#29992;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#35760;&#24405;&#20013;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#25152;&#38754;&#20020;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#22312;&#35813;&#21333;&#20803;&#20013;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#20256;&#24863;&#22120;&#36755;&#20837;&#21644;&#35760;&#24518;&#29366;&#24577;&#20998;&#21035;&#23548;&#20986;&#30340;&#20851;&#38190;-&#26597;&#35810;&#23545;&#30456;&#20851;&#32852;&#65292;&#36319;&#36394;&#23545;&#35937;&#32534;&#30721;&#22312;&#36830;&#32493;&#24103;&#19978;&#30340;&#20301;&#32622;&#12290;&#28982;&#21518;&#21033;&#29992;&#24471;&#21040;&#30340;&#36319;&#36394;&#27169;&#24335;&#26469;&#33719;&#21462;&#22330;&#26223;&#21160;&#24577;&#21644;&#22238;&#24402;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#25552;&#21462;&#30340;&#36895;&#24230;&#20272;&#35745;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#29366;&#24577;&#36827;&#34892;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Deep Learning methods for environment segmentation and velocity estimation rely on Convolutional Recurrent Neural Networks to exploit spatio-temporal relationships within obtained sensor data. These approaches derive scene dynamics implicitly by correlating novel input and memorized data utilizing ConvNets. We show how ConvNets suffer from architectural restrictions for this task. Based on these findings, we then provide solutions to various issues on exploiting spatio-temporal correlations in a sequence of sensor recordings by presenting a novel Recurrent Neural Network unit utilizing Transformer mechanisms. Within this unit, object encodings are tracked across consecutive frames by correlating key-query pairs derived from sensor inputs and memory states, respectively. We then use resulting tracking patterns to obtain scene dynamics and regress velocities. In a last step, the memory state of the Recurrent Neural Network is projected based on extracted velocity estimates to res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#25513;&#30721;Token Transformer&#30340;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#26694;&#26550;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06125</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;Token Transformer&#30340;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Joint Channel Estimation and Feedback with Masked Token Transformers in Massive MIMO Systems. (arXiv:2306.06125v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#25513;&#30721;Token Transformer&#30340;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#26694;&#26550;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22522;&#31449;&#20855;&#26377;&#19979;&#34892;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26102;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#22312;&#39057;&#20998;&#21452;&#24037;&#65288;FDD&#65289;&#27169;&#24335;&#19979;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#32479;&#36890;&#36947;&#35774;&#35745;&#32780;&#38750;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#20004;&#20010;&#32593;&#32476;&#12290;&#31532;&#19968;&#20010;&#32593;&#32476;&#26159;&#20449;&#36947;&#20272;&#35745;&#32593;&#32476;&#65292;&#37319;&#29992;&#21452;&#37325;&#25439;&#22833;&#35774;&#35745;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#23436;&#25972;&#30340;&#20449;&#36947;&#20449;&#24687;&#24182;&#28040;&#38500;&#20449;&#36947;&#22122;&#22768;&#12290;&#31532;&#20108;&#20010;&#32593;&#32476;&#26159;&#21387;&#32553;&#21644;&#21453;&#39304;&#32593;&#32476;&#12290;&#21463;&#25513;&#30721;Token Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#25513;&#30721;Token&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20986;&#33394;&#30340;&#20272;&#35745;&#21644;&#21387;&#32553;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#20223;&#30495;&#32467;&#26524;&#21644;&#21066;&#24369;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the base station has downlink channel status information (CSI), the huge potential of large-scale multiple input multiple output (MIMO) in frequency division duplex (FDD) mode can be fully exploited. In this paper, we propose a deep-learning-based joint channel estimation and feedback framework to realize channel estimation and feedback in massive MIMO systems. Specifically, we use traditional channel design rather than end-to-end methods. Our model contains two networks. The first network is a channel estimation network, which adopts a double loss design, and can accurately estimate the full channel information while removing channel noises. The second network is a compression and feedback network. Inspired by the masked token transformer, we propose a learnable mask token method to obtain excellent estimation and compression performance. The extensive simulation results and ablation studies show that our method outperforms state-of-the-art channel estimation and feedback methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#21644;K-means&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#23558;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#24178;&#25200;&#27874;&#24418;&#32858;&#31867;&#20026;&#28041;&#21450;&#30005;&#21387;&#19979;&#38477;&#12289;&#20013;&#26029;&#12289;&#30636;&#24577;&#12289;&#27491;&#24120;&#21644;&#35856;&#27874;&#30072;&#21464;&#31561;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#24322;&#24120;&#27874;&#24418;&#21644;&#24120;&#35268;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.06124</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised clustering of disturbances in power systems via deep convolutional autoencoders. (arXiv:2306.06124v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#21644;K-means&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#23558;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#24178;&#25200;&#27874;&#24418;&#32858;&#31867;&#20026;&#28041;&#21450;&#30005;&#21387;&#19979;&#38477;&#12289;&#20013;&#26029;&#12289;&#30636;&#24577;&#12289;&#27491;&#24120;&#21644;&#35856;&#27874;&#30072;&#21464;&#31561;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#24322;&#24120;&#27874;&#24418;&#21644;&#24120;&#35268;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#30005;&#32593;&#26816;&#27979;&#21040;&#24322;&#24120;&#20107;&#20214;&#26102;&#65292;&#30005;&#21147;&#36136;&#37327;(PQ)&#20202;&#22120;&#20250;&#35760;&#24405;PQ&#20107;&#20214;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#20998;&#31867;&#35760;&#24405;&#30340;&#27874;&#24418;&#24182;&#24110;&#21161;&#30005;&#21147;&#31995;&#32479;&#24037;&#31243;&#24072;&#35786;&#26029;&#21644;&#32416;&#27491;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#22312;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#26399;&#38388;&#25429;&#33719;&#30340;&#35768;&#22810;&#27874;&#24418;&#38656;&#35201;&#36827;&#34892;&#26631;&#35760;&#20197;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#24037;&#31243;&#24072;&#38656;&#35201;&#25163;&#21160;&#22788;&#29702;&#25110;&#26410;&#30475;&#21040;&#22823;&#37327;&#25968;&#25454;&#35760;&#24405;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#21644;K-means&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#25216;&#26415;&#65292;&#21487;&#23558;PQ&#20107;&#20214;&#32858;&#31867;&#20026;&#28041;&#21450;&#30005;&#21387;&#19979;&#38477;&#12289;&#20013;&#26029;&#12289;&#30636;&#24577;&#12289;&#27491;&#24120;&#21644;&#35856;&#27874;&#30072;&#21464;&#31561;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#24322;&#24120;&#27874;&#24418;&#21644;&#24120;&#35268;&#27874;&#24418;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22312;&#20998;&#24067;&#32593;&#26684;&#20013;&#35760;&#24405;&#30340;&#19977;&#30456;&#22330;&#33719;&#24471;&#30340;&#30005;&#21387;&#27874;&#24418;&#36827;&#34892;&#28436;&#31034;&#12290;&#39318;&#20808;&#65292;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#23558;&#36755;&#20837;&#20449;&#21495;&#21387;&#32553;&#20026;&#19968;&#32452;&#36739;&#20302;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Power quality (PQ) events are recorded by PQ meters whenever anomalous events are detected on the power grid. Using neural networks with machine learning can aid in accurately classifying the recorded waveforms and help power system engineers diagnose and rectify the root causes of problems. However, many of the waveforms captured during a disturbance in the power system need to be labeled for supervised learning, leaving a large number of data recordings for engineers to process manually or go unseen. This paper presents an autoencoder and K-means clustering-based unsupervised technique that can be used to cluster PQ events into categories like sag, interruption, transients, normal, and harmonic distortion to enable filtering of anomalous waveforms from recurring or normal waveforms. The method is demonstrated using three-phase, field-obtained voltage waveforms recorded in a distribution grid. First, a convolutional autoencoder compresses the input signals into a set of lower feature 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06123</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#35843;&#26597;&#25253;&#21578;&#12299;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#34987;&#25551;&#32472;&#20026;&#35843;&#35797;&#21644;&#20449;&#20219;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27835;&#30103;&#26041;&#24335;&#65292;&#20197;&#21450;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#20986;&#20102;&#26368;&#26032;&#35299;&#37322;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#36825;&#20123;&#36827;&#23637;&#20196;&#20154;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#20135;&#29983;&#36136;&#30097;&#12290;&#25805;&#32437;&#12289;&#27450;&#39575;&#25110;&#27927;&#30333;&#27169;&#22411;&#25512;&#29702;&#35777;&#25454;&#30340;&#21487;&#33021;&#24615;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#21644;&#30693;&#35782;&#21457;&#29616;&#20013;&#20135;&#29983;&#19981;&#21033;&#21518;&#26524;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;50&#22810;&#31687;&#35770;&#25991;&#30340;&#30740;&#31350;&#65292;&#27010;&#36848;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#20197;&#21450;&#20844;&#24179;&#24230;&#37327;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#38450;&#24481;&#25915;&#20987;&#24182;&#35774;&#35745;&#40065;&#26834;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#21015;&#20986;XAI&#20013;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#27010;&#36848;&#20102;&#23545;&#25239;&#24615;XAI&#65288;AdvXAI&#65289;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20004;&#20010;&#21452;&#37325;&#38543;&#26426;&#33258;&#27880;&#24847;&#26144;&#23556;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#36981;&#24490;&#21270;&#23398;&#21453;&#24212;&#29289;&#29702;&#32422;&#26463;&#30340;&#30005;&#23376;&#20877;&#20998;&#37197;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.06119</link><description>&lt;p&gt;
&#21452;&#37325;&#38543;&#26426;&#22270;&#32593;&#32476;&#38750;&#33258;&#22238;&#24402;&#21453;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Doubly Stochastic Graph-based Non-autoregressive Reaction Prediction. (arXiv:2306.06119v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20004;&#20010;&#21452;&#37325;&#38543;&#26426;&#33258;&#27880;&#24847;&#26144;&#23556;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#36981;&#24490;&#21270;&#23398;&#21453;&#24212;&#29289;&#29702;&#32422;&#26463;&#30340;&#30005;&#23376;&#20877;&#20998;&#37197;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26426;&#21453;&#24212;&#39044;&#27979;&#26159;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24314;&#27169;&#30005;&#23376;&#20877;&#20998;&#37197;&#23454;&#29616;&#20102;&#38750;&#33258;&#22238;&#24402;&#21453;&#24212;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;top-1&#31934;&#24230;&#24182;&#23454;&#29616;&#20102;&#24182;&#34892;&#25277;&#26679;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#30005;&#23376;&#20998;&#37197;&#35268;&#21017;&#21644;&#23545;&#31216;&#35268;&#21017;&#36825;&#20004;&#20010;&#22522;&#26412;&#35268;&#21017;&#12290;&#36825;&#31181;&#21270;&#23398;&#21453;&#24212;&#29289;&#29702;&#32422;&#26463;&#30340;&#36829;&#21453;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20004;&#20010;&#21452;&#37325;&#38543;&#26426;&#33258;&#27880;&#24847;&#26144;&#23556;&#32467;&#21512;&#36215;&#26469;&#65292;&#24471;&#21040;&#36981;&#24490;&#36825;&#20004;&#20010;&#32422;&#26463;&#30340;&#30005;&#23376;&#20877;&#20998;&#37197;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#25193;&#23637;&#21040;&#19968;&#20010;&#20855;&#26377;&#22686;&#24378;&#32422;&#26463;&#30340;&#36890;&#29992;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;Sinkhorn&#31639;&#27861;&#26469;&#36845;&#20195;&#26356;&#26032;&#33258;&#25105;&#20851;&#27880;&#26144;&#23556;&#65292;&#36827;&#32780;&#24341;&#20837;&#20102;&#21452;&#37325;&#20445;&#23432;&#32422;&#26463;&#20316;&#20026;&#39069;&#22806;&#30340;&#20449;&#24687;&#20248;&#20808;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organic reaction prediction is a critical task in drug discovery. Recently, researchers have achieved non-autoregressive reaction prediction by modeling the redistribution of electrons, resulting in state-of-the-art top-1 accuracy, and enabling parallel sampling. However, the current non-autoregressive decoder does not satisfy two essential rules of electron redistribution modeling simultaneously: the electron-counting rule and the symmetry rule. This violation of the physical constraints of chemical reactions impairs model performance. In this work, we propose a new framework called that combines two doubly stochastic self-attention mappings to obtain electron redistribution predictions that follow both constraints. We further extend our solution to a general multi-head attention mechanism with augmented constraints. To achieve this, we apply Sinkhorn's algorithm to iteratively update self-attention mappings, which imposes doubly conservative constraints as additional informative prio
&lt;/p&gt;</description></item><item><title>&#26080;&#20154;&#26426;&#25668;&#24433;&#27979;&#37327;&#25216;&#26415;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#27979;&#23450;&#27827;&#27969;&#27700;&#38754;&#39640;&#31243;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20272;&#31639;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06118</link><description>&lt;p&gt;
&#21033;&#29992;&#26080;&#20154;&#26426;&#25668;&#24433;&#27979;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#20272;&#31639;&#27827;&#27969;&#27700;&#38754;&#39640;&#31243;
&lt;/p&gt;
&lt;p&gt;
Estimation of River Water Surface Elevation Using UAV Photogrammetry and Machine Learning. (arXiv:2306.06118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06118
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#25668;&#24433;&#27979;&#37327;&#25216;&#26415;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#27979;&#23450;&#27827;&#27969;&#27700;&#38754;&#39640;&#31243;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20272;&#31639;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#25668;&#24433;&#27979;&#37327;&#25216;&#26415;&#21487;&#20197;&#21046;&#20316;&#22320;&#24418;&#27491;&#23556;&#24433;&#20687;&#21644;&#25968;&#23383;&#22320;&#34920;&#27169;&#22411;&#65288;DSM&#65289;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#31181;&#25216;&#26415;&#21046;&#20316;&#30340;&#27700;&#20307;DSM&#20250;&#26174;&#31034;&#27700;&#38754;&#25197;&#26354;&#65292;&#20174;&#32780;&#26080;&#27861;&#20934;&#30830;&#27979;&#23450;&#27700;&#38754;&#39640;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#25668;&#24433;&#27979;&#37327;DSM&#21644;&#27491;&#23556;&#24433;&#20687;&#20013;&#29992;&#20316;&#27700;&#38754;&#39640;&#31243;&#20272;&#31639;&#22120;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#8220;&#27700;&#36793;&#32536;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21069;&#21518;&#25351;&#25968;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#28388;&#38500;&#20102;&#24322;&#24120;&#20540;&#12290;&#37319;&#29992;&#38142;&#30721;&#36827;&#34892;&#27700;&#38754;&#39640;&#31243;&#20540;&#30340;&#32447;&#24615;&#22238;&#24402;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#20272;&#31639;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#27492;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#22312;&#20908;&#23395;&#21644;&#22799;&#23395;&#26465;&#20214;&#19979;&#21019;&#24314;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22312;&#19968;&#26465;&#23567;&#30340;&#20302;&#22320;&#27827;&#27969;&#19978;&#25910;&#38598;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned aerial vehicle (UAV) photogrammetry allows for the creation of orthophotos and digital surface models (DSMs) of a terrain. However, DSMs of water bodies mapped with this technique reveal water surface distortions, preventing the use of photogrammetric data for accurate determination of water surface elevation (WSE). Firstly, we propose a new solution in which a convolutional neural network (CNN) is used as a WSE estimator from photogrammetric DSMs and orthophotos. Second, we improved the previously known "water-edge" method by filtering the outliers using a forward-backwards exponential weighted moving average. Further improvement in these two methods was achieved by performing a linear regression of the WSE values against chainage. The solutions estimate the uncertainty of the predictions. This is the first approach in which DL was used for this task. A brand new machine learning data set has been created. It was collected on a small lowland river in winter and summer conditi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;3D&#23039;&#21183;&#20272;&#35745;&#21644;&#24815;&#24615;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#22312;&#29305;&#23450;&#36816;&#21160;&#38203;&#28860;&#20013;&#30340;&#31934;&#24230;&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#29992;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.06117</link><description>&lt;p&gt;
&#36816;&#21160;&#27835;&#30103;&#20013;3D&#23039;&#21183;&#20272;&#35745;&#21644;&#24815;&#24615;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;
&lt;/p&gt;
&lt;p&gt;
Strengths and Weaknesses of 3D Pose Estimation and Inertial Motion Capture System for Movement Therapy. (arXiv:2306.06117v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;3D&#23039;&#21183;&#20272;&#35745;&#21644;&#24815;&#24615;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#22312;&#29305;&#23450;&#36816;&#21160;&#38203;&#28860;&#20013;&#30340;&#31934;&#24230;&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#23039;&#21183;&#20272;&#35745;&#20026;&#24555;&#36895;&#12289;&#38750;&#20405;&#20837;&#24615;&#19988;&#20934;&#30830;&#30340;&#36816;&#21160;&#20998;&#26512;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#36825;&#23545;&#20110;&#20020;&#24202;&#24212;&#29992;&#20063;&#20855;&#26377;&#29305;&#27530;&#30340;&#20852;&#36259;&#12290;&#30446;&#21069;&#65292;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#34987;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#24378;&#22823;&#32780;&#31934;&#30830;&#30340;&#25968;&#25454;&#33719;&#21462;&#65292;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;3D&#20301;&#32622;&#20272;&#35745;&#26041;&#27861;MeTrabs&#19982;&#24050;&#24314;&#31435;&#30340;&#24815;&#24615;&#20256;&#24863;&#22120;&#31995;&#32479;MTw Awinda&#22312;&#29305;&#23450;&#36816;&#21160;&#38203;&#28860;&#20013;&#30340;&#31934;&#24230;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#24182;&#25552;&#20379;&#20102;10&#21517;&#21463;&#35797;&#32773;&#22312;&#21508;&#31181;&#36816;&#21160;&#27835;&#30103;&#38203;&#28860;&#26399;&#38388;&#30340;&#24179;&#34892;&#35760;&#24405;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;Awinda&#31995;&#32479;&#30340;&#20449;&#24687;&#21644;&#21333;&#30524;&#23039;&#24577;&#20272;&#35745;&#30340;&#24103;&#34987;&#21516;&#27493;&#12290;&#20026;&#20102;&#36827;&#34892;&#27604;&#36739;&#65292;&#23545;&#36381;&#20851;&#33410;&#12289;&#33181;&#20851;&#33410;&#12289;&#32972;&#37096;&#21644;&#32920;&#37096;&#30340;&#20851;&#33410;&#35282;&#24230;&#36827;&#34892;&#20102;&#20020;&#24202;&#30456;&#20851;&#21442;&#25968;&#30340;&#20272;&#35745;&#21644;&#35780;&#20272;&#65292;&#20351;&#29992;&#24179;&#22343;&#20540;&#12289;&#20013;&#20301;&#25968;&#21644;&#35745;&#31639;&#20986;&#30340;&#19981;&#21516;&#38203;&#28860;&#20013;&#20851;&#33410;&#35282;&#24230;&#20043;&#38388;&#30340;&#26368;&#22823;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D pose estimation offers the opportunity for fast, non-invasive, and accurate motion analysis. This is of special interest also for clinical use. Currently, motion capture systems are used, as they offer robust and precise data acquisition, which is essential in the case of clinical applications. In this study, we investigate the accuracy of the state-of-the-art 3D position estimation approach MeTrabs, compared to the established inertial sensor system MTw Awinda for specific motion exercises. The study uses and provides an evaluation dataset of parallel recordings from 10 subjects during various movement therapy exercises. The information from the Awinda system and the frames for monocular pose estimation are synchronized. For the comparison, clinically relevant parameters for joint angles of ankle, knee, back, and elbow flexion-extension were estimated and evaluated using mean, median, and maximum deviation between the calculated joint angles for the different exercises, camera posi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#36817;&#26399;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#19982;&#35780;&#20272;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#35774;&#35745;&#29305;&#28857;&#12289;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.06116</link><description>&lt;p&gt;
&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of Deep Learning Methods for Retinal Vessel Segmentation. (arXiv:2306.06116v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36817;&#26399;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#19982;&#35780;&#20272;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#35774;&#35745;&#29305;&#28857;&#12289;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#22312;&#35768;&#22810;&#30524;&#37096;&#21644;&#20840;&#36523;&#24615;&#30142;&#30149;&#30340;&#27835;&#30103;&#21644;&#35786;&#26029;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#26041;&#27861;&#34987;&#23454;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26469;&#33258;&#39640;&#24433;&#21709;&#26399;&#21002;&#21644;&#20250;&#35758;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#31616;&#35201;&#22238;&#39038;&#12290;&#22238;&#39038;&#30340;&#30446;&#26631;&#26159;&#65306;(1)&#35780;&#20272;&#26368;&#26032;&#26041;&#27861;&#30340;&#35774;&#35745;&#29305;&#28857;&#65292;(2)&#25253;&#21578;&#21644;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#30340;&#23450;&#37327;&#20540;&#65292;&#20197;&#21450;(3)&#20998;&#26512;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for automated retinal vessel segmentation play an important role in the treatment and diagnosis of many eye and systemic diseases. With the fast development of deep learning methods, more and more retinal vessel segmentation methods are implemented as deep neural networks. In this paper, we provide a brief review of recent deep learning methods from highly influential journals and conferences. The review objectives are: (1) to assess the design characteristics of the latest methods, (2) to report and analyze quantitative values of performance evaluation metrics, and (3) to analyze the advantages and disadvantages of the recent solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#32500;&#19977;&#32500;&#24418;&#29366;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#24314;&#27169;&#27773;&#36710;&#38459;&#21147;&#31995;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#34920;&#31034;&#26041;&#27861;&#22312;&#38459;&#21147;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06110</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21644;&#27861;&#21521;&#28210;&#26579;&#30340;&#27773;&#36710;&#38459;&#21147;&#31995;&#25968;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Surrogate Modeling of Car Drag Coefficient with Depth and Normal Renderings. (arXiv:2306.06110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#32500;&#19977;&#32500;&#24418;&#29366;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#24314;&#27169;&#27773;&#36710;&#38459;&#21147;&#31995;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#34920;&#31034;&#26041;&#27861;&#22312;&#38459;&#21147;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#21019;&#24314;&#19977;&#32500;&#24418;&#29366;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#26377;&#21487;&#33021;&#25913;&#21464;&#27773;&#36710;&#35774;&#35745;&#12290;&#22312;&#24037;&#31243;&#35774;&#35745;&#21644;&#20248;&#21270;&#20013;&#65292;&#35780;&#20272;&#24037;&#31243;&#25351;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20351;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#24615;&#33021;&#24847;&#35782;&#24182;&#20351;&#20854;&#33021;&#22815;&#21019;&#24314;&#39640;&#24615;&#33021;&#35774;&#35745;&#65292;&#36825;&#20123;&#25351;&#26631;&#30340;&#20195;&#29702;&#24314;&#27169;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#19977;&#32500;&#24418;&#29366;&#34920;&#31034;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#23398;&#20064;&#65292;&#35201;&#20040;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#20195;&#29702;&#24314;&#27169;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#32500;&#19977;&#32500;&#24418;&#29366;&#34920;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#20195;&#29702;&#38459;&#21147;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#23427;&#22312;&#39044;&#27979;&#19977;&#32500;&#27773;&#36710;&#38459;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;9,070&#20010;&#39640;&#36136;&#37327;&#19977;&#32500;&#27773;&#36710;&#32593;&#26684;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#29992;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#27169;&#25311;&#35745;&#31639;&#30340;&#38459;&#21147;&#31995;&#25968;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20108;&#32500;&#34920;&#31034;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#19977;&#32500;&#34920;&#31034;&#26356;&#20855;&#34920;&#29616;&#21147;&#65292;&#22312;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#26102;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#38459;&#21147;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have made significant progress in automating the creation of 3D shapes, which has the potential to transform car design. In engineering design and optimization, evaluating engineering metrics is crucial. To make generative models performance-aware and enable them to create high-performing designs, surrogate modeling of these metrics is necessary. However, the currently used representations of three-dimensional (3D) shapes either require extensive computational resources to learn or suffer from significant information loss, which impairs their effectiveness in surrogate modeling. To address this issue, we propose a new two-dimensional (2D) representation of 3D shapes. We develop a surrogate drag model based on this representation to verify its effectiveness in predicting 3D car drag. We construct a diverse dataset of 9,070 high-quality 3D car meshes labeled by drag coefficients computed from computational fluid dynamics (CFD) simulations to train our model. Our expe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28431;&#27934;&#21305;&#37197;&#26041;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#28431;&#27934;&#20195;&#30721;&#24211;&#20013;&#30340;&#28431;&#27934;&#27169;&#24335;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#35821;&#21477;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35782;&#21035;&#28431;&#27934;&#33539;&#22260;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06109</link><description>&lt;p&gt;
&#23398;&#20064;&#37327;&#21270;&#28431;&#27934;&#27169;&#24335;&#24182;&#21305;&#37197;&#20197;&#23450;&#20301;&#35821;&#21477;&#32423;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Learning to Quantize Vulnerability Patterns and Match to Locate Statement-Level Vulnerabilities. (arXiv:2306.06109v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06109
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28431;&#27934;&#21305;&#37197;&#26041;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#28431;&#27934;&#20195;&#30721;&#24211;&#20013;&#30340;&#28431;&#27934;&#27169;&#24335;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#35821;&#21477;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35782;&#21035;&#28431;&#27934;&#33539;&#22260;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35782;&#21035;&#36719;&#20214;&#28431;&#27934;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#21516;&#26131;&#21463;&#25915;&#20987;&#30340;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#21487;&#33021;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#26131;&#21463;&#25915;&#20987;&#33539;&#22260;&#65292;&#36825;&#20123;&#33539;&#22260;&#24418;&#25104;&#21487;&#36890;&#36807;&#30417;&#30563;&#35757;&#32451;&#20026;DL&#27169;&#22411;&#25152;&#23398;&#30340;&#21487;&#36776;&#35748;&#30340;&#28431;&#27934;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#28431;&#27934;&#30340;&#26131;&#25915;&#20987;&#33539;&#22260;&#20173;&#20197;&#19981;&#21516;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#26684;&#24335;&#22312;&#31243;&#24207;&#20013;&#34920;&#29616;&#65292;&#36825;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#35821;&#21477;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#20173;&#26410;&#21033;&#29992;&#26131;&#21463;&#25915;&#20987;&#31243;&#24207;&#20013;&#20986;&#29616;&#30340;&#28431;&#27934;&#27169;&#24335;&#12290;&#20026;&#20805;&#20998;&#21033;&#29992;&#26131;&#21463;&#25915;&#20987;&#27169;&#24335;&#24182;&#37322;&#25918;DL&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28431;&#27934;&#21305;&#37197;&#26041;&#27861;&#65292;&#21463;&#31243;&#24207;&#20998;&#26512;&#24037;&#20855;&#30340;&#21551;&#21457;&#65292;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#26469;&#23450;&#20301;&#28431;&#27934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20221;&#28431;&#27934;&#20195;&#30721;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#37327;&#21270;&#28431;&#27934;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models have become increasingly popular in identifying software vulnerabilities. Prior studies found that vulnerabilities across different vulnerable programs may exhibit similar vulnerable scopes, implicitly forming discernible vulnerability patterns that can be learned by DL models through supervised training. However, vulnerable scopes still manifest in various spatial locations and formats within a program, posing challenges for models to accurately identify vulnerable statements. Despite this challenge, state-of-the-art vulnerability detection approaches fail to exploit the vulnerability patterns that arise in vulnerable programs. To take full advantage of vulnerability patterns and unleash the ability of DL models, we propose a novel vulnerability-matching approach in this paper, drawing inspiration from program analysis tools that locate vulnerabilities based on pre-defined patterns. Specifically, a vulnerability codebook is learned, which consists of quantize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Elliptic++&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#27604;&#29305;&#24065;&#32593;&#32476;&#20013;&#30340;&#20132;&#26131;&#21644;&#22320;&#22336;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#20197;&#32508;&#21512;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#26041;&#27861;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#65292;&#20174;&#32780;&#26597;&#20986;&#27450;&#35784;&#20132;&#26131;&#21644;&#38750;&#27861;&#22320;&#22336;&#12290;</title><link>http://arxiv.org/abs/2306.06108</link><description>&lt;p&gt;
&#25581;&#31192;&#27604;&#29305;&#24065;&#32593;&#32476;&#20013;&#30340;&#27450;&#35784;&#20132;&#26131;&#21644;&#38750;&#27861;&#33410;&#28857;&#65306;&#37329;&#34701;&#21462;&#35777;&#30340;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Demystifying Fraudulent Transactions and Illicit Nodes in the Bitcoin Network for Financial Forensics. (arXiv:2306.06108v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Elliptic++&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#27604;&#29305;&#24065;&#32593;&#32476;&#20013;&#30340;&#20132;&#26131;&#21644;&#22320;&#22336;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#20197;&#32508;&#21512;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#26041;&#27861;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#65292;&#20174;&#32780;&#26597;&#20986;&#27450;&#35784;&#20132;&#26131;&#21644;&#38750;&#27861;&#22320;&#22336;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#36890;&#36807;&#25366;&#25496;&#20854;&#24320;&#25918;&#19988;&#19981;&#21487;&#21464;&#30340;&#20132;&#26131;&#25968;&#25454;&#20026;&#37329;&#34701;&#21462;&#35777;&#25552;&#20379;&#20102;&#29420;&#29305;&#19988;&#21487;&#36861;&#28335;&#30340;&#28192;&#36947;&#12290;&#36817;&#26399;&#65292;&#20351;&#29992;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#22914;&#27927;&#38065;&#21644;&#20854;&#20182;&#27450;&#35784;&#27963;&#21160;&#65292;&#24050;&#25104;&#20026;&#20027;&#27969;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#29305;&#24065;&#32593;&#32476;&#20013;&#30340;&#27450;&#35784;&#26816;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#29420;&#21019;&#24615;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Elliptic++&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25193;&#23637;&#20102;Elliptic&#20132;&#26131;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;822k&#20010;&#27604;&#29305;&#24065;&#38065;&#21253;&#22320;&#22336;&#65288;&#33410;&#28857;&#65289;&#65292;&#27599;&#20010;&#22320;&#22336;&#26377;56&#20010;&#29305;&#24449;&#21644;1.27M&#20010;&#26102;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#22235;&#31181;&#31867;&#22411;&#30340;&#22270;&#25968;&#25454;&#65306;&#65288;i&#65289;&#20132;&#26131;&#20043;&#38388;&#30340;&#22270;&#65292;&#20195;&#34920;&#27604;&#29305;&#24065;&#32593;&#32476;&#20013;&#30340;&#36164;&#37329;&#27969;&#21160;&#65292;&#65288;ii&#65289;&#22320;&#22336;&#20043;&#38388;&#30340;&#20132;&#20114;&#22270;&#65292;&#25429;&#25417;&#20132;&#26131;&#31867;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#27450;&#35784;&#20132;&#26131;&#21644;&#38750;&#27861;&#22320;&#22336;&#65288;&#21442;&#19982;&#32773;&#65289;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain provides the unique and accountable channel for financial forensics by mining its open and immutable transaction data. A recent surge has been witnessed by training machine learning models with cryptocurrency transaction data for anomaly detection, such as money laundering and other fraudulent activities. This paper presents a holistic applied data science approach to fraud detection in the Bitcoin network with two original contributions. First, we contribute the Elliptic++ dataset, which extends the Elliptic transaction dataset to include over 822k Bitcoin wallet addresses (nodes), each with 56 features, and 1.27M temporal interactions. This enables both the detection of fraudulent transactions and the detection of illicit addresses (actors) in the Bitcoin network by leveraging four types of graph data: (i) the transaction-to-transaction graph, representing the money flow in the Bitcoin network, (ii) the address-to-address interaction graph, capturing the types of transacti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27700;&#37197;&#31649;&#32593;&#32476;&#27844;&#28431;&#25506;&#27979;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20998;&#31867;&#27861;&#12290;&#37325;&#28857;&#30740;&#31350;&#25915;&#20987;&#32773;&#22312;&#27700;&#32593;&#20013;&#23547;&#25214;&#26368;&#19981;&#25935;&#24863;&#28857;&#65292;&#20351;&#29992;&#19977;&#31181;&#31639;&#27861;&#26041;&#27861;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#20004;&#20010;&#22522;&#20934;&#27700;&#37197;&#31649;&#32593;&#32476;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.06107</link><description>&lt;p&gt;
&#27700;&#37197;&#31649;&#32593;&#32476;&#27844;&#28431;&#25506;&#27979;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Leakage Detectors in Water Distribution Networks. (arXiv:2306.06107v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27700;&#37197;&#31649;&#32593;&#32476;&#27844;&#28431;&#25506;&#27979;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20998;&#31867;&#27861;&#12290;&#37325;&#28857;&#30740;&#31350;&#25915;&#20987;&#32773;&#22312;&#27700;&#32593;&#20013;&#23547;&#25214;&#26368;&#19981;&#25935;&#24863;&#28857;&#65292;&#20351;&#29992;&#19977;&#31181;&#31639;&#27861;&#26041;&#27861;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#20004;&#20010;&#22522;&#20934;&#27700;&#37197;&#31649;&#32593;&#32476;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#22312;&#19981;&#34987;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#21046;&#23450;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#23545;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#27604;&#22914;&#30417;&#27979;&#27700;&#37197;&#31649;&#32593;&#32476;&#65292;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#30340;&#26356;&#22909;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#20998;&#31867;&#27861;&#65292;&#38024;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#27700;&#37197;&#31649;&#32593;&#32476;&#27844;&#28431;&#25506;&#27979;&#22120;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#25915;&#20987;&#32773;&#23547;&#25214;&#26368;&#19981;&#25935;&#24863;&#28857;&#65292;&#20063;&#23601;&#26159;&#21487;&#33021;&#21457;&#29983;&#26368;&#22823;&#21487;&#33021;&#26410;&#34987;&#26816;&#27979;&#27844;&#28431;&#30340;&#27700;&#32593;&#20301;&#32622;&#12290;&#22522;&#20110;&#26368;&#19981;&#25935;&#24863;&#28857;&#38382;&#39064;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#26041;&#27861;&#26469;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#12290;&#32467;&#26524;&#22312;&#20004;&#20010;&#22522;&#20934;&#27700;&#37197;&#31649;&#32593;&#32476;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Machine Learning models are vulnerable to adversarial attacks: There exist methodologies that add a small (imperceptible) perturbation to an input such that the model comes up with a wrong prediction. Better understanding of such attacks is crucial in particular for models used in security-critical domains, such as monitoring of water distribution networks, in order to devise counter-measures enhancing model robustness and trustworthiness.  We propose a taxonomy for adversarial attacks against machine learning based leakage detectors in water distribution networks. Following up on this, we focus on a particular type of attack: an adversary searching the least sensitive point, that is, the location in the water network where the largest possible undetected leak could occur. Based on a mathematical formalization of the least sensitive point problem, we use three different algorithmic approaches to find a solution. Results are evaluated on two benchmark water distribution networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#30340;MetNet-3&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#23545;20&#23567;&#26102;&#20869;&#30340;&#22825;&#27668;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;MetNet-3&#30340;&#25216;&#26415;&#21019;&#26032;&#21253;&#25324;&#21487;&#23398;&#20064;&#21367;&#31215;&#12289;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25345;&#32493;&#24615;&#21551;&#21457;&#27861;&#26469;&#22806;&#25512;&#21021;&#22987;&#26465;&#20214;&#25110;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#35266;&#27979;&#25968;&#25454;&#26356;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06079</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#26085;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Day Forecasts from Sparse Observations. (arXiv:2306.06079v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#30340;MetNet-3&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#23545;20&#23567;&#26102;&#20869;&#30340;&#22825;&#27668;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;MetNet-3&#30340;&#25216;&#26415;&#21019;&#26032;&#21253;&#25324;&#21487;&#23398;&#20064;&#21367;&#31215;&#12289;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25345;&#32493;&#24615;&#21551;&#21457;&#27861;&#26469;&#22806;&#25512;&#21021;&#22987;&#26465;&#20214;&#25110;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#35266;&#27979;&#25968;&#25454;&#26356;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#24314;&#27169;&#22825;&#27668;&#26465;&#20214;&#30340;&#26367;&#20195;&#33539;&#20363;&#12290;&#31070;&#32463;&#27169;&#22411;&#22312;&#25968;&#25454;&#21487;&#29992;&#26102;&#20197;&#23569;&#20110;1&#31186;&#30340;&#36895;&#24230;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#20197;&#38750;&#24120;&#39640;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#36827;&#34892;&#39044;&#27979;&#12290;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#20174;&#22823;&#27668;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#36825;&#26159;&#36825;&#20123;&#27169;&#22411;&#29420;&#29305;&#30340;&#20248;&#21183;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20165;&#20165;&#39044;&#27979;&#38477;&#27700;&#36825;&#19968;&#21807;&#19968;&#21464;&#37327;&#26102;&#65292;&#20165;&#33021;&#20351;&#29992;&#22823;&#27668;&#35266;&#27979;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#25165;&#33021;&#36798;&#21040;&#19982;&#29616;&#26377;&#27010;&#29575;&#24615;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30456;&#24403;&#30340;&#33391;&#22909;&#34920;&#29616;&#21040;12&#20010;&#23567;&#26102;&#30340;&#25552;&#21069;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MetNet-3&#65292;&#23427;&#26174;&#33879;&#25193;&#23637;&#20102;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#31070;&#32463;&#27169;&#22411;&#33021;&#22815;&#33391;&#22909;&#39044;&#27979;&#30340;&#24341;&#23548;&#26102;&#38388;&#33539;&#22260;&#21644;&#21464;&#37327;&#12290;MetNet-3&#20174;&#23494;&#38598;&#21644;&#31232;&#30095;&#30340;&#25968;&#25454;&#20256;&#24863;&#22120;&#20013;&#23398;&#20064;&#65292;&#24182;&#20026;&#38477;&#27700;&#12289;&#39118;&#12289;&#28201;&#24230;&#21644;&#38706;&#28857;&#36827;&#34892;24&#23567;&#26102;&#30340;&#39044;&#27979;&#12290;MetNet-3&#22312;&#20307;&#31995;&#32467;&#26500;&#23618;&#38754;&#24341;&#20837;&#20102;&#35768;&#22810;&#25216;&#26415;&#21019;&#26032;&#65292;&#36825;&#34987;&#35777;&#26126;&#23545;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#65292;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#26102;&#31354;&#21367;&#31215;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#31070;&#32463;&#27169;&#22411;&#27867;&#21270;&#20026;&#20165;&#25509;&#21463;&#31232;&#30095;&#30340;&#27668;&#21387;&#35745;&#35266;&#27979;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#25345;&#32493;&#24615;&#21551;&#21457;&#27861;&#26469;&#22806;&#25512;&#21021;&#22987;&#26465;&#20214;&#65292;&#25110;&#36890;&#36807;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#25968;&#20540;&#27169;&#22411;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#35266;&#27979;&#25968;&#25454;&#30340;&#26041;&#27861;&#26159;&#26377;&#30410;&#30340;&#12290;MetNet-3&#22312;&#38477;&#27700;&#12289;&#28201;&#24230;&#21644;&#38706;&#28857;&#39044;&#27979;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#22823;&#27668;&#27169;&#22411;&#22312;&#25552;&#21069;&#33267;24&#23567;&#26102;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks offer an alternative paradigm for modeling weather conditions. The ability of neural models to make a prediction in less than a second once the data is available and to do so with very high temporal and spatial resolution, and the ability to learn directly from atmospheric observations, are just some of these models' unique advantages. Neural models trained using atmospheric observations, the highest fidelity and lowest latency data, have to date achieved good performance only up to twelve hours of lead time when compared with state-of-the-art probabilistic Numerical Weather Prediction models and only for the sole variable of precipitation. In this paper, we present MetNet-3 that extends significantly both the lead time range and the variables that an observation based neural model can predict well. MetNet-3 learns from both dense and sparse data sensors and makes predictions up to 24 hours ahead for precipitation, wind, temperature and dew point. MetNet-3 introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;FIM&#65292;&#19968;&#31181;&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#35745;&#31639;Fisher&#20449;&#24687;&#24230;&#37327;&#65288;FIM&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#27969;&#24418;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#24418;&#29305;&#24449;&#30340;&#25551;&#36848;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.06062</link><description>&lt;p&gt;
&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#23398;&#20064;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#31070;&#32463;FIM
&lt;/p&gt;
&lt;p&gt;
Neural FIM for learning Fisher Information Metrics from point cloud data. (arXiv:2306.06062v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;FIM&#65292;&#19968;&#31181;&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#35745;&#31639;Fisher&#20449;&#24687;&#24230;&#37327;&#65288;FIM&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#27969;&#24418;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#24418;&#29305;&#24449;&#30340;&#25551;&#36848;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#25454;&#25193;&#25955;&#23884;&#20837;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#38543;&#22788;&#21487;&#35265;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#25581;&#31034;&#25968;&#25454;&#28508;&#22312;&#20869;&#22312;&#20960;&#20309;&#30340;&#21487;&#34892;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#20854;&#31163;&#25955;&#24615;&#65292;&#25193;&#25955;&#23884;&#20837;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;FIM&#65292;&#19968;&#31181;&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#35745;&#31639;Fisher&#20449;&#24687;&#24230;&#37327;&#65288;FIM&#65289;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#23545;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#27969;&#24418;&#24314;&#27169;&#12290;&#31070;&#32463;FIM&#20174;&#31163;&#25955;&#30340;&#28857;&#20113;&#25968;&#25454;&#20013;&#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#22240;&#27492;&#20174;&#24230;&#37327;&#20013;&#33719;&#21462;&#30340;&#20449;&#24687;&#21487;&#20197;&#21578;&#35785;&#25105;&#20204;&#27969;&#24418;&#30340;&#29305;&#24449;&#65292;&#22914;&#20307;&#31215;&#21644;&#27979;&#22320;&#32447;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31070;&#32463;FIM&#22312;&#36873;&#25321;PHATE&#21487;&#35270;&#21270;&#26041;&#27861;&#30340;&#21442;&#25968;&#20197;&#21450;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;IPSC&#37325;&#32534;&#31243;&#21644;PBMCs&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#30340;&#20998;&#25903;&#28857;&#21644;&#32858;&#31867;&#20013;&#24515;&#23884;&#20837;&#20013;&#33719;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although data diffusion embeddings are ubiquitous in unsupervised learning and have proven to be a viable technique for uncovering the underlying intrinsic geometry of data, diffusion embeddings are inherently limited due to their discrete nature. To this end, we propose neural FIM, a method for computing the Fisher information metric (FIM) from point cloud data - allowing for a continuous manifold model for the data. Neural FIM creates an extensible metric space from discrete point cloud data such that information from the metric can inform us of manifold characteristics such as volume and geodesics. We demonstrate Neural FIM's utility in selecting parameters for the PHATE visualization method as well as its ability to obtain information pertaining to local volume illuminating branching points and cluster centers embeddings of a toy dataset and two single-cell datasets of IPSC reprogramming and PBMCs (immune cells).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26435;&#37325;&#20923;&#32467;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;EEG&#20219;&#21153;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#24615;&#33021;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05775</link><description>&lt;p&gt;
&#26435;&#37325;&#20923;&#32467;&#65306;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#33041;&#30005;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weight Freezing: A Regularization Approach for Fully Connected Layers with an Application in EEG Classification. (arXiv:2306.05775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26435;&#37325;&#20923;&#32467;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;EEG&#20219;&#21153;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#24615;&#33021;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33041;&#30005;&#35299;&#30721;&#39046;&#22495;&#65292;&#25552;&#39640;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#26435;&#37325;&#20923;&#32467;&#8221;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;ANN&#27491;&#21017;&#21270;&#21644;&#31070;&#32463;&#31185;&#23398;&#20808;&#39564;&#30693;&#35782;&#30340;&#21407;&#21017;&#12290;&#26435;&#37325;&#20923;&#32467;&#30340;&#27010;&#24565;&#22260;&#32469;&#30528;&#36890;&#36807;&#20923;&#32467;&#20840;&#36830;&#25509;&#23618;&#20013;&#30340;&#29305;&#23450;&#26435;&#37325;&#26469;&#20943;&#23569;&#26576;&#20123;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;EEG&#20219;&#21153;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#25513;&#30721;&#30697;&#38453;&#21644;&#38408;&#20540;&#26469;&#30830;&#23450;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38656;&#35201;&#20923;&#32467;&#30340;&#26435;&#37325;&#27604;&#20363;&#26469;&#23454;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#34987;&#25513;&#34109;&#30340;&#26435;&#37325;&#35774;&#32622;&#20026;&#38646;&#65292;&#26435;&#37325;&#20923;&#32467;&#19981;&#20165;&#21487;&#20197;&#22312;&#20855;&#26377;&#20840;&#36830;&#25509;&#23618;&#20998;&#31867;&#22120;&#30340;&#32593;&#32476;&#20013;&#23454;&#29616;&#31232;&#30095;&#36830;&#25509;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#20840;&#36830;&#25509;&#23618;&#30340;&#26377;&#25928;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of EEG decoding, enhancing the performance of artificial neural networks (ANNs) carries significant potential. This study introduces a novel approach, termed "weight freezing", that is anchored on the principles of ANN regularization and neuroscience prior knowledge. The concept of weight freezing revolves around the idea of reducing certain neurons' influence on the decision-making process for a specific EEG task by freezing specific weights in the fully connected layer during the backpropagation process. This is actualized through the use of a mask matrix and a threshold to determine the proportion of weights to be frozen during backpropagation. Moreover, by setting the masked weights to zero, weight freezing can not only realize sparse connections in networks with a fully connected layer as the classifier but also function as an efficacious regularization method for fully connected layers. Through experiments involving three distinct ANN architectures and three widely r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;</title><link>http://arxiv.org/abs/2306.05708</link><description>&lt;p&gt;
&#32447;&#24615;&#25193;&#25955;&#25552;&#21319;&#20102;&#24555;&#36895;&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion. (arXiv:2306.05708v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24930;&#25512;&#29702;&#36895;&#24230;&#20351;&#24471;&#23427;&#20204;&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#19981;&#23454;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#65288;LinDiff&#65289;&#65292;&#20197;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#23454;&#29616;&#23545;&#22122;&#22768;&#35821;&#38899;&#30340;&#26377;&#25928;&#20840;&#23616;&#24314;&#27169;&#65292;LinDiff&#37319;&#29992;&#20102;&#22522;&#20110;&#20998;&#21306;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#36755;&#20837;&#20449;&#21495;&#21010;&#20998;&#20026;&#23567;&#34917;&#19969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models have shown extraordinary ability on various generative tasks. However, their slow inference speed renders them impractical in speech synthesis. This paper proposes a linear diffusion model (LinDiff) based on an ordinary differential equation to simultaneously reach fast inference and high sample quality. Firstly, we employ linear interpolation between the target and noise to design a diffusion sequence for training, while previously the diffusion path that links the noise and target is a curved segment. When decreasing the number of sampling steps (i.e., the number of line segments used to fit the path), the ease of fitting straight lines compared to curves allows us to generate higher quality samples from a random noise with fewer iterations. Secondly, to reduce computational complexity and achieve effective global modeling of noisy speech, LinDiff employs a patch-based processing approach that partitions the input signal into small patches. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Q-learning&#31639;&#27861;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;Markov&#21338;&#24328;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#31616;&#21333;&#21644;&#28145;&#20837;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.05700</link><description>&lt;p&gt;
&#20004;&#20154;&#38646;&#21644;Markov&#21338;&#24328;&#30340;&#26497;&#23567;&#26497;&#22823;Q-learning&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65306;&#20999;&#25442;&#31995;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Analysis of Minimax Q-Learning for Two-Player Zero-Sum Markov Games: Switching System Approach. (arXiv:2306.05700v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Q-learning&#31639;&#27861;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;Markov&#21338;&#24328;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#31616;&#21333;&#21644;&#28145;&#20837;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;Q-learning&#31639;&#27861;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;Markov&#21338;&#24328;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#25105;&#20204;&#38024;&#23545;&#26497;&#23567;&#26497;&#22823;Q-learning&#31639;&#27861;&#20197;&#21450;&#30456;&#24212;&#30340;&#20215;&#20540;&#36845;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#20215;&#20540;&#36845;&#20195;&#21644;Q-learning&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26497;&#23567;&#26497;&#22823;Q-learning&#30340;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#20215;&#20540;&#36845;&#20195;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#26497;&#23567;&#26497;&#22823;Q-learning&#30340;&#36827;&#19968;&#27493;&#27934;&#23519;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#31616;&#21333;&#21644;&#28145;&#20837;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#20123;&#39069;&#22806;&#30340;&#27934;&#23519;&#21147;&#26377;&#28508;&#21147;&#25581;&#31034;&#25511;&#21046;&#29702;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#31038;&#21306;&#27010;&#24565;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#65292;&#24182;&#20419;&#36827;&#23427;&#20204;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this paper is to investigate the finite-time analysis of a Q-learning algorithm applied to two-player zero-sum Markov games. Specifically, we establish a finite-time analysis of both the minimax Q-learning algorithm and the corresponding value iteration method. To enhance the analysis of both value iteration and Q-learning, we employ the switching system model of minimax Q-learning and the associated value iteration. This approach provides further insights into minimax Q-learning and facilitates a more straightforward and insightful convergence analysis. We anticipate that the introduction of these additional insights has the potential to uncover novel connections and foster collaboration between concepts in the fields of control theory and reinforcement learning communities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05554</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#20013;&#30340;&#24212;&#29992;&#21644;&#39044;&#27979;&#65306;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks. (arXiv:2306.05554v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#65288;COUCSI&#65289;&#26159;&#19968;&#31181;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#28070;&#28287;&#30456;&#21462;&#20195;&#20102;&#38750;&#28070;&#28287;&#30456;&#30340;&#20301;&#32622;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#26089;&#26399;&#65288;ET&#65289;&#21644;&#26202;&#26399;&#65288;LT&#65289;COUCSI&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#20197;&#25913;&#36827;PINNs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#33258;&#21464;&#37327;&#23558;COUCSI&#38382;&#39064;&#20998;&#21035;&#29992;XT-&#65292;XY-&#21644;Z-&#19977;&#31181;&#31561;&#25928;&#24418;&#24335;&#36827;&#34892;&#25551;&#36848;&#65306;&#31532;&#19968;&#20010;&#25551;&#36848;&#20102;&#39281;&#21644;&#24230;&#20316;&#20026;&#35268;&#33539;&#21270;&#20301;&#32622;X&#21644;&#26102;&#38388;T&#30340;&#20989;&#25968;;&#31532;&#20108;&#20010;&#25551;&#36848;&#20102;X&#21644;Y=T^0.5&#20316;&#20026;&#20989;&#25968;&#30340;&#39281;&#21644;&#24230;;&#31532;&#19977;&#20010;&#20316;&#20026;Z=X/T^0.5&#30340;&#21807;&#19968;&#20989;&#25968;&#65288;&#20165;&#22312;ET&#19979;&#26377;&#25928;&#65289;&#12290;&#35813;PINN&#27169;&#22411;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#65292;&#24182;&#22522;&#20110;&#26368;&#23567;&#21270;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#20002;&#22833;&#39033;&#21644;&#19982;&#21021;&#22987;&#36793;&#30028;&#26465;&#20214;&#30456;&#23545;&#24212;&#30340;&#39033;&#12290;&#27809;&#26377;&#21512;&#25104;&#25110;&#23454;&#39564;&#25968;&#25454;&#34987;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Countercurrent spontaneous imbibition (COUCSI) is a process in porous materials in which a wetting phase displaces non-wetting phase. In this work, we investigate for the first time the application of Physics-Informed Neural Networks (PINNs) in solving the 1D COUCSI problem in both early (ET) and late (LT) times. Also novel, we examine the Change-of-Variables technique for improving the performance of PINNs. We formulated the COUCSI problem in three equivalent forms by changing the independent variables: XT-, XY-, and Z-formulations. The first describes saturation as function of normalized position X and time T; the second as function of X and Y=T^0.5; and the third as a sole function of Z=X/T^0.5 (valid only at ET). The PINN model was generated using a feed-forward neural network and trained based on minimizing a weighted loss function, including the physics-informed loss term and terms corresponding to the initial and boundary conditions. No synthetical or experimental data were invo
&lt;/p&gt;</description></item><item><title>CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04979</link><description>&lt;p&gt;
CoCo: &#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#30340;&#32806;&#21512;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04979
&lt;/p&gt;
&lt;p&gt;
CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#26497;&#22823;&#30340;&#20195;&#20215;&#26469;&#33719;&#24471;&#12290;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25506;&#32034;&#20854;&#20182;&#26631;&#27880;&#22270;&#20197;&#22686;&#24378;&#30446;&#26631;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#22914;&#20309;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21040;&#39046;&#22495;&#36866;&#24212;&#20013;&#20173;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#23545;&#22270;&#25299;&#25169;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#39046;&#22495;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoCo&#65288;Coupled Contrastive Graph Representation Learning&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#32806;&#21512;&#23398;&#20064;&#20998;&#25903;&#20013;&#25552;&#21462;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#12290;CoCo&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#25903;&#21644;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#20998;&#25903;&#65292;&#20998;&#21035;&#29992;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#24335;&#25506;&#32034;&#22270;&#25299;&#25169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32806;&#21512;&#20998;&#25903;&#32467;&#21512;&#21040;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04919</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36125;&#21494;&#26031;&#31890;&#23376;&#27969;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework. (arXiv:2306.04919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#27979;&#37327;&#23545;&#20110;&#36890;&#36807;&#21487;&#38752;&#30340;&#29366;&#24577;&#25512;&#26029;&#23454;&#29616;&#31934;&#30830;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24320;&#21457;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#36719;&#27979;&#37327;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031; (DPFB) &#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26080;&#30446;&#26631;&#29366;&#24577;&#26631;&#31614;&#24773;&#20917;&#19979;&#36827;&#34892;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#65292;&#20197;&#25191;&#34892;&#28508;&#22312;&#30340;&#36328;&#39046;&#22495;&#36719;&#24863;&#30693;&#38382;&#39064;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#22312;&#26694;&#26550;&#26680;&#24515;&#65292;&#25105;&#20204;&#32467;&#21512;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340;&#31890;&#23376;&#27969;&#65292;&#36890;&#36807;&#20248;&#21270;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#26469;&#25191;&#34892;&#27169;&#22411;&#25552;&#21462;&#30340;&#28508;&#22312;&#21644;&#38544;&#34255;&#29305;&#24449;&#30340;&#31934;&#30830;&#36125;&#21494;&#26031;&#26356;&#26032;&#12290;&#30001;&#27492;&#65292;&#36825;&#20123;&#36129;&#29486;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#26377;&#26426;&#30340;&#36817;&#20284;&#21518;&#39564;&#29305;&#24449;&#34920;&#31034;&#65292;&#33021;&#22815;&#34920;&#24449;&#22797;&#26434;&#30340;&#36328;&#39046;&#22495;&#31995;&#32479;&#21160;&#21147;&#23398;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven soft sensors are essential for achieving accurate perception through reliable state inference. However, developing representative soft sensor models is challenged by issues such as missing labels, domain adaptability, and temporal coherence in data. To address these challenges, we propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft sensor modeling in the absence of target state labels. In particular, a sequential Bayes objective is first formulated to perform the maximum likelihood estimation underlying the cross-domain soft sensing problem. At the core of the framework, we incorporate a physics-inspired particle flow that optimizes the sequential Bayes objective to perform an exact Bayes update of the model extracted latent and hidden features. As a result, these contributions enable the proposed framework to learn a cohesive approximate posterior feature representation capable of characterizing complex cross-domain system dynamics and performing effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#27604;&#20854;&#20182;&#32463;&#20856;&#31639;&#27861;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2306.04828</link><description>&lt;p&gt;
&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#19982;GNN&#30340;&#24555;&#36895;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Fast and Effective GNN Training with Linearized Random Spanning Trees. (arXiv:2306.04828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#27604;&#20854;&#20182;&#32463;&#20856;&#31639;&#27861;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#30340;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#35757;&#32451;GNN&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32447;&#24615;&#21270;&#20174;&#36755;&#20837;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#38543;&#26426;&#29983;&#25104;&#26641;&#24471;&#21040;&#19968;&#31995;&#21015;&#36335;&#24452;&#22270;&#26469;&#36880;&#27493;&#31934;&#32454;&#21270;&#26435;&#37325;&#26356;&#26032;&#25805;&#20316;&#12290;&#36335;&#24452;&#22270;&#34987;&#35774;&#35745;&#20026;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#22522;&#26412;&#25299;&#25169;&#21644;&#33410;&#28857;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#36335;&#24452;&#22270;&#30340;&#31232;&#30095;&#24615;&#20351;&#24471;GNN&#35757;&#32451;&#26356;&#36731;&#20415;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#22806;&#65292;&#36824;&#26377;&#21161;&#20110;&#32531;&#35299;&#36807;&#24230;&#21387;&#32553;&#21644;&#36807;&#24230;&#24179;&#28369;&#31561;&#32463;&#20856;&#35757;&#32451;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65292;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#21644;&#27979;&#35797;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new effective and scalable framework for training GNNs in supervised node classification tasks, given graph-structured data. Our approach increasingly refines the weight update operations on a sequence of path graphs obtained by linearizing random spanning trees extracted from the input network. The path graphs are designed to retain essential topological and node information of the original graph. At the same time, the sparsity of path graphs enables a much lighter GNN training which, besides scalability, helps in mitigating classical training issues, like over-squashing and over-smoothing. We carry out an extensive experimental investigation on a number of real-world graph benchmarks, where we apply our framework to graph convolutional networks, showing simultaneous improvement of both training speed and test accuracy, as compared to well-known baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#28436;&#21592;&#32593;&#32476;&#26469;&#20248;&#21270;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#31995;&#32479;&#30340;&#39057;&#29575;&#21644;&#21152;&#36895;&#24230;&#26354;&#32447;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.04660</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning. (arXiv:2306.04660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#28436;&#21592;&#32593;&#32476;&#26469;&#20248;&#21270;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#31995;&#32479;&#30340;&#39057;&#29575;&#21644;&#21152;&#36895;&#24230;&#26354;&#32447;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#65288;GLOSA&#65289;&#31995;&#32479;&#24314;&#35758;&#36710;&#36742;&#36895;&#24230;&#65292;&#20197;&#24110;&#21161;&#23427;&#20204;&#22312;&#32511;&#33394;&#26102;&#38388;&#36890;&#36807;&#36335;&#21475;&#65292;&#20174;&#32780;&#36890;&#36807;&#26368;&#23567;&#21270;&#22312;&#36335;&#21475;&#20572;&#36710;&#21644;&#24608;&#36895;&#26102;&#38388;&#26469;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#21644;&#29123;&#26009;&#28040;&#32791;&#12290;&#20294;&#26159;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20248;&#21270;GLOSA&#31639;&#27861;&#65292;&#24573;&#30053;&#20102;GLOSA&#31995;&#32479;&#30340;&#36895;&#24230;&#24314;&#35758;&#39057;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#20123;&#30740;&#31350;&#22312;&#27599;&#20010;&#20915;&#31574;&#27493;&#39588;&#25552;&#20379;&#36895;&#24230;&#24314;&#35758;&#65292;&#23548;&#33268;&#20887;&#20313;&#24314;&#35758;&#65292;&#32780;&#20854;&#20182;&#20154;&#20165;&#20026;&#36710;&#36742;&#35745;&#31639;&#26368;&#20339;&#36895;&#24230;&#65292;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#20132;&#36890;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;PPO&#65288;H-PPO&#65289;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;GLOSA&#65288;AF-GLOSA&#65289;&#27169;&#22411;&#65292;&#20854;&#37319;&#29992;&#20102;&#19968;&#20010;actor-critic&#26550;&#26500;&#21644;&#19968;&#20010;&#28151;&#21512;actor&#32593;&#32476;&#12290;&#28151;&#21512;&#28436;&#21592;&#32593;&#32476;&#21253;&#25324;&#19968;&#20010;&#31163;&#25955;&#28436;&#21592;&#65292;&#36755;&#20986;&#21672;&#35810;&#39057;&#29575;&#21644;&#19968;&#20010;&#36830;&#32493;&#28436;&#21592;&#65292;&#36755;&#20986;&#21152;&#36895;&#24230;&#26354;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;AF-GLOSA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Green Light Optimal Speed Advisory (GLOSA) system suggests speeds to vehicles to assist them in passing through intersections during green intervals, thus reducing traffic congestion and fuel consumption by minimizing the number of stops and idle times at intersections. However, previous research has focused on optimizing the GLOSA algorithm, neglecting the frequency of speed advisory by the GLOSA system. Specifically, some studies provide speed advisory profile at each decision step, resulting in redundant advisory, while others calculate the optimal speed for the vehicle only once, which cannot adapt to dynamic traffic. In this paper, we propose an Adaptive Frequency GLOSA (AF-GLOSA) model based on Hybrid Proximal Policy Optimization (H-PPO), which employs an actor-critic architecture with a hybrid actor network. The hybrid actor network consists of a discrete actor that outputs advisory frequency and a continuous actor that outputs acceleration profiles. Additionally, we design a no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#20165;&#20174;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.04581</link><description>&lt;p&gt;
&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations. (arXiv:2306.04581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#20165;&#20174;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20174;&#25945;&#24072;&#25110;&#19987;&#23478;&#30340;&#28436;&#31034;&#20013;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#30340;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#26102;&#38388;&#19978;&#25193;&#23637;&#30340;&#31574;&#30053;&#25110;&#36873;&#39033;&#36827;&#34892;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#36712;&#36857;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#36712;&#36857;&#20998;&#27495;&#24230;&#37327;&#65292;&#20197;&#26816;&#27979;&#21644;&#20002;&#24323;&#24050;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#36712;&#36857;&#37096;&#20998;&#65292;&#24182;&#21487;&#33021;&#38477;&#20302;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#22914;&#26524;&#29992;&#20110;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36873;&#39033;&#30340;&#31639;&#27861;&#26469;&#20998;&#21106;&#36712;&#36857;&#65292;&#24182;&#21482;&#20174;&#24050;&#30830;&#23450;&#20026;&#21487;&#25509;&#21463;&#30340;&#36712;&#36857;&#37096;&#20998;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;&#20462;&#22797;&#37096;&#20998;&#36712;&#36857;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning to perform a task from demonstrations given by teachers or experts, when some of the experts' demonstrations might be adversarial and demonstrate an incorrect way to perform the task. We propose a novel technique that can identify parts of demonstrated trajectories that have not been significantly modified by the adversary and utilize them for learning, using temporally extended policies or options. We first define a trajectory divergence measure based on the spatial and temporal features of demonstrated trajectories to detect and discard parts of the trajectories that have been significantly modified by an adversarial expert, and, could degrade the learner's performance, if used for learning, We then use an options-based algorithm that partitions trajectories and learns only from the parts of trajectories that have been determined as admissible. We provide theoretical results of our technique to show that repairing partial trajectories improves the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04440</link><description>&lt;p&gt;
&#20197;&#21452;&#31574;&#30053;&#20026;&#33258;&#27169;&#22411;&#30340;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dual policy as self-model for planning. (arXiv:2306.04440v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#20195;&#29702;&#36890;&#36807;&#25506;&#32034;&#21487;&#33021;&#30340;&#26410;&#26469;&#29366;&#24577;&#26469;&#36873;&#25321;&#20505;&#36873;&#21160;&#20316;&#12290;&#24403;&#23384;&#22312;&#39640;&#32500;&#34892;&#21160;&#31354;&#38388;&#26102;&#65292;&#20026;&#20102;&#27169;&#25311;&#26410;&#26469;&#29366;&#24577;&#65292;&#24517;&#39035;&#20351;&#29992;&#33258;&#24049;&#30340;&#20915;&#31574;&#31574;&#30053;&#26469;&#38480;&#21046;&#25152;&#38656;&#25506;&#32034;&#30340;&#21160;&#20316;&#25968;&#37327;&#12290;&#25105;&#20204;&#23558;&#29992;&#20110;&#27169;&#25311;&#33258;&#24049;&#20915;&#31574;&#30340;&#27169;&#22411;&#31216;&#20026;&#20195;&#29702;&#30340;&#33258;&#25105;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#35268;&#21010;&#34892;&#21160;&#26102;&#65292;&#19990;&#30028;&#27169;&#22411;&#36890;&#24120;&#19982;&#33258;&#25105;&#27169;&#22411;&#19968;&#36215;&#38544;&#21547;&#22320;&#20351;&#29992;&#65292;&#20294;&#22914;&#20309;&#35774;&#35745;&#33258;&#25105;&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#21463;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#12290;&#22312;&#36825;&#26679;&#30340;&#21452;&#31574;&#30053;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#26080;&#27169;&#22411;&#31574;&#30053;&#21644;&#19968;&#20010;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#20998;&#21035;&#29992;&#20110;&#26080;&#27169;&#22411;&#21160;&#20316;&#21644;&#35745;&#21010;&#21160;&#20316;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29983;&#24577;&#30456;&#20851;&#30340;&#21442;&#25968;&#29615;&#22659;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#35270;&#35282;&#30340;&#38543;&#26426;&#20248;&#21270;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#65292;&#26041;&#24335;&#20027;&#35201;&#26159;&#35757;&#32451;&#20915;&#31574;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#30340;newsvendor&#38382;&#39064;&#21644;&#32463;&#27982;&#20998;&#37197;&#38382;&#39064;&#19978;&#22343;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#20915;&#31574;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;</title><link>http://arxiv.org/abs/2306.04174</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#35270;&#35282;&#30340;&#38543;&#26426;&#20248;&#21270;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-End Learning for Stochastic Optimization: A Bayesian Perspective. (arXiv:2306.04174v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#35270;&#35282;&#30340;&#38543;&#26426;&#20248;&#21270;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#65292;&#26041;&#24335;&#20027;&#35201;&#26159;&#35757;&#32451;&#20915;&#31574;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#30340;newsvendor&#38382;&#39064;&#21644;&#32463;&#27982;&#20998;&#37197;&#38382;&#39064;&#19978;&#22343;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#20915;&#31574;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#35270;&#35282;&#30340;&#38543;&#26426;&#20248;&#21270;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#26631;&#20934;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#30340;&#24605;&#24819;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#21518;&#39564;&#36125;&#21494;&#26031;&#34892;&#21160;&#26144;&#23556;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20026;&#35299;&#20915;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#30340;newsvendor&#38382;&#39064;&#21644;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#32463;&#27982;&#20998;&#37197;&#38382;&#39064;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#35757;&#32451;&#26041;&#26696;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#20197;&#21450;&#20915;&#31574;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a principled approach to end-to-end learning in stochastic optimization. First, we show that the standard end-to-end learning algorithm admits a Bayesian interpretation and trains a posterior Bayes action map. Building on the insights of this analysis, we then propose new end-to-end learning algorithms for training decision maps that output solutions of empirical risk minimization and distributionally robust optimization problems, two dominant modeling paradigms in optimization under uncertainty. Numerical results for a synthetic newsvendor problem illustrate the key differences between alternative training schemes. We also investigate an economic dispatch problem based on real data to showcase the impact of the neural network architecture of the decision maps on their test performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20986;&#33394;&#32479;&#35745;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#20998;&#24067;&#24335;&#25311;&#29275;&#39039;(DQN)&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#19981;&#38656;&#35201;&#29275;&#39039;&#30697;&#38453;&#27714;&#36870;&#25110;&#36890;&#20449;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#25968;&#20540;&#20998;&#26512;&#35777;&#26126;&#20854;&#32479;&#35745;&#29305;&#24615;&#21644;&#26377;&#38480;&#30340;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04111</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#25311;&#29275;&#39039;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Quasi-Newton Updating for Large-Scale Distributed Learning. (arXiv:2306.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20986;&#33394;&#32479;&#35745;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#20998;&#24067;&#24335;&#25311;&#29275;&#39039;(DQN)&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#19981;&#38656;&#35201;&#29275;&#39039;&#30697;&#38453;&#27714;&#36870;&#25110;&#36890;&#20449;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#25968;&#20540;&#20998;&#26512;&#35777;&#26126;&#20854;&#32479;&#35745;&#29305;&#24615;&#21644;&#26377;&#38480;&#30340;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#35745;&#31639;&#23545;&#20110;&#29616;&#20195;&#32479;&#35745;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20986;&#33394;&#30340;&#32479;&#35745;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#20998;&#24067;&#24335;&#25311;&#29275;&#39039;(DQN)&#26694;&#26550;&#12290;&#22312;DQN&#26041;&#27861;&#20013;&#65292;&#19981;&#38656;&#35201;&#29275;&#39039;&#30697;&#38453;&#27714;&#36870;&#25110;&#36890;&#20449;&#65292;&#36825;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#30456;&#20851;&#26041;&#27861;&#21482;&#20998;&#26512;&#25968;&#20540;&#25910;&#25947;&#65292;&#24182;&#38656;&#35201;&#21457;&#25955;&#30340;&#36845;&#20195;&#27425;&#25968;&#25165;&#33021;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DQN&#26041;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#24182;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#32467;&#26524;&#20272;&#35745;&#22120;&#22312;&#23569;&#37327;&#36845;&#20195;&#19979;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#20998;&#26512;&#35777;&#26126;&#20102;&#26377;&#38480;&#30340;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed computing is critically important for modern statistical analysis. Herein, we develop a distributed quasi-Newton (DQN) framework with excellent statistical, computation, and communication efficiency. In the DQN method, no Hessian matrix inversion or communication is needed. This considerably reduces the computation and communication complexity of the proposed method. Notably, related existing methods only analyze numerical convergence and require a diverging number of iterations to converge. However, we investigate the statistical properties of the DQN method and theoretically demonstrate that the resulting estimator is statistically efficient over a small number of iterations under mild conditions. Extensive numerical analyses demonstrate the finite sample performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#20803;&#25968;&#25454;&#38598;&#24182;&#20803;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#25968;&#25454;&#38598;&#26102;&#20351;&#29992;&#35813;&#39044;&#27979;&#22120;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.03828</link><description>&lt;p&gt;
Quick-Tune&#65306;&#24555;&#36895;&#23398;&#20064;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#24494;&#35843;&#23427;
&lt;/p&gt;
&lt;p&gt;
Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How. (arXiv:2306.03828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#20803;&#25968;&#25454;&#38598;&#24182;&#20803;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#25968;&#25454;&#38598;&#26102;&#20351;&#29992;&#35813;&#39044;&#27979;&#22120;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#19981;&#26029;&#38754;&#20020;&#19968;&#20010;&#38382;&#39064;&#65306;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#35813;&#22914;&#20309;&#24494;&#35843;&#23427;&#20197;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32852;&#21512;&#25628;&#32034;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35780;&#20272;&#36229;&#36807;20k&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#22312;87&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;24&#20010;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#26469;&#29983;&#25104;&#22823;&#35268;&#27169;&#20803;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20854;&#23398;&#20064;&#26354;&#32447;&#19978;&#20803;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#20197;&#29992;&#20110;&#24555;&#36895;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#36873;&#25321;&#19968;&#20010;&#20934;&#30830;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#25214;&#21040;&#23427;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the hyperparameters for finetuning it. Our method transfers knowledge about the performance of many pretrained models with multiple hyperparameter configurations on a series of datasets. To this aim, we evaluated over 20k hyperparameter configurations for finetuning 24 pretrained image classification models on 87 datasets to generate a large-scale meta-dataset. We meta-learn a multi-fidelity performance predictor on the learning curves of this meta-dataset and use it for fast hyperparameter optimization on new datasets. We empirically demonstrate that our resulting approach can quickly select an accurate pretrained model for a new dataset together with its optimal hyperparameters.
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>http://arxiv.org/abs/2306.02913</link><description>&lt;p&gt;
&#20998;&#25955;&#21270;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;
&lt;/p&gt;
&lt;p&gt;
Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02913
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;D-SGD&#65289;&#20801;&#35768;&#22312;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#25511;&#21046;&#19979;&#65292;&#22823;&#37327;&#35774;&#22791;&#21516;&#26102;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#29702;&#35770;&#35748;&#20026;&#65292;&#20998;&#25955;&#21270;&#19981;&#21487;&#36991;&#20813;&#22320;&#21066;&#24369;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25361;&#25112;&#20256;&#32479;&#20449;&#24565;&#65292;&#25552;&#20986;&#20102;&#23436;&#20840;&#26032;&#30340;&#35282;&#24230;&#26469;&#29702;&#35299;&#20998;&#25955;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#38750;&#20984;&#38750;-$\beta$-&#24179;&#28369;&#35774;&#32622;&#19979;&#65292;D-SGD&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#24179;&#22343;&#26041;&#21521;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#31181;&#24778;&#20154;&#30340;&#28176;&#36817;&#31561;&#20215;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#27491;&#21017;&#21270;-&#20248;&#21270;&#26435;&#34913;&#20197;&#21450;&#20998;&#25955;&#21270;&#30340;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;D-SGD&#20013;&#23384;&#22312;&#19968;&#20010;&#33258;&#30001;&#30340;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#20272;&#35745;&#65307;&#65288;2&#65289;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#65307;&#65288;3&#65289;D-SGD&#30340;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#19981;&#20250;&#38543;&#30528;&#24635;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#36825;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2306.02561</link><description>&lt;p&gt;
LLM-Blender: &#21033;&#29992;&#25104;&#23545;&#25490;&#21517;&#21644;&#29983;&#25104;&#34701;&#21512;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#21516;&#20248;&#21183;&#26469;&#36798;&#21040;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;PairRanker&#21644;GenFuser&#65292;&#20197;&#24212;&#23545;&#19981;&#21516;&#31034;&#20363;&#30340;&#26368;&#20248;LLMs&#21487;&#20197;&#26174;&#30528;&#21464;&#21270;&#30340;&#35266;&#23519;&#12290;PairRanker&#20351;&#29992;&#19987;&#38376;&#30340;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#12290;&#23427;&#32852;&#21512;&#32534;&#30721;&#36755;&#20837;&#25991;&#26412;&#21644;&#19968;&#23545;&#20505;&#36873;&#32773;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#20248;&#36234;&#32773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PairRanker&#19982;ChatGPT&#30340;&#25490;&#21517;&#30456;&#20851;&#24615;&#26368;&#39640;&#12290;&#28982;&#21518;&#65292;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#20943;&#23569;&#23427;&#20204;&#30340;&#24369;&#28857;&#26469;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#20419;&#36827;&#22823;&#35268;&#27169;&#35780;&#20272;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MixInstruct&#65292;&#23427;&#26159;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#65292;&#20855;&#26377;oracle p&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#21453;&#29109;&#12289;&#21387;&#32553;&#22240;&#23376;&#21644;&#36328;&#20998;&#35010;F1&#20998;&#25968;&#20026;&#30446;&#26631;&#30340;&#20132;&#27969;&#20195;&#30721;&#28436;&#21464;&#20135;&#29289;&#65292;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#24418;&#25104;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#24230;&#37327;&#26469;&#39537;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.02383</link><description>&lt;p&gt;
&#39640;&#25928;&#35937;&#24449;&#20132;&#27969;&#32534;&#30721;&#30340;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
Evolution of Efficient Symbolic Communication Codes. (arXiv:2306.02383v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#21453;&#29109;&#12289;&#21387;&#32553;&#22240;&#23376;&#21644;&#36328;&#20998;&#35010;F1&#20998;&#25968;&#20026;&#30446;&#26631;&#30340;&#20132;&#27969;&#20195;&#30721;&#28436;&#21464;&#20135;&#29289;&#65292;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#24418;&#25104;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#24230;&#37327;&#26469;&#39537;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#32467;&#26500;&#22914;&#20309;&#34987;&#30475;&#20316;&#26159;&#20154;&#38469;&#20132;&#27969;&#20195;&#30721;&#30340;&#28436;&#21464;&#20135;&#29289;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#25991;&#21270;&#26080;&#20851;&#21644;&#36328;&#35821;&#35328;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;&#21453;&#29109;&#12289;&#21387;&#32553;&#22240;&#23376;&#21644;&#36328;&#20998;&#35010;F1&#20998;&#25968;&#12290;&#25506;&#32034;&#26159;&#20316;&#20026;&#26356;&#22823;&#30340;&#26080;&#30417;&#30563;&#35821;&#35328;&#23398;&#20064;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#23436;&#25104;&#30340;&#65292;&#35797;&#22270;&#22312;&#22522;&#20110;&#8220;&#22522;&#26412;&#35821;&#35328;&#32467;&#26500;&#8221;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#20013;&#25191;&#34892;&#20803;&#23398;&#20064;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#19978;&#36848;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20420;&#35821;&#12289;&#20013;&#25991;&#21644;&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35789;&#32423;&#20998;&#35789;&#26631;&#35760;&#21270;&#30740;&#31350;&#20197;&#21450;&#38024;&#23545;&#33521;&#35821;&#30340;&#23376;&#35789;&#20998;&#21106;&#25110;&#24418;&#24577;&#20998;&#26512;&#30740;&#31350;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#24418;&#25104;&#35789;&#32423;&#20998;&#21106;&#25110;&#26631;&#35760;&#21270;&#21487;&#20197;&#36890;&#36807;&#25152;&#26377;&#36825;&#20123;&#24230;&#37327;&#26469;&#39537;&#21160;&#65292;&#21453;&#29109;&#23545;&#33521;&#35821;&#21644;&#20420;&#35821;&#26356;&#30456;&#20851;&#65292;&#32780;&#21387;&#32553;&#22240;&#23376;&#23545;&#20013;&#25991;&#26356;&#20855;&#29305;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper explores how the human natural language structure can be seen as a product of evolution of inter-personal communication code, targeting maximisation of such culture-agnostic and cross-lingual metrics such as anti-entropy, compression factor and cross-split F1 score. The exploration is done as part of a larger unsupervised language learning effort, the attempt is made to perform meta-learning in a space of hyper-parameters maximising F1 score based on the "ground truth" language structure, by means of maximising the metrics mentioned above. The paper presents preliminary results of cross-lingual word-level segmentation tokenisation study for Russian, Chinese and English as well as subword segmentation or morphological parsing study for English. It is found that language structure form the word-level segmentation or tokenisation can be found as driven by all of these metrics, anti-entropy being more relevant to English and Russian while compression factor more specific for Chin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;11&#31181;&#36866;&#24212;&#26041;&#27861;&#22312;&#19981;&#21516;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#26356;&#25935;&#24863;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#27604;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#65292;&#21487;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02080</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (arXiv:2306.02080v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02080
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;11&#31181;&#36866;&#24212;&#26041;&#27861;&#22312;&#19981;&#21516;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#26356;&#25935;&#24863;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#27604;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#65292;&#21487;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#30340;&#21508;&#31181;&#36866;&#24212;&#26041;&#27861;&#65292;&#22914; LoRA&#12289;prompts &#21644; adapters &#31561;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36866;&#24212;&#26041;&#27861;&#23545;&#20110;&#20998;&#24067;&#20301;&#31227;&#30340;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;11&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36866;&#24212;&#26041;&#27861;&#22312;4&#20010;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#23519;&#20102;&#21487;&#29992;&#36866;&#24212;&#31034;&#20363;&#21644;&#36866;&#24212;&#36807;&#31243;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#24341;&#20837;&#20102;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;96&#31181;&#35270;&#35273;&#21644;87&#31181;&#25991;&#26412;&#27745;&#25439;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#36866;&#24212;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;1&#65289;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#27604;&#35270;&#35273;&#27745;&#26579;&#26356;&#25935;&#24863;&#12290;2) &#20840;&#37327;&#24494;&#35843;&#24182;&#19981;&#24635;&#33021;&#25552;&#20379;&#26368;&#39640;&#30340;&#40065;&#26834;&#24615;&#65307;&#30456;&#21453;&#65292;&#36866;&#37197;&#22120;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;3&#65289;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#36890;&#24120;&#27604;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#31354;&#38388;&#20013;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. The robustness of these adaptation methods against distribution shifts have not been studied. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30005;&#21387;&#12289;&#39057;&#29575;&#32553;&#25918;&#21644;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#30340;DNN&#36793;&#32536;&#25512;&#29702;&#26694;&#26550;DVFO&#65292;&#23427;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#32852;&#21512;&#20248;&#21270;DVFS&#21644;&#21368;&#36733;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#32536;&#35774;&#22791;&#35745;&#31639;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;DNN&#27169;&#22411;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01811</link><description>&lt;p&gt;
DVFO&#65306;DNN&#36793;&#32536;&#25512;&#29702;&#30340;&#21160;&#24577;&#30005;&#21387;&#12289;&#39057;&#29575;&#32553;&#25918;&#21644;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
DVFO: Dynamic Voltage, Frequency Scaling and Workload Offloading for DNN Edge Inference. (arXiv:2306.01811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01811
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30005;&#21387;&#12289;&#39057;&#29575;&#32553;&#25918;&#21644;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#30340;DNN&#36793;&#32536;&#25512;&#29702;&#26694;&#26550;DVFO&#65292;&#23427;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#32852;&#21512;&#20248;&#21270;DVFS&#21644;&#21368;&#36733;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#32536;&#35774;&#22791;&#35745;&#31639;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;DNN&#27169;&#22411;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#38480;&#21046;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#19981;&#21516;&#29305;&#24615;&#65292;&#20248;&#21270;&#36793;&#32536;&#35774;&#22791;&#19978;DNN&#25512;&#29702;&#24615;&#33021;&#65288;&#22312;&#33021;&#28304;&#28040;&#32791;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#65289;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#38500;&#20102;&#21160;&#24577;&#30005;&#21387;&#39057;&#29575;&#32553;&#25918;&#65288;DVFS&#65289;&#25216;&#26415;&#65292;&#36793;&#32536;&#20113;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#21327;&#20316;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;DNN&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36793;&#32536;&#20113;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;&#23578;&#26410;&#23545;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#21508;&#31181;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DVFO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;DVFS&#30340;&#36793;&#32536;&#20113;&#21327;&#20316;&#25512;&#29702;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#32852;&#21512;&#20248;&#21270;DVFS&#21644;&#21368;&#36733;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DVFO&#33258;&#21160;&#20849;&#21516;&#20248;&#21270;&#20102;1&#65289;&#36793;&#32536;&#35774;&#22791;&#30340;CPU&#12289;GPU&#21644;&#20869;&#23384;&#39057;&#29575;&#65292;&#20197;&#21450;2&#65289;&#35201;&#21368;&#36733;&#21040;&#20113;&#26381;&#21153;&#22120;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#24605;&#32771;&#21363;&#34892;&#21160;&#30340;&#24182;&#21457;&#26426;&#21046;&#21152;&#36895;DRL&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#36827;&#19968;&#27493;&#38477;&#20302;DNN&#27169;&#22411;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DVFO&#22312;&#25512;&#29702;&#20934;&#30830;&#24230;&#21644;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to edge device resource constraints and different characteristics of deep neural network (DNN) models, it is a big challenge to optimize DNN inference performance in terms of energy consumption and inference latency on edge devices. In addition to the dynamic voltage frequency scaling (DVFS) technique, the edge-cloud architecture provides a collaborative approach to efficient DNN inference. However, current edge-cloud collaborative inference methods have not optimized various compute resources on edge devices. Thus, we propose DVFO, a novel DVFS-enabled edge-cloud collaborative inference framework, which jointly optimize DVFS and offloading parameters via deep reinforcement learning (DRL). Specifically, DVFO automatically co-optimizes 1) CPU, GPU and memory frequencies of edge devices, and 2) feature maps to be offloaded to cloud servers. In addition, it leverages a thinking-while-moving concurrent mechanism to accelerate the DRL learning process, and a spatialchannel attention mec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DeepfakeArt Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24110;&#21161;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#36827;&#34892;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#25361;&#25112;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.01272</link><description>&lt;p&gt;
DeepfakeArt Challenge: &#29992;&#20110;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection. (arXiv:2306.01272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DeepfakeArt Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24110;&#21161;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#36827;&#34892;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#25361;&#25112;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24040;&#22823;&#36827;&#27493;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#21644;&#21069;&#26223;&#65292;&#33539;&#22260;&#20174;&#20250;&#35805;&#20195;&#29702;&#21644;&#25991;&#26412;&#20869;&#23481;&#29983;&#25104;&#21040;&#35821;&#38899;&#21644;&#35270;&#35273;&#21512;&#25104;&#12290;&#22312;&#29983;&#25104;AI&#30340;&#23835;&#36215;&#21644;&#20854;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#37319;&#29992;&#20013;&#65292;&#23545;&#20110;&#29983;&#25104;AI&#30340;&#24694;&#24847;&#29992;&#36884;&#23384;&#22312;&#30528;&#26174;&#30528;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#22312;&#20351;&#29992;&#29983;&#25104;AI&#36827;&#34892;&#35270;&#35273;&#20869;&#23481;&#21512;&#25104;&#30340;&#39046;&#22495;&#20013;&#65292;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#26159;&#22270;&#20687;&#20266;&#36896;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#21253;&#21547;&#25110;&#27966;&#29983;&#33258;&#29256;&#26435;&#20869;&#23481;&#30340;&#22270;&#20687;&#65289;&#21644;&#25968;&#25454;&#27745;&#26579;&#65288;&#21363;&#29983;&#25104;&#34987;&#25932;&#23545;&#27745;&#26579;&#30340;&#22270;&#20687;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#40723;&#21169;&#36127;&#36131;&#20219;&#30340;&#29983;&#25104;AI&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;DeepfakeArt Challenge&#65292;&#19968;&#20010;&#22823;&#22411;&#25361;&#25112;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24110;&#21161;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#36827;&#34892;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tremendous recent advances in generative artificial intelligence techniques have led to significant successes and promise in a wide range of different applications ranging from conversational agents and textual content generation to voice and visual synthesis. Amid the rise in generative AI and its increasing widespread adoption, there has been significant growing concern over the use of generative AI for malicious purposes. In the realm of visual content synthesis using generative AI, key areas of significant concern has been image forgery (e.g., generation of images containing or derived from copyright content), and data poisoning (i.e., generation of adversarially contaminated images). Motivated to address these key concerns to encourage responsible generative AI, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.01095</link><description>&lt;p&gt;
&#22823;&#25209;&#37327;&#31070;&#32463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20840;&#23616;&#20248;&#21270;&#40657;&#30418;&#39640;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#40664;&#35748;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#23427;&#22312;&#22788;&#29702;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#19987;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20195;&#29702;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22823;&#25209;&#37327;&#25968;&#25454;&#65292;&#24314;&#27169;&#22797;&#26434;&#38382;&#39064;&#20197;&#21450;&#20135;&#29983;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;NSGA-II&#30340;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#12290;&#36825;&#31181;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#31574;&#30053;&#20419;&#36827;&#20102;&#26410;&#21208;&#25506;&#21306;&#22495;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#22312;&#25968;&#25454;&#23494;&#38598;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33258;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#25552;&#20379;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22823;&#25511;&#21046;&#65292;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#27169;&#22411;&#25110;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.00986</link><description>&lt;p&gt;
&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#33258;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diffusion Self-Guidance for Controllable Image Generation. (arXiv:2306.00986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33258;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#25552;&#20379;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22823;&#25511;&#21046;&#65292;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#27169;&#22411;&#25110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#20174;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#30340;&#35768;&#22810;&#26041;&#38754;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#36890;&#36807;&#25991;&#26412;&#26469;&#20256;&#36798;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#25552;&#20379;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22823;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20174;&#36825;&#20123;&#34920;&#31034;&#20013;&#25552;&#21462;&#20986;&#23545;&#35937;&#30340;&#24418;&#29366;&#12289;&#20301;&#32622;&#21644;&#22806;&#35266;&#31561;&#23646;&#24615;&#24182;&#29992;&#20110;&#25351;&#23548;&#37319;&#26679;&#12290;&#33258;&#23548;&#31867;&#20284;&#20110;&#20998;&#31867;&#22120;&#24341;&#23548;&#65292;&#20294;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26412;&#36523;&#20013;&#23384;&#22312;&#30340;&#20449;&#21495;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#25110;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32452;&#21512;&#19968;&#32452;&#31616;&#21333;&#30340;&#23646;&#24615;&#26469;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#20363;&#22914;&#20462;&#25913;&#23545;&#35937;&#30340;&#20301;&#32622;&#25110;&#22823;&#23567;&#65292;&#23558;&#19968;&#20010;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#22806;&#35266;&#19982;&#21478;&#19968;&#20010;&#22270;&#20687;&#30340;&#24067;&#23616;&#30456;&#32467;&#21512;&#65292;&#23558;&#22810;&#20010;&#22270;&#20687;&#30340;&#23545;&#35937;&#32452;&#21512;&#25104;&#19968;&#20010;&#65292;&#31561;&#31561;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#33258;&#23548;&#21487;&#20197;&#29992;&#20110;&#32534;&#36753;&#30495;&#23454;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#24352;&#37327;&#25968;&#25454;&#20013;&#27169;&#24335;&#30340;&#26174;&#33879;&#24615;&#65292;&#35813;&#26694;&#26550;&#25193;&#23637;&#20102;&#30697;&#38453;&#25968;&#25454;&#27169;&#24335;&#30340;&#32479;&#35745;&#23398;&#21407;&#29702;&#65292;&#24182;&#20174;&#22810;&#20010;&#29983;&#29289;&#21270;&#23398;&#21644;&#29983;&#29289;&#25216;&#26415;&#39046;&#22495;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#20013;&#25910;&#38598;&#20102;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.00643</link><description>&lt;p&gt;
TriSig&#65306;&#35780;&#20272;&#19977;&#20803;&#32452;&#31751;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;
&lt;/p&gt;
&lt;p&gt;
TriSig: Assessing the statistical significance of triclusters. (arXiv:2306.00643v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#24352;&#37327;&#25968;&#25454;&#20013;&#27169;&#24335;&#30340;&#26174;&#33879;&#24615;&#65292;&#35813;&#26694;&#26550;&#25193;&#23637;&#20102;&#30697;&#38453;&#25968;&#25454;&#27169;&#24335;&#30340;&#32479;&#35745;&#23398;&#21407;&#29702;&#65292;&#24182;&#20174;&#22810;&#20010;&#29983;&#29289;&#21270;&#23398;&#21644;&#29983;&#29289;&#25216;&#26415;&#39046;&#22495;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#20013;&#25910;&#38598;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#25968;&#25454;&#20998;&#26512;&#33021;&#22815;&#25581;&#31034;&#20174;&#30697;&#38453;&#25968;&#25454;&#20013;&#26080;&#27861;&#33719;&#24471;&#30340;&#26032;&#22411;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#20174;&#36825;&#20123;&#27169;&#24335;&#20013;&#25512;&#26029;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#26377;&#20851;&#30142;&#30149;&#36827;&#23637;&#12289;&#29983;&#29289;&#29983;&#20135;&#36807;&#31243;&#12289;&#22825;&#27668;&#27874;&#21160;&#21644;&#32676;&#20307;&#21160;&#21147;&#23398;&#30340;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#34394;&#20551;&#21644;&#20887;&#20313;&#30340;&#27169;&#24335;&#20250;&#22952;&#30861;&#36825;&#20010;&#36807;&#31243;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#35780;&#20272;&#24352;&#37327;&#25968;&#25454;&#27169;&#24335;&#20559;&#31163;&#31354;&#26399;&#26395;&#30340;&#27010;&#29575;&#65292;&#25193;&#23637;&#29616;&#26377;&#29992;&#20110;&#35780;&#20272;&#30697;&#38453;&#25968;&#25454;&#27169;&#24335;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#26412;&#21407;&#21017;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#38024;&#23545;&#35823;&#25253;&#21457;&#29616;&#30340;&#20108;&#39033;&#27979;&#35797;&#65292;&#21253;&#25324;&#65306;&#21464;&#37327;&#20381;&#36182;&#24615;&#12289;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#23545;Benjamini-Hochberg&#31243;&#24207;&#19979;&#30340;p&#20540;&#20462;&#27491;&#30340;&#20998;&#26512;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#29983;&#29289;&#21270;&#23398;&#21644;&#29983;&#29289;&#25216;&#26415;&#39046;&#22495;&#20013;&#19981;&#21516;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#20013;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#19977;&#20803;&#32452;&#32858;&#31867;&#31639;&#27861;&#26469;&#25910;&#38598;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor data analysis allows researchers to uncover novel patterns and relationships that cannot be obtained from matrix data alone. The information inferred from the patterns provides valuable insights into disease progression, bioproduction processes, weather fluctuations, and group dynamics. However, spurious and redundant patterns hamper this process. This work aims at proposing a statistical frame to assess the probability of patterns in tensor data to deviate from null expectations, extending well-established principles for assessing the statistical significance of patterns in matrix data. A comprehensive discussion on binomial testing for false positive discoveries is entailed at the light of: variable dependencies, temporal dependencies and misalignments, and \textit{p}-value corrections under the Benjamini-Hochberg procedure. Results gathered from the application of state-of-the-art triclustering algorithms over distinct real-world case studies in biochemical and biotechnologic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#29420;&#31435;&#20851;&#31995;&#23545;&#22240;&#26524;&#27169;&#20223;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#27169;&#20223;&#21487;&#34892;&#24615;&#20915;&#31574;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#22270;&#24418;&#26631;&#20934;&#20197;&#21450;&#19968;&#20010;&#26377;&#22768;&#30340;&#31639;&#27861;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00585</link><description>&lt;p&gt;
&#29305;&#23450;&#19978;&#19979;&#25991;&#29420;&#31435;&#20851;&#31995;&#19979;&#30340;&#22240;&#26524;&#21487;&#27169;&#20223;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Imitability Under Context-Specific Independence Relations. (arXiv:2306.00585v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#29420;&#31435;&#20851;&#31995;&#23545;&#22240;&#26524;&#27169;&#20223;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#27169;&#20223;&#21487;&#34892;&#24615;&#20915;&#31574;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#22270;&#24418;&#26631;&#20934;&#20197;&#21450;&#19968;&#20010;&#26377;&#22768;&#30340;&#31639;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35748;&#35782;&#21040;&#65292;&#22312;&#25191;&#34892;&#27169;&#20223;&#23398;&#20064;&#26102;&#24573;&#30053;&#22240;&#26524;&#26426;&#21046;&#30340;&#32570;&#28857;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#20223;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#35268;&#36991;&#22240;&#26524;&#28151;&#28102;&#21644;&#22240;&#26524;&#20559;&#24046;&#65292;&#20294;&#26159;&#23545;&#20110;&#20851;&#20110;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#30340;&#20854;&#20182;&#20449;&#24687;&#30340;&#28508;&#22312;&#22909;&#22788;&#21364;&#26410;&#34987;&#25506;&#32034;&#12290;&#36825;&#20123;&#24573;&#30053;&#30340;&#20449;&#24687;&#20043;&#19968;&#26159;&#29305;&#23450;&#19978;&#19979;&#25991;&#29420;&#31435;&#24615;&#65288;CSI&#65289;&#65292;&#21363;&#20165;&#22312;&#26576;&#20123;&#19978;&#19979;&#25991;&#20013;&#20445;&#25345;&#29420;&#31435;&#24615;&#12290;&#24403;&#24050;&#30693;CSI&#20851;&#31995;&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22240;&#26524;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20851;&#20110;&#27169;&#20223;&#21487;&#34892;&#24615;&#30340;&#20915;&#31574;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;CSI&#19979;&#30340;&#27169;&#20223;&#23398;&#20064;&#24517;&#35201;&#30340;&#22270;&#24418;&#26631;&#20934;&#65292;&#24182;&#34920;&#26126;&#22312;&#19968;&#31181;&#32467;&#26500;&#20551;&#35774;&#19979;&#65292;&#36825;&#19968;&#26631;&#20934;&#20063;&#26159;&#20805;&#20998;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#22768;&#30340;&#31639;&#27861;&#26041;&#27861;&#29992;&#20110;&#22522;&#20110;&#22240;&#26524;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;CSI&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drawbacks of ignoring the causal mechanisms when performing imitation learning have recently been acknowledged. Several approaches both to assess the feasibility of imitation and to circumvent causal confounding and causal misspecifications have been proposed in the literature. However, the potential benefits of the incorporation of additional information about the underlying causal structure are left unexplored. An example of such overlooked information is context-specific independence (CSI), i.e., independence that holds only in certain contexts. We consider the problem of causal imitation learning when CSI relations are known. We prove that the decision problem pertaining to the feasibility of imitation in this setting is NP-hard. Further, we provide a necessary graphical criterion for imitation learning under CSI and show that under a structural assumption, this criterion is also sufficient. Finally, we propose a sound algorithmic approach for causal imitation learning which takes 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SuperNorm&#30340;&#19987;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23884;&#20837;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#21644;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#20869;&#37096;&#36830;&#25509;&#20449;&#24687;&#30340;&#26126;&#30830;&#32771;&#34385;&#65292;&#20174;&#32780;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19903</link><description>&lt;p&gt;
&#20351;&#29992;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#23884;&#20837;&#24402;&#19968;&#21270;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization. (arXiv:2305.19903v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SuperNorm&#30340;&#19987;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23884;&#20837;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#21644;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#20869;&#37096;&#36830;&#25509;&#20449;&#24687;&#30340;&#26126;&#30830;&#32771;&#34385;&#65292;&#20174;&#32780;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31867;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24378;&#22823;&#23398;&#20064;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#36890;&#24120;&#24573;&#30053;&#20102;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#30340;&#37325;&#35201;&#32467;&#26500;&#29305;&#24449;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#19987;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#8212;&#8212;SUbgraph-sPEcific FactoR Embedded Normalization&#65288;SuperNorm&#65289;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#35813;&#26041;&#26696;&#26126;&#30830;&#32771;&#34385;&#20102;&#27599;&#20010;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20869;&#37096;&#36830;&#25509;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;BatchNorm&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#23884;&#20837;&#20102;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#65292;&#24182;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#20197;&#25552;&#39640;&#21306;&#20998;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#65292;&#25351;&#20986;&#36890;&#36807;&#25913;&#21892;&#30340;SuperNorm&#65292;&#20219;&#24847;GNN&#33267;&#23569;&#19982;1-WL&#27979;&#35797;&#19968;&#26679;&#33021;&#22815;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.19512</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#24335;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#22312;&#21487;&#25511;&#21046;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35797;&#22270;&#23558;&#36825;&#31181;&#21487;&#25511;&#24615;&#36816;&#29992;&#21040;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#65288;&#22914;&#39044;&#35757;&#32451;&#26435;&#37325;&#65289;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#24615;&#33021;&#21644;&#21487;&#25511;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;StylePTB&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#24335;&#27169;&#22411;&#65292;&#36825;&#26159;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35780;&#20272;&#20219;&#21153;&#30456;&#27604;&#65292;StylePTB&#20013;&#30340;&#20219;&#21153;&#38656;&#35201;&#23545;&#36755;&#20986;&#25991;&#26412;&#36827;&#34892;&#26356;&#21152;&#31934;&#32454;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;StylePTB&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20010;&#21035;&#21644;&#32452;&#21512;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;StylePTB&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#30340;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
&lt;/p&gt;</description></item><item><title>SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.19442</link><description>&lt;p&gt;
SimFBO&#65306;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#21452;&#23618;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19442
&lt;/p&gt;
&lt;p&gt;
SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#12289;&#24494;&#35843;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#39046;&#22495;&#20013;&#23884;&#22871;&#20248;&#21270;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32852;&#37030;&#21452;&#23618;&#20248;&#21270;&#65288;FBO&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#26174;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FBO&#31639;&#27861;&#24448;&#24448;&#28041;&#21450;&#22797;&#26434;&#30340;&#35745;&#31639;&#65292;&#24182;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#22810;&#20010;&#23376;&#24490;&#29615;&#65292;&#27599;&#20010;&#23376;&#24490;&#29615;&#21253;&#21547;&#22810;&#20010;&#36890;&#20449;&#36718;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimFBO&#30340;&#31616;&#21333;&#28789;&#27963;&#30340;FBO&#26694;&#26550;&#65292;&#23427;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#23376;&#24490;&#29615;&#65292;&#24182;&#21253;&#25324;&#19968;&#31181;&#24191;&#20041;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#21644;&#26356;&#26032;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#31995;&#32479;&#32423;&#24322;&#26500;&#40065;&#26834;FBO&#65288;ShroFBO&#65289;&#20316;&#20026;SimFBO&#30340;&#21464;&#20307;&#65292;&#20854;&#23545;&#26412;&#22320;&#35745;&#31639;&#30340;&#24322;&#26500;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#21644;&#26080;&#26367;&#25442;&#30340;&#23458;&#25143;&#31471;&#37319;&#26679;&#19979;&#65292;SimFBO&#21644;ShroFBO&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#21152;&#36895;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#20803;&#23398;&#20064;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19148</link><description>&lt;p&gt;
&#32531;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#31614;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21508;&#31181;&#35774;&#35745;&#35774;&#32622;&#65292;&#22914;&#36873;&#25321;&#21644;&#39034;&#24207;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#21487;&#33021;&#20351;&#27169;&#22411;&#23545;&#26576;&#31181;&#29305;&#23450;&#39044;&#27979;&#20559;&#35265;&#65292;&#32780;&#36825;&#31181;&#39044;&#27979;&#24182;&#19981;&#21453;&#26144;&#23545;&#20219;&#21153;&#30340;&#29702;&#35299;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#35752;&#35770;&#20102;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#65292;&#20294;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#21644;&#20943;&#32531;&#20854;&#24433;&#21709;&#30340;&#31995;&#32479;&#35843;&#26597;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#23450;&#20041;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65306;&#39321;&#33609;&#26631;&#31614;&#20559;&#24046;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20559;&#24046;&#21644;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#65288;&#25105;&#20204;&#39318;&#27425;&#27010;&#24565;&#21270;&#21644;&#26816;&#27979;&#21040;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20808;&#21069;&#30340;&#26631;&#31614;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#25152;&#26377;&#19977;&#31181;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#20351;LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21482;&#33021;&#23454;&#29616;&#38543;&#26426;&#32423;&#21035;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#31649;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36873;&#25321;&#22914;&#20309;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words f
&lt;/p&gt;</description></item><item><title>W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18624</link><description>&lt;p&gt;
W-procer: &#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18624
&lt;/p&gt;
&lt;p&gt;
W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#37197;&#32622;&#21147;&#27714;&#20943;&#23569;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#34987;&#27880;&#37322;&#20026;&#8220;O&#8221;&#65288;&#21363;&#8220;OUTSIDE&#8221;&#65289;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#24076;&#26395;&#34987;&#25512;&#31163;&#21040;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20026;&#8220;O&#8221;&#20197;&#22806;&#30340;&#20854;&#20182;&#23454;&#20307;&#65292;&#36825;&#31181;&#35774;&#23450;&#25928;&#26524;&#19981;&#20339;&#65292;&#21487;&#33021;&#20250;&#24471;&#20986;&#21547;&#26377;&#22122;&#22768;&#21407;&#22411;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#8220;O&#8221;&#26631;&#31614;&#23454;&#20307;&#19982;&#26377;&#26631;&#31614;&#23454;&#20307;&#30456;&#20851;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;W-PROCER&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#23637;&#24320;&#12290;&#36825;&#20123;&#32452;&#20214;&#22312;&#21327;&#21161;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;W-PROCER&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38134;&#34892;&#20132;&#26131;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#24369;&#30417;&#30563;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#26368;&#23567;&#21270;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#33021;&#22815;&#24555;&#36895;&#25193;&#23637;&#21040;&#26032;&#30340;&#21644;&#32452;&#21512;&#29992;&#20363;&#65292;&#21487;&#29992;&#20110;&#36130;&#21153;&#20581;&#24247;&#25253;&#21578;&#21644;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#31561;&#37329;&#34701;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18430</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#24369;&#30417;&#30563;&#38134;&#34892;&#20132;&#26131;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable and Weakly Supervised Bank Transaction Classification. (arXiv:2305.18430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38134;&#34892;&#20132;&#26131;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#24369;&#30417;&#30563;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#26368;&#23567;&#21270;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#33021;&#22815;&#24555;&#36895;&#25193;&#23637;&#21040;&#26032;&#30340;&#21644;&#32452;&#21512;&#29992;&#20363;&#65292;&#21487;&#29992;&#20110;&#36130;&#21153;&#20581;&#24247;&#25253;&#21578;&#21644;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#31561;&#37329;&#34701;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#24369;&#30417;&#30563;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#23545;&#38134;&#34892;&#20132;&#26131;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21551;&#21457;&#24335;&#21644;&#39046;&#22495;&#30693;&#35782;&#26469;&#35757;&#32451;&#20934;&#30830;&#30340;&#20132;&#26131;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#23545;&#26114;&#36149;&#21644;&#38590;&#20197;&#33719;&#21462;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31471;&#21040;&#31471;&#25968;&#25454;&#31649;&#36947;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20132;&#26131;&#25991;&#26412;&#23884;&#20837;&#12289;&#38170;&#23450;&#12289;&#26631;&#31614;&#29983;&#25104;&#12289;&#21028;&#21035;&#24335;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20197;&#21450;&#31995;&#32479;&#26550;&#26500;&#27010;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;&#24066;&#22330;&#39046;&#20808;&#35299;&#20915;&#26041;&#26696;&#12289;&#23454;&#29616;&#20934;&#30830;&#20998;&#31867;&#24182;&#19988;&#21487;&#20197;&#24555;&#36895;&#25193;&#23637;&#21040;&#26032;&#30340;&#21644;&#32452;&#21512;&#29992;&#20363;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#21453;&#36807;&#26469;&#21487;&#20197;&#24320;&#21551;&#35768;&#22810;&#37329;&#34701;&#24212;&#29992;&#65292;&#20363;&#22914;&#36130;&#21153;&#20581;&#24247;&#25253;&#21578;&#21644;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to categorize bank transactions using weak supervision, natural language processing, and deep neural network techniques. Our approach minimizes the reliance on expensive and difficult-to-obtain manual annotations by leveraging heuristics and domain knowledge to train accurate transaction classifiers. We present an effective and scalable end-to-end data pipeline, including data preprocessing, transaction text embedding, anchoring, label generation, discriminative neural network training, and an overview of the system architecture. We demonstrate the effectiveness of our method by showing it outperforms existing market-leading solutions, achieves accurate categorization, and can be quickly extended to novel and composite use cases. This can in turn unlock many financial applications such as financial health reporting and credit risk assessment.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.17537</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#22270;&#35760;&#24518;&#24314;&#27169;&#21160;&#24577;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#22914;&#23621;&#23460;&#31561;&#65292;&#23547;&#25214;&#29289;&#21697;&#30340;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#38656;&#35201;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#39044;&#27979;&#29289;&#21697;&#20301;&#32622;&#26469;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#38142;&#36335;&#39044;&#27979;&#38382;&#39064;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#22270;&#34920;&#36798;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;&#25151;&#38388;&#21644;&#29289;&#21697;&#26159;&#33410;&#28857;&#65292;&#22312;&#36793;&#32536;&#20013;&#32534;&#30721;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#65292;&#20195;&#29702;&#20154;&#20165;&#30693;&#36947;&#26356;&#25913;&#22270;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#23545;&#20110;&#29616;&#26377;&#30340;&#38142;&#36335;&#39044;&#27979;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#34920;&#31034; - &#22330;&#26223;&#22270;&#35760;&#24518;&#65288;SGM&#65289; - &#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#20154;&#30340;&#32047;&#31215;&#35266;&#23519;&#38598;&#21512;&#65292;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#20174;SGM&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#12290;&#25105;&#20204;&#22312;&#21160;&#24577;&#25151;&#23627;&#27169;&#25311;&#22120;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23427;&#25353;&#29031;&#35821;&#20041;&#27169;&#24335;&#21019;&#24314;&#19981;&#21516;&#30340;&#21160;&#24577;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17303</link><description>&lt;p&gt;
&#20174;&#40657;&#30418;&#27169;&#22411;&#21040;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21270;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;AI&#27169;&#22411;&#26159;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21363;&#20351;&#36755;&#20837;&#20998;&#24067;&#36731;&#24494;&#31227;&#20301;&#65288;&#20363;&#22914;&#25195;&#25551;&#20202;&#31867;&#22411;&#65289;&#65292;&#20063;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#32780;&#25918;&#23556;&#31185;&#21307;&#29983;&#21017;&#20381;&#36182;&#20110;&#24322;&#24120;&#24615;&#30340;&#36890;&#29992;&#25551;&#36848;&#24615;&#35268;&#21017;&#12290;&#24494;&#35843;&#27169;&#22411;&#20197;&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#38024;&#23545;&#26410;&#30693;&#30340;&#30446;&#26631;&#22495;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35748;&#20026;NN&#30340;&#21487;&#35299;&#37322;&#32452;&#20214;&#22823;&#33268;&#26159;&#22495;&#19981;&#21464;&#30340;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#21450;&#23427;&#20204;&#30340;BB&#21464;&#20307;&#12290;&#22312;&#28304;&#22495;&#20013;&#25105;&#20204;&#20808;&#20351;&#29992;&#20154;&#31867;&#29702;&#35299;&#30340;&#27010;&#24565;&#20174;BB&#24320;&#22987;&#65292;&#23558;&#20854;&#25552;&#28860;&#25104;&#19968;&#32452;&#27973;&#26174;&#26131;&#25026;&#30340;interpretable&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;interpretable&#27169;&#22411;&#37117;&#35206;&#30422;&#20102;&#25968;&#25454;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20855;&#26377;&#19968;&#32452;interpretable&#27169;&#22411;&#30340;&#28151;&#21512;&#21487;&#20197;&#23454;&#29616;&#19982;BB&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#33021;&#37327;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#24191;&#20041;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#22810;&#31181;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#65292;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15639</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24191;&#20041;p-Laplacian&#27491;&#21017;&#21270;&#26694;&#26550;&#22270;&#21367;&#31215;&#32593;&#32476;: &#25910;&#25947;&#24615;&#12289;&#33021;&#37327;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#25193;&#25955;&#35757;&#32451;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion. (arXiv:2305.15639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#33021;&#37327;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#24191;&#20041;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#22810;&#31181;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#65292;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65288;pL-UFG&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#24314;&#31435;&#23545;&#20854;&#24615;&#36136;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;Framelet&#21367;&#31215;&#21518;&#38598;&#25104;p-Laplacian&#30340;&#38544;&#24335;&#23618;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;pL-UFG&#28176;&#36817;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;&#36890;&#36807;&#25506;&#32034;pL-UFG&#30340;&#24191;&#20041;Dirichlet&#33021;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Dirichlet&#33021;&#37327;&#20445;&#25345;&#38750;&#38646;&#65292;&#30830;&#20445;&#22312;pL-UFG&#25509;&#36817;&#25910;&#25947;&#26102;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#33021;&#37327;&#35270;&#35282;&#38416;&#26126;&#20102;pL-UFG&#20013;&#30340;&#38544;&#24335;&#23618;&#19982;&#22270;Framelets&#21327;&#21516;&#24037;&#20316;&#65292;&#22686;&#24378;&#20102;&#35813;&#27169;&#22411;&#23545;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38544;&#24335;&#23618;&#21487;&#20197;&#34987;&#35299;&#37322;&#25104;&#24191;&#20041;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;&#36825;&#20123;&#22810;&#26041;&#38754;&#30340;&#20998;&#26512;&#23548;&#33268;&#20102;&#32479;&#19968;&#30340;&#32467;&#35770;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive theoretical analysis of graph p-Laplacian based framelet network (pL-UFG) to establish a solid understanding of its properties. We begin by conducting a convergence analysis of the p-Laplacian based implicit layer integrated after the framelet convolution, providing insights into the asymptotic behavior of pL-UFG. By exploring the generalized Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains non-zero, ensuring the avoidance of over-smoothing issues in pL-UFG as it approaches convergence. Furthermore, we elucidate the dynamic energy perspective through which the implicit layer in pL-UFG synergizes with graph framelets, enhancing the model's adaptability to both homophilic and heterophilic data. Remarkably, we establish that the implicit layer can be interpreted as a generalized non-linear diffusion process, enabling training using diverse schemes. These multifaceted analyses lead to unified conclusions that provide novel insi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28784;&#30418;&#23376;&#26041;&#27861;&#26469;&#39044;&#27979;&#20050;&#20051;&#29699;&#36712;&#36857;&#65292;&#27604;&#20004;&#31181;&#40657;&#30418;&#23376;&#26041;&#27861;&#39044;&#27979;&#24615;&#33021;&#26356;&#22909;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#29699;&#21457;&#23556;&#22120;&#30340;&#21442;&#25968;&#20013;&#21021;&#22987;&#21270;&#33258;&#26059;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#23454;&#29616;&#20102;&#39640;&#22238;&#29699;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.15189</link><description>&lt;p&gt;
&#40657;&#30418;&#23376;&#19982;&#28784;&#30418;&#23376;&#65306;&#20851;&#20110;&#23398;&#20064;&#24102;&#26377;&#33258;&#26059;&#21644;&#30896;&#25758;&#30340;&#20050;&#20051;&#29699;&#36712;&#36857;&#39044;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Black-Box vs. Gray-Box: A Case Study on Learning Table Tennis Ball Trajectory Prediction with Spin and Impacts. (arXiv:2305.15189v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28784;&#30418;&#23376;&#26041;&#27861;&#26469;&#39044;&#27979;&#20050;&#20051;&#29699;&#36712;&#36857;&#65292;&#27604;&#20004;&#31181;&#40657;&#30418;&#23376;&#26041;&#27861;&#39044;&#27979;&#24615;&#33021;&#26356;&#22909;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#29699;&#21457;&#23556;&#22120;&#30340;&#21442;&#25968;&#20013;&#21021;&#22987;&#21270;&#33258;&#26059;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#23454;&#29616;&#20102;&#39640;&#22238;&#29699;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20050;&#20051;&#29699;&#36712;&#36857;&#36807;&#28388;&#21644;&#39044;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#28784;&#30418;&#23376;&#26041;&#27861;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#26500;&#24314;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#26469;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#25512;&#26029;&#29699;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#19982;&#26410;&#32771;&#34385;&#20808;&#39564;&#29289;&#29702;&#30693;&#35782;&#30340;&#20004;&#31181;&#40657;&#30418;&#23376;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#29699;&#21457;&#23556;&#22120;&#30340;&#21442;&#25968;&#20013;&#21021;&#22987;&#21270;&#33258;&#26059;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#38271;&#26102;&#38388;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#27604;&#20165;&#20174;&#27979;&#37327;&#30340;&#29699;&#20301;&#32622;&#20272;&#35745;&#33258;&#26059;&#12290;&#20934;&#30830;&#39044;&#27979;&#29699;&#30340;&#36712;&#36857;&#23545;&#20110;&#25104;&#21151;&#22238;&#29699;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#20805;&#27668;&#20154;&#24037;&#32908;&#32905;&#26426;&#22120;&#20154;&#30340;&#22238;&#29699;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;29/30&#65288;97.7&#65285;&#65289;&#30340;&#22238;&#29699;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a method for table tennis ball trajectory filtering and prediction. Our gray-box approach builds on a physical model. At the same time, we use data to learn parameters of the dynamics model, of an extended Kalman filter, and of a neural model that infers the ball's initial condition. We demonstrate superior prediction performance of our approach over two black-box approaches, which are not supplied with physical prior knowledge. We demonstrate that initializing the spin from parameters of the ball launcher using a neural network drastically improves long-time prediction performance over estimating the spin purely from measured ball positions. An accurate prediction of the ball trajectory is crucial for successful returns. We therefore evaluate the return performance with a pneumatic artificial muscular robot and achieve a return rate of 29/30 (97.7%).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13592</link><description>&lt;p&gt;
&#21033;&#29992;&#65288;&#27169;&#31946;&#27979;&#35797;&#65289;&#27979;&#35797;&#29992;&#20363;&#26469;&#29702;&#35299;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#30340;&#35821;&#20041;&#29702;&#35299;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#23558;&#32534;&#31243;&#35821;&#35328;&#35270;&#20026;&#21478;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#22312;&#31243;&#24207;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;LLM&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#31243;&#24207;&#27605;&#31455;&#19982;&#25991;&#26412;&#26377;&#26412;&#36136;&#30340;&#21306;&#21035;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#20005;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#31243;&#24207;&#21450;&#20854;&#22522;&#26412;&#21333;&#20803;&#65288;&#21363;&#20989;&#25968;&#21644;&#23376;&#31243;&#24207;&#65289;&#26088;&#22312;&#23637;&#31034;&#21508;&#31181;&#34892;&#20026;&#21644;/&#25110;&#25552;&#20379;&#21487;&#33021;&#30340;&#36755;&#20986;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#36755;&#20837;&#12290;&#36755;&#20837;&#21644;&#21487;&#33021;&#30340;&#36755;&#20986;/&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#34920;&#31034;&#20989;&#25968;/&#23376;&#31243;&#24207;&#65292;&#24182;&#27010;&#36848;&#20102;&#25972;&#20010;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#31181;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#31243;&#24207;&#30340;&#26356;&#28145;&#20837;&#35821;&#20041;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#36275;&#22815;&#20195;&#34920;&#24615;&#30340;&#36755;&#20837;&#20197;&#35302;&#21457;&#22823;&#37327;&#25191;&#34892;&#65292;&#21487;&#20197;&#20351;&#29992;&#27169;&#31946;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of mo
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;NTK&#36924;&#36817;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#27169;&#22411;&#32553;&#25918;&#22240;&#23376;$\alpha=O(T)$&#26102;&#22312;&#35757;&#32451;&#26102;&#38388;$T$&#20043;&#21069;&#21487;&#20197;&#20351;&#24471;NTK&#36924;&#36817;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.13141</link><description>&lt;p&gt;
NTK&#36924;&#36817;&#26377;&#25928;&#30340;&#32039;&#20945;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tight conditions for when the NTK approximation is valid. (arXiv:2305.13141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;NTK&#36924;&#36817;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#27169;&#22411;&#32553;&#25918;&#22240;&#23376;$\alpha=O(T)$&#26102;&#22312;&#35757;&#32451;&#26102;&#38388;$T$&#20043;&#21069;&#21487;&#20197;&#20351;&#24471;NTK&#36924;&#36817;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#36924;&#36817;&#22312;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#27169;&#22411;&#26102;&#20309;&#26102;&#26377;&#25928;&#12290;&#22312;Chizat&#31561;&#20154;2019&#24180;&#30340;&#25042;&#24816;&#35757;&#32451;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#22240;&#23376;&#20026;$\alpha=O(T)$&#30340;&#27169;&#22411;&#32553;&#25918;&#23601;&#36275;&#20197;&#20351;NTK&#36924;&#36817;&#22312;&#35757;&#32451;&#26102;&#38388;$T$&#20043;&#21069;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#32039;&#20945;&#30340;&#65292;&#24182;&#19988;&#25913;&#21892;&#20102;Chizat&#31561;&#20154;2019&#24180;&#30340;&#20808;&#21069;&#30028;&#38480;&#65292;&#21518;&#32773;&#38656;&#35201;&#26356;&#22823;&#30340;&#32553;&#25918;&#22240;&#23376;$\alpha=O(T^2)$&#12290;
&lt;/p&gt;
&lt;p&gt;
We study when the neural tangent kernel (NTK) approximation is valid for training a model with the square loss. In the lazy training setting of Chizat et al. 2019, we show that rescaling the model by a factor of $\alpha = O(T)$ suffices for the NTK approximation to be valid until training time $T$. Our bound is tight and improves on the previous bound of Chizat et al. 2019, which required a larger rescaling factor of $\alpha = O(T^2)$.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#24378;&#22823;&#40657;&#30418;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25351;&#26631;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#21333;&#20010;&#26679;&#26412;&#30340;&#31283;&#23450;&#24615;&#21644;&#26597;&#35810;&#19982;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;RF&#21644;NTK&#22238;&#24402;&#65292;&#35777;&#26126;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.12100</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#65306;&#23545;&#20110;&#38543;&#26426;&#29305;&#24449;&#21644;NTK&#29305;&#24449;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stability, Generalization and Privacy: Precise Analysis for Random and NTK Features. (arXiv:2305.12100v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#24378;&#22823;&#40657;&#30418;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25351;&#26631;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#21333;&#20010;&#26679;&#26412;&#30340;&#31283;&#23450;&#24615;&#21644;&#26597;&#35810;&#19982;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;RF&#21644;NTK&#22238;&#24402;&#65292;&#35777;&#26126;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24674;&#22797;&#25915;&#20987;&#65292;&#24341;&#36215;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30340;&#25285;&#24551;&#12290;&#38024;&#23545;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#31561;&#24120;&#35265;&#31639;&#27861;&#36890;&#24120;&#19981;&#33021;&#30452;&#25509;&#23454;&#26045;&#23433;&#20840;&#20445;&#38556;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#29305;&#23450;&#24378;&#22823;&#40657;&#30418;&#23376;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36890;&#36807;&#20004;&#20010;&#30475;&#20284;&#19981;&#21516;&#20294;&#26377;&#32852;&#31995;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#19968;&#26159;&#30456;&#23545;&#20110;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#27169;&#22411;&#31283;&#23450;&#24615;&#65292;&#21478;&#19968;&#20010;&#26159;&#25915;&#20987;&#26597;&#35810;&#21644;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;&#34429;&#28982;&#21069;&#32773;&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#38416;&#36848;&#65292;&#24182;&#19982;&#32463;&#20856;&#24037;&#20316;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#30456;&#20851;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#31532;&#20108;&#31181;&#29305;&#24615;&#26159;&#26032;&#39062;&#30340;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#32467;&#26524;&#20026;&#20004;&#31181;&#21407;&#22411;&#35774;&#32622;&#25552;&#20379;&#20102;&#29305;&#24449;&#23545;&#40784;&#30340;&#31934;&#30830;&#21051;&#30011;&#65306;&#38543;&#26426;&#29305;&#24449;&#65288;RF&#65289;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#22238;&#24402;&#12290;&#36825;&#35777;&#26126;&#65292;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#33021;&#22815;&#24471;&#21040;&#21152;&#24378;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20854;&#20182;&#26377;&#36259;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models can be vulnerable to recovery attacks, raising privacy concerns to users, and widespread algorithms such as empirical risk minimization (ERM) often do not directly enforce safety guarantees. In this paper, we study the safety of ERM-trained models against a family of powerful black-box attacks. Our analysis quantifies this safety via two separate terms: (i) the model stability with respect to individual training samples, and (ii) the feature alignment between the attacker query and the original data. While the first term is well established in learning theory and it is connected to the generalization error in classical work, the second one is, to the best of our knowledge, novel. Our key technical result provides a precise characterization of the feature alignment for the two prototypical settings of random features (RF) and neural tangent kernel (NTK) regression. This proves that privacy strengthens with an increase in the generalization capability, unveiling also
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#25193;&#23637;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#36991;&#20813;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#38454;&#39057;&#29575;&#20197;&#21450;&#35823;&#35782;&#21035;&#20302;&#38454;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09779</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#20811;&#26381;&#31070;&#32463;&#32593;&#32476;&#30340;&#20302;&#38454;&#35889;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks. (arXiv:2305.09779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#25193;&#23637;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#36991;&#20813;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#38454;&#39057;&#29575;&#20197;&#21450;&#35823;&#35782;&#21035;&#20302;&#38454;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#23398;&#20064;&#20219;&#24847;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#20294;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#27169;&#22411;&#24120;&#24120;&#34920;&#29616;&#20986;&#23545;&#8220;&#26356;&#31616;&#21333;&#8221;&#20989;&#25968;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#20613;&#37324;&#21494;&#65288;Walsh-Hadamard&#65289;&#21464;&#25442;&#65292;&#20174;&#31163;&#25955;&#65288;&#38646;&#19968;&#65289;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#31616;&#21333;&#24615;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#20613;&#37324;&#21494;&#31995;&#25968;&#30340;&#8220;&#38454;&#8221;&#26469;&#25429;&#25417;&#31616;&#21333;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#26377;&#23398;&#20064;&#36739;&#20302;&#38454;&#39057;&#29575;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35889;&#20559;&#24046;&#21521;&#36739;&#31616;&#21333;&#29305;&#24449;&#30340;&#36235;&#21183;&#23454;&#38469;&#19978;&#20250;&#25439;&#23475;&#31070;&#32463;&#32593;&#32476;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26356;&#39640;&#30340;&#38454;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#36824;&#26377;&#21161;&#20110;&#36991;&#20813;&#23545;&#20302;&#38454;&#39057;&#29575;&#30340;&#38169;&#35823;&#35782;&#21035;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#22312;&#20302;&#25968;&#25454;&#37327;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the capacity of neural nets to learn arbitrary functions, models trained through gradient descent often exhibit a bias towards ``simpler'' functions. Various notions of simplicity have been introduced to characterize this behavior. Here, we focus on the case of neural networks with discrete (zero-one) inputs through the lens of their Fourier (Walsh-Hadamard) transforms, where the notion of simplicity can be captured through the \emph{degree} of the Fourier coefficients. We empirically show that neural networks have a tendency to learn lower-degree frequencies. We show how this spectral bias towards simpler features can in fact \emph{hurt} the neural network's generalization on real-world datasets. To remedy this we propose a new scalable functional regularization scheme that aids the neural network to learn higher degree frequencies. Our regularizer also helps avoid erroneous identification of low-degree frequencies, which further improves generalization. We extensively evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#37325;&#24314;&#22270;&#20687;&#65292;&#32780;&#19981;&#20351;&#29992;&#27531;&#24046;&#25968;&#25454;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22270;&#20687;&#37325;&#26500;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#21407;&#22987;&#22270;&#20687;&#30340;&#31934;&#31616;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#19982;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#37325;&#28857;&#20851;&#27880;&#36710;&#36742;&#26816;&#27979;&#21644;&#20998;&#31867;&#31561;&#20851;&#38190;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.08265</link><description>&lt;p&gt;
&#26080;&#38656;&#27531;&#24046;&#35745;&#31639;&#30340;&#36710;&#36742;&#26816;&#27979;&#19982;&#20998;&#31867;&#65306;&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#27880;&#20837;&#21152;&#36895;HEVC&#22270;&#20687;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle Detection and Classification without Residual Calculation: Accelerating HEVC Image Decoding with Random Perturbation Injection. (arXiv:2305.08265v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#37325;&#24314;&#22270;&#20687;&#65292;&#32780;&#19981;&#20351;&#29992;&#27531;&#24046;&#25968;&#25454;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22270;&#20687;&#37325;&#26500;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#21407;&#22987;&#22270;&#20687;&#30340;&#31934;&#31616;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#19982;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#37325;&#28857;&#20851;&#27880;&#36710;&#36742;&#26816;&#27979;&#21644;&#20998;&#31867;&#31561;&#20851;&#38190;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20998;&#26512;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#20132;&#36890;&#30417;&#25511;&#26041;&#38754;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#39640;&#25928;&#35270;&#39057;&#25968;&#25454;&#22788;&#29702;&#21644;&#29702;&#35299;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#35270;&#39057;&#35299;&#30721;&#25216;&#26415;&#35745;&#31639;&#23494;&#38598;&#19988;&#26102;&#38388;&#32791;&#36153;&#22823;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#22312;&#21387;&#32553;&#22495;&#20013;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25200;&#21160;&#30340;&#21387;&#32553;&#22495;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#20132;&#36890;&#30417;&#25511;&#24212;&#29992;&#65292;&#22312;HEVC&#27604;&#29305;&#27969;&#20013;&#37325;&#24314;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#24314;&#35758;&#20351;&#29992;&#38543;&#26426;&#25200;&#21160;&#20195;&#26367;&#27531;&#24046;&#25968;&#20540;&#65292;&#21019;&#36896;&#20986;&#21407;&#22987;&#22270;&#20687;&#30340;&#31934;&#31616;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#19982;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#37325;&#28857;&#20851;&#27880;&#36710;&#36742;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#36890;&#36807;&#19981;&#20351;&#29992;&#27531;&#24046;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#22270;&#20687;&#37325;&#26500;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of video analytics, particularly traffic surveillance, there is a growing need for efficient and effective methods for processing and understanding video data. Traditional full video decoding techniques can be computationally intensive and time-consuming, leading researchers to explore alternative approaches in the compressed domain. This study introduces a novel random perturbation-based compressed domain method for reconstructing images from High Efficiency Video Coding (HEVC) bitstreams, specifically designed for traffic surveillance applications. To the best of our knowledge, our method is the first to propose substituting random perturbations for residual values, creating a condensed representation of the original image while retaining information relevant to video understanding tasks, particularly focusing on vehicle detection and classification as key use cases.  By not using residual data, our proposed method significantly reduces the data needed in the image recon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05832</link><description>&lt;p&gt;
&#22240;&#26524;&#20449;&#24687;&#20998;&#31163;&#65306;&#20026;&#25239;&#20998;&#24067;&#36716;&#31227;&#35774;&#35745;&#20195;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#19982;&#26368;&#32456;&#20351;&#29992;&#24773;&#20917;&#19981;&#21516;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#39044;&#27979;&#20998;&#24067;&#36716;&#31227;&#65292;&#26377;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#30340;&#22240;&#26524;&#21644;&#21453;&#22240;&#26524;&#21464;&#37327;&#37117;&#26159;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#20026;&#19979;&#28216;&#35266;&#27979;&#21464;&#37327;&#24320;&#21457;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#36825;&#20123;&#21464;&#37327;&#20805;&#24403;&#20195;&#29702;&#12290;&#25105;&#20204;&#36873;&#25321;&#26377;&#21161;&#20110;&#24314;&#31435;&#31283;&#23450;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#35757;&#32451;&#20219;&#21153;&#20174;&#20195;&#29702;&#20013;&#25552;&#21462;&#22686;&#24378;&#31283;&#23450;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;ODM&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#19982;&#21407;&#22987;ODM&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#36817;&#21313;&#20493;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24863;&#30693;&#20998;&#21306;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;ODM&#25509;&#36817;&#20840;&#23616;&#30340;ODM&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#24212;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;SVRG&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.04837</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;Scalable Optimal Margin Distribution Machine&#65289;
&lt;/p&gt;
&lt;p&gt;
Scalable Optimal Margin Distribution Machine. (arXiv:2305.04837v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;ODM&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#19982;&#21407;&#22987;ODM&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#36817;&#21313;&#20493;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24863;&#30693;&#20998;&#21306;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;ODM&#25509;&#36817;&#20840;&#23616;&#30340;ODM&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#24212;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;SVRG&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;ODM&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#26681;&#25454;&#26032;&#30340;&#36793;&#32536;&#29702;&#35770;&#24314;&#31435;&#65292;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22823;&#38388;&#38548;&#30340;&#23545;&#24212;&#26041;&#27861;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687;&#20854;&#20182;&#26680;&#26041;&#27861;&#19968;&#26679;&#65292;&#23427;&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#26041;&#38754;&#26222;&#36941;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;ODM&#65292;&#19982;&#21407;&#22987;ODM&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#36817;&#21313;&#20493;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24863;&#30693;&#20998;&#21306;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;ODM&#25509;&#36817;&#20840;&#23616;&#30340;ODM&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#24403;&#24212;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;SVRG&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#35757;&#32451;&#12290;&#22823;&#37327;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#26497;&#39640;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#24694;&#21270;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal margin Distribution Machine (ODM) is a newly proposed statistical learning framework rooting in the novel margin theory, which demonstrates better generalization performance than the traditional large margin based counterparts. Nonetheless, it suffers from the ubiquitous scalability problem regarding both computation time and memory as other kernel methods. This paper proposes a scalable ODM, which can achieve nearly ten times speedup compared to the original ODM training method. For nonlinear kernels, we propose a novel distribution-aware partition method to make the local ODM trained on each partition be close and converge fast to the global one. When linear kernel is applied, we extend a communication efficient SVRG method to accelerate the training further. Extensive empirical studies validate that our proposed method is highly computational efficient and almost never worsen the generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01210</link><description>&lt;p&gt;
ChatGPT&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#20005;&#26684;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#32508;&#21512;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#34987;&#38271;&#26399;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#30452;&#25509;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#20013;&#29992;&#25143;&#30340;&#24847;&#22270;&#29983;&#25104;&#20195;&#30721;&#12290;&#20195;&#30721;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#31574;&#21010;&#22909;&#30340;&#32508;&#21512;&#38382;&#39064;&#21644;&#21508;&#31181;&#36755;&#20837;/&#36755;&#20986;&#27979;&#35797;&#29992;&#20363;&#65292;&#34987;&#29992;&#26469;&#34913;&#37327;&#21508;&#31181;LLMs&#22312;&#20195;&#30721;&#32508;&#21512;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#27979;&#35797;&#29992;&#20363;&#22312;&#23436;&#20840;&#35780;&#20272;&#29983;&#25104;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#26041;&#38754;&#65292;&#25968;&#37327;&#21644;&#36136;&#37327;&#37117;&#21487;&#33021;&#26377;&#25152;&#38480;&#21046;&#12290;&#36825;&#31181;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#38480;&#21046;&#24341;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#22312;LLMs&#26102;&#20195;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvalPlus&#8212;&#8212;&#19968;&#20010;&#35780;&#20272;LLM-synthesized&#20195;&#30721;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#20005;&#26684;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#12290;EvalPlus&#25509;&#21463;&#22522;&#30784;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#33258;&#21160;&#36755;&#20837;&#29983;&#25104;&#27493;&#39588;&#65292;&#20351;&#29992;LLM-based&#21644;&#22522;&#20110;&#21464;&#24322;&#30340;&#26041;&#27861;&#29983;&#25104;&#21644;&#22810;&#26679;&#21270;&#22823;&#37327;&#26032;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#22235;&#31181;&#35780;&#20998;&#35268;&#21017;&#30340;&#25193;&#23637;&#65292;&#35777;&#26126;&#22312;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#31163;&#25955;&#21270;&#31243;&#24230;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#26159;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#24182;&#19988;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#23545;&#25968;&#24471;&#20998;&#21644;&#24067;&#33713;&#23572;&#24471;&#20998;&#30340;&#25193;&#23637;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.00621</link><description>&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#30340;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Proper Scoring Rules for Survival Analysis. (arXiv:2305.00621v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#22235;&#31181;&#35780;&#20998;&#35268;&#21017;&#30340;&#25193;&#23637;&#65292;&#35777;&#26126;&#22312;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#31163;&#25955;&#21270;&#31243;&#24230;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#26159;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#24182;&#19988;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#23545;&#25968;&#24471;&#20998;&#21644;&#24067;&#33713;&#23572;&#24471;&#20998;&#30340;&#25193;&#23637;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#20272;&#35745;&#26410;&#26469;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#20851;&#20110;&#20005;&#26684;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30340;&#22522;&#26412;&#29702;&#35770;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#20102;&#35299;&#20854;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24120;&#29992;&#30340;&#22235;&#31181;&#20005;&#26684;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#25193;&#23637;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#25193;&#23637;&#22312;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#31163;&#25955;&#21270;&#31243;&#24230;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#26159;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#27604;&#36739;&#20102;&#36825;&#20123;&#25193;&#23637;&#35780;&#20998;&#35268;&#21017;&#30340;&#20272;&#35745;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#23545;&#25968;&#24471;&#20998;&#21644;&#24067;&#33713;&#23572;&#24471;&#20998;&#30340;&#25193;&#23637;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is the problem of estimating probability distributions for future event times, which can be seen as a problem in uncertainty quantification. Although there are fundamental theories on strictly proper scoring rules for uncertainty quantification, little is known about those for survival analysis. In this paper, we investigate extensions of four major strictly proper scoring rules for survival analysis and we prove that these extensions are proper under certain conditions, which arise from the discretization of the estimation of probability distributions. We also compare the estimation performances of these extended scoring rules by using real datasets, and the extensions of the logarithmic score and the Brier score performed the best.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#28436;&#31034;&#20102;&#22312;ResNet&#21644;ConvNeXt&#31561;&#32463;&#20856;&#21644;&#29616;&#20195;CNN&#19978;&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#65292;&#24182;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#65292;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14836</link><description>&lt;p&gt;
&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#23545;&#22823;&#35268;&#27169;CNN&#36827;&#34892;&#25935;&#24863;&#35843;&#25972;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#23433;&#20840;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption. (arXiv:2304.14836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#28436;&#31034;&#20102;&#22312;ResNet&#21644;ConvNeXt&#31561;&#32463;&#20856;&#21644;&#29616;&#20195;CNN&#19978;&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#65292;&#24182;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#65292;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#36817;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#36235;&#21183;&#26159;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21152;&#23494;&#25968;&#25454;&#19978;&#25191;&#34892;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#35757;&#32451;&#36866;&#29992;&#20110;HE&#30340;&#21152;&#23494;&#25110;&#26410;&#21152;&#23494;&#30340;&#28145;&#23618;CNN&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#22522;&#26412;&#21644;&#29616;&#20195;CNN&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#20363;&#22914;ResNet&#21644;ConvNeXt&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;HELayers SDK&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#20135;&#29983;&#20102;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#26102;&#65292;&#25105;&#20204;&#30340;ResNet-18/50/101&#23454;&#29616;&#20165;&#38656;&#35201;7&#12289;31&#21644;57&#20998;&#38047;&#65292;&#36825;&#34920;&#26126;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#23454;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#22312;HE&#19979;&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#21644;&#36339;&#36291;&#36830;&#25509;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning solutions have recently gained significant attention. One promising research trend is using Homomorphic Encryption (HE), a method for performing computation over encrypted data. One major challenge in this approach is training HE-friendly, encrypted or unencrypted, deep CNNs with decent accuracy. We propose a novel training method for HE-friendly models, and demonstrate it on fundamental and modern CNNs, such as ResNet and ConvNeXt. After training, we evaluate our models by running encrypted samples using HELayers SDK and proving that they yield the desired results. When running on a GPU over the ImageNet dataset, our ResNet-18/50/101 implementations take only 7, 31 and 57 minutes, respectively, which shows that this solution is practical. Furthermore, we present several insights on handling the activation functions and skip-connections under HE. Finally, we demonstrate in an unprecedented way how to perform secure zero-shot prediction using a CLIP m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13620</link><description>&lt;p&gt;
ChartSumm&#65306;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23558;&#22270;&#34920;&#36716;&#25442;&#20026;&#25991;&#26412;&#25688;&#35201;&#26159;&#35270;&#38556;&#20154;&#22763;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#21516;&#26102;&#20026;&#29992;&#25143;&#25552;&#20379;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#31934;&#30830;&#27934;&#23519;&#21147;&#12290;&#22823;&#22411;&#12289;&#32467;&#26500;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#22987;&#32456;&#26159;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20849;84363&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#20027;&#39064;&#21644;&#22270;&#34920;&#31867;&#22411;&#65292;&#21487;&#29983;&#25104;&#38271;&#30701;&#25688;&#35201;&#12290;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23454;&#29616;&#21508;&#31181;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#24471;&#20998;&#26469;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20135;&#29983;&#38169;&#35273;&#65292;&#28431;&#25481;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#19981;&#27491;&#30830;&#22320;&#35299;&#37322;&#22270;&#34920;&#20013;&#30340;&#22797;&#26434;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#24037;&#20855;&#25506;&#35752;&#20102;&#23558;ChartSumm&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25104;&#20026;&#19968;&#20010;&#26377;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.11473</link><description>&lt;p&gt;
(&#21521;&#37327;)&#31354;&#38388;&#19981;&#26159;&#26368;&#21518;&#30340;&#30086;&#22495;&#65306;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#24040;&#39069;&#25237;&#36164;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#34429;&#28982;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#20027;&#23472;&#20102;&#20135;&#21697;&#25628;&#32034;&#20013;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#20294;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#21521;&#37327;&#21270;&#26412;&#36523;&#20063;&#21457;&#29983;&#20102;&#24040;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#31435;&#22330;&#35770;&#25991;&#20197;&#30456;&#21453;&#30340;&#26041;&#24335;&#20027;&#24352;&#65292;&#21363;&#31243;&#24207;&#21512;&#25104;&#23545;&#35768;&#22810;&#26597;&#35810;&#21644;&#24066;&#22330;&#20013;&#30340;&#22823;&#37327;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#34892;&#19994;&#37325;&#35201;&#24615;&#65292;&#27010;&#36848;&#20102;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#22522;&#20110;&#25105;&#20204;&#22312;Tooso&#26500;&#24314;&#31867;&#20284;&#31995;&#32479;&#30340;&#32463;&#39564;&#65292;&#22238;&#31572;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#21453;&#23545;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As ecommerce continues growing, huge investments in ML and NLP for Information Retrieval are following. While the vector space model dominated retrieval modelling in product search - even as vectorization itself greatly changed with the advent of deep learning -, our position paper argues in a contrarian fashion that program synthesis provides significant advantages for many queries and a significant number of players in the market. We detail the industry significance of the proposed approach, sketch implementation details, and address common objections drawing from our experience building a similar system at Tooso.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07297</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#35753;&#26234;&#33021;&#20307;&#33021;&#22815;&#21644;&#20154;&#31867;&#26377;&#25928;&#22320;&#21327;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#35753;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#20197;&#27492;&#35299;&#20915;&#22312;&#32570;&#20047;&#36739;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24120;&#24120;&#20250;&#25910;&#25947;&#21040;&#20154;&#31867;&#24182;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#19968;&#20010;&#22312;&#20154;&#31867;&#25351;&#20196;&#19979;&#30340;&#20808;&#39564;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#19982;&#20154;&#31867;&#21916;&#22909;&#19968;&#33268;&#30340;&#22343;&#34913;&#28857;&#12290;&#36890;&#36807;&#27010;&#24565;&#35777;&#26126;&#29615;&#22659;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Hanabi&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;instructRL&#25910;&#25947;&#20110;&#28385;&#36275;&#32473;&#23450;&#25351;&#20196;&#30340;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30693;&#36947;&#35821;&#35328;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.06858</link><description>&lt;p&gt;
Vax-Culture: &#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978;&#30123;&#33495;&#35752;&#35770;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#26399;&#38388;&#65292;&#30123;&#33495;&#29369;&#35947;&#32487;&#32493;&#26159;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#35813;&#29369;&#35947;&#30772;&#22351;&#20102;&#30123;&#33495;&#36816;&#21160;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#30830;&#23450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21453;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#19981;&#26029;&#22686;&#38271;&#26159;&#35813;&#38382;&#39064;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23558;&#25512;&#29305;&#20316;&#20026;&#35823;&#23548;&#20869;&#23481;&#30340;&#26469;&#28304;&#65292;&#24182;&#26088;&#22312;&#25552;&#21462;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#19982;&#30123;&#33495;&#26377;&#20851;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#20511;&#21161;&#19987;&#19994;&#27807;&#36890;&#21644;&#26032;&#38395;&#32972;&#26223;&#30340;&#27880;&#37322;&#20154;&#21592;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#26368;&#32456;&#24076;&#26395;&#36825;&#21487;&#20197;&#24102;&#26469;&#26377;&#25928;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#36890;&#20449;&#31574;&#30053;&#65292;&#20197;&#25509;&#35302;&#37027;&#20123;&#25345;&#21453;&#30123;&#33495;&#20449;&#20208;&#32773;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20449;&#24687;&#26377;&#21161;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine hesitancy continues to be a main challenge for public health officials during the COVID-19 pandemic. As this hesitancy undermines vaccine campaigns, many researchers have sought to identify its root causes, finding that the increasing volume of anti-vaccine misinformation on social media platforms is a key element of this problem. We explored Twitter as a source of misleading content with the goal of extracting overlapping cultural and political beliefs that motivate the spread of vaccine misinformation. To do this, we have collected a data set of vaccine-related Tweets and annotated them with the help of a team of annotators with a background in communications and journalism. Ultimately we hope this can lead to effective and targeted public health communication strategies for reaching individuals with anti-vaccine beliefs. Moreover, this information helps with developing Machine Learning models to automatically detect vaccine misinformation posts and combat their negative impa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#21644;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06094</link><description>&lt;p&gt;
&#33021;&#37327;&#24341;&#23548;&#30340;&#29109;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#21644;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#65288;EBMs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24050;&#32463;&#26377;&#25968;&#21313;&#24180;&#30340;&#21382;&#21490;&#12290;&#33258;&#20004;&#21315;&#24180;&#20195;&#36215;&#65292;&#19968;&#30452;&#26377;&#24456;&#22810;&#39640;&#25928;&#30340;&#26041;&#27861;&#36890;&#36807;&#33021;&#37327;&#21183;&#65288;&#38750;&#24402;&#19968;&#21270;&#30340;&#20284;&#28982;&#20989;&#25968;&#65289;&#26469;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;OT&#27714;&#35299;&#22120;&#65292;&#21463;&#21040;&#30340;&#25506;&#32034;&#35201;&#23569;&#24471;&#22810;&#65292;&#20165;&#26377;&#19968;&#20123;&#36817;&#26399;&#30340;&#30740;&#31350;&#65288;&#19981;&#21253;&#25324;&#21033;&#29992;OT&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#38382;&#39064;&#30340;WGAN&#26041;&#27861;&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;EBMs&#21644;&#29109;&#27491;&#21017;&#21270;OT&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#21033;&#29992;&#21069;&#32773;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#25216;&#26415;&#25913;&#36827;&#26469;&#20016;&#23500;&#21518;&#32773;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#26631;&#20934;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#31616;&#21333;&#36215;&#35265;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#31616;&#30701;&#21644;&#38271;&#36305;&#30340;EBMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) are known in the Machine Learning community for the decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. We validate the applicability of our method on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For the sake of simplicity, we choose simple short- and long- run EBMs as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#22522;&#20110;&#35774;&#35745;&#26032;&#30340;&#29702;&#35770;&#12289;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#22686;&#37327;&#19982;&#23436;&#20840;&#39564;&#35777;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;MNIST&#21644;CIFAR10&#20197;&#21450;ACAS-XU&#20998;&#31867;&#22120;&#30340;&#26356;&#39640;&#25928;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.01874</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#37327;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Incremental Verification of Neural Networks. (arXiv:2304.01874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01874
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#22522;&#20110;&#35774;&#35745;&#26032;&#30340;&#29702;&#35770;&#12289;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#22686;&#37327;&#19982;&#23436;&#20840;&#39564;&#35777;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;MNIST&#21644;CIFAR10&#20197;&#21450;ACAS-XU&#20998;&#31867;&#22120;&#30340;&#26356;&#39640;&#25928;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#23436;&#20840;&#39564;&#35777;&#21487;&#20197;&#30830;&#23450;DNNs&#26159;&#21542;&#22312;&#26080;&#38480;&#36755;&#20837;&#38598;&#19978;&#28385;&#36275;&#25152;&#38656;&#30340;&#21487;&#20449;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#40065;&#26834;&#24615;&#65292;&#20844;&#27491;&#24615;&#65289;&#12290;&#23613;&#31649;&#22810;&#24180;&#26469;&#24050;&#32463;&#21462;&#24471;&#20102;&#26497;&#22823;&#30340;&#36827;&#23637;&#65292;&#20197;&#25913;&#21892;&#23436;&#20840;&#39564;&#35777;&#22120;&#22312;&#21333;&#20010;DNNs&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#26159;&#24403;&#37096;&#32626;&#30340;DNN&#36827;&#34892;&#26356;&#26032;&#20197;&#25552;&#39640;&#20854;&#25512;&#29702;&#36895;&#24230;&#25110;&#20934;&#30830;&#24615;&#26102;&#65292;&#23427;&#20204;&#22312;&#26412;&#36136;&#19978;&#25928;&#29575;&#20302;&#19979;&#12290;&#36825;&#26159;&#22240;&#20026;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#36816;&#34892;&#26114;&#36149;&#30340;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#26356;&#26032;&#21518;&#30340;DNN&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#22522;&#20110;&#35774;&#35745;&#26032;&#30340;&#29702;&#35770;&#12289;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#30340;&#22686;&#37327;&#21644;&#23436;&#20840;DNN&#39564;&#35777;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#19968;&#20010;&#21517;&#20026;IVAN&#30340;&#24037;&#20855;&#20013;&#23454;&#29616;&#65292;&#23545;&#20110;&#39564;&#35777;MNIST&#21644;CIFAR10&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#30340;&#24635;&#20307;&#20960;&#20309;&#24179;&#22343;&#21152;&#36895;&#27604;&#20026;2.4&#20493;&#65292;&#23545;&#20110;ACAS-XU&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#30340;&#24635;&#20307;&#20960;&#20309;&#24179;&#22343;&#21152;&#36895;&#27604;&#20026;3.8&#20493;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complete verification of deep neural networks (DNNs) can exactly determine whether the DNN satisfies a desired trustworthy property (e.g., robustness, fairness) on an infinite set of inputs or not. Despite the tremendous progress to improve the scalability of complete verifiers over the years on individual DNNs, they are inherently inefficient when a deployed DNN is updated to improve its inference speed or accuracy. The inefficiency is because the expensive verifier needs to be run from scratch on the updated DNN. To improve efficiency, we propose a new, general framework for incremental and complete DNN verification based on the design of novel theory, data structure, and algorithms. Our contributions implemented in a tool named IVAN yield an overall geometric mean speedup of 2.4x for verifying challenging MNIST and CIFAR10 classifiers and a geometric mean speedup of 3.8x for the ACAS-XU classifiers over the state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FITCARL&#30340;TKGC&#26041;&#27861;&#65292;&#23427;&#23558;&#23569;&#26679;&#26412;&#23398;&#20064;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#24402;&#32435;&#23398;&#20064;&#21644;&#39044;&#27979;&#12290;&#22312;FITCARL&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#32593;&#32476;&#26469;&#25351;&#23548;&#25628;&#32034;&#36807;&#31243;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#26679;&#26412;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.00613</link><description>&lt;p&gt;
&#20351;&#29992;&#32622;&#20449;&#24230;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#25913;&#36827;&#23569;&#26679;&#26412;&#24402;&#32435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using Confidence-Augmented Reinforcement Learning. (arXiv:2304.00613v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FITCARL&#30340;TKGC&#26041;&#27861;&#65292;&#23427;&#23558;&#23569;&#26679;&#26412;&#23398;&#20064;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#24402;&#32435;&#23398;&#20064;&#21644;&#39044;&#27979;&#12290;&#22312;FITCARL&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#32593;&#32476;&#26469;&#25351;&#23548;&#25628;&#32034;&#36807;&#31243;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#26679;&#26412;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#65288;TKGC&#65289;&#26088;&#22312;&#39044;&#27979;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#20013;&#23454;&#20307;&#20043;&#38388;&#32570;&#22833;&#30340;&#32852;&#31995;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;TKGC&#26041;&#27861;&#20165;&#32771;&#34385;&#22312;&#35757;&#32451;&#38598;&#20013;&#30475;&#21040;&#30340;&#23454;&#20307;&#20043;&#38388;&#39044;&#27979;&#32570;&#22833;&#30340;&#32852;&#31995;&#65292;&#32780;&#26080;&#27861;&#22312;&#20851;&#20110;&#26032;&#20986;&#29616;&#30340;&#26410;&#35265;&#23454;&#20307;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;TKG&#23569;&#26679;&#26412;&#22330;&#26223;&#22806;&#38142;&#25509;&#39044;&#27979;&#65292;&#20854;&#20013;TKGC&#27169;&#22411;&#38656;&#35201;&#22312;&#20851;&#20110;&#20165;&#26377;&#23569;&#37327;&#35266;&#23519;&#26679;&#26412;&#30340;&#26032;&#20986;&#29616;&#23454;&#20307;&#26041;&#38754;&#23454;&#29616;&#33391;&#22909;&#30340;&#38142;&#25509;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FITCARL&#30340;TKGC&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23569;&#26679;&#26412;&#23398;&#20064;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;FITCARL&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#36328;&#36234;&#25972;&#20010;TKG&#23547;&#25214;&#39044;&#27979;&#31572;&#26696;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31574;&#30053;&#32593;&#32476;&#65292;&#26681;&#25454;&#36941;&#21382;&#30340;&#36335;&#24452;&#25351;&#23548;&#25628;&#32034;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22359;&#65292;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#26469;&#22686;&#21152;&#35757;&#32451;&#38598;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;FITCARL&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#22806;&#38142;&#25509;&#39044;&#27979;&#21644;&#20256;&#32479;TKGC&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal knowledge graph completion (TKGC) aims to predict the missing links among the entities in a temporal knwoledge graph (TKG). Most previous TKGC methods only consider predicting the missing links among the entities seen in the training set, while they are unable to achieve great performance in link prediction concerning newly-emerged unseen entities. Recently, a new task, i.e., TKG few-shot out-of-graph (OOG) link prediction, is proposed, where TKGC models are required to achieve great link prediction performance concerning newly-emerged entities that only have few-shot observed examples. In this work, we propose a TKGC method FITCARL that combines few-shot learning with reinforcement learning to solve this task. In FITCARL, an agent traverses through the whole TKG to search for the prediction answer. A policy network is designed to guide the search process based on the traversed path. To better address the data scarcity problem in the few-shot setting, we introduce a module tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#20013;&#25552;&#21462;3D&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23039;&#24577;&#30340;2D GAN&#30340;&#39640;&#25928;3D&#24863;&#30693;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12865</link><description>&lt;p&gt;
NeRF-GAN&#33976;&#39311;&#65306;&#22522;&#20110;&#21367;&#31215;&#30340;&#39640;&#25928;3D&#24863;&#30693;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions. (arXiv:2303.12865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#20013;&#25552;&#21462;3D&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23039;&#24577;&#30340;2D GAN&#30340;&#39640;&#25928;3D&#24863;&#30693;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23039;&#24577;&#30340;&#21367;&#31215;&#29983;&#25104;&#27169;&#22411;&#22312;&#20174;&#21333;&#35270;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;3D&#19968;&#33268;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#36275;&#22815;&#30340;3D&#20808;&#39564;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;&#23558;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#31561;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#20013;&#29983;&#25104;3D&#24863;&#30693;&#22270;&#20687;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;NeRF-GAN&#21033;&#29992;&#20102;&#19977;&#32500;&#31070;&#32463;&#34920;&#31034;&#21644;&#20307;&#31215;&#28210;&#26579;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#20013;&#25552;&#21462;3D&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#37325;&#23457;&#22522;&#20110;&#23039;&#24577;&#30340;&#20108;&#32500;GAN&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#23454;&#29616;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#22312;&#22522;&#20110;&#23039;&#24577;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#37325;&#29992;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#30340;&#33391;&#22909;&#35299;&#32806;&#28508;&#22312;&#31354;&#38388;&#65292;&#30452;&#25509;&#29983;&#25104;&#19982;&#28508;&#22312;&#30340;3D&#34920;&#36798;&#30456;&#23545;&#24212;&#30340;3D&#19968;&#33268;&#22270;&#29255;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of 3D neural representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21152;&#36895;&#20840;&#33145;MRI&#25195;&#25551;&#30340;&#26032;&#26041;&#27861;GLADE&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#26144;&#23556;&#25439;&#22833;&#26469;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;3D&#33145;&#37096;MR&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.11831</link><description>&lt;p&gt;
GLADE&#65306;&#29992;&#20110;&#38750;&#37197;&#23545;&#36229;&#20998;&#36776;&#29575;&#21508;&#21521;&#24322;&#24615;MRI&#30340;&#26799;&#24230;&#25439;&#22833;&#22686;&#24378;&#36864;&#21270;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic MRI. (arXiv:2303.11831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21152;&#36895;&#20840;&#33145;MRI&#25195;&#25551;&#30340;&#26032;&#26041;&#27861;GLADE&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#26144;&#23556;&#25439;&#22833;&#26469;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;3D&#33145;&#37096;MR&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#38750;&#37197;&#23545;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21508;&#21521;&#24322;&#24615;3D&#22270;&#20687;&#20013;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;3D&#33145;&#37096;MR&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;CycleGAN&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#26144;&#23556;&#25439;&#22833;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#21508;&#21521;&#24322;&#24615;&#20307;&#31215;&#39640;&#20998;&#36776;&#29575;&#65288;&#38754;&#20869;&#65289;&#25968;&#25454;&#30340;&#19981;&#37325;&#21472;&#30340;&#34917;&#19969;&#65292;&#24378;&#21046;&#32593;&#32476;&#29983;&#25104;&#22120;&#22686;&#21152;&#20302;&#20998;&#36776;&#29575;&#65288;&#38754;&#22806;&#65289;&#20999;&#29255;&#30340;&#20998;&#36776;&#29575;&#12290;&#36825;&#23558;&#20351;&#22312;&#30701;&#26102;&#38388;&#20869;&#20197;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;&#22270;&#20687;&#36827;&#34892;&#20840;&#33145;&#25195;&#25551;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to synthesise high-resolution isotropic 3D abdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a modified CycleGAN architecture with a gradient mapping loss, we leverage disjoint patches from the high-resolution (in-plane) data of an anisotropic volume to enforce the network generator to increase the resolution of the low-resolution (through-plane) slices. This will enable accelerated whole-abdomen scanning with high-resolution isotropic images within short breath-hold times.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.10158</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32508;&#36848;&#65306;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#20135;&#29983;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#21487;&#29992;&#20110;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20016;&#23500;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#22312;AI&#20013;&#30340;&#20316;&#29992;&#24471;&#21040;&#20102;&#26174;&#33879;&#25918;&#22823;&#65292;&#24341;&#21457;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#36825;&#19968;&#26032;&#20852;&#27010;&#24565;&#30340;&#20986;&#29616;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#27880;&#24847;&#21147;&#36880;&#28176;&#20174;&#25512;&#36827;&#27169;&#22411;&#35774;&#35745;&#36716;&#21521;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#30340;&#24517;&#35201;&#24615;&#65292;&#38543;&#21518;&#20174;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#29702;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#20197;&#21450;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#25105;&#20204;&#36824;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#35752;&#35770;&#20102;&#25361;&#25112;&#65292;&#24182;&#21015;&#20986;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#25552;&#20379;&#36328;&#36234;&#21508;&#20010;&#38454;&#27573;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#20840;&#29699;&#35270;&#35282;&#30340;&#32508;&#21512;&#24615;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#40664;&#40664;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#21487;&#20449;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#33021;&#21147;&#21306;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#34913;&#37327;&#26080;&#33021;&#65292;&#22686;&#21152;&#26080;&#33021;&#24471;&#20998;&#20250;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09989</link><description>&lt;p&gt;
&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#25214;&#21040;&#33021;&#21147;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Competence Regions in Domain Generalization. (arXiv:2303.09989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#40664;&#40664;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#21487;&#20449;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#33021;&#21147;&#21306;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#34913;&#37327;&#26080;&#33021;&#65292;&#22686;&#21152;&#26080;&#33021;&#24471;&#20998;&#20250;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#40664;&#40664;&#22833;&#36133;&#30340;&#38382;&#39064;&#65292;&#21363;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#20551;&#35774;&#26377;&#19968;&#20010;&#28201;&#21644;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27169;&#22411;&#20272;&#35745;&#30340;&#33021;&#21147;&#39044;&#31034;&#30528;&#21487;&#20449;&#21709;&#24212;&#26102;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#25298;&#32477;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#21487;&#20449;&#24230;&#36890;&#36807;&#19982;&#20998;&#31867;&#22120;&#24615;&#33021;&#23494;&#20999;&#30456;&#20851;&#30340;&#20195;&#29702;&#26080;&#33021;&#20998;&#25968;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#20998;&#31867;&#30340;&#26080;&#33021;&#24471;&#20998;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#24378;&#35843;&#20102;&#25298;&#32477;&#29575;&#19982;&#20934;&#30830;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#26631;&#20934;&#39046;&#22495;&#27867;&#21270;&#22522;&#20934;&#65292;&#24182;&#32771;&#34385;&#22312;&#38381;&#21512;&#21644;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#26469;&#34913;&#37327;&#26080;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#26080;&#33021;&#20998;&#25968;&#30830;&#23454;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#26174;&#30528;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We propose a "learning to reject" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21521;&#37327;&#21270;&#25216;&#26415;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#23884;&#20837;&#26041;&#26696;&#20855;&#26377;Hamming&#36317;&#31163;&#24847;&#20041;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23450;&#37327;&#36793;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20013;&#30340;&#24120;&#25968;&#21463;&#25991;&#26723;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.07203</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#21521;&#37327;&#21270;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Text Vectorizers. (arXiv:2303.07203v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21521;&#37327;&#21270;&#25216;&#26415;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#23884;&#20837;&#26041;&#26696;&#20855;&#26377;Hamming&#36317;&#31163;&#24847;&#20041;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23450;&#37327;&#36793;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20013;&#30340;&#24120;&#25968;&#21463;&#25991;&#26723;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#27169;&#22411;&#23545;&#36755;&#20837;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#31532;&#19968;&#23618;&#23884;&#20837;&#65292;&#23558;&#35789;&#27719;&#24207;&#21015;&#36716;&#25442;&#20026;&#21521;&#37327;&#34920;&#31034;&#12290;&#34429;&#28982;&#36830;&#32493;&#36755;&#20837;&#30340;&#31283;&#20581;&#24615;&#24050;&#32463;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#65292;&#20294;&#32771;&#34385;&#21040;&#31163;&#25955;&#21464;&#21270;(&#27604;&#22914;&#26367;&#25442;&#21477;&#23376;&#20013;&#30340;&#19968;&#20010;&#35789;)&#65292;&#24773;&#20917;&#23601;&#19981;&#37027;&#20040;&#26126;&#30830;&#20102;&#12290;&#26412;&#25991;&#27491;&#24335;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#23884;&#20837;&#26041;&#26696;(&#22914;&#25340;&#25509;&#12289;TF-IDF&#12289;&#27573;&#33853;&#21521;&#37327;)&#22312;Hamming&#36317;&#31163;&#24847;&#20041;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20026;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#23450;&#37327;&#36793;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20013;&#30340;&#24120;&#25968;&#22914;&#20309;&#21463;&#25991;&#26723;&#38271;&#24230;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#20363;&#21152;&#20197;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental issue in machine learning is the robustness of the model with respect to changes in the input. In natural language processing, models typically contain a first embedding layer, transforming a sequence of tokens into vector representations. While the robustness with respect to changes of continuous inputs is well-understood, the situation is less clear when considering discrete changes, for instance replacing a word by another in an input sentence. Our work formally proves that popular embedding schemes, such as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit robustness in the H\"older or Lipschitz sense with respect to the Hamming distance. We provide quantitative bounds for these schemes and demonstrate how the constants involved are affected by the length of the document. These findings are exemplified through a series of numerical examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexGen&#30340;&#25216;&#26415;&#65292;&#20351;&#29992;&#21333;&#20010;GPU&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#21534;&#21520;&#25512;&#29702;&#12290;FlexGen&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;GPU&#12289;CPU&#21644;&#30913;&#30424;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#65292;&#25628;&#32034;&#26377;&#25928;&#30340;&#24352;&#37327;&#23384;&#20648;&#21644;&#35775;&#38382;&#27169;&#24335;&#65292;&#24182;&#23558;&#26435;&#37325;&#21644;&#27880;&#24847;&#21147;&#32531;&#23384;&#21387;&#32553;&#20026;4&#20010;&#20301;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;FlexGen&#20855;&#26377;&#26356;&#22823;&#30340;&#25209;&#37327;&#36873;&#25321;&#31354;&#38388;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#21152;&#20102;&#26368;&#22823;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06865</link><description>&lt;p&gt;
&#21333;&#20010;GPU&#30340;&#39640;&#21534;&#21520;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#25216;&#26415;&#8212;&#8212;FlexGen
&lt;/p&gt;
&lt;p&gt;
FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. (arXiv:2303.06865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexGen&#30340;&#25216;&#26415;&#65292;&#20351;&#29992;&#21333;&#20010;GPU&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#21534;&#21520;&#25512;&#29702;&#12290;FlexGen&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;GPU&#12289;CPU&#21644;&#30913;&#30424;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#65292;&#25628;&#32034;&#26377;&#25928;&#30340;&#24352;&#37327;&#23384;&#20648;&#21644;&#35775;&#38382;&#27169;&#24335;&#65292;&#24182;&#23558;&#26435;&#37325;&#21644;&#27880;&#24847;&#21147;&#32531;&#23384;&#21387;&#32553;&#20026;4&#20010;&#20301;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;FlexGen&#20855;&#26377;&#26356;&#22823;&#30340;&#25209;&#37327;&#36873;&#25321;&#31354;&#38388;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#21152;&#20102;&#26368;&#22823;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#20351;&#20854;&#21482;&#33021;&#22312;&#22810;&#20010;&#39640;&#31471;&#21152;&#36895;&#22120;&#19978;&#23454;&#29616;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#23545;&#25209;&#22788;&#29702;&#19981;&#25935;&#24863;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#36164;&#28304;&#65288;&#22914;&#21333;&#20010;&#26222;&#36890;GPU&#65289;&#36827;&#34892;&#39640;&#21534;&#21520;&#37327;LLM&#25512;&#29702;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlexGen&#65292;&#19968;&#31181;&#29992;&#20110;&#36816;&#34892;&#20855;&#26377;GPU&#20869;&#23384;&#38480;&#21046;&#30340;LLMs&#30340;&#39640;&#21534;&#21520;&#37327;&#29983;&#25104;&#24341;&#25806;&#12290;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;GPU&#12289;CPU&#21644;&#30913;&#30424;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#30828;&#20214;&#36164;&#28304;&#32422;&#26463;&#19979;&#28789;&#27963;&#37197;&#32622;FlexGen&#12290;&#36890;&#36807;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#25628;&#32034;&#26377;&#25928;&#30340;&#24352;&#37327;&#23384;&#20648;&#21644;&#35775;&#38382;&#27169;&#24335;&#12290;FlexGen&#36824;&#23558;&#26435;&#37325;&#21644;&#27880;&#24847;&#21147;&#32531;&#23384;&#21387;&#32553;&#20026;4&#20010;&#20301;&#65292;&#20960;&#20046;&#27809;&#26377;&#31934;&#24230;&#25439;&#22833;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;FlexGen&#20855;&#26377;&#26356;&#22823;&#30340;&#25209;&#37327;&#36873;&#25321;&#31354;&#38388;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#21152;&#20102;&#26368;&#22823;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#65292;&#24403;&#22312;&#21333;&#20010;16GB GPU&#19978;&#36816;&#34892;OPT-175B&#26102;&#65292;FlexGen&#30340;&#21534;&#21520;&#37327;&#20026;&#27599;&#31186;90&#20010;&#24207;&#21015;&#65292;&#27604;Megatron-LM&#24555;4.5&#20493;&#65292;&#27604;&#20351;&#29992;&#21333;&#20010;&#26222;&#36890;GPU&#30340;&#31454;&#20105;&#32773;&#24555;7.5-19&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06833</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#20989;&#25968;&#20540;&#26597;&#25214;&#20854;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;&#20110;&#29983;&#25104;&#26041;&#31243;&#24207;&#21015;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#33719;&#30410;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#27604;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20851;&#27880;&#30340;&#26159;&#20511;&#37492;&#25991;&#26412;&#29983;&#25104;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#24573;&#30053;&#20102;&#26041;&#31243;&#30340;&#29305;&#23450;&#30446;&#26631;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TPSR&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#34701;&#20837;&#21040;Transformer&#35299;&#30721;&#36807;&#31243;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#21516;&#65292;TPSR&#20801;&#35768;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#65288;&#22914;&#25311;&#21512;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65289;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36229;&#29615;&#38754;&#19978;&#24314;&#31435;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20869;&#22312;&#30340;&#26680;&#30456;&#20851;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#23450;&#20041;&#22312;&#36229;&#29615;&#38754;&#19978;&#30340;&#21521;&#37327;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992; HvM-based GP &#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#36882;&#24402;&#23450;&#20301;&#65292;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36319;&#36394;&#31934;&#24230;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.06799</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#21521;&#27969;&#24418;&#31215;&#30340;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process on the Product of Directional Manifolds. (arXiv:2303.06799v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36229;&#29615;&#38754;&#19978;&#24314;&#31435;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20869;&#22312;&#30340;&#26680;&#30456;&#20851;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#23450;&#20041;&#22312;&#36229;&#29615;&#38754;&#19978;&#30340;&#21521;&#37327;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992; HvM-based GP &#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#36882;&#24402;&#23450;&#20301;&#65292;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36319;&#36394;&#31934;&#24230;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26041;&#21521;&#27969;&#24418;&#31215;&#19978;&#24314;&#31435;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110; von Mises &#20998;&#24067;&#30340;&#24490;&#29615;&#26680;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;&#36229;&#29615;&#32500; von Mises&#65288;HvM&#65289;&#26680;&#65292;&#20197;&#32771;&#34385;&#24490;&#29615;&#20851;&#32852;&#32452;&#20214;&#26469;&#24314;&#31435;&#36229;&#29615;&#38754;&#19978;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#36890;&#36807;&#20351;&#29992;&#20869;&#22312;&#30340;&#26680;&#30456;&#20851;&#27169;&#22411;&#65292;&#36816;&#29992;&#22810;&#36755;&#20986; GP &#22238;&#24402;&#36827;&#34892;&#23398;&#20064;&#65292;&#29992;&#20110;&#23450;&#20041;&#22312;&#36229;&#29615;&#38754;&#19978;&#30340;&#21521;&#37327;&#20540;&#20989;&#25968;&#12290;&#20026;&#36816;&#34892;&#26102;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#20998;&#26512;&#23548;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#22522;&#20110;&#36317;&#31163;&#30340;&#20256;&#24863;&#22120;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992; HvM-based GP &#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#36882;&#24402;&#23450;&#20301;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21442;&#25968;&#27169;&#22411;&#21644;&#22522;&#20110;&#20256;&#32479;&#26680;&#35774;&#35745;&#30340;&#39640;&#26031;&#36807;&#31243;&#30456;&#27604;&#65292;HvM-based GP &#23454;&#29616;&#20102;&#26356;&#20248;&#30340;&#36319;&#36394;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a principled study on establishing Gaussian processes (GPs) with inputs on the product of directional manifolds. A circular kernel is first presented according to the von Mises distribution. Based thereon, the so-called hypertoroidal von Mises (HvM) kernel is proposed to establish GPs on hypertori with consideration of correlational circular components. The proposed HvM kernel is demonstrated with multi-output GP regression for learning vector-valued functions defined on hypertori using the intrinsic coregionalization model. Analytical derivatives in hyperparameter optimization are provided for runtime-critical applications. For evaluation, we synthesize a ranging-based sensor network and employ the HvM-based GPs for data-driven recursive localization. The numerical results show that the HvM-based GP achieves superior tracking accuracy compared to parametric model and GPs based on conventional kernel designs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP-Fast MH&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.06171</link><description>&lt;p&gt;
DP-Fast MH: &#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#31169;&#26377;&#12289;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;Metropolis-Hastings&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP-Fast MH&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new DP-Fast MH algorithm for large-scale Bayesian inference, which is accurate, fast, and privacy-preserving.
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#23427;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#33647;&#29289;&#35774;&#35745;&#21644;&#25919;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#20123;&#24120;&#35265;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#25935;&#24863;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25552;&#20379;&#20102;&#20855;&#26377;&#24378;&#22823;&#26368;&#22351;&#24773;&#20917;&#38544;&#31169;&#20445;&#35777;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#24182;&#24050;&#21457;&#23637;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20998;&#26512;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Metropolis-Hastings&#65288;MH&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#26368;&#22522;&#26412;&#30340;MCMC&#26041;&#27861;&#20043;&#19968;&#65292;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31169;&#26377;MCMC&#31639;&#27861;&#20026;&#20102;&#33719;&#24471;&#38544;&#31169;&#32780;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31934;&#30830;&#19988;&#24555;&#36895;&#30340;DP MH&#31639;&#27861;&#65292;&#22823;&#22810;&#25968;&#36845;&#20195;&#20013;&#20165;&#20351;&#29992;&#19968;&#20010;&#23567;&#25209;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#38544;&#31169;&#12289;&#21487;&#25193;&#23637;&#24615;&#65288;&#21363;&#25209;&#37327;&#22823;&#23567;&#65289;&#21644;&#25928;&#29575;&#65288;&#21363;&#25910;&#25947;&#36895;&#24230;&#65289;&#20043;&#38388;&#30340;&#19977;&#37325;&#26435;&#34913;&#65292;&#20174;&#29702;&#35770;&#19978;&#35828;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, the data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically char
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;Wasserstein Belief Updater&#8221;&#31639;&#27861;&#26469;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#65292;&#23454;&#29616;&#20102;&#23545;&#21382;&#21490;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03284</link><description>&lt;p&gt;
"Wasserstein Believer:&#36890;&#36807;&#21487;&#38752;&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;"
&lt;/p&gt;
&lt;p&gt;
The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;Wasserstein Belief Updater&#8221;&#31639;&#27861;&#26469;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#65292;&#23454;&#29616;&#20102;&#23545;&#21382;&#21490;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#26159;&#24314;&#27169;&#20195;&#29702;&#26080;&#27861;&#24863;&#30693;&#21040;&#23436;&#25972;&#29366;&#24577;&#30340;&#29615;&#22659;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#38656;&#35201;&#32771;&#34385;&#36807;&#21435;&#30340;&#35266;&#23519;&#21644;&#34892;&#21160;&#36827;&#34892;&#25512;&#29702;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#21382;&#21490;&#31354;&#38388;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20165;&#20165;&#35760;&#20303;&#23436;&#25972;&#21382;&#21490;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20445;&#25345;&#27169;&#25311;&#30495;&#23454;&#29366;&#24577;&#30340;&#32622;&#20449;&#27010;&#29575;&#20998;&#24067;&#21487;&#20197;&#20316;&#20026;&#21382;&#21490;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#20294;&#20854;&#35745;&#31639;&#38656;&#35201;&#35775;&#38382;&#29615;&#22659;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#20063;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#21387;&#32553;&#35266;&#23519;-&#34892;&#21160;&#21382;&#21490;&#20197;&#23398;&#20064;&#20805;&#20998;&#30340;&#32479;&#35745;&#37327;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#25104;&#21151;&#30340;&#20445;&#35777;&#24182;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#31574;&#30053;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Wasserstein Belief Updater &#65292;&#36825;&#26159;&#19968;&#31181;RL&#31639;&#27861;&#65292;&#23427;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Markov Decision Processes (POMDPs) are useful tools to model environments where the full state cannot be perceived by an agent. As such the agent needs to reason taking into account the past observations and actions. However, simply remembering the full history is generally intractable due to the exponential growth in the history space. Keeping a probability distribution that models the belief over what the true state is can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is also intractable. State-of-the-art algorithms use Recurrent Neural Networks to compress the observation-action history aiming to learn a sufficient statistic, but they lack guarantees of success and can lead to sub-optimal policies. To overcome this, we propose the Wasserstein Belief Updater, an RL algorithm that learns a latent model of the POMDP and an approximation of the belief update. Our approach comes with theoreti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;-GlucoSynth&#65292;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#26469;&#29983;&#25104;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#65292;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#23433;&#20840;&#30340;&#21516;&#26102;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;.</title><link>http://arxiv.org/abs/2303.01621</link><description>&lt;p&gt;
GlucoSynth&#65306;&#29983;&#25104;&#24046;&#20998;&#31169;&#26377;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces. (arXiv:2303.01621v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;-GlucoSynth&#65292;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#26469;&#29983;&#25104;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#65292;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#23433;&#20840;&#30340;&#21516;&#26102;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#31169;&#26377;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#30340;&#38382;&#39064;&#65292;&#36825;&#20010;&#20219;&#21153;&#21487;&#25512;&#24191;&#21040;&#35768;&#22810;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#34880;&#31958;&#25968;&#25454;&#30340;&#20808;&#22825;&#29305;&#24449;&#65292;&#20063;&#26080;&#27861;&#22312;&#19981;&#20005;&#37325;&#38477;&#20302;&#21512;&#25104;&#25968;&#25454;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20219;&#20309;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GlucoSynth&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;GAN&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#32771;&#34385;&#26102;&#24207;&#21160;&#24577;&#30340;&#21516;&#26102;&#65292;&#20445;&#30041;&#36712;&#36857;&#20013;motif&#65288;&#34880;&#31958;&#20107;&#20214;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;120&#19975;&#26465;&#34880;&#31958;&#36712;&#36857;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65307;GlucoSynth&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;DART&#31574;&#30053;&#65292;&#20854;&#20013;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26041;&#27861;&#35757;&#32451;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#32858;&#21512;&#36825;&#20123;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#32467;&#21512;&#20854;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#37325;&#22797;&#32858;&#21512;&#27493;&#39588;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.14685</link><description>&lt;p&gt;
DART: &#22810;&#26679;&#21270;&#32858;&#21512;&#37325;&#22797;&#35757;&#32451;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks. (arXiv:2302.14685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;DART&#31574;&#30053;&#65292;&#20854;&#20013;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26041;&#27861;&#35757;&#32451;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#32858;&#21512;&#36825;&#20123;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#32467;&#21512;&#20854;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#37325;&#22797;&#32858;&#21512;&#27493;&#39588;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#12290;&#25913;&#36827;&#27867;&#21270;&#30340;&#24120;&#35265;&#35757;&#32451;&#31574;&#30053;&#21253;&#25324;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#12289;&#38598;&#25104;&#21644;&#27169;&#22411;&#24179;&#22343;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#24778;&#20154;&#31616;&#21333;&#20294;&#24378;&#26377;&#21147;&#30340;&#27867;&#21270;&#22522;&#20934;&#65292;&#23427;&#21033;&#29992;&#20102;&#35757;&#32451;&#23567;&#25209;&#37327;&#20013;&#30340;&#22810;&#31181;&#19981;&#21516;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36825;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#26356;&#24179;&#34913;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diversify-Aggregate-Repeat Training (DART)&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#20351;&#29992;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;(&#25110;&#39046;&#22495;)&#35757;&#32451;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#25439;&#22833;&#30406;&#22320;&#65292;&#24182;&#36827;&#19968;&#27493;&#32858;&#21512;&#23427;&#20204;&#30340;&#26435;&#37325;&#65292;&#32467;&#21512;&#23427;&#20204;&#30340;&#19987;&#19994;&#30693;&#35782;&#24182;&#33719;&#24471;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#22797;&#32858;&#21512;&#27493;&#39588;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#20248;&#21270;&#36712;&#36857;&#65292;&#24182;&#30830;&#20445;&#21333;&#20010;&#27169;&#22411;&#20855;&#26377;&#36275;&#22815;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#65292;&#22312;&#23558;&#23427;&#20204;&#32452;&#21512;&#26102;&#21487;&#20197;&#33719;&#24471;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#24418;&#24335;&#65292;&#23427;&#36843;&#20351;&#27169;&#22411;&#25506;&#32034;&#25439;&#22833;&#26223;&#35266;&#30340;&#22810;&#31181;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DART&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;ImageNet&#21644;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization of neural networks is crucial for deploying them safely in the real world. Common training strategies to improve generalization involve the use of data augmentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong benchmark for generalization which utilizes diverse augmentations within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different augmentations (or domains) to explore the loss basin, and further Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the overall optimization trajectory and also ensures that the individual models have a sufficiently low loss barrier to obtain improved generalization on combining them. We shed light on our approach by 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#23454;&#38469;&#25968;&#25454;&#21644;&#22312;&#32447;&#27169;&#25311;&#25968;&#25454;&#65292;&#23558;&#21453;&#21521; L_1 &#27491;&#21017;&#21270;&#29992;&#20110;&#20851;&#38190;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#26377;&#25928;&#22320;&#26597;&#35810;&#26356;&#39640;&#32500;&#24230;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13726</link><description>&lt;p&gt;
(Re)$^2$H2O: &#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#30340;&#21453;&#21521;&#27491;&#21017;&#21270;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
(Re)$^2$H2O: Autonomous Driving Scenario Generation via Reversely Regularized Hybrid Offline-and-Online Reinforcement Learning. (arXiv:2302.13726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13726
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#23454;&#38469;&#25968;&#25454;&#21644;&#22312;&#32447;&#27169;&#25311;&#25968;&#25454;&#65292;&#23558;&#21453;&#21521; L_1 &#27491;&#21017;&#21270;&#29992;&#20110;&#20851;&#38190;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#26377;&#25928;&#22320;&#26597;&#35810;&#26356;&#39640;&#32500;&#24230;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#21450;&#20854;&#24191;&#27867;&#37319;&#29992;&#19968;&#30452;&#34987;&#23492;&#20104;&#21402;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#21487;&#20449;&#30340;&#20840;&#38754;&#27979;&#35797;&#31243;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#20165;&#34892;&#19994;&#38590;&#20197;&#22823;&#35268;&#27169;&#29983;&#20135;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;(AV)&#65292;&#32780;&#19988;&#20844;&#20247;&#21644;&#20915;&#31574;&#32773;&#20063;&#27809;&#26377;&#35828;&#26381;&#25509;&#21463;&#21019;&#26032;&#12290;&#29983;&#25104;&#23545;AV&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#26159;&#27979;&#35797;&#30340;&#37325;&#35201;&#31532;&#19968;&#27493;&#12290;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#33258;&#28982;&#20294;&#36807;&#20110;&#23433;&#20840;&#30340;&#39550;&#39542;&#34892;&#20026;&#65292;&#32780;&#27169;&#25311;&#21017;&#20801;&#35768;&#26080;&#38480;&#21046;&#22320;&#25506;&#32034;&#22810;&#26679;&#21270;&#21644;&#28608;&#36827;&#30340;&#20132;&#36890;&#22330;&#26223;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20013;&#30340;&#26356;&#39640;&#32500;&#24230;&#25628;&#32034;&#31354;&#38388;&#20351;&#24471;&#27809;&#26377;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#20316;&#20026;&#38544;&#24335;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#23558;&#20004;&#32773;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#31163;&#32447;&#23454;&#38469;&#25968;&#25454;&#21644;&#22312;&#32447;&#27169;&#25311;&#25968;&#25454;&#21516;&#26102;&#23398;&#20064;&#29983;&#25104;&#22330;&#26223;&#20284;&#20046;&#26159;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37327;&#36523;&#25171;&#36896;&#20102;&#19968;&#20010;&#31216;&#20026; (Re)$^2$H2O &#30340;&#21453;&#21521;&#27491;&#21017;&#21270;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23454;&#29616;&#22330;&#26223;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31163;&#32447;&#37096;&#20998;&#20351;&#29992;&#21453;&#21521; L_1 &#27491;&#21017;&#21270;&#26469;&#25552;&#21462;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#25351;&#23548;&#20256;&#36755;&#21040;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#28385;&#36275;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#32447;&#37096;&#20998;&#21017;&#36890;&#36807;&#20174;&#31163;&#32447;&#25968;&#25454;&#23398;&#21040;&#30340;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#26597;&#35810;&#26356;&#39640;&#32500;&#24230;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#22330;&#26223;&#12290;&#22312; CARLA &#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving and its widespread adoption have long held tremendous promise. Nevertheless, without a trustworthy and thorough testing procedure, not only does the industry struggle to mass-produce autonomous vehicles (AV), but neither the general public nor policymakers are convinced to accept the innovations. Generating safety-critical scenarios that present significant challenges to AV is an essential first step in testing. Real-world datasets include naturalistic but overly safe driving behaviors, whereas simulation would allow for unrestricted exploration of diverse and aggressive traffic scenarios. Conversely, higher-dimensional searching space in simulation disables efficient scenario generation without real-world data distribution as implicit constraints. In order to marry the benefits of both, it seems appealing to learn to generate scenarios from both offline real-world and online simulation data simultaneously. Therefore, we tailor a Reversely Regularized Hybrid Offline-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;VAE&#27169;&#22411;&#23481;&#37327;&#65292;&#37319;&#29992;&#26080;&#38480;&#28151;&#21512;&#30340;&#38750;&#23545;&#31216;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#20855;&#26377;&#20998;&#24067;&#25311;&#21512;&#33021;&#21147;&#21644;&#35843;&#25972;&#25968;&#25454;&#38544;&#31169;&#32423;&#21035;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11294</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#65306;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation. (arXiv:2302.11294v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;VAE&#27169;&#22411;&#23481;&#37327;&#65292;&#37319;&#29992;&#26080;&#38480;&#28151;&#21512;&#30340;&#38750;&#23545;&#31216;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#20855;&#26377;&#20998;&#24067;&#25311;&#21512;&#33021;&#21147;&#21644;&#35843;&#25972;&#25968;&#25454;&#38544;&#31169;&#32423;&#21035;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#35745;&#31639;&#24314;&#27169;&#26041;&#38754;&#24456;&#39640;&#25928;&#65292;&#20294;&#39640;&#26031;&#20551;&#35774;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#23427;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#23481;&#37327;&#65288;&#21363;&#20998;&#24067;&#26063;&#30340;&#34920;&#36798;&#33021;&#21147;&#65289;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;VAE&#26694;&#26550;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;VAE&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#30001;&#26080;&#38480;&#32452;&#21512;&#30340;&#38750;&#23545;&#31216;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#26500;&#25104;&#65292;&#20855;&#26377;&#36830;&#32493;&#21464;&#37327;&#30340;&#20998;&#24067;&#25311;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#20272;&#35745;&#19968;&#33324;&#20998;&#20301;&#20989;&#25968;&#30340;&#38750;&#21442;&#25968;M-estimator&#30340;&#29305;&#27530;&#24418;&#24335;&#34920;&#31034;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#19982;&#20998;&#20301;&#25968;&#20272;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#36731;&#26494;&#35843;&#25972;&#25968;&#25454;&#38544;&#31169;&#32423;&#21035;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussianity assumption has been consistently criticized as a main limitation of the Variational Autoencoder (VAE), despite its efficiency in computational modeling. In this paper, we propose a new approach that expands the model capacity (i.e., expressive power of distributional family) without sacrificing the computational advantages of the VAE framework. Our VAE model's decoder is composed of an infinite mixture of asymmetric Laplacian distribution, which possesses general distribution fitting capabilities for continuous variables. Our model is represented by a special form of a nonparametric M-estimator for estimating general quantile functions, and we theoretically establish the relevance between the proposed model and quantile estimation. We apply the proposed model to synthetic data generation, and particularly, our model demonstrates superiority in easily adjusting the level of data privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#26435;&#37325;&#24635;&#21644;&#21487;&#33021;&#23567;&#20110;1&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#20316;&#32773;&#25506;&#32034;&#20102;&#26368;&#20248;&#32553;&#23567;&#22240;&#23376;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26412;&#22320;&#21608;&#26399;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#23458;&#25143;&#31471;&#30456;&#24178;&#24615;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#23545;&#32858;&#21512;&#26435;&#37325;&#20197;&#25551;&#32472;&#23458;&#25143;&#31471;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FLLAW&#65289;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#21644;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2302.10911</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting Weighted Aggregation in Federated Learning with Neural Networks. (arXiv:2302.10911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#26435;&#37325;&#24635;&#21644;&#21487;&#33021;&#23567;&#20110;1&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#20316;&#32773;&#25506;&#32034;&#20102;&#26368;&#20248;&#32553;&#23567;&#22240;&#23376;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26412;&#22320;&#21608;&#26399;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#23458;&#25143;&#31471;&#30456;&#24178;&#24615;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#23545;&#32858;&#21512;&#26435;&#37325;&#20197;&#25551;&#32472;&#23458;&#25143;&#31471;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FLLAW&#65289;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#21644;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#23545;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#21152;&#26435;&#32858;&#21512;&#20197;&#29983;&#25104;&#20840;&#23616;&#27169;&#22411;&#65292;&#32858;&#21512;&#26435;&#37325;&#34987;&#26631;&#20934;&#21270;&#65288;&#26435;&#37325;&#21644;&#20026;1&#65289;&#24182;&#19982;&#26412;&#22320;&#25968;&#25454;&#22823;&#23567;&#25104;&#27604;&#20363;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21152;&#26435;&#32858;&#21512;&#36807;&#31243;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;FL&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#26435;&#37325;&#24635;&#21644;&#21487;&#33021;&#23567;&#20110;1&#65292;&#23548;&#33268;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#65288;&#31867;&#20284;&#20110;&#26435;&#37325;&#34928;&#20943;&#65289;&#24182;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20248;&#32553;&#23567;&#22240;&#23376;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26412;&#22320;&#21608;&#26399;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#23545;&#32858;&#21512;&#26435;&#37325;&#20197;&#25551;&#32472;&#23458;&#25143;&#31471;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#23458;&#25143;&#31471;&#30456;&#24178;&#24615;&#26469;&#30740;&#31350;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#28857;&#12290;&#22312;&#36827;&#20837;&#20020;&#30028;&#28857;&#20043;&#21069;&#65292;&#30456;&#24178;&#24615;&#26356;&#39640;&#30340;&#23458;&#25143;&#31471;&#22312;&#27867;&#21270;&#20013;&#21457;&#25381;&#20102;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#19978;&#36848;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20855;&#26377;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FLLAW&#65289;&#65292;&#23427;&#20801;&#35768;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#21644;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FLLAW&#22312;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#25239;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning (FL), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of FL. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients' data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients' importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Before entering the critical point, more coherent clients play more essential roles in generalization. Based on the above insights, we propose an effective method for Federated Learning with Learnable Ag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10724</link><description>&lt;p&gt;
ChatGPT&#65306;&#24212;&#20184;&#21315;&#20107;&#30340;&#19975;&#33021;&#22411; AI&#65292;&#20294;&#26080;&#25152;&#19987;&#31934;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#25512;&#20986;&#20102;&#32842;&#22825;&#29983;&#25104;&#39044;&#35757;&#32451; Transformer&#65288;ChatGPT&#65289;&#65292;&#38761;&#26032;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797; ChatGPT &#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38750;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#35268;&#27169;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#26816;&#39564;&#20102; ChatGPT &#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20219;&#21153;&#29978;&#33267;&#23545;&#20154;&#31867;&#32780;&#35328;&#37117;&#26159;&#20027;&#35266;&#30340;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#12289;&#25915;&#20987;&#24615;&#21644;&#31435;&#22330;&#26816;&#27979;&#12290;&#21478;&#19968;&#20123;&#20219;&#21153;&#21017;&#38656;&#35201;&#26356;&#23458;&#35266;&#30340;&#25512;&#29702;&#65292;&#22914;&#35789;&#20041;&#28040;&#27495;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#23545; GPT-4 &#27169;&#22411;&#22312;&#20116;&#20010;&#36873;&#23450;&#30340; NLP &#20219;&#21153;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#33258;&#21160;&#21270;&#20102; ChatGPT &#21644; GPT-4 &#30340;&#24341;&#23548;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#36229;&#36807; 49k &#20010;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;SOTA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978; ChatGPT &#30340;&#24615;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545; AdaGrad &#27493;&#24133;&#19979;&#30340;SGD&#31639;&#27861;&#30340;&#26356;&#21152;&#20840;&#38754;&#19988;&#26080;&#38480;&#21046;&#24615;&#30340;&#20998;&#26512;&#65292;&#25903;&#25345;&#22810;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#39640;&#27010;&#29575;&#19979;&#22788;&#29702;&#26410;&#30693;&#21442;&#25968;&#21644;&#26080;&#30028;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.08783</link><description>&lt;p&gt;
AdaGrad &#27493;&#24133;&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65306;&#23545;&#26410;&#30693;&#21442;&#25968;&#12289;&#26080;&#30028;&#26799;&#24230;&#21644;&#20223;&#23556;&#26041;&#24046;&#30340;&#20840;&#36866;&#24212;&#24615;&#39640;&#27010;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance. (arXiv:2302.08783v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545; AdaGrad &#27493;&#24133;&#19979;&#30340;SGD&#31639;&#27861;&#30340;&#26356;&#21152;&#20840;&#38754;&#19988;&#26080;&#38480;&#21046;&#24615;&#30340;&#20998;&#26512;&#65292;&#25903;&#25345;&#22810;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#39640;&#27010;&#29575;&#19979;&#22788;&#29702;&#26410;&#30693;&#21442;&#25968;&#21644;&#26080;&#30028;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102; AdaGrad &#27493;&#24133;&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65306;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#33258;&#36866;&#24212; (&#33258;&#35843;&#33410;) &#30340;&#19968;&#38454;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#12290;&#23613;&#31649;&#32463;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#29616;&#26377;&#30340;&#20998;&#26512;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#32570;&#38519;&#65306;&#23427;&#20204;&#35201;&#20040;&#20551;&#23450;&#23545;&#38382;&#39064;&#21442;&#25968;&#26377;&#19968;&#23450;&#30340;&#20102;&#35299;&#65292;&#35201;&#20040;&#35774;&#23450;&#24378;&#30340;&#20840;&#23616;&#21033;&#26222;&#24076;&#33576;&#26465;&#20214;&#65292;&#25110;&#32773;&#19981;&#33021;&#32473;&#20986;&#39640;&#27010;&#29575;&#21487;&#38752;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#20984;&#21644;&#38750;&#20984; (&#24179;&#28369;) &#24773;&#20917;&#19979;&#65292;&#23545;&#36825;&#31181;&#22522;&#26412;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#26080;&#20219;&#20309;&#38480;&#21046;&#22320;&#20998;&#26512;&#65292;&#21478;&#22806;&#25903;&#25345;&#19968;&#33324;&#30340;&#8220;&#20223;&#23556;&#26041;&#24046;&#8221;&#22122;&#22768;&#27169;&#22411;&#65292;&#24182;&#22312;&#20302;&#22122;&#22768;&#21644;&#39640;&#22122;&#22768;&#21306;&#22495;&#20013;&#25552;&#20379;&#38160;&#21033;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Stochastic Gradient Descent with AdaGrad stepsizes: a popular adaptive (self-tuning) method for first-order stochastic optimization. Despite being well studied, existing analyses of this method suffer from various shortcomings: they either assume some knowledge of the problem parameters, impose strong global Lipschitz conditions, or fail to give bounds that hold with high probability. We provide a comprehensive analysis of this basic method without any of these limitations, in both the convex and non-convex (smooth) cases, that additionally supports a general ``affine variance'' noise model and provides sharp rates of convergence in both the low-noise and high-noise~regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#35745;&#31639;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#26089;&#26399;&#25512;&#27979;&#27979;&#28145;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#25317;&#25380;&#20107;&#20214;&#20837;&#21475;&#30340;&#25512;&#25380;&#35782;&#21035;&#65292;&#21253;&#25324;&#20462;&#25913;&#21644;&#35757;&#32451;EfficientNetV2B0&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12289;&#20934;&#30830;&#24555;&#36895;&#30340;&#28145;&#24230;&#20809;&#27969;&#27169;&#22411;&#21450;&#24425;&#33394;&#29615;&#26041;&#27861;&#30340;&#25972;&#21512;&#20197;&#23454;&#26102;&#20998;&#26512;&#35270;&#39057;&#27969;&#24182;&#35782;&#21035;&#25512;&#25380;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#29616;&#22330;&#37319;&#38598;&#25216;&#26415;&#21644;&#20113;&#35745;&#31639;&#29615;&#22659;&#23454;&#26102;&#25910;&#38598;&#20154;&#32676;&#35270;&#39057;&#27969;&#24182;&#25552;&#20379;&#26089;&#26399;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.08237</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20113;&#35745;&#31639;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#25317;&#25380;&#20107;&#20214;&#20837;&#21475;&#26089;&#26399;&#25512;&#27979;&#27979;&#28145;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Cloud-based Deep Learning Framework for Early Detection of Pushing at Crowded Event Entrances. (arXiv:2302.08237v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#35745;&#31639;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#26089;&#26399;&#25512;&#27979;&#27979;&#28145;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#25317;&#25380;&#20107;&#20214;&#20837;&#21475;&#30340;&#25512;&#25380;&#35782;&#21035;&#65292;&#21253;&#25324;&#20462;&#25913;&#21644;&#35757;&#32451;EfficientNetV2B0&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12289;&#20934;&#30830;&#24555;&#36895;&#30340;&#28145;&#24230;&#20809;&#27969;&#27169;&#22411;&#21450;&#24425;&#33394;&#29615;&#26041;&#27861;&#30340;&#25972;&#21512;&#20197;&#23454;&#26102;&#20998;&#26512;&#35270;&#39057;&#27969;&#24182;&#35782;&#21035;&#25512;&#25380;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#29616;&#22330;&#37319;&#38598;&#25216;&#26415;&#21644;&#20113;&#35745;&#31639;&#29615;&#22659;&#23454;&#26102;&#25910;&#38598;&#20154;&#32676;&#35270;&#39057;&#27969;&#24182;&#25552;&#20379;&#26089;&#26399;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20107;&#20214;&#20837;&#21475;&#30340;&#25317;&#25380;&#21487;&#33021;&#23548;&#33268;&#21361;&#24613;&#21644;&#21361;&#21450;&#29983;&#21629;&#30340;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#24403;&#20154;&#20204;&#24320;&#22987;&#30456;&#20114;&#25512;&#25380;&#20197;&#26356;&#24555;&#22320;&#21040;&#36798;&#30446;&#30340;&#22320;&#26102;&#12290;&#33258;&#21160;&#20934;&#30830;&#22320;&#35782;&#21035;&#25512;&#25380;&#34892;&#20026;&#23558;&#26377;&#21161;&#20110;&#32452;&#32455;&#32773;&#21644;&#23433;&#20445;&#37096;&#38431;&#21450;&#26102;&#24178;&#39044;&#21644;&#20943;&#36731;&#21361;&#38505;&#23616;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#35745;&#31639;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#26089;&#26399;&#25512;&#27979;&#27979;&#28145;&#26694;&#26550;&#65292;&#29992;&#20110;&#25317;&#25380;&#20107;&#20214;&#20837;&#21475;&#30340;&#25512;&#25380;&#35782;&#21035;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#20462;&#25913;&#24182;&#35757;&#32451;EfficientNetV2B0&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25509;&#30528;&#23558;&#35813;&#27169;&#22411;&#19982;&#20934;&#30830;&#24555;&#36895;&#30340;&#28145;&#24230;&#20809;&#27969;&#27169;&#22411;&#21450;&#24425;&#33394;&#29615;&#26041;&#27861;&#36827;&#34892;&#25972;&#21512;&#65292;&#23454;&#26102;&#20998;&#26512;&#35270;&#39057;&#27969;&#24182;&#35782;&#21035;&#25512;&#25380;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#29616;&#22330;&#37319;&#38598;&#25216;&#26415;&#21644;&#20113;&#35745;&#31639;&#29615;&#22659;&#23454;&#26102;&#25910;&#38598;&#20154;&#32676;&#35270;&#39057;&#27969;&#24182;&#25552;&#20379;&#26089;&#26399;&#32467;&#26524;&#12290;&#22522;&#20110;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowding at the entrances of large events may lead to critical and life-threatening situations, particularly when people start pushing each other to reach the event faster. Automatic and timely identification of pushing behavior would help organizers and security forces to intervene early and mitigate dangerous situations. In this paper, we propose a cloud-based deep learning framework for automatic early detection of pushing in crowded event entrances. The proposed framework initially modifies and trains the EfficientNetV2B0 Convolutional Neural Network model. Subsequently, it integrates the adapted model with an accurate and fast pre-trained deep optical flow model with the color wheel method to analyze video streams and identify pushing patches in real-time. Moreover, the framework uses live capturing technology and a cloud-based environment to collect video streams of crowds in real-time and provide early-stage results. A novel dataset is generated based on five real-world experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#21050;&#28608;&#24067;&#37324;&#28170;&#25955;&#23556;&#30340;&#20809;&#23376;&#27700;&#24211;&#35745;&#31639;&#24179;&#21488;&#65292;&#33021;&#22815;&#21033;&#29992;&#20809;&#23398;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#24555;&#36895;&#12289;&#20302;&#21151;&#29575;&#21644;&#26356;&#22823;&#24102;&#23485;&#30340;&#23454;&#26102;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07698</link><description>&lt;p&gt;
&#21050;&#28608;&#24067;&#37324;&#28170;&#25955;&#23556;&#39537;&#21160;&#30340;&#20809;&#23376;&#27700;&#24211;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Photonic reservoir computing enabled by stimulated Brillouin scattering. (arXiv:2302.07698v2 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#21050;&#28608;&#24067;&#37324;&#28170;&#25955;&#23556;&#30340;&#20809;&#23376;&#27700;&#24211;&#35745;&#31639;&#24179;&#21488;&#65292;&#33021;&#22815;&#21033;&#29992;&#20809;&#23398;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#24555;&#36895;&#12289;&#20302;&#21151;&#29575;&#21644;&#26356;&#22823;&#24102;&#23485;&#30340;&#23454;&#26102;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25512;&#21160;&#26410;&#26469;&#25216;&#26415;&#30340;&#21019;&#24314;&#65292;&#36825;&#20123;&#25216;&#26415;&#20250;&#25913;&#21464;&#20154;&#31867;&#29983;&#27963;&#21644;&#24037;&#20316;&#30340;&#26041;&#24335;&#65292;&#21019;&#36896;&#20986;&#25913;&#21464;&#25105;&#20204;&#22788;&#29702;&#20219;&#21153;&#21644;&#27963;&#21160;&#26041;&#24335;&#30340;&#26032;&#26041;&#26696;&#65292;&#20294;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#22788;&#29702;&#12289;&#22823;&#37327;&#30340;&#25968;&#25454;&#20256;&#36755;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#20851;&#27880;&#20110;&#24320;&#21457;&#19968;&#31181;&#26032;&#22411;&#35745;&#31639;&#24179;&#21488;&#65292;&#36825;&#31181;&#24179;&#21488;&#21463;&#33041;&#30340;&#26550;&#26500;&#30340;&#21551;&#21457;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21033;&#29992;&#20809;&#23376;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#24555;&#36895;&#12289;&#20302;&#21151;&#29575;&#21644;&#26356;&#22823;&#24102;&#23485;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21050;&#28608;&#24067;&#37324;&#28170;&#25955;&#23556;&#30340;&#20809;&#23376;&#27700;&#24211;&#35745;&#31639;&#26550;&#26500;&#30340;&#26032;&#22411;&#35745;&#31639;&#24179;&#21488;&#12290;&#36825;&#20010;&#26032;&#30340;&#20809;&#23376;&#27700;&#24211;&#35745;&#31639;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#30001;&#19968;&#20010;&#23436;&#20840;&#34987;&#21160;&#20809;&#23398;&#31995;&#32479;&#26500;&#25104;&#30340;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#19982;&#39640;&#24615;&#33021;&#30340;&#20809;&#23398;&#22797;&#29992;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#23454;&#29616;&#23454;&#26102;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) drives the creation of future technologies that disrupt the way humans live and work, creating new solutions that change the way we approach tasks and activities, but it requires a lot of data processing, large amounts of data transfer, and computing speed. It has led to a growing interest of research in developing a new type of computing platform which is inspired by the architecture of the brain specifically those that exploit the benefits offered by photonic technologies, fast, low-power, and larger bandwidth. Here, a new computing platform based on the photonic reservoir computing architecture exploiting the non-linear wave-optical dynamics of the stimulated Brillouin scattering is reported. The kernel of the new photonic reservoir computing system is constructed of an entirely passive optical system. Moreover, it is readily suited for use in conjunction with high performance optical multiplexing techniques to enable real-time artificial intelligence. H
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FITCompress&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#20351;&#29992;&#21098;&#26525;&#21644;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26469;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#36873;&#25321;&#65292;&#20197;&#23454;&#29616;&#26368;&#20248;&#21387;&#32553;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Fisher&#20449;&#24687;&#24230;&#37327;&#21644;&#21387;&#32553;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#21487;&#22312;&#20445;&#25345;&#27169;&#22411;&#21407;&#22987;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#26032;&#39062;&#30340;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2302.07612</link><description>&lt;p&gt;
&#36861;&#27714;&#26368;&#20248;&#21387;&#32553;&#65306;&#32852;&#21512;&#21098;&#26525;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Compression: Joint Pruning and Quantization. (arXiv:2302.07612v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FITCompress&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#20351;&#29992;&#21098;&#26525;&#21644;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26469;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#36873;&#25321;&#65292;&#20197;&#23454;&#29616;&#26368;&#20248;&#21387;&#32553;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Fisher&#20449;&#24687;&#24230;&#37327;&#21644;&#21387;&#32553;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#21487;&#22312;&#20445;&#25345;&#27169;&#22411;&#21407;&#22987;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#26032;&#39062;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21387;&#32553;&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#32593;&#32476;&#21387;&#32553;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#21363;&#37327;&#21270;&#21644;&#21098;&#26525;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20250;&#20184;&#20986;&#24615;&#33021;&#30340;&#20195;&#20215;&#12290;&#30830;&#23450;&#36866;&#29992;&#20110;&#21333;&#20010;&#23618;&#21644;&#21442;&#25968;&#30340;&#26368;&#26377;&#25928;&#37327;&#21270;&#21644;&#21098;&#26525;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#35745;&#31639;&#37327;&#26114;&#36149;&#19988;&#29305;&#21035;&#30340;&#25968;&#20540;&#20248;&#21270;&#25216;&#26415;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FITCompress&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#32479;&#19968;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#38598;&#25104;&#20102;&#36880;&#23618;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#12290;&#36890;&#36807;&#21033;&#29992;Fisher&#20449;&#24687;&#24230;&#37327;&#21644;&#21387;&#32553;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;FITCompress&#21487;&#20197;&#20026;&#32473;&#23450;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#21387;&#32553;&#32422;&#26463;&#20248;&#21270;&#36873;&#25321;&#21098;&#26525;&#25513;&#30721;&#21644;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#37197;&#32622;&#30340;&#32452;&#21512;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#29978;&#33267;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21021;&#22987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model compression is instrumental in optimizing deep neural network inference on resource-constrained hardware. The prevailing methods for network compression, namely quantization and pruning, have been shown to enhance efficiency at the cost of performance. Determining the most effective quantization and pruning strategies for individual layers and parameters remains a challenging problem, often requiring computationally expensive and ad hoc numerical optimization techniques. This paper introduces FITCompress, a novel method integrating layer-wise mixed-precision quantization and unstructured pruning using a unified heuristic approach. By leveraging the Fisher Information Metric and path planning through compression space, FITCompress optimally selects a combination of pruning mask and mixed-precision quantization configuration for a given pre-trained model and compression constraint. Experiments on computer vision and natural language processing benchmarks demonstrate that our propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;Bandits&#20013;&#23398;&#20064;&#30340;&#22522;&#26412;&#26497;&#38480;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#21487;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#36807;&#31243;&#21644;&#36890;&#29992;&#19968;&#33268;&#24615;&#31639;&#27861;&#30340;&#29305;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#25239;&#22870;&#21169;&#19979;&#30340;&#20048;&#35266;&#36890;&#29992;&#19968;&#33268;&#24615;&#23398;&#20064;&#30340;&#19981;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07186</link><description>&lt;p&gt;
&#36890;&#29992;&#23398;&#20064;&#20013;&#23545;&#25239;&#22870;&#21169;&#22312;&#19978;&#19979;&#25991;Bandits&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Rewards in Universal Learning for Contextual Bandits. (arXiv:2302.07186v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;Bandits&#20013;&#23398;&#20064;&#30340;&#22522;&#26412;&#26497;&#38480;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#21487;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#36807;&#31243;&#21644;&#36890;&#29992;&#19968;&#33268;&#24615;&#31639;&#27861;&#30340;&#29305;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#25239;&#22870;&#21169;&#19979;&#30340;&#20048;&#35266;&#36890;&#29992;&#19968;&#33268;&#24615;&#23398;&#20064;&#30340;&#19981;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;Bandits&#20013;&#23398;&#20064;&#30340;&#22522;&#26412;&#26497;&#38480;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#21462;&#20915;&#20110;&#20854;&#34892;&#20026;&#21644;&#24050;&#30693;&#19978;&#19979;&#25991;&#65292;&#36825;&#25193;&#23637;&#20102;&#32463;&#20856;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#22312;&#26377;&#38468;&#21152;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#23545;&#33021;&#22815;&#23454;&#29616;&#20122;&#32447;&#24615;&#36951;&#25022;&#30340;&#36890;&#29992;&#19968;&#33268;&#24615;&#31639;&#27861;&#24863;&#20852;&#36259;&#65292;&#30456;&#23545;&#20110;&#20219;&#20309;&#21487;&#27979;&#23450;&#30340;&#22266;&#23450;&#31574;&#30053;&#65292;&#26080;&#38656;&#20219;&#20309;&#21151;&#33021;&#31867;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#26426;&#21046;&#21487;&#20197;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#22870;&#21169;&#19979;&#65292;&#19978;&#19979;&#25991;Bandits&#30340;&#20048;&#35266;&#36890;&#29992;&#19968;&#33268;&#24615;&#23398;&#20064;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fundamental limits of learning in contextual bandits, where a learner's rewards depend on their actions and a known context, which extends the canonical multi-armed bandit to the case where side-information is available. We are interested in universally consistent algorithms, which achieve sublinear regret compared to any measurable fixed policy, without any function class restriction. For stationary contextual bandits, when the underlying reward mechanism is time-invariant, Blanchard et. al (2022) characterized learnable context processes for which universal consistency is achievable; and further gave algorithms ensuring universal consistency whenever this is achievable, a property known as optimistic universal consistency. It is well understood, however, that reward mechanisms can evolve over time, possibly adversarially, and depending on the learner's actions. We show that optimistic universal learning for contextual bandits with adversarial rewards is impossible in gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#65292;&#23427;&#20351;&#29992;&#27985;&#25311;&#22278;&#20915;&#31574;&#36793;&#30028;&#21487;&#20197;&#20248;&#21270;&#27979;&#22320;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#31454;&#20105;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2302.06807</link><description>&lt;p&gt;
&#36229;&#20284;&#26354;&#31354;&#38388;&#30340;&#22823;&#38388;&#38548;&#20998;&#31867;&#30340;&#27985;&#25311;&#22278;&#20915;&#31574;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space. (arXiv:2302.06807v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#65292;&#23427;&#20351;&#29992;&#27985;&#25311;&#22278;&#20915;&#31574;&#36793;&#30028;&#21487;&#20197;&#20248;&#21270;&#27979;&#22320;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#31454;&#20105;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29992;&#36229;&#20284;&#26354;&#31354;&#38388;&#34920;&#31034;&#23618;&#27425;&#32467;&#26500;&#21270;&#25968;&#25454;&#24050;&#32463;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#21516;&#26102;&#65292;&#25991;&#29486;&#20013;&#20063;&#25552;&#20986;&#20102;&#20960;&#20010;&#38024;&#23545;&#36825;&#20123;&#31354;&#38388;&#20013;&#25968;&#25454;&#20998;&#31867;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20027;&#35201;&#20351;&#29992;&#36229;&#24179;&#38754;&#25110;&#27979;&#22320;&#32447;&#20316;&#20026;&#20915;&#31574;&#36793;&#30028;&#65292;&#20351;&#29992;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#35774;&#32622;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27985;&#25311;&#22278;&#20915;&#31574;&#36793;&#30028;&#30340;&#26032;&#22411;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#65292;&#23427;&#21487;&#20197;&#23548;&#33268;&#19968;&#20010;&#27979;&#22320;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#26469;&#20248;&#21270;&#65292;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#20110; SOTA &#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic spaces have been quite popular in the recent past for representing hierarchically organized data. Further, several classification algorithms for data in these spaces have been proposed in the literature. These algorithms mainly use either hyperplanes or geodesics for decision boundaries in a large margin classifiers setting leading to a non-convex optimization problem. In this paper, we propose a novel large margin classifier based on horospherical decision boundaries that leads to a geodesically convex optimization problem that can be optimized using any Riemannian gradient descent technique guaranteeing a globally optimal solution. We present several experiments depicting the competitive performance of our classifier in comparison to SOTA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#27010;&#29575;&#30005;&#36335;&#65288;PC&#65289;&#23545;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#65307;&#36890;&#36807;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#22788;&#29702;&#30340;&#38543;&#26426;&#22833;&#27963;&#25512;&#26029;&#65288;TDI&#65289;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21333;&#20010;&#27491;&#21521;&#20256;&#36882;&#20013;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;PC&#23545;&#20998;&#24067;&#28418;&#31227;&#21644;OOD&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06544</link><description>&lt;p&gt;
&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#27010;&#29575;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Circuits That Know What They Don't Know. (arXiv:2302.06544v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#27010;&#29575;&#30005;&#36335;&#65288;PC&#65289;&#23545;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#65307;&#36890;&#36807;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#22788;&#29702;&#30340;&#38543;&#26426;&#22833;&#27963;&#25512;&#26029;&#65288;TDI&#65289;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21333;&#20010;&#27491;&#21521;&#20256;&#36882;&#20013;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;PC&#23545;&#20998;&#24067;&#28418;&#31227;&#21644;OOD&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#65288;PC&#65289;&#26159;&#19968;&#31181;&#20801;&#35768;&#20934;&#30830;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#23427;&#20204;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#33391;&#22909;&#26657;&#20934;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#34920;&#26126; PC &#23454;&#38469;&#19978;&#19981;&#20855;&#26377;&#23545;OOD&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#36827;&#32780;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#22788;&#29702;&#30340;&#38543;&#26426;&#22833;&#27963;&#25512;&#26029;&#65288;TDI&#65289;&#8212;&#8212;&#19968;&#31181;&#25512;&#26029;&#31243;&#24207;&#65292;&#36890;&#36807;&#26041;&#24046;&#20256;&#25773;&#23548;&#20986;&#33945;&#29305;&#21345;&#27931;&#22833;&#27963;&#65288;MCD&#65289;&#30340;&#35299;&#26512;&#35299;&#26469;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; MCD &#19981;&#21516;&#65292;TDI&#19981;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#32593;&#32476;&#35780;&#20272;&#23601;&#21487;&#20197;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35780;&#20272;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;TDI&#25913;&#21892;&#20102;PC&#23545;&#20998;&#24067;&#28418;&#31227;&#21644;OOD&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic circuits (PCs) are models that allow exact and tractable probabilistic inference. In contrast to neural networks, they are often assumed to be well-calibrated and robust to out-of-distribution (OOD) data. In this paper, we show that PCs are in fact not robust to OOD data, i.e., they don't know what they don't know. We then show how this challenge can be overcome by model uncertainty quantification. To this end, we propose tractable dropout inference (TDI), an inference procedure to estimate uncertainty by deriving an analytical solution to Monte Carlo dropout (MCD) through variance propagation. Unlike MCD in neural networks, which comes at the cost of multiple network evaluations, TDI provides tractable sampling-free uncertainty estimates in a single forward pass. TDI improves the robustness of PCs to distribution shift and OOD data, demonstrated through a series of experiments evaluating the classification confidence and uncertainty estimates on real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#20102;&#19968;&#20010;"&#35748;&#35777;&#24726;&#35770;"&#65292;&#35748;&#35777;&#34429;&#28982;&#21487;&#20197;&#23637;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#39069;&#22806;&#25581;&#31034;&#20102;&#26377;&#20851;&#35748;&#35777;&#27169;&#22411;&#30340;&#20449;&#24687;&#20063;&#25104;&#20026;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#23548;&#33268;&#26356;&#22909;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04379</link><description>&lt;p&gt;
&#35748;&#35777;&#24726;&#35770;: &#35748;&#35777;&#20250;&#25581;&#31034;&#26356;&#22909;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Certification Paradox: Certifications Admit Better Attacks. (arXiv:2302.04379v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#20102;&#19968;&#20010;"&#35748;&#35777;&#24726;&#35770;"&#65292;&#35748;&#35777;&#34429;&#28982;&#21487;&#20197;&#23637;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#39069;&#22806;&#25581;&#31034;&#20102;&#26377;&#20851;&#35748;&#35777;&#27169;&#22411;&#30340;&#20449;&#24687;&#20063;&#25104;&#20026;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#23548;&#33268;&#26356;&#22909;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20445;&#35777;&#26377;&#19968;&#20010;&#26377;&#30028;&#21306;&#22495;&#20869;&#19981;&#23384;&#22312;&#23545;&#25239;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#35748;&#35777;&#26426;&#21046;&#22312;&#23637;&#31034;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;: &#35748;&#35777;&#26159;&#21542;&#20250;&#26377;&#20219;&#20309;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#65292;&#36890;&#36807;&#25581;&#31034;&#26377;&#20851;&#35748;&#35777;&#27169;&#22411;&#30340;&#39069;&#22806;&#20449;&#24687;&#65311;&#25105;&#20204;&#20197;&#32943;&#23450;&#30340;&#31572;&#26696;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#35748;&#35777;&#19981;&#20165;&#27979;&#37327;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#23637;&#29616;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"&#35748;&#35777;&#24863;&#30693;&#25915;&#20987;"&#65292;&#22312;&#38024;&#23545;&#32463;&#36807;&#35748;&#35777;&#30340;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#26102;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#27604;&#20197;&#21069;&#30340;&#20219;&#20309;&#26041;&#27861;&#26356;&#39057;&#32321;&#22320;&#20135;&#29983;&#26356;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23454;&#29616;&#20102;&#26368;&#22810;34%&#30340;&#25200;&#21160;&#35268;&#33539;&#20013;&#20301;&#25968;&#30340;&#20943;&#23567;(&#27604;&#36739;&#30446;&#26631;&#21644;&#25915;&#20987;&#23454;&#20363;)&#65292;&#21516;&#26102;&#38656;&#35201;&#30340;&#35745;&#31639;&#26102;&#38388;&#27604;PGD&#31561;&#26041;&#27861;&#23569;&#20102;90%&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23454;&#29616;&#20102;&#22914;&#27492;&#26174;&#30528;&#30340;&#25200;&#21160;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38477;&#20302;&#65292;&#31361;&#26174;&#20102;&#20197;&#35748;&#35777;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#30340;&#19968;&#31181;&#24726;&#35770;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35748;&#35777;&#19981;&#20165;&#25581;&#31034;&#20102;&#31283;&#20581;&#27169;&#22411;&#30340;&#23646;&#24615;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#29992;&#26469;&#21457;&#36215;&#26356;&#26377;&#25928;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In guaranteeing that no adversarial examples exist within a bounded region, certification mechanisms play an important role in demonstrating the robustness of neural networks. In this work we ask: Could certifications have any unintended consequences, through exposing additional information about certified models? We answer this question in the affirmative, demonstrating that certifications not only measure model robustness but also present a new attack surface. We propose \emph{Certification Aware Attacks}, that produce smaller adversarial perturbations more than twice as frequently as any prior approach, when launched against certified models. Our attacks achieve an up to $34\%$ reduction in the median perturbation norm (comparing target and attack instances), while requiring $90 \%$ less computational time than approaches like PGD. That our attacks achieve such significant reductions in perturbation size and computational cost highlights an apparent paradox in deploying certificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#36172;&#29575;&#21453;&#39304;&#21644;&#26102;&#38388;&#21464;&#21270;&#38656;&#27714;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22312;&#32447;&#31639;&#27861;&#26469;&#22788;&#29702;&#24635;&#38656;&#27714;&#37327;&#30340;&#22312;&#32447;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.04182</link><description>&lt;p&gt;
&#22312;&#26102;&#38388;&#21464;&#21270;&#38656;&#27714;&#19979;&#30340;&#36172;&#29575;&#21453;&#39304;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Online Resource Allocation: Bandits feedback and Advice on Time-varying Demands. (arXiv:2302.04182v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#36172;&#29575;&#21453;&#39304;&#21644;&#26102;&#38388;&#21464;&#21270;&#38656;&#27714;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22312;&#32447;&#31639;&#27861;&#26469;&#22788;&#29702;&#24635;&#38656;&#27714;&#37327;&#30340;&#22312;&#32447;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#24102;&#26377;&#36172;&#29575;&#21453;&#39304;&#21644;&#26102;&#38388;&#21464;&#21270;&#38656;&#27714;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20316;&#21697;&#37117;&#20570;&#20986;&#20102;&#38656;&#27714;&#21040;&#36798;&#36807;&#31243;&#26159;&#31283;&#24577;&#30340;&#24378;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#22312;&#32447;&#24191;&#21578;&#21644;&#25910;&#30410;&#31649;&#29702;&#65292;&#35813;&#36807;&#31243;&#21487;&#33021;&#26159;&#22806;&#29983;&#30340;&#21644;&#38750;&#31283;&#24577;&#30340;&#65292;&#22914;&#19981;&#26029;&#21464;&#21270;&#30340;&#20114;&#32852;&#32593;&#27969;&#37327;&#12290;&#21463;&#26368;&#36817;&#30340;&#24102;&#26377;&#24314;&#35758;&#26694;&#26550;&#30340;&#22312;&#32447;&#31639;&#27861;&#28608;&#21169;[Mitazenmacher&#21644;Vassilvitskii&#65292;\emph {Commun&#12290;ACM} 2022]&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#32447;&#24314;&#35758;&#22914;&#20309;&#36890;&#30693;&#31574;&#30053;&#35774;&#35745;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#21363;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#20219;&#20309;&#31639;&#27861;&#37117;&#20250;&#22312;&#24724;&#24680;&#30340;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#24635;&#38656;&#27714;&#37327;&#30340;&#22312;&#32447;&#39044;&#27979;&#12290;&#22312;&#33719;&#24471;&#22312;&#32447;&#24314;&#35758;&#30340;&#25903;&#25345;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#34987;&#35777;&#26126;&#22312;&#29702;&#35770;&#24615;&#33021;&#21644;&#26377;&#21069;&#36884;&#30340;&#25968;&#20540;&#26041;&#38754;&#37117;&#26377;&#20986;&#33394;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a general online resource allocation model with bandit feedback and time-varying demands. While online resource allocation has been well studied in the literature, most existing works make the strong assumption that the demand arrival process is stationary. In practical applications, such as online advertisement and revenue management, however, this process may be exogenous and non-stationary, like the constantly changing internet traffic. Motivated by the recent Online Algorithms with Advice framework [Mitazenmacher and Vassilvitskii, \emph{Commun. ACM} 2022], we explore how online advice can inform policy design. We establish an impossibility result that any algorithm perform poorly in terms of regret without any advice in our setting. In contrast, we design an robust online algorithm that leverages the online predictions on the total demand volumes. Empowered with online advice, our proposed algorithm is shown to have both theoretical performance and promising numerical 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; Adversarial Minority Influence (AMI) &#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#22810;&#26234;&#33021;&#20307;&#20114;&#21160;&#21644;&#21512;&#20316;&#30446;&#26631;&#19979;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#21512;&#20316;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;&#20986; 2.2&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.03322</link><description>&lt;p&gt;
&#12298;&#36890;&#36807;&#23545;&#25239;&#24615;&#23569;&#25968;&#27966;&#24433;&#21709;&#25915;&#20987;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence. (arXiv:2302.03322v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; Adversarial Minority Influence (AMI) &#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#22810;&#26234;&#33021;&#20307;&#20114;&#21160;&#21644;&#21512;&#20316;&#30446;&#26631;&#19979;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#21512;&#20316;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;&#20986; 2.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(c-MARL)&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#24369;&#28857;&#65292;&#36825;&#26159;&#22312;&#23454;&#38469;&#23454;&#29616;&#20043;&#21069;c-MARL&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;&#30446;&#21069;&#22522;&#20110;&#35266;&#23519;&#30340;&#25915;&#20987;&#65292;&#21463;&#21040;&#30333;&#30418;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#24573;&#35270;&#20102;c-MARL&#30340;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#21644;&#21512;&#20316;&#30446;&#26631;&#65292;&#23548;&#33268;&#25915;&#20987;&#33021;&#21147;&#21463;&#21040;&#23454;&#38469;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Adversarial Minority Influence (AMI)&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#29992;&#32780;&#24378;&#22823;&#30340;&#38024;&#23545;c-MARL&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;AMI&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#19981;&#20102;&#35299;&#21463;&#23475;&#32773;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21457;&#21160;&#25915;&#20987;&#12290;AMI&#20063;&#26159;&#24378;&#22823;&#30340;&#65292;&#22240;&#20026;&#23427;&#32771;&#34385;&#20102;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#20114;&#21160;&#21644;&#20195;&#29702;&#30340;&#21512;&#20316;&#30446;&#26631;&#65292;&#20351;&#21333;&#20010;&#23545;&#25239;&#24615;&#20195;&#29702;&#33021;&#22815;&#21333;&#26041;&#38754;&#35823;&#23548;&#22823;&#22810;&#25968;&#21463;&#23475;&#32773;&#24418;&#25104;&#26377;&#38024;&#23545;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#21512;&#20316;&#12290;&#36825;&#21453;&#26144;&#20102;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#23569;&#25968;&#27966;&#24433;&#21709;&#29616;&#35937;&#12290;&#20026;&#20102;&#22312;&#22797;&#26434;&#30340;&#20195;&#29702;&#26041;&#24335;&#20132;&#20114;&#19979;&#23454;&#29616;&#26368;&#22823;&#30340;&#21463;&#23475;&#32773;&#25919;&#31574;&#20559;&#24046;&#65292;&#25105;&#20204;&#22522;&#20110;&#21338;&#24328;&#29702;&#35770;&#20998;&#26512;&#25512;&#23548;&#20986;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;AMI&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AMI&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#32771;&#34385;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#30340;&#24517;&#35201;&#24615;&#65292;&#34920;&#26126;AMI&#21487;&#20197;&#23454;&#29616;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;&#20986;2.2&#20493;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study probes the vulnerabilities of cooperative multi-agent reinforcement learning (c-MARL) under adversarial attacks, a critical determinant of c-MARL's worst-case performance prior to real-world implementation. Current observation-based attacks, constrained by white-box assumptions, overlook c-MARL's complex multi-agent interactions and cooperative objectives, resulting in impractical and limited attack capabilities. To address these shortcomes, we propose Adversarial Minority Influence (AMI), a practical and strong for c-MARL. AMI is a practical black-box attack and can be launched without knowing victim parameters. AMI is also strong by considering the complex multi-agent interaction and the cooperative goal of agents, enabling a single adversarial agent to unilaterally misleads majority victims to form targeted worst-case cooperation. This mirrors minority influence phenomena in social psychology. To achieve maximum deviation in victim policies under complex agent-wise intera
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#29983;&#25104;&#30340;&#22495;&#32034;&#24341;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#27934;&#23519;&#35270;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02561</link><description>&lt;p&gt;
&#22495;&#32034;&#24341;&#21464;&#20998;&#36125;&#21494;&#26031;&#65306;&#21487;&#35299;&#37322;&#30340;&#22495;&#32034;&#24341;&#29992;&#20110;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation. (arXiv:2302.02561v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#29983;&#25104;&#30340;&#22495;&#32034;&#24341;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#27934;&#23519;&#35270;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22495;&#32034;&#24341;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#24635;&#26159;&#26377;&#36825;&#26679;&#30340;&#22495;&#32034;&#24341;&#21487;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#27010;&#29575;&#35282;&#24230;&#25552;&#20379;&#20102;&#22495;&#32034;&#24341;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#22495;&#32034;&#24341;&#65292;&#20174;&#32780;&#25552;&#20379;&#39069;&#22806;&#30340;&#22495;&#20851;&#31995;&#27934;&#23519;&#65292;&#24182;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#25239;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#22312;&#24179;&#34913;&#28857;&#22788;&#25214;&#21040;&#20102;&#26368;&#20248;&#30340;&#22495;&#32034;&#24341;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#22495;&#32034;&#24341;&#65292;&#20351;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;&#29616;&#26377;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Wang-ML-Lab/VDI&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have shown that leveraging domain index can significantly boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20559;&#22909;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#20559;&#22909;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20559;&#26012;&#24615;&#38382;&#39064;&#24182;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.01513</link><description>&lt;p&gt;
&#23454;&#29992;&#20559;&#22909;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#20559;&#26012;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Preferential Bayesian Optimization with Skew Gaussian Processes. (arXiv:2302.01513v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20559;&#22909;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#20559;&#22909;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20559;&#26012;&#24615;&#38382;&#39064;&#24182;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20559;&#22909;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#20559;&#22909;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#20854;&#20013;&#21487;&#38752;&#30340;&#21453;&#39304;&#20165;&#38480;&#20110;&#31216;&#20026;&#20915;&#26007;&#30340;&#25104;&#23545;&#27604;&#36739;&#12290;&#20559;&#22909;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#34920;&#31034;&#28789;&#27963;&#30340;&#20559;&#22909;&#32467;&#26500;&#65292;&#20294;&#20854;&#21518;&#39564;&#20998;&#24067;&#26159;&#19968;&#20010;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#20559;&#26012;&#39640;&#26031;&#36807;&#31243;&#12290;&#30446;&#21069;&#65292;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#20559;&#22909;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#26159;&#39640;&#26031;&#36817;&#20284;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#24573;&#30053;&#20102;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#20559;&#26012;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25913;&#36827;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#30340;&#20559;&#22909;&#36125;&#21494;&#26031;&#20248;&#21270;&#20272;&#35745;&#65292;&#36890;&#36807;Gibbs&#25277;&#26679;&#21644;&#25512;&#23548;&#20302;&#26041;&#24046;MC&#20272;&#35745;&#22120;&#23637;&#31034;&#20986;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study preferential Bayesian optimization (BO) where reliable feedback is limited to pairwise comparison called duels. An important challenge in preferential BO, which uses the preferential Gaussian process (GP) model to represent flexible preference structure, is that the posterior distribution is a computationally intractable skew GP. The most widely used approach for preferential BO is Gaussian approximation, which ignores the skewness of the true posterior. Alternatively, Markov chain Monte Carlo (MCMC) based preferential BO is also proposed. In this work, we first verify the accuracy of Gaussian approximation, from which we reveal the critical problem that the predictive probability of duels can be inaccurate. This observation motivates us to improve the MCMC-based estimation for skew GP, for which we show the practical efficiency of Gibbs sampling and derive the low variance MC estimator. However, the computational time of MCMC can still be a bottleneck in practice. Towards bui
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#39640;&#26031;&#36807;&#31243;&#19978;&#32622;&#20449;&#30028;&#38480;(IRGP-UCB)&#65292;&#23427;&#20351;&#29992;&#21452;&#21442;&#25968;&#25351;&#25968;&#20998;&#24067;,&#21462;&#24471;&#20102;&#26356;&#32039;&#23494;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#36991;&#20813;&#20102;&#21518;&#26399;&#36807;&#24230;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2302.01511</link><description>&lt;p&gt;
&#38543;&#26426;&#39640;&#26031;&#36807;&#31243;&#19978;&#32622;&#20449;&#30028;&#38480;&#65306;&#26356;&#32039;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Randomized Gaussian Process Upper Confidence Bound with Tighter Bayesian Regret Bounds. (arXiv:2302.01511v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#39640;&#26031;&#36807;&#31243;&#19978;&#32622;&#20449;&#30028;&#38480;(IRGP-UCB)&#65292;&#23427;&#20351;&#29992;&#21452;&#21442;&#25968;&#25351;&#25968;&#20998;&#24067;,&#21462;&#24471;&#20102;&#26356;&#32039;&#23494;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#36991;&#20813;&#20102;&#21518;&#26399;&#36807;&#24230;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#19978;&#32622;&#20449;&#30028;&#38480;(GP-UCB)&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#24456;&#26377;&#21069;&#36884;&#30340;&#40657;&#30418;&#20248;&#21270;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22312;&#23450;&#29702;&#20013;&#32622;&#20449;&#21442;&#25968;$\beta$&#30456;&#24403;&#22823;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20197;&#35797;&#25506;&#24615;&#30340;&#26041;&#24335;&#36873;&#25321;&#12290; &#38543;&#26426;GP-UCB(RGP-UCB)&#20351;&#29992;&#19968;&#20010;&#36981;&#24490;Gamma&#20998;&#24067;&#30340;&#38543;&#26426;&#32622;&#20449;&#21442;&#25968;&#26469;&#20943;&#36731;&#25163;&#21160;&#25351;&#23450;$\beta$&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#23558;RGP-UCB&#30340;&#36951;&#25022;&#20998;&#26512;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#20998;&#24067;&#31867;&#20013;&#65292;&#21253;&#25324;Gamma&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#21442;&#25968;&#25351;&#25968;&#20998;&#24067;&#30340;&#25913;&#36827;&#22411;RGP-UCB(IRGP-UCB)&#65292;&#23427;&#23454;&#29616;&#20102;&#26356;&#32039;&#23494;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#38480;&#12290;IRGP-UCB&#19981;&#38656;&#35201;&#22312;&#36845;&#20195;&#27425;&#25968;&#26041;&#38754;&#22686;&#21152;&#32622;&#20449;&#21442;&#25968;,&#20174;&#32780;&#36991;&#20813;&#20102;&#21518;&#26399;&#36807;&#24230;&#25506;&#32034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;IRGP-UCB&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process upper confidence bound (GP-UCB) is a theoretically promising approach for black-box optimization; however, the confidence parameter $\beta$ is considerably large in the theorem and chosen heuristically in practice. Then, randomized GP-UCB (RGP-UCB) uses a randomized confidence parameter, which follows the Gamma distribution, to mitigate the impact of manually specifying $\beta$. This study first generalizes the regret analysis of RGP-UCB to a wider class of distributions, including the Gamma distribution. Furthermore, we propose improved RGP-UCB (IRGP-UCB) based on a two-parameter exponential distribution, which achieves tighter Bayesian regret bounds. IRGP-UCB does not require an increase in the confidence parameter in terms of the number of iterations, which avoids over-exploration in the later iterations. Finally, we demonstrate the effectiveness of IRGP-UCB through extensive experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#65292;&#36866;&#29992;&#20110;&#28857;&#20113;&#32593;&#26684;&#22270;&#25110;&#949;-&#26368;&#36817;&#37051;&#22270;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00942</link><description>&lt;p&gt;
&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#22120;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Graph Field Integrators Meet Point Clouds. (arXiv:2302.00942v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#65292;&#36866;&#29992;&#20110;&#28857;&#20113;&#32593;&#26684;&#22270;&#25110;&#949;-&#26368;&#36817;&#37051;&#22270;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#31867;&#21035;&#65292;&#29992;&#20110;&#23545;&#32534;&#30721;&#28857;&#20113;&#30340;&#22270;&#24418;&#36827;&#34892;&#39640;&#25928;&#22330;&#31215;&#20998;&#12290;&#31532;&#19968;&#31867;&#31639;&#27861;&#20351;&#29992;&#28857;&#20113;&#32593;&#26684;&#22270;&#30340;&#26377;&#30028;&#20111;&#26684;&#65292;&#31532;&#20108;&#31867;&#31639;&#27861;&#21017;&#20351;&#29992;&#28857;&#20113;&#30340;&#27969;&#34892;&#30340;&#949;-&#26368;&#36817;&#37051;&#22270;&#34920;&#31034;&#26041;&#27861;&#12290;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#34987;&#30475;&#20316; Fast Multipole Methods(FMMs) &#30340;&#21487;&#34892;&#26367;&#20195;&#65292;&#20294;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25991;&#31456;&#37325;&#28857;&#30740;&#31350;&#22522;&#20110;&#28857;&#20043;&#38388;&#27493;&#38271;&#20998;&#24067;&#65288;&#22914;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#65289;&#25152;&#24341;&#21457;&#30340;&#20960;&#20309;&#23398;&#12290;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25991;&#31456;&#33719;&#24471;&#20102;&#32467;&#26500;&#22270;&#35770;&#30340;&#26032;&#32467;&#26524;&#12290;&#25991;&#31456;&#36824;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#21018;&#24615;&#21644;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#34920;&#38754;&#25554;&#20540;&#65288;&#29305;&#21035;&#26159;&#29992;&#20110;&#32593;&#26684;&#21160;&#24577;&#24314;&#27169;&#65289;&#65292;&#28857;&#20113;&#30340;Wasserstein&#36317;&#31163;&#35745;&#31639;&#20197;&#21450;Gromov-Wasserstein&#21464;&#20307;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, SeparatorFactorization(SF), leverages the bounded genus of point cloud mesh graphs, while the second class, RFDiffusion(RFD), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods (FMMs), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct. We also perform exhaustive empirical evaluation, including on-surface interpolation for rigid and deformable objects (particularly for mesh-dynamics modeling), Wasserstein distance computations for point clouds, and the Gromov-Wasserstein variant.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#25345;&#32493;&#23398;&#20064;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.00487</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#32508;&#36848;&#65306;&#29702;&#35770;&#12289;&#26041;&#27861;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Continual Learning: Theory, Method and Application. (arXiv:2302.00487v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#25345;&#32493;&#23398;&#20064;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#19981;&#26029;&#33719;&#21462;&#12289;&#26356;&#26032;&#12289;&#31215;&#32047;&#21644;&#21033;&#29992;&#30693;&#35782;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#36825;&#31181;&#33021;&#21147;&#31216;&#20026;&#25345;&#32493;&#23398;&#20064;&#65292;&#20026;AI&#31995;&#32479;&#33258;&#36866;&#24212;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#26032;&#20219;&#21153;&#36890;&#24120;&#20250;&#23548;&#33268;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#36817;&#24180;&#26469;&#65292;&#19981;&#26029;&#28044;&#29616;&#30340;&#21508;&#31181;&#36827;&#23637;&#22823;&#22823;&#25193;&#23637;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#29702;&#35299;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#23398;&#20064;&#32508;&#36848;&#65292;&#26088;&#22312;&#36830;&#25509;&#22522;&#26412;&#35774;&#32622;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#20195;&#34920;&#24615;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#27010;&#25324;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65306;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#21644;&#32456;&#36523;&#30340;&#23398;&#20064;&#65292;&#20197;&#21450;&#23454;&#29616;&#26356;&#28145;&#23618;&#27425;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#27491;&#21017;&#21270;&#12289;&#22522;&#20110;&#22238;&#25918;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#26426;&#22120;&#20154;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;</title><link>http://arxiv.org/abs/2301.13443</link><description>&lt;p&gt;
&#26032;&#30340;&#20998;&#24067;&#27700;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#24323;&#29992;$\Delta$DP&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity. (arXiv:2301.13443v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#24179;&#31561;&#23545;&#24453;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#35748;&#21487;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#12290;&#20026;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36861;&#27714;&#24120;&#29992;&#24230;&#37327;&#26041;&#27861;$\Delta DP$&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;$\Delta DP$&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#65306;i) &#38646;&#20540;$\Delta DP$&#19981;&#20445;&#35777;&#27665;&#26063;&#32479;&#35745;&#24179;&#31561;&#30340;&#38646;&#36829;&#35268;&#65292;ii) $\Delta DP$&#20540;&#38543;&#19981;&#21516;&#20998;&#31867;&#38408;&#20540;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#20197;&#31934;&#30830;&#27979;&#37327;&#19981;&#21516;&#27665;&#26063;&#32479;&#35745;&#32676;&#20307;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic group
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#27169;&#25311;&#29615;&#22659;&#20013;&#30456;&#20851;&#26041;&#38754;&#30340;&#8220;&#26368;&#23567;&#20215;&#20540;&#31561;&#20215;&#37096;&#20998;&#27169;&#22411;&#8221;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#35268;&#21010;&#22312;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.10119</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#23567;&#20215;&#20540;&#31561;&#20215;&#37096;&#20998;&#27169;&#22411;&#30340;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#21644;&#40065;&#26834;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning. (arXiv:2301.10119v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#27169;&#25311;&#29615;&#22659;&#20013;&#30456;&#20851;&#26041;&#38754;&#30340;&#8220;&#26368;&#23567;&#20215;&#20540;&#31561;&#20215;&#37096;&#20998;&#27169;&#22411;&#8221;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#35268;&#21010;&#22312;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32431;&#20132;&#20114;&#20013;&#23398;&#20064;&#29615;&#22659;&#27169;&#22411;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#33267;&#20851;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#26234;&#33021;&#20307;&#30340;&#29615;&#22659;&#30340;&#27599;&#20010;&#26041;&#38754;&#36827;&#34892;&#24314;&#27169;&#65292;&#26080;&#35770;&#36825;&#20123;&#26041;&#38754;&#26159;&#21542;&#22312;&#25552;&#20986;&#26368;&#20248;&#20915;&#31574;&#26041;&#38754;&#37325;&#35201;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#27169;&#22411;&#24182;&#19981;&#36866;&#21512;&#22312;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#25191;&#34892;&#21487;&#25193;&#23637;&#21644;&#40065;&#26834;&#30340;&#35268;&#21010;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#21482;&#27169;&#25311;&#29615;&#22659;&#20013;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;&#26368;&#23567;&#20215;&#20540;&#31561;&#20215;&#37096;&#20998;&#27169;&#22411;&#8221;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#26469;&#35777;&#26126;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#30340;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#28982;&#21518;&#36827;&#34892;&#23454;&#39564;&#20197;&#20174;&#23454;&#35777;&#35282;&#24230;&#35828;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#26368;&#21518;&#25552;&#20986;&#19968;&#20123;&#26377;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning models of the environment from pure interaction is often considered an essential component of building lifelong reinforcement learning agents. However, the common practice in model-based reinforcement learning is to learn models that model every aspect of the agent's environment, regardless of whether they are important in coming up with optimal decisions or not. In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call "minimal value-equivalent partial models". After providing a formal definition for these models, we provide theoretical results demonstrating the scalability advantages of performing planning with such models and then perform experiments to empirically illustrate our theoretical results. Then, we provide some useful heuristics on how to learn these kinds
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#20013;&#25152;&#26377;&#35760;&#24405;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#20934;&#30830;&#29575;&#21644;&#21361;&#23475;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10053</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#32447;&#24615;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Linear Reconstruction Approach for Attribute Inference Attacks against Synthetic Data. (arXiv:2301.10053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#20013;&#25152;&#26377;&#35760;&#24405;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#20934;&#30830;&#29575;&#21644;&#21361;&#23475;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65288;SDG&#65289;&#30340;&#21457;&#23637;&#34987;&#35465;&#20026;&#20849;&#20139;&#25935;&#24863;&#25968;&#25454;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#38590;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;SDG&#26088;&#22312;&#23398;&#20064;&#30495;&#23454;&#25968;&#25454;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#29983;&#25104;&#32467;&#26500;&#21644;&#32479;&#35745;&#23398;&#30456;&#20284;&#30340;&#8220;&#20154;&#36896;&#8221;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#25512;&#35770;&#25915;&#20987;&#21487;&#33021;&#20250;&#30772;&#22351;&#38544;&#31169;&#65292;&#20294;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#24322;&#24120;&#35760;&#24405;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#12290;&#35813;&#25915;&#20987;&#22522;&#20110;&#32447;&#24615;&#37325;&#26500;&#26041;&#27861;&#29992;&#20110;&#32858;&#21512;&#32479;&#35745;&#25968;&#25454;&#65292;&#25915;&#20987;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#24322;&#24120;&#20540;&#65292;&#32780;&#26159;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#35760;&#24405;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;&#26368;&#20808;&#36827;&#30340;SDG&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#21644;&#26368;&#36817;&#30340;&#24046;&#20998;&#38544;&#31169;SDG&#26426;&#21046;&#12290;&#36890;&#36807;&#23450;&#20041;&#24418;&#24335;&#21270;&#30340;&#38544;&#31169;&#21338;&#24328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#29978;&#33267;&#23545;&#20219;&#24847;&#35760;&#24405;&#37117;&#21487;&#20197;&#39640;&#24230;&#31934;&#30830;&#65292;&#24182;&#19988;&#36825;&#26159;&#25915;&#20987;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in synthetic data generation (SDG) have been hailed as a solution to the difficult problem of sharing sensitive data while protecting privacy. SDG aims to learn statistical properties of real data in order to generate "artificial" data that are structurally and statistically similar to sensitive data. However, prior research suggests that inference attacks on synthetic data can undermine privacy, but only for specific outlier records. In this work, we introduce a new attribute inference attack against synthetic data. The attack is based on linear reconstruction methods for aggregate statistics, which target all records in the dataset, not only outliers. We evaluate our attack on state-of-the-art SDG algorithms, including Probabilistic Graphical Models, Generative Adversarial Networks, and recent differentially private SDG mechanisms. By defining a formal privacy game, we show that our attack can be highly accurate even on arbitrary records, and that this is the result o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#24322;&#36136;&#22270;&#20013;&#30340;&#26377;&#31526;&#21495;&#20256;&#25773;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#22270;&#30340;&#26032;&#31574;&#30053;&#65292;&#24182;&#20811;&#26381;&#20102;&#20854;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.08918</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26377;&#31526;&#21495;&#20256;&#25773;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Signed Propagation for Graph Neural Networks. (arXiv:2301.08918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#24322;&#36136;&#22270;&#20013;&#30340;&#26377;&#31526;&#21495;&#20256;&#25773;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#22270;&#30340;&#26032;&#31574;&#30053;&#65292;&#24182;&#20811;&#26381;&#20102;&#20854;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#20256;&#36882;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21516;&#36136;&#22270;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#36136;&#22270;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#21364;&#24456;&#24046;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#26696;&#12290;&#29305;&#21035;&#22320;&#65292;&#32763;&#36716;&#36793;&#30340;&#31526;&#21495;&#26159;&#22522;&#20110;&#22362;&#23454;&#29702;&#35770;&#22522;&#30784;&#30340;&#24182;&#19988;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#20998;&#26512;&#20551;&#23450;&#20102;&#20108;&#20803;&#20998;&#31867;&#22330;&#26223;&#65292;&#22240;&#27492;&#21463;&#21040;&#24212;&#29992;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#23558;&#20197;&#21069;&#30340;&#29702;&#35299;&#25193;&#23637;&#21040;&#22810;&#31867;&#21035;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20004;&#20010;&#32570;&#28857;&#65306;&#65288;1&#65289;&#22810;&#36339;&#37051;&#23621;&#30340;&#31526;&#21495;&#21462;&#20915;&#20110;&#28040;&#24687;&#20256;&#36882;&#36335;&#24452;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36825;&#20063;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#20363;&#22914;&#65292;&#20914;&#31361;&#35777;&#25454;&#65289;&#65292;&#21487;&#33021;&#24433;&#21709;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#30340;&#22270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#32467;&#21512;&#20102;&#21407;&#26377;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20854;&#32570;&#28857;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message-passing Graph Neural Networks (GNNs), which collect information from adjacent nodes, achieve satisfying results on homophilic graphs. However, their performances are dismal in heterophilous graphs, and many researchers have proposed a plethora of schemes to solve this problem. Especially, flipping the sign of edges is rooted in a strong theoretical foundation, and attains significant performance enhancements. Nonetheless, previous analyses assume a binary class scenario and they may suffer from confined applicability. This paper extends the prior understandings to multi-class scenarios and points out two drawbacks: (1) the sign of multi-hop neighbors depends on the message propagation paths and may incur inconsistency, (2) it also increases the prediction uncertainty (e.g., conflict evidence) which can impede the stability of the algorithm. Based on the theoretical understanding, we introduce a novel strategy that is applicable to multi-class graphs. The proposed scheme combine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#29702;&#20998;&#31163;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;Ar/N$_2$&#25918;&#30005;&#20013;AlN&#28293;&#23556;&#21644;&#34180;&#33180;&#27785;&#31215;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#28151;&#21512;&#21453;&#24212;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;Monte Carlo&#27169;&#25311;&#23454;&#29616;&#20102;&#22635;&#20805;&#21442;&#25968;&#31354;&#38388;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.03524</link><description>&lt;p&gt;
&#29992;&#20110;&#39044;&#27979;Ar/N$_2$&#25918;&#30005;&#20013;AlN&#28293;&#23556;&#21644;&#34180;&#33180;&#27785;&#31215;&#30340;&#29289;&#29702;&#20998;&#31163;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-separating artificial neural networks for predicting sputtering and thin film deposition of AlN in Ar/N$_2$ discharges on experimental timescales. (arXiv:2301.03524v1 [cond-mat.mtrl-sci] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#29702;&#20998;&#31163;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;Ar/N$_2$&#25918;&#30005;&#20013;AlN&#28293;&#23556;&#21644;&#34180;&#33180;&#27785;&#31215;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#28151;&#21512;&#21453;&#24212;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;Monte Carlo&#27169;&#25311;&#23454;&#29616;&#20102;&#22635;&#20805;&#21442;&#25968;&#31354;&#38388;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#27169;&#25311;&#31561;&#31163;&#23376;&#20307;-&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#26159;&#19968;&#20010;&#22810;&#23610;&#24230;&#21644;&#22810;&#29289;&#29702;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#22635;&#20805;&#21442;&#25968;&#31354;&#38388;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#35745;&#31639;&#29983;&#25104;&#25968;&#25454;&#27169;&#25311;&#30340;&#35745;&#31639;&#25104;&#26412;&#24040;&#22823;&#30340;&#23454;&#38469;&#24212;&#29992;&#38382;&#39064;&#65292;&#20174;&#32780;&#38024;&#23545;Ar / N$_2$ &#25918;&#30005;&#20013; AlN &#28293;&#23556;&#27785;&#31215;&#24320;&#21457;&#20102;&#29289;&#29702;&#20998;&#31163;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#28151;&#21512;&#21453;&#24212;&#20998;&#23376;&#21160;&#21147;&#23398;/&#26102;&#38388;&#25139;&#21147;&#20559;&#32622;Monte Carlo&#27169;&#25311;&#38543;&#26426;&#31561;&#31163;&#23376;&#20307;-&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;/&#25193;&#25955;&#36807;&#31243;&#65292;&#24314;&#31435;&#20102;&#35813;&#27169;&#22411;&#12290;&#23558;&#36825;&#31181;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#23454;&#39564;&#21442;&#32771;&#26696;&#20363;&#65292;&#21487;&#20197;&#31995;&#32479;&#20998;&#26512;&#31890;&#23376;&#36890;&#37327;&#21457;&#23556;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
Understanding and modeling plasma-surface interactions frame a multi-scale as well as multi-physics problem. Scale-bridging machine learning surface surrogate models have been demonstrated to perceive the fundamental atomic fidelity for the physical vapor deposition of pure metals. However, the immense computational cost of the data-generating simulations render a practical application with predictions on relevant timescales impracticable. This issue is resolved in this work for the sputter deposition of AlN in Ar/N$_2$ discharges by developing a scheme that populates the parameter spaces effectively. Hybrid reactive molecular dynamics / time-stamped force-bias Monte Carlo simulations of randomized plasma-surface interactions / diffusion processes are used to setup a physics-separating artificial neural network. The application of this generic machine learning model to a specific experimental reference case study enables the systematic analysis of the particle flux emission as well as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37038;&#20214;&#26426;&#21046;&#65288;TEM&#65289;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#23427;&#37319;&#29992;&#26412;&#22320;&#36890;&#20449;&#21644;&#28040;&#24687;&#38142;&#36716;&#21457;&#30340;&#26041;&#24335;&#36827;&#34892;&#36890;&#20449;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#25311;&#25152;&#26377;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2301.01919</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#37038;&#20214;&#26426;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21487;&#25193;&#23637;&#27807;&#36890;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Communication for Multi-Agent Reinforcement Learning via Transformer-Based Email Mechanism. (arXiv:2301.01919v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37038;&#20214;&#26426;&#21046;&#65288;TEM&#65289;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#23427;&#37319;&#29992;&#26412;&#22320;&#36890;&#20449;&#21644;&#28040;&#24687;&#38142;&#36716;&#21457;&#30340;&#26041;&#24335;&#36827;&#34892;&#36890;&#20449;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#25311;&#25152;&#26377;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#35759;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#21512;&#20316;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24191;&#25773;&#20449;&#24687;&#23548;&#33268;&#20449;&#24687;&#20887;&#20313;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#25152;&#26377;&#20854;&#20182;&#26234;&#33021;&#20307;&#27169;&#25311;&#25104;&#30446;&#26631;&#26469;&#23398;&#20064;&#26377;&#38024;&#23545;&#24615;&#30340;&#36890;&#20449;&#65292;&#20294;&#26159;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#21464;&#21270;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#20219;&#21153;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;Transformer-based Email Mechanism&#65288;TEM&#65289;&#12290;&#26234;&#33021;&#20307;&#37319;&#29992;&#26412;&#22320;&#36890;&#20449;&#65292;&#20165;&#21521;&#21487;&#20197;&#35266;&#23519;&#21040;&#30340;&#26234;&#33021;&#20307;&#21457;&#36865;&#28040;&#24687;&#65292;&#26080;&#38656;&#27169;&#25311;&#25152;&#26377;&#26234;&#33021;&#20307;&#12290;&#21463;&#21040;&#20154;&#31867;&#30005;&#23376;&#37038;&#20214;&#36716;&#21457;&#21512;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#28040;&#24687;&#38142;&#20197;&#23558;&#20449;&#24687;&#36716;&#21457;&#32473;&#35266;&#23519;&#33539;&#22260;&#20043;&#22806;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#24341;&#20837;Transformer&#26469;&#32534;&#30721;&#21644;&#35299;&#30721;&#28040;&#24687;&#38142;&#65292;&#20197;&#36873;&#25321;&#19979;&#19968;&#20010;&#25509;&#25910;&#32773;&#12290;&#22312;&#22810;&#20010;&#21512;&#20316;&#30340;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#39564;&#35777;&#26126;TEM&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication can impressively improve cooperation in multi-agent reinforcement learning (MARL), especially for partially-observed tasks. However, existing works either broadcast the messages leading to information redundancy, or learn targeted communication by modeling all the other agents as targets, which is not scalable when the number of agents varies. In this work, to tackle the scalability problem of MARL communication for partially-observed tasks, we propose a novel framework Transformer-based Email Mechanism (TEM). The agents adopt local communication to send messages only to the ones that can be observed without modeling all the agents. Inspired by human cooperation with email forwarding, we design message chains to forward information to cooperate with the agents outside the observation range. We introduce Transformer to encode and decode the message chain to choose the next receiver selectively. Empirically, TEM outperforms the baselines on multiple cooperative MARL benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#38899;&#39057;DeepFake&#26816;&#27979;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#19977;&#31181;&#26816;&#27979;&#20307;&#31995;&#32467;&#26500;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#20197;&#21450;&#37319;&#29992;&#33258;&#36866;&#24212;&#35757;&#32451;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#26469;&#25552;&#39640;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36824;&#39318;&#27425;&#36866;&#24212;&#20102;RawNet3&#20307;&#31995;&#32467;&#26500;&#29992;&#20110;DeepFake&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2212.14597</link><description>&lt;p&gt;
&#25269;&#24481;&#38024;&#23545;&#38899;&#39057;DeepFake&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defense Against Adversarial Attacks on Audio DeepFake Detection. (arXiv:2212.14597v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#38899;&#39057;DeepFake&#26816;&#27979;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#19977;&#31181;&#26816;&#27979;&#20307;&#31995;&#32467;&#26500;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#20197;&#21450;&#37319;&#29992;&#33258;&#36866;&#24212;&#35757;&#32451;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#26469;&#25552;&#39640;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36824;&#39318;&#27425;&#36866;&#24212;&#20102;RawNet3&#20307;&#31995;&#32467;&#26500;&#29992;&#20110;DeepFake&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;DeepFake&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#30340;&#20154;&#24037;&#35821;&#38899;&#65292;&#20854;&#20027;&#35201;&#30446;&#30340;&#26159;&#20197;&#39640;&#24230;&#20196;&#20154;&#20449;&#26381;&#30340;&#26041;&#24335;&#27450;&#39575;&#21548;&#20247;&#12290;&#23427;&#20204;&#30340;&#36136;&#37327;&#36275;&#20197;&#22312;&#23433;&#20840;&#21644;&#38544;&#31169;&#26041;&#38754;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#65292;&#21253;&#25324;&#26032;&#38395;&#30340;&#21487;&#38752;&#24615;&#25110;&#35837;&#35876;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#29983;&#25104;&#30340;&#35821;&#38899;&#65292;&#20197;&#38450;&#27490;&#36825;&#20123;&#23041;&#32961;&#12290;&#26412;&#30740;&#31350;&#28085;&#30422;&#20102;&#38024;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20027;&#39064;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#21521;&#36755;&#20837;&#25968;&#25454;&#28155;&#21152;&#34920;&#38754;&#30340;&#26356;&#38590;&#34987;&#20154;&#31867;&#23519;&#35273;&#30340;&#21464;&#21270;&#26469;&#38477;&#20302;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#22312;&#20004;&#20010;&#22330;&#26223;&#65288;&#30333;&#30418;&#21644;&#20351;&#29992;&#21487;&#20256;&#36882;&#24615;&#65289;&#20013;&#35780;&#20272;3&#20010;&#26816;&#27979;&#20307;&#31995;&#32467;&#26500;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#33258;&#36866;&#24212;&#35757;&#32451;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#26469;&#21152;&#24378;&#23427;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#39318;&#27425;&#36866;&#24212;&#20102;RawNet3&#20307;&#31995;&#32467;&#26500;&#29992;&#20110;DeepFake&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio DeepFakes (DF) are artificially generated utterances created using deep learning, with the primary aim of fooling the listeners in a highly convincing manner. Their quality is sufficient to pose a severe threat in terms of security and privacy, including the reliability of news or defamation. Multiple neural network-based methods to detect generated speech have been proposed to prevent the threats. In this work, we cover the topic of adversarial attacks, which decrease the performance of detectors by adding superficial (difficult to spot by a human) changes to input data. Our contribution contains evaluating the robustness of 3 detection architectures against adversarial attacks in two scenarios (white-box and using transferability) and enhancing it later by using adversarial training performed by our novel adaptive training. Moreover, one of the investigated architectures is RawNet3, which, to the best of our knowledge, we adapted for the first time to DeepFake detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#21333;&#30456;&#27969;&#25968;&#20540;&#27169;&#25311;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23454;&#26102;&#39044;&#27979;&#21644;&#25913;&#21892;&#20004;&#30456;&#27969;&#30340;&#29123;&#27833;/&#31354;&#27668;&#28151;&#21512;&#29289;&#65292;&#20174;&#32780;&#25552;&#39640;&#28065;&#25159;&#21457;&#21160;&#26426;&#29123;&#27833;&#23460;&#21943;&#23556;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.12731</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#27169;&#24577;&#20998;&#35299;&#30340;&#20004;&#30456;&#21516;&#24515;&#21943;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Forecasting through deep learning and modal decomposition in two-phase concentric jets. (arXiv:2212.12731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#21333;&#30456;&#27969;&#25968;&#20540;&#27169;&#25311;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23454;&#26102;&#39044;&#27979;&#21644;&#25913;&#21892;&#20004;&#30456;&#27969;&#30340;&#29123;&#27833;/&#31354;&#27668;&#28151;&#21512;&#29289;&#65292;&#20174;&#32780;&#25552;&#39640;&#28065;&#25159;&#21457;&#21160;&#26426;&#29123;&#27833;&#23460;&#21943;&#23556;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#24037;&#20316;&#26088;&#22312;&#25552;&#39640;&#28065;&#25159;&#21457;&#21160;&#26426;&#29123;&#27833;&#23460;&#21943;&#23556;&#22120;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#24847;&#21619;&#30528;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#27745;&#26579;&#29289;&#12290;&#20026;&#20102;&#23454;&#29616;&#23454;&#26102;&#39044;&#27979;&#21644;&#25913;&#21892;&#29123;&#27833;/&#31354;&#27668;&#28151;&#21512;&#29289;&#65292;&#38656;&#35201;&#24320;&#21457;&#20801;&#35768;&#36825;&#31181;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#25152;&#20570;&#30340;&#24037;&#20316;&#28041;&#21450;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#65288;&#38590;&#20197;&#27979;&#37327;&#65289;&#25110;&#23436;&#25972;&#38382;&#39064;&#30340;&#25968;&#20540;&#35299;&#65288;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65289;&#12290;&#21518;&#32773;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#31995;&#32479;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#24320;&#21457;&#23454;&#26102;&#39044;&#27979;&#24037;&#20855;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#65288;&#30456;&#23545;&#26356;&#20415;&#23452;&#30340;&#65289;&#21333;&#30456;&#27969;&#25968;&#20540;&#27169;&#25311;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#23384;&#22312;&#20999;&#21521;&#19981;&#36830;&#32493;&#24615;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20004;&#30456;&#27969;&#20013;&#30340;&#28151;&#21512;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#20316;&#20026;PDE&#20195;&#29702;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;NN&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#21160;&#24577;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to improve fuel chamber injectors' performance in turbofan engines, thus implying improved performance and reduction of pollutants. This requires the development of models that allow real-time prediction and improvement of the fuel/air mixture. However, the work carried out to date involves using experimental data (complicated to measure) or the numerical resolution of the complete problem (computationally prohibitive). The latter involves the resolution of a system of partial differential equations (PDE). These problems make difficult to develop a real-time prediction tool. Therefore, in this work, we propose using machine learning in conjunction with (complementarily cheaper) single-phase flow numerical simulations in the presence of tangential discontinuities to estimate the mixing process in two-phase flows. In this meaning we study the application of two proposed neural network (NN) models as PDE surrogate models. Where the future dynamics is predicted by the NN, gi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#25972;&#25968;&#24207;&#21015;&#26469;&#25628;&#32034;&#25968;&#23398;&#24120;&#25968;&#30340;&#20844;&#24335;&#65292;&#25512;&#23548;&#20986;&#20102;&#22810;&#20010;&#24120;&#25968;&#30340;&#26032;&#20844;&#24335;&#65292;&#21253;&#25324; $\pi$ &#21644; Catalan &#24120;&#25968;&#12290;</title><link>http://arxiv.org/abs/2212.09470</link><description>&lt;p&gt;
&#22522;&#20110;&#25972;&#25968;&#24207;&#21015;&#20998;&#26512;&#30340;&#25968;&#23398;&#24120;&#25968;&#25512;&#27979;&#33258;&#21160;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automated Search for Conjectures on Mathematical Constants using Analysis of Integer Sequences. (arXiv:2212.09470v2 [math.NT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#25972;&#25968;&#24207;&#21015;&#26469;&#25628;&#32034;&#25968;&#23398;&#24120;&#25968;&#30340;&#20844;&#24335;&#65292;&#25512;&#23548;&#20986;&#20102;&#22810;&#20010;&#24120;&#25968;&#30340;&#26032;&#20844;&#24335;&#65292;&#21253;&#25324; $\pi$ &#21644; Catalan &#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#25968;&#23398;&#24120;&#25968;&#25152;&#26500;&#25104;&#30340;&#20844;&#24335;&#23545;&#20110;&#31185;&#23398;&#21644;&#25968;&#23398;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#35777;&#26126;&#36825;&#20123;&#24120;&#25968;&#30340;&#26080;&#29702;&#24615;&#12290;&#28982;&#32780;&#65292;&#21382;&#21490;&#19978;&#20026;&#36825;&#20123;&#20844;&#24335;&#30340;&#21457;&#29616;&#65292;&#20165;&#38480;&#20110;&#26497;&#23569;&#25968;&#20255;&#22823;&#30340;&#25968;&#23398;&#23478;&#65292;&#22914;&#25289;&#39532;&#21162;&#37329;&#12289;&#27431;&#25289;&#21644;&#39640;&#26031;&#65292;&#24182;&#19988;&#21463;&#21046;&#20110;&#25628;&#32034;&#31354;&#38388;&#30340;&#38480;&#21046;&#21644;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#26512;&#25972;&#25968;&#24207;&#21015;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102; "&#26522;&#20030;&#31526;&#21495;&#36830;&#20998;&#25968;-&#26757;&#35199;&#39564;&#35777; (ESMA) "&#31639;&#27861;&#65292;&#24314;&#31435;&#22312; Berlekamp-Massey &#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#20197;&#35782;&#21035;&#19982;&#25968;&#23398;&#24120;&#25968;&#30456;&#20851;&#30340;&#25972;&#25968;&#24207;&#21015;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#21457;&#29616;&#20960;&#20010;&#30693;&#21517;&#25968;&#23398;&#24120;&#25968;&#30340;&#26032;&#20844;&#24335;&#65292;&#21253;&#25324; $\pi$ &#21644; Catalan &#24120;&#25968;&#12290;ESMA &#31639;&#27861;&#20026;&#33258;&#21160;&#21457;&#29616;&#25968;&#23398;&#24120;&#25968;&#30340;&#20844;&#24335;&#25552;&#20379;&#20102;&#20840;&#26032;&#19988;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formulas involving fundamental mathematical constants had a great impact on various fields of science and mathematics, for example aiding in proofs of irrationality of constants. However, the discovery of such formulas has historically remained scarce, often perceived as an act of mathematical genius by great mathematicians such as Ramanujan, Euler, and Gauss. Recent efforts to automate the discovery of formulas for mathematical constants, such as the Ramanujan Machine project, relied on exhaustive search. Despite several successful discoveries, exhaustive search remains limited by the space of options that can be covered and by the need for vast amounts of computational resources. Here we propose a fundamentally different method to search for conjectures on mathematical constants: through analysis of integer sequences. We introduce the Enumerated Signed-continued-fraction Massey Approve (ESMA) algorithm, which builds on the Berlekamp-Massey algorithm to identify patterns in integer se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27931;&#20262;&#20857;&#32676;&#33258;&#32534;&#30721;&#22120;&#65288;LGAE&#65289;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#19978;&#36848;&#32676;&#30340;&#31561;&#21464;&#24615;&#65292;&#19988;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#30340;&#21387;&#32553;&#12289;&#37325;&#26500;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2212.07347</link><description>&lt;p&gt;
&#27931;&#20262;&#20857;&#32676;&#31561;&#21464;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Lorentz group equivariant autoencoders. (arXiv:2212.07347v2 [hep-ex] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27931;&#20262;&#20857;&#32676;&#33258;&#32534;&#30721;&#22120;&#65288;LGAE&#65289;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#19978;&#36848;&#32676;&#30340;&#31561;&#21464;&#24615;&#65292;&#19988;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#30340;&#21387;&#32553;&#12289;&#37325;&#26500;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39640;&#33021;&#29289;&#29702;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12289;&#20223;&#30495;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#30340;&#24037;&#20316;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20174;&#38024;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#36866;&#21512;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20363;&#22914;&#20197;&#20854;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#31561;&#21464;&#12290;&#36825;&#20123;&#20559;&#24046;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#21152;&#24615;&#33021;&#20248;&#33391;&#21644;&#21487;&#35299;&#37322;&#65292;&#24182;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#27931;&#20262;&#20857;&#32676;&#33258;&#32534;&#30721;&#22120;&#65288;LGAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36866;&#24403;&#30340;&#12289;&#27491;&#20132;&#30340;&#27931;&#20262;&#20857;&#32676;$\mathrm{SO}^+(3,1)$&#31561;&#21464;&#65292;&#24182;&#19988;&#28508;&#22312;&#31354;&#38388;&#22312;&#32676;&#30340;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#22312;LHC&#30340;&#21943;&#27880;&#19978;&#23637;&#31034;&#20102;&#20960;&#20010;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#20960;&#20010;&#21387;&#32553;&#12289;&#37325;&#26500;&#21644;&#24322;&#24120;&#26816;&#27979;&#25351;&#26631;&#19978;&#20248;&#20110;&#22270;&#24418;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
There has been significant work recently in developing machine learning (ML) models in high energy physics (HEP) for tasks such as classification, simulation, and anomaly detection. Often these models are adapted from those designed for datasets in computer vision or natural language processing, which lack inductive biases suited to HEP data, such as equivariance to its inherent symmetries. Such biases have been shown to make models more performant and interpretable, and reduce the amount of training data needed. To that end, we develop the Lorentz group autoencoder (LGAE), an autoencoder model equivariant with respect to the proper, orthochronous Lorentz group $\mathrm{SO}^+(3,1)$, with a latent space living in the representations of the group. We present our architecture and several experimental results on jets at the LHC and find it outperforms graph and convolutional neural network baseline models on several compression, reconstruction, and anomaly detection metrics. We also demons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;</title><link>http://arxiv.org/abs/2212.06951</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;AI&#20262;&#29702;: &#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#21306;&#22359;&#38142;&#23433;&#20840;&#20027;&#39064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#20351;&#29992;&#20998;&#24067;&#24335;&#32593;&#32476;&#35753;&#35745;&#31639;&#26426;&#31995;&#32479;&#26356;&#23433;&#20840;&#65292;&#20294;&#24403;&#21069;&#30340;&#21306;&#22359;&#38142;&#35774;&#35745;&#22312;&#20132;&#26131;&#39034;&#24207;&#26041;&#38754;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30719;&#24037;&#21487;&#20197;&#37325;&#26032;&#25490;&#24207;&#20132;&#26131;&#20197;&#29983;&#25104;&#21033;&#28070;&#65292;&#36825;&#34987;&#31216;&#20026;&#30719;&#24037;&#21487;&#25552;&#21462;&#20215;&#20540;&#65288;MEV&#65289;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#35748;&#20026;MEV&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;Flashbots&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#20998;&#26512;&#20102;&#21306;&#22359;&#38142;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;MEV&#22312;&#26356;&#24191;&#27867;&#30340;AI&#31038;&#20250;&#20013;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#20840;&#38754;&#20998;&#26512;&#20102;MEV&#25512;&#25991;&#20013;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36229;&#36807;20,000&#20010;MEV&#21644;Flashbots&#26631;&#31614;&#30340;&#25512;&#25991;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21306;&#22359;&#38142;&#19978;MEV&#27963;&#21160;&#30340;&#20849;&#21516;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
&lt;/p&gt;</description></item><item><title>XPER&#26041;&#27861;&#33021;&#34913;&#37327;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20855;&#20307;&#36129;&#29486;&#65292;&#24182;&#21487;&#29992;&#20110;&#22788;&#29702;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#26500;&#24314;&#21516;&#36136;&#21270;&#20010;&#20307;&#32676;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.05866</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24615;&#33021;&#65306;&#34913;&#37327;&#39044;&#27979;&#24615;&#33021;&#30340;&#39537;&#21160;&#21147;
&lt;/p&gt;
&lt;p&gt;
Explainable Performance: Measuring the Driving Forces of Predictive Performance. (arXiv:2212.05866v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05866
&lt;/p&gt;
&lt;p&gt;
XPER&#26041;&#27861;&#33021;&#34913;&#37327;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20855;&#20307;&#36129;&#29486;&#65292;&#24182;&#21487;&#29992;&#20110;&#22788;&#29702;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#26500;&#24314;&#21516;&#36136;&#21270;&#20010;&#20307;&#32676;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;XPER&#65288;eXplainable PERformance&#65289;&#26041;&#27861;&#26469;&#34913;&#37327;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20855;&#20307;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#22522;&#20110;Shapley&#20540;&#65292;&#26082;&#19981;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#24615;&#33021;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;XPER&#21487;&#22312;&#27169;&#22411;&#32423;&#21035;&#25110;&#20010;&#20307;&#32423;&#21035;&#23454;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;XPER&#20855;&#26377;&#26631;&#20934;&#35299;&#37322;&#24615;&#26041;&#27861;&#65288;SHAP&#65289;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#36151;&#27454;&#36829;&#32422;&#39044;&#27979;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;XPER&#22788;&#29702;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#22806;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#20010;&#20307;XPER&#20540;&#23545;&#20182;&#20204;&#36827;&#34892;&#32858;&#31867;&#26469;&#26500;&#24314;&#21516;&#36136;&#21270;&#30340;&#20010;&#20307;&#32676;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#20272;&#35745;&#32676;&#20307;&#29305;&#23450;&#30340;&#27169;&#22411;&#27604;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#20010;&#20307;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the XPER (eXplainable PERformance) methodology to measure the specific contribution of the input features to the predictive performance of a model. Our methodology is theoretically grounded on Shapley values and is both model-agnostic and performance metric-agnostic. Furthermore, XPER can be implemented either at the model level or at the individual level. We demonstrate that XPER has as a special case the standard explainability method in machine learning (SHAP). In a loan default forecasting application, we show how XPER can be used to deal with heterogeneity issues and significantly boost out-of-sample performance. To do so, we build homogeneous groups of individuals by clustering them based on their individual XPER values. We find that estimating group-specific models yields a much higher predictive accuracy than with a one-fits-all model.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21333;&#20010;&#26799;&#24230;&#26597;&#35810;&#21487;&#37325;&#26500;&#35757;&#32451;&#25968;&#25454;&#65292;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2212.03714</link><description>&lt;p&gt;
&#20174;&#27169;&#22411;&#26799;&#24230;&#37325;&#26500;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Training Data from Model Gradient, Provably. (arXiv:2212.03714v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03714
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21333;&#20010;&#26799;&#24230;&#26597;&#35810;&#21487;&#37325;&#26500;&#35757;&#32451;&#25968;&#25454;&#65292;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#26041;&#38754;&#65292;&#29702;&#35299;&#27169;&#22411;&#26799;&#24230;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#27844;&#38706;&#26377;&#20851;&#35757;&#32451;&#26679;&#26412;&#30340;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65306;&#21363;&#20351;&#27809;&#26377;&#35757;&#32451;&#25110;&#35760;&#24518;&#25968;&#25454;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#20174;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#21442;&#25968;&#20540;&#22788;&#36827;&#34892;&#30340;&#21333;&#20010;&#26799;&#24230;&#26597;&#35810;&#20013;&#23436;&#20840;&#37325;&#26500;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#35782;&#21035;&#24615;&#65306;&#20351;&#29992;&#27973;&#23618;&#25110;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21508;&#31181;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#39640;&#25928;&#31639;&#27861;&#26469;&#37325;&#26500;&#35757;&#32451;&#25968;&#25454;&#12290;&#20316;&#20026;&#25581;&#31034;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#35777;&#26126;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#20102;&#23545;&#38544;&#31169;&#30340;&#28508;&#22312;&#20005;&#37325;&#23041;&#32961;&#65292;&#23588;&#20854;&#26159;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this paper, we present a surprising result: even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild conditions: with shallow or deep neural networks and a wide range of activation functions. We also present a statistically and computationally efficient algorithm based on tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential severe threats to privacy, especially in federated learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#23454;&#29616;&#30340;&#32852;&#37030;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20801;&#35768;&#22810;&#20010;&#26041;&#20849;&#21516;&#35757;&#32451;&#20027;&#39064;&#27169;&#22411;&#21644;&#20445;&#25252;&#33410;&#28857;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2212.02269</link><description>&lt;p&gt;
&#32852;&#37030;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Neural Topic Models. (arXiv:2212.02269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#23454;&#29616;&#30340;&#32852;&#37030;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20801;&#35768;&#22810;&#20010;&#26041;&#20849;&#21516;&#35757;&#32451;&#20027;&#39064;&#27169;&#22411;&#21644;&#20445;&#25252;&#33410;&#28857;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20027;&#39064;&#24314;&#27169;&#24050;&#25104;&#20026;&#32452;&#32455;&#21644;&#24635;&#32467;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#25110;&#22312;&#20854;&#20013;&#25628;&#32034;&#29305;&#23450;&#27169;&#24335;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#20174;&#19981;&#21516;&#26469;&#28304;&#20132;&#21449;&#20998;&#26512;&#25968;&#25454;&#26102;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#38544;&#31169;&#38382;&#39064;&#12290;&#32852;&#37030;&#20027;&#39064;&#24314;&#27169;&#36890;&#36807;&#20801;&#35768;&#22810;&#20010;&#26041;&#20849;&#21516;&#35757;&#32451;&#20027;&#39064;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#20854;&#25968;&#25454;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#23454;&#29616;&#30340;&#32852;&#37030;&#23454;&#29616;&#65292;&#26174;&#31034;&#20854;&#22312;&#33410;&#28857;&#25991;&#26723;&#30340;&#20027;&#39064;&#22810;&#26679;&#24615;&#21644;&#24314;&#31435;&#32852;&#21512;&#27169;&#22411;&#30340;&#38656;&#35201;&#26102;&#30340;&#20248;&#21183;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#24403;&#20110;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#65292;&#20294;&#20445;&#25252;&#33410;&#28857;&#30340;&#38544;&#31169;&#12290;&#20351;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35828;&#26126;&#20102;&#36825;&#31181;&#32852;&#37030;&#22330;&#26223;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last years, topic modeling has emerged as a powerful technique for organizing and summarizing big collections of documents or searching for particular patterns in them. However, privacy concerns may arise when cross-analyzing data from different sources. Federated topic modeling solves this issue by allowing multiple parties to jointly train a topic model without sharing their data. While several federated approximations of classical topic models do exist, no research has been conducted on their application for neural topic models. To fill this gap, we propose and analyze a federated implementation based on state-of-the-art neural topic modeling implementations, showing its benefits when there is a diversity of topics across the nodes' documents and the need to build a joint model. In practice, our approach is equivalent to a centralized model training, but preserves the privacy of the nodes. Advantages of this federated scenario are illustrated by means of experiments using b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32447;&#24615;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20998;&#31163;&#38382;&#39064;&#65292;&#25351;&#20986;&#23545;&#20110;&#35782;&#21035;&#24615;&#24178;&#39044;&#25968;&#25454;&#26159;&#24517;&#35201;&#30340;&#65292;&#32780;&#27599;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#21333;&#19968;&#24178;&#39044;&#23601;&#36275;&#22815;&#20102;&#12290;</title><link>http://arxiv.org/abs/2211.16467</link><description>&lt;p&gt;
&#32447;&#24615;&#24178;&#39044;&#19979;&#30340;&#22240;&#26524;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Linear Causal Disentanglement via Interventions. (arXiv:2211.16467v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32447;&#24615;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20998;&#31163;&#38382;&#39064;&#65292;&#25351;&#20986;&#23545;&#20110;&#35782;&#21035;&#24615;&#24178;&#39044;&#25968;&#25454;&#26159;&#24517;&#35201;&#30340;&#65292;&#32780;&#27599;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#21333;&#19968;&#24178;&#39044;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20998;&#31163;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#26469;&#34920;&#31034;&#28041;&#21450;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#22914;&#26524;&#28508;&#22312;&#27169;&#22411;&#21644;&#20174;&#28508;&#22312;&#21464;&#37327;&#21040;&#35266;&#27979;&#21464;&#37327;&#30340;&#36716;&#25442;&#37117;&#26159;&#21807;&#19968;&#30340;&#65292;&#21017;&#34920;&#31034;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35266;&#27979;&#21464;&#37327;&#26159;&#32447;&#24615;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32447;&#24615;&#36716;&#25442;&#12290;&#23545;&#20110;&#35782;&#21035;&#24615;&#65292;&#24178;&#39044;&#25968;&#25454;&#26159;&#24517;&#35201;&#30340;&#65306;&#22914;&#26524;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#32570;&#23569;&#24178;&#39044;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#23384;&#22312;&#26080;&#27861;&#21306;&#20998;&#30340;&#19981;&#21516;&#27169;&#22411;&#12290;&#21453;&#20043;&#65292;&#25105;&#20204;&#23637;&#31034;&#27599;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#21333;&#19968;&#24178;&#39044;&#23601;&#36275;&#22815;&#20102;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20351;&#29992;&#20102;&#19968;&#20010;&#30697;&#38453;&#30340;RQ&#20998;&#35299;&#30340;&#25512;&#24191;&#65292;&#21462;&#20195;&#20102;&#36890;&#24120;&#30340;&#27491;&#20132;&#21644;&#19978;&#19977;&#35282;&#26465;&#20214;&#65292;&#32780;&#26159;&#29992;&#22522;&#20110;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30830;&#23450;&#30340;&#34892;&#30340;&#20559;&#24207;&#30340;&#31867;&#20284;&#26465;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#20998;&#31163;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal disentanglement seeks a representation of data involving latent variables that relate to one another via a causal model. A representation is identifiable if both the latent model and the transformation from latent to observed variables are unique. In this paper, we study observed variables that are a linear transformation of a linear latent causal model. Data from interventions are necessary for identifiability: if one latent variable is missing an intervention, we show that there exist distinct models that cannot be distinguished. Conversely, we show that a single intervention on each latent variable is sufficient for identifiability. Our proof uses a generalization of the RQ decomposition of a matrix that replaces the usual orthogonal and upper triangular conditions with analogues depending on a partial order on the rows of the matrix, with partial order determined by a latent causal model. We corroborate our theoretical results with a method for causal disentanglement that ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#21487;&#38752;&#24615;&#35780;&#20272;&#25215;&#35834;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#22522;&#20110; GNN &#30340;&#32467;&#26500;&#39044;&#27979;&#21457;&#30005;&#26426;&#30340;&#25215;&#35834;&#21644;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23545;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#34892;&#24615;&#20462;&#22797;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.15755</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#21487;&#38752;&#24615;&#35780;&#20272;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Confidence-Aware Graph Neural Networks for Learning Reliability Assessment Commitments. (arXiv:2211.15755v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#21487;&#38752;&#24615;&#35780;&#20272;&#25215;&#35834;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#22522;&#20110; GNN &#30340;&#32467;&#26500;&#39044;&#27979;&#21457;&#30005;&#26426;&#30340;&#25215;&#35834;&#21644;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23545;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#34892;&#24615;&#20462;&#22797;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#27604;&#20363;&#22686;&#21152;&#21644;&#39044;&#27979;&#35823;&#24046;&#30340;&#25552;&#39640;&#65292;&#21487;&#38752;&#24615;&#35780;&#20272;&#25215;&#35834;(Reliability Assessment Commitment, RAC)&#20248;&#21270;&#22312;&#30005;&#32593;&#36816;&#34892;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;RAC&#20844;&#24335;&#25193;&#23637;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#23427;&#25552;&#20986;&#20102;RACLearn&#65292;&#21033;&#29992;&#22522;&#20110;Graph Neural Network (GNN)&#30340;&#26550;&#26500;&#26469;&#39044;&#27979;&#21457;&#30005;&#26426;&#30340;&#25215;&#35834;&#21644;&#32447;&#36335;&#32422;&#26463;&#65292;&#20026;&#27599;&#20010;&#25215;&#35834;&#39044;&#27979;&#20851;&#32852;&#19968;&#20010;&#32622;&#20449;&#24230;&#20540;&#65292;&#24182;&#36873;&#25321;&#19968;&#32452;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#65292;&#23545;&#20854;&#36827;&#34892;&#21487;&#34892;&#24615;&#20462;&#22797;&#65292;&#24182;&#21033;&#29992;&#21487;&#34892;&#30340;&#39044;&#27979;&#19982;&#32422;&#26463;&#29366;&#24577;&#24341;&#23548;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RACLearn&#22312;Midcontinent Independent System Operator (MISO)&#20351;&#29992;&#30340;&#20934;&#30830;RAC&#20844;&#24335;&#19978;&#65292;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability Assessment Commitment (RAC) Optimization is increasingly important in grid operations due to larger shares of renewable generations in the generation mix and increased prediction errors. Independent System Operators (ISOs) also aim at using finer time granularities, longer time horizons, and possibly stochastic formulations for additional economic and reliability benefits. The goal of this paper is to address the computational challenges arising in extending the scope of RAC formulations. It presents RACLearn that (1) uses a Graph Neural Network (GNN) based architecture to predict generator commitments and active line constraints, (2) associates a confidence value to each commitment prediction, (3) selects a subset of the high-confidence predictions, which are (4) repaired for feasibility, and (5) seeds a state-of-the-art optimization algorithm with feasible predictions and active constraints. Experimental results on exact RAC formulations used by the Midcontinent Independe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#65292;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.11679</link><description>&lt;p&gt;
&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mean Shift Mask Transformer for Unseen Object Instance Segmentation. (arXiv:2211.11679v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#65292;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#23454;&#20363;&#30340;&#20998;&#21106;&#26159;&#26426;&#22120;&#20154;&#38656;&#35201;&#25484;&#25569;&#30340;&#20851;&#38190;&#24863;&#30693;&#25216;&#33021;&#20043;&#19968;&#65292;&#23427;&#26377;&#21161;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#20316;&#26410;&#30693;&#29289;&#20307;&#12290;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#31639;&#27861;&#19981;&#21487;&#24494;&#20998;&#65292;&#20351;&#20854;&#38590;&#20197;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65288;MSMFormer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#27169;&#25311; von Mises-Fisher&#65288;vMF&#65289;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#31639;&#27861;&#65292;&#20801;&#35768;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#12290;&#20854;&#26680;&#24515;&#32452;&#20214;&#26159;&#36229;&#29699;&#38754;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#22312;&#36229;&#29699;&#38754;&#19978;&#26356;&#26032;&#29289;&#20307;&#26597;&#35810;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;MSMFormer&#24212;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MSMFormer&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36716;&#31227;&#23398;&#20064;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#26631;&#25968;&#25454;&#38598;&#22823;&#26102;&#65292;&#27979;&#35797;&#35823;&#24046;&#28436;&#21464;&#20250;&#26377;&#26356;&#26126;&#26174;&#30340;&#21452;&#23792;&#25928;&#24212;&#12290;&#26356;&#22823;&#30340;&#28304;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#23548;&#33268;&#36739;&#24930;&#30340;&#30446;&#26631;DNN&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20923;&#32467;&#23618;&#25968;&#37327;&#21487;&#20197;&#30830;&#23450;&#36716;&#31227;&#23398;&#20064;&#26159;&#21542;&#26377;&#25928;&#22320;&#20302;&#20110;...</title><link>http://arxiv.org/abs/2211.11074</link><description>&lt;p&gt;
&#20923;&#32467;&#36807;&#24230;&#21442;&#25968;&#21270;&#65306;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36716;&#31227;&#23398;&#20064;&#30340;&#21452;&#23792;&#36879;&#35270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Frozen Overparameterization: A Double Descent Perspective on Transfer Learning of Deep Neural Networks. (arXiv:2211.11074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36716;&#31227;&#23398;&#20064;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#26631;&#25968;&#25454;&#38598;&#22823;&#26102;&#65292;&#27979;&#35797;&#35823;&#24046;&#28436;&#21464;&#20250;&#26377;&#26356;&#26126;&#26174;&#30340;&#21452;&#23792;&#25928;&#24212;&#12290;&#26356;&#22823;&#30340;&#28304;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#23548;&#33268;&#36739;&#24930;&#30340;&#30446;&#26631;DNN&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20923;&#32467;&#23618;&#25968;&#37327;&#21487;&#20197;&#30830;&#23450;&#36716;&#31227;&#23398;&#20064;&#26159;&#21542;&#26377;&#25928;&#22320;&#20302;&#20110;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36716;&#31227;&#23398;&#20064;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#25105;&#20204;&#37319;&#29992;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#35270;&#35282;&#8212;&#8212;&#29305;&#24449;&#25554;&#20540;&#65288;&#21363;&#36817;&#20284;&#20110;&#38646;&#30340;&#35757;&#32451;&#35823;&#24046;&#65289;&#21644;&#21452;&#23792;&#29616;&#35937;&#8212;&#8212;&#26469;&#35299;&#37322;&#36716;&#31227;&#23398;&#20064;&#35774;&#32622;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24494;&#22937;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#22312;&#30446;&#26631;DNN&#35757;&#32451;&#20013;&#20445;&#25345;&#20923;&#32467;&#30340;&#36716;&#31227;&#23618;&#25968;&#37327;&#20197;&#21450;&#28304;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#23545;&#36716;&#31227;&#23398;&#20064;&#27867;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#30446;&#26631;&#35757;&#32451;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#22312;&#30446;&#26631;DNN&#35757;&#32451;&#36807;&#31243;&#20013;&#27979;&#35797;&#35823;&#24046;&#28436;&#21464;&#20250;&#26377;&#26356;&#26126;&#26174;&#30340;&#21452;&#23792;&#25928;&#24212;&#12290;&#27492;&#22806;&#65292;&#26356;&#22823;&#30340;&#28304;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#33021;&#23548;&#33268;&#36739;&#24930;&#30340;&#30446;&#26631;DNN&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20923;&#32467;&#23618;&#25968;&#37327;&#21487;&#20197;&#30830;&#23450;&#36716;&#31227;&#23398;&#20064;&#26159;&#21542;&#26377;&#25928;&#22320;&#20302;&#20110;...
&lt;/p&gt;
&lt;p&gt;
We study the generalization behavior of transfer learning of deep neural networks (DNNs). We adopt the overparameterization perspective -- featuring interpolation of the training data (i.e., approximately zero train error) and the double descent phenomenon -- to explain the delicate effect of the transfer learning setting on generalization performance. We study how the generalization behavior of transfer learning is affected by the dataset size in the source and target tasks, the number of transferred layers that are kept frozen in the target DNN training, and the similarity between the source and target tasks. We show that the test error evolution during the target DNN training has a more significant double descent effect when the target training dataset is sufficiently large. In addition, a larger source training dataset can yield a slower target DNN training. Moreover, we demonstrate that the number of frozen layers can determine whether the transfer learning is effectively underpar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#26377;&#36164;&#28304;&#32447;&#24615;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#36739;&#20302;&#30340;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#31639;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.07484</link><description>&lt;p&gt;
&#24102;&#35013;&#36733;&#21644;&#35206;&#30422;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#65306;&#22522;&#20110;&#22238;&#24402;&#30340;&#27169;&#22359;&#21270;Lagrangian&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression. (arXiv:2211.07484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#26377;&#36164;&#28304;&#32447;&#24615;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#36739;&#20302;&#30340;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#31639;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#20854;&#20013;&#31639;&#27861;&#22312;&#24635;&#28040;&#36153;&#30340;&#32447;&#24615;&#32422;&#26463;&#19979;&#20351;&#29992;&#22810;&#20010;&#36164;&#28304;&#12290;&#36825;&#20010;&#38382;&#39064;&#25512;&#24191;&#20102;&#24102;&#32972;&#21253;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;(CBwK)&#65292;&#20801;&#35768;&#35013;&#36733;&#21644;&#35206;&#30422;&#32422;&#26463;&#65292;&#20197;&#21450;&#27491;&#36127;&#36164;&#28304;&#28040;&#32791;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#33021;&#22815;&#23454;&#29616;&#36864;&#21270;&#30340;&#21518;&#24724;&#12290;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#23545;&#20110;CBwK&#65292;&#23427;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;LagrangianBwK(Immorlica&#31561;&#20154;&#65292;FOCS 2019)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;CBwK&#30340;Lagrangian&#25216;&#26415;&#65292;&#20197;&#21450;SquareCB(Foster&#21644;Rakhlin&#65292;ICML 2020)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#30340;&#22238;&#24402;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#26412;&#36136;&#19978;&#30340;&#27169;&#22359;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We present a new algorithm that is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for CBwK when an algorithm must stop once some constraint is violated. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65292;&#35299;&#20026;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#24182;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#34987;&#31216;&#20026;Gibbs&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.06617</link><description>&lt;p&gt;
&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Empirical Risk Minimization with Relative Entropy Regularization. (arXiv:2211.06617v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65292;&#35299;&#20026;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#24182;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#34987;&#31216;&#20026;Gibbs&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20551;&#23450;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65288;measure&#65289;&#32780;&#38750;&#27010;&#29575;&#27979;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM-RER&#65289;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;ERM-RER&#38382;&#39064;&#30340;&#27867;&#21270;&#65292;&#20801;&#35768;&#26356;&#22823;&#31243;&#24230;&#22320;&#28789;&#27963;&#22320;&#24182;&#20837;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#36825;&#20123;&#24615;&#36136;&#20013;&#65292;&#22914;&#26524;&#23384;&#22312;ERM-RER&#38382;&#39064;&#30340;&#35299;&#65292;&#21017;&#35813;&#35299;&#26159;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#65292;&#36890;&#24120;&#19982;&#21442;&#32771;&#27979;&#24230;&#30456;&#20114;&#32477;&#23545;&#36830;&#32493;&#12290;&#36825;&#26679;&#30340;&#35299;&#23545;&#20110;ERM&#38382;&#39064;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#65292;&#32780;&#19981;&#38656;&#20851;&#24515;ERM&#38382;&#39064;&#26159;&#21542;&#26377;&#35299;&#12290;&#24403;&#20174;ERM-RER&#38382;&#39064;&#30340;&#35299;&#25277;&#21462;&#27169;&#22411;&#26102;&#65292;&#22266;&#23450;&#25968;&#25454;&#38598;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#20122;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#65288;Gibbs&#31639;&#27861;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The empirical risk minimization (ERM) problem with relative entropy regularization (ERM-RER) is investigated under the assumption that the reference measure is a {\sigma}-finite measure, and not necessarily a probability measure. Under this assumption, which leads to a generalization of the ERM-RER problem allowing a larger degree of flexibility for incorporating prior knowledge, numerous relevant properties are stated. Among these properties, the solution to this problem, if it exists, is shown to be a unique probability measure, often mutually absolutely continuous with the reference measure. Such a solution exhibits a probably-approximately-correct guarantee for the ERM problem independently of whether the latter possesses a solution. For a fixed dataset, the empirical risk is shown to be a sub-Gaussian random variable when the models are sampled from the solution to the ERM-RER problem. The generalization capabilities of the solution to the ERM-RER problem (the Gibbs algorithm) are
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20998;&#31163;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20849;&#21516;&#25551;&#36848;&#20102;Al&#30340;&#28293;&#23556;&#21644;&#34180;&#33180;&#29983;&#38271;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#34920;&#38754;&#29366;&#24577;&#21644;&#32570;&#38519;&#32467;&#26500;&#65292;&#20840;&#38754;&#25551;&#36848;&#20102;&#22522;&#26412;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.04796</link><description>&lt;p&gt;
&#29289;&#29702;&#20998;&#31163;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;Ar&#31561;&#31163;&#23376;&#20307;&#25918;&#30005;&#20013;Al&#28293;&#23556;&#21644;&#34180;&#33180;&#27785;&#31215;&#30340;&#21021;&#26399;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
Physics-separating artificial neural networks for predicting initial stages of Al sputtering and thin film deposition in Ar plasma discharges. (arXiv:2211.04796v1 [cond-mat.mtrl-sci] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04796
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20998;&#31163;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20849;&#21516;&#25551;&#36848;&#20102;Al&#30340;&#28293;&#23556;&#21644;&#34180;&#33180;&#29983;&#38271;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#34920;&#38754;&#29366;&#24577;&#21644;&#32570;&#38519;&#32467;&#26500;&#65292;&#20840;&#38754;&#25551;&#36848;&#20102;&#22522;&#26412;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Al&#34180;&#33180;&#28293;&#23556;&#27785;&#31215;&#30340;&#27169;&#25311;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#31561;&#31163;&#23376;&#20307;&#21644;&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#27169;&#22411;&#12290;&#24314;&#31435;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#26356;&#39640;&#30340;&#25277;&#35937;&#31243;&#24230;&#21644;&#24573;&#30053;&#22522;&#26412;&#30340;&#21407;&#23376;&#20445;&#30495;&#24230;&#12290;&#20197;&#21069;&#20851;&#20110;&#28293;&#23556;&#36807;&#31243;&#30340;&#24037;&#20316;&#36890;&#36807;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#26412;&#30340;&#34920;&#38754;&#29366;&#24577;&#65288;&#21363;&#21270;&#23398;&#35745;&#37327;&#65289;&#20316;&#20026;&#38745;&#24577;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#34920;&#38754;&#29366;&#24577;&#21644;&#32570;&#38519;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#20998;&#31163;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#20849;&#21516;&#25551;&#36848;&#28293;&#23556;&#21644;&#29983;&#38271;&#12290;&#25551;&#36848;&#31561;&#31163;&#23376;&#20307;-&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#26469;&#33258;&#20110;Al&#20013;&#24615;&#31890;&#23376;&#21644;Ar$^+$&#31163;&#23376;&#25758;&#20987;Al&#65288;001&#65289;&#34920;&#38754;&#30340;&#28151;&#21512;&#21453;&#24212;&#20998;&#23376;&#21160;&#21147;&#23398;/&#26102;&#38388;&#25139;&#21147;&#20559;&#24046;Monte Carlo&#27169;&#25311;&#12290;&#35777;&#26126;&#36890;&#36807;&#32771;&#34385;&#34920;&#38754;&#29366;&#24577;&#21644;&#32570;&#38519;&#32467;&#26500;&#26469;&#20840;&#38754;&#25551;&#36848;&#22522;&#26412;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#23398;&#20064;&#31561;&#31163;&#23376;&#20307;-&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;su
&lt;/p&gt;
&lt;p&gt;
Simulations of Al thin film sputter depositions rely on accurate plasma and surface interaction models. Establishing the latter commonly requires a higher level of abstraction and means to dismiss the fundamental atomic fidelity. Previous works on sputtering processes addressed this issue by establishing machine learning surrogate models, which include a basic surface state (i.e., stoichiometry) as static input. In this work, an evolving surface state and defect structure are introduced to jointly describe sputtering and growth with physics-separating artificial neural networks. The data describing the plasma-surface interactions stem from hybrid reactive molecular dynamics/time-stamped force bias Monte Carlo simulations of Al neutrals and Ar$^+$ ions impinging onto Al(001) surfaces. It is demonstrated that the fundamental processes are comprehensively described by taking the surface state as well as defect structure into account. Hence, a machine learning plasma-surface interaction su
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#29109;&#26368;&#20248;&#20256;&#36755;(EOT)&#35745;&#21010;&#65292;&#23427;&#20855;&#26377;&#31471;&#21040;&#31471;&#23398;&#20064;&#12289;&#24555;&#36895;&#25512;&#29702;&#21644;&#22788;&#29702;&#23567;&#20540;&#29109;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#20248;&#28857;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;EOT&#20219;&#21153;&#20013;&#21457;&#25381;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.01156</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#36807;&#31243;&#30340;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Entropic Neural Optimal Transport via Diffusion Processes. (arXiv:2211.01156v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01156
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#29109;&#26368;&#20248;&#20256;&#36755;(EOT)&#35745;&#21010;&#65292;&#23427;&#20855;&#26377;&#31471;&#21040;&#31471;&#23398;&#20064;&#12289;&#24555;&#36895;&#25512;&#29702;&#21644;&#22788;&#29702;&#23567;&#20540;&#29109;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#20248;&#28857;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;EOT&#20219;&#21153;&#20013;&#21457;&#25381;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#29109;&#26368;&#20248;&#20256;&#36755;(EOT)&#35745;&#21010;&#65292;&#36825;&#20123;&#20998;&#24067;&#21487;&#36890;&#36807;&#26679;&#26412;&#33719;&#24471;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#21160;&#24577;&#29256;&#26412;EOT&#30340;&#38797;&#28857;&#37325;&#26500;&#65292;&#21363;Schr&#246;dinger&#26725;&#38382;&#39064;&#12290;&#19982;&#22823;&#35268;&#27169;EOT&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31471;&#21040;&#31471;&#30340;&#65292;&#30001;&#21333;&#20010;&#23398;&#20064;&#27493;&#39588;&#32452;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#20801;&#35768;&#22788;&#29702;&#29109;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#23567;&#20540;&#65292;&#36825;&#22312;&#26576;&#20123;&#23454;&#38469;&#24212;&#29992;&#38382;&#39064;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22823;&#35268;&#27169;EOT&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel neural algorithm for the fundamental problem of computing the entropic optimal transport (EOT) plan between continuous probability distributions which are accessible by samples. Our algorithm is based on the saddle point reformulation of the dynamic version of EOT which is known as the Schr\"odinger Bridge problem. In contrast to the prior methods for large-scale EOT, our algorithm is end-to-end and consists of a single learning step, has fast inference procedure, and allows handling small values of the entropy regularization coefficient which is of particular importance in some applied problems. Empirically, we show the performance of the method on several large-scale EOT tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#20174;&#28508;&#22312;&#35828;&#35805;&#20154;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19968;&#33268;&#24615;&#21644;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#26356;&#24378;&#30340;&#22768;&#38899;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.00375</link><description>&lt;p&gt;
&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#22768;&#38899;
&lt;/p&gt;
&lt;p&gt;
Generating Multilingual Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#20174;&#28508;&#22312;&#35828;&#35805;&#20154;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19968;&#33268;&#24615;&#21644;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#26356;&#24378;&#30340;&#22768;&#38899;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#30340;&#24615;&#21035;&#26159;&#20854;&#34987;&#24863;&#30693;&#36523;&#20221;&#30340;&#20851;&#38190;&#20803;&#32032;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30028;&#38754;&#24320;&#22987;&#37319;&#29992;&#19981;&#26126;&#30830;&#30340;&#24615;&#21035;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#30028;&#23450;&#20026;&#30007;&#24615;&#25110;&#22899;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#65292;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#29983;&#25104;&#26032;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#35821;&#38899;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#26377;&#25928;&#22320;&#20174;&#28508;&#22312;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#26469;&#23454;&#29616;&#30340;&#12290;&#24191;&#27867;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#28165;&#26970;&#22320;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#22768;&#38899;&#65292;&#36825;&#20123;&#22768;&#38899;&#22312;&#25152;&#26377;&#32771;&#23519;&#30340;&#35821;&#35328;&#20013;&#37117;&#34987;&#35748;&#20026;&#27604;&#22522;&#32447;&#22768;&#38899;&#26356;&#20855;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24615;&#21035;&#35748;&#30693;&#34987;&#21457;&#29616;&#22312;&#21548;&#20247;&#30340;&#20004;&#20010;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65306;&#27597;&#35821;&#21644;&#24615;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21487;&#38752;&#22320;&#29983;&#25104;&#22810;&#31181;&#24615;&#21035;&#19981;&#26126;&#30830;&#22768;&#38899;&#30340;&#31995;&#32479;&#24615;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gender of any voice user interface is a key element of its perceived identity. Recently, there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#25191;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#25351;&#23450;&#39640;&#32500;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#26063;&#26102;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#26080;&#32541;&#33719;&#21462;&#21644;&#34920;&#31034;&#22797;&#26434;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2210.13319</link><description>&lt;p&gt;
MARS: &#20989;&#25968;&#31354;&#38388;&#20013;&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MARS: Meta-Learning as Score Matching in the Function Space. (arXiv:2210.13319v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#25191;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#25351;&#23450;&#39640;&#32500;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#26063;&#26102;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#26080;&#32541;&#33719;&#21462;&#21644;&#34920;&#31034;&#22797;&#26434;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26088;&#22312;&#20174;&#19968;&#32452;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#22312;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#25351;&#23450;&#19968;&#32452;&#21487;&#34892;&#30340;&#39640;&#32500;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#26063;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#20803;&#23398;&#20064;&#38480;&#21046;&#24615;&#30340;&#23545;&#35282;&#39640;&#26031;&#20808;&#39564;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#20989;&#25968;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#35270;&#35282;&#26469;&#30475;&#24453;&#20803;&#23398;&#20064;&#65292;&#23558;&#20808;&#39564;&#35270;&#20026;&#38543;&#26426;&#36807;&#31243;&#65292;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#25191;&#34892;&#25512;&#29702;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#20803;&#35757;&#32451;&#20219;&#21153;&#35270;&#20026;&#20174;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#23558;&#20803;&#23398;&#20064;&#24418;&#24335;&#21270;&#20026;&#32463;&#39564;&#20272;&#35745;&#36825;&#20010;&#38543;&#26426;&#36807;&#31243;&#30340;&#23450;&#24459;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20803;&#23398;&#20064;&#20998;&#25968;&#20989;&#25968;&#65292;&#26080;&#32541;&#33719;&#21462;&#21644;&#34920;&#31034;&#22797;&#26434;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning aims to extract useful inductive biases from a set of related datasets. In Bayesian meta-learning, this is typically achieved by constructing a prior distribution over neural network parameters. However, specifying families of computationally viable prior distributions over the high-dimensional neural network parameters is difficult. As a result, existing approaches resort to meta-learning restrictive diagonal Gaussian priors, severely limiting their expressiveness and performance. To circumvent these issues, we approach meta-learning through the lens of functional Bayesian neural network inference, which views the prior as a stochastic process and performs inference in the function space. Specifically, we view the meta-training tasks as samples from the data-generating process and formalize meta-learning as empirically estimating the law of this stochastic process. Our approach can seamlessly acquire and represent complex prior knowledge by meta-learning the score functi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroPrim&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#29983;&#25104;&#26641;&#38382;&#39064;&#65292;&#37319;&#29992;Prim&#31639;&#27861;&#20943;&#23569;&#20102;&#21160;&#20316;&#21644;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;REINFORCE&#35757;&#32451;&#20102;&#32467;&#26524;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#19977;&#20010;&#22256;&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.12453</link><description>&lt;p&gt;
NeuroPrim: &#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;NP-hard&#29983;&#25104;&#26641;&#38382;&#39064;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeuroPrim: An Attention-based Model for Solving NP-hard Spanning Tree Problems. (arXiv:2210.12453v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroPrim&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#29983;&#25104;&#26641;&#38382;&#39064;&#65292;&#37319;&#29992;Prim&#31639;&#27861;&#20943;&#23569;&#20102;&#21160;&#20316;&#21644;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;REINFORCE&#35757;&#32451;&#20102;&#32467;&#26524;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#19977;&#20010;&#22256;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#29305;&#27530;&#32422;&#26463;&#26465;&#20214;&#30340;&#29983;&#25104;&#26641;&#38382;&#39064;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24448;&#24448;&#38590;&#20197;&#35299;&#20915;&#65292;&#36890;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#31639;&#27861;&#35774;&#35745;&#21644;&#25351;&#25968;&#32423;&#30340;&#26102;&#38388;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#39030;&#28857;&#24207;&#21015;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#30001;&#36793;&#26500;&#25104;&#35299;&#38598;&#30340;&#19968;&#33324;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#21508;&#31181;&#29983;&#25104;&#26641;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroPrim&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;&#22270;&#19978;&#30340;&#19968;&#33324;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23450;&#20041;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#35299;&#20915;&#21508;&#31181;&#29983;&#25104;&#26641;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Prim&#31639;&#27861;&#20943;&#23569;&#20102;&#21160;&#20316;&#21644;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;REINFORCE&#35757;&#32451;&#20102;&#32467;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#19977;&#20010;&#22256;&#38590;&#38382;&#39064;&#65306;&#24230;&#32422;&#26463;&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;DCMST&#65289;&#38382;&#39064;&#65292;&#26368;&#23567;&#36335;&#30001;&#25104;&#26412;&#29983;&#25104;&#26641;&#65288;MRCT&#65289;&#38382;&#39064;&#21644;&#26368;&#23567;&#23454;&#38469;&#36317;&#31163;&#29983;&#25104;&#26641;&#30340;&#38382;&#39064;&#65288;MST&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spanning tree problems with specialized constraints can be difficult to solve in real-world scenarios, often requiring intricate algorithmic design and exponential time. Recently, there has been growing interest in end-to-end deep neural networks for solving routing problems. However, such methods typically produce sequences of vertices, which makes it difficult to apply them to general combinatorial optimization problems where the solution set consists of edges, as in various spanning tree problems. In this paper, we propose NeuroPrim, a novel framework for solving various spanning tree problems by defining a Markov Decision Process (MDP) for general combinatorial optimization problems on graphs. Our approach reduces the action and state space using Prim's algorithm and trains the resulting model using REINFORCE. We apply our framework to three difficult problems on Euclidean space: the Degree-constrained Minimum Spanning Tree (DCMST) problem, the Minimum Routing Cost Spanning Tree (M
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;MRIs&#30340;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#22810;&#20998;&#25903;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#24182;&#36798;&#21040;&#20102;99.05%&#30340;&#19977;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.12331</link><description>&lt;p&gt;
&#28145;&#24230;&#22810;&#20998;&#25903;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#29992;&#20110;&#22522;&#20110;&#33041;MRIs&#30340;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Multi-Branch CNN Architecture for Early Alzheimer's Detection from Brain MRIs. (arXiv:2210.12331v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;MRIs&#30340;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#22810;&#20998;&#25903;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#24182;&#36798;&#21040;&#20102;99.05%&#30340;&#19977;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#21487;&#33021;&#23548;&#33268;&#30196;&#21574;&#21644;&#20005;&#37325;&#30340;&#22823;&#33041;&#21151;&#33021;&#19979;&#38477;&#65292;&#22914;&#26524;&#27809;&#26377;&#39044;&#38450;&#24615;&#25252;&#29702;&#65292;&#20250;&#25233;&#21046;&#31616;&#21333;&#20219;&#21153;&#30340;&#23436;&#25104;&#12290;&#36926;9&#20998;&#20043;1&#30340;&#32654;&#22269;&#20154;&#24739;&#26377;&#30001;AD&#24341;&#36215;&#30340;&#30196;&#21574;&#30151;&#65292;&#26410;&#24471;&#21040;&#25253;&#37228;&#30340;AD&#30456;&#20851;&#30196;&#21574;&#24739;&#32773;&#25252;&#29702;&#20215;&#20540;&#20026;2716&#20159;&#32654;&#20803;&#12290;&#22240;&#27492;&#65292;&#20026;&#38450;&#27490;AD&#36827;&#19968;&#27493;&#36827;&#23637;&#65292;&#24050;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#26089;&#26399;AD&#35786;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#20854;&#20182;&#21487;&#29992;&#20110;&#26089;&#26399;AD&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#65288;ADNI&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;7866819&#20010;&#21442;&#25968;&#32452;&#25104;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#19977;&#20010;&#19981;&#21516;&#30340;&#21367;&#31215;&#20998;&#25903;&#65292;&#27599;&#20010;&#20998;&#25903;&#30340;&#38271;&#24230;&#19981;&#21516;&#12290;&#27599;&#20010;&#20998;&#25903;&#30001;&#19981;&#21516;&#30340;&#21367;&#31215;&#26680;&#22823;&#23567;&#32452;&#25104;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#24739;&#26377;&#38750;&#30196;&#21574;&#12289;&#36731;&#24230;&#30196;&#21574;&#25110;&#20013;&#24230;&#30196;&#21574;&#30340;&#24739;&#32773;&#65292;&#20854;&#19977;&#20998;&#31867;&#20934;&#30830;&#29575;&#20026;99.05&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is a neuro-degenerative disease that can cause dementia and result severe reduction in brain function inhibiting simple tasks especially if no preventative care is taken. Over 1 in 9 Americans suffer from AD induced dementia and unpaid care for people with AD related dementia is valued at $271.6 billion. Hence, various approaches have been developed for early AD diagnosis to prevent its further progression. In this paper, we first review other approaches that could be used for early detection of AD. We then give an overview of our dataset that was from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and propose a deep Convolutional Neural Network (CNN) architecture consisting of 7,866,819 parameters. This model has three different convolutional branches with each having a different length. Each branch is comprised of different kernel sizes. This model can predict whether a patient is non-demented, mild-demented, or moderately demented with a 99.05% three
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#21453;&#28436;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#38450;&#24481;&#26426;&#21046;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2210.10880</link><description>&lt;p&gt;
&#23398;&#20064;&#21453;&#28436;&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#29992;&#20110;&#26799;&#24230;&#21453;&#28436;&#30340;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Learning to Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning. (arXiv:2210.10880v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#21453;&#28436;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#38450;&#24481;&#26426;&#21046;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#21487;&#20197;&#20174;&#32852;&#37030;&#23398;&#20064; (FL) &#27169;&#22411;&#26799;&#24230;&#20013;&#24674;&#22797;&#35757;&#32451;&#26679;&#26412;&#65292;&#26500;&#25104;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20005;&#37325;&#23041;&#32961;&#12290;&#20026;&#20102;&#20943;&#36731;&#27492;&#28431;&#27934;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#21407;&#21017;&#24615;&#38450;&#24481;&#21644;&#22522;&#20110;&#26799;&#24230;&#21387;&#32553;&#30340;&#21551;&#21457;&#24335;&#38450;&#24481;&#26469;&#20316;&#20026;&#23545;&#31574;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#38450;&#24481;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#26799;&#24230;&#21387;&#32553;&#30340;&#38450;&#24481;&#65292;&#35813;&#38450;&#24481;&#20801;&#35768;&#27169;&#22411;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#21457;&#29616;&#20302;&#20272;&#20102; FL &#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#20316;&#20026;&#21453;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20010;&#22312;&#36741;&#21161;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#21453;&#28436;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient inversion attack enables recovery of training samples from model gradients in federated learning (FL), and constitutes a serious threat to data privacy. To mitigate this vulnerability, prior work proposed both principled defenses based on differential privacy, as well as heuristic defenses based on gradient compression as countermeasures. These defenses have so far been very effective, in particular those based on gradient compression that allow the model to maintain high accuracy while greatly reducing the effectiveness of attacks. In this work, we argue that such findings underestimate the privacy risk in FL. As a counterexample, we show that existing defenses can be broken by a simple adaptive attack, where a model trained on auxiliary data is able to invert gradients on both vision and language tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#28608;&#27963;&#22270;&#30340;&#31232;&#30095;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#25968;&#30340;&#21464;&#21387;&#22120;&#37197;&#32622;&#21644;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#20013;&#37117;&#20986;&#29616;&#20102;&#31232;&#30095;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2210.06313</link><description>&lt;p&gt;
&#24608;&#24816;&#31070;&#32463;&#20803;&#29616;&#35937;&#65306;&#21464;&#21387;&#22120;&#27169;&#22411;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers. (arXiv:2210.06313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#28608;&#27963;&#22270;&#30340;&#31232;&#30095;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#25968;&#30340;&#21464;&#21387;&#22120;&#37197;&#32622;&#21644;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#20013;&#37117;&#20986;&#29616;&#20102;&#31232;&#30095;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28608;&#27963;&#22270;&#31232;&#30095;&#30340;&#22855;&#29305;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20013;&#38388;&#23618;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#36755;&#20986;&#26469;&#34920;&#31034;&#28608;&#27963;&#22270;&#65292;&#31232;&#30095;&#26159;&#25351;&#24179;&#22343;&#24773;&#20917;&#19979;&#27599;&#20010;&#36755;&#20837;&#21040;MLP&#30340;&#38750;&#38646;&#20803;&#32032;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;T5-Base&#20026;3.0&#65285;&#65292;ViT-B16&#20026;6.3&#65285;&#65289;&#12290;&#27492;&#22806;&#65292;&#36739;&#22823;&#30340;&#21464;&#21387;&#22120;&#21644;&#26356;&#23485;&#30340;MLP&#38544;&#34255;&#23618;&#32500;&#24230;&#20250;&#20135;&#29983;&#26356;&#31232;&#30095;&#30340;&#28608;&#27963;&#22270;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31232;&#30095;&#30340;&#20986;&#29616;&#26159;&#19968;&#31181;&#26222;&#36941;&#29616;&#35937;&#65292;&#23427;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#20986;&#29616;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#20013;&#65292;&#22312;&#19981;&#21516;&#23618;&#25968;&#30340;&#21464;&#21387;&#22120;&#37197;&#32622;&#21644;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#20063;&#21253;&#25324;MLP-&#28151;&#21512;&#22120;&#21644;2&#23618;MLP&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20351;&#29992;&#20855;&#26377;&#38543;&#26426;&#26631;&#31614;&#25110;&#38543;&#26426;&#36755;&#20837;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20063;&#20250;&#20986;&#29616;&#31232;&#30095;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the curious phenomenon for machine learning models with Transformer architectures that their activation maps are sparse. By activation map we refer to the intermediate output of the multi-layer perceptrons (MLPs) after a ReLU activation function, and by sparse we mean that on average very few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each input to MLP. Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels, as well as for other architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also emerges using training datasets with random labels, or with random inputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#23545;&#20110;Manifold-Mixup&#26041;&#27861;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.03123</link><description>&lt;p&gt;
&#28151;&#21512;&#27744;&#21270;&#22312;&#22522;&#20110;Mixup&#30340;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing. (arXiv:2210.03123v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#23545;&#20110;Manifold-Mixup&#26041;&#27861;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22270;&#24418;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#21644;&#28304;&#20195;&#30721;&#20998;&#31867;&#26041;&#38754;&#12290;&#36890;&#24120;&#65292;GNN&#26159;&#30001;&#20132;&#26367;&#22270;&#23618;&#21644;&#22270;&#27744;&#21270;&#23618;&#26500;&#25104;&#30340;&#65292;&#20132;&#26367;&#22270;&#23618;&#21487;&#20197;&#23398;&#20064;&#22270;&#33410;&#28857;&#29305;&#24449;&#30340;&#36716;&#25442;&#65292;&#32780;&#22270;&#27744;&#21270;&#23618;&#21017;&#20351;&#29992;&#22270;&#27744;&#21270;&#31639;&#23376;&#65288;&#20363;&#22914;Max&#27744;&#21270;&#65289;&#26377;&#25928;&#22320;&#20943;&#23569;&#33410;&#28857;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#22270;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#22686;&#24378;GNN&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20154;&#20204;&#24191;&#27867;&#37319;&#29992;&#20102;Manifold-Mixup&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#32447;&#24615;&#28151;&#21512;&#19968;&#23545;&#22270;&#25968;&#25454;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#26469;&#29983;&#25104;&#21512;&#25104;&#22270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;Manifold-Mixup&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#22270;&#27744;&#21270;&#31639;&#23376;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#24182;&#27809;&#26377;&#36827;&#34892;&#24456;&#22810;&#20851;&#20110;&#36825;&#31181;&#24433;&#21709;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#26089;&#26399;&#25506;&#32034;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;Mixup&#30340;&#22270;&#24418;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;Max-pooling&#21644;Attention-pooling&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27744;&#21270;&#32467;&#26500;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN)-based graph learning has been popular in natural language and programming language processing, particularly in text and source code classification. Typically, GNNs are constructed by incorporating alternating layers which learn transformations of graph node features, along with graph pooling layers that use graph pooling operators (e.g., Max-pooling) to effectively reduce the number of nodes while preserving the semantic information of the graph. Recently, to enhance GNNs in graph learning tasks, Manifold-Mixup, a data augmentation technique that produces synthetic graph data by linearly mixing a pair of graph data and their labels, has been widely adopted. However, the performance of Manifold-Mixup can be highly affected by graph pooling operators, and there have not been many studies that are dedicated to uncovering such affection. To bridge this gap, we take an early step to explore how graph pooling operators affect the performance of Mixup-based graph le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#29575;&#32479;&#19968;&#20998;&#27835;&#32593;&#32476;&#65288;PPOU-Net&#65289;&#27169;&#22411;&#24212;&#29992;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#19978;&#30340;&#33258;&#36866;&#24212;&#38477;&#32500;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;&#26399;&#26395;&#26497;&#22823;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#32500;&#24230;&#24773;&#20917;&#19979;&#65292;PPOU-Net&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.02694</link><description>&lt;p&gt;
&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#30340;&#27010;&#29575;&#32479;&#19968;&#20998;&#27835;&#32593;&#32476;&#65288;PPOU-Net&#65289;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Probabilistic partition of unity networks for high-dimensional regression problems. (arXiv:2210.02694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#29575;&#32479;&#19968;&#20998;&#27835;&#32593;&#32476;&#65288;PPOU-Net&#65289;&#27169;&#22411;&#24212;&#29992;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#19978;&#30340;&#33258;&#36866;&#24212;&#38477;&#32500;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;&#26399;&#26395;&#26497;&#22823;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#32500;&#24230;&#24773;&#20917;&#19979;&#65292;PPOU-Net&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#27010;&#29575;&#32479;&#19968;&#20998;&#27835;&#32593;&#32476;(PPOU-Net)&#27169;&#22411;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38477;&#32500;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#30446;&#26631;&#20989;&#25968;&#34987;&#20302;&#32500;&#27969;&#24418;&#19978;&#27599;&#20010;&#32858;&#31867;&#20851;&#32852;&#30340;&#20855;&#26377;&#26412;&#22320;&#22266;&#23450;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#25152;&#36924;&#36817;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#26399;&#26395;&#26497;&#22823;(EM)&#31639;&#27861;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#20132;&#26367;&#25191;&#34892;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#65306;(i)&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032; DNN &#31995;&#25968;;(ii)&#20351;&#29992;&#20174;EM&#31639;&#27861;&#25512;&#23548;&#20986;&#30340;&#38381;&#24335;&#20844;&#24335;&#26356;&#26032;&#27599;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#22312;&#27010;&#29575;&#20844;&#24335;&#19979;&#65292;&#27493;&#39588;(ii)&#20801;&#35768;&#20005;&#37325;&#21487;&#24182;&#34892;&#30340;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#35299;&#30340;&#24418;&#24335;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#32500;&#24230;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#19982;&#21487;&#27604;&#36739;&#22823;&#23567;&#30340;&#22522;&#20934;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;PPOU-Net&#31283;&#23450;&#22320;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the probabilistic partition of unity network (PPOU-Net) model in the context of high-dimensional regression problems and propose a general framework focusing on adaptive dimensionality reduction. With the proposed framework, the target function is approximated by a mixture of experts model on a low-dimensional manifold, where each cluster is associated with a local fixed-degree polynomial. We present a training strategy that leverages the expectation maximization (EM) algorithm. During the training, we alternate between (i) applying gradient descent to update the DNN coefficients; and (ii) using closed-form formulae derived from the EM algorithm to update the mixture of experts model parameters. Under the probabilistic formulation, step (ii) admits the form of embarrassingly parallelizable weighted least-squares solves. The PPOU-Nets consistently outperform the baseline fully-connected neural networks of comparable sizes in numerical experiments of various data dimensions. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20808;&#39564;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23454;&#29616;&#23433;&#20840;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21516;&#26102;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36873;&#25321;&#31526;&#21512;&#23433;&#20840;&#35201;&#27714;&#30340;&#20808;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#20803;&#23398;&#20064;&#20808;&#39564;&#21152;&#24555;&#20102;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#25913;&#36827;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00762</link><description>&lt;p&gt;
&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Priors for Safe Bayesian Optimization. (arXiv:2210.00762v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20808;&#39564;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23454;&#29616;&#23433;&#20840;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21516;&#26102;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36873;&#25321;&#31526;&#21512;&#23433;&#20840;&#35201;&#27714;&#30340;&#20808;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#20803;&#23398;&#20064;&#20808;&#39564;&#21152;&#24555;&#20102;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#25913;&#36827;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#20248;&#21270;&#25511;&#21046;&#22120;&#21442;&#25968;&#24182;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#37327;&#21270;&#30446;&#26631;&#21644;&#32422;&#26463;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#23433;&#20840;&#22320;&#25351;&#23548;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#21487;&#38752;&#30340;&#27169;&#22411;&#36229;&#21442;&#25968;&#20197;&#36991;&#20813;&#23433;&#20840;&#36829;&#35268;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20154;&#24037;&#35774;&#35745;&#36866;&#21512;&#30340;&#27010;&#29575;&#27169;&#22411;&#21487;&#33021;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20808;&#39564;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23454;&#29616;&#23433;&#20840;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#20511;&#21161;&#20803;&#23398;&#20064;&#31639;&#27861; F-PACOH&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#22312;&#22522;&#20934;&#20989;&#25968;&#21644;&#39640;&#31934;&#24230;&#36816;&#21160;&#31995;&#32479;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#19981;&#30830;&#23450;&#24230;&#24230;&#37327;&#21644;&#21069;&#27839;&#25628;&#32034;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36873;&#25321;&#31526;&#21512;&#23433;&#20840;&#35201;&#27714;&#30340;&#20808;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#20803;&#23398;&#20064;&#20808;&#39564;&#21152;&#24555;&#20102;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#25913;&#36827;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In robotics, optimizing controller parameters under safety constraints is an important challenge. Safe Bayesian optimization (BO) quantifies uncertainty in the objective and constraints to safely guide exploration in such settings. Hand-designing a suitable probabilistic model can be challenging, however. In the presence of unknown safety constraints, it is crucial to choose reliable model hyper-parameters to avoid safety violations. Here, we propose a data-driven approach to this problem by meta-learning priors for safe BO from offline data. We build on a meta-learning algorithm, F-PACOH, capable of providing reliable uncertainty quantification in settings of data scarcity. As core contribution, we develop a novel framework for choosing safety-compliant priors in a data-riven manner via empirical uncertainty metrics and a frontier search algorithm. On benchmark functions and a high-precision motion system, we demonstrate that our meta-learned priors accelerate the convergence of safe 
&lt;/p&gt;</description></item><item><title>Holographic-(V)AE&#26159;&#19968;&#31181;&#22312;&#20613;&#31435;&#21494;&#31354;&#38388;&#20013;&#30340;SO(3)-&#31561;&#21464;&#24615;(Variational)&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#20998;&#24067;&#20110;&#25351;&#23450;3D&#21407;&#28857;&#21608;&#22260;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#24212;&#30340;&#28508;&#31354;&#38388;&#32534;&#30721;&#20102;&#29699;&#38754;&#22270;&#20687;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2209.15567</link><description>&lt;p&gt;
Holographic-(V)AE:&#19968;&#31181;&#22312;&#20613;&#31435;&#21494;&#31354;&#38388;&#20013;&#30340;SO(3)-&#31561;&#21464;&#24615;(Variational)&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Holographic-(V)AE: an end-to-end SO(3)-Equivariant (Variational) Autoencoder in Fourier Space. (arXiv:2209.15567v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15567
&lt;/p&gt;
&lt;p&gt;
Holographic-(V)AE&#26159;&#19968;&#31181;&#22312;&#20613;&#31435;&#21494;&#31354;&#38388;&#20013;&#30340;SO(3)-&#31561;&#21464;&#24615;(Variational)&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#20998;&#24067;&#20110;&#25351;&#23450;3D&#21407;&#28857;&#21608;&#22260;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#24212;&#30340;&#28508;&#31354;&#38388;&#32534;&#30721;&#20102;&#29699;&#38754;&#22270;&#20687;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#31561;&#21464;&#24615;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#25968;&#25454;&#25928;&#29575;&#39640;&#30340;&#35299;&#20915;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#23562;&#37325;&#25968;&#25454;&#30340;&#30456;&#20851;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30417;&#30563;&#29983;&#25104;&#26041;&#38754;&#65292;&#36825;&#31181;&#33539;&#20363;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20840;&#24687;&#24335;-(&#21464;&#20998;)&#33258;&#32534;&#30721;&#22120;&#65288;Holographic-(V)AE&#65289;&#30340;&#20840;&#23610;&#23544;SO(3)&#31561;&#21464;(Variational)&#33258;&#32534;&#30721;&#22120;&#65292;&#36866;&#29992;&#20110;&#20998;&#24067;&#22312;&#19977;&#32500;&#25351;&#23450;&#21407;&#28857;&#21608;&#22260;&#30340;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#12290;H-(V)AE&#34987;&#35757;&#32451;&#20026;&#37325;&#26500;&#25968;&#25454;&#30340;&#29699;&#24418;&#20613;&#31435;&#21494;&#32534;&#30721;&#65292;&#23398;&#20064;&#36807;&#31243;&#20013;&#33719;&#24471;&#20855;&#26377;&#26368;&#22823;&#20449;&#24687;&#20869;&#23481;&#30340;&#26059;&#36716;&#19981;&#21464;&#23884;&#20837;&#21644;&#29992;&#20110;&#25551;&#36848;&#25968;&#25454;&#26041;&#21521;&#30340;&#31561;&#21464;&#24103;&#30340;&#20302;&#32500;&#25968;&#25454;&#34920;&#24449;&#65288;&#21363;&#28508;&#31354;&#38388;&#65289;&#12290;&#25105;&#20204;&#23545;H-(V)AE&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#20064;&#21040;&#30340;&#28508;&#31354;&#38388;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#29699;&#38754;&#22270;&#20687;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group-equivariant neural networks have emerged as a data-efficient approach to solve classification and regression tasks, while respecting the relevant symmetries of the data. However, little work has been done to extend this paradigm to the unsupervised and generative domains. Here, we present Holographic-(Variational) Auto Encoder (H-(V)AE), a fully end-to-end SO(3)-equivariant (variational) autoencoder in Fourier space, suitable for unsupervised learning and generation of data distributed around a specified origin in 3D. H-(V)AE is trained to reconstruct the spherical Fourier encoding of data, learning in the process a low-dimensional representation of the data (i.e., a latent space) with a maximally informative rotationally invariant embedding alongside an equivariant frame describing the orientation of the data. We extensively test the performance of H-(V)AE on diverse datasets. We show that the learned latent space efficiently encodes the categorical features of spherical images.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12336</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32500;&#21487;&#36798;&#24615;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#23433;&#20840;&#20445;&#38556;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating Formal Safety Assurances for High-Dimensional Reachability. (arXiv:2209.12336v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12336
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#33258;&#20027;&#31995;&#32479;&#25552;&#20379;&#27491;&#24335;&#30340;&#23433;&#20840;&#21644;&#24615;&#33021;&#20445;&#35777;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#21704;&#23494;&#39039;-&#38597;&#31185;&#27604;&#65288;HJ&#65289;&#21487;&#36798;&#24615;&#20998;&#26512;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#25552;&#20379;&#36825;&#20123;&#20445;&#35777;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#38750;&#32447;&#24615;&#31995;&#32479;&#21160;&#24577;&#12289;&#26377;&#30028;&#23545;&#25239;&#31995;&#32479;&#24178;&#25200;&#20197;&#21450;&#29366;&#24577;&#21644;&#36755;&#20837;&#32422;&#26463;&#12290;&#20294;&#26159;&#65292;&#23427;&#28041;&#21450;&#21040;&#27714;&#35299;PDE&#65292;&#20854;&#35745;&#31639;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#38543;&#30528;&#29366;&#24577;&#32500;&#24230;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20351;&#20854;&#22312;&#22823;&#22411;&#31995;&#32479;&#19978;&#30340;&#30452;&#25509;&#20351;&#29992;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;DeepReach&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#27491;&#24358;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#26469;&#20811;&#26381;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#21487;&#36798;&#24615;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#35201;&#27714;&#38543;&#21487;&#36798;&#31649;&#22797;&#26434;&#24615;&#32780;&#19981;&#26159;&#29366;&#24577;&#31354;&#38388;&#32500;&#24230;&#32780;&#21464;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#20986;&#29616;&#38169;&#35823;&#65292;&#22240;&#27492;&#35745;&#31639;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#23433;&#20840;&#65292;&#36825;&#27809;&#26377;&#36798;&#21040;&#25105;&#20204;&#25552;&#20379;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#30340;&#24635;&#20307;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#24863;&#30693;&#30340;&#39640;&#32500;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing formal safety and performance guarantees for autonomous systems is becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing these guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the state dimensionality, making its direct use on large-scale systems intractable. A recently proposed method called DeepReach overcomes this challenge by leveraging a sinusoidal neural PDE solver for high-dimensional reachability problems, whose computational requirements scale with the complexity of the underlying reachable tube rather than the state space dimension. Unfortunately, neural networks can make errors and thus the computed solution may not be safe, which falls short of achieving our overarching goal to provide fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#24615;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65288;RPMD&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#36882;&#22686;&#27493;&#36827;&#65292;&#31639;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2209.10579</link><description>&lt;p&gt;
&#20445;&#35777;&#40065;&#26834;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#19968;&#38454;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
First-order Policy Optimization for Robust Markov Decision Process. (arXiv:2209.10579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#24615;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65288;RPMD&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#36882;&#22686;&#27493;&#36827;&#65292;&#31639;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35299;&#20915;&#40065;&#26834;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#32452;&#20855;&#26377;&#19981;&#30830;&#23450;&#36716;&#31227;&#26680;&#30340;&#25240;&#25187;&#12289;&#26377;&#38480;&#29366;&#24577;&#12289;&#26377;&#38480;&#21160;&#20316;&#31354;&#38388;&#30340;MDP&#12290;&#35268;&#21010;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#40065;&#26834;&#31574;&#30053;&#26469;&#20248;&#21270;&#23545;&#36716;&#31227;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#20540;&#65292;&#22240;&#27492;&#21253;&#25324;&#26631;&#20934;MDP&#35268;&#21010;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#23545;&#20110;$(\mathbf{s},\mathbf{a})$-&#30697;&#24418;&#19981;&#30830;&#23450;&#38598;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#40065;&#26834;&#30446;&#26631;&#30340;&#20960;&#20010;&#32467;&#26500;&#24615;&#35266;&#23519;&#65292;&#20174;&#32780;&#20415;&#20110;&#24320;&#21457;&#22522;&#20110;&#31574;&#30053;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#21363;&#40065;&#26834;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65288;RPMD&#65289;&#12290;&#20351;&#29992;&#32447;&#24615;&#36882;&#22686;&#30340;&#27493;&#38271;&#65292;&#24314;&#31435;&#20102;&#25214;&#21040;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#30340;$\mathcal{O}(\log(1/\epsilon))$&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#24403;&#19968;&#38454;&#20449;&#24687;&#20165;&#36890;&#36807;&#19982;&#21517;&#20041;&#29615;&#22659;&#30340;&#22312;&#32447;&#20132;&#20114;&#33719;&#24471;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#40065;&#26834;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#30340;&#38543;&#26426;&#21464;&#20307;&#65292;&#21363;SRPMD&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20248;&#31574;&#30053;&#30340;&#21462;&#24471;&#26041;&#24335;&#19982;&#22522;&#20110;&#19968;&#38454;&#20449;&#24687;&#30340;&#26041;&#27861;&#26159;&#31561;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of solving robust Markov decision process (MDP), which involves a set of discounted, finite state, finite action space MDPs with uncertain transition kernels. The goal of planning is to find a robust policy that optimizes the worst-case values against the transition uncertainties, and thus encompasses the standard MDP planning as a special case. For $(\mathbf{s},\mathbf{a})$-rectangular uncertainty sets, we establish several structural observations on the robust objective, which facilitates the development of a policy-based first-order method, namely the robust policy mirror descent (RPMD). An $\mathcal{O}(\log(1/\epsilon))$ iteration complexity for finding an $\epsilon$-optimal policy is established with linearly increasing stepsizes. We further develop a stochastic variant of the robust policy mirror descent method, named SRPMD, when the first-order information is only available through online interactions with the nominal environment. We show that the optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#26500;&#65292;&#21253;&#21547;&#31163;&#25955;&#29942;&#39048;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.11240</link><description>&lt;p&gt;
&#31163;&#25955;&#38190;&#20540;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Discrete Key-Value Bottleneck. (arXiv:2207.11240v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#26500;&#65292;&#21253;&#21547;&#31163;&#25955;&#29942;&#39048;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;i.i.d.&#25968;&#25454;&#27969;&#21644;&#26631;&#27880;&#25968;&#25454;&#20016;&#23500;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#31561;&#38750;&#24179;&#31283;&#35757;&#32451;&#25968;&#25454;&#27969;&#20250;&#20986;&#29616;&#25361;&#25112;&#12290;&#30446;&#21069;&#24050;&#26377;&#30340;&#19968;&#20010;&#26377;&#25928;&#26041;&#27861;&#26159;&#22312;&#22823;&#37327;&#21487;&#29992;&#25968;&#25454;&#19978;&#23545;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26032;&#20219;&#21153;&#65292;&#26356;&#26032;&#36825;&#20123;&#32534;&#30721;&#22120;&#30340;&#26435;&#37325;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#24494;&#35843;&#22823;&#37327;&#30340;&#26435;&#37325;&#65292;&#24182;&#19988;&#20250;&#24536;&#35760;&#20808;&#21069;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24314;&#31435;&#22312;&#21253;&#21547;&#25104;&#23545;&#20998;&#31163;&#21487;&#23398;&#20064;&#38190;&#20540;&#20195;&#30721;&#30340;&#31163;&#25955;&#29942;&#39048;&#30340;&#22522;&#30784;&#19978;&#12290;&#25105;&#20204;&#30340;&#33539;&#24335;&#26159;&#36827;&#34892;&#32534;&#30721;&#12289;&#36890;&#36807;&#31163;&#25955;&#29942;&#39048;&#36827;&#34892;&#34920;&#31034;&#22788;&#29702;&#12289;&#35299;&#30721;&#12290;&#22312;&#36825;&#37324;&#65292;&#36755;&#20837;&#34987;&#39304;&#36865;&#21040;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#29992;&#20110;&#36873;&#25321;&#26368;&#36817;&#30340;&#38190;&#65292;&#24182;&#23558;&#30456;&#24212;&#30340;&#20540;&#39304;&#36865;&#21040;&#27491;&#22312;&#25191;&#34892;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes. Our paradigm will be to encode; process the representation via a discrete bottleneck; and decode. Here, the input is fed to the pre-trained encoder, the output of the encoder is used to select the nearest keys, and the corresponding values are fed to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#24863;&#30693;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;MACL&#65289;&#31574;&#30053;&#65292;&#20854;&#28201;&#24230;&#36866;&#24212;&#20110;&#21453;&#26144;&#23454;&#20363;&#21028;&#21035;&#20219;&#21153;&#22522;&#26412;&#32622;&#20449;&#24230;&#30340;&#23545;&#40784;&#24133;&#24230;&#30340;&#22823;&#23567;&#65292;&#35299;&#20915;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32479;&#19968;&#24615;&#23481;&#24525;&#22256;&#22659;&#65288;UTD&#65289;&#21644;&#26799;&#24230;&#38477;&#20302;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#65292;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.07874</link><description>&lt;p&gt;
&#27169;&#22411;&#24863;&#30693;&#30340;&#23545;&#27604;&#23398;&#20064;: &#36808;&#21521;&#25670;&#33073;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Model-Aware Contrastive Learning: Towards Escaping the Dilemmas. (arXiv:2207.07874v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#24863;&#30693;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;MACL&#65289;&#31574;&#30053;&#65292;&#20854;&#28201;&#24230;&#36866;&#24212;&#20110;&#21453;&#26144;&#23454;&#20363;&#21028;&#21035;&#20219;&#21153;&#22522;&#26412;&#32622;&#20449;&#24230;&#30340;&#23545;&#40784;&#24133;&#24230;&#30340;&#22823;&#23567;&#65292;&#35299;&#20915;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32479;&#19968;&#24615;&#23481;&#24525;&#22256;&#22659;&#65288;UTD&#65289;&#21644;&#26799;&#24230;&#38477;&#20302;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#65292;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#26368;&#24120;&#35265;&#30340;&#22522;&#20110;InfoNCE&#30340;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#22256;&#22659;&#65292;&#22914;&#32479;&#19968;&#24615;&#23481;&#24525;&#22256;&#22659;&#65288;UTD&#65289;&#21644;&#26799;&#24230;&#38477;&#20302;&#65292;&#20004;&#32773;&#37117;&#19982;$\mathcal{P}_{ij}$&#39033;&#26377;&#20851;&#12290;&#24050;&#32463;&#30830;&#35748;UTD&#21487;&#33021;&#23548;&#33268;&#24847;&#22806;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#35748;&#20026;&#28201;&#24230;&#30340;&#22266;&#23450;&#24615;&#26159;UTD&#30340;&#32618;&#39745;&#31096;&#39318;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#27169;&#22411;&#24863;&#30693;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;MACL&#65289;&#31574;&#30053;&#26469;&#20016;&#23500;CL&#25439;&#22833;&#23478;&#26063;&#65292;&#20854;&#28201;&#24230;&#36866;&#24212;&#20110;&#21453;&#26144;&#23454;&#20363;&#21028;&#21035;&#20219;&#21153;&#22522;&#26412;&#32622;&#20449;&#24230;&#30340;&#23545;&#40784;&#24133;&#24230;&#30340;&#22823;&#23567;&#65292;&#28982;&#21518;&#20351;CL&#25439;&#22833;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23545;&#30828;&#36127;&#38754;&#30340;&#24809;&#32602;&#21147;&#24230;&#12290;&#20851;&#20110;&#21478;&#19968;&#20010;&#22256;&#22659;-&#26799;&#24230;&#38477;&#20302;&#38382;&#39064;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#26799;&#24230;&#32553;&#25918;&#22240;&#23376;&#30340;&#26497;&#38480;&#65292;&#21487;&#20197;&#20174;&#32479;&#19968;&#30340;&#35282;&#24230;&#35299;&#37322;&#20026;&#20160;&#20040;&#19968;&#20123;&#26368;&#36817;&#30340;&#26041;&#27861;&#23454;&#38469;&#19978;&#26159;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;DDoS&#25915;&#20987;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MACL&#21487;&#20197;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#65292;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) continuously achieves significant breakthroughs across multiple domains. However, the most common InfoNCE-based methods suffer from some dilemmas, such as \textit{uniformity-tolerance dilemma} (UTD) and \textit{gradient reduction}, both of which are related to a $\mathcal{P}_{ij}$ term. It has been identified that UTD can lead to unexpected performance degradation. We argue that the fixity of temperature is to blame for UTD. To tackle this challenge, we enrich the CL loss family by presenting a Model-Aware Contrastive Learning (MACL) strategy, whose temperature is adaptive to the magnitude of alignment that reflects the basic confidence of the instance discrimination task, then enables CL loss to adjust the penalty strength for hard negatives adaptively. Regarding another dilemma, the gradient reduction issue, we derive the limits of an involved gradient scaling factor, which allows us to explain from a unified perspective why some recent approaches are effect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21160;&#24577;&#20998;&#35299;&#26041;&#27861;&#20998;&#26512;&#20102; SGD &#22312;&#22266;&#23450;&#28857;&#38468;&#36817;&#30340;&#23646;&#24615;&#65292;&#24674;&#22797;&#20102;&#30495;&#27491;&#30340;&#8220;&#33021;&#37327;&#8221;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#26435;&#37325;&#26041;&#24046;&#21644;&#25439;&#22833;&#20989;&#25968;&#24179;&#22374;&#24230;&#21453;&#24120;&#20851;&#31995;&#30340;&#24726;&#35770;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#23398;&#31185;&#25552;&#20379;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.04932</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26041;&#24046; - &#24179;&#22374;&#20851;&#31995;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent and Anomaly of Variance-flatness Relation in Artificial Neural Networks. (arXiv:2207.04932v2 [nlin.AO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21160;&#24577;&#20998;&#35299;&#26041;&#27861;&#20998;&#26512;&#20102; SGD &#22312;&#22266;&#23450;&#28857;&#38468;&#36817;&#30340;&#23646;&#24615;&#65292;&#24674;&#22797;&#20102;&#30495;&#27491;&#30340;&#8220;&#33021;&#37327;&#8221;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#26435;&#37325;&#26041;&#24046;&#21644;&#25439;&#22833;&#20989;&#25968;&#24179;&#22374;&#24230;&#21453;&#24120;&#20851;&#31995;&#30340;&#24726;&#35770;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#23398;&#31185;&#25552;&#20379;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#20854;&#25104;&#21151;&#30340;&#29702;&#35770;&#21407;&#21017;&#19968;&#30452;&#21463;&#21040;&#25345;&#32493;&#30340;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#25253;&#21578;&#20102;&#31070;&#32463;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22312; SGD &#19979;&#39537;&#21160;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22374;&#24230;&#20043;&#38388;&#30340;&#21453;&#24120;&#65288;&#21453;&#65289;&#20851;&#31995;[Feng&#65286;Tu&#65292;PNAS 118&#65292;0027&#65288;2021&#65289;]&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#31181;&#20284;&#20046;&#36829;&#21453;&#32479;&#35745;&#29289;&#29702;&#23398;&#21407;&#21017;&#30340;&#29616;&#35937;&#65292;&#36890;&#36807;&#21160;&#24577;&#20998;&#35299;&#26041;&#27861;&#20998;&#26512;&#20102; SGD &#22312;&#22266;&#23450;&#28857;&#38468;&#36817;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24674;&#22797;&#20102;&#30495;&#27491;&#30340;&#8220;&#33021;&#37327;&#8221;&#20989;&#25968;&#65292;&#19979;&#38754;&#26159;&#26222;&#36941;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#12290;&#23427;&#19982;&#19968;&#33324;&#30340;&#25104;&#26412;&#20989;&#25968;&#19981;&#21516;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#20010;&#21453;&#24120;&#25152;&#24341;&#21457;&#30340;&#24726;&#35770;&#12290;&#26412;&#30740;&#31350;&#26159;&#32463;&#20856;&#32479;&#35745;&#21147;&#23398;&#21644;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#31185;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#26377;&#28508;&#21147;&#20026;&#21518;&#32773;&#25552;&#20379;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD), a widely used algorithm in deep-learning neural networks has attracted continuing studies for the theoretical principles behind its success. A recent work reports an anomaly (inverse) relation between the variance of neural weights and the landscape flatness of the loss function driven under SGD [Feng &amp; Tu, PNAS 118, 0027 (2021)]. To investigate this seemingly violation of statistical physics principle, the properties of SGD near fixed points are analysed via a dynamic decomposition method. Our approach recovers the true "energy" function under which the universal Boltzmann distribution holds. It differs from the cost function in general and resolves the paradox raised by the the anomaly. The study bridges the gap between the classical statistical mechanics and the emerging discipline of artificial intelligence, with potential for better algorithms to the latter.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AMC &#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#35206;&#30422;&#26377;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#21363;&#32534;&#30721;&#21306;&#22495;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35270;&#35273; grounding &#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#65292;&#26377;&#26395;&#25104;&#20026;&#35270;&#35273; grounding &#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.15462</link><description>&lt;p&gt;
&#36890;&#36807;&#40723;&#21169;&#19968;&#33268;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26469;&#25913;&#36827;&#35270;&#35273; grounding
&lt;/p&gt;
&lt;p&gt;
Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AMC &#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#35206;&#30422;&#26377;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#21363;&#32534;&#30721;&#21306;&#22495;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35270;&#35273; grounding &#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#65292;&#26377;&#26395;&#25104;&#20026;&#35270;&#35273; grounding &#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#30340;&#25439;&#22833;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#19982;&#21306;&#22495;&#32423;&#27880;&#37322;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30446;&#26631;&#31216;&#20026; Attention Mask Consistency&#65288;AMC&#65289;&#65292;&#24182;&#35777;&#26126;&#23427;&#20135;&#29983;&#20102;&#27604;&#20381;&#36182;&#20110;&#21306;&#22495;&#32423;&#27880;&#37322;&#30340;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#35270;&#35273; grounding &#24615;&#33021;&#12290; AMC &#36890;&#36807;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#25513;&#30721;&#65292;&#22312;&#21253;&#21547;&#27492;&#31867;&#27880;&#37322;&#30340;&#22270;&#20687;&#20013;&#65292;&#25226;&#23427;&#20204;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#20027;&#35201;&#38598;&#20013;&#22312;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#20869;&#12290;&#29305;&#21035;&#22320;&#65292;&#19968;&#20010;&#22312;&#26631;&#20934;&#35270;&#35273;-&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#20043;&#19978;&#29992; AMC &#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312; Flickr30k &#35270;&#35273; grounding &#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;86.59%&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#30456;&#27604;&#26368;&#20339;&#32467;&#26524;&#33719;&#24471;&#20102;5.48%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20195;&#34920;&#36798;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a margin-based loss for vision-language model pretraining that encourages gradient-based explanations that are consistent with region-level annotations. We refer to this objective as Attention Mask Consistency (AMC) and demonstrate that it produces superior visual grounding performance compared to models that rely instead on region-level annotations for explicitly training an object detector such as Faster R-CNN. AMC works by encouraging gradient-based explanation masks that focus their attention scores mostly within annotated regions of interest for images that contain such annotations. Particularly, a model trained with AMC on top of standard vision-language modeling objectives obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding benchmark, an absolute improvement of 5.48% when compared to the best previous model. Our approach also performs exceedingly well on established benchmarks for referring expression comprehension and offers the added bene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;BONET&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#26679;&#26412;&#31574;&#30053;&#21512;&#25104;&#36712;&#36857;&#20197;&#24110;&#21161;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2206.10786</link><description>&lt;p&gt;
&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pretraining for Black-Box Optimization. (arXiv:2206.10786v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;BONET&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#26679;&#26412;&#31574;&#30053;&#21512;&#25104;&#36712;&#36857;&#20197;&#24110;&#21161;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#28041;&#21450;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#40657;&#30418;&#20248;&#21270; (BBO) &#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#24120;&#20551;&#35774;&#22312;&#32447;&#20989;&#25968;&#35780;&#20272;&#30340;&#39044;&#31639;&#24456;&#23567;&#65292;&#20294;&#24448;&#24448;&#21487;&#20197;&#35775;&#38382;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22266;&#23450;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#36924;&#36817;&#20989;&#25968;&#25110;&#20854;&#21453;&#20989;&#25968;&#65292;&#20294;&#22312;&#31163;&#25968;&#25454;&#20998;&#24067;&#36739;&#36828;&#26102;&#19981;&#22815;&#31934;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BONET&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;&#12290;&#22312;BONET&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#23450;&#38271;&#36712;&#36857;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#20351;&#29992;&#20174;&#20302;&#20445;&#30495;&#24230;&#26679;&#26412;&#21040;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#30340;&#21333;&#35843;&#36716;&#25442;&#30340;&#31616;&#21333;&#21551;&#21457;&#24335;&#26469;&#21512;&#25104;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#30340;&#36712;&#36857;&#12290;&#22312;Design-Bench&#19978;&#20351;&#29992;&#34987;&#22240;&#26524;&#25513;&#34109;&#30340;Transformer&#23454;&#20363;&#21270;BONET&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#24179;&#22343;&#25490;&#21517;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose BONET, a generative framework for pretraining a novel black-box optimizer using offline datasets. In BONET, we train an autoregressive model on fixed-length trajectories derived from an offline dataset. We design a sampling strategy to synthesize trajectories from offline data using a simple heuristic of rolling out monotonic transitions from low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using a causally masked Transformer and evaluate it on Design-Bench, where we rank the best on averag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#21644;&#37325;&#29699;&#21160;&#37327;&#36827;&#34892;&#21152;&#36895;&#65292;&#22312;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#20013;&#20445;&#25345;&#24555;&#36895;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.07553</link><description>&lt;p&gt;
&#35770;&#23567;&#25209;&#37327;&#37325;&#29699;&#21160;&#37327;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the fast convergence of minibatch heavy ball momentum. (arXiv:2206.07553v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#21644;&#37325;&#29699;&#21160;&#37327;&#36827;&#34892;&#21152;&#36895;&#65292;&#22312;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#20013;&#20445;&#25345;&#24555;&#36895;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#30340;&#38543;&#26426;&#21160;&#37327;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#20013;&#65292;&#20294;&#30001;&#20110;&#36824;&#27809;&#26377;&#21152;&#36895;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#19982;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#33391;&#22909;&#24615;&#33021;&#24182;&#19981;&#30456;&#31526;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23637;&#31034;&#65292;&#38543;&#26426;&#37325;&#29699;&#21160;&#37327;&#22312;&#20108;&#27425;&#26368;&#20248;&#21270;&#38382;&#39064;&#20013;&#20445;&#25345;&#65288;&#30830;&#23450;&#24615;&#65289;&#37325;&#29699;&#21160;&#37327;&#30340;&#24555;&#36895;&#32447;&#24615;&#29575;&#65292;&#33267;&#23569;&#22312;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#36827;&#34892;&#23567;&#25209;&#37327;&#22788;&#29702;&#26102;&#12290;&#25105;&#20204;&#25152;&#30740;&#31350;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24102;&#23567;&#25209;&#37327;&#22788;&#29702;&#21644;&#37325;&#29699;&#21160;&#37327;&#30340;&#21152;&#36895;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#12290;&#35813;&#20998;&#26512;&#20381;&#36182;&#20110;&#20180;&#32454;&#20998;&#35299;&#21160;&#37327;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#29420;&#31435;&#38543;&#26426;&#30697;&#38453;&#20056;&#31215;&#30340;&#35889;&#33539;&#22260;&#38598;&#20013;&#30028;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#28436;&#31034;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#30456;&#24403;&#23574;&#38160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simple stochastic momentum methods are widely used in machine learning optimization, but their good practical performance is at odds with an absence of theoretical guarantees of acceleration in the literature. In this work, we aim to close the gap between theory and practice by showing that stochastic heavy ball momentum retains the fast linear rate of (deterministic) heavy ball momentum on quadratic optimization problems, at least when minibatching with a sufficiently large batch size. The algorithm we study can be interpreted as an accelerated randomized Kaczmarz algorithm with minibatching and heavy ball momentum. The analysis relies on carefully decomposing the momentum transition matrix, and using new spectral norm concentration bounds for products of independent random matrices. We provide numerical illustrations demonstrating that our bounds are reasonably sharp.
&lt;/p&gt;</description></item><item><title>D-Struct&#26159;&#19968;&#31181;&#21487;&#24494;&#21644;&#21487;&#20256;&#36755;&#30340;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#32467;&#26500;&#21487;&#20197;&#22312;&#21516;&#19968;&#39046;&#22495;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#20256;&#36755;&#65292;&#27604;NOTEARS&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.06354</link><description>&lt;p&gt;
&#21487;&#24494;&#21644;&#21487;&#20256;&#36755;&#30340;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentiable and Transportable Structure Learning. (arXiv:2206.06354v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06354
&lt;/p&gt;
&lt;p&gt;
D-Struct&#26159;&#19968;&#31181;&#21487;&#24494;&#21644;&#21487;&#20256;&#36755;&#30340;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#32467;&#26500;&#21487;&#20197;&#22312;&#21516;&#19968;&#39046;&#22495;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#20256;&#36755;&#65292;&#27604;NOTEARS&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;&#22312;&#23427;&#20204;&#30340;&#32467;&#26500;&#20013;&#32534;&#30721;&#20102;&#20851;&#20110;&#29305;&#23450;&#20998;&#24067;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#36825;&#20123;&#32467;&#26500;&#25152;&#38656;&#30340;&#35745;&#31639;&#36890;&#24120;&#26159;&#21464;&#37327;&#25968;&#30340;&#36229;&#25351;&#25968;&#65292;&#22240;&#20026;&#25512;&#26029;&#38656;&#35201;&#25195;&#25551;&#19968;&#20010;&#32452;&#21512;&#25968;&#37327;&#24040;&#22823;&#30340;&#28508;&#22312;&#32467;&#26500;&#31354;&#38388;&#12290;&#30452;&#21040;&#26368;&#36817;&#30340;&#36827;&#23637;&#25165;&#20351;&#24471;&#20351;&#29992;&#21487;&#24494;&#24230;&#37327;&#25628;&#32034;&#36825;&#20010;&#31354;&#38388;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#25628;&#32034;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;D-Struct&#65292;&#23427;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#24674;&#22797;&#20102;&#21457;&#29616;&#32467;&#26500;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#20256;&#36755;&#24615;&#65292;&#21516;&#26102;&#20173;&#28982;&#23436;&#20840;&#21487;&#24494;&#12290;&#22240;&#20026;D-Struct&#20173;&#28982;&#26159;&#21487;&#24494;&#30340;&#65292;&#25152;&#20197;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#21487;&#24494;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directed acyclic graphs (DAGs) encode a lot of information about a particular distribution in their structure. However, compute required to infer these structures is typically super-exponential in the number of variables, as inference requires a sweep of a combinatorially large space of potential structures. That is, until recent advances made it possible to search this space using a differentiable metric, drastically reducing search time. While this technique -- named NOTEARS -- is widely considered a seminal work in DAG-discovery, it concedes an important property in favour of differentiability: transportability. To be transportable, the structures discovered on one dataset must apply to another dataset from the same domain. We introduce D-Struct which recovers transportability in the discovered structures through a novel architecture and loss function while remaining fully differentiable. Because D-Struct remains differentiable, our method can be easily adopted in existing different
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#21307;&#30103;&#26426;&#26500;&#38388;&#25968;&#25454;&#20849;&#20139;&#30340;&#38544;&#31169;&#38480;&#21046;&#21644;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#24615;&#12290;&#35813;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#35777;&#26126;&#20197;&#21450;&#22312;&#29616;&#23454;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.05581</link><description>&lt;p&gt;
&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Offline Reinforcement Learning. (arXiv:2206.05581v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#21307;&#30103;&#26426;&#26500;&#38388;&#25968;&#25454;&#20849;&#20139;&#30340;&#38544;&#31169;&#38480;&#21046;&#21644;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#24615;&#12290;&#35813;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#35777;&#26126;&#20197;&#21450;&#22312;&#29616;&#23454;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35777;&#25454;&#25110;&#25968;&#25454;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#23545;&#20110;&#20010;&#24615;&#21270;&#21307;&#30103;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#21463;&#30410;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#34429;&#28982;&#21307;&#30103;&#26426;&#26500;&#38388;&#26377;&#22823;&#37327;&#20581;&#24247;&#25968;&#25454;&#21487;&#29992;&#65292;&#20294;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#23427;&#20204;&#26080;&#27861;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#31449;&#28857;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#31163;&#32447;RL&#31639;&#27861;&#26159;&#24517;&#35201;&#30340;&#19988;&#26377;&#21069;&#36884;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31449;&#28857;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#65292;&#20801;&#35768;&#31449;&#28857;&#20043;&#38388;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#25928;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#20998;&#26512;&#31449;&#28857;&#32423;&#29305;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31163;&#32447;RL&#32852;&#37030;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#24615;&#65292;&#20165;&#38656;&#35201;&#36890;&#36807;&#20132;&#25442;&#25688;&#35201;&#32479;&#35745;&#37327;&#36827;&#34892;&#19968;&#36718;&#36890;&#20449;&#20132;&#20114;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#26080;&#38656;&#20551;&#35774;&#31449;&#28857;&#20043;&#38388;&#20855;&#26377;&#30456;&#21516;&#30340;&#36716;&#25442;&#21160;&#24577;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence-based or data-driven dynamic treatment regimes are essential for personalized medicine, which can benefit from offline reinforcement learning (RL). Although massive healthcare data are available across medical institutions, they are prohibited from sharing due to privacy constraints. Besides, heterogeneity exists in different sites. As a result, federated offline RL algorithms are necessary and promising to deal with the problems. In this paper, we propose a multi-site Markov decision process model which allows both homogeneous and heterogeneous effects across sites. The proposed model makes the analysis of the site-level features possible. We design the first federated policy optimization algorithm for offline RL with sample complexity. The proposed algorithm is communication-efficient and privacy-preserving, which requires only a single round of communication interaction by exchanging summary statistics. We give a theoretical guarantee for the proposed algorithm without the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Beyond the Imitation Game&#22522;&#20934;&#27979;&#35797;&#65288;BIG-bench&#65289;&#65292;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;204&#20010;&#21508;&#39046;&#22495;&#30340;&#38590;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#21644;&#20934;&#22791;&#12290;</title><link>http://arxiv.org/abs/2206.04615</link><description>&lt;p&gt;
&#36229;&#36234;&#27169;&#20223;&#28216;&#25103;&#65306;&#37327;&#21270;&#21644;&#25299;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Beyond the Imitation Game&#22522;&#20934;&#27979;&#35797;&#65288;BIG-bench&#65289;&#65292;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;204&#20010;&#21508;&#39046;&#22495;&#30340;&#38590;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#21644;&#20934;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#25968;&#37327;&#19978;&#30340;&#25552;&#21319;&#21644;&#26032;&#30340;&#23450;&#24615;&#33021;&#21147;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#22312;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#20294;&#36825;&#20123;&#26032;&#30340;&#33021;&#21147;&#30446;&#21069;&#23578;&#26410;&#34987;&#20805;&#20998;&#25551;&#36848;&#12290;&#20026;&#20102;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#65292;&#20026;&#21095;&#21464;&#30340;&#26032;&#22411;&#27169;&#22411;&#33021;&#21147;&#20570;&#20934;&#22791;&#65292;&#24182;&#32531;&#35299;&#31038;&#20250;&#26377;&#23475;&#24433;&#21709;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#26377;&#21644;&#36817;&#26399;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Beyond the Imitation Game&#22522;&#20934;&#27979;&#35797;&#65288;BIG-bench&#65289;&#12290;BIG-bench&#30446;&#21069;&#21253;&#25324;204&#20010;&#20219;&#21153;&#65292;&#30001;132&#20010;&#26426;&#26500;&#30340;450&#21517;&#20316;&#32773;&#36129;&#29486;&#12290;&#20219;&#21153;&#20027;&#39064;&#22810;&#26679;&#65292;&#28085;&#30422;&#20102;&#35821;&#35328;&#23398;&#12289;&#20799;&#31461;&#21457;&#23637;&#12289;&#25968;&#23398;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#29983;&#29289;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#31038;&#20250;&#20559;&#35265;&#12289;&#36719;&#20214;&#24320;&#21457;&#31561;&#31561;&#12290;BIG-bench&#19987;&#27880;&#20110;&#37027;&#20123;&#34987;&#35748;&#20026;&#36229;&#20986;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;OpenAI&#30340;GPT&#27169;&#22411;&#21644;&#35895;&#27468;&#20869;&#37096;&#30340;&#23494;&#38598;&#36716;&#25442;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#29305;&#23450;&#20915;&#31574;&#38382;&#39064;&#30340;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26681;&#25454;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#65292;&#22312;&#36951;&#25022;&#26368;&#23567;&#30340;&#21069;&#25552;&#19979;&#26368;&#22823;&#31243;&#24230;&#22320;&#25910;&#38598;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.02326</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#30340;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Instance-Optimal Algorithms for Interactive Decision Making. (arXiv:2206.02326v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#29305;&#23450;&#20915;&#31574;&#38382;&#39064;&#30340;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26681;&#25454;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#65292;&#22312;&#36951;&#25022;&#26368;&#23567;&#30340;&#21069;&#25552;&#19979;&#26368;&#22823;&#31243;&#24230;&#22320;&#25910;&#38598;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#65288;&#36172;&#21338;&#26426;&#38382;&#39064;&#12289;&#24378;&#21270;&#23398;&#20064;&#31561;&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#32858;&#28966;&#20110;&#24230;&#37327;&#31639;&#27861;&#22312;&#26368;&#38590;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#26368;&#23567;&#21270;&#36951;&#25022;&#12290;&#28982;&#32780;&#65292;&#29702;&#24819;&#30340;&#31639;&#27861;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#38382;&#39064;&#23454;&#20363;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#23545;&#26131;&#20110;&#22788;&#29702;&#30340;&#23454;&#20363;&#20135;&#29983;&#27604;&#26368;&#22351;&#24773;&#20917;&#19979;&#26356;&#23567;&#30340;&#36951;&#25022;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#36866;&#29992;&#20110;&#26377;&#38480;&#20915;&#31574;&#20010;&#25968;&#30340;&#19968;&#33324;&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#30340;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;&#12290;&#22312;&#27599;&#20010;&#23454;&#20363; $f$ &#19978;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#25152;&#26377;&#19968;&#33268;&#31639;&#27861;&#65288;&#37027;&#20123;&#22312;&#25152;&#26377;&#23454;&#20363;&#19978;&#23454;&#29616;&#38750;&#24179;&#20961;&#36951;&#25022;&#30340;&#31639;&#27861;&#65289;&#65292;&#24182;&#19988;&#20855;&#26377; $\mathcal{C}(f) \ln n$ &#30340;&#28176;&#36817;&#36951;&#25022;&#65292;&#20854;&#20013; $\mathcal{C}(f)$ &#26159; $f$ &#30340;&#22797;&#26434;&#24230;&#30340;&#31934;&#30830;&#34920;&#24449;&#12290;&#31639;&#27861;&#30340;&#20851;&#38190;&#27493;&#39588;&#28041;&#21450;&#24102;&#26377;&#27963;&#21160;&#25968;&#25454;&#25910;&#38598;&#30340;&#20551;&#35774;&#26816;&#39564;&#12290;&#23427;&#35745;&#31639;&#20986;&#26368;&#32463;&#27982;&#30340;&#20915;&#31574;&#65292;&#36890;&#36807;&#36825;&#20123;&#20915;&#31574;&#31639;&#27861;&#25910;&#38598;&#35266;&#23519;&#32467;&#26524;&#20197;&#26816;&#39564;&#20272;&#35745;&#20540;&#26159;&#21542;&#33853;&#22312;&#39044;&#26399;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research on interactive decision making problems (bandits, reinforcement learning, etc.) mostly focuses on the minimax regret that measures the algorithm's performance on the hardest instance. However, an ideal algorithm should adapt to the complexity of a particular problem instance and incur smaller regrets on easy instances than worst-case instances. In this paper, we design the first asymptotic instance-optimal algorithm for general interactive decision making problems with finite number of decisions under mild conditions. On every instance $f$, our algorithm outperforms all consistent algorithms (those achieving non-trivial regrets on all instances), and has asymptotic regret $\mathcal{C}(f) \ln n$, where $\mathcal{C}(f)$ is an exact characterization of the complexity of $f$. The key step of the algorithm involves hypothesis testing with active data collection. It computes the most economical decisions with which the algorithm collects observations to test whether an estimate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#32479;&#19968;&#22312;&#20849;&#20139;&#30340;&#26550;&#26500;&#21644;&#30446;&#26631;&#19979;&#36827;&#34892;&#65292;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20197;&#23454;&#29616;&#20004;&#20010;&#35270;&#22270;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2206.00621</link><description>&lt;p&gt;
&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#65306;&#36808;&#21521;&#32479;&#19968;&#30340;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#32479;&#19968;&#22312;&#20849;&#20139;&#30340;&#26550;&#26500;&#21644;&#30446;&#26631;&#19979;&#36827;&#34892;&#65292;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20197;&#23454;&#29616;&#20004;&#20010;&#35270;&#22270;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#19982;&#20849;&#20139;&#26550;&#26500;&#21644;&#30446;&#26631;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#21363;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#20855;&#26377;&#23558;&#21516;&#19968;&#23545;&#35937;&#30340;&#20004;&#20010;&#19981;&#21516;&#35270;&#22270;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#21516;&#35821;&#20041;&#31354;&#38388;&#30340;&#30456;&#21516;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#21363;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65289;&#21644;&#22810;&#35821;&#35328;&#25968;&#25454;&#65288;&#21363;&#24179;&#34892;&#21477;&#23545;&#65289;&#35270;&#20026;&#21516;&#19968;&#23545;&#35937;&#30340;&#20004;&#20010;&#19981;&#21516;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23545;&#40784;&#20004;&#20010;&#35270;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#23545;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;CCLM&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#22522;&#20934;IGLUE&#21644;&#20004;&#20010;&#22810;&#35821;&#35328;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;RealAEs&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;Android&#39046;&#22495;&#32422;&#26463;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.15128</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#39046;&#22495;&#32422;&#26463;&#20197;&#22686;&#24378;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65306;&#36890;&#36807;RealAEs&#21319;&#32423;
&lt;/p&gt;
&lt;p&gt;
Level Up with RealAEs: Leveraging Domain Constraints in Feature Space to Strengthen Robustness of Android Malware Detection. (arXiv:2205.15128v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;RealAEs&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;Android&#39046;&#22495;&#32422;&#26463;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#38754;&#20020;&#30340;&#23545;&#25239;&#31034;&#20363;&#23481;&#26131;&#21463;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#39046;&#22495;&#32422;&#26463;&#19979;&#30340;&#21487;&#34892;&#23545;&#25239;&#26679;&#26412;(RealAEs)&#12290; &#22312;&#29616;&#23454;&#25915;&#20987;&#19979;&#65292;RealAEs&#27604;&#19981;&#21487;&#34892;&#30340;&#23545;&#25239;&#26679;&#26412;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;Android&#39046;&#22495;&#32422;&#26463;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#23558;&#39046;&#22495;&#32422;&#26463;&#35299;&#37322;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#38477;&#20302;&#26816;&#27979;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability to adversarial examples remains one major obstacle for Machine Learning (ML)-based Android malware detection. Realistic attacks in the Android malware domain create Realizable Adversarial Examples (RealAEs), i.e., AEs that satisfy the domain constraints of Android malware. Recent studies have shown that using such RealAEs in Adversarial Training (AT) is more effective in defending against realistic attacks than using unrealizable AEs (unRealAEs). This is because RealAEs allow defenders to explore certain pockets in the feature space that are vulnerable to realistic attacks. However, existing defenses commonly generate RealAEs in the problem space, which is known to be time-consuming and impractical for AT. In this paper, we propose to generate RealAEs in the feature space, leading to a simpler and more efficient solution. Our approach is driven by a novel interpretation of Android domain constraints in the feature space. More concretely, our defense first learns featu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32454;&#33268;&#30740;&#31350;&#20102;&#28857;&#31215;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#38024;&#23545; $m\propto d^r$ &#39640;&#38454;&#26631;&#24230;&#20851;&#31995;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#27979;&#35797;&#35823;&#24046;&#12289;&#20559;&#24046;&#21644;&#26041;&#24046;&#20844;&#24335;&#12290;</title><link>http://arxiv.org/abs/2205.14846</link><description>&lt;p&gt;
&#28857;&#31215;&#26680;&#22238;&#24402;&#30340;&#31934;&#30830;&#23398;&#20064;&#26354;&#32447;&#21644;&#39640;&#38454;&#26631;&#24230;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression. (arXiv:2205.14846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32454;&#33268;&#30740;&#31350;&#20102;&#28857;&#31215;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#38024;&#23545; $m\propto d^r$ &#39640;&#38454;&#26631;&#24230;&#20851;&#31995;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#27979;&#35797;&#35823;&#24046;&#12289;&#20559;&#24046;&#21644;&#26041;&#24046;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#26029;&#25512;&#36827;&#35745;&#31639;&#21069;&#27839;&#65292;&#24320;&#21457;&#23545;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#32553;&#25918;&#26041;&#26696;&#19979;&#39044;&#26399;&#24615;&#33021;&#25552;&#39640;&#30340;&#31934;&#30830;&#20272;&#35745;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#20851;&#20110;&#25551;&#36848;&#39044;&#27979;&#35823;&#24046;&#22914;&#20309;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#32780;&#21464;&#21270;&#30340;&#23398;&#20064;&#26354;&#32447;&#30340;&#29702;&#35770;&#29702;&#35299;&#21463;&#38480;&#20110;&#22823;&#26679;&#26412;&#28176;&#36817;&#24615; ($m\to\infty$) &#25110;&#23545;&#20110;&#26576;&#20123;&#31616;&#21333;&#25968;&#25454;&#20998;&#24067;&#30340;&#39640;&#32500;&#28176;&#36817;&#24615;&#65292;&#20854;&#20013;&#26679;&#26412;&#25968;&#37327;&#19982;&#32500;&#25968;&#25104;&#32447;&#24615;&#27604;&#20363; ($m\propto d$)&#12290;&#36825;&#20004;&#20010;&#33539;&#30068;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#21253;&#25324;&#25152;&#26377;&#39640;&#38454;&#26631;&#24230;&#20851;&#31995; $m\propto d^r$&#65292;&#36825;&#26159;&#26412;&#25991;&#30340;&#30740;&#31350;&#23545;&#35937;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#28857;&#31215;&#26680;&#23725;&#22238;&#24402;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312; $m/d\rightarrow2r$ &#30340; $r$ &#38454;&#28176;&#36817;&#26631;&#24230;&#19979;&#65288;&#20854;&#20013; $m\to\infty$&#65289;&#65292;&#23545;&#20110;&#20174;&#29699;&#38754;&#19978;&#22343;&#21248;&#25277;&#21462;&#30340;&#25968;&#25454;&#65292;&#27979;&#35797;&#35823;&#24046;&#12289;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#31934;&#30830;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
As modern machine learning models continue to advance the computational frontier, it has become increasingly important to develop precise estimates for expected performance improvements under different model and data scaling regimes. Currently, theoretical understanding of the learning curves that characterize how the prediction error depends on the number of samples is restricted to either large-sample asymptotics ($m\to\infty$) or, for certain simple data distributions, to the high-dimensional asymptotics in which the number of samples scales linearly with the dimension ($m\propto d$). There is a wide gulf between these two regimes, including all higher-order scaling relations $m\propto d^r$, which are the subject of the present paper. We focus on the problem of kernel ridge regression for dot-product kernels and present precise formulas for the test error, bias, and variance, for data drawn uniformly from the sphere in the $r$th-order asymptotic scaling regime $m\to\infty$ with $m/d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;QUIC-FL&#26041;&#27861;&#65292;&#21363;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#24555;&#36895;&#26080;&#20559;&#21387;&#32553;&#65292;&#36890;&#36807;&#25913;&#36827;DME&#25216;&#26415;&#23454;&#29616;&#20102;&#26368;&#20248;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.13341</link><description>&lt;p&gt;
QUIC-FL&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#24555;&#36895;&#26080;&#20559;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
QUIC-FL: Quick Unbiased Compression for Federated Learning. (arXiv:2205.13341v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;QUIC-FL&#26041;&#27861;&#65292;&#21363;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#24555;&#36895;&#26080;&#20559;&#21387;&#32553;&#65292;&#36890;&#36807;&#25913;&#36827;DME&#25216;&#26415;&#23454;&#29616;&#20102;&#26368;&#20248;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#22343;&#20540;&#20272;&#35745;&#65288;DME&#65289;&#26159;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#65292;&#22312;&#20854;&#20013;$n$&#20010;&#23458;&#25143;&#31471;&#21521;&#21442;&#25968;&#26381;&#21153;&#22120;&#36890;&#20449;&#21521;&#37327;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#20272;&#31639;&#20854;&#24179;&#22343;&#20540;&#12290;&#26412;&#25991;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;DME&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;$O(1/n)$&#30340;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#65288;NMSE&#65289;&#20445;&#35777;&#65292;&#36890;&#36807;&#28176;&#36827;&#25913;&#36827;&#32534;&#30721;&#25110;&#35299;&#30721;&#65288;&#25110;&#20004;&#32773;&#65289;&#30340;&#22797;&#26434;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#24418;&#24335;&#21270;&#20102;&#38382;&#39064;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#29616;&#25104;&#30340;&#25968;&#23398;&#27714;&#35299;&#22120;&#26469;&#35774;&#35745;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Mean Estimation (DME), in which $n$ clients communicate vectors to a parameter server that estimates their average, is a fundamental building block in communication-efficient federated learning. In this paper, we improve on previous DME techniques that achieve the optimal $O(1/n)$ Normalized Mean Squared Error (NMSE) guarantee by asymptotically improving the complexity for either encoding or decoding (or both). To achieve this, we formalize the problem in a novel way that allows us to use off-the-shelf mathematical solvers to design the quantization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19979;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20855;&#20307;&#24037;&#20855;-L2O&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#26469;&#23454;&#29616;&#27599;&#20010;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#35777;&#20070;&#26469;&#39564;&#35777;&#27169;&#22411;&#25512;&#26029;&#26159;&#21542;&#21487;&#20449;&#12290;</title><link>http://arxiv.org/abs/2204.14174</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#23454;&#29616;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI via Learning to Optimize. (arXiv:2204.14174v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.14174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19979;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20855;&#20307;&#24037;&#20855;-L2O&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#26469;&#23454;&#29616;&#27599;&#20010;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#35777;&#20070;&#26469;&#39564;&#35777;&#27169;&#22411;&#25512;&#26029;&#26159;&#21542;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19981;&#26131;&#29702;&#35299;&#30340;&#40657;&#30418;&#27169;&#22411;&#24456;&#24120;&#35265;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;XAI&#30340;&#26680;&#24515;&#22312;&#20110;&#24314;&#31435;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;XAI&#30340;&#20855;&#20307;&#24037;&#20855;&#65292;&#21487;&#22312;&#38656;&#35201;&#32534;&#30721;&#20808;&#39564;&#30693;&#35782;&#19988;&#38656;&#35201;&#26631;&#35760;&#19981;&#21487;&#20449;&#25512;&#26029;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#23398;&#20064;&#20248;&#21270;&#8221;&#65288;L2O&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#25512;&#26029;&#37117;&#35299;&#20915;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;L2O&#27169;&#22411;&#26131;&#20110;&#23454;&#29616;&#65292;&#30452;&#25509;&#32534;&#30721;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#20135;&#29983;&#29702;&#35770;&#20445;&#35777;&#65288;&#20363;&#22914;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#35777;&#20070;&#65292;&#20197;&#39564;&#35777;&#27169;&#22411;&#25512;&#26029;&#26159;&#21542;&#21487;&#20449;&#12290;&#25991;&#31456;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#23383;&#20856;&#30340;&#20449;&#21495;&#24674;&#22797;&#12289;CT&#25104;&#20687;&#21644;&#21152;&#23494;&#36164;&#20135;&#22871;&#21033;&#20132;&#26131;&#31561;&#24212;&#29992;&#30340;&#25968;&#20540;&#20363;&#23376;&#12290;&#20195;&#30721;&#21644;&#26356;&#22810;&#25991;&#26723;&#21487;&#22312;https://xai-l2o.research.typal.academy&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indecipherable black boxes are common in machine learning (ML), but applications increasingly require explainable artificial intelligence (XAI). The core of XAI is to establish transparent and interpretable data-driven algorithms. This work provides concrete tools for XAI in situations where prior knowledge must be encoded and untrustworthy inferences flagged. We use the "learn to optimize" (L2O) methodology wherein each inference solves a data-driven optimization problem. Our L2O models are straightforward to implement, directly encode prior knowledge, and yield theoretical guarantees (e.g. satisfaction of constraints). We also propose use of interpretable certificates to verify whether model inferences are trustworthy. Numerical examples are provided in the applications of dictionary-based signal recovery, CT imaging, and arbitrage trading of cryptoassets. Code and additional documentation can be found at https://xai-l2o.research.typal.academy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;SINFONY&#65292;&#23427;&#36890;&#36807;&#23545;&#28040;&#24687;&#36827;&#34892;&#25968;&#25454;&#20943;&#23569;&#21644;&#21487;&#38752;&#20256;&#36755;&#26469;&#26368;&#22909;&#22320;&#20445;&#30041;&#35821;&#20041;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2204.13366</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Semantic Information Recovery in Wireless Networks. (arXiv:2204.13366v4 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;SINFONY&#65292;&#23427;&#36890;&#36807;&#23545;&#28040;&#24687;&#36827;&#34892;&#25968;&#25454;&#20943;&#23569;&#21644;&#21487;&#38752;&#20256;&#36755;&#26469;&#26368;&#22909;&#22320;&#20445;&#30041;&#35821;&#20041;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;1949&#24180;Weaver&#25552;&#20986;&#30340;&#35821;&#20041;&#36890;&#20449;&#24605;&#24819;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290; &#23427;&#25171;&#30772;&#20102;Shannon&#30340;&#32463;&#20856;&#35774;&#35745;&#33539;&#20363;&#65292;&#26088;&#22312;&#20256;&#36882;&#28040;&#24687;&#30340;&#21547;&#20041;&#65292;&#21363;&#35821;&#20041;&#65292;&#32780;&#19981;&#26159;&#20854;&#30830;&#20999;&#29256;&#26412;&#65292;&#20174;&#32780;&#20801;&#35768;&#33410;&#30465;&#20449;&#24687;&#36895;&#29575;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;Basu&#31561;&#20154;&#30340;&#24314;&#27169;&#35821;&#20041;&#30340;&#22522;&#26412;&#26041;&#27861;&#25193;&#23637;&#21040;&#23436;&#25972;&#36890;&#20449;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#38544;&#21547;&#30340;&#38543;&#26426;&#21464;&#37327;&#26469;&#24314;&#27169;&#35821;&#20041;&#65292;&#24182;&#23558;&#35821;&#20041;&#36890;&#20449;&#20219;&#21153;&#23450;&#20041;&#20026;&#36890;&#36807;&#36890;&#20449;&#20449;&#36947;&#23545;&#28040;&#24687;&#36827;&#34892;&#25968;&#25454;&#20943;&#23569;&#21644;&#21487;&#38752;&#20256;&#36755;&#65292;&#20174;&#32780;&#26368;&#22909;&#22320;&#20445;&#30041;&#35821;&#20041;&#12290; &#25105;&#20204;&#23558;&#27492;&#20219;&#21153;&#20316;&#20026;&#31471;&#21040;&#31471;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#65292;&#20801;&#35768;&#22312;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#30340;&#21516;&#26102;&#36827;&#34892;&#21387;&#32553;&#12290; &#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;ML&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;SINFONY&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#24067;&#24335;&#22810;&#28857;&#22330;&#26223;&#65306;SIN&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent success of Machine Learning (ML) tools in wireless communications, the idea of semantic communication by Weaver from 1949 has gained attention. It breaks with Shannon's classic design paradigm by aiming to transmit the meaning of a message, i.e., semantics, rather than its exact version and thus allows for savings in information rate. In this work, we extend the fundamental approach from Basu et al. for modeling semantics to the complete communications Markov chain. Thus, we model semantics by means of hidden random variables and define the semantic communication task as the data-reduced and reliable transmission of messages over a communication channel such that semantics is best preserved. We cast this task as an end-to-end Information Bottleneck problem, allowing for compression while preserving relevant information most. As a solution approach, we propose the ML-based semantic communication system SINFONY and use it for a distributed multipoint scenario: SIN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.11740</link><description>&lt;p&gt;
&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#23545;&#20851;&#38190;&#26399;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#23454;&#29616;&#31361;&#35302;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#12290;&#65288;arXiv: 2203.11740v12 [cs.NE] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#31361;&#35302;&#20849;&#20139;&#36830;&#25509;&#26435;&#37325;&#20043;&#22806;&#65292;PNN&#36824;&#21253;&#25324;&#31361;&#35302;&#26377;&#25928;&#33539;&#22260;&#30340;&#26435;&#37325;[14-25]&#12290;PNN&#32771;&#34385;&#31361;&#35302;&#24378;&#24230;&#24179;&#34913;&#22312;&#31361;&#35302;&#21534;&#22124;&#30340;&#21160;&#24577;&#21644;&#38271;&#24230;&#24120;&#25968;&#20043;&#21644;&#30340;&#38745;&#24577;&#20013;[14]&#65292;&#24182;&#21253;&#21547;&#20102;&#40060;&#32676;&#34892;&#20026;&#30340;&#20808;&#23548;&#34892;&#20026;&#12290;&#31361;&#35302;&#24418;&#25104;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#20250;&#25233;&#21046;&#26641;&#31361;&#29983;&#25104;[15]&#12290;&#31867;&#20284;&#20110;Spring Boot&#20013;&#30340;&#24378;&#21046;&#38887;&#24615;&#65292;&#21453;&#21521;&#22238;&#36335;&#30340;&#35760;&#24518;&#25345;&#20037;&#24230;&#26799;&#24230;&#20063;&#23384;&#22312;&#12290;&#30456;&#23545;&#36739;&#22909;&#21644;&#36739;&#24046;&#30340;&#26799;&#24230;&#20449;&#24687;&#23384;&#20648;&#22312;&#31867;&#20284;&#20110;&#33041;&#35126;&#30340;&#35760;&#24518;&#30165;&#36857;&#32454;&#32990;&#20013;&#65292;&#22312;&#21453;&#21521;&#22238;&#36335;&#30340;&#31361;&#35302;&#24418;&#25104;&#20013;&#12290;&#20105;&#35758;&#35748;&#20026;&#20154;&#31867;&#28023;&#39532;&#31070;&#32463;&#20803;&#30340;&#20877;&#29983;&#33021;&#21147;&#26159;&#21542;&#25345;&#32493;&#21040;&#32769;&#24180;&#65292;&#24182;&#21487;&#33021;&#22312;&#21518;&#26399;&#36845;&#20195;&#20013;&#24418;&#25104;&#26032;&#30340;&#26356;&#38271;&#30340;&#22238;&#36335;[17,18]&#12290;&#20851;&#38381;&#20851;&#38190;&#26399;&#20250;&#23548;&#33268;&#31070;&#32463;&#32010;&#20081;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;[19]&#12290;&#32771;&#34385;&#21040;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#28608;&#27963;&#31361;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#22238;&#24402;&#31639;&#27861;&#65292;&#21487;&#38024;&#23545;&#22823;&#31867;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23454;&#20363;&#24207;&#21015;&#30340;&#23545;&#25239;&#24615;&#22238;&#24212;&#65292;&#22312;&#36890;&#29992;&#21487;&#20998;&#31163;&#25351;&#26631;&#31354;&#38388;&#19978;&#23454;&#29616;&#24378;&#19968;&#33268;&#24615;&#30340;&#36890;&#29992;&#19968;&#33268;&#24615;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2203.05067</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25239;&#22238;&#24212;&#30340;&#36890;&#29992;&#22238;&#24402;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Regression with Adversarial Responses. (arXiv:2203.05067v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#22238;&#24402;&#31639;&#27861;&#65292;&#21487;&#38024;&#23545;&#22823;&#31867;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23454;&#20363;&#24207;&#21015;&#30340;&#23545;&#25239;&#24615;&#22238;&#24212;&#65292;&#22312;&#36890;&#29992;&#21487;&#20998;&#31163;&#25351;&#26631;&#31354;&#38388;&#19978;&#23454;&#29616;&#24378;&#19968;&#33268;&#24615;&#30340;&#36890;&#29992;&#19968;&#33268;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#36890;&#29992;&#21487;&#20998;&#31163;&#25351;&#26631;&#31354;&#38388;&#19978;&#65292;&#23545;&#20110;&#22823;&#31867;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23454;&#20363;&#24207;&#21015;&#30340;&#23545;&#25239;&#22238;&#24402;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20851;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#29305;&#24449;&#35828;&#26126;&#12290;&#25105;&#20204;&#32771;&#34385;&#24378;&#19968;&#33268;&#24615;&#30340;&#36890;&#29992;&#19968;&#33268;&#24615;&#23398;&#20064;&#65292;&#26080;&#38656;&#23545;&#20540;&#22238;&#24212;&#36827;&#34892;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65306;&#36825;&#31181;&#30446;&#26631;&#21487;&#22312;&#27604;&#24179;&#31283;&#36807;&#31243;&#26356;&#22823;&#30340;&#23454;&#20363;&#24207;&#21015;&#31867;&#20013;&#23454;&#29616;&#65292;&#24182;&#25581;&#31034;&#20102;&#20540;&#31354;&#38388;&#20043;&#38388;&#30340;&#26681;&#26412;&#20108;&#20998;&#27861;&#65306;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#26377;&#38480;&#26102;&#38388;&#27573;&#22343;&#20540;&#20272;&#35745;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#20048;&#35266;&#30340;&#36890;&#29992;&#24615;&#23398;&#20064;&#35268;&#21017;&#65292;&#21363;&#22914;&#26524;&#23427;&#20204;&#26410;&#33021;&#23454;&#29616;&#36890;&#29992;&#19968;&#33268;&#24615;&#65292;&#21017;&#20854;&#20182;&#20219;&#20309;&#31639;&#27861;&#20063;&#23558;&#22833;&#36133;&#12290;&#23545;&#20110;&#26410;&#30028;&#38480;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28201;&#21644;&#30340;&#21487;&#31215;&#26465;&#20214;&#65292;&#20854;&#19979;&#26377;&#23545;&#25239;&#24615;&#22238;&#24402;&#30340;&#31639;&#27861;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#30456;&#21516;&#30340;&#24037;&#20855;&#24212;&#29992;&#20110;&#24102;&#26377;&#23545;&#25239;&#24615;&#35823;&#24046;&#30340;&#36890;&#29992;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide algorithms for regression with adversarial responses under large classes of non-i.i.d. instance sequences, on general separable metric spaces, with provably minimal assumptions. We also give characterizations of learnability in this regression context. We consider universal consistency which asks for strong consistency of a learner without restrictions on the value responses. Our analysis shows that such an objective is achievable for a significantly larger class of instance sequences than stationary processes, and unveils a fundamental dichotomy between value spaces: whether finite-horizon mean estimation is achievable or not. We further provide optimistically universal learning rules, i.e., such that if they fail to achieve universal consistency, any other algorithms will fail as well. For unbounded losses, we propose a mild integrability condition under which there exist algorithms for adversarial regression under large classes of non-i.i.d. instance sequences. In additio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#29616;&#23454;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.04769</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Autoregressive based Drift Detection Method. (arXiv:2203.04769v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#29616;&#23454;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#27169;&#22411;&#26159;&#22312;&#21382;&#21490;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#20540;&#12290;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#22312;&#26102;&#38388;&#19978;&#19981;&#21457;&#29983;&#21464;&#21270;&#65288;&#24179;&#31283;&#24615;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#65292;&#27169;&#22411;&#24517;&#39035;&#36866;&#24212;&#26032;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#65292;&#23548;&#33268;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26032;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;ADDM&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21040;&#31616;&#21333;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#30340;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#65292;&#24182;&#22312;&#26816;&#27979;&#21508;&#31181;&#27010;&#24565;&#28418;&#31227;&#26041;&#38754;&#20855;&#26377;&#32463;&#39564;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the classic machine learning framework, models are trained on historical data and used to predict future values. It is assumed that the data distribution does not change over time (stationarity). However, in real-world scenarios, the data generation process changes over time and the model has to adapt to the new incoming data. This phenomenon is known as concept drift and leads to a decrease in the predictive model's performance. In this study, we propose a new concept drift detection method based on autoregressive models called ADDM. This method can be integrated into any machine learning algorithm from deep neural networks to simple linear regression model. Our results show that this new concept drift detection method outperforms the state-of-the-art drift detection methods, both on synthetic data sets and real-world data sets. Our approach is theoretically guaranteed as well as empirical and effective for the detection of various concept drifts. In addition to the drift detector,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#19981;&#36830;&#32493;&#24615;&#23398;&#20064;&#30340;MVS&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#37325;&#24314;&#23436;&#25972;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#31934;&#24230;&#65292;&#26041;&#27861;&#26159;&#32852;&#21512;&#20272;&#35745;&#28145;&#24230;&#21644;&#36793;&#30028;&#22320;&#22270;&#65292;&#20854;&#20013;&#36793;&#30028;&#22320;&#22270;&#29992;&#20110;&#36827;&#19968;&#27493;&#32454;&#21270;&#28145;&#24230;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2203.01391</link><description>&lt;p&gt;
DDL-MVS: MVS&#32593;&#32476;&#28145;&#24230;&#19981;&#36830;&#32493;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DDL-MVS: Depth Discontinuity Learning for MVS Networks. (arXiv:2203.01391v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01391
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#19981;&#36830;&#32493;&#24615;&#23398;&#20064;&#30340;MVS&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#37325;&#24314;&#23436;&#25972;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#31934;&#24230;&#65292;&#26041;&#27861;&#26159;&#32852;&#21512;&#20272;&#35745;&#28145;&#24230;&#21644;&#36793;&#30028;&#22320;&#22270;&#65292;&#20854;&#20013;&#36793;&#30028;&#22320;&#22270;&#29992;&#20110;&#36827;&#19968;&#27493;&#32454;&#21270;&#28145;&#24230;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;MVS&#26041;&#27861;&#31934;&#24230;&#36739;&#39640;&#20294;&#23436;&#25972;&#24615;&#19981;&#36275;&#65292;&#32780;&#26368;&#36817;&#24320;&#21457;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#35270;&#22270;&#31435;&#20307;&#25216;&#26415;&#20855;&#26377;&#23436;&#25972;&#24615;&#25913;&#21892;&#65292;&#20294;&#29306;&#29298;&#20102;&#31934;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#19981;&#36830;&#32493;&#24615;&#23398;&#20064;&#30340;MVS&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#20272;&#35745;&#28145;&#24230;&#21644;&#36793;&#30028;&#22320;&#22270;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#20445;&#30041;&#37325;&#24314;&#30340;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#32852;&#21512;&#20272;&#35745;&#28145;&#24230;&#21644;&#36793;&#30028;&#22320;&#22270;&#65292;&#20854;&#20013;&#36793;&#30028;&#22320;&#22270;&#26126;&#30830;&#29992;&#20110;&#36827;&#19968;&#27493;&#32454;&#21270;&#28145;&#24230;&#22320;&#22270;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;MVS&#31649;&#36947;&#20013;&#65292;&#20854;&#20013;&#37325;&#24314;&#21462;&#20915;&#20110;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#22320;&#22270;&#20272;&#35745;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20934;&#30456;&#27604;&#25913;&#21892;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28304;&#20195;&#30721;&#21363;&#23558;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional MVS methods have good accuracy but struggle with completeness, while recently developed learning-based multi-view stereo (MVS) techniques have improved completeness except accuracy being compromised. We propose depth discontinuity learning for MVS methods, which further improves accuracy while retaining the completeness of the reconstruction. Our idea is to jointly estimate the depth and boundary maps where the boundary maps are explicitly used for further refinement of the depth maps. We validate our idea and demonstrate that our strategies can be easily integrated into the existing learning-based MVS pipeline where the reconstruction depends on high-quality depth map estimation. Extensive experiments on various datasets show that our method improves reconstruction quality compared to baseline. Experiments also demonstrate that the presented model and strategies have good generalization capabilities. The source code will be available soon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#32570;&#22833;&#25968;&#25454;&#22270;&#27169;&#22411;&#30340;&#21487;&#26816;&#39564;&#24615;&#21644;&#35774;&#35745;&#25311;&#21512;&#20248;&#24230;&#27979;&#35797;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2203.00132</link><description>&lt;p&gt;
&#35770;&#32570;&#22833;&#25968;&#25454;&#27169;&#22411;&#20013;&#30340;&#21487;&#26816;&#39564;&#24615;&#21644;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
On Testability and Goodness of Fit Tests in Missing Data Models. (arXiv:2203.00132v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#32570;&#22833;&#25968;&#25454;&#22270;&#27169;&#22411;&#30340;&#21487;&#26816;&#39564;&#24615;&#21644;&#35774;&#35745;&#25311;&#21512;&#20248;&#24230;&#27979;&#35797;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25551;&#36848;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#25551;&#36848;&#24314;&#27169;&#20551;&#35774;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#20013;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#24471;&#21040;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#22270;&#25152;&#32534;&#30721;&#30340;&#20551;&#35774;&#26159;&#21542;&#25104;&#31435;&#65292;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#23545;&#36825;&#20123;&#20551;&#35774;&#30340;&#39564;&#35777;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#19977;&#31867;&#32570;&#22833;&#25968;&#25454;&#22270;&#27169;&#22411;&#30340;&#21487;&#26816;&#39564;&#24615;&#21644;&#35774;&#35745;&#25311;&#21512;&#20248;&#24230;&#27979;&#35797;&#30340;&#26032;&#35265;&#35299;&#12290;&#25506;&#35752;&#30340;&#27169;&#22411;&#31867;&#21035;&#21253;&#25324;&#65306;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;&#36864;&#20986;/&#25130;&#23614;&#30340;&#32437;&#21521;&#30740;&#31350;&#30340;&#24207;&#36143;&#32570;&#22833;&#38543;&#26426;&#27169;&#22411;&#21644;&#32570;&#22833;&#38750;&#38543;&#26426;&#27169;&#22411;&#65292;&#20197;&#21450;&#21487;&#20197;&#24212;&#29992;&#20110;&#27178;&#25130;&#38754;&#30740;&#31350;&#21644;&#35843;&#26597;&#30340;&#38750;&#33258;&#25105;&#25130;&#26029;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant progress has been made in developing identification and estimation techniques for missing data problems where modeling assumptions can be described via a directed acyclic graph. The validity of results using such techniques rely on the assumptions encoded by the graph holding true; however, verification of these assumptions has not received sufficient attention in prior work. In this paper, we provide new insights on the testable implications of three broad classes of missing data graphical models, and design goodness-of-fit tests for them. The classes of models explored are: sequential missing-at-random and missing-not-at-random models which can be used for modeling longitudinal studies with dropout/censoring, and a no self-censoring model which can be applied to cross-sectional studies and surveys.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20984;&#21644;&#38750;&#20984;&#32452;&#21512;&#20989;&#25968;&#27714;&#21644;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;Oracle&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#21152;&#36895;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.12396</link><description>&lt;p&gt;
&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#38543;&#26426;&#20248;&#21270;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Finite-Sum Coupled Compositional Stochastic Optimization: Theory and Applications. (arXiv:2202.12396v7 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20984;&#21644;&#38750;&#20984;&#32452;&#21512;&#20989;&#25968;&#27714;&#21644;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;Oracle&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#21152;&#36895;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#20989;&#25968;&#27714;&#21644;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#27714;&#21644;&#39033;&#30340;&#20869;&#23618;&#20989;&#25968;&#19982;&#30456;&#24212;&#30340;&#27714;&#21644;&#32034;&#24341;&#37197;&#23545;&#12290;&#25105;&#20204;&#23558;&#36825;&#31867;&#38382;&#39064;&#31216;&#20026;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;&#65288;FCCO&#65289;&#12290;&#36825;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#29992;&#20110;&#20248;&#21270;&#38750;&#20984;&#25110;&#20984;&#32452;&#21512;&#27979;&#37327;/&#30446;&#26631;&#65292;&#20363;&#22914;&#24179;&#22343;&#31934;&#24230;&#65288;AP&#65289;&#65292;p-&#33539;&#25968;&#25512;&#36827;&#65292;&#21015;&#34920;&#24335;&#25490;&#21517;&#25439;&#22833;&#65292;&#37051;&#22495;&#32452;&#20214;&#20998;&#26512;&#65288;NCA&#65289;&#65292;&#28145;&#24230;&#29983;&#23384;&#20998;&#26512;&#21644;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#22312;&#26576;&#20123;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#20026;&#38750;&#20984;&#21644;&#20984;&#30446;&#26631;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#38543;&#26426;&#31639;&#27861;&#30340;&#20840;&#38754;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#32467;&#26524;&#26159;&#20351;&#29992;&#22522;&#20110;&#31227;&#21160;&#24179;&#22343;&#30340;&#20272;&#35745;&#22120;&#19982;&#23567;&#25209;&#37327;&#24182;&#34892;&#21152;&#36895;&#25913;&#36827;&#20102;Oracle&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#36824;&#23637;&#31034;&#20102;....
&lt;/p&gt;
&lt;p&gt;
This paper studies stochastic optimization for a sum of compositional functions, where the inner-level function of each summand is coupled with the corresponding summation index. We refer to this family of problems as finite-sum coupled compositional optimization (FCCO). It has broad applications in machine learning for optimizing non-convex or convex compositional measures/objectives such as average precision (AP), p-norm push, listwise ranking losses, neighborhood component analysis (NCA), deep survival analysis, deep latent variable models, etc., which deserves finer analysis. Yet, existing algorithms and analyses are restricted in one or other aspects. The contribution of this paper is to provide a comprehensive convergence analysis of a simple stochastic algorithm for both non-convex and convex objectives. Our key result is the improved oracle complexity with the parallel speed-up by using the moving-average based estimator with mini-batching. Our theoretical analysis also exhibit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;on-policy&#21644;off-policy RL&#24773;&#20917;&#19979;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#24191;&#20041;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31283;&#24577;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22343;&#20540;-&#26041;&#24046;&#21644;&#30072;&#21464;&#39118;&#38505;&#24230;&#37327;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2202.11046</link><description>&lt;p&gt;
&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A policy gradient approach for optimization of smooth risk measures. (arXiv:2202.11046v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;on-policy&#21644;off-policy RL&#24773;&#20917;&#19979;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#24191;&#20041;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31283;&#24577;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22343;&#20540;-&#26041;&#24046;&#21644;&#30072;&#21464;&#39118;&#38505;&#24230;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21253;&#25324;on-policy&#21644;off-policy&#24773;&#20917;&#12290;&#25105;&#20204;&#32771;&#34385;&#26102;&#38388;&#27573;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#32047;&#31215;&#25240;&#25187;&#22870;&#21169;&#30340;&#24191;&#20041;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#26469;&#24314;&#27169;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#27169;&#26495;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20998;&#21035;&#22312;on-policy&#21644;off-policy RL&#24773;&#20917;&#19979;&#20248;&#21270;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#38750;&#28176;&#36827;&#24615;&#30028;&#65292;&#37327;&#21270;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#25910;&#25947;&#21040;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31283;&#24577;&#28857;&#30340;&#36895;&#29575;&#12290;&#20316;&#20026;&#29305;&#20363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20998;&#21035;&#24212;&#29992;&#20110;&#22343;&#20540;-&#26041;&#24046;&#21644;&#30072;&#21464;&#39118;&#38505;&#24230;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose policy gradient algorithms for solving a risk-sensitive reinforcement learning problem in on-policy as well as off-policy settings. We consider episodic Markov decision processes, and model the risk using the broad class of smooth risk measures of the cumulative discounted reward. We propose two template policy gradient algorithms that optimize a smooth risk measure in on-policy and off-policy RL settings, respectively. We derive non-asymptotic bounds that quantify the rate of convergence to our proposed algorithms to a stationary point of the smooth risk measure. As special cases, we establish that our algorithms apply to the optimization of mean-variance and distortion risk measures, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;-&#23545;&#20598;&#34920;&#36848;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#25554;&#20540;&#24230;&#37327;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#25910;&#25947;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2202.10506</link><description>&lt;p&gt;
&#21152;&#36895;&#27491;-&#23545;&#20598;&#26041;&#27861;&#27714;&#35299;&#27491;&#21017;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Accelerating Primal-dual Methods for Regularized Markov Decision Processes. (arXiv:2202.10506v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;-&#23545;&#20598;&#34920;&#36848;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#25554;&#20540;&#24230;&#37327;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#25910;&#25947;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#20851;&#27880;&#29109;&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#27491;-&#23545;&#20598;&#34920;&#36848;&#12290;&#30001;&#20110;&#32570;&#20047;&#20005;&#26684;&#30340;&#20984;&#24615;&#21644;&#20985;&#24615;&#65292;&#26631;&#20934;&#30340;&#19968;&#38454;&#26041;&#27861;&#25910;&#25947;&#32531;&#24930;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20108;&#27425;&#20984;&#21270;&#30340;&#27491;-&#23545;&#20598;&#34920;&#36848;&#12290;&#26032;&#34920;&#36848;&#30340;&#33258;&#28982;&#26799;&#24230;&#19978;&#21319;&#19979;&#38477;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#21644;&#25351;&#25968;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;&#24230;&#37327;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#25910;&#25947;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#37117;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropy regularized Markov decision processes have been widely used in reinforcement learning. This paper is concerned with the primal-dual formulation of the entropy regularized problems. Standard first-order methods suffer from slow convergence due to the lack of strict convexity and concavity. To address this issue, we first introduce a new quadratically convexified primal-dual formulation. The natural gradient ascent descent of the new formulation enjoys global convergence guarantee and exponential convergence rate. We also propose a new interpolating metric that further accelerates the convergence significantly. Numerical results are provided to demonstrate the performance of the proposed methods under multiple settings.
&lt;/p&gt;</description></item><item><title>GoSafeOpt&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23433;&#20840;&#25506;&#32034;&#39640;&#32500;&#31995;&#32479;&#24182;&#25552;&#20379;&#20840;&#23616;&#26368;&#20248;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.09562</link><description>&lt;p&gt;
GoSafeOpt: &#21487;&#25193;&#23637;&#30340;&#23433;&#20840;&#20840;&#23616;&#20248;&#21270;&#21160;&#24577;&#31995;&#32479;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
GoSafeOpt: Scalable Safe Exploration for Global Optimization of Dynamical Systems. (arXiv:2201.09562v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09562
&lt;/p&gt;
&lt;p&gt;
GoSafeOpt&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23433;&#20840;&#25506;&#32034;&#39640;&#32500;&#31995;&#32479;&#24182;&#25552;&#20379;&#20840;&#23616;&#26368;&#20248;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#22312;&#29289;&#29702;&#31995;&#32479;&#19978;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21363;&#20351;&#19968;&#27425;&#22833;&#36133;&#20063;&#21487;&#33021;&#23548;&#33268;&#26114;&#36149;&#30340;&#30828;&#20214;&#25439;&#22351;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20445;&#35777;&#23433;&#20840;&#65288;&#21363;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#27809;&#26377;&#22833;&#36133;&#65289;&#30340;&#26080;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#20165;&#23616;&#38480;&#20110;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;GoSafe&#31639;&#27861;&#26159;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#20363;&#22806;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#26080;&#27861;&#22788;&#29702;&#39640;&#32500;&#31995;&#32479;&#65292;&#22240;&#27492;&#26080;&#27861;&#24212;&#29992;&#20110;&#22823;&#22810;&#25968;&#23454;&#38469;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;GoSafeOpt&#20316;&#20026;&#31532;&#19968;&#20010;&#33021;&#22815;&#23433;&#20840;&#22320;&#21457;&#29616;&#39640;&#32500;&#31995;&#32479;&#30340;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#24182;&#20855;&#26377;&#23433;&#20840;&#21644;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#26426;&#26800;&#33218;&#19978;&#23637;&#31034;&#20102;GoSafeOpt&#27604;&#20854;&#20182;&#27169;&#22411;&#26080;&#20851;&#30340;&#23433;&#20840;&#23398;&#20064;&#26041;&#27861;&#26356;&#20248;&#31168;&#65292;&#32780;GoSafe&#22312;&#35813;&#26426;&#26800;&#33218;&#19978;&#23558;&#26159;&#31105;&#27490;&#20351;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning optimal control policies directly on physical systems is challenging since even a single failure can lead to costly hardware damage. Most existing model-free learning methods that guarantee safety, i.e., no failures, during exploration are limited to local optima. A notable exception is the GoSafe algorithm, which, unfortunately, cannot handle high-dimensional systems and hence cannot be applied to most real-world dynamical systems. This work proposes GoSafeOpt as the first algorithm that can safely discover globally optimal policies for high-dimensional systems while giving safety and optimality guarantees. We demonstrate the superiority of GoSafeOpt over competing model-free safe learning methods on a robot arm that would be prohibitive for GoSafe.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#19982;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.11217</link><description>&lt;p&gt;
&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22312;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#19979;&#30340;&#24212;&#29992;&#65306;&#26234;&#33021;&#36710;&#36742;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles. (arXiv:2112.11217v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#19982;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#32467;&#26500;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#23433;&#20840;&#24615;&#21644;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#36817;&#20284;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#20316;&#21697;&#35752;&#35770;&#20102;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#24615;&#32422;&#26463;&#19979;&#35774;&#35745;&#23433;&#20840;RL&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#65292;&#20197;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#29992;&#20110;&#39044;&#27979;&#31574;&#30053;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25351;&#23548;&#31574;&#30053;&#23433;&#20840;&#26356;&#26032;&#12290;&#24050;&#35777;&#26126;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#20998;&#26512;&#20102;&#28436;&#21592;&#35780;&#35770;&#23478;&#23454;&#29616;&#30340;&#25910;&#25947;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#27169;&#25311;&#30340;Sa&#20013;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recently, safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier force-based control policy structure to guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic implementation is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#20445;&#38505;&#34892;&#19994;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#29616;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.09466</link><description>&lt;p&gt;
&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#65306;&#35299;&#20915;&#20445;&#38505;&#34892;&#19994;&#20013;&#30340;&#26631;&#27880;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fair Active Learning: Solving the Labeling Problem in Insurance. (arXiv:2112.09466v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#20445;&#38505;&#34892;&#19994;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#29616;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22312;&#20445;&#38505;&#34892;&#19994;&#24191;&#27867;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#37325;&#22823;&#38556;&#30861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20419;&#36827;&#20844;&#24179;&#24615;&#12290;&#26368;&#21021;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#20445;&#38505;&#25968;&#25454;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#38477;&#20302;&#26631;&#27880;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#24378;&#35843;&#25968;&#25454;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#20027;&#21160;&#23398;&#20064;&#25277;&#26679;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#20445;&#38505;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#35813;&#20998;&#26512;&#24378;&#35843;&#20102;&#23454;&#29616;&#20844;&#27491;&#27169;&#22411;&#25512;&#26029;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#22797;&#21046;&#24213;&#23618;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#26679;&#20449;&#24687;&#37327;&#20805;&#36275;&#19988;&#20844;&#24179;&#30340;&#23454;&#20363;&#65292;&#22312;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36825;&#19968;&#28857;&#22312;&#20445;&#38505;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses significant obstacles that arise from the widespread use of machine learning models in the insurance industry, with a specific focus on promoting fairness. The initial challenge lies in effectively leveraging unlabeled data in insurance while reducing the labeling effort and emphasizing data relevance through active learning techniques. The paper explores various active learning sampling methodologies and evaluates their impact on both synthetic and real insurance datasets. This analysis highlights the difficulty of achieving fair model inferences, as machine learning models may replicate biases and discrimination found in the underlying data. To tackle these interconnected challenges, the paper introduces an innovative fair active learning method. The proposed approach samples informative and fair instances, achieving a good balance between model predictive performance and fairness, as confirmed by numerical experiments on insurance datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TND-NAS&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#21644;&#22810;&#30446;&#26631;NAS&#30340;&#20860;&#23481;&#24615;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.03892</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm. (arXiv:2111.03892v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TND-NAS&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#21644;&#22810;&#30446;&#26631;NAS&#30340;&#20860;&#23481;&#24615;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#24050;&#25104;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#39046;&#22495;&#30340;&#20027;&#27969;&#30740;&#31350;&#35838;&#39064;&#65292;&#30456;&#23545;&#20110;&#26089;&#26399;&#30340;EA-based&#21644;RL-based&#26041;&#27861;&#65292;&#20854;&#39640;&#25928;&#29575;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24050;&#19981;&#20877;&#33021;&#22815;&#33258;&#28982;&#22320;&#24212;&#23545;&#19981;&#21487;&#24494;&#21442;&#25968;&#65292;&#22914;&#33021;&#28304;&#21644;&#36164;&#28304;&#21463;&#38480;&#25928;&#29575;&#31561;&#12290;&#38024;&#23545;&#22810;&#30446;&#26631;NAS&#39046;&#22495;&#30340;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#23545;&#27599;&#20010;&#20505;&#36873;&#26550;&#26500;&#36827;&#34892;&#21807;&#19968;&#30340;&#20248;&#21270;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TND-NAS&#65292;&#23427;&#20855;&#26377;&#19981;&#21487;&#24494;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;&#21644;&#19981;&#21516;iable NAS&#26694;&#26550;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;TND-NAS&#22312;&#25628;&#32034;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable architecture search has gradually become the mainstream research topic in the field of Neural Architecture Search (NAS) for its high efficiency compared with the early NAS (EA-based, RL-based) methods. Recent differentiable NAS also aims at further improving the search performance and reducing the GPU-memory consumption. However, these methods are no longer naturally capable of tackling the non-differentiable objectives, e.g., energy, resource-constrained efficiency, and other metrics, let alone the multi-objective search demands. Researches in the multi-objective NAS field target this but requires vast computational resources cause of the sole optimization of each candidate architecture. In light of this discrepancy, we propose the TND-NAS, which is with the merits of the high efficiency in differentiable NAS framework and the compatibility among non-differentiable metrics in Multi-objective NAS. Under the differentiable NAS framework, with the continuous relaxation of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#33719;&#24471;&#30830;&#20999;&#21160;&#21147;&#23398;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#21629;&#21517;&#20026;&#8220;FJet&#8221;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35856;&#25391;&#23376;&#12289;&#25670;&#21644;&#36798;&#33452;&#22855;&#25391;&#33633;&#22120;&#31561;&#31034;&#20363;&#20013;&#34987;&#35777;&#26126;&#21487;&#20197;&#20934;&#30830;&#22320;&#22797;&#21046;&#21160;&#21147;&#23398;&#24182;&#24674;&#22797;&#28508;&#22312;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2110.06917</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Extracting Dynamical Models from Data. (arXiv:2110.06917v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#33719;&#24471;&#30830;&#20999;&#21160;&#21147;&#23398;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#21629;&#21517;&#20026;&#8220;FJet&#8221;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35856;&#25391;&#23376;&#12289;&#25670;&#21644;&#36798;&#33452;&#22855;&#25391;&#33633;&#22120;&#31561;&#31034;&#20363;&#20013;&#34987;&#35777;&#26126;&#21487;&#20197;&#20934;&#30830;&#22320;&#22797;&#21046;&#21160;&#21147;&#23398;&#24182;&#24674;&#22797;&#28508;&#22312;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#23545;&#30456;&#31354;&#38388;&#21464;&#37327;&#30340;&#26356;&#26032;&#24314;&#27169;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#30830;&#23450;&#31995;&#32479;&#22522;&#30784;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#12290;&#21487;&#20197;&#20934;&#30830;&#22320;&#22797;&#21046;&#35856;&#25391;&#23376;&#12289;&#25670;&#21644;&#36798;&#33452;&#22855;&#25391;&#33633;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27599;&#20010;&#31034;&#20363;&#20013;&#20934;&#30830;&#22320;&#24674;&#22797;&#20102;&#28508;&#22312;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#27492;&#26041;&#27861;&#21629;&#21517;&#20026;&#8220;FJet&#8221;&#65292;&#31867;&#20284;&#20110;&#24471;&#21040;&#30340;&#27169;&#22411;&#20110;Runge-Kutta&#25968;&#20540;&#31215;&#20998;&#26041;&#26696;&#30340;Taylor&#32423;&#25968;&#23637;&#24320;&#12290;&#36825;&#31181;&#31867;&#27604;&#20855;&#26377;&#26174;&#24335;&#25581;&#31034;&#36866;&#24403;&#20989;&#25968;&#24418;&#24335;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of determining the underlying dynamics of a system when only given data of its state over time has challenged scientists for decades. In this paper, the approach of using machine learning to model the {\em updates} of the phase space variables is introduced; this is done as a function of the phase space variables. (More generally, the modeling is done over the jet space of the variables.) This approach is shown to accurately replicate the dynamics for the examples of the harmonic oscillator, the pendulum, and the Duffing oscillator; the underlying differential equation is also accurately recovered in each example. In addition, the results in no way depend on how the data is sampled over time (i.e., regularly or irregularly). It is demonstrated that this approach (named "FJet") is similar to the model resulting from a Taylor series expansion of the Runge-Kutta (RK) numerical integration scheme. This analogy confers the advantage of explicitly revealing the appropriate functi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#38469;&#20351;&#29992;&#24182;&#21487;&#20197;&#26377;&#25928;&#36867;&#36991;&#40657;&#30418;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#8212;&#8212;EvadeDroid&#65292;&#24182;&#19988;&#21487;&#20197;&#20445;&#30041;&#21407;&#22987;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.03301</link><description>&lt;p&gt;
EvadeDroid&#65306;&#19968;&#31181;&#23545;&#40657;&#30418;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#23454;&#29992;&#36867;&#36920;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box Android Malware Detection. (arXiv:2110.03301v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#38469;&#20351;&#29992;&#24182;&#21487;&#20197;&#26377;&#25928;&#36867;&#36991;&#40657;&#30418;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#8212;&#8212;EvadeDroid&#65292;&#24182;&#19988;&#21487;&#20197;&#20445;&#30041;&#21407;&#22987;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24320;&#21457;&#36867;&#36991;&#25915;&#20987;&#26469;&#24191;&#27867;&#25506;&#32034;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#28431;&#27934;&#65307;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#36825;&#20123;&#25915;&#20987;&#30340;&#23454;&#29992;&#24615;&#20173;&#28982;&#26377;&#20105;&#35758;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#23450;&#25915;&#20987;&#32773;&#30693;&#36947;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#34892;&#20026;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#30446;&#26631;&#20998;&#31867;&#22120;&#35775;&#38382;&#26435;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EvadeDroid&#65292;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#20915;&#31574;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#26088;&#22312;&#26377;&#25928;&#36867;&#36991;&#40657;&#30418;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#12290;&#38500;&#20102;&#29983;&#25104;&#23454;&#38469;&#30340;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#36867;&#36991;&#25915;&#20987;&#36824;&#21487;&#20197;&#20445;&#30041;&#21407;&#22987;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;n-gram&#30340;&#26041;&#27861;&#26500;&#24314;&#26469;&#33258;&#33391;&#24615;&#26679;&#26412;&#30340;&#21151;&#33021;&#20445;&#25345;&#36716;&#25442;&#65292;&#36825;&#20123;&#36716;&#25442;&#19982;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#20855;&#26377;&#25805;&#20316;&#30721;&#32423;&#21035;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, researchers have extensively explored the vulnerabilities of Android malware detectors to adversarial examples through the development of evasion attacks; however, the practicality of these attacks in real-world scenarios remains arguable. The majority of studies have assumed attackers know the details of the target classifiers used for malware detection, while in reality, malicious actors have limited access to the target classifiers. This paper introduces EvadeDroid, a practical decision-based adversarial attack designed to effectively evade black-box Android malware detectors in real-world scenarios. In addition to generating real-world adversarial malware, the proposed evasion attack can also preserve the functionality of the original malware applications (apps). EvadeDroid constructs a collection of functionality-preserving transformations derived from benign donors that share opcode-level similarity with malware apps by leveraging an n-gram-based approach. T
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SubseasonalClimateUSA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#32654;&#22269;&#30340;&#20122;&#23395;&#33410;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#20316;&#32773;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#23545;&#22810;&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2109.10399</link><description>&lt;p&gt;
SubseasonalClimateUSA: &#29992;&#20110;&#20122;&#23395;&#33410;&#39044;&#27979;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking. (arXiv:2109.10399v3 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.10399
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SubseasonalClimateUSA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#32654;&#22269;&#30340;&#20122;&#23395;&#33410;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#20316;&#32773;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#23545;&#22810;&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#30340;&#20122;&#23395;&#33410;&#39044;&#27979;&#23545;&#36164;&#28304;&#37197;&#32622;&#21644;&#27668;&#20505;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#39044;&#27979;&#31038;&#21306;&#25552;&#20986;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#25216;&#33021;&#26377;&#38480;&#65292;&#24182;&#19988;&#39044;&#27979;&#30446;&#26631;&#20197;&#19968;&#31181;&#22797;&#26434;&#30340;&#26041;&#24335;&#20381;&#36182;&#20110;&#26412;&#22320;&#22825;&#27668;&#21644;&#20840;&#29699;&#27668;&#20505;&#21464;&#37327;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26174;&#31034;&#20986;&#25512;&#36827;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25972;&#29702;&#65292;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;&#22810;&#20010;&#30456;&#20851;&#25968;&#25454;&#26469;&#28304;&#12289;&#25991;&#20214;&#26684;&#24335;&#21644;&#26102;&#38388;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#32858;&#21512;&#36827;&#34892;&#25972;&#21512;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#24182;&#21152;&#36895;&#26410;&#26469;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SubseasonalClimateUSA&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#32654;&#22269;&#30340;&#20122;&#23395;&#33410;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#23545;&#21508;&#31181;&#19981;&#21516;&#30340;&#20122;&#23395;&#33410;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#25805;&#20316;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#12289;&#21476;&#20856;&#30340;&#27668;&#35937;&#22522;&#32447;&#20197;&#21450;&#21313;&#20010;&#32479;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and climate adaptation but poses many challenges for the forecasting community. At this forecast horizon, physics-based dynamical models have limited skill, and the targets for prediction depend in a complex manner on both local weather and global climate variables. Recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggregation across multiple relevant data sources, file formats, and temporal and spatial resolutions. To streamline this process and accelerate future development, we introduce SubseasonalClimateUSA, a curated dataset for training and benchmarking subseasonal forecasting models in the United States. We use this dataset to benchmark a diverse suite of subseasonal models, including operational dynamical models, classical meteorological baselines, and ten sta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#31561;&#31163;&#23376;&#20307;&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#34180;&#33180;&#28293;&#23556;&#27785;&#31215;&#27169;&#25311;&#12290;&#36890;&#36807;&#35757;&#32451;&#21367;&#31215;$\beta$&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#20943;&#23569;&#39640;&#32500;&#33021;&#37327;&#35282;&#20998;&#24067;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#32771;&#34385;&#21464;&#37327;&#32780;&#19981;&#26159;&#21333;&#20010;&#22266;&#23450;&#30340;Ti-Al&#21270;&#23398;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2109.01406</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#31561;&#31163;&#23376;&#20307;&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#28293;&#23556;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
An efficient plasma-surface interaction surrogate model for sputtering processes based on autoencoder neural networks. (arXiv:2109.01406v2 [physics.comp-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#31561;&#31163;&#23376;&#20307;&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#34180;&#33180;&#28293;&#23556;&#27785;&#31215;&#27169;&#25311;&#12290;&#36890;&#36807;&#35757;&#32451;&#21367;&#31215;$\beta$&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#20943;&#23569;&#39640;&#32500;&#33021;&#37327;&#35282;&#20998;&#24067;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#32771;&#34385;&#21464;&#37327;&#32780;&#19981;&#26159;&#21333;&#20010;&#22266;&#23450;&#30340;Ti-Al&#21270;&#23398;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34180;&#33180;&#28293;&#23556;&#27785;&#31215;&#30340;&#27169;&#25311;&#38656;&#35201;&#23558;&#27668;&#30456;&#20013;&#30340;&#31561;&#31163;&#23376;&#20307;&#21644;&#29289;&#36136;&#36755;&#36816;&#19982;&#36793;&#30028;&#34920;&#38754;&#30340;&#29983;&#38271;/&#28293;&#23556;&#36807;&#31243;&#20998;&#31163;&#12290;&#22522;&#20110;&#35299;&#26512;&#34920;&#36798;&#24335;&#25110;&#26597;&#25214;&#34920;&#30340;&#30028;&#38754;&#27169;&#22411;&#22312;&#26412;&#36136;&#19978;&#23558;&#36825;&#31181;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#38480;&#21046;&#20026;&#26368;&#23569;&#37327;&#12290;&#26368;&#36817;&#24050;&#32463;&#23637;&#31034;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#29992;&#20110;Ar&#31163;&#23376;&#36720;&#20987;Ti-Al&#22797;&#21512;&#38774;&#26448;&#12290;&#28982;&#32780;&#65292;&#25152;&#36873;&#30340;&#32593;&#32476;&#32467;&#26500;&#65288;&#21363;&#22810;&#23618;&#24863;&#30693;&#22120;&#65289;&#25552;&#20379;&#20102;&#32422;400&#19975;&#33258;&#30001;&#24230;&#65292;&#36825;&#23384;&#22312;&#36807;&#24230;&#25311;&#21512;&#30456;&#20851;&#21160;&#24577;&#24182;&#23558;&#27169;&#22411;&#22797;&#26434;&#21270;&#33267;&#19981;&#21487;&#38752;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#19978;&#26356;&#20026;&#22797;&#26434;&#20294;&#21442;&#25968;&#26041;&#38754;&#31616;&#21270;&#30340;&#22238;&#24402;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#38024;&#23545;&#19968;&#20010;&#25193;&#23637;&#26041;&#26696;&#32771;&#34385;&#21464;&#37327;&#32780;&#19981;&#26159;&#21333;&#20010;&#22266;&#23450;&#30340;Ti-Al&#21270;&#23398;&#35745;&#37327;&#12290;&#35757;&#32451;&#20102;&#19968;&#20010;&#21367;&#31215;$\beta$&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#20943;&#23569;&#39640;&#32500;&#33021;&#37327;&#35282;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Simulations of thin film sputter deposition require the separation of the plasma and material transport in the gas-phase from the growth/sputtering processes at the bounding surfaces. Interface models based on analytic expressions or look-up tables inherently restrict this complex interaction to a bare minimum. A machine learning model has recently been shown to overcome this remedy for Ar ions bombarding a Ti-Al composite target. However, the chosen network structure (i.e., a multilayer perceptron) provides approximately 4 million degrees of freedom, which bears the risk of overfitting the relevant dynamics and complicating the model to an unreliable extend. This work proposes a conceptually more sophisticated but parameterwise simplified regression artificial neural network for an extended scenario, considering a variable instead of a single fixed Ti-Al stoichiometry. A convolutional $\beta$-variational autoencoder is trained to reduce the high-dimensional energy-angular distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#20998;&#24067;&#20048;&#35266;&#20248;&#21270;&#65288;DOO&#65289;&#27169;&#22411;&#65292;&#22312;&#22806;&#25512;&#38382;&#39064;&#19978;&#22987;&#32456;&#33021;&#22815;&#36229;&#36234;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#65288;SAA&#65289;&#65307;&#28982;&#32780;&#65292;&#20048;&#35266;&#35299;&#30340;&#40065;&#26834;&#24615;&#36739;&#24046;&#19988;&#26356;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2105.12342</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#20987;&#36133;SAA&#65288;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#65289;&#30340;&#22806;&#25512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A data-driven approach to beating SAA out-of-sample. (arXiv:2105.12342v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.12342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#20998;&#24067;&#20048;&#35266;&#20248;&#21270;&#65288;DOO&#65289;&#27169;&#22411;&#65292;&#22312;&#22806;&#25512;&#38382;&#39064;&#19978;&#22987;&#32456;&#33021;&#22815;&#36229;&#36234;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#65288;SAA&#65289;&#65307;&#28982;&#32780;&#65292;&#20048;&#35266;&#35299;&#30340;&#40065;&#26834;&#24615;&#36739;&#24046;&#19988;&#26356;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#38382;&#39064;&#30340;&#35299;&#26377;&#26102;&#21487;&#33021;&#27604;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#65288;SAA&#65289;&#30340;&#39044;&#26399;&#22870;&#21169;&#35201;&#39640;&#65292;&#20294;&#24182;&#19981;&#20445;&#35777;&#24635;&#26159;&#36825;&#26679;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31867;&#20998;&#24067;&#20048;&#35266;&#20248;&#21270;&#65288;DOO&#65289;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#25105;&#20204;&#32771;&#34385;&#26368;&#20248;&#24773;&#20917;&#65288;DOO&#65289;&#21644;&#26368;&#22351;&#24773;&#20917;&#65288;DRO&#65289;&#27169;&#22411;&#65292;&#37027;&#20040;&#24635;&#26159;&#21487;&#20197;&#8220;&#20987;&#36133;&#8221;SAA&#30340;&#22806;&#25512;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#65292;&#36825;&#26159;&#26377;&#20195;&#20215;&#30340;&#65306;&#20048;&#35266;&#35299;&#27604;&#26368;&#22351;&#24773;&#20917;&#25110;SAA&#20248;&#21270;&#22120;&#26356;&#25935;&#24863;&#20110;&#27169;&#22411;&#38169;&#35823;&#65292;&#22240;&#27492;&#19981;&#22826;&#40065;&#26834;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#26377;&#38480;&#26102;&#24456;&#38590;&#26657;&#20934;&#26368;&#22351;&#25110;&#26368;&#20248;&#24773;&#20917;&#27169;&#22411;&#20197;&#36229;&#36234;SAA&#12290;
&lt;/p&gt;
&lt;p&gt;
While solutions of Distributionally Robust Optimization (DRO) problems can sometimes have a higher out-of-sample expected reward than the Sample Average Approximation (SAA), there is no guarantee. In this paper, we introduce a class of Distributionally Optimistic Optimization (DOO) models, and show that it is always possible to ``beat" SAA out-of-sample if we consider not just worst-case (DRO) models but also best-case (DOO) ones. We also show, however, that this comes at a cost: Optimistic solutions are more sensitive to model error than either worst-case or SAA optimizers, and hence are less robust and calibrating the worst- or best-case model to outperform SAA may be difficult when data is limited.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;AC-RL&#25511;&#21046;&#22120;&#65292;&#22312;&#22806;&#29615;&#20013;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30830;&#20445;&#31283;&#23450;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#22312;&#20869;&#29615;&#20013;&#37319;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#12290;&#35813;&#25511;&#21046;&#22120;&#36866;&#29992;&#20110;&#20004;&#31867;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#33021;&#36866;&#24212;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#21644;&#36755;&#20837;&#24133;&#20540;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2105.06577</link><description>&lt;p&gt;
&#37319;&#29992;&#33258;&#36866;&#24212;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22312;&#32447;&#31639;&#27861;&#19982;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Online Algorithms and Policies Using Adaptive and Machine Learning Approaches. (arXiv:2105.06577v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.06577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;AC-RL&#25511;&#21046;&#22120;&#65292;&#22312;&#22806;&#29615;&#20013;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30830;&#20445;&#31283;&#23450;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#22312;&#20869;&#29615;&#20013;&#37319;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#12290;&#35813;&#25511;&#21046;&#22120;&#36866;&#29992;&#20110;&#20004;&#31867;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#33021;&#36866;&#24212;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#21644;&#36755;&#20837;&#24133;&#20540;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#21463;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#36827;&#34892;&#23454;&#26102;&#25511;&#21046;&#21644;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22806;&#29615;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#23545;&#20110;&#21517;&#20041;&#21160;&#21147;&#23398;&#30340;&#31283;&#23450;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#21516;&#26102;&#22312;&#20869;&#29615;&#20013;&#37319;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#65288;AC&#65289;&#65292;&#20197;&#20415;&#22312;&#23454;&#26102;&#20013;&#65292;AC&#23558;&#38381;&#29615;&#21160;&#21147;&#23398;&#25910;&#32553;&#21040;&#30001;RL&#36319;&#36394;&#30340;&#31283;&#23450;&#36712;&#36857;&#12290;&#32771;&#34385;&#20102;&#20004;&#31867;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#20004;&#32773;&#37117;&#26159;&#25511;&#21046;&#20851;&#32852;&#30340;&#12290;&#31532;&#19968;&#31867;&#21160;&#24577;&#31995;&#32479;&#21033;&#29992;&#25193;&#23637;&#24418;&#24335;&#30340;&#24179;&#34913;&#28857;&#21644;&#26446;&#20122;&#26222;&#35834;&#22827;&#26041;&#27861;&#65292;&#32780;&#31532;&#20108;&#31867;&#38750;&#32447;&#24615;&#31995;&#32479;&#21017;&#20351;&#29992;&#25910;&#32553;&#29702;&#35770;&#12290;&#38024;&#23545;&#36825;&#20004;&#31867;&#31995;&#32479;&#25552;&#20986;&#20102;AC-RL&#25511;&#21046;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24341;&#23548;&#22312;&#32447;&#31574;&#30053;&#20445;&#35777;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#38454;&#35843;&#35856;&#22120;&#26469;&#36866;&#24212;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#21644;&#36755;&#20837;&#24133;&#20540;&#38480;&#21046;&#12290;&#38500;&#20102;&#24314;&#31435;&#31283;&#23450;&#24615;&#20445;&#35777;&#22806;&#65292;&#26412;&#25991;&#36824;&#24212;&#29992;&#20223;&#30495;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of real-time control and learning in dynamic systems subjected to parametric uncertainties. We propose a combination of a Reinforcement Learning (RL) based policy in the outer loop suitably chosen to ensure stability and optimality for the nominal dynamics, together with Adaptive Control (AC) in the inner loop so that in real-time AC contracts the closed-loop dynamics towards a stable trajectory traced out by RL. Two classes of nonlinear dynamic systems are considered, both of which are control-affine. The first class of dynamic systems utilizes equilibrium points %with expansion forms around these points and a Lyapunov approach while second class of nonlinear systems uses contraction theory. AC-RL controllers are proposed for both classes of systems and shown to lead to online policies that guarantee stability using a high-order tuner and accommodate parametric uncertainties and magnitude limits on the input. In addition to establishing a stability gua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#65292;&#35748;&#20026;&#36825;&#31867;&#39034;&#24207;&#23454;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#20250;&#24369;&#25910;&#25947;&#21040;&#25193;&#25955;&#26497;&#38480;&#65292;&#24182;&#33021;&#33719;&#24471;&#20851;&#20110;&#20960;&#31181;&#39034;&#24207;&#23454;&#39564;&#30340;&#21518;&#24724;&#21644;&#20449;&#24565;&#28436;&#21464;&#30340;&#22810;&#20010;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2101.09855</link><description>&lt;p&gt;
&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#30340;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Weak Signal Asymptotics for Sequentially Randomized Experiments. (arXiv:2101.09855v5 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.09855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#65292;&#35748;&#20026;&#36825;&#31867;&#39034;&#24207;&#23454;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#20250;&#24369;&#25910;&#25947;&#21040;&#25193;&#25955;&#26497;&#38480;&#65292;&#24182;&#33021;&#33719;&#24471;&#20851;&#20110;&#20960;&#31181;&#39034;&#24207;&#23454;&#39564;&#30340;&#21518;&#24724;&#21644;&#20449;&#24565;&#28436;&#21464;&#30340;&#22810;&#20010;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#65292;&#21253;&#25324;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#23454;&#39564;&#12290;&#22312;&#19968;&#20010;$n$&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35753;&#19981;&#21516;&#21160;&#20316;&#30340;&#24179;&#22343;&#22870;&#21169;&#38388;&#38553;&#25353;&#29031;$1/\sqrt{n}$&#30340;&#27604;&#20363;&#32553;&#25918;&#65292;&#20197;&#20445;&#25345;&#23398;&#20064;&#20219;&#21153;&#30340;&#38590;&#24230;&#38543;&#30528;$n$&#30340;&#22686;&#38271;&#32780;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#20250;&#24369;&#25910;&#25947;&#21040;&#25193;&#25955;&#26497;&#38480;&#65292;&#20854;&#20013;&#65292;&#33218;&#36873;&#25321;&#30340;&#27010;&#29575;&#20250;&#38543;&#30528;&#29366;&#24577;&#30340;&#21464;&#21270;&#32780;&#25345;&#32493;&#21464;&#21270;&#65292;&#24182;&#22312;&#28385;&#36275;&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#12290;&#25193;&#25955;&#26497;&#38480;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#31934;&#32454;&#30340;&#12289;&#29305;&#23450;&#20110;&#23454;&#20363;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#33719;&#24471;&#20851;&#20110;&#20960;&#31181;&#39034;&#24207;&#23454;&#39564;&#30340;&#21518;&#24724;&#21644;&#20449;&#24565;&#28436;&#21464;&#30340;&#22810;&#20010;&#35265;&#35299;&#65292;&#21253;&#25324;&#27748;&#26222;&#26862;&#37319;&#26679;&#65288;&#20294;&#19981;&#21253;&#25324;&#19981;&#28385;&#36275;&#25105;&#20204;&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;UCB&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#26377;&#39034;&#24207;&#23454;&#39564;&#30340;&#34920;&#29616;&#37117;&#33021;&#22312;&#38271;&#26102;&#38388;&#20869;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use the lens of weak signal asymptotics to study a class of sequentially randomized experiments, including those that arise in solving multi-armed bandit problems. In an experiment with $n$ time steps, we let the mean reward gaps between actions scale to the order $1/\sqrt{n}$ so as to preserve the difficulty of the learning task as $n$ grows. In this regime, we show that the sample paths of a class of sequentially randomized experiments -- adapted to this scaling regime and with arm selection probabilities that vary continuously with state -- converge weakly to a diffusion limit, given as the solution to a stochastic differential equation. The diffusion limit enables us to derive refined, instance-specific characterization of stochastic dynamics, and to obtain several insights on the regret and belief evolution of a number of sequential experiments including Thompson sampling (but not UCB, which does not satisfy our continuity assumption). We show that all sequential experiments wh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#38543;&#26426;&#27169;&#25311;&#22120;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#30340;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#24314;&#27169;&#12289;&#20272;&#35745;&#21644;&#27169;&#25311;&#20855;&#26377;&#19968;&#33324;&#38750;&#24179;&#31283;&#21644;&#22810;&#32500;&#38543;&#26426;&#21040;&#36798;&#36895;&#29575;&#30340;&#21040;&#36798;&#36807;&#31243;&#65292;&#24182;&#22312;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#21644;&#33322;&#31354;&#20132;&#36890;&#24314;&#27169;&#21644;&#20223;&#30495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2012.13940</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#24212;&#29992;&#20110;&#21040;&#36798;&#27169;&#25311;&#21644;&#24314;&#27169;&#30340;&#21452;&#37325;&#38543;&#26426;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Doubly Stochastic Simulator with Applications in Arrivals Modeling and Simulation. (arXiv:2012.13940v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.13940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#38543;&#26426;&#27169;&#25311;&#22120;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#30340;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#24314;&#27169;&#12289;&#20272;&#35745;&#21644;&#27169;&#25311;&#20855;&#26377;&#19968;&#33324;&#38750;&#24179;&#31283;&#21644;&#22810;&#32500;&#38543;&#26426;&#21040;&#36798;&#36895;&#29575;&#30340;&#21040;&#36798;&#36807;&#31243;&#65292;&#24182;&#22312;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#21644;&#33322;&#31354;&#20132;&#36890;&#24314;&#27169;&#21644;&#20223;&#30495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#32463;&#20856;&#30340;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#22120;&#21644;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#27169;&#25311;&#12289;&#20272;&#35745;&#21644;&#27169;&#25311;&#20855;&#26377;&#19968;&#33324;&#38750;&#24179;&#31283;&#21644;&#22810;&#32500;&#38543;&#26426;&#21040;&#36798;&#36895;&#29575;&#30340;&#24191;&#27867;&#21040;&#36798;&#36807;&#31243;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#38543;&#26426;&#27169;&#25311;&#22120;&#65292;&#23427;&#38598;&#25104;&#20102;&#38543;&#26426;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21644;&#32463;&#20856;&#30340;&#33945;&#29305;&#21345;&#32599;&#27850;&#26494;&#27169;&#25311;&#22120;&#65292;&#21033;&#29992;&#20102;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#21644;&#33322;&#31354;&#20132;&#36890;&#24314;&#27169;&#21644;&#20223;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework that integrates classical Monte Carlo simulators and Wasserstein generative adversarial networks to model, estimate, and simulate a broad class of arrival processes with general non-stationary and multi-dimensional random arrival rates. Classical Monte Carlo simulators have advantages at capturing the interpretable "physics" of a stochastic object, whereas neural-network-based simulators have advantages at capturing less-interpretable complicated dependence within a high-dimensional distribution. We propose a doubly stochastic simulator that integrates a stochastic generative neural network and a classical Monte Carlo Poisson simulator, to utilize both advantages. Such integration brings challenges to both theoretical reliability and computational tractability for the estimation of the simulator given real data, where the estimation is done through minimizing the Wasserstein distance between the distribution of the simulation output and the distribution of real d
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26412;&#22320;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#32423;&#21035;&#19978;&#23454;&#29616;&#20195;&#29702;&#32452;&#21512;&#24615;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#36890;&#36807;&#29420;&#31435;&#30340;&#21487;&#37325;&#29992;&#32452;&#20214;&#20801;&#35768;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#23450;&#20041;RL&#20195;&#29702;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#26032;&#39062;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2007.02622</link><description>&lt;p&gt;
&#39640;&#24230;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#24211;&#20013;&#20998;&#24067;&#24335;&#26550;&#26500;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Integrating Distributed Architectures in Highly Modular RL Libraries. (arXiv:2007.02622v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.02622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26412;&#22320;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#32423;&#21035;&#19978;&#23454;&#29616;&#20195;&#29702;&#32452;&#21512;&#24615;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#36890;&#36807;&#29420;&#31435;&#30340;&#21487;&#37325;&#29992;&#32452;&#20214;&#20801;&#35768;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#23450;&#20041;RL&#20195;&#29702;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#26032;&#39062;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38656;&#35201;&#20855;&#26377;&#36275;&#22815;&#28789;&#27963;&#24615;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#36731;&#26494;&#22320;&#21407;&#22411;&#21270;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#36991;&#20813;&#19981;&#20999;&#23454;&#38469;&#30340;&#23454;&#39564;&#21608;&#36716;&#26102;&#38388;&#12290;&#20026;&#20102;&#21305;&#37197;&#31532;&#19968;&#20010;&#35201;&#27714;&#65292;&#26368;&#27969;&#34892;&#30340;RL&#24211;&#20513;&#23548;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#20195;&#29702;&#32452;&#21512;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#23454;&#39564;&#21644;&#24320;&#21457;&#12290;&#20026;&#20102;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#23558;RL&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#37319;&#26679;&#21644;&#35745;&#31639;&#36164;&#28304;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#36804;&#20170;&#20026;&#27490;&#24456;&#38590;&#19982;&#27169;&#22359;&#21270;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#20801;&#35768;&#22312;&#26412;&#22320;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#32423;&#21035;&#19978;&#23454;&#29616;&#20195;&#29702;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#29420;&#31435;&#30340;&#21487;&#37325;&#29992;&#32452;&#20214;&#20801;&#35768;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#23450;&#20041;RL&#20195;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#22797;&#21046;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#32034;&#22810;&#20010;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#26032;&#39062;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancing reinforcement learning (RL) requires tools that are flexible enough to easily prototype new methods while avoiding impractically slow experimental turnaround times. To match the first requirement, the most popular RL libraries advocate for highly modular agent composability, which facilitates experimentation and development. To solve challenging environments within reasonable time frames, scaling RL to large sampling and computing resources has proved a successful strategy. However, this capability has been so far difficult to combine with modularity. In this work, we explore design choices to allow agent composability both at a local and distributed level of execution. We propose a versatile approach that allows the definition of RL agents at different scales through independent reusable components. We demonstrate experimentally that our design choices allow us to reproduce classical benchmarks, explore multiple distributed architectures, and solve novel and complex environm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#19981;&#21516;&#31181;&#31867;&#30340;&#22270;&#20687;&#36716;&#25442;&#21518;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#26159;&#21542;&#20419;&#36827;&#20102;&#36328;&#31867;&#21035;&#30340;&#22270;&#20687;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36328;&#31867;&#21035;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#34920;&#31034;&#25152;&#20419;&#36827;&#30340;&#65292;&#25903;&#25345;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#24615;&#39537;&#21160;&#32780;&#19981;&#26159;&#19987;&#38376;&#30340;&#23376;&#32593;&#32476;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2007.00112</link><description>&lt;p&gt;
&#36328;&#31867;&#21035;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#65306;&#40065;&#26834;&#24615;&#26159;&#21542;&#30001;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#39537;&#21160;&#65311;
&lt;/p&gt;
&lt;p&gt;
Robustness to Transformations Across Categories: Is Robustness To Transformations Driven by Invariant Neural Representations?. (arXiv:2007.00112v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.00112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#19981;&#21516;&#31181;&#31867;&#30340;&#22270;&#20687;&#36716;&#25442;&#21518;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#26159;&#21542;&#20419;&#36827;&#20102;&#36328;&#31867;&#21035;&#30340;&#22270;&#20687;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36328;&#31867;&#21035;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#34920;&#31034;&#25152;&#20419;&#36827;&#30340;&#65292;&#25903;&#25345;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#24615;&#39537;&#21160;&#32780;&#19981;&#26159;&#19987;&#38376;&#30340;&#23376;&#32593;&#32476;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;&#35782;&#21035;&#29289;&#20307;&#22312;&#21464;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65288;&#20363;&#22914;&#27169;&#31946;&#25110;&#22122;&#38899;&#65289;&#65292;&#24403;&#36825;&#20123;&#21464;&#25442;&#34987;&#21253;&#21547;&#22312;&#35757;&#32451;&#38598;&#20013;&#26102;&#12290;&#19968;&#20010;&#35299;&#37322;&#36825;&#31181;&#40065;&#26834;&#24615;&#30340;&#20551;&#35774;&#26159;&#65292;DCNNs&#21457;&#23637;&#20986;&#30340;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#22312;&#22270;&#20687;&#36716;&#25442;&#26102;&#19981;&#21457;&#29983;&#25913;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20551;&#35774;&#30340;&#30495;&#23454;&#31243;&#24230;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#40065;&#26834;&#24615;&#21487;&#33021;&#26159;&#36890;&#36807;&#19982;&#19981;&#21464;&#24615;&#19981;&#21516;&#30340;&#29305;&#24615;&#23454;&#29616;&#30340;&#65292;&#20363;&#22914;&#65292;&#32593;&#32476;&#30340;&#26576;&#20123;&#37096;&#20998;&#21487;&#33021;&#19987;&#38376;&#29992;&#20110;&#35782;&#21035;&#36716;&#25442;&#25110;&#38750;&#36716;&#25442;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#20419;&#36827;&#23545;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#20102;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#20986;&#29616;&#30340;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#35813;&#35757;&#32451;&#33539;&#24335;&#20013;&#65292;&#21482;&#26377;&#19968;&#20123;&#23545;&#35937;&#31867;&#21035;&#22312;&#35757;&#32451;&#26399;&#38388;&#34987;&#21464;&#25442;&#65292;&#28982;&#21518;&#35780;&#20215;DCNN&#26159;&#21542;&#23545;&#25152;&#26377;&#31867;&#21035;&#30340;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#37027;&#20123;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36328;&#31867;&#21035;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#34920;&#31034;&#25152;&#20419;&#36827;&#30340;&#65292;&#25903;&#25345;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#24615;&#39537;&#21160;&#32780;&#19981;&#26159;&#19987;&#38376;&#30340;&#23376;&#32593;&#32476;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (eg. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. However, to what extent this hypothesis holds true is an outstanding question, as robustness to transformations could be achieved with properties different from invariance, eg. parts of the network could be specialized to recognize either transformed or non-transformed images. This paper investigates the conditions under which invariant neural representations emerge by leveraging that they facilitate robustness to transformations beyond the training distribution. Concretely, we analyze a training paradigm in which only some object categories are seen transformed during training and evaluate whether the DCNN is robust to transformations a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;&#19968;&#31181;&#22522;&#20110;GRASP&#30340;&#26368;&#23567;&#24310;&#36831;&#38382;&#39064;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#21305;&#37197;&#25110;&#20248;&#20110;&#35299;&#30340;&#36136;&#37327;&#65292;&#22312;&#22823;&#22823;&#32553;&#30701;&#35745;&#31639;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#36824;&#25104;&#21151;&#22320;&#24341;&#20837;&#20102;88&#20010;&#26032;&#30340;&#35299;&#25104;&#26412;&#20540;&#12290;</title><link>http://arxiv.org/abs/1908.10705</link><description>&lt;p&gt;
&#36816;&#29992;&#25968;&#25454;&#25366;&#25496;&#25913;&#36827;&#26368;&#23567;&#24310;&#36831;&#38382;&#39064;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving a State-of-the-Art Heuristic for the Minimum Latency Problem with Data Mining. (arXiv:1908.10705v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1908.10705
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;&#19968;&#31181;&#22522;&#20110;GRASP&#30340;&#26368;&#23567;&#24310;&#36831;&#38382;&#39064;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#21305;&#37197;&#25110;&#20248;&#20110;&#35299;&#30340;&#36136;&#37327;&#65292;&#22312;&#22823;&#22823;&#32553;&#30701;&#35745;&#31639;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#36824;&#25104;&#21151;&#22320;&#24341;&#20837;&#20102;88&#20010;&#26032;&#30340;&#35299;&#25104;&#26412;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#36816;&#31609;&#23398;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20854;&#20013;&#19968;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#26159;&#23558;&#36138;&#24515;&#38543;&#26426;&#33258;&#36866;&#24212;&#25628;&#32034;&#31243;&#24207;&#65288;GRASP&#65289;&#19982;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#39640;&#36136;&#37327;&#35299;&#20013;&#21457;&#29616;&#30340;&#39057;&#32321;&#27169;&#24335;&#65292;&#22312;&#20445;&#35777;&#25628;&#32034;&#33539;&#22260;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#26412;&#25991;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#25913;&#36827;&#20102;&#19968;&#20010;&#22522;&#20110;GRASP&#30340;&#26368;&#23567;&#24310;&#36831;&#38382;&#39064;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#20004;&#20010;&#38382;&#39064;&#21464;&#20307;&#12290;&#35745;&#31639;&#23454;&#39564;&#35777;&#26126;&#65292;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#33021;&#22815;&#22312;&#36739;&#22823;&#25968;&#37327;&#30340;&#23454;&#20363;&#19978;&#21305;&#37197;&#25110;&#25913;&#21892;&#35299;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;88&#20010;&#26032;&#30340;&#35299;&#25104;&#26412;&#20540;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32479;&#35745;&#26174;&#33879;&#24615;&#26816;&#39564;&#12289;&#25366;&#25496;&#27169;&#24335;&#30340;&#24433;&#21709;&#12289;&#31561;&#26102;&#38388;&#27604;&#36739;&#21644;&#26102;&#38388;&#21040;&#30446;&#26631;&#26354;&#32447;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, hybrid metaheuristics have become a trend in operations research. A successful example combines the Greedy Randomized Adaptive Search Procedures (GRASP) and data mining techniques, where frequent patterns found in high-quality solutions can lead to an efficient exploration of the search space, along with a significant reduction of computational time. In this work, a GRASP-based state-of-the-art heuristic for the Minimum Latency Problem (MLP) is improved by means of data mining techniques for two MLP variants. Computational experiments showed that the approaches with data mining were able to match or improve the solution quality for a large number of instances, together with a substantial reduction of running time. In addition, 88 new cost values of solutions are introduced into the literature. To support our results, tests of statistical significance, impact of using mined patterns, equal time comparisons and time-to-target plots are provided.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#30452;&#25509;&#27169;&#25311;&#34920;&#38754;&#30340;&#25968;&#25454;&#65292;&#22312;&#36816;&#34892;&#26102;&#20026;&#27169;&#22411;&#25552;&#20379;&#32570;&#22833;&#30340;&#34920;&#38754;&#20449;&#24687;&#65292;&#36825;&#23558;&#26377;&#25928;&#20419;&#36827;&#31561;&#31163;&#23376;&#20307;&#22788;&#29702;&#39046;&#22495;&#30340;&#24320;&#23637;&#12290;</title><link>http://arxiv.org/abs/1810.04510</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31561;&#31163;&#23376;&#20307;&#34920;&#38754;&#30028;&#38754;&#32806;&#21512;&#28293;&#23556;&#21644;&#27668;&#30456;&#20256;&#36755;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Machine learning plasma-surface interface for coupling sputtering and gas-phase transport simulations. (arXiv:1810.04510v1 [physics.plasm-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1810.04510
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#30452;&#25509;&#27169;&#25311;&#34920;&#38754;&#30340;&#25968;&#25454;&#65292;&#22312;&#36816;&#34892;&#26102;&#20026;&#27169;&#22411;&#25552;&#20379;&#32570;&#22833;&#30340;&#34920;&#38754;&#20449;&#24687;&#65292;&#36825;&#23558;&#26377;&#25928;&#20419;&#36827;&#31561;&#31163;&#23376;&#20307;&#22788;&#29702;&#39046;&#22495;&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#28293;&#23556;&#27785;&#31215;&#20316;&#34180;&#33180;&#30340;&#22788;&#29702;&#65292;&#26412;&#36136;&#19978;&#21462;&#20915;&#20110;&#39640;&#33021;&#31890;&#23376;&#19982;&#30446;&#26631;&#34920;&#38754;&#30340;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#38543;&#21518;&#30340;&#31890;&#23376;&#20256;&#36755;&#12290;&#24213;&#23618;&#29289;&#29702;&#29616;&#35937;&#30340;&#38271;&#24230;&#21644;&#26102;&#38388;&#23610;&#24230;&#27178;&#36328;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#19981;&#21487;&#33021;&#23454;&#29616;&#36328;&#36234;&#25152;&#26377;&#26102;&#38388;&#21644;&#38271;&#24230;&#23610;&#24230;&#30340;&#29702;&#35770;&#25551;&#36848;&#12290;&#21487;&#20197;&#29305;&#21035;&#20174;&#22522;&#26412;&#34920;&#38754;&#21644;&#31561;&#31163;&#23376;&#20307;&#36807;&#31243;&#30340;&#33391;&#22909;&#20998;&#31163;&#30340;&#26102;&#38388;&#23610;&#24230;&#20013;&#33719;&#24471;&#20248;&#21183;&#12290;&#21487;&#20197;&#20174;&#34920;&#38754;&#27169;&#22411;&#20013;&#35745;&#31639;&#34920;&#38754;&#29305;&#24615;&#65292;&#24182;&#23558;&#20854;&#23384;&#20648;&#20110;&#33509;&#24178;&#20856;&#22411;&#24773;&#20917;&#19979;&#12290;&#38543;&#21518;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#27169;&#22411;&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#35299;&#26512;&#34920;&#36798;&#24335;&#25110;&#26597;&#25214;&#34920;&#65289;&#23558;&#34920;&#38754;&#25968;&#25454;&#25552;&#20379;&#32473;&#27668;&#30456;&#20256;&#36755;&#27169;&#25311;&#24182;&#29992;&#20110;&#23450;&#20041;&#25554;&#20837;&#36793;&#30028;&#26465;&#20214;&#12290;&#20294;&#26159;&#65292;&#22312;&#36816;&#34892;&#26102;&#35780;&#20272;&#20013;&#65292;&#32500;&#25252;&#30340;&#34920;&#38754;&#25968;&#25454;&#21487;&#33021;&#35777;&#26126;&#19981;&#36275;&#22815;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#36890;&#36807;&#25554;&#20540;&#65288;&#24120;&#35265;&#65289;&#12289;&#22806;&#25512;&#65288;&#22312;&#32570;&#20047;&#26356;&#22909;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65289;&#25110;&#21333;&#29420;&#35843;&#29992;&#34920;&#38754;&#30452;&#25509;&#27169;&#25311; (DSM) &#27169;&#22411;&#26469;&#33719;&#21462;&#32570;&#22833;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174; DSM &#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#22312;&#36816;&#34892;&#26102;&#20026;&#27169;&#22411;&#25552;&#20379;&#32570;&#22833;&#30340;&#34920;&#38754;&#20449;&#24687;&#12290;&#25152;&#24471;&#21040;&#30340;&#32452;&#21512;&#27169;&#22411;&#22312;&#21508;&#31181;&#38774;&#26448;&#26009;&#21644;&#24816;&#24615;&#27668;&#20307;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35752;&#35770;&#20102;&#23558;&#35813;&#26041;&#27861;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#26448;&#26009;&#21644;&#27668;&#20307;&#33539;&#22260;&#20869;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thin film processing by means of sputter deposition inherently depends on the interaction of energetic particles with a target surface and the subsequent particle transport. The length and time scales of the underlying physical phenomena span orders of magnitudes. A theoretical description which bridges all time and length scales is not practically possible. Advantage can be taken particularly from the well-separated time scales of the fundamental surface and plasma processes. Initially, surface properties may be calculated from a surface model and stored for a number of representative cases. Subsequently, the surface data may be provided to gas-phase transport simulations via appropriate model interfaces (e.g., analytic expressions or look-up tables) and utilized to define insertion boundary conditions. During run-time evaluation, however, the maintained surface data may prove to be not sufficient. In this case, missing data may be obtained by interpolation (common), extrapolation (in
&lt;/p&gt;</description></item></channel></rss>