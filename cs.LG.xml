<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22270;&#37325;&#20889;&#20316;&#20026;&#24314;&#27169;&#21644;&#20998;&#26512;&#22797;&#26434;&#22270;&#24418;&#36716;&#25442;&#30340;&#24037;&#20855;&#65292;&#21516;&#26102;&#23558;GNN&#34920;&#31034;&#20026;&#22270;&#37325;&#20889;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#21644;&#20998;&#26512;GNN&#30340;&#26550;&#26500;&#21644;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18632</link><description>&lt;p&gt;
&#22270;&#37325;&#20889;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Rewriting for Graph Neural Networks. (arXiv:2305.18632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22270;&#37325;&#20889;&#20316;&#20026;&#24314;&#27169;&#21644;&#20998;&#26512;&#22797;&#26434;&#22270;&#24418;&#36716;&#25442;&#30340;&#24037;&#20855;&#65292;&#21516;&#26102;&#23558;GNN&#34920;&#31034;&#20026;&#22270;&#37325;&#20889;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#21644;&#20998;&#26512;GNN&#30340;&#26550;&#26500;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25903;&#25345;&#23545;&#33410;&#28857;&#12289;&#36793;&#32536;&#12289;&#23646;&#24615;&#25110;&#22270;&#23646;&#24615;&#36827;&#34892;&#25512;&#29702;&#12290;&#22270;&#37325;&#20889;&#30740;&#31350;&#22522;&#20110;&#35268;&#21017;&#30340;&#22270;&#24418;&#25805;&#20316;&#65292;&#20197;&#24314;&#27169;&#22797;&#26434;&#30340;&#22270;&#24418;&#36716;&#25442;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#65288;i&#65289;&#22270;&#37325;&#20889;&#21253;&#21547;GNN&#24182;&#21487;&#20197;&#20316;&#20026;&#30740;&#31350;&#21644;&#27604;&#36739;&#23427;&#20204;&#30340;&#27491;&#24335;&#27169;&#22411;&#65292;&#65288;ii&#65289;&#23558;GNN&#34920;&#31034;&#20026;&#22270;&#37325;&#20889;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#21644;&#20998;&#26512;GNN&#12289;&#23427;&#20204;&#30340;&#26550;&#26500;&#21644;&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;Graph Rewriting Neural Networks (GReNN)&#20316;&#20026;GNN&#30340;&#26032;&#35821;&#20041;&#22522;&#30784;&#21644;&#24037;&#31243;&#23398;&#31185;&#12290;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#31867;&#20284;&#20110;&#20197;Groove&#22270;&#24418;&#37325;&#20889;&#27169;&#22411;&#23454;&#29616;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#23545;&#21160;&#24577;&#26356;&#26032;&#30340;&#22686;&#37327;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given graphs as input, Graph Neural Networks (GNNs) support the inference of nodes, edges, attributes, or graph properties. Graph Rewriting investigates the rule-based manipulation of graphs to model complex graph transformations. We propose that, therefore, (i) graph rewriting subsumes GNNs and could serve as formal model to study and compare them, and (ii) the representation of GNNs as graph rewrite systems can help to design and analyse GNNs, their architectures and algorithms. Hence we propose Graph Rewriting Neural Networks (GReNN) as both novel semantic foundation and engineering discipline for GNNs. We develop a case study reminiscent of a Message Passing Neural Network realised as a Groove graph rewriting model and explore its incremental operation in response to dynamic updates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#26292;&#38632;&#25511;&#21046;&#31574;&#30053;&#24182;&#20272;&#35745;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#26292;&#38632;&#32593;&#32476;&#20013;&#30340;&#25511;&#21046;&#36164;&#20135;&#65292;&#35843;&#25972;&#26292;&#38632;&#32593;&#32476;&#34892;&#20026;&#65292;&#20943;&#23569;&#22478;&#24066;&#27946;&#28061;&#39118;&#38505;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.18630</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26292;&#38632;&#25511;&#21046;&#31574;&#30053;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Identification of stormwater control strategies and their associated uncertainties using Bayesian Optimization. (arXiv:2305.18630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#26292;&#38632;&#25511;&#21046;&#31574;&#30053;&#24182;&#20272;&#35745;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#26292;&#38632;&#32593;&#32476;&#20013;&#30340;&#25511;&#21046;&#36164;&#20135;&#65292;&#35843;&#25972;&#26292;&#38632;&#32593;&#32476;&#34892;&#20026;&#65292;&#20943;&#23569;&#22478;&#24066;&#27946;&#28061;&#39118;&#38505;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#25511;&#21046;&#36234;&#26469;&#36234;&#25104;&#20026;&#24212;&#23545;&#24555;&#36895;&#28436;&#21464;&#30340;&#22825;&#27668;&#27169;&#24335;&#23545;&#26292;&#38632;&#31995;&#32479;&#25805;&#20316;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#38477;&#38632;&#39044;&#27979;&#21644;&#23454;&#26102;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#21487;&#20197;&#21160;&#24577;&#37197;&#32622;&#26292;&#38632;&#32593;&#32476;&#20013;&#30340;&#25511;&#21046;&#36164;&#20135;&#65292;&#35843;&#25972;&#26292;&#38632;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20943;&#23569;&#22478;&#24066;&#27946;&#28061;&#30340;&#39118;&#38505;&#65292;&#23558;&#27969;&#37327;&#22343;&#34913;&#21040;&#29992;&#27700;&#20877;&#29983;&#35774;&#26045;&#65292;&#24182;&#20445;&#25252;&#25509;&#25910;&#27700;&#20307;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#26679;&#30340;&#25511;&#21046;&#31574;&#30053;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#19988;&#36824;&#19981;&#23384;&#22312;&#19968;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#23454;&#26045;&#36825;&#20123;&#25511;&#21046;&#31574;&#30053;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#26292;&#38632;&#25511;&#21046;&#31574;&#30053;&#24182;&#20272;&#35745;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#35782;&#21035;&#23454;&#38469;&#20013;&#21487;&#34892;&#30340;&#25511;&#21046;&#31574;&#30053;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic control is emerging as an effective methodology for operating stormwater systems under stress from rapidly evolving weather patterns. Informed by rainfall predictions and real-time sensor measurements, control assets in the stormwater network can be dynamically configured to tune the behavior of the stormwater network to reduce the risk of urban flooding, equalize flows to the water reclamation facilities, and protect the receiving water bodies. However, developing such control strategies requires significant human and computational resources, and a methodology does not yet exist for quantifying the risks associated with implementing these control strategies. To address these challenges, in this paper, we introduce a Bayesian Optimization-based approach for identifying stormwater control strategies and estimating the associated uncertainties. We evaluate the efficacy of this approach in identifying viable control strategies in a simulated environment on real-world inspired comb
&lt;/p&gt;</description></item><item><title>Global-QSGD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#32553;&#25918;&#37327;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35823;&#24046;&#21453;&#39304;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#36798;$O(\ sqrt{n})$&#30340;&#39069;&#22806;&#21387;&#32553;&#27604;&#12290;</title><link>http://arxiv.org/abs/2305.18627</link><description>&lt;p&gt;
&#20840;&#23616;&#32553;&#25918;&#37327;&#21270;&#65306;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#23454;&#29992;&#30340;&#26080;&#28014;&#28857;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees. (arXiv:2305.18627v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18627
&lt;/p&gt;
&lt;p&gt;
Global-QSGD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#32553;&#25918;&#37327;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35823;&#24046;&#21453;&#39304;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#36798;$O(\ sqrt{n})$&#30340;&#39069;&#22806;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26159;&#25512;&#21160;&#28145;&#24230;&#23398;&#20064;&#36817;&#26399;&#36827;&#23637;&#30340;&#20027;&#35201;&#39537;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#20449;&#24120;&#24120;&#26159;&#31995;&#32479;&#30340;&#20027;&#35201;&#29942;&#39048;&#24182;&#20855;&#26377;&#39640;&#26114;&#30340;&#20195;&#20215;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#35774;&#35745;&#39640;&#25928;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#26082;&#33021;&#22312;&#32463;&#39564;&#19978;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21448;&#33021;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20840;&#23616;-QSGD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#21270;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#20840;&#23616;&#32553;&#25918;&#35774;&#35745;&#26469;&#21152;&#36895;&#22522;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;Global-QSGD&#26159;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#20005;&#26684;&#30340;Allreduce&#20860;&#23481;&#21387;&#32553;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#21387;&#32553;&#35823;&#24046;&#21644;&#36890;&#20449;&#33410;&#30465;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#26469;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#21152;&#36895;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#26080;&#20559;&#24615;&#65292;Global-QSGD&#19981;&#20381;&#36182;&#26114;&#36149;&#30340;&#35823;&#24046;&#21453;&#39304;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#27969;&#34892;&#30340;QSGD&#37327;&#21270;&#33021;&#25552;&#20379;&#39640;&#36798;$O(\sqrt{n})$ &#30340;&#39069;&#22806;&#21387;&#32553;&#27604;&#65288;&#20854;&#20013;$n$&#34920;&#31034;&#24037;&#20316;&#32773;&#30340;&#25968;&#37327;&#65289;&#12290;&#20026;&#20102;&#33719;&#24471;&#29702;&#35770;&#20445;&#35777;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20449;&#24687;&#35770;&#21644;&#20984;&#20998;&#26512;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient distributed training is a principal driver of recent advances in deep learning. However, communication often proves costly and becomes the primary bottleneck in these systems. As a result, there is a demand for the design of efficient communication mechanisms that can empirically boost throughput while providing theoretical guarantees. In this work, we introduce Global-QSGD, a novel family of quantization operators, engineered to accelerate distributed training based on global scaling. We demonstrate that Global-QSGD is the first theoretically rigorous Allreduce-compatible compression mechanism that achieves a provable speed-up by striking a balance between compression error and communication savings. Importantly, Global-QSGD does not rely on costly error feedback due to its inherent unbiasedness and offers up to $O(\sqrt{n})$ additional compression ratio compared to the popular QSGD quantization ($n$ represents the number of workers). To obtain theoretical guarantees, we gen
&lt;/p&gt;</description></item><item><title>W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18624</link><description>&lt;p&gt;
W-procer: &#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18624
&lt;/p&gt;
&lt;p&gt;
W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#37197;&#32622;&#21147;&#27714;&#20943;&#23569;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#34987;&#27880;&#37322;&#20026;&#8220;O&#8221;&#65288;&#21363;&#8220;OUTSIDE&#8221;&#65289;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#24076;&#26395;&#34987;&#25512;&#31163;&#21040;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20026;&#8220;O&#8221;&#20197;&#22806;&#30340;&#20854;&#20182;&#23454;&#20307;&#65292;&#36825;&#31181;&#35774;&#23450;&#25928;&#26524;&#19981;&#20339;&#65292;&#21487;&#33021;&#20250;&#24471;&#20986;&#21547;&#26377;&#22122;&#22768;&#21407;&#22411;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#8220;O&#8221;&#26631;&#31614;&#23454;&#20307;&#19982;&#26377;&#26631;&#31614;&#23454;&#20307;&#30456;&#20851;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;W-PROCER&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#23637;&#24320;&#12290;&#36825;&#20123;&#32452;&#20214;&#22312;&#21327;&#21161;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;W-PROCER&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
&lt;/p&gt;</description></item><item><title>Alfred&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#31034;&#21019;&#24314;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30001;&#19987;&#23478;&#32534;&#20889;&#30340;&#31243;&#24207;&#24335;&#24369;&#30417;&#30563;(PWS)&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20026;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#20027;&#39064;&#19987;&#19994;&#30693;&#35782;&#12290;Alfred&#20026;&#36825;&#31181;&#26032;&#20852;&#33539;&#24335;&#30340;&#20851;&#38190;&#27493;&#39588;&#25552;&#20379;&#31616;&#21333;&#30340;Python&#25509;&#21475;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#26631;&#27880;&#30340;&#39640;&#21534;&#21520;&#37327;&#21518;&#31471;&#12290;</title><link>http://arxiv.org/abs/2305.18623</link><description>&lt;p&gt;
Alfred: &#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Alfred: A System for Prompted Weak Supervision. (arXiv:2305.18623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18623
&lt;/p&gt;
&lt;p&gt;
Alfred&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#31034;&#21019;&#24314;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30001;&#19987;&#23478;&#32534;&#20889;&#30340;&#31243;&#24207;&#24335;&#24369;&#30417;&#30563;(PWS)&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20026;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#20027;&#39064;&#19987;&#19994;&#30693;&#35782;&#12290;Alfred&#20026;&#36825;&#31181;&#26032;&#20852;&#33539;&#24335;&#30340;&#20851;&#38190;&#27493;&#39588;&#25552;&#20379;&#31616;&#21333;&#30340;Python&#25509;&#21475;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#26631;&#27880;&#30340;&#39640;&#21534;&#21520;&#37327;&#21518;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Alfred&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#25552;&#31034;&#21019;&#24314;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#31243;&#24207;&#24335;&#24369;&#30417;&#30563;(PWS)&#31995;&#32479;&#12290;&#19982;&#20856;&#22411;&#30340;PWS&#31995;&#32479;&#19981;&#21516;&#65292;&#20854;&#20013;&#24369;&#30417;&#30563;&#28304;&#30001;&#19987;&#23478;&#32534;&#20889;&#30340;&#31243;&#24207;&#65292;Alfred&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20026;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#20027;&#39064;&#19987;&#19994;&#30693;&#35782;&#12290;Alfred&#20026;&#36825;&#31181;&#26032;&#20852;&#33539;&#24335;&#30340;&#20851;&#38190;&#27493;&#39588;&#25552;&#20379;&#31616;&#21333;&#30340;Python&#25509;&#21475;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#26631;&#27880;&#30340;&#39640;&#21534;&#21520;&#37327;&#21518;&#31471;&#12290;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#21019;&#24314;&#12289;&#35780;&#20272;&#21644;&#23436;&#21892;&#22522;&#20110;&#25552;&#31034;&#30340;&#24369;&#30417;&#30563;&#26469;&#28304;&#65307;&#23558;&#32467;&#26524;&#26144;&#23556;&#21040;&#24369;&#26631;&#31614;&#65307;&#24182;&#20351;&#29992;&#26631;&#31614;&#27169;&#22411;&#35299;&#20915;&#19981;&#21516;&#24847;&#35265;&#12290;Alfred&#25903;&#25345;&#34987;&#33258;&#31649;&#29702;&#30340;&#35745;&#31639;&#38598;&#32676;&#25552;&#20379;&#30340;&#27169;&#22411;&#26381;&#21153;&#65292;&#23454;&#29616;&#26080;&#32541;&#30340;&#26412;&#22320;&#24320;&#21457;&#20307;&#39564;&#12290;&#23427;&#20351;&#29992;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#26426;&#21046;&#33258;&#21160;&#20248;&#21270;&#25552;&#31034;&#30340;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#31616;&#21333;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#20248;&#21270;&#21487;&#20197;&#23558;&#26597;&#35810;&#21534;&#21520;&#37327;&#25552;&#39640;2.9&#20493;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data labeling. Users can quickly create, evaluate, and refine their prompt-based weak supervision sources; map the results to weak labels; and resolve their disagreements with a label model. Alfred enables a seamless local development experience backed by models served from self-managed computing clusters. It automatically optimizes the execution of prompts with optimized batching mechanisms. We find that this optimization improves query throughput by 2.9x versus a naive approach. We present two example
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPA&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#21160;&#24577;&#22270;&#20013;&#21363;&#26102;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18622</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#22823;&#22411;&#21160;&#24577;&#22270;&#30340;&#21363;&#26102;&#34920;&#31034;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Instant Representation Learning for Recommendation over Large Dynamic Graphs. (arXiv:2305.18622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18622
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPA&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#21160;&#24577;&#22270;&#20013;&#21363;&#26102;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#20102;&#35299;&#29992;&#25143;&#20559;&#22909;&#65292;&#32780;&#29616;&#20195;&#25512;&#33616;&#27169;&#22411;&#24320;&#22987;&#21033;&#29992;&#29992;&#25143;&#34920;&#29616;&#20986;&#30340;&#21508;&#31181;&#34892;&#20026;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#34892;&#20026;&#22270;&#19981;&#20165;&#26159;&#22810;&#37325;&#30340;&#65292;&#32780;&#19988;&#26159;&#21160;&#24577;&#30340;&#65292;&#21363;&#22270;&#38543;&#26102;&#38388;&#24555;&#36895;&#28436;&#21464;&#65292;&#28155;&#21152;&#25110;&#21024;&#38500;&#21508;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#32536;&#65292;&#36825;&#23548;&#33268;&#37051;&#22495;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#27969;&#21160;&#21160;&#21147;&#23398;&#65292;&#22240;&#27492;&#19968;&#26086;&#22270;&#24418;&#21457;&#29983;&#26174;&#30528;&#28436;&#21464;&#65292;&#23427;&#20204;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#37051;&#22495;&#25200;&#21160;&#20250;&#24694;&#21270;&#22522;&#20110;&#37051;&#23621;&#32858;&#21512;&#30340;&#22270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SUPA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21160;&#24577;&#22810;&#37325;&#24322;&#26500;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#19982;&#37051;&#23621;&#32858;&#21512;&#20307;&#31995;&#32467;&#26500;&#30456;&#27604;&#65292;SUPA&#24320;&#21457;&#20102;&#19968;&#20010;&#26102;&#38388;&#24863;&#30693;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#26469;&#33258;&#21508;&#31181;&#37051;&#23621;&#30340;&#33410;&#28857;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#19968;&#20010;&#21160;&#24577;&#22270;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#22270;&#32423;&#21160;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#20197;&#22312;&#32447;&#26041;&#24335;&#39640;&#25928;&#22320;&#29983;&#25104;&#19982;&#20854;&#21382;&#21490;&#34892;&#20026;&#30456;&#20851;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#12290;&#19977;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;SUPA&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are able to learn user preferences based on user and item representations via their historical behaviors. To improve representation learning, recent recommendation models start leveraging information from various behavior types exhibited by users. In real-world scenarios, the user behavioral graph is not only multiplex but also dynamic, i.e., the graph evolves rapidly over time, with various types of nodes and edges added or deleted, which causes the Neighborhood Disturbance. Nevertheless, most existing methods neglect such streaming dynamics and thus need to be retrained once the graph has significantly evolved, making them unsuitable in the online learning environment. Furthermore, the Neighborhood Disturbance existing in dynamic graphs deteriorates the performance of neighbor-aggregation based graph models. To this end, we propose SUPA, a novel graph neural network for dynamic multiplex heterogeneous graphs. Compared to neighbor-aggregation architecture, SUPA dev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20284;&#28982;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#12289;&#32553;&#25918;&#23450;&#24459;&#21644;&#22686;&#21152;&#35745;&#31639;&#65292;&#25104;&#21151;&#26500;&#24314;&#21644;&#21457;&#24067;&#20102;&#19968;&#20010;&#36229;&#36807;&#23567;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20248;&#20110;GPT-2 124M&#12290;</title><link>http://arxiv.org/abs/2305.18619</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Likelihood-Based Diffusion Language Models. (arXiv:2305.18619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20284;&#28982;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#12289;&#32553;&#25918;&#23450;&#24459;&#21644;&#22686;&#21152;&#35745;&#31639;&#65292;&#25104;&#21151;&#26500;&#24314;&#21644;&#21457;&#24067;&#20102;&#19968;&#20010;&#36229;&#36807;&#23567;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20248;&#20110;GPT-2 124M&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#20204;&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#23578;&#26410;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#26631;&#20934;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#33719;&#24471;&#38750;&#24494;&#19981;&#36275;&#36947;&#30340;&#20284;&#28982;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#21462;&#20102;&#25514;&#26045;&#26469;&#32553;&#23567;&#33258;&#22238;&#24402;&#21644;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20284;&#28982;&#24046;&#24322;&#65292;&#30446;&#26631;&#26159;&#26500;&#24314;&#21644;&#21457;&#24067;&#19968;&#20010;&#36229;&#36807;&#23567;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#12289;&#32553;&#25918;&#23450;&#24459;&#21644;&#22686;&#21152;&#35745;&#31639;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#22312;&#31639;&#27861;&#21069;&#27839;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#31181;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#35770;&#25913;&#36827;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#25193;&#25955;&#27169;&#22411;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#21457;&#29616;&#35745;&#31639;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#26696;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#24046;&#21035;&#24456;&#22823;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#32553;&#25918;&#20998;&#26512;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;Plaid 1B&#65292;&#19968;&#20010;&#22823;&#22411;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20284;&#28982;&#24230;&#21644;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#19978;&#20248;&#20110;GPT-2 124M&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#26469;&#22788;&#29702;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;NTS&#25968;&#25454;&#20013;&#30340;&#22270;&#21160;&#24577;&#21644;&#32570;&#22833;&#36793;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18612</link><description>&lt;p&gt;
&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders. (arXiv:2305.18612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#26469;&#22788;&#29702;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;NTS&#25968;&#25454;&#20013;&#30340;&#22270;&#21160;&#24577;&#21644;&#32570;&#22833;&#36793;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26159;&#36817;&#24180;&#26469;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#22823;&#31867;&#65292;&#21253;&#25324;&#65288;1&#65289;&#20027;&#35201;&#20851;&#27880;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#30340;&#28145;&#24230;&#36882;&#24402;&#25110;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;MTS&#22266;&#26377;&#22270;&#32467;&#26500;&#30340;&#25299;&#25169;&#20449;&#24687;&#20316;&#20026;&#25554;&#34917;&#30340;&#20851;&#31995;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#25299;&#25169;&#20449;&#24687;&#65292;&#35201;&#20040;&#20551;&#23450;&#22270;&#32467;&#26500;&#22266;&#23450;&#19988;&#20934;&#30830;&#24050;&#30693;&#12290;&#22240;&#27492;&#65292;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NTS&#65289;&#25968;&#25454;&#20013;&#65292;&#23427;&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22270;&#21160;&#24577;&#36827;&#34892;&#31934;&#30830;&#30340;&#25554;&#34917;&#65292;&#20854;&#20013;&#24213;&#23618;&#22270;&#19981;&#26029;&#21464;&#21270;&#24182;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#36793;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21253;&#21547;&#33410;&#28857;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20013;&#32570;&#22833;&#20540;&#30340;NTS&#25554;&#34917;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PGE-VAE&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#23450;&#20301;&#32534;&#30721;&#25216;&#26415;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#21512;&#24182;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#22270;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#21644;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22270;&#29983;&#25104;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#30340;&#36793;&#24182;&#36866;&#24212;&#22270;&#21160;&#24577;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#21644;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18599</link><description>&lt;p&gt;
&#25552;&#39640;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization for Multimodal Fake News Detection. (arXiv:2305.18599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#21644;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#19981;&#26029;&#20256;&#25773;&#21450;&#20854;&#24778;&#20154;&#30340;&#24433;&#21709;&#24050;&#32463;&#20419;&#20351;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#24320;&#21457;&#20986;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#35268;&#27169;&#36739;&#23567;&#25110;&#29305;&#23450;&#20027;&#39064;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37319;&#29992;&#24182;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25805;&#20316;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#26088;&#22312;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36328;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#21463;&#21040;&#25805;&#20316;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20943;&#23569;&#20559;&#24046;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#26356;&#26377;&#24847;&#20041;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing proliferation of misinformation and its alarming impact have motivated both industry and academia to develop approaches for fake news detection. However, state-of-the-art approaches are usually trained on datasets of smaller size or with a limited set of specific topics. As a consequence, these models lack generalization capabilities and are not applicable to real-world data. In this paper, we propose three models that adopt and fine-tune state-of-the-art multimodal transformers for multimodal fake news detection. We conduct an in-depth analysis by manipulating the input data aimed to explore models performance in realistic use cases on social media. Our study across multiple models demonstrates that these systems suffer significant performance drops against manipulated data. To reduce the bias and improve model generalization, we suggest training data augmentation to conduct more meaningful experiments for fake news detection on social media. The proposed data augmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25910;&#25947;&#20998;&#26512;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#28508;&#22312;&#30340;&#25238;&#21160;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#36827;&#19968;&#27493;&#32467;&#21512;&#22810;&#20010;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.18594</link><description>&lt;p&gt;
&#22522;&#20110;&#21327;&#20316;&#23398;&#20064;&#30340;&#20998;&#26512;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Analytic End-to-End Deep Learning Algorithm based on Collaborative Learning. (arXiv:2305.18594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25910;&#25947;&#20998;&#26512;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#28508;&#22312;&#30340;&#25238;&#21160;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#36827;&#19968;&#27493;&#32467;&#21512;&#22810;&#20010;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#25511;&#21046;&#24212;&#29992;&#20013;&#65292;&#31995;&#32479;&#30340;&#29702;&#35770;&#20998;&#26512;&#23545;&#30830;&#20445;&#31283;&#23450;&#24615;&#25110;&#25910;&#25947;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#21487;&#38752;&#30340;&#36816;&#34892;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#31995;&#32479;&#29702;&#35299;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#40657;&#30418;&#26041;&#27861;&#65292;&#26356;&#22810;&#22320;&#20851;&#27880;&#32463;&#39564;&#24615;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25910;&#25947;&#20998;&#26512;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#28508;&#22312;&#30340;&#25238;&#21160;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#19981;&#26131;&#23548;&#33268;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20004;&#23618;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#36827;&#19968;&#27493;&#32467;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most control applications, theoretical analysis of the systems is crucial in ensuring stability or convergence, so as to ensure safe and reliable operations and also to gain a better understanding of the systems for further developments. However, most current deep learning methods are black-box approaches that are more focused on empirical studies. Recently, some results have been obtained for convergence analysis of end-to end deep learning based on non-smooth ReLU activation functions, which may result in chattering for control tasks. This paper presents a convergence analysis for end-to-end deep learning of fully connected neural networks (FNN) with smooth activation functions. The proposed method therefore avoids any potential chattering problem, and it also does not easily lead to gradient vanishing problems. The proposed End-to-End algorithm trains multiple two-layer fully connected networks concurrently and collaborative learning can be used to further combine their strengths
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18593</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#24314;&#27169;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Diffusion Modeling for Anomaly Detection. (arXiv:2305.18593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#32780;&#38395;&#21517;&#65292;&#25104;&#20026;&#22522;&#20110;&#23494;&#24230;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#20505;&#36873;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#23588;&#20854;&#26159;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#22791;&#24456;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#36890;&#36807;&#31616;&#21270;DDPM&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24341;&#20986;&#21478;&#19968;&#31181;&#31216;&#20026;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65288;DTPM&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;DTPM&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#30340;&#25193;&#25955;&#26102;&#38388;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#27492;&#21518;&#39564;&#20998;&#24067;&#30340;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;ADBenh&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20020;&#24202;&#35774;&#32622;&#19979;&#37117;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.18592</link><description>&lt;p&gt;
12&#23548;&#32852;&#24515;&#30005;&#22270;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks Generalization and Fine-Tuning for 12-lead ECG Classification. (arXiv:2305.18592v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20020;&#24202;&#35774;&#32622;&#19979;&#37117;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#37117;&#33268;&#21147;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#35760;&#24405;&#26469;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#12290;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#22823;&#23567;&#21644;&#21442;&#25968;&#26041;&#38754;&#37117;&#26377;&#25152;&#19981;&#21516;&#65292;&#22914;&#24739;&#32773;&#20803;&#25968;&#25454;&#12289;&#26631;&#27880;ECG&#30340;&#21307;&#29983;&#25968;&#37327;&#12289;&#29992;&#20110;ECG&#35760;&#24405;&#30340;&#35774;&#22791;&#31867;&#22411;&#12289;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#31561;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#35757;&#32451;&#22312;&#19968;&#20010;ECG&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#36136;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#19968;&#23450;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#25110;&#20020;&#24202;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#36827;&#19968;&#27493;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#25913;&#21892;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#31169;&#20154;&#25968;&#25454;&#38598;TIS&#21644;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;PTB-XL&#19978;&#35757;&#32451;&#20102;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30456;&#36739;&#20110;&#20165;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32593;&#32476;&#24182;&#36827;&#34892;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#20165;&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#23601;&#36275;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies are aimed at diagnosing heart diseases based on 12-lead electrocardiographic (ECG) records using deep learning methods. These studies usually use specific datasets that differ in size and parameters, such as patient metadata, number of doctors annotating ECGs, types of devices for ECG recording, data preprocessing techniques, etc. It is well-known that high-quality deep neural networks trained on one ECG dataset do not necessarily perform well on another dataset or clinical settings. In this paper, we propose a methodology to improve the quality of heart disease prediction regardless of the dataset by training neural networks on a variety of datasets with further fine-tuning for the specific dataset. To show its applicability, we train different neural networks on a large private dataset TIS containing various ECG records from multiple hospitals and on a relatively small public dataset PTB-XL. We demonstrate that training the networks on a large dataset and fine-tuning
&lt;/p&gt;</description></item><item><title>Coeditor&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#36718;&#20195;&#30721;&#33258;&#21160;&#32534;&#36753;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#19968;&#20195;&#30721;&#24211;&#20013;&#30340;&#26368;&#36817;&#21464;&#21270;&#26469;&#39044;&#27979;&#23545;&#20195;&#30721;&#21306;&#22495;&#30340;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18584</link><description>&lt;p&gt;
Coeditor&#65306;&#21033;&#29992;&#19978;&#19979;&#25991;&#21464;&#21270;&#36827;&#34892;&#22810;&#36718;&#20195;&#30721;&#33258;&#21160;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing. (arXiv:2305.18584v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18584
&lt;/p&gt;
&lt;p&gt;
Coeditor&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#36718;&#20195;&#30721;&#33258;&#21160;&#32534;&#36753;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#19968;&#20195;&#30721;&#24211;&#20013;&#30340;&#26368;&#36817;&#21464;&#21270;&#26469;&#39044;&#27979;&#23545;&#20195;&#30721;&#21306;&#22495;&#30340;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#26469;&#32500;&#25252;&#21644;&#37325;&#26500;&#29616;&#26377;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#21069;&#24037;&#20316;&#37117;&#20165;&#20851;&#27880;&#20110;&#21019;&#24314;&#26032;&#20195;&#30721;&#65292;&#24573;&#30053;&#20102;&#23545;&#32534;&#36753;&#29616;&#26377;&#20195;&#30721;&#30340;&#29420;&#29305;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#22810;&#36718;&#20195;&#30721;&#33258;&#21160;&#32534;&#36753;&#30340;&#35774;&#32622;&#65292;&#26088;&#22312;&#22522;&#20110;&#21516;&#19968;&#20195;&#30721;&#24211;&#20013;&#30340;&#26368;&#36817;&#21464;&#21270;&#26469;&#39044;&#27979;&#23545;&#20195;&#30721;&#21306;&#22495;&#30340;&#32534;&#36753;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;Coeditor&#26159;&#19968;&#20010;&#32463;&#36807;&#32454;&#21270;&#30340;CodeT5&#27169;&#22411;&#65292;&#20855;&#26377;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#30340;&#22686;&#24378;&#21151;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#24046;&#24322;&#26684;&#24335;&#23545;&#20195;&#30721;&#26356;&#25913;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#37319;&#29992;&#38745;&#24577;&#20998;&#26512;&#26469;&#24418;&#25104;&#22823;&#22411;&#23450;&#21046;&#27169;&#22411;&#19978;&#19979;&#25991;&#65292;&#20197;&#30830;&#20445;&#36866;&#24403;&#30340;&#39044;&#27979;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;1650&#20010;&#24320;&#28304;Python&#39033;&#30446;&#30340;&#25552;&#20132;&#21382;&#21490;&#20013;&#25910;&#38598;&#20102;&#19968;&#20010;&#20195;&#30721;&#32534;&#36753;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#31616;&#21270;&#30340;&#21333;&#36718;&#21333;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;Coeditor&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#20248;&#20110;&#26368;&#20339;&#30340;&#20195;&#30721;&#23436;&#25104;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#36817;&#20046;&#32763;&#20493;&#65292;&#21363;&#20351;&#20351;&#29992;&#30340;&#27169;&#22411;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, neglecting the unique requirements of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned CodeT5 model with enhancements specifically designed for code editing tasks. We encode code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms the best code completion approach -- nearly doubling its exact-match accuracy, despite using a much smaller model 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QATS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35299;&#30721;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#24207;&#21015;&#12290;&#23427;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20026;&#22810;&#23545;&#25968;&#21644;&#31435;&#26041;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#30456;&#23545;&#36739;&#23569;&#29366;&#24577;&#30340;&#22823;&#22411;HMM&#12290;</title><link>http://arxiv.org/abs/2305.18578</link><description>&lt;p&gt;
&#24555;&#36895;&#33258;&#36866;&#24212;&#19977;&#20803;&#20998;&#21106;&#65306;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#26377;&#25928;&#35299;&#30721;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quick Adaptive Ternary Segmentation: An Efficient Decoding Procedure For Hidden Markov Models. (arXiv:2305.18578v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18578
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QATS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35299;&#30721;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#24207;&#21015;&#12290;&#23427;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20026;&#22810;&#23545;&#25968;&#21644;&#31435;&#26041;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#30456;&#23545;&#36739;&#23569;&#29366;&#24577;&#30340;&#22823;&#22411;HMM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#20197;&#19981;&#21487;&#35266;&#23519;&#30340;&#65288;&#38544;&#34255;&#30340;&#65289;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#21487;&#35266;&#27979;&#30340;&#36807;&#31243;&#20026;&#29305;&#24449;&#65292;&#21518;&#32773;&#26159;&#38544;&#34255;&#38142;&#30340;&#22122;&#22768;&#29256;&#26412;&#12290;&#20174;&#22024;&#26434;&#30340;&#35266;&#27979;&#20013;&#35299;&#30721;&#21407;&#22987;&#20449;&#21495;&#65288;&#21363;&#38544;&#34255;&#38142;&#65289;&#26159;&#20960;&#20046;&#25152;&#26377;&#22522;&#20110;HMM&#30340;&#25968;&#25454;&#20998;&#26512;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#29616;&#26377;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#22914;&#32500;&#29305;&#27604;&#31639;&#27861;&#65292;&#22312;&#35266;&#27979;&#24207;&#21015;&#38271;&#24230;&#26368;&#22810;&#32447;&#24615;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#39532;&#23572;&#21487;&#22827;&#38142;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#20013;&#20855;&#26377;&#27425;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;&#33258;&#36866;&#24212;&#19977;&#20803;&#20998;&#21106;&#65288;QATS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20998;&#32780;&#27835;&#20043;&#30340;&#36807;&#31243;&#65292;&#21487;&#22312;&#24207;&#21015;&#38271;&#24230;&#30340;&#22810;&#23545;&#25968;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29366;&#24577;&#31354;&#38388;&#30340;&#19977;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#35299;&#30721;&#38544;&#34255;&#30340;&#24207;&#21015;&#65292;&#22240;&#27492;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#30456;&#23545;&#36739;&#23569;&#29366;&#24577;&#30340;&#22823;&#35268;&#27169;HMM&#12290;&#35813;&#31243;&#24207;&#36824;&#24314;&#35758;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#23384;&#20648;&#26041;&#24335;&#65292;&#21363;&#29305;&#23450;&#30340;&#32047;&#31215;&#24635;&#21644;&#12290;&#23454;&#36136;&#19978;&#65292;&#20272;&#35745;&#30340;&#29366;&#24577;&#24207;&#21015;&#25353;&#39034;&#24207;&#26368;&#22823;&#21270;&#23616;&#37096;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hidden Markov models (HMMs) are characterized by an unobservable (hidden) Markov chain and an observable process, which is a noisy version of the hidden chain. Decoding the original signal (i.e., hidden chain) from the noisy observations is one of the main goals in nearly all HMM based data analyses. Existing decoding algorithms such as the Viterbi algorithm have computational complexity at best linear in the length of the observed sequence, and sub-quadratic in the size of the state space of the Markov chain. We present Quick Adaptive Ternary Segmentation (QATS), a divide-and-conquer procedure which decodes the hidden sequence in polylogarithmic computational complexity in the length of the sequence, and cubic in the size of the state space, hence particularly suited for large scale HMMs with relatively few states. The procedure also suggests an effective way of data storage as specific cumulative sums. In essence, the estimated sequence of states sequentially maximizes local likeliho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21463;&#21040;&#25968;&#23398;&#21551;&#21457;&#30340;L2O&#27169;&#22411;&#65292;&#20854;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#33021;&#65292;&#24182;&#22522;&#20110;&#25104;&#21151;&#30340;&#26356;&#26032;&#35268;&#21017;&#36890;&#24120;&#28385;&#36275;&#30340;&#22522;&#26412;&#25968;&#23398;&#26465;&#20214;&#36827;&#34892;&#20102;&#25512;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.18577</link><description>&lt;p&gt;
&#20026;&#23398;&#20064;&#20248;&#21270;&#26500;&#24314;&#25968;&#23398;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Constituting Mathematical Structures for Learning to Optimize. (arXiv:2305.18577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21463;&#21040;&#25968;&#23398;&#21551;&#21457;&#30340;L2O&#27169;&#22411;&#65292;&#20854;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#33021;&#65292;&#24182;&#22522;&#20110;&#25104;&#21151;&#30340;&#26356;&#26032;&#35268;&#21017;&#36890;&#24120;&#28385;&#36275;&#30340;&#22522;&#26412;&#25968;&#23398;&#26465;&#20214;&#36827;&#34892;&#20102;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064;&#20248;&#21270;&#31639;&#27861;&#30340;&#23398;&#20064;&#20248;&#21270;(L2O)&#25216;&#26415;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#31181;&#36890;&#29992;&#30340;L2O&#26041;&#27861;&#21442;&#25968;&#21270;&#20102;&#36845;&#20195;&#26356;&#26032;&#35268;&#21017;&#65292;&#24182;&#23558;&#26356;&#26032;&#26041;&#21521;&#20316;&#20026;&#40657;&#30418;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#12290;&#34429;&#28982;&#36890;&#29992;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#20294;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#33021;&#36807;&#25311;&#21512;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#20013;&#12290;&#26412;&#25991;&#25512;&#23548;&#20102;&#25104;&#21151;&#30340;&#26356;&#26032;&#35268;&#21017;&#36890;&#24120;&#28385;&#36275;&#30340;&#22522;&#26412;&#25968;&#23398;&#26465;&#20214;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;L2O&#27169;&#22411;&#65292;&#20854;&#32467;&#26500;&#21463;&#21040;&#25968;&#23398;&#21551;&#21457;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;L2O&#27169;&#22411;&#30340;&#33391;&#22909;&#23454;&#39564;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Optimize (L2O), a technique that utilizes machine learning to learn an optimization algorithm automatically from data, has gained arising attention in recent years. A generic L2O approach parameterizes the iterative update rule and learns the update direction as a black-box network. While the generic approach is widely applicable, the learned model can overfit and may not generalize well to out-of-distribution test sets. In this paper, we derive the basic mathematical conditions that successful update rules commonly satisfy. Consequently, we propose a novel L2O model with a mathematics-inspired structure that is broadly applicable and generalized well to out-of-distribution problems. Numerical simulations validate our theoretical findings and demonstrate the superior empirical performance of the proposed L2O model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.18569</link><description>&lt;p&gt;
ChatGPT&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#35299;&#20915;LLM&#20013;&#19981;&#20844;&#24179;&#30340;&#38382;&#39064;&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;LLM&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#39046;&#22495;&#26102;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20844;&#24179;&#35780;&#20272;&#26041;&#38754;&#65292;&#25968;&#37327;&#20998;&#26512;&#21644;&#28145;&#20837;&#30740;&#31350;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;ChatGPT&#22312;&#21253;&#25324;&#25945;&#32946;&#12289;&#29359;&#32618;&#23398;&#12289;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#21644;&#20010;&#20154;&#20844;&#24179;&#24615;&#65292;&#24182;&#35266;&#23519;&#20102;&#22312;&#19968;&#31995;&#21015;&#26377;&#20559;&#25110;&#26080;&#20559;&#25552;&#31034;&#19979;ChatGPT&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#20415;&#20110;&#20559;&#35265;&#32531;&#35299;&#65292;&#20419;&#36827;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;PaLI-X&#65292;&#36890;&#36807;&#25193;&#23637;&#27169;&#22411;&#32452;&#20214;&#21644;&#35757;&#32451;&#20219;&#21153;&#33539;&#22260;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#26032;&#24615;&#33021;&#27700;&#24179;&#65292;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#38382;&#31572;&#12289;&#23545;&#35937;&#26816;&#27979;&#12289;&#35270;&#39057;&#38382;&#31572;&#21644;&#35270;&#39057;&#23383;&#24149;&#31561;&#65292;&#21516;&#26102;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18565</link><description>&lt;p&gt;
PaLI-X&#65306;&#20851;&#20110;&#25193;&#23637;&#19968;&#31181;&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PaLI-X: On Scaling up a Multilingual Vision and Language Model. (arXiv:2305.18565v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18565
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;PaLI-X&#65292;&#36890;&#36807;&#25193;&#23637;&#27169;&#22411;&#32452;&#20214;&#21644;&#35757;&#32451;&#20219;&#21153;&#33539;&#22260;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#26032;&#24615;&#33021;&#27700;&#24179;&#65292;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#38382;&#31572;&#12289;&#23545;&#35937;&#26816;&#27979;&#12289;&#35270;&#39057;&#38382;&#31572;&#21644;&#35270;&#39057;&#23383;&#24149;&#31561;&#65292;&#21516;&#26102;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25193;&#23637;PaLI-X&#65292;&#19968;&#31181;&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#37197;&#26041;&#21644;&#32467;&#26524;&#65292;&#21253;&#25324;&#32452;&#20214;&#30340;&#22823;&#23567;&#21644;&#35757;&#32451;&#20219;&#21153;&#33539;&#22260;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;&#23383;&#24149;&#21644;&#38382;&#31572;&#20219;&#21153;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#25991;&#26723;&#29702;&#35299;&#21644;&#23569;&#37327;&#65288;&#19978;&#19979;&#25991;&#20869;&#65289;&#23398;&#20064;&#20197;&#21450;&#23545;&#35937;&#26816;&#27979;&#12289;&#35270;&#39057;&#38382;&#31572;&#21644;&#35270;&#39057;&#23383;&#24149;&#31561;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;PaLI-X &#22312;&#22823;&#22810;&#25968;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#65288;&#32771;&#34385;&#20102;25&#20010;&#20197;&#19978;&#30340;&#27979;&#35797;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20123;&#26032;&#20852;&#30340;&#33021;&#21147;&#65292;&#22914;&#22797;&#26434;&#35745;&#25968;&#21644;&#22810;&#35821;&#35328;&#23545;&#35937;&#26816;&#27979;&#65292;&#36825;&#20123;&#20219;&#21153;&#24182;&#27809;&#26377;&#26126;&#30830;&#21015;&#22312;&#35757;&#32451;&#33539;&#22260;&#20043;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.
&lt;/p&gt;</description></item><item><title>SHARP&#26159;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#24335;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;&#21160;&#24577;&#36830;&#25509;&#21644;&#28608;&#27963;&#22238;&#25918;&#26469;&#22238;&#25918;&#22788;&#29702;&#36807;&#30340;&#31070;&#32463;&#27169;&#24335;&#65292;&#20248;&#20808;&#22238;&#25918;&#26368;&#36817;&#23398;&#21040;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#25345;&#32493;&#26356;&#26032;&#25152;&#26377;&#23618;&#12290;</title><link>http://arxiv.org/abs/2305.18563</link><description>&lt;p&gt;
SHARP: &#31232;&#30095;&#24615;&#21644;&#38544;&#34255;&#28608;&#27963;&#22238;&#25918;&#29992;&#20110;&#31070;&#32463;&#21551;&#21457;&#24335;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SHARP: Sparsity and Hidden Activation RePlay for Neuro-Inspired Continual Learning. (arXiv:2305.18563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18563
&lt;/p&gt;
&lt;p&gt;
SHARP&#26159;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#24335;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;&#21160;&#24577;&#36830;&#25509;&#21644;&#28608;&#27963;&#22238;&#25918;&#26469;&#22238;&#25918;&#22788;&#29702;&#36807;&#30340;&#31070;&#32463;&#27169;&#24335;&#65292;&#20248;&#20808;&#22238;&#25918;&#26368;&#36817;&#23398;&#21040;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#25345;&#32493;&#26356;&#26032;&#25152;&#26377;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23398;&#20064;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#25110;&#24658;&#23450;&#30340;&#29615;&#22659;&#12290;&#36830;&#32493;&#23398;&#20064;(CL)&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#20351;DNN&#33021;&#22815;&#36880;&#27493;&#31215;&#32047;&#30693;&#35782;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#12290;&#21463;&#21040;&#22823;&#33041;&#22914;&#20309;&#24041;&#22266;&#35760;&#24518;&#30340;&#21551;&#21457;&#65292;&#22238;&#25918;&#26159;CL&#30340;&#24378;&#26377;&#21147;&#31574;&#30053;&#65292;&#23427;&#28041;&#21450;&#23545;DNN&#36827;&#34892;&#26032;&#21644;&#25152;&#26377;&#24050;&#35265;&#31867;&#21035;&#30340;&#28151;&#21512;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22238;&#25918;&#26041;&#27861;&#24573;&#30053;&#20102;&#29983;&#29289;&#22238;&#25918;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;: 1) &#22823;&#33041;&#22238;&#25918;&#22788;&#29702;&#36807;&#30340;&#31070;&#32463;&#27169;&#24335;&#32780;&#19981;&#26159;&#21407;&#22987;&#36755;&#20837;&#65292;2)&#23427;&#20248;&#20808;&#22238;&#25918;&#26368;&#36817;&#23398;&#21040;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#37325;&#28201;&#25152;&#26377;&#36807;&#21435;&#30340;&#32463;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SHARP&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;CL&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;&#21160;&#24577;&#36830;&#25509;&#21644;&#28608;&#27963;&#22238;&#25918;&#12290;&#19982;&#20854;&#20182;&#28608;&#27963;&#22238;&#25918;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20551;&#23450;&#27809;&#26377;&#21463;&#21040;&#22238;&#25918;&#30340;&#23618;&#24050;&#32463;&#32463;&#36807;&#39044;&#35757;&#32451;&#24182;&#22266;&#23450;&#65292;SHARP&#21487;&#20197;&#25345;&#32493;&#26356;&#26032;&#25152;&#26377;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) struggle to learn in dynamic environments since they rely on fixed datasets or stationary environments. Continual learning (CL) aims to address this limitation and enable DNNs to accumulate knowledge incrementally, similar to human learning. Inspired by how our brain consolidates memories, a powerful strategy in CL is replay, which involves training the DNN on a mixture of new and all seen classes. However, existing replay methods overlook two crucial aspects of biological replay: 1) the brain replays processed neural patterns instead of raw input, and 2) it prioritizes the replay of recently learned information rather than revisiting all past experiences. To address these differences, we propose SHARP, an efficient neuro-inspired CL method that leverages sparse dynamic connectivity and activation replay. Unlike other activation replay methods, which assume layers not subjected to replay have been pretrained and fixed, SHARP can continually update all layers
&lt;/p&gt;</description></item><item><title>DelBugV&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#22686;&#37327;&#35843;&#35797;&#25216;&#26415;&#36827;&#34892;DNN&#39564;&#35777;&#22120;&#35843;&#35797;&#30340;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#26356;&#31616;&#21333;&#30340;&#39564;&#35777;&#23454;&#20363;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#21457;&#29616;&#39564;&#35777;&#22120;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.18558</link><description>&lt;p&gt;
DelBugV: Delta-Debugging&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;
&lt;/p&gt;
&lt;p&gt;
DelBugV: Delta-Debugging Neural Network Verifiers. (arXiv:2305.18558v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18558
&lt;/p&gt;
&lt;p&gt;
DelBugV&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#22686;&#37327;&#35843;&#35797;&#25216;&#26415;&#36827;&#34892;DNN&#39564;&#35777;&#22120;&#35843;&#35797;&#30340;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#26356;&#31616;&#21333;&#30340;&#39564;&#35777;&#23454;&#20363;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#21457;&#29616;&#39564;&#35777;&#22120;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20986;&#29616;&#20005;&#37325;&#30340;&#38169;&#35823;&#65307;&#36825;&#24341;&#36215;&#20102;&#23545;&#20854;&#36827;&#34892;&#24418;&#24335;&#39564;&#35777;&#30340;&#37325;&#22823;&#20852;&#36259;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;DNN&#39564;&#35777;&#22120;&#26159;&#22797;&#26434;&#30340;&#24037;&#20855;&#65292;&#23427;&#20204;&#26412;&#36523;&#23481;&#26131;&#21463;&#21040;&#27491;&#30830;&#24615;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;DNN&#39564;&#35777;&#22120;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#27491;&#22312;&#39564;&#35777;&#30340;DNN&#30340;&#22823;&#23567;&#65292;&#22240;&#27492;&#35843;&#35797;&#27492;&#31867;&#38169;&#35823;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DelBugV&#30340;&#26032;&#22411;&#24037;&#20855;&#65292;&#23427;&#20351;&#29992;&#33258;&#21160;&#21270;&#30340;&#22686;&#37327;&#35843;&#35797;&#25216;&#26415;&#22312;DNN&#39564;&#35777;&#22120;&#19978;&#36827;&#34892;&#35843;&#35797;&#12290;&#32473;&#23450;&#19968;&#20010;&#26377;&#25925;&#38556;&#30340;DNN&#39564;&#35777;&#22120;&#21644;&#19968;&#20010;&#27491;&#30830;&#30340;&#39564;&#35777;&#22120;&#20316;&#20026;&#21442;&#32771;&#28857;&#65288;&#25110;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20165;&#19968;&#20010;&#26377;&#25925;&#38556;&#30340;&#39564;&#35777;&#22120;&#65289;&#65292;DelBugV&#21487;&#20197;&#20135;&#29983;&#26356;&#31616;&#21333;&#30340;DNN&#39564;&#35777;&#23454;&#20363;&#65292;&#20173;&#28982;&#20250;&#35302;&#21457;&#19981;&#38656;&#35201;&#30340;&#34892;&#20026;--&#36825;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#35843;&#35797;&#25925;&#38556;&#39564;&#35777;&#22120;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#26159;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#38468;&#21152;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#21644;&#31574;&#30053;&#36827;&#34892;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are becoming a key component in diverse systems across the board. However, despite their success, they often err miserably; and this has triggered significant interest in formally verifying them. Unfortunately, DNN verifiers are intricate tools, and are themselves susceptible to soundness bugs. Due to the complexity of DNN verifiers, as well as the sizes of the DNNs being verified, debugging such errors is a daunting task. Here, we present a novel tool, named DelBugV, that uses automated delta debugging techniques on DNN verifiers. Given a malfunctioning DNN verifier and a correct verifier as a point of reference (or, in some cases, just a single, malfunctioning verifier), DelBugV can produce much simpler DNN verification instances that still trigger undesired behavior -- greatly facilitating the task of debugging the faulty verifier. Our tool is modular and extensible, and can easily be enhanced with additional network simplification methods and strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18553</link><description>&lt;p&gt;
&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Controllable Path of Destruction. (arXiv:2305.18553v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27585;&#28781;&#36335;&#24452;&#65288;PoD&#65289;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#30772;&#22351;&#19968;&#32452;&#29289;&#21697;&#26469;&#20135;&#29983;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#20026;&#27599;&#20010;&#30772;&#22351;&#27493;&#39588;&#21019;&#24314;&#19968;&#20010;&#19982;&#30456;&#24212;&#20462;&#22797;&#21160;&#20316;&#30456;&#20851;&#30340;&#35757;&#32451;&#23454;&#20363;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#36890;&#36807;&#20174;&#20219;&#24847;&#29366;&#24577;&#8220;&#20462;&#22797;&#8221;&#26469;&#29983;&#25104;&#26032;&#30340;&#29289;&#21697;&#12290;PoD&#26041;&#27861;&#22312;&#21407;&#22987;&#35757;&#32451;&#31034;&#20363;&#26041;&#38754;&#38750;&#24120;&#33410;&#30465;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#21512;&#30001;&#20998;&#31867;&#25968;&#25454;&#32452;&#25104;&#30340;&#21151;&#33021;&#37096;&#20214;&#65292;&#20363;&#22914;&#28216;&#25103;&#20851;&#21345;&#21644;&#31163;&#25955;&#30340;3D&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#25193;&#23637;&#21040;&#20801;&#35768;&#35774;&#35745;&#24072;&#25511;&#21046;&#29983;&#25104;&#30340;&#29289;&#21697;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#21521;&#26500;&#25104;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#24341;&#20837;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#22312;2D&#22320;&#29282;&#35774;&#32622;&#20197;&#21450;&#23567;&#22411;3D&#20048;&#39640;&#27773;&#36710;&#39046;&#22495;&#27979;&#35797;&#20102;&#21487;&#25511;PoD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path of Destruction (PoD) is a self-supervised method for learning iterative generators. The core idea is to produce a training set by destroying a set of artifacts, and for each destructive step create a training instance based on the corresponding repair action. A generator trained on this dataset can then generate new artifacts by ``repairing'' from arbitrary states. The PoD method is very data-efficient in terms of original training examples and well-suited to functional artifacts composed of categorical data, such as game levels and discrete 3D structures. In this paper, we extend the Path of Destruction method to allow designer control over aspects of the generated artifacts. Controllability is introduced by adding conditional inputs to the state-action pairs that make up the repair trajectories. We test the controllable PoD method in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#8212;&#8212;&#32447;&#24615;&#32676;&#32593;&#32476;(LGNs)&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#20316;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#31354;&#38388;&#19978;&#30340;&#32447;&#24615;&#32676;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#25351;&#23450;&#25152;&#38656;&#30340;&#23545;&#31216;&#24615;&#12290;&#35813;&#32467;&#26500;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#21450;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18552</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32447;&#24615;&#32676;
&lt;/p&gt;
&lt;p&gt;
Learning Linear Groups in Neural Networks. (arXiv:2305.18552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#8212;&#8212;&#32447;&#24615;&#32676;&#32593;&#32476;(LGNs)&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#20316;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#31354;&#38388;&#19978;&#30340;&#32447;&#24615;&#32676;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#25351;&#23450;&#25152;&#38656;&#30340;&#23545;&#31216;&#24615;&#12290;&#35813;&#32467;&#26500;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#21450;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;&#31561;&#21464;&#24615;&#22312;&#26550;&#26500;&#20013;&#32534;&#30721;&#39046;&#22495;&#30693;&#35782;&#65292;&#21487;&#23548;&#33268;&#26356;&#22823;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20107;&#20808;&#25351;&#23450;&#25152;&#38656;&#30340;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#32676;&#32593;&#32476;&#65288;LGNs&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#20316;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#31354;&#38388;&#19978;&#30340;&#32447;&#24615;&#32676;&#12290;&#32447;&#24615;&#32676;&#20855;&#26377;&#22266;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#34920;&#31034;&#20026;&#26377;&#38480;&#30697;&#38453;&#12290;LGNs&#23398;&#20064;&#32676;&#20307;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#30417;&#30563;&#25110;&#38544;&#34255;&#23545;&#31216;&#24615;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#36825;&#20123;&#32676;&#20307;&#21487;&#20197;&#26144;&#23556;&#21040;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;LGNs&#26469;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#32676;&#20307;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;; &#25105;&#20204;&#35777;&#26126;&#65292;&#32447;&#24615;&#32676;&#32467;&#26500;&#21462;&#20915;&#20110;&#25968;&#25454;&#20998;&#24067;&#21644;&#32771;&#34385;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Employing equivariance in neural networks leads to greater parameter efficiency and improved generalization performance through the encoding of domain knowledge in the architecture; however, the majority of existing approaches require an a priori specification of the desired symmetries. We present a neural network architecture, Linear Group Networks (LGNs), for learning linear groups acting on the weight space of neural networks. Linear groups are desirable due to their inherent interpretability, as they can be represented as finite matrices. LGNs learn groups without any supervision or knowledge of the hidden symmetries in the data and the groups can be mapped to well known operations in machine learning. We use LGNs to learn groups on multiple datasets while considering different downstream tasks; we demonstrate that the linear group structure depends on both the data distribution and the considered task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20803;&#22238;&#24402;&#20998;&#26512;&#30740;&#31350;&#20102;&#30701;&#26399;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#30340;&#24433;&#21709;&#22240;&#32032;&#65292;&#21457;&#29616;&#32593;&#32476;&#32423;&#21035;&#12289;&#39044;&#27979;&#31890;&#24230;&#21644;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#26159;&#24433;&#21709;&#39044;&#27979;&#31934;&#24230;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.18550</link><description>&lt;p&gt;
&#30701;&#26399;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#35823;&#24046;&#30340;&#20803;&#22238;&#24402;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Meta-Regression Analysis of Errors in Short-Term Electricity Load Forecasting. (arXiv:2305.18550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20803;&#22238;&#24402;&#20998;&#26512;&#30740;&#31350;&#20102;&#30701;&#26399;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#30340;&#24433;&#21709;&#22240;&#32032;&#65292;&#21457;&#29616;&#32593;&#32476;&#32423;&#21035;&#12289;&#39044;&#27979;&#31890;&#24230;&#21644;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#26159;&#24433;&#21709;&#39044;&#27979;&#31934;&#24230;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#30005;&#21147;&#38656;&#27714;&#23545;&#20110;&#30830;&#20445;&#30005;&#21147;&#20379;&#24212;&#30340;&#21487;&#38752;&#21644;&#25104;&#26412;&#25928;&#30410;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20840;&#29699;&#36716;&#22411;&#20197;&#21450;&#20379;&#26262;&#21644;&#20132;&#36890;&#30340;&#30005;&#27668;&#21270;&#65292;&#20934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#26412;&#25991;&#21033;&#29992;&#26469;&#33258;59&#31687;&#30740;&#31350;&#20013;421&#20010;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#65292;&#36827;&#34892;&#20102;&#24433;&#21709;&#30701;&#26399;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#31934;&#24230;&#30340;&#22240;&#32032;&#30340;&#20803;&#22238;&#24402;&#20998;&#26512;&#65288;MRA&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32593;&#32476;&#32423;&#21035;&#65288;&#29305;&#21035;&#26159;&#20010;&#20307;&#12289;&#32858;&#21512;&#21644;&#31995;&#32479;&#65289;&#12289;&#39044;&#27979;&#31890;&#24230;&#21644;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#20284;&#20046;&#23545;MAPE&#12289;&#25991;&#29486;&#35745;&#37327;&#25968;&#25454;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#39044;&#27979;&#35823;&#24046;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting electricity demand plays a critical role in ensuring reliable and cost-efficient operation of the electricity supply. With the global transition to distributed renewable energy sources and the electrification of heating and transportation, accurate load forecasts become even more important. While numerous empirical studies and a handful of review articles exist, there is surprisingly little quantitative analysis of the literature, most notably none that identifies the impact of factors on forecasting performance across the entirety of empirical studies. In this article, we therefore present a Meta-Regression Analysis (MRA) that examines factors that influence the accuracy of short-term electricity load forecasts. We use data from 421 forecast models published in 59 studies. While the grid level (esp. individual vs. aggregated vs. system), the forecast granularity, and the algorithms used seem to have a significant impact on the MAPE, bibliometric data, dataset sizes, and pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#24378;&#25932;&#25163;&#24773;&#20917;&#19979;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.18543</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Lipschitz Bandits to Adversarial Corruptions. (arXiv:2305.18543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#24378;&#25932;&#25163;&#24773;&#20917;&#19979;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#36172;&#24466;&#31639;&#27861;&#26159;&#19968;&#31181;&#22788;&#29702;&#23450;&#20041;&#22312;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#36830;&#32493;&#33218;&#38598;&#30340;&#38543;&#26426;&#36172;&#24466;&#31639;&#27861;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#22870;&#21169;&#20989;&#25968;&#21463;&#21040;Lipschitz&#32422;&#26463;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Lipschitz&#36172;&#24466;&#38382;&#39064;&#65292;&#21363;&#22312;&#23545;&#25239;&#24615;&#30772;&#22351;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#36866;&#24212;&#25932;&#25163;&#23558;&#38543;&#26426;&#22870;&#21169;&#25439;&#22351;&#21040;&#24635;&#39044;&#31639; $C$&#12290; &#39044;&#31639;&#36890;&#36807;&#26102;&#38388;&#36328;&#24230; $T$ &#20013;&#30340;&#30772;&#22351;&#27700;&#24179;&#20043;&#21644;&#26469;&#34913;&#37327;&#12290; &#25105;&#20204;&#32771;&#34385;&#24369;&#21644;&#24378;&#25932;&#25163;&#65292;&#20854;&#20013;&#24369;&#25932;&#25163;&#22312;&#25915;&#20987;&#20043;&#21069;&#19981;&#30693;&#36947;&#24403;&#21069;&#30340;&#34892;&#21160;&#65292;&#32780;&#24378;&#25932;&#25163;&#21487;&#20197;&#35266;&#23519;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#34892;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;&#65292;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#25932;&#25163;&#19979;&#65292;&#29978;&#33267;&#22312;&#25439;&#22351;&#24635;&#39044;&#31639; $C$ &#26410;&#21521;&#20195;&#29702;&#25259;&#38706;&#30340;&#24773;&#20917;&#19979;&#65292;&#22343;&#33021;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#22312;&#27599;&#31181;&#31867;&#22411;&#30340;&#25932;&#25163;&#19979;&#25552;&#20379;&#19979;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24378;&#31867;&#22411;&#19979;&#26159;&#26368;&#20248;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#20197;&#35828;&#26126;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lipschitz bandit is a variant of stochastic bandits that deals with a continuous arm set defined on a metric space, where the reward function is subject to a Lipschitz constraint. In this paper, we introduce a new problem of Lipschitz bandits in the presence of adversarial corruptions where an adaptive adversary corrupts the stochastic rewards up to a total budget $C$. The budget is measured by the sum of corruption levels across the time horizon $T$. We consider both weak and strong adversaries, where the weak adversary is unaware of the current action before the attack, while the strong one can observe it. Our work presents the first line of robust Lipschitz bandit algorithms that can achieve sub-linear regret under both types of adversary, even when the total budget of corruption $C$ is unrevealed to the agent. We provide a lower bound under each type of adversary, and show that our algorithm is optimal under the strong case. Finally, we conduct experiments to illustrate the effecti
&lt;/p&gt;</description></item><item><title>&#24425;&#34425;&#32593;&#32476;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#20869;&#31070;&#32463;&#20803;&#26435;&#37325;&#20114;&#30456;&#29420;&#31435;&#30340;&#23545;&#40784;&#21644;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#26469;&#36827;&#34892;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#65292;&#22312;ImageNet&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18512</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#40657;&#30418;&#20013;&#30340;&#24425;&#34425;
&lt;/p&gt;
&lt;p&gt;
A Rainbow in Deep Network Black Boxes. (arXiv:2305.18512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18512
&lt;/p&gt;
&lt;p&gt;
&#24425;&#34425;&#32593;&#32476;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#20869;&#31070;&#32463;&#20803;&#26435;&#37325;&#20114;&#30456;&#29420;&#31435;&#30340;&#23545;&#40784;&#21644;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#26469;&#36827;&#34892;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#65292;&#22312;ImageNet&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24425;&#34425;&#32593;&#32476;&#20316;&#20026;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32423;&#32852;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#65292;&#20854;&#26435;&#37325;&#20998;&#24067;&#26159;&#21487;&#20197;&#23398;&#20064;&#30340;&#12290;&#23427;&#20551;&#35774;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#26435;&#37325;&#20381;&#36182;&#24615;&#34987;&#20943;&#23569;&#21040;&#23558;&#36755;&#20837;&#28608;&#27963;&#23545;&#20934;&#30340;&#26059;&#36716;&#12290;&#23618;&#20869;&#30340;&#31070;&#32463;&#20803;&#26435;&#37325;&#22312;&#36825;&#31181;&#23545;&#40784;&#21518;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#23427;&#20204;&#30340;&#28608;&#27963;&#23450;&#20041;&#20102;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#21464;&#24471;&#30830;&#23450;&#30340;&#20869;&#26680;&#12290;&#36825;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;ResNets&#20013;&#36890;&#36807;&#25968;&#23383;&#39564;&#35777;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#20998;&#24067;&#20855;&#26377;&#20302;&#31209;&#21327;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#24425;&#34425;&#32593;&#32476;&#22312;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#19982;&#30333;&#33394;&#38543;&#26426;&#29305;&#24449;&#20043;&#38388;&#20132;&#26367;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#26031;&#26435;&#37325;&#20998;&#24067;&#30340;&#39640;&#26031;&#24425;&#34425;&#32593;&#32476;&#23450;&#20041;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20351;&#29992;&#23567;&#27874;&#25955;&#23556;&#32593;&#32476;&#36827;&#34892;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#25968;&#23383;&#39564;&#35777;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;SGD&#26356;&#26032;&#26435;&#37325;&#30340;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce rainbow networks as a probabilistic model of trained deep neural networks. The model cascades random feature maps whose weight distributions are learned. It assumes that dependencies between weights at different layers are reduced to rotations which align the input activations. Neuron weights within a layer are independent after this alignment. Their activations define kernels which become deterministic in the infinite-width limit. This is verified numerically for ResNets trained on the ImageNet dataset. We also show that the learned weight distributions have low-rank covariances. Rainbow networks thus alternate between linear dimension reductions and non-linear high-dimensional embeddings with white random features. Gaussian rainbow networks are defined with Gaussian weight distributions. These models are validated numerically on image classification on the CIFAR-10 dataset, with wavelet scattering networks. We further show that during training, SGD updates the weight cov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#39046;&#22495;&#8220;&#20146;&#27835;&#30103;&#8221;&#25805;&#20316;&#30340;&#38480;&#21046;&#65292;&#19988;&#32771;&#34385;&#21040;&#20102;&#25805;&#20316;&#39044;&#31639;&#30340;&#20855;&#26377;&#20449;&#24687;&#39044;&#31639;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#23558;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#21644;&#24773;&#22659;&#36172;&#21338;&#26426;&#23398;&#20064;&#31639;&#27861;&#26377;&#26426;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18511</link><description>&lt;p&gt;
&#20855;&#26377;&#20449;&#24687;&#39044;&#31639;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits with Budgeted Information Reveal. (arXiv:2305.18511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#39046;&#22495;&#8220;&#20146;&#27835;&#30103;&#8221;&#25805;&#20316;&#30340;&#38480;&#21046;&#65292;&#19988;&#32771;&#34385;&#21040;&#20102;&#25805;&#20316;&#39044;&#31639;&#30340;&#20855;&#26377;&#20449;&#24687;&#39044;&#31639;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#23558;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#21644;&#24773;&#22659;&#36172;&#21338;&#26426;&#23398;&#20064;&#31639;&#27861;&#26377;&#26426;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;&#24120;&#29992;&#20110;&#25512;&#33616;&#20010;&#24615;&#21270;&#30340;&#21307;&#30103;&#22788;&#29702;&#26041;&#24335;&#65292;&#20294;&#22312;&#23454;&#38469;&#25805;&#20316;&#20013;&#65292;&#20026;&#20445;&#35777;&#27835;&#30103;&#25928;&#26524;&#65292;&#21307;&#29983;&#36890;&#24120;&#38656;&#35201;&#35201;&#27714;&#24739;&#32773;&#37319;&#21462;&#27809;&#26377;&#30452;&#25509;&#22909;&#22788;&#30340;&#8220;&#20146;&#27835;&#30103;&#8221;&#25805;&#20316;&#65292;&#32780;&#19988;&#20020;&#24202;&#21307;&#29983;&#30340;&#25805;&#20316;&#39044;&#31639;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26377;&#25928;&#32467;&#21512;&#20102;&#20004;&#31181;&#31639;&#27861;&#26041;&#27861;&#20043;&#38271;&#65306;1&#65289;&#19968;&#20010;&#20915;&#23450;&#26368;&#20339;&#26102;&#26426;&#19982;&#24739;&#32773;&#32852;&#31995;&#30340;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#65288;primal-dual&#65289;&#31639;&#27861;&#65292;2&#65289;&#29992;&#20110;&#21521;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#20855;&#26377;&#20122;&#32447;&#24615;&#30340;&#22238;&#24402;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual bandit algorithms are commonly used in digital health to recommend personalized treatments. However, to ensure the effectiveness of the treatments, patients are often requested to take actions that have no immediate benefit to them, which we refer to as pro-treatment actions. In practice, clinicians have a limited budget to encourage patients to take these actions and collect additional information. We introduce a novel optimization and learning algorithm to address this problem. This algorithm effectively combines the strengths of two algorithmic approaches in a seamless manner, including 1) an online primal-dual algorithm for deciding the optimal timing to reach out to patients, and 2) a contextual bandit learning algorithm to deliver personalized treatment to the patient. We prove that this algorithm admits a sub-linear regret bound. We illustrate the usefulness of this algorithm on both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>RLAD&#26159;&#39318;&#20010;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#24212;&#29992;&#22522;&#20110;&#20687;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#36335;&#24452;&#28857;&#32534;&#30721;&#22120;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18510</link><description>&lt;p&gt;
RLAD&#65306;&#22522;&#20110;&#20687;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban Environments. (arXiv:2305.18510v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18510
&lt;/p&gt;
&lt;p&gt;
RLAD&#26159;&#39318;&#20010;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#24212;&#29992;&#22522;&#20110;&#20687;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#36335;&#24452;&#28857;&#32534;&#30721;&#22120;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#24212;&#29992;&#20110;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#24863;&#30693;&#35757;&#32451;&#19982;&#39550;&#39542;&#31574;&#30053;&#35757;&#32451;&#20998;&#31163;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#20026;&#20102;&#36991;&#20813;&#22312;&#31574;&#30053;&#32593;&#32476;&#26049;&#36793;&#35757;&#32451;&#21367;&#31215;&#32534;&#30721;&#22120;&#65292;&#22240;&#20026;&#36825;&#31181;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#29305;&#24449;&#34920;&#31034;&#36864;&#21270;&#21644;&#33258;&#25105;&#36807;&#24230;&#25311;&#21512;&#31561;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#29615;&#22659;&#34920;&#31034;&#19982;&#19979;&#28216;&#20219;&#21153;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RLAD&#65292;&#36825;&#26159;&#39318;&#20010;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#24212;&#29992;&#22522;&#20110;&#20687;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLfP&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#22312;&#27492;&#39046;&#22495;&#20013;RLfP&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#65306;i&#65289;&#19968;&#31181;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#22270;&#20687;&#22686;&#24378;&#21644;&#33258;&#36866;&#24212;&#23616;&#37096;&#20449;&#21495;&#28151;&#21512;&#65288;A-LIX&#65289;&#23618;&#30340;&#20248;&#21183;&#65307;ii&#65289;WayConv1D&#65292;&#19968;&#31181;&#21033;&#29992;2D&#20960;&#20309;&#20449;&#24687;&#30340;&#36335;&#24452;&#28857;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches of Reinforcement Learning (RL) applied in urban Autonomous Driving (AD) focus on decoupling the perception training from the driving policy training. The main reason is to avoid training a convolution encoder alongside a policy network, which is known to have issues related to sample efficiency, degenerated feature representations, and catastrophic self-overfitting. However, this paradigm can lead to representations of the environment that are not aligned with the downstream task, which may result in suboptimal performances. To address this limitation, this paper proposes RLAD, the first Reinforcement Learning from Pixels (RLfP) method applied in the urban AD domain. We propose several techniques to enhance the performance of an RLfP algorithm in this domain, including: i) an image encoder that leverages both image augmentations and Adaptive Local Signal Mixing (A-LIX) layers; ii) WayConv1D, which is a waypoint encoder that harnesses the 2D geometrical information of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#23545;&#20110;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#65292;&#20854;&#27425;&#20248;&#24615;&#24517;&#39035;&#24402;&#22240;&#20110;&#22823;&#30340;&#20559;&#24046;&#32780;&#38750;&#26041;&#24046;&#65292;&#24182;&#19988;&#22312;ERM&#30340;&#24179;&#26041;&#35823;&#24046;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#20013;&#65292;&#26041;&#24046;&#39033;&#24517;&#28982;&#20855;&#26377;&#26497;&#23567;&#30340;&#22833;&#35823;&#29575;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;Chatterjee&#30340;&#19981;&#21487;&#20801;&#35768;&#24615;&#23450;&#29702;&#30340;&#31616;&#21333;&#35777;&#26126;&#65292;&#24182;&#34920;&#31034;&#20182;&#20204;&#30340;&#20272;&#35745;&#34920;&#26126;ERM&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18508</link><description>&lt;p&gt;
&#35770;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#24046;&#12289;&#21487;&#20801;&#35768;&#24615;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Variance, Admissibility, and Stability of Empirical Risk Minimization. (arXiv:2305.18508v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#23545;&#20110;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#65292;&#20854;&#27425;&#20248;&#24615;&#24517;&#39035;&#24402;&#22240;&#20110;&#22823;&#30340;&#20559;&#24046;&#32780;&#38750;&#26041;&#24046;&#65292;&#24182;&#19988;&#22312;ERM&#30340;&#24179;&#26041;&#35823;&#24046;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#20013;&#65292;&#26041;&#24046;&#39033;&#24517;&#28982;&#20855;&#26377;&#26497;&#23567;&#30340;&#22833;&#35823;&#29575;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;Chatterjee&#30340;&#19981;&#21487;&#20801;&#35768;&#24615;&#23450;&#29702;&#30340;&#31616;&#21333;&#35777;&#26126;&#65292;&#24182;&#34920;&#31034;&#20182;&#20204;&#30340;&#20272;&#35745;&#34920;&#26126;ERM&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21487;&#33021;&#20250;&#36798;&#21040;&#26497;&#23567;&#30340;&#26368;&#22823;&#22833;&#35823;&#29575;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#20449;&#24687;&#26159;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;ERM&#30340;&#27425;&#20248;&#24615;&#24517;&#39035;&#24402;&#22240;&#20110;&#22823;&#30340;&#20559;&#24046;&#32780;&#38750;&#26041;&#24046;&#12290;&#22312;ERM&#30340;&#24179;&#26041;&#35823;&#24046;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#20013;&#65292;&#26041;&#24046;&#39033;&#24517;&#28982;&#20855;&#26377;&#26497;&#23567;&#30340;&#22833;&#35823;&#29575;&#12290;&#25105;&#20204;&#20026;&#22266;&#23450;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#12289;&#20351;&#29992;&#27010;&#29575;&#26041;&#27861;&#35777;&#26126;&#36825;&#19968;&#20107;&#23454;&#30340;&#35777;&#26126;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#38543;&#26426;&#35774;&#35745;&#35774;&#32622;&#19979;&#20026;&#21508;&#31181;&#27169;&#22411;&#35777;&#26126;&#20102;&#36825;&#19968;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102; Chatterjee &#19981;&#21487;&#20801;&#35768;&#24615;&#23450;&#29702; (Chatterjee, 2014, Theorem 1.4) &#30340;&#31616;&#21333;&#35777;&#26126;&#65292;&#35813;&#23450;&#29702;&#25351;&#20986;&#65292;&#22312;&#22266;&#23450;&#35774;&#35745;&#35774;&#32622;&#20013;&#65292;ERM&#19981;&#33021;&#34987;&#25490;&#38500;&#20026;&#19968;&#31181;&#26368;&#20248;&#26041;&#27861;&#65292;&#24182;&#23558;&#35813;&#32467;&#26524;&#25193;&#23637;&#21040;&#38543;&#26426;&#35774;&#35745;&#35774;&#32622;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#34920;&#26126;ERM&#30340;&#31283;&#23450;&#24615;&#65292;&#20026;Caponnetto&#21644;Rakhlin(2006)&#30340;&#38750;Donsker&#31867;&#30340;&#20027;&#35201;&#32467;&#26524;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that Empirical Risk Minimization (ERM) with squared loss may attain minimax suboptimal error rates (Birg\'e and Massart, 1993). The key message of this paper is that, under mild assumptions, the suboptimality of ERM must be due to large bias rather than variance. More precisely, in the bias-variance decomposition of the squared error of the ERM, the variance term necessarily enjoys the minimax rate. In the case of fixed design, we provide an elementary proof of this fact using the probabilistic method. Then, we prove this result for various models in the random design setting. In addition, we provide a simple proof of Chatterjee's admissibility theorem (Chatterjee, 2014, Theorem 1.4), which states that ERM cannot be ruled out as an optimal method, in the fixed design setting, and extend this result to the random design setting. We also show that our estimates imply stability of ERM, complementing the main result of Caponnetto and Rakhlin (2006) for non-Donsker classes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{S}^{d-1}$&#19978;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#23485;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#34920;&#26126;&#24403;&#23485;&#24230;$m\rightarrow\infty$&#26102;&#65292;&#27531;&#24046;&#32593;&#32476;&#26680;(RNK)&#32479;&#19968;&#25910;&#25947;&#21040;&#27531;&#24046;&#31070;&#32463;&#20999;&#21521;&#26680;(RNTK)&#65292;&#24182;&#19988;&#26089;&#20572;&#31574;&#30053;&#30340;&#23485;&#27531;&#24046;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#26497;&#23567;&#21270;&#36895;&#29575;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#26102;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2305.18506</link><description>&lt;p&gt;
&#23485;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Generalization Ability of Wide Residual Networks. (arXiv:2305.18506v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{S}^{d-1}$&#19978;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#23485;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#34920;&#26126;&#24403;&#23485;&#24230;$m\rightarrow\infty$&#26102;&#65292;&#27531;&#24046;&#32593;&#32476;&#26680;(RNK)&#32479;&#19968;&#25910;&#25947;&#21040;&#27531;&#24046;&#31070;&#32463;&#20999;&#21521;&#26680;(RNTK)&#65292;&#24182;&#19988;&#26089;&#20572;&#31574;&#30053;&#30340;&#23485;&#27531;&#24046;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#26497;&#23567;&#21270;&#36895;&#29575;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#26102;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{S}^{d-1}$&#19978;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#23485;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#65292;&#24403;&#23485;&#24230;$m\rightarrow\infty$&#26102;&#65292;&#27531;&#24046;&#32593;&#32476;&#26680;(RNK)&#32479;&#19968;&#25910;&#25947;&#21040;&#27531;&#24046;&#31070;&#32463;&#20999;&#21521;&#26680;(RNTK)&#12290;&#36825;&#31181;&#32479;&#19968;&#25910;&#25947;&#36827;&#19968;&#27493;&#20445;&#35777;&#20102;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#25910;&#25947;&#20110;&#30456;&#23545;&#20110;RNTK&#30340;&#26680;&#22238;&#24402;&#30340;&#35823;&#24046;&#12290;&#20316;&#20026;&#30452;&#25509;&#25512;&#35770;&#65292;&#25105;&#20204;&#25351;&#20986;&#65306;$i$)&#22914;&#26524;&#30446;&#26631;&#22238;&#24402;&#20989;&#25968;&#33853;&#22312;&#19982;RNTK&#30456;&#20851;&#32852;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#65292;&#37319;&#29992;&#26089;&#20572;&#31574;&#30053;&#30340;&#23485;&#27531;&#24046;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#26497;&#23567;&#21270;&#36895;&#29575;&#65307;$ii$)&#22914;&#26524;&#35757;&#32451;&#21040;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#65292;&#21017;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#23485;&#27531;&#24046;&#32593;&#32476;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#20123;&#23454;&#39564;&#26469;&#35843;&#21644;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#19982;&#24191;&#27867;&#35266;&#23519;&#21040;&#30340;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#8221;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the generalization ability of the wide residual network on $\mathbb{S}^{d-1}$ with the ReLU activation function. We first show that as the width $m\rightarrow\infty$, the residual network kernel (RNK) uniformly converges to the residual neural tangent kernel (RNTK). This uniform convergence further guarantees that the generalization error of the residual network converges to that of the kernel regression with respect to the RNTK. As direct corollaries, we then show $i)$ the wide residual network with the early stopping strategy can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space (RKHS) associated with the RNTK; $ii)$ the wide residual network can not generalize well if it is trained till overfitting the data. We finally illustrate some experiments to reconcile the contradiction between our theoretical result and the widely observed ``benign overfitting phenomenon''
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;</title><link>http://arxiv.org/abs/2305.18505</link><description>&lt;p&gt;
&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#33539;&#20363;&#65292;&#22312;&#27492;&#33539;&#20363;&#19979;&#65292;RL&#20195;&#29702;&#23398;&#20064;&#20351;&#29992;&#23545;&#36712;&#36857;&#30340;&#25104;&#23545;&#20248;&#20808;&#32423;&#21453;&#39304;&#26469;&#26368;&#20248;&#21270;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#26126;&#30830;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#23613;&#31649;RLHF&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#29992;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#23454;&#35777;&#30740;&#31350;&#24182;&#26410;&#35299;&#20915;&#22914;&#20309;&#39640;&#25928;&#37319;&#26679;&#36712;&#36857;&#23545;&#20197;&#26597;&#35810;&#20154;&#31867;&#21453;&#39304;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#25506;&#32034;&#24615;&#36712;&#36857;&#65292;&#22312;&#25910;&#38598;&#20219;&#20309;&#20154;&#31867;&#21453;&#39304;&#20043;&#21069;&#65292;&#20351;&#23398;&#20064;&#38544;&#34255;&#30340;&#22870;&#21169;&#20989;&#25968;&#26356;&#21152;&#20934;&#30830;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#22522;&#20110;&#20559;&#22909;&#27169;&#22411;&#19979;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#20154;&#31867;&#21453;&#39304;&#26356;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#32435;&#20837;&#32447;&#24615;&#21644;&#20302;&#31209;MDPs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#30340;&#21453;&#39304;&#30340;RLHF&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#20248;&#21270;&#20855;&#26377;&#26377;&#38480;&#21453;&#39304;&#30340;&#20219;&#21153;&#26102;&#33719;&#24471;&#25506;&#32034;&#24615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22312;AI&#20844;&#24179;&#39046;&#22495;&#30340;&#36830;&#32493;&#20445;&#25252;&#23646;&#24615;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#26063;&#25351;&#26631;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;HGR&#25351;&#26631;&#65292;&#22312;&#35821;&#20041;&#19978;&#26356;&#20026;&#34917;&#20805;&#65292;&#20855;&#26377;&#20805;&#20998;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#65292;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#40065;&#26834;&#65292;&#21487;&#37197;&#32622;&#20197;&#36866;&#24212;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#65292;&#24182;&#20801;&#35768;&#23450;&#20041;&#32454;&#31890;&#24230;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.18504</link><description>&lt;p&gt;
&#21487;&#37197;&#32622;&#20844;&#24179;&#24615;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#24191;&#20041;&#19981;&#21516;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Generalized Disparate Impact for Configurable Fairness Solutions in ML. (arXiv:2305.18504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18504
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22312;AI&#20844;&#24179;&#39046;&#22495;&#30340;&#36830;&#32493;&#20445;&#25252;&#23646;&#24615;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#26063;&#25351;&#26631;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;HGR&#25351;&#26631;&#65292;&#22312;&#35821;&#20041;&#19978;&#26356;&#20026;&#34917;&#20805;&#65292;&#20855;&#26377;&#20805;&#20998;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#65292;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#40065;&#26834;&#65292;&#21487;&#37197;&#32622;&#20197;&#36866;&#24212;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#65292;&#24182;&#20801;&#35768;&#23450;&#20041;&#32454;&#31890;&#24230;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#36830;&#32493;&#20445;&#25252;&#23646;&#24615;&#30340;AI&#20844;&#24179;&#39046;&#22495;&#20570;&#20102;&#20004;&#39033;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Hirschfeld-Gebelein-Renyi&#65288;HGR&#65289;&#25351;&#26631;&#65288;&#30446;&#21069;&#20165;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#65289;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#20294;&#21463;&#21040;&#19968;&#20123;&#20851;&#38190;&#38480;&#21046;&#65292;&#21253;&#25324;&#35821;&#20041;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#26063;&#25351;&#26631;&#65292;&#22312;&#35821;&#20041;&#19978;&#26159;HGR&#30340;&#34917;&#20805;&#65307;&#20855;&#26377;&#20805;&#20998;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#65307;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#40065;&#26834;&#65307;&#21487;&#37197;&#32622;&#20197;&#36866;&#24212;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20801;&#35768;&#25105;&#20204;&#23450;&#20041;&#32454;&#31890;&#24230;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#20801;&#35768;&#26576;&#20123;&#31867;&#22411;&#30340;&#20381;&#36182;&#24615;&#24182;&#26377;&#36873;&#25321;&#22320;&#31105;&#27490;&#20854;&#20182;&#31867;&#22411;&#30340;&#20381;&#36182;&#12290;&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#20445;&#25252;&#23646;&#24615;&#30340;&#21487;&#29992;&#36873;&#39033;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20844;&#24179;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#34920;&#31034;&#20986;&#20102;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We make two contributions in the field of AI fairness over continuous protected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR) indicator (the only one currently available for such a case) is valuable but subject to a few crucial limitations regarding semantics, interpretability, and robustness. Second, we introduce a family of indicators that are: 1) complementary to HGR in terms of semantics; 2) fully interpretable and transparent; 3) robust over finite samples; 4) configurable to suit specific applications. Our approach also allows us to define fine-grained constraints to permit certain types of dependence and forbid others selectively. By expanding the available options for continuous protected attributes, our approach represents a significant contribution to the area of fair artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21521;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#36716;&#22411;&#65292;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21183;&#12290;&#30740;&#31350;&#32773;&#20204;&#26681;&#25454;&#27169;&#22411;&#33021;&#21147;&#30830;&#23450;&#40065;&#26834;&#24615;&#35780;&#20272;&#32500;&#24230;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#32500;&#24230;&#25351;&#23450;&#21512;&#29702;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.18503</link><description>&lt;p&gt;
&#20174;&#23545;&#25239;&#24615;&#31454;&#20105;&#21040;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#35780;&#20215;&#65306;&#25512;&#21160;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework. (arXiv:2305.18503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21521;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#36716;&#22411;&#65292;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21183;&#12290;&#30740;&#31350;&#32773;&#20204;&#26681;&#25454;&#27169;&#22411;&#33021;&#21147;&#30830;&#23450;&#40065;&#26834;&#24615;&#35780;&#20272;&#32500;&#24230;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#32500;&#24230;&#25351;&#23450;&#21512;&#29702;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#21521;&#36755;&#20837;&#28155;&#21152;&#35821;&#20041;&#20445;&#30041;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#25200;&#21160;&#26469;&#21457;&#29616;&#27169;&#22411;&#30340;&#24369;&#28857;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#38271;&#26399;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#31454;&#20105;&#26159;&#31639;&#27861;&#20013;&#24515;&#30340;&#65292;&#20026;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25216;&#26415;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#23454;&#36341;&#21487;&#33021;&#23384;&#22312;&#35780;&#20272;&#19981;&#20840;&#38754;&#12289;&#35780;&#20272;&#21327;&#35758;&#19981;&#23454;&#29992;&#20197;&#21450;&#23545;&#25239;&#26679;&#26412;&#22833;&#25928;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21521;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#36716;&#22411;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#27169;&#22411;&#33021;&#21147;&#30830;&#23450;&#40065;&#26834;&#24615;&#35780;&#20272;&#32500;&#24230;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#32500;&#24230;&#25351;&#23450;&#21512;&#29702;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#21253;&#25324;&#35780;&#20272;&#35774;&#32622;&#21644;&#25351;&#26631;&#65292;&#20197;&#28385;&#36275;&#23454;&#38469;&#38656;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual adversarial attacks can discover models' weaknesses by adding semantic-preserved but misleading perturbations to the inputs. The long-lasting adversarial attack-and-defense arms race in Natural Language Processing (NLP) is algorithm-centric, providing valuable techniques for automatic robustness evaluation. However, the existing practice of robustness evaluation may exhibit issues of incomprehensive evaluation, impractical evaluation protocol, and invalid adversarial samples. In this paper, we aim to set up a unified automatic robustness evaluation framework, shifting towards model-centric evaluation to further exploit the advantages of adversarial attacks. To address the above challenges, we first determine robustness evaluation dimensions based on model capabilities and specify the reasonable algorithm to generate adversarial samples for each dimension. Then we establish the evaluation protocol, including evaluation settings and metrics, under realistic demands. Finally, we u
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#22312; SGD &#19979;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21333;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#21482;&#20250;&#22686;&#21152;&#19968;&#23450;&#22240;&#23376;&#30340;&#25910;&#25947;&#24615;&#65292;&#19981;&#21516;&#32500;&#24230;&#21644;&#23485;&#24230;&#30340;&#21069;&#32622;&#22240;&#23376;&#31934;&#30830;&#32467;&#26524;&#25581;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18502</link><description>&lt;p&gt;
&#36867;&#31163;&#24179;&#24248;&#65306;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22312; SGD &#19979;&#23398;&#20064;&#22256;&#38590;&#30340;&#21333;&#25351;&#26631;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Escaping mediocrity: how two-layer networks learn hard single-index models with SGD. (arXiv:2305.18502v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18502
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#22312; SGD &#19979;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21333;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#21482;&#20250;&#22686;&#21152;&#19968;&#23450;&#22240;&#23376;&#30340;&#25910;&#25947;&#24615;&#65292;&#19981;&#21516;&#32500;&#24230;&#21644;&#23485;&#24230;&#30340;&#21069;&#32622;&#22240;&#23376;&#31934;&#30830;&#32467;&#26524;&#25581;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#19979;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21333;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#21021;&#22987;&#21270;&#26102;&#23384;&#22312;&#35768;&#22810;&#24179;&#22374;&#26041;&#21521;&#30340;&#25361;&#25112;&#24615;&#24773;&#20917;&#12290;&#24050;&#32463;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#36890;&#24120;&#38656;&#35201; $n=O(d\log{d})$ &#20010;&#26679;&#26412;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#39640;&#32500;&#24230;&#21644;&#19981;&#21516;&#23485;&#24230;&#24773;&#20917;&#19979;&#30340;&#21069;&#32622;&#22240;&#23376;&#30340;&#31934;&#30830;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#31867;&#20013;&#65292;&#36807;&#21442;&#25968;&#21270;&#21482;&#20250;&#22686;&#21152;&#19968;&#23450;&#22240;&#23376;&#30340;&#25910;&#25947;&#24615;&#12290;&#36825;&#20123;&#35265;&#35299;&#22522;&#20110; SGD &#21160;&#24577;&#30340;&#20302;&#32500;&#24230;&#38543;&#26426;&#36807;&#31243;&#27169;&#22411;&#65292;&#20854;&#20013;&#36867;&#31163;&#24179;&#24248;&#31561;&#21516;&#20110;&#35745;&#31639;&#20986;&#31449;&#20986;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#36807;&#31243;&#30340;&#30830;&#23450;&#24615;&#36817;&#20284;&#36275;&#20197;&#20195;&#34920;&#36867;&#36920;&#26102;&#38388;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#38543;&#26426;&#24615;&#30340;&#20316;&#29992;&#21487;&#33021;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the sample complexity for two-layer neural networks to learn a single-index target function under Stochastic Gradient Descent (SGD), focusing on the challenging regime where many flat directions are present at initialization. It is well-established that in this scenario $n=O(d\log{d})$ samples are typically needed. However, we provide precise results concerning the pre-factors in high-dimensional contexts and for varying widths. Notably, our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class. These insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time. Yet, we demonstrate that a deterministic approximation of this process adequately represents the escape time, implying that the role of stochasticity may be minimal in this scenario.
&lt;/p&gt;</description></item><item><title>&#21452;&#37325;&#22810;&#27493;&#39588;&#31163;&#31574;&#30053;Actor-Critic&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#32467;&#21512;&#22810;&#27493;&#39588;&#31574;&#30053;&#25913;&#36827;&#21644;&#31574;&#30053;&#35780;&#20272;&#26469;&#20419;&#36827;&#26368;&#20248;&#25511;&#21046;&#65292;&#25552;&#39640;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#22312;&#19968;&#33324;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#35774;&#32622;&#20013;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.18501</link><description>&lt;p&gt;
DoMo-AC: &#21452;&#37325;&#22810;&#27493;&#39588;&#31163;&#31574;&#30053;Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoMo-AC: Doubly Multi-step Off-policy Actor-Critic Algorithm. (arXiv:2305.18501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18501
&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;&#22810;&#27493;&#39588;&#31163;&#31574;&#30053;Actor-Critic&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#32467;&#21512;&#22810;&#27493;&#39588;&#31574;&#30053;&#25913;&#36827;&#21644;&#31574;&#30053;&#35780;&#20272;&#26469;&#20419;&#36827;&#26368;&#20248;&#25511;&#21046;&#65292;&#25552;&#39640;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#22312;&#19968;&#33324;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#35774;&#32622;&#20013;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#39588;&#23398;&#20064;&#22312;&#31574;&#30053;&#35780;&#20272;&#20013;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#20294;&#22312;&#26368;&#20248;&#25511;&#21046;&#24773;&#20917;&#19979;&#65292;&#22810;&#27493;&#39588;&#23398;&#20064;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#65292;&#21407;&#22240;&#26159;&#22810;&#27493;&#39588;&#31574;&#30053;&#25913;&#36827;&#38656;&#35201;&#30340;&#25805;&#20316;&#26080;&#27861;&#29992;&#38543;&#26426;&#26679;&#26412;&#26469;&#36817;&#20284;&#65292;&#38459;&#30861;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#37325;&#22810;&#27493;&#39588;&#31163;&#31574;&#30053;VI(DoMo-VI)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;Oracle&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#27493;&#39588;&#31574;&#30053;&#25913;&#36827;&#21644;&#31574;&#30053;&#35780;&#20272;&#12290;DoMo-VI&#20445;&#35777;&#25910;&#25947;&#36895;&#24230;&#21152;&#36895;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#35774;&#32622;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;&#22810;&#27493;&#31163;&#31574;&#30053;actor-critic(DoMo-AC)&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#38469;&#30340;DoMo-VI&#31639;&#27861;&#23454;&#20363;&#12290;DoMo-AC&#24341;&#20837;&#20102;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#20197;&#30830;&#20445;&#25913;&#36827;&#30340;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;&#24403;&#19982;...
&lt;/p&gt;
&lt;p&gt;
Multi-step learning applies lookahead over multiple time steps and has proved valuable in policy evaluation settings. However, in the optimal control case, the impact of multi-step learning has been relatively limited despite a number of prior efforts. Fundamentally, this might be because multi-step policy improvements require operations that cannot be approximated by stochastic samples, hence hindering the widespread adoption of such methods in practice. To address such limitations, we introduce doubly multi-step off-policy VI (DoMo-VI), a novel oracle algorithm that combines multi-step policy improvements and policy evaluations. DoMo-VI enjoys guaranteed convergence speed-up to the optimal policy and is applicable in general off-policy learning settings. We then propose doubly multi-step off-policy actor-critic (DoMo-AC), a practical instantiation of the DoMo-VI algorithm. DoMo-AC introduces a bias-variance trade-off that ensures improved policy gradient estimates. When combined with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18500</link><description>&lt;p&gt;
VAST&#65306;&#19968;&#31181;&#35270;&#21548;&#23383;&#24149;&#25991;&#26412;&#20840;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23436;&#20840;&#25506;&#32034;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#65292;&#32780;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#20840;&#27169;&#24577;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;VAST-27M&#65292;&#24314;&#31435;&#22810;&#27169;&#24577;&#35270;&#39057;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#24182;&#19982;&#25991;&#26412;&#36827;&#34892;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;2700&#19975;&#20010;&#24320;&#25918;&#39046;&#22495;&#35270;&#39057;&#29255;&#27573;&#65292;&#24182;&#20998;&#21035;&#35757;&#32451;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#22120;&#20197;&#29983;&#25104;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29983;&#25104;&#30340;&#23383;&#24149;&#12289;&#23383;&#24149;&#21644;&#25351;&#23548;&#25552;&#31034;&#38598;&#25104;&#21040;&#20840;&#27169;&#24577;&#23383;&#24149;&#20013;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;VAST-27M&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#65292;&#23427;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#24182;&#26356;&#22909;&#22320;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#37326;&#22806;&#35270;&#39057;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#19990;&#30028;&#27169;&#22411;&#65288;ContextWM&#65289;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;ContextWM&#37319;&#29992;&#19978;&#19979;&#25991;&#21270;&#25193;&#23637;&#30340;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#24314;&#27169;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#39640;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25511;&#21046;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18499</link><description>&lt;p&gt;
&#22522;&#20110;&#37326;&#22806;&#35270;&#39057;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning. (arXiv:2305.18499v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#37326;&#22806;&#35270;&#39057;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#19990;&#30028;&#27169;&#22411;&#65288;ContextWM&#65289;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;ContextWM&#37319;&#29992;&#19978;&#19979;&#25991;&#21270;&#25193;&#23637;&#30340;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#24314;&#27169;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#39640;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25511;&#21046;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#22312;&#21508;&#31181;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35843;&#26597;&#20102;&#36825;&#31181;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#20165;&#38480;&#20110;&#29305;&#23450;&#39046;&#22495;&#25110;&#27169;&#25311;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#37327;&#37326;&#22806;&#35270;&#39057;&#36827;&#34892;&#39044;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#23398;&#20064;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#37326;&#22806;&#35270;&#39057;&#23384;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#22240;&#32032;&#65292;&#22914;&#38169;&#32508;&#22797;&#26434;&#30340;&#32972;&#26223;&#21644;&#32441;&#29702;&#22806;&#35266;&#65292;&#36825;&#20351;&#24471;&#19990;&#30028;&#27169;&#22411;&#26080;&#27861;&#25552;&#21462;&#20849;&#20139;&#30340;&#19990;&#30028;&#30693;&#35782;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#21270;&#19990;&#30028;&#27169;&#22411;&#65288;ContextWM&#65289;&#65292;&#26174;&#24335;&#22320;&#23545;&#19978;&#19979;&#25991;&#21644;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#20811;&#26381;&#37326;&#22806;&#35270;&#39057;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#20419;&#36827;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#21270;&#25193;&#23637;&#30340;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#26469;&#25429;&#25417;&#39640;&#32423;&#29366;&#24577;&#21644;&#20302;&#32423;&#35266;&#23519;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#37326;&#22806;&#35270;&#39057;&#23545;ContextWM&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#27604;&#65292;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25511;&#21046;&#24615;&#33021;&#22343;&#24471;&#21040;&#20102;&#26174;&#30528;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training methods utilizing large and diverse datasets have achieved tremendous success across a range of domains. Recent work has investigated such unsupervised pre-training methods for model-based reinforcement learning (MBRL) but is limited to domain-specific or simulated data. In this paper, we study the problem of pre-training world models with abundant in-the-wild videos for efficient learning of downstream visual control tasks. However, in-the-wild videos are complicated with various contextual factors, such as intricate backgrounds and textured appearance, which precludes a world model from extracting shared world knowledge to generalize better. To tackle this issue, we introduce Contextualized World Models (ContextWM) that explicitly model both the context and dynamics to overcome the complexity and diversity of in-the-wild videos and facilitate knowledge transfer between distinct scenes. Specifically, a contextualized extension of the latent dynamics model is 
&lt;/p&gt;</description></item><item><title>ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.18498</link><description>&lt;p&gt;
ANPL&#65306;&#20351;&#29992;&#20132;&#20114;&#24335;&#20998;&#35299;&#32534;&#35793;&#33258;&#28982;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18498
&lt;/p&gt;
&lt;p&gt;
ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#22312;&#36890;&#36807;&#33258;&#28982;&#20132;&#20114;&#22686;&#24378;&#32534;&#31243;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#23558;&#24120;&#35265;&#30340;&#20351;&#29992;&#27169;&#24335;&#32534;&#35793;&#20026;&#32534;&#31243;&#35821;&#35328;&#65292;&#20363;&#22914;Python&#65292;&#20294;&#22914;&#20309;&#32534;&#36753;&#21644;&#35843;&#35797;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31243;&#24207;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ANPL&#65292;&#19968;&#31181;&#32534;&#31243;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#20998;&#35299;&#29305;&#23450;&#20110;&#29992;&#25143;&#30340;&#20219;&#21153;&#12290;&#22312;ANPL&#31243;&#24207;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#35813;&#33609;&#22270;&#25351;&#23450;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#25968;&#25454;&#27969;&#12290;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#23558;&#29983;&#25104;&#21151;&#33021;&#30340;&#26114;&#36149;&#20219;&#21153;&#21368;&#36733;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#32473;&#23450;&#19968;&#20010;ANPL&#31243;&#24207;&#65292;ANPL&#32534;&#35793;&#22120;&#20250;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23454;&#29616;&#23380;&#20013;&#30340;&#21151;&#33021;&#65292;&#24182;&#36981;&#23432;&#33609;&#22270;&#20013;&#25351;&#23450;&#30340;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#23558;ANPL&#37096;&#32626;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#65292;&#23427;&#26159;&#19968;&#32452;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#29305;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advents of Large Language Models (LLMs) have shown promise in augmenting programming using natural interactions. However, while LLMs are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an LLM-generated program. We introduce ANPL, a programming system that allows users to decompose user-specific tasks. In an ANPL program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the LLM. Given an ANPL program, the ANPL compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline p
&lt;/p&gt;</description></item><item><title>&#27492;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20030;&#38598;&#23725;&#20272;&#35745;&#22120;&#20013;&#23376;&#37319;&#26679;&#21644;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#21457;&#29616;&#20108;&#32773;&#22312;&#19968;&#23450;&#36335;&#24452;&#20013;&#26159;&#28176;&#36817;&#31561;&#20215;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#27861;&#30830;&#23450;&#31561;&#20215;&#36335;&#24452;&#65292;&#38388;&#25509;&#35299;&#20915;&#20102;&#23725;&#22238;&#24402;&#35843;&#20248;&#20013;&#39044;&#27979;&#39118;&#38505;&#21333;&#35843;&#24615;&#30340;&#24433;&#21709;&#22240;&#32032;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18496</link><description>&lt;p&gt;
&#23376;&#37319;&#26679;&#19982;&#23725;&#22238;&#24402;&#30340;&#24191;&#20041;&#31561;&#20215;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalized equivalences between subsampling and ridge regularization. (arXiv:2305.18496v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18496
&lt;/p&gt;
&lt;p&gt;
&#27492;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20030;&#38598;&#23725;&#20272;&#35745;&#22120;&#20013;&#23376;&#37319;&#26679;&#21644;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#21457;&#29616;&#20108;&#32773;&#22312;&#19968;&#23450;&#36335;&#24452;&#20013;&#26159;&#28176;&#36817;&#31561;&#20215;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#27861;&#30830;&#23450;&#31561;&#20215;&#36335;&#24452;&#65292;&#38388;&#25509;&#35299;&#20915;&#20102;&#23725;&#22238;&#24402;&#35843;&#20248;&#20013;&#39044;&#27979;&#39118;&#38505;&#21333;&#35843;&#24615;&#30340;&#24433;&#21709;&#22240;&#32032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20030;&#38598;&#23725;&#20272;&#35745;&#22120;&#65292;&#24314;&#31435;&#20102;&#23376;&#37319;&#26679;&#21644;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#31934;&#30830;&#32467;&#26500;&#21644;&#39118;&#38505;&#31561;&#20215;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#24403;&#29992;&#19981;&#21516;&#30340;&#23725;&#27491;&#21017;&#21270;&#27700;&#24179;$\lambda$&#21644;&#23376;&#37319;&#26679;&#27604;&#20363;$\psi$&#25311;&#21512;&#23376;&#26679;&#23725;&#20272;&#35745;&#22120;&#30340;&#32447;&#24615;&#21644;&#20108;&#27425;&#27867;&#20989;&#65292;&#22312;$(\lambda,\psi)$-&#24179;&#38754;&#19978;&#27839;&#30528;&#29305;&#23450;&#36335;&#24452;&#28176;&#36817;&#31561;&#20215;&#65288;&#20854;&#20013;$\psi$&#26159;&#29305;&#24449;&#32500;&#24230;&#19982;&#23376;&#37319;&#26679;&#22823;&#23567;&#30340;&#27604;&#29575;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#35201;&#27714;&#29305;&#24449;&#21644;&#21709;&#24212;&#20998;&#24067;&#20855;&#26377;&#26377;&#30028;&#30697;&#65292;&#24182;&#20801;&#35768;&#20219;&#24847;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;$(\lambda,\psi)$&#30340;&#31561;&#20215;&#36335;&#24452;&#12290;&#25105;&#20204;&#32467;&#26524;&#30340;&#38388;&#25509;&#21547;&#20041;&#26159;&#65292;&#22312;&#25968;&#25454;&#26041;&#38754;&#27604;&#20363;&#20013;&#65292;&#35843;&#20248;&#30340;&#23725;&#22238;&#24402;&#21576;&#29616;&#20986;&#21333;&#35843;&#39044;&#27979;&#39118;&#38505;&#12290;&#36825;&#35299;&#20915;&#20102;Nakkiran&#31561;&#20154;&#25552;&#20986;&#30340;&#19968;&#20010;&#36817;&#26399;&#26410;&#35299;&#20915;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#22312;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#21644;&#28201;&#21644;&#30340;&#27491;&#21017;&#26465;&#20214;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish precise structural and risk equivalences between subsampling and ridge regularization for ensemble ridge estimators. Specifically, we prove that linear and quadratic functionals of subsample ridge estimators, when fitted with different ridge regularization levels $\lambda$ and subsample aspect ratios $\psi$, are asymptotically equivalent along specific paths in the $(\lambda, \psi )$-plane (where $\psi$ is the ratio of the feature dimension to the subsample size). Our results only require bounded moment assumptions on feature and response distributions and allow for arbitrary joint distributions. Furthermore, we provide a datadependent method to determine the equivalent paths of $(\lambda, \psi )$. An indirect implication of our equivalences is that optimally-tuned ridge regression exhibits a monotonic prediction risk in the data aspect ratio. This resolves a recent open problem raised by Nakkiran et al. under general data distributions and a mild regularity condition that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24212;&#29992;dropout&#12289;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#21644;&#27491;&#21017;&#21270;&#31561;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#34987;&#21160;TiO2 ReRAM&#20132;&#21449;&#26834;&#30828;&#20214;&#20256;&#36755;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#31934;&#24230;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.18495</link><description>&lt;p&gt;
&#25552;&#39640;Ex-Situ&#31070;&#32463;&#32593;&#32476;&#20256;&#36755;&#21040;&#34987;&#21160;TiO2 ReRAM&#21313;&#23383;&#26550;&#19978;&#30340;&#40065;&#26834;&#24615;&#30340;&#30828;&#20214;&#24863;&#30693;&#35757;&#32451;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Hardware-aware Training Techniques for Improving Robustness of Ex-Situ Neural Network Transfer onto Passive TiO2 ReRAM Crossbars. (arXiv:2305.18495v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24212;&#29992;dropout&#12289;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#21644;&#27491;&#21017;&#21270;&#31561;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#34987;&#21160;TiO2 ReRAM&#20132;&#21449;&#26834;&#30828;&#20214;&#20256;&#36755;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#31934;&#24230;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#21160;&#30005;&#38459;&#24335;&#38543;&#26426;&#23384;&#21462;&#23384;&#20648;&#22120;&#65288;ReRAM&#65289;&#21313;&#23383;&#26550;&#38453;&#21015;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#20852;&#25216;&#26415;&#65292;&#29992;&#20110;&#27169;&#25311;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65292;&#19982;&#20027;&#21160;&#30005;&#38459;&#24335;&#26230;&#20307;&#31649;&#19982;&#30005;&#38459;&#22120;&#30340;&#32452;&#21512;&#65288;1T1R&#65289;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#38598;&#25104;&#23494;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30828;&#20214;&#21464;&#24322;&#24615;&#65292;&#22914;&#20599;&#36305;&#36335;&#24452;&#30005;&#27969;&#12289;&#20559;&#32622;&#26041;&#26696;&#25928;&#24212;&#21644;&#30005;&#23548;&#35843;&#35856;&#19981;&#20934;&#30830;&#31561;&#65292;&#24403;&#21069;&#23558;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#20256;&#36755;&#21040;&#21313;&#23383;&#26550;&#32467;&#26500;&#20013;&#23384;&#20648;&#22120;&#35774;&#22791;&#30340;&#30005;&#23548;&#24577;&#26102;&#31934;&#24230;&#26377;&#26174;&#33879;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;dropout&#12289;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#21644;&#27491;&#21017;&#21270;&#31561;&#26041;&#27861;&#29992;&#20110;&#36866;&#24212;TiO2&#20132;&#21449;&#26834;&#30340;&#21464;&#24322;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#36866;&#21512;&#20854;&#30828;&#20214;&#20256;&#36755;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;&#30828;&#20214;&#27169;&#25311;&#22120;&#65292;&#22312;&#25968;&#21315;&#27425;&#26435;&#37325;&#20256;&#36755;&#20013;&#27604;&#36739;&#25152;&#25552;&#20986;&#30340;&#30828;&#20214;&#24863;&#30693;&#32593;&#32476;&#21644;&#24120;&#35268;&#20840;&#36830;&#25509;&#32593;&#32476;&#30340;&#36755;&#20986;&#21644;&#31934;&#24230;&#26469;&#28436;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passive resistive random access memory (ReRAM) crossbar arrays, a promising emerging technology used for analog matrix-vector multiplications, are far superior to their active (1T1R) counterparts in terms of the integration density. However, current transfers of neural network weights into the conductance state of the memory devices in the crossbar architecture are accompanied by significant losses in precision due to hardware variabilities such as sneak path currents, biasing scheme effects and conductance tuning imprecision. In this work, training approaches that adapt techniques such as dropout, the reparametrization trick and regularization to TiO2 crossbar variabilities are proposed in order to generate models that are better adapted to their hardware transfers. The viability of this approach is demonstrated by comparing the outputs and precision of the proposed hardware-aware network with those of a regular fully connected network over a few thousand weight transfers using the ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363; ExactSDM &#21644; SoftSDM&#65292;&#23558;&#39034;&#24207;&#20381;&#36182;&#27169;&#22411; (SDM) &#36866;&#24212;&#21040;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034; (LSR) &#20013;&#65292;&#20197;&#35299;&#20915;&#38271;&#31687;&#25991;&#26723;&#26816;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312; MSMARCO &#25991;&#26723;&#21644; TREC Robust04 &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ExactSDM &#21644; SoftSDM &#37117;&#20248;&#20110;&#29616;&#26377;&#30340; LSR &#32858;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18494</link><description>&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#22312;&#38271;&#25991;&#26723;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adapting Learned Sparse Retrieval for Long Documents. (arXiv:2305.18494v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363; ExactSDM &#21644; SoftSDM&#65292;&#23558;&#39034;&#24207;&#20381;&#36182;&#27169;&#22411; (SDM) &#36866;&#24212;&#21040;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034; (LSR) &#20013;&#65292;&#20197;&#35299;&#20915;&#38271;&#31687;&#25991;&#26723;&#26816;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312; MSMARCO &#25991;&#26723;&#21644; TREC Robust04 &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ExactSDM &#21644; SoftSDM &#37117;&#20248;&#20110;&#29616;&#26377;&#30340; LSR &#32858;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#26816;&#32034; (LSR) &#26159;&#19968;&#31181;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#25442;&#25104;&#19982;&#35789;&#27719;&#34920;&#23545;&#40784;&#30340;&#31232;&#30095;&#26435;&#37325;&#21521;&#37327;&#30340;&#31070;&#32463;&#26816;&#32034;&#26041;&#27861;&#12290;&#34429;&#28982;&#20687; Splade &#36825;&#26679;&#30340; LSR &#26041;&#27861;&#22312;&#30701;&#25991;&#27573;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#26356;&#38271;&#30340;&#25991;&#26723;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36866;&#24212; LSR &#21040;&#38271;&#25991;&#26723;&#30340;&#29616;&#26377;&#32858;&#21512;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25509;&#36817;&#25171;&#20998;&#23545;&#20110; LSR &#22788;&#29702;&#38271;&#25991;&#26723;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23558;&#39034;&#24207;&#20381;&#36182;&#27169;&#22411; (SDM) &#36866;&#24212;&#21040; LSR &#30340;&#26041;&#27861;&#65306;ExactSDM &#21644; SoftSDM&#12290;ExactSDM &#20551;&#23450;&#21482;&#26377;&#31934;&#30830;&#30340;&#26597;&#35810;&#39033;&#20381;&#36182;&#24615;&#65292;&#32780; SoftSDM &#20351;&#29992;&#28508;&#22312;&#20989;&#25968;&#23545;&#26597;&#35810;&#39033;&#21450;&#20854;&#25193;&#23637;&#39033; (&#21363;&#20351;&#29992;&#36716;&#25442;&#22120;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#22836;&#35782;&#21035;&#30340;&#39033;) &#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312; MSMARCO &#25991;&#26723;&#21644; TREC Robust04 &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ExactSDM &#21644; SoftSDM &#37117;&#20248;&#20110;&#29616;&#26377;&#30340; LSR &#32858;&#21512;&#26041;&#27861;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#25991;&#26723;&#38271;&#24230;&#32422;&#26463;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned sparse retrieval (LSR) is a family of neural retrieval methods that transform queries and documents into sparse weight vectors aligned with a vocabulary. While LSR approaches like Splade work well for short passages, it is unclear how well they handle longer documents. We investigate existing aggregation approaches for adapting LSR to longer documents and find that proximal scoring is crucial for LSR to handle long documents. To leverage this property, we proposed two adaptations of the Sequential Dependence Model (SDM) to LSR: ExactSDM and SoftSDM. ExactSDM assumes only exact query term dependence, while SoftSDM uses potential functions that model the dependence of query terms and their expansion terms (i.e., terms identified using a transformer's masked language modeling head).  Experiments on the MSMARCO Document and TREC Robust04 datasets demonstrate that both ExactSDM and SoftSDM outperform existing LSR aggregation approaches for different document length constraints. Surp
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22522;&#20110;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#32771;&#34385;&#20102;&#33021;&#28304;&#21644;&#20449;&#21495;&#34928;&#20943;&#31561;&#22240;&#32032;&#65292;&#20026;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.18493</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Insights from the Design Space Exploration of Flow-Guided Nanoscale Localization. (arXiv:2305.18493v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18493
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22522;&#20110;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#32771;&#34385;&#20102;&#33021;&#28304;&#21644;&#20449;&#21495;&#34928;&#20943;&#31561;&#22240;&#32032;&#65292;&#20026;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22826;&#36203;&#20857;&#26080;&#32447;&#36890;&#20449;&#33021;&#21147;&#30340;&#32435;&#31859;&#35774;&#22791;&#20026;&#22312;&#20154;&#31867;&#34880;&#28082;&#20013;&#36827;&#34892;&#27969;&#23548;&#21521;&#23450;&#20301;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#27492;&#31867;&#23450;&#20301;&#20351;&#24471;&#23558;&#25152;&#24863;&#21463;&#21040;&#30340;&#20107;&#20214;&#30340;&#20301;&#32622;&#19982;&#20107;&#20214;&#26412;&#36523;&#36827;&#34892;&#21305;&#37197;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31934;&#20934;&#21307;&#30103;&#26041;&#38754;&#30340;&#26089;&#26399;&#21644;&#31934;&#20934;&#35786;&#26029;&#12289;&#38477;&#20302;&#25104;&#26412;&#21644;&#20405;&#20837;&#24615;&#12290;&#27969;&#23548;&#21521;&#23450;&#20301;&#20173;&#22788;&#20110;&#21407;&#22987;&#38454;&#27573;&#65292;&#21482;&#26377;&#23569;&#25968;&#35770;&#25991;&#28041;&#21450;&#27492;&#38382;&#39064;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#35780;&#20272;&#20173;&#28982;&#20197;&#38750;&#26631;&#20934;&#21270;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#36890;&#24120;&#21482;&#32771;&#34385;&#21333;&#19968;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24573;&#30053;&#20102;&#22312;&#36825;&#31181;&#35268;&#27169;&#65288;&#20363;&#22914;&#65292;&#32435;&#31859;&#22120;&#20214;&#30340;&#33021;&#37327;&#21463;&#38480;&#65289;&#21644;&#23545;&#20110;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#20307;&#20869;&#22826;&#36203;&#20857;&#20256;&#25773;&#30340;&#20005;&#37325;&#34928;&#20943;&#65289;&#19979;&#30456;&#20851;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#35780;&#20272;&#20855;&#26377;&#20302;&#27700;&#24179;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#20197;&#23458;&#35266;&#30340;&#26041;&#24335;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20256;&#36755;&#33021;&#37327;&#28040;&#32791;&#21644;&#20449;&#21495;&#34928;&#20943;&#65292;&#23545;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32771;&#34385;&#20102;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#65288;&#20363;&#22914;&#33021;&#37327;&#28040;&#32791;&#21644;&#23450;&#20301;&#31934;&#24230;&#65289;&#21644;&#25361;&#25112;&#65288;&#20363;&#22914;&#36523;&#20307;&#36816;&#21160;&#21644;&#34880;&#21387;&#65289;&#65292;&#23548;&#33268;&#25105;&#20204;&#21487;&#20197;&#20026;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#25552;&#20379;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nanodevices with Terahertz (THz)-based wireless communication capabilities are providing a primer for flow-guided localization within the human bloodstreams. Such localization is allowing for assigning the locations of sensed events with the events themselves, providing benefits in precision medicine along the lines of early and precise diagnostics, and reduced costs and invasiveness. Flow-guided localization is still in a rudimentary phase, with only a handful of works targeting the problem. Nonetheless, the performance assessments of the proposed solutions are already carried out in a non-standardized way, usually along a single performance metric, and ignoring various aspects that are relevant at such a scale (e.g., nanodevices' limited energy) and for such a challenging environment (e.g., extreme attenuation of in-body THz propagation). As such, these assessments feature low levels of realism and cannot be compared in an objective way. Toward addressing this issue, we account for t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#37327;&#25104;&#23545;&#26679;&#20363;&#30340;&#20391;&#20449;&#24687;&#30452;&#25509;&#23398;&#20064;&#25968;&#25454;&#32858;&#31867;&#30340;&#26041;&#27861;DMS&#65292;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26080;&#38656;&#30693;&#36947;&#31867;&#21035;&#25968;&#12289;&#31867;&#20013;&#24515;&#25110;&#32773;&#20219;&#20309;&#30456;&#20284;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20391;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#27714;&#23558;&#30456;&#21516;&#30340;&#25968;&#25454;&#28857;&#20998;&#25104;&#19981;&#21516;&#30340;&#32858;&#31867;&#65292;&#19988;&#22312;&#22266;&#26377;&#30340;&#21644;&#38750;&#22266;&#26377;&#30340;&#25968;&#25454;&#38598;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18492</link><description>&lt;p&gt;
DMS&#65306;&#22522;&#20110;&#20391;&#20449;&#24687;&#30340;&#26080;&#38656;&#30693;&#36947;&#31867;&#21035;&#25968;&#21644;&#36317;&#31163;&#24230;&#37327;&#30340;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific Clustering Using Side Information. (arXiv:2305.18492v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#37327;&#25104;&#23545;&#26679;&#20363;&#30340;&#20391;&#20449;&#24687;&#30452;&#25509;&#23398;&#20064;&#25968;&#25454;&#32858;&#31867;&#30340;&#26041;&#27861;DMS&#65292;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26080;&#38656;&#30693;&#36947;&#31867;&#21035;&#25968;&#12289;&#31867;&#20013;&#24515;&#25110;&#32773;&#20219;&#20309;&#30456;&#20284;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20391;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#27714;&#23558;&#30456;&#21516;&#30340;&#25968;&#25454;&#28857;&#20998;&#25104;&#19981;&#21516;&#30340;&#32858;&#31867;&#65292;&#19988;&#22312;&#22266;&#26377;&#30340;&#21644;&#38750;&#22266;&#26377;&#30340;&#25968;&#25454;&#38598;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#25104;&#23545;&#26679;&#20363;&#30340;&#20391;&#20449;&#24687;&#30452;&#25509;&#23398;&#20064;&#25968;&#25454;&#32858;&#31867;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26080;&#38656;&#30693;&#36947;&#31867;&#21035;&#25968;&#12289;&#31867;&#20013;&#24515;&#25110;&#32773;&#20219;&#20309;&#30456;&#20284;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20391;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#27714;&#23558;&#30456;&#21516;&#30340;&#25968;&#25454;&#28857;&#20998;&#25104;&#19981;&#21516;&#30340;&#32858;&#31867;&#12290;&#21463;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#33258;&#23450;&#20041;&#30340;&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#32858;&#31867;&#26041;&#27861;&#8212;&#8212;Differentiable Mean Shift (DMS)&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#26080;&#20851;&#32858;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#24378;&#22823;&#30340;&#32858;&#31867;&#23450;&#20041;&#65292;&#32780;&#19981;&#24517;&#24378;&#21046;&#35201;&#27714;&#27599;&#20010;&#31751;&#22312;&#35757;&#32451;&#26399;&#38388;&#21576;&#29616;&#12290;DMS&#22312;&#22266;&#26377;&#30340;&#21644;&#38750;&#22266;&#26377;&#30340;&#25968;&#25454;&#38598;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach, in which we learn to cluster data directly from side information, in the form of a small set of pairwise examples. Unlike previous methods, with or without side information, we do not need to know the number of clusters, their centers or any kind of distance metric for similarity. Our method is able to divide the same data points in various ways dependant on the needs of a specific task, defined by the side information. Contrastingly, other work generally finds only the intrinsic, most obvious, clusters. Inspired by the mean shift algorithm, we implement our new clustering approach using a custom iterative neural network to create Differentiable Mean Shift (DMS), a state of the art, dataset agnostic, clustering method. We found that it was possible to train a strong cluster definition without enforcing a constraint that each cluster must be presented during training. DMS outperforms current methods in both the intrinsic and non-intrinsic dataset tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;TD&#23398;&#20064;&#20013;&#23545;&#34920;&#31034;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#31471;&#21040;&#31471;TD&#23398;&#20064;&#22312;&#29615;&#22659;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#20005;&#26684;&#38477;&#20302;&#20540;&#36924;&#36817;&#35823;&#24046;&#65292;&#22312;&#29615;&#22659;&#36827;&#19968;&#27493;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#34920;&#31034;&#21160;&#24577;&#36830;&#25509;&#21040;&#36716;&#31227;&#30697;&#38453;&#30340;&#35889;&#20998;&#35299;&#12290;&#20174;&#38543;&#26426;&#29983;&#25104;&#30340;&#22870;&#21169;&#20013;&#36866;&#21512;&#22810;&#20010;&#20540;&#20989;&#25968;&#20316;&#20026;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#29992;&#36741;&#21161;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18491</link><description>&lt;p&gt;
TD&#23398;&#20064;&#20013;&#23545;&#34920;&#31034;&#21160;&#24577;&#30340;&#26356;&#22909;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards a Better Understanding of Representation Dynamics under TD-learning. (arXiv:2305.18491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;TD&#23398;&#20064;&#20013;&#23545;&#34920;&#31034;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#31471;&#21040;&#31471;TD&#23398;&#20064;&#22312;&#29615;&#22659;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#20005;&#26684;&#38477;&#20302;&#20540;&#36924;&#36817;&#35823;&#24046;&#65292;&#22312;&#29615;&#22659;&#36827;&#19968;&#27493;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#34920;&#31034;&#21160;&#24577;&#36830;&#25509;&#21040;&#36716;&#31227;&#30697;&#38453;&#30340;&#35889;&#20998;&#35299;&#12290;&#20174;&#38543;&#26426;&#29983;&#25104;&#30340;&#22870;&#21169;&#20013;&#36866;&#21512;&#22810;&#20010;&#20540;&#20989;&#25968;&#20316;&#20026;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#29992;&#36741;&#21161;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TD&#23398;&#20064;&#26159;&#20540;&#39044;&#27979;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20540;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#19982;&#29366;&#24577;&#34920;&#31034;&#30340;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#32771;&#34385;&#19968;&#20010;&#38382;&#39064;&#65306;&#31471;&#21040;&#31471;TD&#23398;&#20064;&#22914;&#20309;&#38543;&#26102;&#38388;&#24433;&#21709;&#34920;&#31034;&#65311;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;TD&#23398;&#20064;&#19979;&#30340;&#34920;&#31034;&#21160;&#24577;&#12290;&#22312;&#29615;&#22659;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#31471;&#21040;&#31471;TD&#23398;&#20064;&#21487;&#20005;&#26684;&#38477;&#20302;&#26102;&#38388;&#19978;&#30340;&#20540;&#36924;&#36817;&#35823;&#24046;&#12290;&#22312;&#29615;&#22659;&#36827;&#19968;&#27493;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#34920;&#31034;&#21160;&#24577;&#36830;&#25509;&#21040;&#36716;&#31227;&#30697;&#38453;&#30340;&#35889;&#20998;&#35299;&#12290;&#35813;&#21457;&#29616;&#35777;&#23454;&#20102;&#20174;&#38543;&#26426;&#29983;&#25104;&#30340;&#22870;&#21169;&#20013;&#36866;&#21512;&#22810;&#20010;&#20540;&#20989;&#25968;&#20316;&#20026;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#29992;&#36741;&#21161;&#20219;&#21153;&#65292;&#25105;&#20204;&#22312;&#34920;&#26684;&#21644;Atari&#28216;&#25103;&#22871;&#20214;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
TD-learning is a foundation reinforcement learning (RL) algorithm for value prediction. Critical to the accuracy of value predictions is the quality of state representations. In this work, we consider the question: how does end-to-end TD-learning impact the representation over time? Complementary to prior work, we provide a set of analysis that sheds further light on the representation dynamics under TD-learning. We first show that when the environments are reversible, end-to-end TD-learning strictly decreases the value approximation error over time. Under further assumptions on the environments, we can connect the representation dynamics with spectral decomposition over the transition matrix. This latter finding establishes fitting multiple value functions from randomly generated rewards as a useful auxiliary task for representation learning, as we empirically validate on both tabular and Atari game suites.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38160;&#24230;&#35843;&#25972;&#30340;&#30340;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#26799;&#24230;&#19979;&#38477;SANE&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#36136;&#37327;&#30340;&#26377;&#25928;&#32500;&#25968;&#24230;&#37327;&#65292;&#24182;&#19988;&#23545;&#22823;&#23398;&#20064;&#29575;&#20063;&#26377;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18490</link><description>&lt;p&gt;
&#22522;&#20110;&#38160;&#24230;&#35843;&#25972;&#30340;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#26799;&#24230;&#19979;&#38477;SANE&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SANE: The phases of gradient descent through Sharpness Adjusted Number of Effective parameters. (arXiv:2305.18490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38160;&#24230;&#35843;&#25972;&#30340;&#30340;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#26799;&#24230;&#19979;&#38477;SANE&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#36136;&#37327;&#30340;&#26377;&#25928;&#32500;&#25968;&#24230;&#37327;&#65292;&#24182;&#19988;&#23545;&#22823;&#23398;&#20064;&#29575;&#20063;&#26377;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#25104;&#21151;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35843;&#26597;&#20102;&#25439;&#22833;&#38754;&#26354;&#29575;&#22914;&#20309;&#24433;&#21709;&#35299;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26399;&#38388;&#30340;Hessian&#30697;&#38453;&#12290;&#25105;&#20204;&#37325;&#30003;&#20102;&#8220;&#30830;&#23450;&#33391;&#22909;&#8221;&#25110;&#8220;&#26377;&#25928;&#8221;&#21442;&#25968;&#30340;&#25968;&#37327;&#19982;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23558;&#20854;&#28436;&#31034;&#20026;&#27169;&#22411;&#27604;&#36739;&#24037;&#20855;&#12290;&#36890;&#36807;&#32771;&#34385;&#23616;&#37096;&#26354;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sharpness Adjusted Number of Effective parameters (SANE)&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#35299;&#36136;&#37327;&#30340;&#26377;&#25928;&#32500;&#25968;&#24230;&#37327;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;SANE&#23545;&#22823;&#23398;&#20064;&#29575;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#20195;&#34920;&#20102;&#26377;&#21560;&#24341;&#21147;&#20294;&#22768;&#21517;&#29436;&#34249;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#24182;&#34920;&#24449;&#20102;&#22823;&#23398;&#20064;&#29575;&#19979;&#8220;&#25439;&#22833;&#30406;&#22320;&#8221;&#30340;Hessian&#30697;&#38453;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25193;&#23637;&#25105;&#20204;&#30340;&#20998;&#26512;&#21040;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20840;&#32593;&#32476;Hessian&#30697;&#38453;&#30340;&#36817;&#20284;&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#33258;&#28982;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural networks are undeniably successful. Numerous studies have investigated how the curvature of loss landscapes can affect the quality of solutions. In this work we consider the Hessian matrix during network training. We reiterate the connection between the number of "well-determined" or "effective" parameters and the generalisation performance of neural nets, and we demonstrate its use as a tool for model comparison. By considering the local curvature, we propose Sharpness Adjusted Number of Effective parameters (SANE), a measure of effective dimensionality for the quality of solutions. We show that SANE is robust to large learning rates, which represent learning regimes that are attractive but (in)famously unstable. We provide evidence and characterise the Hessian shifts across "loss basins" at large learning rates. Finally, extending our analysis to deeper neural networks, we provide an approximation to the full-network Hessian, exploiting the natural ordering of neural we
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#26234;&#33021;&#25163;&#26426;&#22270;&#20687;&#20013;&#26816;&#27979;&#29492;&#30168;&#30149;&#27602;&#65292;&#21487;&#20197;&#24110;&#21161;&#20302;&#25910;&#20837;&#22269;&#23478;&#21644;&#19981;&#20855;&#22791;&#30456;&#24212;&#30123;&#33495;&#21644;&#26816;&#27979;&#35774;&#26045;&#30340;&#21306;&#22495;&#36827;&#34892;&#21021;&#27493;&#31579;&#26597;&#12290;</title><link>http://arxiv.org/abs/2305.18489</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#26816;&#27979;&#26234;&#33021;&#25163;&#26426;&#22270;&#29255;&#20013;&#30340;&#29492;&#30168;&#30149;&#27602;
&lt;/p&gt;
&lt;p&gt;
A Transfer Learning and Explainable Solution to Detect mpox from Smartphones images. (arXiv:2305.18489v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18489
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#26234;&#33021;&#25163;&#26426;&#22270;&#20687;&#20013;&#26816;&#27979;&#29492;&#30168;&#30149;&#27602;&#65292;&#21487;&#20197;&#24110;&#21161;&#20302;&#25910;&#20837;&#22269;&#23478;&#21644;&#19981;&#20855;&#22791;&#30456;&#24212;&#30123;&#33495;&#21644;&#26816;&#27979;&#35774;&#26045;&#30340;&#21306;&#22495;&#36827;&#34892;&#21021;&#27493;&#31579;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#26376;&#65292;&#21407;&#26412;&#20165;&#27969;&#34892;&#20110;&#26377;&#38480;&#19990;&#30028;&#33539;&#22260;&#20869;&#30340;&#29492;&#30168;&#30149;&#27602;&#65288;mpox&#65289;&#24320;&#22987;&#22312;&#22810;&#20010;&#22269;&#23478;&#20256;&#25773;&#65292;&#26368;&#32456;&#34987;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#23459;&#24067;&#20026;&#8220;&#22269;&#38469;&#20851;&#27880;&#30340;&#20844;&#20849;&#21355;&#29983;&#32039;&#24613;&#20107;&#20214;&#8221;&#12290;&#30001;&#20110;&#22810;&#20010;&#22269;&#23478;&#23384;&#22312;&#25345;&#32493;&#30340;&#30149;&#20363;&#21644;&#21487;&#33021;&#30340;&#26032;&#29190;&#21457;&#65292;&#35813;&#35686;&#25253;&#20110;2023&#24180;2&#26376;&#24471;&#20197;&#26356;&#26032;&#12290;&#29305;&#21035;&#26159;&#37027;&#20123;&#32570;&#20047;&#30123;&#33495;&#21644;&#26816;&#27979;&#35774;&#26045;&#30340;&#20302;&#25910;&#20837;&#22269;&#23478;&#65292;&#38754;&#20020;&#30528;&#26497;&#22823;&#30340;&#39118;&#38505;&#12290;&#29492;&#30168;&#30149;&#27602;&#24863;&#26579;&#30340;&#30151;&#29366;&#20043;&#19968;&#26159;&#30382;&#30137;&#21644;&#29190;&#21457;&#30340;&#20986;&#29616;&#65292;&#36825;&#21487;&#33021;&#20250;&#20419;&#20351;&#20154;&#20204;&#23547;&#27714;&#21307;&#30103;&#24314;&#35758;&#12290;&#19968;&#31181;&#21487;&#33021;&#24110;&#21161;&#26681;&#25454;&#30382;&#25439;&#22806;&#35266;&#36827;&#34892;&#21021;&#27493;&#31579;&#26597;&#30340;&#25216;&#26415;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#65292;&#20294;&#35201;&#20351;&#36825;&#31181;&#25216;&#26415;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20351;&#29992;&#65292;&#23427;&#24212;&#35813;&#21487;&#20197;&#30452;&#25509;&#22312;&#20154;&#20204;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#20351;&#29992;&#65292;&#24182;&#19988;&#33021;&#36890;&#30693;&#36828;&#31243;&#21307;&#30103;&#19987;&#23478;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#26234;&#33021;&#25163;&#26426;&#22270;&#20687;&#20013;&#26816;&#27979;&#29492;&#30168;&#30149;&#27602;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent months, the monkeypox (mpox) virus -- previously endemic in a limited area of the world -- has started spreading in multiple countries until being declared a ``public health emergency of international concern'' by the World Health Organization. The alert was renewed in February 2023 due to a persisting sustained incidence of the virus in several countries and worries about possible new outbreaks. Low-income countries with inadequate infrastructures for vaccine and testing administration are particularly at risk.  A symptom of mpox infection is the appearance of skin rashes and eruptions, which can drive people to seek medical advice. A technology that might help perform a preliminary screening based on the aspect of skin lesions is the use of Machine Learning for image classification. However, to make this technology suitable on a large scale, it should be usable directly on mobile devices of people, with a possible notification to a remote medical expert.  In this work, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21518;&#39564;&#38598;&#20013;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#22240;&#23376;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#26029;&#22240;&#23376;&#32500;&#25968;&#21644;&#21152;&#36733;&#30697;&#38453;&#30340;&#31232;&#30095;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#21487;&#34892;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18488</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#36866;&#24212;&#21518;&#39564;&#38598;&#20013;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#22240;&#23376;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Bayesian sparse factor model with adaptive posterior concentration. (arXiv:2305.18488v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21518;&#39564;&#38598;&#20013;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#22240;&#23376;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#26029;&#22240;&#23376;&#32500;&#25968;&#21644;&#21152;&#36733;&#30697;&#38453;&#30340;&#31232;&#30095;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#21487;&#34892;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#31232;&#30095;&#22240;&#23376;&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#26082;&#21487;&#20197;&#25512;&#26029;&#22240;&#23376;&#32500;&#25968;&#65292;&#21448;&#21487;&#20197;&#25512;&#26029;&#21152;&#36733;&#30697;&#38453;&#30340;&#31232;&#30095;&#32467;&#26500;&#12290;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#24341;&#20837;&#20102;&#19968;&#23450;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#24471;&#31232;&#30095;&#27700;&#24179;&#21644;&#22240;&#23376;&#32500;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#21518;&#39564;&#38598;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#21518;&#39564;&#20998;&#24067;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#38598;&#20013;&#20110;&#30495;&#23454;&#22240;&#23376;&#32500;&#24230;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#21518;&#39564;&#19968;&#33268;&#24615;&#20250;&#38543;&#30528;&#30495;&#23454;&#21152;&#36733;&#30697;&#38453;&#30340;&#31232;&#30095;&#27700;&#24179;&#21644;&#22122;&#22768;&#26041;&#24046;&#32780;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#22240;&#23376;&#32500;&#25968;&#30340;&#26368;&#20248;&#26816;&#27979;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#33719;&#24471;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#21518;&#39564;&#38598;&#20013;&#36895;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new Bayesian inference method for a high-dimensional sparse factor model that allows both the factor dimensionality and the sparse structure of the loading matrix to be inferred. The novelty is to introduce a certain dependence between the sparsity level and the factor dimensionality, which leads to adaptive posterior concentration while keeping computational tractability. We show that the posterior distribution asymptotically concentrates on the true factor dimensionality, and more importantly, this posterior consistency is adaptive to the sparsity level of the true loading matrix and the noise variance. We also prove that the proposed Bayesian model attains the optimal detection rate of the factor dimensionality in a more general situation than those found in the literature. Moreover, we obtain a near-optimal posterior concentration rate of the covariance matrix. Numerical studies are conducted and show the superiority of the proposed method compared with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#22826;&#38451;&#36752;&#29031;&#24230;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22825;&#31354;&#22270;&#20687;&#30340;&#30456;&#20851;&#29305;&#24449;&#20197;&#21450;&#22825;&#31354;&#22270;&#20687;&#20043;&#38388;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#23545;&#26410;&#26469;&#30340;&#36752;&#29031;&#24230;&#36827;&#34892;&#20102;&#26377;&#25928;&#30340;&#39044;&#27979;&#65292;&#39044;&#27979;&#20934;&#30830;&#29575;&#27604;&#26234;&#33021;&#25345;&#32493;&#27169;&#22411;&#39640;&#20986;21.45&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.18487</link><description>&lt;p&gt;
&#22826;&#38451;&#36752;&#29031;&#24230;&#39044;&#27979;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Solar Irradiance Anticipative Transformer. (arXiv:2305.18487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#22826;&#38451;&#36752;&#29031;&#24230;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22825;&#31354;&#22270;&#20687;&#30340;&#30456;&#20851;&#29305;&#24449;&#20197;&#21450;&#22825;&#31354;&#22270;&#20687;&#20043;&#38388;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#23545;&#26410;&#26469;&#30340;&#36752;&#29031;&#24230;&#36827;&#34892;&#20102;&#26377;&#25928;&#30340;&#39044;&#27979;&#65292;&#39044;&#27979;&#20934;&#30830;&#29575;&#27604;&#26234;&#33021;&#25345;&#32493;&#27169;&#22411;&#39640;&#20986;21.45&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#30701;&#26399;&#22826;&#38451;&#36752;&#29031;&#24230;&#39044;&#27979;&#12290;&#32473;&#23450;&#19968;&#31995;&#21015;&#22825;&#31354;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#32534;&#30721;&#36830;&#32493;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#24182;&#39304;&#20837;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#20013;&#65292;&#20197;&#39044;&#27979;&#26410;&#26469;&#26410;&#35265;&#36807;&#30340;&#22825;&#31354;&#22270;&#20687;&#30456;&#20851;&#30340;&#36752;&#29031;&#24230;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21482;&#20851;&#27880;&#19982;&#36752;&#29031;&#24230;&#39044;&#27979;&#30456;&#20851;&#30340;&#29305;&#24449;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#21464;&#21387;&#22120;&#21487;&#20197;&#25429;&#25417;&#22825;&#31354;&#22270;&#20687;&#20043;&#38388;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#27604;&#26234;&#33021;&#25345;&#32493;&#27169;&#22411;&#22312;&#19968;&#20010;&#26032;&#24341;&#20837;&#30340;&#20840;&#22825;&#31354;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;15&#20998;&#38047;&#39044;&#27979;&#20013;&#36798;&#21040;&#20102;21.45&#65285;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an anticipative transformer-based model for short-term solar irradiance forecasting. Given a sequence of sky images, our proposed vision transformer encodes features of consecutive images, feeding into a transformer decoder to predict irradiance values associated with future unseen sky images. We show that our model effectively learns to attend only to relevant features in images in order to forecast irradiance. Moreover, the proposed anticipative transformer captures long-range dependencies between sky images to achieve a forecasting skill of 21.45 % on a 15 minute ahead prediction for a newly introduced dataset of all-sky images when compared to a smart persistence model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#19982;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#31995;&#21015;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18485</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Autoencoding Conditional Neural Processes for Representation Learning. (arXiv:2305.18485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#19982;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#31995;&#21015;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;(CNPs)&#26159;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#27169;&#22411;&#26063;&#32676;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#20540;&#20013;&#23398;&#20064;&#20986;&#19968;&#20010;&#38543;&#26426;&#36807;&#31243;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;CNPs &#22312;&#19978;&#19979;&#25991;&#22270;&#20687;&#34917;&#20840;&#20013;&#24471;&#21040;&#20102;&#29305;&#21035;&#30340;&#24212;&#29992;&#65292;&#21363;&#36890;&#36807;&#35266;&#23519;&#26576;&#20123;&#20301;&#32622;&#30340;&#20687;&#32032;&#20540;&#26469;&#39044;&#27979;&#20854;&#20182;&#26410;&#35266;&#23519;&#20301;&#32622;&#19978;&#30340;&#20540;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#26679;&#19968;&#20010; CNP &#30340;&#20687;&#32032;&#36873;&#25321;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#25110;&#32773;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;(&#20363;&#22914;&#20687;&#32032;&#26041;&#24046;)&#23548;&#20986;&#30340;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#36716;&#21464;&#19968;&#19979;&#65306;&#19968;&#20010; CNP &#24819;&#35201;&#35266;&#23519;&#21738;&#20123;&#20687;&#32032;&#65311;&#20063;&#23601;&#26159;&#35828;&#65292;&#21738;&#20123;&#20687;&#32032;&#20801;&#35768;&#25311;&#21512; CNP&#65292;&#36825;&#26679;&#30340;&#20687;&#32032;&#33021;&#21578;&#35785;&#25105;&#20204;&#19968;&#20123;&#20851;&#20110;&#28508;&#22312;&#22270;&#20687;&#30340;&#20449;&#24687;&#21527;&#65311;&#23558;&#25552;&#20379;&#32473; CNP &#30340;&#19978;&#19979;&#25991;&#35270;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#19968;&#27425;&#24615;&#21464;&#20998;&#26694;&#26550;&#65292;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(Partical Pixel Space VAE, PPS-VAE)&#65292;&#21516;&#26102;&#39044;&#27979;&#36825;&#20010;&#19978;&#19979;&#25991;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010; CNP&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102; PPS-VAE&#65292;&#21457;&#29616;&#36890;&#36807;&#30456;&#23545;&#22823;&#23567;&#25110;&#21464;&#21270;&#39044;&#27979;&#20687;&#32032;&#30340;&#36873;&#25321;&#21487;&#20197;&#23433;&#25490;&#23398;&#20064;&#65292;&#19988;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#20102;&#19978;&#19979;&#25991;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional neural processes (CNPs) are a flexible and efficient family of models that learn to learn a stochastic process from observations. In the visual domain, they have seen particular application in contextual image completion - observing pixel values at some locations to predict a distribution over values at other unobserved locations. However, the choice of pixels in learning such a CNP is typically either random or derived from a simple statistical measure (e.g. pixel variance). Here, we turn the problem on its head and ask: which pixels would a CNP like to observe? That is, which pixels allow fitting CNP, and do such pixels tell us something about the underlying image? Viewing the context provided to the CNP as fixed-size latent representations, we construct an amortised variational framework, Partial Pixel Space Variational Autoencoder (PPS-VAE), for predicting this context simultaneously with learning a CNP. We evaluate PPS-VAE on a set of vision datasets, and find that not
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20613;&#37324;&#21494;&#21464;&#25442;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32452;&#30340;&#28508;&#22312;&#32447;&#24615;&#20316;&#29992;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#38544;&#34255;&#32467;&#26500;&#30340;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.18484</link><description>&lt;p&gt;
&#31070;&#32463;&#20613;&#37324;&#21494;&#21464;&#25442;&#65306;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Fourier Transform: A General Approach to Equivariant Representation Learning. (arXiv:2305.18484v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18484
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20613;&#37324;&#21494;&#21464;&#25442;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32452;&#30340;&#28508;&#22312;&#32447;&#24615;&#20316;&#29992;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#38544;&#34255;&#32467;&#26500;&#30340;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#21462;&#25968;&#25454;&#38544;&#34255;&#32467;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#31561;&#21464;&#20851;&#31995;&#27010;&#24565;&#36215;&#30528;&#20013;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30740;&#31350;&#37117;&#24314;&#31435;&#22312;&#24314;&#31569;&#29702;&#35770;&#21644;&#23545;&#25968;&#25454;&#24418;&#24335;&#30340;&#30456;&#24212;&#20551;&#35774;&#20043;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;NFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#32452;&#30340;&#28508;&#22312;&#32447;&#24615;&#20316;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#20851;&#20110;&#32452;&#22914;&#20309;&#20316;&#29992;&#20110;&#25968;&#25454;&#30340;&#26174;&#24335;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;NFT&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#34920;&#26126;&#31561;&#21464;&#29305;&#24449;&#30340;&#23384;&#22312;&#65292;&#21363;&#22312;&#31561;&#21464;&#24615;&#23398;&#20064;&#20013;&#26222;&#36941;&#20551;&#23450;&#30340;&#65292;&#31561;&#20215;&#20110;&#25968;&#25454;&#31354;&#38388;&#20013;&#23384;&#22312;&#19968;&#32452;&#19981;&#21464;&#26680;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#23454;&#39564;&#32467;&#26524;&#65292;&#28436;&#31034;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#20851;&#20110;&#25805;&#20316;&#32452;&#30340;&#30693;&#35782;&#30340;&#20856;&#22411;&#22330;&#26223;&#20013;&#24212;&#29992;NFT&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry learning has proven to be an effective approach for extracting the hidden structure of data, with the concept of equivariance relation playing the central role. However, most of the current studies are built on architectural theory and corresponding assumptions on the form of data. We propose Neural Fourier Transform (NFT), a general framework of learning the latent linear action of the group without assuming explicit knowledge of how the group acts on data. We present the theoretical foundations of NFT and show that the existence of a linear equivariant feature, which has been assumed ubiquitously in equivariance learning, is equivalent to the existence of a group invariant kernel on the dataspace. We also provide experimental results to demonstrate the application of NFT in typical scenarios with varying levels of knowledge about the acting group.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#20998;&#35010;&#25216;&#26415;&#35299;&#20915;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#20445;&#35777;&#21644;GPU&#24182;&#34892;&#21270;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.18483</link><description>&lt;p&gt;
&#23558;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#24102;&#20837;&#39640;&#36895;&#65306;&#36866;&#29992;&#20110;GPU&#30340;&#20998;&#35010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bringing regularized optimal transport to lightspeed: a splitting method adapted for GPUs. (arXiv:2305.18483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#20998;&#35010;&#25216;&#26415;&#35299;&#20915;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#20445;&#35777;&#21644;GPU&#24182;&#34892;&#21270;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;Douglas-Rachford&#20998;&#35010;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#24191;&#27867;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;GPU&#24182;&#34892;&#21270;&#65292;&#20351;&#20854;&#22312;&#35768;&#22810;&#38382;&#39064;&#19978;&#27604;&#29616;&#26377;&#25216;&#26415;&#24555;&#24471;&#22810;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#24212;&#29992;&#20013;&#38416;&#36848;&#20102;&#20854;&#31454;&#20105;&#21147;&#65292;&#21253;&#25324;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an efficient algorithm for regularized optimal transport. In contrast to previous methods, we use the Douglas-Rachford splitting technique to develop an efficient solver that can handle a broad class of regularizers. The algorithm has strong global convergence guarantees, low per-iteration cost, and can exploit GPU parallelization, making it considerably faster than the state-of-the-art for many problems. We illustrate its competitiveness in several applications, including domain adaptation and learning of generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#19978;&#19979;&#35013;&#30340;&#26102;&#23578;&#29289;&#20307;&#26816;&#27979;&#30340;&#27969;&#31243;&#65292;&#21487;&#20197;&#33258;&#21160;&#21306;&#20998;&#22270;&#20687;&#20013;&#30340;&#19978;&#34915;&#21644;&#19979;&#34915;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#20154;&#20307;&#37096;&#20301;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#24615;&#33021;&#30340;&#22522;&#20110;&#38170;&#28857;&#21644;&#26080;&#38170;&#28857;&#30340;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#65292;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18482</link><description>&lt;p&gt;
&#38754;&#21521;&#19978;&#19979;&#35013;&#30340;&#26102;&#23578;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fashion Object Detection for Tops &amp; Bottoms. (arXiv:2305.18482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#19978;&#19979;&#35013;&#30340;&#26102;&#23578;&#29289;&#20307;&#26816;&#27979;&#30340;&#27969;&#31243;&#65292;&#21487;&#20197;&#33258;&#21160;&#21306;&#20998;&#22270;&#20687;&#20013;&#30340;&#19978;&#34915;&#21644;&#19979;&#34915;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#20154;&#20307;&#37096;&#20301;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#24615;&#33021;&#30340;&#22522;&#20110;&#38170;&#28857;&#21644;&#26080;&#38170;&#28857;&#30340;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#65292;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#34892;&#19994;&#20043;&#19968;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#36817;&#24180;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#29289;&#20307;&#26816;&#27979;&#21644;&#26381;&#35013;&#20998;&#21106;&#31561;&#20219;&#21153;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#35270;&#35273;&#35299;&#20915;&#26041;&#26696;&#22312;&#26102;&#23578;&#34892;&#19994;&#20013;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#35768;&#22810;&#38382;&#39064;&#36824;&#36828;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#24320;&#31665;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#25552;&#20379;&#26399;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#23427;&#21487;&#20197;&#25509;&#21463;&#21547;&#22122;&#38899;&#30340;&#24102;&#26377;&#20154;&#29289;&#30340;&#22270;&#20687;&#65292;&#24182;&#29305;&#21035;&#26816;&#27979;&#23646;&#20110;&#19978;&#35013;&#25110;&#19979;&#35013;&#30340;&#26381;&#35013;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#20102;&#21487;&#20197;&#22312;&#22270;&#20687;&#20013;&#25214;&#21040;&#20154;&#20307;&#37096;&#20301;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#20840;&#36523;&#23545;&#21322;&#36523;&#65292;&#25110;&#32773;&#26681;&#26412;&#27809;&#26377;&#25214;&#21040;&#20154;&#20307;&#12290;&#28982;&#21518;&#65292;&#20854;&#20182;&#27169;&#22411;&#36890;&#36807;&#30693;&#36947;&#26377;&#20154;&#21644;&#20854;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#25105;&#20204;&#24182;&#19981;&#24635;&#26159;&#26377;&#19968;&#20010;&#23436;&#25972;&#30340;&#20154;&#20307;&#65289;&#26469;&#25214;&#21040;&#22270;&#20687;&#30340;&#36793;&#30028;&#26694;/&#21306;&#22495;&#65292;&#36825;&#20123;&#26694;&#24456;&#21487;&#33021;&#23545;&#24212;&#20110;&#19979;&#35013;&#25110;&#19978;&#35013;&#12290;&#23545;&#20110;&#36793;&#30028;&#26694;/&#21306;&#22495;&#30340;&#21019;&#24314;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#38170;&#28857;&#21644;&#26080;&#38170;&#28857;&#30340;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#24615;&#33021;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;Polyvore&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion is one of the largest world's industries and computer vision techniques have been becoming more popular in recent years, in particular, for tasks such as object detection and apparel segmentation. Even with the rapid growth in computer vision solutions, specifically for the fashion industry, many problems are far for being resolved. Therefore, not at all times, adjusting out-of-the-box pre-trained computer vision models will provide the desired solution. In the present paper is proposed a pipeline that takes a noisy image with a person and specifically detects the regions with garments that are bottoms or tops. Our solution implements models that are capable of finding human parts in an image e.g. full-body vs half-body, or no human is found. Then, other models knowing that there's a human and its composition (e.g. not always we have a full-body) finds the bounding boxes/regions of the image that very likely correspond to a bottom or a top. For the creation of bounding boxes/re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#20154;&#26426;&#36741;&#21161;&#30340;&#20803;&#23431;&#23449;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#36164;&#28304;&#20998;&#37197;&#21644;&#36712;&#36857;&#25511;&#21046;&#38598;&#25104;&#21040;&#31995;&#32479;&#27169;&#22411;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#21644;&#20984;&#20248;&#21270;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#8220;&#21512;&#20316;&#8221;&#26041;&#24335;&#35299;&#20915;&#26102;&#38388;&#39034;&#24207;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#24471;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.18481</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#20984;&#20248;&#21270;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;UAV&#30340;&#33258;&#20027;&#20803;&#23431;&#23449;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Framework of Reinforcement Learning and Convex Optimization for UAV-Based Autonomous Metaverse Data Collection. (arXiv:2305.18481v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#20154;&#26426;&#36741;&#21161;&#30340;&#20803;&#23431;&#23449;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#36164;&#28304;&#20998;&#37197;&#21644;&#36712;&#36857;&#25511;&#21046;&#38598;&#25104;&#21040;&#31995;&#32479;&#27169;&#22411;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#21644;&#20984;&#20248;&#21270;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#8220;&#21512;&#20316;&#8221;&#26041;&#24335;&#35299;&#20915;&#26102;&#38388;&#39034;&#24207;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#24471;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#30001;&#20110;&#25104;&#26412;&#21644;&#28789;&#27963;&#24615;&#30340;&#20248;&#21183;&#65292;&#22312;&#25552;&#20379;&#36890;&#20449;&#26381;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20852;&#20803;&#23431;&#23449;&#21644;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#32972;&#26223;&#19979;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;UAV&#36741;&#21161;&#30340;&#20803;&#23431;&#23449;&#32593;&#32476;&#65292;&#20854;&#20013;UAV&#25193;&#23637;&#20102;&#22522;&#31449;&#65288;BS&#65289;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#25910;&#38598;&#36335;&#36793;&#21333;&#20803;&#65288;RSU&#65289;&#20135;&#29983;&#30340;&#20803;&#23431;&#23449;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#65292;&#31995;&#32479;&#27169;&#22411;&#20013;&#38598;&#25104;&#20102;&#36164;&#28304;&#20998;&#37197;&#21644;&#36712;&#36857;&#25511;&#21046;&#12290;&#20248;&#21270;&#38382;&#39064;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#20351;&#20256;&#32479;&#20984;&#20248;&#21270;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;UAV&#36741;&#21161;&#30340;&#20803;&#23431;&#23449;&#32593;&#32476;&#31995;&#32479;&#27169;&#22411;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#21644;&#20984;&#20248;&#21270;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#8220;&#21512;&#20316;&#8221;&#26041;&#24335;&#35299;&#20915;&#26102;&#38388;&#39034;&#24207;&#20248;&#21270;&#38382;&#39064;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#22312;&#20445;&#35777;&#25910;&#38598;&#25968;&#25454;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned aerial vehicles (UAVs) are promising for providing communication services due to their advantages in cost and mobility, especially in the context of the emerging Metaverse and Internet of Things (IoT). This paper considers a UAV-assisted Metaverse network, in which UAVs extend the coverage of the base station (BS) to collect the Metaverse data generated at roadside units (RSUs). Specifically, to improve the data collection efficiency, resource allocation and trajectory control are integrated into the system model. The time-dependent nature of the optimization problem makes it non-trivial to be solved by traditional convex optimization methods. Based on the proposed UAV-assisted Metaverse network system model, we design a hybrid framework with reinforcement learning and convex optimization to {cooperatively} solve the time-sequential optimization problem. Simulation results show that the proposed framework is able to reduce the mission completion time with a given transmission 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#20998;&#31867;&#20154;&#20307;&#24418;&#29366;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#23454;&#20363;&#20998;&#21106;&#21644;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22411;&#65292;&#26080;&#38656;&#22522;&#20110;&#19977;&#32500;&#20154;&#20307;&#37325;&#24314;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#24320;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#23558;&#27492;&#26041;&#27861;&#22312;&#26032;&#39062;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.18480</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#20154;&#20307;&#24418;&#29366;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Human Body Shape Classification Based on a Single Image. (arXiv:2305.18480v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#20998;&#31867;&#20154;&#20307;&#24418;&#29366;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#23454;&#20363;&#20998;&#21106;&#21644;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22411;&#65292;&#26080;&#38656;&#22522;&#20110;&#19977;&#32500;&#20154;&#20307;&#37325;&#24314;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#24320;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#23558;&#27492;&#26041;&#27861;&#22312;&#26032;&#39062;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#22312;&#32447;&#26102;&#35013;&#25512;&#33616;&#31995;&#32479;&#25509;&#32435;&#20102;&#28040;&#36153;&#32773;&#30340;&#36523;&#20307;&#24418;&#29366;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#20998;&#31867;&#20154;&#20307;&#24418;&#29366;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;Open-Source&#22522;&#20934;&#25968;&#25454;&#38598;&#35757;&#32451;&#23454;&#20363;&#20998;&#21106;&#21644;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22411;&#65292;&#38024;&#23545;&#22122;&#22768;&#29615;&#22659;&#20855;&#26377;&#40065;&#26834;&#30340;&#32972;&#26223;&#24046;&#20998;&#33021;&#21147;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#22522;&#20110;&#19977;&#32500;&#20154;&#20307;&#37325;&#24314;&#65292;&#32780;&#26159;&#22522;&#20110;&#20851;&#38190;&#28857;&#20272;&#35745;&#20998;&#31867;&#65292;&#25805;&#20316;&#26102;&#26080;&#38656;&#21382;&#21490;&#20449;&#24687;&#35745;&#31639;&#25152;&#38656;&#25152;&#26377;&#27979;&#37327;&#20540;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#24615;&#35780;&#20272;&#19982;&#29616;&#26377;&#30340;&#20307;&#24418;&#20998;&#31867;&#22120;&#20197;&#21450;&#23450;&#37327;&#35780;&#20272;&#19982;&#25105;&#20204;&#25552;&#20379;&#32473;&#31038;&#21306;&#20351;&#29992;&#30340;&#26032;&#39062;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23545;&#27604;&#12290;&#25152;&#20135;&#29983;&#30340;&#20154;&#20307;&#24418;&#29366;&#20998;&#31867;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#36755;&#20837;&#22823;&#23567;&#21644;&#36866;&#37197;&#25512;&#33616;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is high demand for online fashion recommender systems that incorporate the needs of the consumer's body shape. As such, we present a methodology to classify human body shape from a single image. This is achieved through the use of instance segmentation and keypoint estimation models, trained only on open-source benchmarking datasets. The system is capable of performing in noisy environments owing to to robust background subtraction. The proposed methodology does not require 3D body recreation as a result of classification based on estimated keypoints, nor requires historical information about a user to operate - calculating all required measurements at the point of use. We evaluate our methodology both qualitatively against existing body shape classifiers and quantitatively against a novel dataset of images, which we provide for use to the community. The resultant body shape classification can be utilised in a variety of downstream tasks, such as input to size and fit recommendat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23558;Human Action Recognition&#20013;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20043;&#19968;&#8212;&#8212;X3D&#26144;&#23556;&#21040;&#20219;&#20309;FPGA&#35774;&#22791;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#23454;&#29616;&#20102;&#38024;&#23545;&#36825;&#31181;&#22797;&#26434;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;Human Action Recognition&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.18479</link><description>&lt;p&gt;
FMM-X3D&#65306;&#22522;&#20110;FPGA&#30340;X3D&#24314;&#27169;&#21644;&#26144;&#23556;&#29992;&#20110;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition. (arXiv:2305.18479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23558;Human Action Recognition&#20013;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20043;&#19968;&#8212;&#8212;X3D&#26144;&#23556;&#21040;&#20219;&#20309;FPGA&#35774;&#22791;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#23454;&#29616;&#20102;&#38024;&#23545;&#36825;&#31181;&#22797;&#26434;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;Human Action Recognition&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#30340;&#20851;&#27880;&#65292;&#22312;&#30417;&#25511;&#31995;&#32479;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#20154;&#20307;&#30417;&#27979;&#31995;&#32479;&#21644;&#35270;&#39057;&#26816;&#32034;&#31561;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#31995;&#32479;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#23558;Human Action Recognition&#20013;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20043;&#19968;&#8212;&#8212;X3D&#26144;&#23556;&#21040;&#20219;&#20309;FPGA&#35774;&#22791;&#19978;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#24037;&#20855;&#27969;&#29983;&#25104;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#22522;&#20110;&#27969;&#30340;&#30828;&#20214;&#31995;&#32479;&#65292;&#32771;&#34385;&#20102;FPGA&#35774;&#22791;&#30340;&#21487;&#29992;&#36164;&#28304;&#21644;&#22806;&#37096;&#23384;&#20648;&#22120;&#29305;&#24615;&#12290;&#25152;&#29983;&#25104;&#30340;&#35774;&#35745;&#25512;&#36827;&#20102;&#24403;&#21069;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#24182;&#39318;&#27425;&#23454;&#29616;&#20102;&#38024;&#23545;&#36825;&#31181;&#22797;&#26434;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;Human Action Recognition&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D Convolutional Neural Networks are gaining increasing attention from researchers and practitioners and have found applications in many domains, such as surveillance systems, autonomous vehicles, human monitoring systems, and video retrieval. However, their widespread adoption is hindered by their high computational and memory requirements, especially when resource-constrained systems are targeted. This paper addresses the problem of mapping X3D, a state-of-the-art model in Human Action Recognition that achieves accuracy of 95.5\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflow generates an optimised stream-based hardware system, taking into account the available resources and off-chip memory characteristics of the FPGA device. The generated designs push further the current performance-accuracy pareto front, and enable for the first time the targeting of such complex model architectures for the Human Action Recognition task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24207;&#21015;&#24314;&#27169;&#26102;&#30340;&#36924;&#36817;&#24615;&#36136;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#29305;&#24449;&#25551;&#36848;, &#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#21367;&#31215;&#20307;&#31995;&#32467;&#26500;&#25152;&#33021;&#25429;&#25417;&#30340;&#39034;&#24207;&#20851;&#31995;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18478</link><description>&lt;p&gt;
&#21069;&#21521;&#21644;&#21453;&#21521;&#36924;&#36817;&#29702;&#35770;&#22312;&#32447;&#24615;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Forward and Inverse Approximation Theory for Linear Temporal Convolutional Networks. (arXiv:2305.18478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24207;&#21015;&#24314;&#27169;&#26102;&#30340;&#36924;&#36817;&#24615;&#36136;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#29305;&#24449;&#25551;&#36848;, &#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#21367;&#31215;&#20307;&#31995;&#32467;&#26500;&#25152;&#33021;&#25429;&#25417;&#30340;&#39034;&#24207;&#20851;&#31995;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#26102;&#24207;&#24207;&#21015;&#26102;&#30340;&#36924;&#36817;&#24615;&#36136;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#65288;&#31867;&#20284;&#20110;Jackson&#30340;&#32467;&#26524;&#65289;&#21644;&#21453;&#21521;&#36924;&#36817;&#23450;&#29702;&#65288;&#31867;&#20284;&#20110;Bernstein&#30340;&#32467;&#26524;&#65289;&#65292;&#20108;&#32773;&#20849;&#21516;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#21367;&#31215;&#20307;&#31995;&#32467;&#26500;&#25152;&#33021;&#25429;&#25417;&#30340;&#39034;&#24207;&#20851;&#31995;&#31867;&#22411;&#12290;&#35813;&#36895;&#29575;&#20272;&#35745;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31934;&#32454;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#32467;&#26524;&#65292;&#32780;&#21453;&#21521;&#36924;&#36817;&#23450;&#29702;&#26159;&#26032;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theoretical analysis of the approximation properties of convolutional architectures when applied to the modeling of temporal sequences. Specifically, we prove an approximation rate estimate (Jackson-type result) and an inverse approximation theorem (Bernstein-type result), which together provide a comprehensive characterization of the types of sequential relationships that can be efficiently captured by a temporal convolutional architecture. The rate estimate improves upon a previous result via the introduction of a refined complexity measure, whereas the inverse approximation theorem is new.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18477</link><description>&lt;p&gt;
&#36229;&#36234;&#20803;&#25968;&#25454;&#65306;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#36827;&#34892;&#36328;&#29256;&#26412;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#26159;&#20840;&#29699;&#28216;&#25103;&#24066;&#22330;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#26159;&#22686;&#38271;&#26368;&#24555;&#30340;&#28216;&#25103;&#32454;&#20998;&#39046;&#22495;&#12290;&#36825;&#23548;&#33268;&#20102;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#30340;&#39046;&#22495;&#20135;&#29983;&#65292;&#20854;&#20351;&#29992;&#28216;&#25103;&#25552;&#21462;&#30340;&#36965;&#27979;&#25968;&#25454;&#26469;&#20026;&#29609;&#23478;&#12289;&#25945;&#32451;&#12289;&#25773;&#38899;&#21592;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#30340;&#20307;&#32946;&#27604;&#36187;&#30456;&#27604;&#65292;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#30340;&#26426;&#21046;&#21644;&#35268;&#21017;&#32463;&#24120;&#21457;&#29983;&#24555;&#36895;&#21464;&#21270;&#12290;&#30001;&#20110;&#28216;&#25103;&#21442;&#25968;&#30340;&#39057;&#32321;&#26356;&#25913;&#65292;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#27169;&#22411;&#30340;&#20351;&#29992;&#23551;&#21629;&#21487;&#33021;&#24456;&#30701;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#30053;&#20102;&#12290;&#26412;&#25991;&#25552;&#21462;&#28216;&#25103;&#35774;&#35745;&#20449;&#24687;&#65288;&#21363;&#34917;&#19969;&#35828;&#26126;&#65289;&#65292;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#12290;&#20197;Dota 2&#28216;&#25103;&#20013;&#20987;&#26432;&#27425;&#25968;&#30340;&#39044;&#27979;&#20026;&#26696;&#20363;&#65292;&#21033;&#29992;&#36825;&#31181;&#21019;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#27492;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21253;&#25324;&#24120;&#35268;&#25216;&#26415;&#22312;&#20869;&#30340;&#20004;&#20010;&#19981;&#21516;&#22522;&#32447;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#36824;&#20811;&#26381;&#20102;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#20013;&#29256;&#26412;&#26356;&#36845;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#29992;&#20110;&#31934;&#30830;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.18475</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340;&#36924;&#36817;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#29992;&#20110;&#31934;&#30830;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26159;&#24207;&#21015;&#24314;&#27169;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#26550;&#26500;&#65292;&#20294;&#20854;&#24037;&#20316;&#21407;&#29702;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#36924;&#36817;&#24207;&#21015;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#12290;&#36890;&#36807;&#25512;&#23548;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#65292;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#19968;&#20010;&#26126;&#30830;&#30340;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#12290;&#36825;&#20010;&#20272;&#35745;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#30340;&#20851;&#38190;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#26263;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20855;&#20307;&#22320;&#35752;&#35770;&#21464;&#21387;&#22120;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24471;&#21040;&#20102;&#25968;&#23383;&#23454;&#39564;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#37325;&#26032;&#20998;&#26512;&#20102;&#24863;&#30693;&#21387;&#21147;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#27979;&#35797;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#19981;&#30456;&#31561;&#65292;&#23637;&#31034;&#20102;&#22312;&#24515;&#29702;&#19978;&#35266;&#23519;&#21040;&#30340;&#19981;&#21516;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.18473</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#24863;&#30693;&#21387;&#21147;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Alg{\i}lanan Stres Testinin Makine \"O\u{g}renmesi ile Analiz Edilmesi. (arXiv:2305.18473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#37325;&#26032;&#20998;&#26512;&#20102;&#24863;&#30693;&#21387;&#21147;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#27979;&#35797;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#19981;&#30456;&#31561;&#65292;&#23637;&#31034;&#20102;&#22312;&#24515;&#29702;&#19978;&#35266;&#23519;&#21040;&#30340;&#19981;&#21516;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#37325;&#26032;&#20998;&#26512;&#24863;&#30693;&#21387;&#21147;&#27979;&#35797;&#65292;&#20197;&#30830;&#23450;150&#20010;&#20010;&#20307;&#30340;&#24863;&#30693;&#21387;&#21147;&#27700;&#24179;&#24182;&#27979;&#37327;&#27979;&#35797;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#35813;&#27979;&#35797;&#21253;&#25324;14&#20010;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#30340;&#24471;&#20998;&#33539;&#22260;&#20026;0&#21040;4&#65292;&#24635;&#24471;&#20998;&#33539;&#22260;&#20026;0-56&#12290;&#20854;&#20013;&#65292;7&#20010;&#38382;&#39064;&#20197;&#36127;&#38754;&#26041;&#24335;&#34920;&#36848;&#24182;&#30456;&#24212;&#35780;&#20998;&#65292;&#32780;&#20854;&#20313;7&#20010;&#38382;&#39064;&#20197;&#27491;&#38754;&#26041;&#24335;&#34920;&#36848;&#24182;&#25353;&#30456;&#21453;&#26041;&#24335;&#35780;&#20998;&#12290;&#35813;&#27979;&#35797;&#36824;&#35774;&#35745;&#20026;&#35782;&#21035;&#20004;&#20010;&#23376;&#22240;&#32032;&#65306;&#24863;&#30693;&#33258;&#25105;&#25928;&#33021;&#21644;&#21387;&#21147;/&#19981;&#36866;&#24863;&#30693;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23637;&#31034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#27979;&#35797;&#38382;&#39064;&#21487;&#33021;&#24182;&#19981;&#20855;&#26377;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#65292;&#25581;&#31034;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#31038;&#20250;&#20013;&#20986;&#29616;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#26368;&#32456;&#35777;&#26126;&#22312;&#24515;&#29702;&#19978;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#27169;&#24335;&#23384;&#22312;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#37325;&#22797;&#29616;&#26377;&#30340;&#24515;&#29702;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to reanalyze the perceived stress test using machine learning to determine the perceived stress levels of 150 individuals and measure the impact of the test questions. The test consists of 14 questions, each scored on a scale of 0 to 4, resulting in a total score range of 0-56. Out of these questions, 7 are formulated in a negative context and scored accordingly, while the remaining 7 are formulated in a positive context and scored in reverse. The test is also designed to identify two sub-factors: perceived self-efficacy and stress/discomfort perception. The main objectives of this research are to demonstrate that test questions may not have equal importance using artificial intelligence techniques, reveal which questions exhibit variations in the society using machine learning, and ultimately demonstrate the existence of distinct patterns observed psychologically. This study provides a different perspective from the existing psychology literature by repeating 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21452;&#21521;&#39044;&#27979;&#32534;&#30721;&#65288;DBPC&#65289;&#23558;&#29616;&#26377;&#30340;&#39044;&#27979;&#32534;&#30721;&#26041;&#27861;&#25193;&#23637;&#20026;&#25903;&#25345;&#27491;&#21521;&#21644;&#21453;&#39304;&#20449;&#24687;&#20256;&#25773;&#30340;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20998;&#31867;&#21644;&#37325;&#24314;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18472</link><description>&lt;p&gt;
&#21452;&#21521;&#20256;&#25773;&#30340;&#28145;&#24230;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#29992;&#20110;&#20998;&#31867;&#21644;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Deep Predictive Coding with Bi-directional Propagation for Classification and Reconstruction. (arXiv:2305.18472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21452;&#21521;&#39044;&#27979;&#32534;&#30721;&#65288;DBPC&#65289;&#23558;&#29616;&#26377;&#30340;&#39044;&#27979;&#32534;&#30721;&#26041;&#27861;&#25193;&#23637;&#20026;&#25903;&#25345;&#27491;&#21521;&#21644;&#21453;&#39304;&#20449;&#24687;&#20256;&#25773;&#30340;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20998;&#31867;&#21644;&#37325;&#24314;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#21452;&#21521;&#39044;&#27979;&#32534;&#30721;&#65288;DBPC&#65289;&#65292;&#20801;&#35768;&#24320;&#21457;&#32593;&#32476;&#20351;&#29992;&#30456;&#21516;&#30340;&#26435;&#37325;&#21516;&#26102;&#25191;&#34892;&#20998;&#31867;&#21644;&#37325;&#24314;&#20219;&#21153;&#12290;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#24050;&#25104;&#20026;&#25903;&#37197;&#22823;&#33041;&#20449;&#24687;&#22788;&#29702;&#30340;&#31361;&#20986;&#29702;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#25903;&#25345;&#27491;&#21521;&#21644;&#21453;&#39304;&#20449;&#24687;&#20256;&#25773;&#30340;&#32593;&#32476;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;PC&#26041;&#27861;&#12290;&#20351;&#29992;DBPC&#35757;&#32451;&#30340;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#23618;&#37117;&#23398;&#20064;&#39044;&#27979;&#21069;&#19968;&#23618;&#21644;&#21518;&#19968;&#23618;&#31070;&#32463;&#20803;&#30340;&#27963;&#21160;&#65292;&#36825;&#20351;&#24471;&#32593;&#32476;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#20998;&#31867;&#21644;&#37325;&#24314;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new learning algorithm, termed Deep Bi-directional Predictive Coding (DBPC) that allows developing networks to simultaneously perform classification and reconstruction tasks using the same weights. Predictive Coding (PC) has emerged as a prominent theory underlying information processing in the brain. The general concept for learning in PC is that each layer learns to predict the activities of neurons in the previous layer which enables local computation of error and in-parallel learning across layers. In this paper, we extend existing PC approaches by developing a network which supports both feedforward and feedback propagation of information. Each layer in the networks trained using DBPC learn to predict the activities of neurons in the previous and next layer which allows the network to simultaneously perform classification and reconstruction tasks using feedforward and feedback propagation, respectively. DBPC also relies on locally available information for le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20165;&#26377;&#20114;&#26131;&#22122;&#22768;&#26041;&#24046;&#21644;&#26377;&#30028;&#24179;&#28369;&#24615;&#20551;&#35774;&#30340;&#38750;&#20984;&#30446;&#26631;&#30340;AdaGrad&#31639;&#27861;&#30340;&#31616;&#21333;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#35777;&#26126;&#20013;&#22522;&#20110;&#36741;&#21161;&#20989;&#25968;$\xi$&#65292;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#32039;&#23494;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#20445;&#26799;&#24230;&#33539;&#25968;&#23567;&#20110;$\varepsilon$&#30340;&#36845;&#20195;&#27425;&#25968;&#20026;$\mathcal{O}(\frac{1}{\varepsilon^2})$&#65292;&#24182;&#32771;&#34385;&#20102;&#19968;&#31181;&#23454;&#38469;&#24179;&#28369;&#20551;&#35774;$(L_0,L_1)$-&#24179;&#28369;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.18471</link><description>&lt;p&gt;
AdaGrad&#31639;&#27861;&#22312;&#38750;&#20984;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;: &#31616;&#26126;&#35777;&#26126;&#21644;&#23485;&#26494;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions. (arXiv:2305.18471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20165;&#26377;&#20114;&#26131;&#22122;&#22768;&#26041;&#24046;&#21644;&#26377;&#30028;&#24179;&#28369;&#24615;&#20551;&#35774;&#30340;&#38750;&#20984;&#30446;&#26631;&#30340;AdaGrad&#31639;&#27861;&#30340;&#31616;&#21333;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#35777;&#26126;&#20013;&#22522;&#20110;&#36741;&#21161;&#20989;&#25968;$\xi$&#65292;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#32039;&#23494;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#20445;&#26799;&#24230;&#33539;&#25968;&#23567;&#20110;$\varepsilon$&#30340;&#36845;&#20195;&#27425;&#25968;&#20026;$\mathcal{O}(\frac{1}{\varepsilon^2})$&#65292;&#24182;&#32771;&#34385;&#20102;&#19968;&#31181;&#23454;&#38469;&#24179;&#28369;&#20551;&#35774;$(L_0,L_1)$-&#24179;&#28369;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#20165;&#26377;&#20114;&#26131;&#22122;&#22768;&#26041;&#24046;&#21644;&#26377;&#30028;&#24179;&#28369;&#24615;&#20551;&#35774;&#30340;&#38750;&#20984;&#30446;&#26631;&#30340;AdaGrad&#31639;&#27861;&#30340;&#31616;&#21333;&#25910;&#25947;&#24615;&#35777;&#26126;&#12290;&#35813;&#35777;&#26126;&#22522;&#26412;&#19978;&#22522;&#20110;&#19968;&#20010;&#26032;&#39062;&#30340;&#36741;&#21161;&#20989;&#25968;$\xi$&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#22788;&#29702;AdaGrad&#26356;&#26032;&#30340;&#20998;&#23376;&#21644;&#20998;&#27597;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#27604;&#29616;&#26377;&#32467;&#26524;\citep{faw2022power}&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#65292;&#24182;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#20960;&#31181;&#26032;&#30340;&#37325;&#35201;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;AdaGrad&#21482;&#38656;&#35201;$\mathcal{O}(\frac{1}{\varepsilon^2})$&#27425;&#36845;&#20195;&#65292;&#23601;&#21487;&#20197;&#30830;&#20445;&#26799;&#24230;&#33539;&#25968;&#23567;&#20110;$\varepsilon$&#65292;&#36825;&#19982;SGD&#30340;&#36895;&#29575;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#27604;AdaGrad&#30340;&#29616;&#26377;&#36895;&#29575;$\mathcal{O}(\frac{1}{\varepsilon^4})$&#26126;&#26174;&#26356;&#32039;&#23494;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25918;&#24323;&#26377;&#30028;&#24179;&#28369;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#19968;&#31181;&#31216;&#20026;$(L_0,L_1)$-&#24179;&#28369;&#26465;&#20214;&#30340;&#23454;&#38469;&#24179;&#28369;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#20801;&#35768;&#26412;&#22320;&#24179;&#28369;&#24615;&#22686;&#38271;
&lt;/p&gt;
&lt;p&gt;
We provide a simple convergence proof for AdaGrad optimizing non-convex objectives under only affine noise variance and bounded smoothness assumptions. The proof is essentially based on a novel auxiliary function $\xi$ that helps eliminate the complexity of handling the correlation between the numerator and denominator of AdaGrad's update. Leveraging simple proofs, we are able to obtain tighter results than existing results \citep{faw2022power} and extend the analysis to several new and important cases. Specifically, for the over-parameterized regime, we show that AdaGrad needs only $\mathcal{O}(\frac{1}{\varepsilon^2})$ iterations to ensure the gradient norm smaller than $\varepsilon$, which matches the rate of SGD and significantly tighter than existing rates $\mathcal{O}(\frac{1}{\varepsilon^4})$ for AdaGrad. We then discard the bounded smoothness assumption and consider a realistic assumption on smoothness called $(L_0,L_1)$-smooth condition, which allows local smoothness to grow w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;Top-k&#31232;&#30095;&#21270;&#20943;&#23569;&#36890;&#20449;&#25928;&#29575;&#30340;Split Learning&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#30456;&#21516;&#21387;&#32553;&#27700;&#24179;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18469</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;Top-k&#31232;&#30095;&#21270;&#20943;&#23569;Split Learning&#30340;&#36890;&#20449;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Reducing Communication for Split Learning by Randomized Top-k Sparsification. (arXiv:2305.18469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;Top-k&#31232;&#30095;&#21270;&#20943;&#23569;&#36890;&#20449;&#25928;&#29575;&#30340;Split Learning&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#30456;&#21516;&#21387;&#32553;&#27700;&#24179;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split Learning&#26159;Vertical Federated Learning (VFL)&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#22312;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36890;&#20449;&#25928;&#29575;&#20173;&#28982;&#26159;Split Learning&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31181;Split Learning&#30340;&#36890;&#20449;&#37327;&#20943;&#23569;&#26041;&#27861;&#65292;&#21253;&#25324;&#20943;&#23569;&#20999;&#21106;&#23618;&#22823;&#23567;&#12289;Top-k&#31232;&#30095;&#21270;&#12289;&#37327;&#21270;&#21644;L1&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#23545;&#20943;&#23569;&#20999;&#21106;&#23618;&#22823;&#23567;&#21644;Top-k&#31232;&#30095;&#21270;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#38543;&#26426;Top-k&#31232;&#30095;&#21270;&#65292;&#20197;&#20351;&#27169;&#22411;&#26356;&#22909;&#22320;&#25512;&#24191;&#21644;&#25910;&#25947;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#36873;&#25321;Top-k&#20803;&#32032;&#30340;&#21516;&#26102;&#65292;&#26377;&#23567;&#27010;&#29575;&#22320;&#36873;&#25321;&#38750;Top-k&#20803;&#32032;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#36890;&#20449;&#37327;&#20943;&#23569;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#38543;&#26426;Top-k&#31232;&#30095;&#21270;&#22312;&#30456;&#21516;&#21387;&#32553;&#27700;&#24179;&#19979;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning is a simple solution for Vertical Federated Learning (VFL), which has drawn substantial attention in both research and application due to its simplicity and efficiency. However, communication efficiency is still a crucial issue for split learning. In this paper, we investigate multiple communication reduction methods for split learning, including cut layer size reduction, top-k sparsification, quantization, and L1 regularization. Through analysis of the cut layer size reduction and top-k sparsification, we further propose randomized top-k sparsification, to make the model generalize and converge better. This is done by selecting top-k elements with a large probability while also having a small probability to select non-top-k elements. Empirical results show that compared with other communication-reduction methods, our proposed randomized top-k sparsification achieves a better model performance under the same compression level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#27969;&#24418;&#37319;&#26679;&#28857;&#26500;&#36896;&#30340;&#22270;&#19982;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#22270;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;&#36830;&#32493;&#27969;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#28388;&#27874;&#22120;&#30340;&#21487;&#20998;&#24615;&#21644;&#36817;&#20284;&#27969;&#24418;&#28388;&#27874;&#22120;&#25152;&#38656;&#34892;&#20026;&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#26435;&#34913;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#20110;&#38750;&#32447;&#24615;&#30340;&#39057;&#29575;&#28151;&#21512;&#23646;&#24615;&#32780;&#24471;&#21040;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.18467</link><description>&lt;p&gt;
&#20960;&#20309;&#22270;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#65306;&#26497;&#38480;&#24615;&#36136;&#21644;&#21028;&#21035;&#24230;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Geometric Graph Filters and Neural Networks: Limit Properties and Discriminability Trade-offs. (arXiv:2305.18467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#27969;&#24418;&#37319;&#26679;&#28857;&#26500;&#36896;&#30340;&#22270;&#19982;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#22270;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;&#36830;&#32493;&#27969;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#28388;&#27874;&#22120;&#30340;&#21487;&#20998;&#24615;&#21644;&#36817;&#20284;&#27969;&#24418;&#28388;&#27874;&#22120;&#25152;&#38656;&#34892;&#20026;&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#26435;&#34913;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#20110;&#38750;&#32447;&#24615;&#30340;&#39057;&#29575;&#28151;&#21512;&#23646;&#24615;&#32780;&#24471;&#21040;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;MNN&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24403;&#22270;&#26159;&#20174;&#27969;&#24418;&#37319;&#26679;&#28857;&#26500;&#36896;&#32780;&#25104;&#26102;&#65292;&#20174;&#32780;&#32534;&#30721;&#20960;&#20309;&#20449;&#24687;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21367;&#31215;MNN&#21644;GNN&#65292;&#20854;&#20013;&#27969;&#24418;&#21644;&#22270;&#21367;&#31215;&#20998;&#21035;&#20197;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#21644;&#22270;Laplacian&#20026;&#23450;&#20041;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#26680;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23494;&#38598;&#21644;&#20013;&#31561;&#31232;&#30095;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#65292;&#34920;&#26126;&#36825;&#20123;&#22270;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;&#36830;&#32493;&#27969;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#36825;&#20010;&#20998;&#26512;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#22270;&#28388;&#27874;&#22120;&#30340;&#21487;&#20998;&#24615;&#21644;&#36817;&#20284;&#27969;&#24418;&#28388;&#27874;&#22120;&#25152;&#38656;&#34892;&#20026;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#37325;&#35201;&#26435;&#34913;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#26435;&#34913;&#22914;&#20309;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#20110;&#38750;&#32447;&#24615;&#30340;&#39057;&#29575;&#28151;&#21512;&#23646;&#24615;&#32780;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the relationship between a graph neural network (GNN) and a manifold neural network (MNN) when the graph is constructed from a set of points sampled from the manifold, thus encoding geometric information. We consider convolutional MNNs and GNNs where the manifold and the graph convolutions are respectively defined in terms of the Laplace-Beltrami operator and the graph Laplacian. Using the appropriate kernels, we analyze both dense and moderately sparse graphs. We prove non-asymptotic error bounds showing that convolutional filters and neural networks on these graphs converge to convolutional filters and neural networks on the continuous manifold. As a byproduct of this analysis, we observe an important trade-off between the discriminability of graph filters and their ability to approximate the desired behavior of manifold filters. We then discuss how this trade-off is ameliorated in neural networks due to the frequency mixing property of nonlinearities. We further d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18466</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35768;&#22810;&#24037;&#20316;&#37117;&#26088;&#22312;&#22312;&#27979;&#35797;&#26102;&#20174;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#20854;&#26631;&#20934;&#35757;&#32451;&#35774;&#32622;&#23545;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#38656;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#8220;Pile&#8221;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26368;&#36817;&#37051;&#32034;&#24341;&#12290;&#32473;&#23450;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26816;&#32034;&#26597;&#35810;&#30340;&#37051;&#23621;&#65292;&#24182;&#22312;&#23545;&#24212;&#20110;&#36825;&#20123;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26816;&#32034;&#21644;&#35757;&#32451;&#20165;20&#20010;&#37051;&#23621;&#65292;&#27599;&#20010;&#37051;&#23621;&#20165;&#36827;&#34892;&#19968;&#27425;&#26799;&#24230;&#36845;&#20195;&#65292;&#23601;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#8220;Pile&#8221;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20108;&#21313;&#20010;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26174;&#33879;&#32553;&#23567;&#20102;&#23567;&#22411;GPT2&#27169;&#22411;&#21644;GPTNeo&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21518;&#32773;&#26159;&#19987;&#38376;&#23545;&#8220;Pile&#8221;&#36827;&#34892;&#25910;&#25947;&#35757;&#32451;&#30340;&#65292;&#20307;&#31215;&#21364;&#26159;&#21069;&#32773;&#30340;&#21313;&#20493;&#20197;&#19978;&#12290;&#28982;&#32780;&#65292;&#20854;&#26041;&#27861;&#30340;&#25104;&#21151;&#36824;&#21462;&#20915;&#20110;&#20805;&#20998;&#30340;&#32034;&#24341;&#36136;&#37327;&#21644;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;Gboard&#20013;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;(DP)&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23458;&#25143;&#21442;&#19982;&#26631;&#20934;&#65292;&#22312;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335;DP&#20445;&#35777;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#21033;&#30340;&#38544;&#31169;-&#25928;&#29992;&#20132;&#25442;&#12290;&#22312;&#23545;&#20844;&#20849;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#37096;&#32626;&#20102;&#36229;&#36807;20&#20010;LMs&#20197;&#23454;&#29616;&#39640;&#25928;&#29992;&#21644;$\rho-$zCDP&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18465</link><description>&lt;p&gt;
&#12298;&#24102;&#24046;&#20998;&#38544;&#31169;&#30340;Gboard&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Gboard Language Models with Differential Privacy. (arXiv:2305.18465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;Gboard&#20013;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;(DP)&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23458;&#25143;&#21442;&#19982;&#26631;&#20934;&#65292;&#22312;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335;DP&#20445;&#35777;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#21033;&#30340;&#38544;&#31169;-&#25928;&#29992;&#20132;&#25442;&#12290;&#22312;&#23545;&#20844;&#20849;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#37096;&#32626;&#20102;&#36229;&#36807;20&#20010;LMs&#20197;&#23454;&#29616;&#39640;&#25928;&#29992;&#21644;$\rho-$zCDP&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35895;&#27468;&#38190;&#30424;(Gboard)&#20013;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;(DP)&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LMs)&#12290;&#25105;&#20204;&#24212;&#29992;DP-FTRL&#31639;&#27861;&#65292;&#22312;&#19981;&#35201;&#27714;&#23545;&#23458;&#25143;&#35774;&#22791;&#36827;&#34892;&#22343;&#21248;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335; DP &#20445;&#35777;&#12290;&#20026;&#20102;&#25552;&#20379;&#26377;&#21033;&#30340;&#38544;&#31169;-&#25928;&#29992;&#20132;&#25442;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#23458;&#25143;&#21442;&#19982;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#20013;&#30340;&#37197;&#32622;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#21098;&#20999;&#20272;&#35745;&#19982;DP-FTRL&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#36873;&#25321;&#21098;&#20999;&#33539;&#25968;&#25110;&#20943;&#23569;&#36229;&#21442;&#25968;&#35843;&#25972;&#20197;&#20934;&#22791;&#35757;&#32451;&#12290;&#20511;&#21161;&#20110;&#23545;&#20844;&#20849;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#37096;&#32626;&#20102;&#36229;&#36807;20&#20010;Gboard LM&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;$\rho \in (0.2, 2)$&#19979;&#23454;&#29616;&#20102;&#39640;&#25928;&#29992;&#21644;$\rho-$zCDP&#38544;&#31169;&#20445;&#35777;&#65292;&#20854;&#20013;&#20004;&#20010;&#27169;&#22411;&#36824;&#20351;&#29992;&#20102;&#23433;&#20840;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
We train language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). We apply the DP-Follow-the-Regularized-Leader (DP-FTRL)~\citep{kairouz21b} algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation~\citep{andrew2019differentially} can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation for training. With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and $\rho-$zCDP privacy guarantees with $\rho \in (0.2, 2)$, with two models additionally trained with secure aggregation~\citep{bonawitz2017practical}. We are happy to announce tha
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#38454;&#27573;&#29305;&#26435;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65288;HIB&#65289;&#65292;&#36890;&#36807;&#25429;&#25417;&#21382;&#21490;&#36712;&#36857;&#30340;&#29305;&#26435;&#30693;&#35782;&#34920;&#31034;&#26469;&#23398;&#20064;&#65292;&#32553;&#23567;&#27169;&#25311;&#21644;&#30495;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.18464</link><description>&lt;p&gt;
&#29305;&#26435;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110; Sim-to-Real &#31574;&#30053;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Privileged Knowledge Distillation for Sim-to-Real Policy Generalization. (arXiv:2305.18464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18464
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#38454;&#27573;&#29305;&#26435;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65288;HIB&#65289;&#65292;&#36890;&#36807;&#25429;&#25417;&#21382;&#21490;&#36712;&#36857;&#30340;&#29305;&#26435;&#30693;&#35782;&#34920;&#31034;&#26469;&#23398;&#20064;&#65292;&#32553;&#23567;&#27169;&#25311;&#21644;&#30495;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#37027;&#37324;&#30340;&#29305;&#26435;&#30693;&#35782;&#65288;&#20363;&#22914;&#21160;&#21147;&#23398;&#65292;&#29615;&#22659;&#65292;&#22320;&#24418;&#65289;&#26159;&#36731;&#26494;&#33719;&#21462;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#26426;&#22120;&#20154;&#20195;&#29702;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;&#26412;&#22320;&#29366;&#24577;&#65288;&#20363;&#22914;&#26426;&#22120;&#20154;&#20851;&#33410;&#30340;&#26412;&#20307;&#24863;&#21453;&#39304;&#65289;&#26469;&#36873;&#25321;&#21160;&#20316;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24046;&#36317;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#36880;&#28176;&#20943;&#23569;&#23545;&#29305;&#26435;&#30693;&#35782;&#30340;&#20381;&#36182;&#25110;&#25191;&#34892;&#20004;&#38454;&#27573;&#31574;&#30053;&#27169;&#20223;&#26469;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#12290;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20805;&#20998;&#21033;&#29992;&#29305;&#26435;&#30693;&#35782;&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21382;&#21490;&#20449;&#24687;&#29942;&#39048;&#65288;HIB&#65289;&#30340;&#26032;&#22411;&#21333;&#38454;&#27573;&#29305;&#26435;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#32553;&#23567;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HIB&#36890;&#36807;&#25429;&#25417;&#21382;&#21490;&#36712;&#36857;&#30340;&#29305;&#26435;&#30693;&#35782;&#34920;&#31034;&#26469;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has recently achieved remarkable success in robotic control. However, most RL methods operate in simulated environments where privileged knowledge (e.g., dynamics, surroundings, terrains) is readily available. Conversely, in real-world scenarios, robot agents usually rely solely on local states (e.g., proprioceptive feedback of robot joints) to select actions, leading to a significant sim-to-real gap. Existing methods address this gap by either gradually reducing the reliance on privileged knowledge or performing a two-stage policy imitation. However, we argue that these methods are limited in their ability to fully leverage the privileged knowledge, resulting in suboptimal performance. In this paper, we propose a novel single-stage privileged knowledge distillation method called the Historical Information Bottleneck (HIB) to narrow the sim-to-real gap. In particular, HIB learns a privileged knowledge representation from historical trajectories by capturing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#26469;&#25552;&#39640;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#36825;&#20123;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18462</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#35821;&#35328;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#26469;&#25552;&#39640;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#36825;&#20123;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIAs)&#26088;&#22312;&#39044;&#27979;&#19968;&#20010;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#23384;&#22312;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#25915;&#20987;&#20381;&#36182;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;&#26356;&#39640;&#30340;&#27010;&#29575;&#20998;&#37197;&#32473;&#35757;&#32451;&#26679;&#26412;&#32780;&#38750;&#38750;&#35757;&#32451;&#28857;&#12290;&#28982;&#32780;&#65292;&#23545;&#27169;&#22411;&#20998;&#25968;&#30340;&#31616;&#21333;&#38408;&#20540;&#35774;&#23450;&#24448;&#24448;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#26679;&#26412;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21442;&#32771;&#27169;&#22411;&#30340;&#25915;&#20987;&#21487;&#20197;&#23558;&#27169;&#22411;&#20998;&#25968;&#19982;&#22312;&#31867;&#20284;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#33719;&#24471;&#30340;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;MIAs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35757;&#32451;&#21442;&#32771;&#27169;&#22411;&#65292;&#36825;&#31181;&#25915;&#20987;&#30340;&#20570;&#27861;&#26159;&#20551;&#23450;&#25932;&#26041;&#30693;&#36947;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20551;&#23450;&#25915;&#20987;&#32773;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#37051;&#22495;&#26679;&#26412;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#22312;&#26356;&#29616;&#23454;&#30340;&#25104;&#21592;&#25512;&#26029;&#22330;&#26223;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#20960;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#23545;&#35805;&#29983;&#25104;&#65292;&#24182;&#31361;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic sce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20219;&#20309;&#25299;&#25169;&#32467;&#26500;&#19978;&#23454;&#29616;&#24102;&#23485;&#26368;&#20248;&#30340;&#20840;&#32858;&#21512;/&#24402;&#32422;&#25955;&#24320;&#65292;&#20026;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#36890;&#29992;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20854;&#20182;&#24418;&#24335;&#30340;&#38598;&#21512;&#36890;&#20449;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18461</link><description>&lt;p&gt;
&#38598;&#21512;&#36890;&#20449;&#24102;&#23485;&#26368;&#20248;&#31649;&#36947;&#35843;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bandwidth Optimal Pipeline Schedule for Collective Communication. (arXiv:2305.18461v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20219;&#20309;&#25299;&#25169;&#32467;&#26500;&#19978;&#23454;&#29616;&#24102;&#23485;&#26368;&#20248;&#30340;&#20840;&#32858;&#21512;/&#24402;&#32422;&#25955;&#24320;&#65292;&#20026;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#36890;&#29992;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20854;&#20182;&#24418;&#24335;&#30340;&#38598;&#21512;&#36890;&#20449;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20219;&#20309;&#32593;&#32476;&#25299;&#25169;&#19978;&#65288;&#26080;&#35770;&#26159;&#21542;&#26377;&#20132;&#25442;&#26426;&#65289;&#29983;&#25104;&#24102;&#23485;&#26368;&#20248;&#30340;&#20840;&#32858;&#21512;/&#24402;&#32422;&#25955;&#24320;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26500;&#24314;&#20102;&#19968;&#31181;&#31649;&#36947;&#35843;&#24230;&#65292;&#21487;&#22312;&#32473;&#23450;&#25299;&#25169;&#20013;&#23454;&#29616;&#20445;&#35777;&#26368;&#20339;&#21487;&#34892;&#24102;&#23485;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23558;&#32593;&#32476;&#25299;&#25169;&#24314;&#27169;&#20026;&#20855;&#26377;&#24322;&#26500;&#38142;&#36335;&#23481;&#37327;&#21644;&#20132;&#25442;&#26426;&#30340;&#26377;&#21521;&#22270;&#65292;&#24182;&#23558;&#20132;&#25442;&#26426;&#30452;&#25509;&#24314;&#27169;&#20026;&#22270;&#34920;&#31034;&#20013;&#30340;&#39030;&#28857;&#12290;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#25299;&#25169;&#22823;&#23567;&#20855;&#26377;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#12290;&#27492;&#24037;&#20316;&#20005;&#37325;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#22270;&#35770;&#30740;&#31350;&#20013;&#30340;&#36793;&#19981;&#30456;&#20132;&#29983;&#25104;&#26641;&#21644;&#36793;&#20998;&#35010;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20840;&#32858;&#21512;&#65292;&#20294;&#26412;&#25991;&#20013;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20026;&#24402;&#32422;&#12289;&#24191;&#25773;&#12289;&#24402;&#32422;&#25955;&#24320;&#21644;&#20840;&#24402;&#32422;&#30340;&#35843;&#24230;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a strongly polynomial-time algorithm to generate bandwidth optimal allgather/reduce-scatter on any network topology, with or without switches. Our algorithm constructs pipeline schedules achieving provably the best possible bandwidth performance on a given topology. To provide a universal solution, we model the network topology as a directed graph with heterogeneous link capacities and switches directly as vertices in the graph representation. The algorithm is strongly polynomial-time with respect to the topology size. This work heavily relies on previous graph theory work on edge-disjoint spanning trees and edge splitting. While we focus on allgather, the methods in this paper can be easily extended to generate schedules for reduce, broadcast, reduce-scatter, and allreduce.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#20855;&#26377;&#20020;&#30028;&#23485;&#24230;&#30340;Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#22495;K&#19978;&#23454;&#29616;$L^p(K,\mathbb{R}^{d_y})$&#30340;UAP&#65292;&#32780;&#26412;&#25991;&#32473;&#20986;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$&#21017;&#36866;&#29992;&#20110;&#20989;&#25968;&#31867;$C(K,\mathbb{R}^{d_y})$&#65292;&#32771;&#34385;&#21040;&#36755;&#20986;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18460</link><description>&lt;p&gt;
Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#22343;&#21248;&#36890;&#29992;&#36924;&#36817;&#20013;&#30340;&#26368;&#23567;&#23485;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal Approximation. (arXiv:2305.18460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18460
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#20855;&#26377;&#20020;&#30028;&#23485;&#24230;&#30340;Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#22495;K&#19978;&#23454;&#29616;$L^p(K,\mathbb{R}^{d_y})$&#30340;UAP&#65292;&#32780;&#26412;&#25991;&#32473;&#20986;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$&#21017;&#36866;&#29992;&#20110;&#20989;&#25968;&#31867;$C(K,\mathbb{R}^{d_y})$&#65292;&#32771;&#34385;&#21040;&#36755;&#20986;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#65288;UAP&#65289;&#30340;&#30740;&#31350;&#21382;&#21490;&#24736;&#20037;&#12290;&#24403;&#32593;&#32476;&#23485;&#24230;&#19981;&#21463;&#38480;&#21046;&#26102;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#38544;&#34255;&#23618;&#21363;&#21487;&#36827;&#34892;UAP&#12290;&#30456;&#21453;&#65292;&#24403;&#28145;&#24230;&#19981;&#21463;&#38480;&#21046;&#26102;&#65292;UAP&#30340;&#23485;&#24230;&#38656;&#35201;&#19981;&#23567;&#20110;&#20020;&#30028;&#23485;&#24230;$w^*_{\min}=\max(d_x,d_y)$, &#20854;&#20013;$d_x$&#21644;$d_y$&#20998;&#21035;&#26159;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#12290;&#26368;&#36817;&#65292;\cite{cai2022achieve}&#34920;&#26126;&#65292;&#20855;&#26377;&#36825;&#31181;&#20020;&#30028;&#23485;&#24230;&#30340;Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#22495;$K$&#19978;&#23454;&#29616;$L^p$&#20989;&#25968;&#30340;UAP&#65292;&#21363;$L^p(K,\mathbb{R}^{d_y})$&#30340;UAP&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20989;&#25968;&#31867;$C(K,\mathbb{R}^{d_y})$&#30340;&#22343;&#21248;UAP&#65292;&#24182;&#32473;&#20986;&#20102;Leaky-ReLU NN&#30340;&#30830;&#20999;&#26368;&#23567;&#23485;&#24230;&#65292;&#20026;$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$&#65292;&#20854;&#20013;&#28041;&#21450;&#36755;&#20986;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;lift-flow-discretization&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22343;&#21248;UAP&#19982;&#25299;&#25169;&#29702;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of universal approximation properties (UAP) for neural networks (NN) has a long history. When the network width is unlimited, only a single hidden layer is sufficient for UAP. In contrast, when the depth is unlimited, the width for UAP needs to be not less than the critical width $w^*_{\min}=\max(d_x,d_y)$, where $d_x$ and $d_y$ are the dimensions of the input and output, respectively. Recently, \cite{cai2022achieve} shows that a leaky-ReLU NN with this critical width can achieve UAP for $L^p$ functions on a compact domain $K$, \emph{i.e.,} the UAP for $L^p(K,\mathbb{R}^{d_y})$. This paper examines a uniform UAP for the function class $C(K,\mathbb{R}^{d_y})$ and gives the exact minimum width of the leaky-ReLU NN as $w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$, which involves the effects of the output dimensions. To obtain this result, we propose a novel lift-flow-discretization approach that shows that the uniform UAP has a deep connection with topological theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;Multi-Task Diffusion Model (MTDiff)&#65292;&#32467;&#21512;&#20102;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22914;&#27492;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#29615;&#22659;&#19979;&#21462;&#24471;&#30456;&#24403;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18459</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#35268;&#21010;&#22120;&#21644;&#25968;&#25454;&#21512;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. (arXiv:2305.18459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;Multi-Task Diffusion Model (MTDiff)&#65292;&#32467;&#21512;&#20102;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22914;&#27492;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#29615;&#22659;&#19979;&#21462;&#24471;&#30456;&#24403;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;NLP&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#20063;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#22797;&#26434;&#31574;&#30053;&#25110;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#35774;&#32622;&#65292;&#27809;&#26377;&#32771;&#34385;&#22810;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#22810;&#26679;&#21270;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Task Diffusion Model&#65288;MTDiff&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#29983;&#25104;&#35268;&#21010;&#21644;&#25968;&#25454;&#21512;&#25104;&#12290;MTDiff&#21033;&#29992;&#22823;&#37327;&#21487;&#29992;&#20110;&#22810;&#20219;&#21153;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#25191;&#34892;&#38544;&#24335;&#30693;&#35782;&#20849;&#20139;&#20197;&#36827;&#34892;&#34394;&#25311;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#23545;&#25239;&#25903;&#25345;&#23545;&#40784;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#29305;&#24449;&#34920;&#31034;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#31216;&#25903;&#25345;&#20998;&#27495;&#65292;&#38024;&#23545;&#26631;&#31614;&#20998;&#24067;&#20559;&#31227;&#30340;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.18458</link><description>&lt;p&gt;
&#24102;&#26631;&#31614;&#20559;&#31227;&#30340;&#22495;&#36866;&#24212;&#20013;&#30340;&#26465;&#20214;&#25903;&#25345;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Conditional Support Alignment for Domain Adaptation with Label Shift. (arXiv:2305.18458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#23545;&#25239;&#25903;&#25345;&#23545;&#40784;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#29305;&#24449;&#34920;&#31034;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#31216;&#25903;&#25345;&#20998;&#27495;&#65292;&#38024;&#23545;&#26631;&#31614;&#20998;&#24067;&#20559;&#31227;&#30340;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#36866;&#24212; (UDA) &#26159;&#25351;&#19968;&#31181;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#27169;&#22411;&#22522;&#20110;&#28304;&#22495;&#19978;&#30340;&#26631;&#35760;&#26679;&#26412;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#39046;&#22495;&#20869;&#29616;&#26377;&#30340;&#20027;&#27969;&#26041;&#27861;&#20381;&#36182;&#20110;&#32463;&#20856;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#20551;&#35774;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26631;&#31614;&#20998;&#24067;&#20559;&#31227;&#19979;&#65292;&#24615;&#33021;&#20122;&#20248;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#23545;&#25239;&#25903;&#25345;&#23545;&#40784; (CASA)&#65292;&#20854;&#26088;&#22312;&#26368;&#23567;&#21270;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#29305;&#24449;&#34920;&#31034;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#31216;&#25903;&#25345;&#20998;&#27495;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#34920;&#31034;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#30446;&#26631;&#39118;&#38505;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#35843;&#25972;&#26465;&#20214;&#29305;&#24449;&#20998;&#24067;&#25903;&#25345;&#19982;&#29616;&#26377;&#30340;UDA&#35774;&#32622;&#20013;&#30340;&#36793;&#32536;&#25903;&#25345;&#23545;&#40784;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) refers to a domain adaptation framework in which a learning model is trained based on the labeled samples on the source domain and unlabelled ones in the target domain. The dominant existing methods in the field that rely on the classical covariate shift assumption to learn domain-invariant feature representation have yielded suboptimal performance under the label distribution shift between source and target domains. In this paper, we propose a novel conditional adversarial support alignment (CASA) whose aim is to minimize the conditional symmetric support divergence between the source's and target domain's feature representation distributions, aiming at a more helpful representation for the classification task. We also introduce a novel theoretical target risk bound, which justifies the merits of aligning the supports of conditional feature distributions compared to the existing marginal support alignment approach in the UDA settings. We then provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;D$^2$PT&#65292;&#19968;&#20010;&#21452;&#36890;&#36947;&#30340;GNN&#26694;&#26550;&#65292;&#20197;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#25968;&#25454;&#32570;&#22833;&#19988;&#30456;&#20114;&#24433;&#21709;&#30340;&#24773;&#20917;&#65292;&#20854;&#20851;&#38190;&#28857;&#21253;&#25324;&#22312;GNN&#20013;&#23454;&#29616;&#38271;&#31243;&#20256;&#25773;&#21644;&#20801;&#35768;&#20449;&#24687;&#20256;&#25773;&#21040;&#20559;&#31163;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.18457</link><description>&lt;p&gt;
&#23398;&#20064;&#24378;&#22823;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20449;&#24687;&#19981;&#36275;&#24773;&#20917;&#19979;
&lt;/p&gt;
&lt;p&gt;
Learning Strong Graph Neural Networks with Weak Information. (arXiv:2305.18457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;D$^2$PT&#65292;&#19968;&#20010;&#21452;&#36890;&#36947;&#30340;GNN&#26694;&#26550;&#65292;&#20197;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#25968;&#25454;&#32570;&#22833;&#19988;&#30456;&#20114;&#24433;&#21709;&#30340;&#24773;&#20917;&#65292;&#20854;&#20851;&#38190;&#28857;&#21253;&#25324;&#22312;GNN&#20013;&#23454;&#29616;&#38271;&#31243;&#20256;&#25773;&#21644;&#20801;&#35768;&#20449;&#24687;&#20256;&#25773;&#21040;&#20559;&#31163;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#36755;&#20837;&#30340;&#22270;&#25968;&#25454;&#23384;&#22312;&#20449;&#24687;&#19981;&#36275;&#26102;&#65288;&#21363;&#19981;&#23436;&#25972;&#30340;&#32467;&#26500;&#12289;&#19981;&#23436;&#25972;&#30340;&#29305;&#24449;&#21644;&#19981;&#20805;&#20998;&#30340;&#26631;&#31614;&#65289;&#65292;GNN&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#22823;&#22810;&#25968;&#27492;&#21069;&#30340;&#30740;&#31350;&#37117;&#35797;&#22270;&#20174;&#20855;&#26377;&#29305;&#23450;&#31867;&#22411;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20294;&#36825;&#36828;&#19981;&#36275;&#20197;&#26377;&#25928;&#24212;&#23545;&#21508;&#31181;&#25968;&#25454;&#32570;&#22833;&#23384;&#22312;&#19988;&#30456;&#20114;&#24433;&#21709;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#21644;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#24369;&#20449;&#24687;&#30340;&#22270;&#23398;&#20064;&#38382;&#39064;(GLWI)&#12290;&#22522;&#20110;&#25105;&#20204;&#32463;&#39564;&#20998;&#26512;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20004;&#20010;&#35774;&#35745;&#20851;&#38190;&#28857;&#26469;&#35299;&#20915;GLWI&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;GNN&#20013;&#23454;&#29616;&#38271;&#31243;&#20256;&#25773;&#65292;&#24182;&#20801;&#35768;&#20449;&#24687;&#20256;&#25773;&#21040;&#37027;&#20123;&#19982;&#26368;&#22823;&#36830;&#25509;&#32452;&#20214;&#38548;&#31163;&#30340;&#20559;&#31163;&#33410;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D$^2$PT&#65292;&#36825;&#26159;&#19968;&#20010;&#21452;&#36890;&#36947;&#30340;GNN&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D$^2$PT, a dual-channel GNN framework that perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#22522;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24102;&#26377;&#27700;&#21360;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#31639;&#27861;&#20998;&#26512;&#20102;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#25152;&#20135;&#29983;&#30340;&#36755;&#20986;&#20998;&#24067;&#21644;&#36923;&#36753;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18456</link><description>&lt;p&gt;
&#35782;&#21035;&#27700;&#21360;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Baselines for Identifying Watermarked Large Language Models. (arXiv:2305.18456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#22522;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24102;&#26377;&#27700;&#21360;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#31639;&#27861;&#20998;&#26512;&#20102;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#25152;&#20135;&#29983;&#30340;&#36755;&#20986;&#20998;&#24067;&#21644;&#36923;&#36753;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26032;&#20852;&#38382;&#39064;&#65306;&#22914;&#20309;&#35782;&#21035;&#24191;&#27867;&#20351;&#29992;&#30340;&#65292;&#20844;&#24320;&#25176;&#31649;&#30340;&#65292;&#38381;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#27700;&#21360;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#22522;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;LLM&#27700;&#21360;&#65292;&#36825;&#20123;&#31639;&#27861;&#20381;&#36182;&#20110;&#20998;&#26512;&#30001;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20196;&#29260;&#21644;&#36923;&#36753;&#20998;&#24067;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24102;&#26377;&#27700;&#21360;&#30340;LLMs tend to&#20135;&#29983;&#20998;&#24067;&#19982;&#26631;&#20934;&#27169;&#22411; qualitatively and identifiably&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#24378;&#24230;&#19979;&#35782;&#21035;&#27700;&#21360;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#32771;&#34385;&#20102;&#27599;&#20010;&#35782;&#21035;&#26426;&#21046;&#22312;&#27700;&#21360;&#22330;&#26223;&#20013;&#30340;&#26435;&#34913;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#21270;&#20102;&#35782;&#21035;LLMs&#20013;&#30340;&#27700;&#21360;&#30340;&#20855;&#20307;&#38382;&#39064;&#65292;&#20197;&#21450;LLM&#27700;&#21360;&#21644;&#27700;&#21360;&#26816;&#27979;&#30340;&#19968;&#33324;&#24615;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#23427;&#20204;&#25552;&#20379;&#20102;&#26694;&#26550;&#21644;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the emerging problem of identifying the presence and use of watermarking schemes in widely used, publicly hosted, closed source large language models (LLMs). We introduce a suite of baseline algorithms for identifying watermarks in LLMs that rely on analyzing distributions of output tokens and logits generated by watermarked and unmarked LLMs. Notably, watermarked LLMs tend to produce distributions that diverge qualitatively and identifiably from standard models. Furthermore, we investigate the identifiability of watermarks at varying strengths and consider the tradeoffs of each of our identification mechanisms with respect to watermarking scenario. Along the way, we formalize the specific problem of identifying watermarks in LLMs, as well as LLM watermarks and watermark detection in general, providing a framework and foundations for studying them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550; Diff-Instruct&#65292;&#33021;&#22815;&#20197;&#26080;&#38656;&#25968;&#25454;&#26041;&#24335;&#23558;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#65292;&#20165;&#38656;&#39044;&#35757;&#32451; DM &#21644;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#26159;&#24314;&#31435;&#22312;&#20005;&#35880;&#30340;&#25968;&#23398;&#22522;&#30784;&#19978;&#30340;&#65292;&#25351;&#23548;&#36807;&#31243;&#30452;&#25509;&#23545;&#24212;&#20110;&#26368;&#23567;&#21270;&#19968;&#31181;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;Integral Kullback-Leibler (IKL) &#25955;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#20687;&#21512;&#25104;&#21644;&#35270;&#39057;&#39044;&#27979;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18455</link><description>&lt;p&gt;
Diff-Instruct: &#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. (arXiv:2305.18455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550; Diff-Instruct&#65292;&#33021;&#22815;&#20197;&#26080;&#38656;&#25968;&#25454;&#26041;&#24335;&#23558;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#65292;&#20165;&#38656;&#39044;&#35757;&#32451; DM &#21644;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#26159;&#24314;&#31435;&#22312;&#20005;&#35880;&#30340;&#25968;&#23398;&#22522;&#30784;&#19978;&#30340;&#65292;&#25351;&#23548;&#36807;&#31243;&#30452;&#25509;&#23545;&#24212;&#20110;&#26368;&#23567;&#21270;&#19968;&#31181;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;Integral Kullback-Leibler (IKL) &#25955;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#20687;&#21512;&#25104;&#21644;&#35270;&#39057;&#39044;&#27979;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35757;&#32451;&#23481;&#26131;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#26679;&#26412;&#36136;&#37327;&#39640;&#65292;&#25193;&#25955;&#27169;&#22411; (DMs) &#24050;&#25104;&#20026;&#29983;&#25104;&#24314;&#27169;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#24182;&#26377;&#22823;&#37327;&#24050;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;&#12290;&#39044;&#35757;&#32451; DMs &#21253;&#21547;&#26377;&#20851;&#25968;&#25454;&#20998;&#24067;&#30340;&#22797;&#26434;&#20449;&#24687;&#65292;&#23545;&#19979;&#28216;&#24212;&#29992;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#32771;&#34385;&#20174;&#39044;&#35757;&#32451; DMs &#20013;&#23398;&#20064;&#24182;&#20197;&#26080;&#38656;&#25968;&#25454;&#26041;&#24335;&#23558;&#20854;&#30693;&#35782;&#20256;&#36882;&#32473;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550; Diff-Instruct&#65292;&#35813;&#26694;&#26550;&#33021;&#25351;&#23548;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#21482;&#35201;&#29983;&#25104;&#30340;&#26679;&#26412;&#22312;&#27169;&#22411;&#21442;&#25968;&#26041;&#38754;&#26159;&#21487;&#24494;&#30340;&#12290;&#25105;&#20204;&#30340; Diff-Instruct &#24314;&#31435;&#22312;&#19968;&#20010;&#20005;&#35880;&#30340;&#25968;&#23398;&#22522;&#30784;&#19978;&#65292;&#20854;&#20013;&#25351;&#23548;&#36807;&#31243;&#30452;&#25509;&#23545;&#24212;&#20110;&#26368;&#23567;&#21270;&#31216;&#20026;&#31215;&#20998;Kullback-Leibler (IKL) &#25955;&#24230;&#30340;&#26032;&#22411;&#25955;&#24230;&#12290;IKL &#26159;&#38024;&#23545; DMs &#23450;&#21046;&#30340;&#65292;&#36890;&#36807;&#35745;&#31639;&#27839;&#25193;&#25955;&#36712;&#36857;&#30340; KL &#25955;&#24230;&#30340;&#31215;&#20998;&#26469;&#25429;&#33719;&#25193;&#25955;&#36807;&#31243;&#20449;&#24687;&#65292;&#22240;&#27492;&#21482;&#38656;&#35201;&#19968;&#20010;&#39044;&#35757;&#32451; DM &#21644;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#31243;&#24207;&#65306;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#20687;&#21512;&#25104;&#21644;&#35270;&#39057;&#39044;&#27979;&#20013;&#23637;&#31034;&#20854;&#20248;&#36234;&#24615;&#26469;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffu
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;8600&#19975;&#20010;&#20998;&#23376;&#30340;&#30005;&#23376;&#24615;&#36136;&#65292;&#35206;&#30422;&#20102;&#20174;&#22522;&#26412;&#21270;&#21512;&#29289;&#21040;&#29983;&#29289;&#20998;&#23376;&#30340;&#21508;&#31181;&#20998;&#23376;&#65292;&#20351;&#29992;&#20102;B3LYP/6-31G*&#21644;PM6&#26041;&#27861;&#35745;&#31639;&#24471;&#20986;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#26684;&#24335;&#20197;&#21450;&#20116;&#20010;&#23376;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.18454</link><description>&lt;p&gt;
"PubChemQC B3LYP/6-31G*//PM6&#25968;&#25454;&#38598;: &#20351;&#29992;B3LYP/6-31G*&#35745;&#31639;&#30340;8600&#19975;&#20998;&#23376;&#30340;&#30005;&#23376;&#32467;&#26500;"
&lt;/p&gt;
&lt;p&gt;
PubChemQC B3LYP/6-31G*//PM6 dataset: the Electronic Structures of 86 Million Molecules using B3LYP/6-31G* calculations. (arXiv:2305.18454v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18454
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;8600&#19975;&#20010;&#20998;&#23376;&#30340;&#30005;&#23376;&#24615;&#36136;&#65292;&#35206;&#30422;&#20102;&#20174;&#22522;&#26412;&#21270;&#21512;&#29289;&#21040;&#29983;&#29289;&#20998;&#23376;&#30340;&#21508;&#31181;&#20998;&#23376;&#65292;&#20351;&#29992;&#20102;B3LYP/6-31G*&#21644;PM6&#26041;&#27861;&#35745;&#31639;&#24471;&#20986;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#26684;&#24335;&#20197;&#21450;&#20116;&#20010;&#23376;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;PubChemQC B3LYP/6-31G*//PM6&#8221;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;85938443&#20010;&#20998;&#23376;&#30340;&#30005;&#23376;&#24615;&#36136;&#12290;&#23427;&#21253;&#25324;&#36712;&#36947;&#65292;&#36712;&#36947;&#33021;&#37327;&#65292;&#24635;&#33021;&#37327;&#65292;&#20598;&#26497;&#30697;&#31561;&#30456;&#20851;&#24615;&#36136;&#12290;&#25968;&#25454;&#38598;&#22218;&#25324;&#20102;&#20174;&#22522;&#26412;&#21270;&#21512;&#29289;&#21040;&#29983;&#29289;&#20998;&#23376;&#30340;&#21508;&#31181;&#20998;&#23376;&#65292;&#20998;&#23376;&#37327;&#39640;&#36798;1000&#65292;&#35206;&#30422;&#20102;94.0%&#30340;&#21407;&#22987;PubChem Compound&#30446;&#24405;(&#25130;&#33267;2016&#24180;8&#26376;29&#26085;)&#12290;&#35813;&#30005;&#23376;&#24615;&#36136;&#26159;&#29992;B3LYP/6-31G*&#21644;PM6&#26041;&#27861;&#35745;&#31639;&#24471;&#20986;&#12290;&#25968;&#25454;&#38598;&#25552;&#20379;&#19977;&#31181;&#26684;&#24335;&#65306;(i) GAMESS&#37327;&#23376;&#21270;&#23398;&#31243;&#24207;&#25991;&#20214;&#65292;(ii) &#36873;&#23450;&#30340;JSON&#36755;&#20986;&#25991;&#20214;&#65292;(iii) &#19968;&#20010;PostgreSQL&#25968;&#25454;&#24211;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26597;&#35810;&#20998;&#23376;&#24615;&#36136;&#12290;&#20116;&#20010;&#23376;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#26356;&#20855;&#20307;&#30340;&#25968;&#25454;&#12290;&#21069;&#20004;&#20010;&#23376;&#38598;&#21253;&#25324;&#20998;&#23376;&#21547;&#26377;C&#12289;H&#12289;O&#21644;N&#65292;&#22312;300&#21644;500&#20998;&#23376;&#37327;&#20197;&#19979;&#12290;&#31532;&#19977;&#21644;&#31532;&#22235;&#20010;&#23376;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20998;&#23376;&#21547;&#26377;C&#12289;H&#12289;N&#12289;O&#12289;P&#12289;S&#12289;F&#21644;Cl&#65292;&#22312;300&#21644;500&#20998;&#23376;&#37327;&#20197;&#19979;&#12290;&#31532;&#20116;&#20010;&#23376;&#38598;
&lt;/p&gt;
&lt;p&gt;
This article presents the "PubChemQC B3LYP/6-31G*//PM6" dataset, containing electronic properties of 85,938,443 molecules. It includes orbitals, orbital energies, total energies, dipole moments, and other relevant properties. The dataset encompasses a wide range of molecules, from essential compounds to biomolecules up to 1000 molecular weight, covering 94.0% of the original PubChem Compound catalog (as of August 29, 2016). The electronic properties were calculated using the B3LYP/6-31G* and PM6 methods. The dataset is available in three formats: (i) GAMESS quantum chemistry program files, (ii) selected JSON output files, and (iii) a PostgreSQL database, enabling researchers to query molecular properties. Five sub-datasets offer more specific data. The first two subsets include molecules with C, H, O, and N, under 300 and 500 molecular weight respectively. The third and fourth subsets contain C, H, N, O, P, S, F, and Cl, under 300 and 500 molecular weight respectively. The fifth subset
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.18453</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#23427;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#25968;&#25454;&#31232;&#32570;&#12289;&#37319;&#38598;&#26041;&#27861;&#19981;&#19968;&#33268;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#12290;Med-DDPM&#30340;&#29420;&#29305;&#29305;&#28857;&#22312;&#20110;&#20351;&#29992;&#35821;&#20041;&#26465;&#20214;&#36827;&#34892;&#19977;&#32500;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23427;&#20415;&#20110;&#21019;&#24314;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;Med-DDPM&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;Med-DDPM&#22312;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;&#23427;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21644;&#23545;&#35937;&#26816;&#27979;&#30456;&#32467;&#21512;&#30340;&#8220;&#22330;&#26223;&#25193;&#25955;&#8221;&#31995;&#32479;&#65292;&#30452;&#25509;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#24863;&#21644;&#29289;&#29702;&#21512;&#29702;&#24615;&#30340;&#22330;&#26223;&#65292;&#33021;&#22815;&#36866;&#24212;&#32654;&#22269;&#19981;&#21516;&#22320;&#21306;&#29983;&#25104;&#29305;&#23450;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.18452</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#29983;&#25104;&#34892;&#36710;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Generating Driving Scenes with Diffusion. (arXiv:2305.18452v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21644;&#23545;&#35937;&#26816;&#27979;&#30456;&#32467;&#21512;&#30340;&#8220;&#22330;&#26223;&#25193;&#25955;&#8221;&#31995;&#32479;&#65292;&#30452;&#25509;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#24863;&#21644;&#29289;&#29702;&#21512;&#29702;&#24615;&#30340;&#22330;&#26223;&#65292;&#33021;&#22815;&#36866;&#24212;&#32654;&#22269;&#19981;&#21516;&#22320;&#21306;&#29983;&#25104;&#29305;&#23450;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#33258;&#39550;&#36710;&#30340;&#24863;&#30693;&#31995;&#32479;&#36755;&#20986;&#30340;&#20132;&#36890;&#22330;&#26223;&#29983;&#25104;&#12290;&#22312;&#25105;&#20204;&#30340;&#8220;&#22330;&#26223;&#25193;&#25955;&#8221;&#31995;&#32479;&#20013;&#65292;&#21463;&#28508;&#22312;&#25193;&#25955;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#21644;&#23545;&#35937;&#26816;&#27979;&#30340;&#26032;&#22411;&#32452;&#21512;&#65292;&#30452;&#25509;&#21019;&#24314;&#20855;&#26377;&#30495;&#23454;&#24863;&#21644;&#29289;&#29702;&#21512;&#29702;&#24615;&#30340;&#31163;&#25955;&#36793;&#30028;&#26694;&#20195;&#29702;&#30340;&#29616;&#23454;&#22330;&#26223;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#22330;&#26223;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#32654;&#22269;&#19981;&#21516;&#22320;&#21306;&#65292;&#20135;&#29983;&#25429;&#25417;&#27599;&#20010;&#22320;&#21306;&#32454;&#33410;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we describe a learned method of traffic scene generation designed to simulate the output of the perception system of a self-driving car. In our "Scene Diffusion" system, inspired by latent diffusion, we use a novel combination of diffusion and object detection to directly create realistic and physically plausible arrangements of discrete bounding boxes for agents. We show that our scene generation model is able to adapt to different regions in the US, producing scenarios that capture the intricacies of each region.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18451</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#20122;&#32467;&#26500;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#24341;&#36215;&#20102;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#39044;&#27979;&#20998;&#23376;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#23427;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20551;&#23450;&#22522;&#20110;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#26500;&#24314;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#25581;&#31034;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;SCM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#20854;&#24178;&#39044;&#26159;&#22522;&#20110;&#25104;&#23545;&#20998;&#23376;&#26465;&#20214;&#30340;&#12290;&#20351;&#29992;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#20174;&#22240;&#26524;&#20122;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#20102;&#19982;&#21270;&#23398;&#21453;&#24212;&#34394;&#20551;&#30456;&#20851;&#30340;&#24555;&#25463;&#20122;&#32467;&#26500;&#30340;&#28151;&#28102;&#25928;&#24212;&#12290;&#26412;&#25991;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;&#24555;&#36895;&#31283;&#23450;&#30340;&#39063;&#31890;&#29699;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18450</link><description>&lt;p&gt;
GBG++&#65306;&#20998;&#31867;&#30340;&#24555;&#36895;&#21644;&#31283;&#23450;&#30340;&#39063;&#31890;&#29699;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GBG++: A Fast and Stable Granular Ball Generation Method for Classification. (arXiv:2305.18450v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;&#24555;&#36895;&#31283;&#23450;&#30340;&#39063;&#31890;&#29699;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#12289;&#31283;&#20581;&#12289;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#25104;&#20026;&#39063;&#31890;&#35745;&#31639;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#39063;&#31890;&#29699;&#35745;&#31639;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39063;&#31890;&#29699;&#29983;&#25104;&#65288;GBG&#65289;&#21644;&#22522;&#20110;&#39063;&#31890;&#29699;&#65288;GB&#65289;&#30340;&#22810;&#31890;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GBG&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;k&#22343;&#20540;&#25110;k&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;GB&#30340;&#20998;&#31867;&#22120;&#20165;&#21333;&#21521;&#32771;&#34385;GB&#30340;&#20960;&#20309;&#29305;&#24449;&#26500;&#24314;&#20998;&#31867;&#35268;&#21017;&#65292;&#32780;&#24573;&#35270;&#20102;GB&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31283;&#23450;&#30340;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;GBG&#65288;GBG++&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;GBG++&#26041;&#27861;&#20165;&#38656;&#35201;&#22312;&#20998;&#21106;&#27599;&#20010;GB&#26102;&#35745;&#31639;&#20174;&#25968;&#25454;&#39537;&#21160;&#30340;&#20013;&#24515;&#21040;&#26410;&#20998;&#21106;&#26679;&#26412;&#30340;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#36873;&#25321;&#20013;&#24515;&#24182;&#35745;&#31639;&#23427;&#21040;&#25152;&#26377;&#26679;&#26412;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#24322;&#24120;&#20540;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing (GBC), as an efficient, robust, and scalable learning method, has become a popular research topic of granular computing. GBC includes two stages: granular ball generation (GBG) and multi-granularity learning based on the granular ball (GB). However, the stability and efficiency of existing GBG methods need to be further improved due to their strong dependence on $k$-means or $k$-division. In addition, GB-based classifiers only unilaterally consider the GB's geometric characteristics to construct classification rules, but the GB's quality is ignored. Therefore, in this paper, based on the attention mechanism, a fast and stable GBG (GBG++) method is proposed first. Specifically, the proposed GBG++ method only needs to calculate the distances from the data-driven center to the undivided samples when splitting each GB, instead of randomly selecting the center and calculating the distances between it to all samples. Moreover, an outlier detection method is introduced
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25552;&#31034;&#65292;&#25511;&#21046;AI bot&#21040;&#36798;&#20219;&#20309;&#29366;&#24577;&#65292;&#32780;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;Bot&#33021;&#20960;&#20046;&#30830;&#23450;&#22320;&#21040;&#36798;&#20219;&#20309;&#24847;&#20041;&#23376;&#38598;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18449</link><description>&lt;p&gt;
&#39535;&#26381;AI Bot&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Taming AI Bots: Controllability of Neural States in Large Language Models. (arXiv:2305.18449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25552;&#31034;&#65292;&#25511;&#21046;AI bot&#21040;&#36798;&#20219;&#20309;&#29366;&#24577;&#65292;&#32780;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;Bot&#33021;&#20960;&#20046;&#30830;&#23450;&#22320;&#21040;&#36798;&#20219;&#20309;&#24847;&#20041;&#23376;&#38598;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25552;&#31034;&#65292;&#23558;AI bot &#25511;&#21046;&#21040;&#20219;&#20309;&#29366;&#24577;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#24847;&#20041;&#8221;&#23450;&#20041;&#65292;&#20415;&#20110;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#26465;&#20214;&#65292;&#34920;&#24449;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25152;&#26174;&#28982;&#35757;&#32451;&#30340;&#8220;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#8221;&#21644;&#8220;&#35757;&#32451;&#33391;&#22909;&#30340;LLM&#8221;&#12290;&#34429;&#28982;&#35757;&#32451;&#33391;&#22909;&#30340;LLM&#26500;&#24314;&#20102;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24847;&#20041;&#23884;&#20837;&#31354;&#38388;&#65292;&#20294;&#26159;&#24847;&#20041;&#26412;&#36523;&#24182;&#19981;&#24418;&#25104;&#19968;&#20010;&#21521;&#37327;&#65288;&#32447;&#24615;&#65289;&#23376;&#31354;&#38388;&#65292;&#32780;&#26159;&#19968;&#20010;&#21830;&#31354;&#38388;&#12290;&#25105;&#20204;&#28982;&#21518;&#34920;&#24449;&#20102;&#26576;&#20010;&#36755;&#20837;&#25552;&#31034;&#30340;&#29366;&#24577;&#25152;&#33021;&#21040;&#36798;&#30340;&#24847;&#20041;&#23376;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#33391;&#22909;&#30340;Bot&#33021;&#22815;&#21040;&#36798;&#20219;&#20309;&#24847;&#20041;&#65292;&#23613;&#31649;&#27010;&#29575;&#24456;&#23567;&#12290;&#25105;&#20204;&#25509;&#30528;&#24341;&#20837;&#20102;&#26356;&#24378;&#30340;&#21487;&#25511;&#24615;&#27010;&#24565;&#65292;&#21363;&#8220;&#20960;&#20046;&#30830;&#23450;&#21487;&#36798;&#24615;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#38480;&#21046;&#21040;&#24847;&#20041;&#31354;&#38388;&#26102;&#65292;AI bot&#26159;&#21487;&#25511;&#30340;&#12290;&#25105;&#20204;&#22312;&#24341;&#20837;&#21151;&#33021;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#36825;&#26679;&#20570;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the question of whether an agent can, by suitable choice of prompts, control an AI bot to any state. To that end, we first introduce a formal definition of ``meaning'' that is amenable to analysis. Then, we characterize ``meaningful data'' on which large language models (LLMs) are ostensibly trained, and ``well-trained LLMs'' through conditions that are largely met by today's LLMs. While a well-trained LLM constructs an embedding space of meanings that is Euclidean, meanings themselves do not form a vector (linear) subspace, but rather a quotient space within. We then characterize the subset of meanings that can be reached by the state of the LLMs for some input prompt, and show that a well-trained bot can reach any meaning albeit with small probability. We then introduce a stronger notion of controllability as {\em almost certain reachability}, and show that, when restricted to the space of meanings, an AI bot is controllable. We do so after introducing a functional characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; &#8220;&#23548;&#21521;&#24615;&#27491;&#21017;&#21270;&#8221; &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#20351;&#24471;&#26576;&#20123;&#21333;&#20803;&#19981;&#37027;&#20040;&#37325;&#35201;&#65292;&#20174;&#32780;&#21487;&#20197;&#21066;&#20943;&#31070;&#32463;&#20803;&#65292;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2305.18448</link><description>&lt;p&gt;
&#26377;&#23548;&#21521;&#24615;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Neural Network Reduction with Guided Regularizers. (arXiv:2305.18448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; &#8220;&#23548;&#21521;&#24615;&#27491;&#21017;&#21270;&#8221; &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#20351;&#24471;&#26576;&#20123;&#21333;&#20803;&#19981;&#37027;&#20040;&#37325;&#35201;&#65292;&#20174;&#32780;&#21487;&#20197;&#21066;&#20943;&#31070;&#32463;&#20803;&#65292;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#25216;&#26415;&#20363;&#22914; $\mathcal{L}_1$ &#21644; $\mathcal{L}_2$ &#27491;&#21017;&#21270;&#22312;&#21024;&#20943;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#31227;&#38500;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26576;&#20010;&#31070;&#32463;&#20803;&#25110;&#36890;&#36947;&#65292;&#25152;&#26377;&#19982;&#35813;&#31070;&#32463;&#20803;&#25110;&#36890;&#36947;&#30456;&#20851;&#30340;&#26435;&#37325;&#20803;&#32032;&#38656;&#35201;&#26159;&#21487;&#21024;&#20943;&#30340;&#65292;&#36825;&#19981;&#33021;&#30001;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026; &#8220;&#23548;&#21521;&#24615;&#27491;&#21017;&#21270;&#8221;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#30340;&#26435;&#37325;&#65292;&#20351;&#19968;&#20123;&#21333;&#20803;&#19981;&#37027;&#20040;&#37325;&#35201;&#65292;&#20174;&#32780;&#21487;&#20197;&#21066;&#20943;&#31070;&#32463;&#20803;&#12290;&#36825;&#19982; $\mathcal{L}_1$ &#21644; $\mathcal{L}_2$ &#27491;&#21017;&#21270;&#30340;&#20998;&#25955;&#31232;&#30095;&#21270;&#19981;&#21516;&#65292;&#20854;&#20013;&#34987;&#32622;&#20026;&#38646;&#30340;&#26435;&#37325;&#30697;&#38453;&#30340;&#20998;&#37327;&#21487;&#20197;&#20301;&#20110;&#20219;&#20309;&#20301;&#32622;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;&#22312;&#22521;&#35757;&#27169;&#22411;&#30340;&#21516;&#26102;&#20013;&#21644;&#19981;&#24517;&#35201;&#30340;&#21333;&#20301;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20462;&#21098;&#20102;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization techniques such as $\mathcal{L}_1$ and $\mathcal{L}_2$ regularizers are effective in sparsifying neural networks (NNs). However, to remove a certain neuron or channel in NNs, all weight elements related to that neuron or channel need to be prunable, which is not guaranteed by traditional regularization. This paper proposes a simple new approach named "Guided Regularization" that prioritizes the weights of certain NN units more than others during training, which renders some of the units less important and thus, prunable. This is different from the scattered sparsification of $\mathcal{L}_1$ and $\mathcal{L}_2$ regularizers where the the components of a weight matrix that are zeroed out can be located anywhere. The proposed approach offers a natural reduction of NN in the sense that a model is being trained while also neutralizing unnecessary units. We empirically demonstrate that our proposed method is effective in pruning NNs while maintaining performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20026;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23457;&#35745;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#38543;&#26426;&#21270;&#30340;canaries&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#23481;&#38169;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#24471;&#21040;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.18447</link><description>&lt;p&gt;
&#21457;&#25381;&#38543;&#26426;&#21270;&#22312;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#23457;&#35745;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Randomization in Auditing Differentially Private ML. (arXiv:2305.18447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20026;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23457;&#35745;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#38543;&#26426;&#21270;&#30340;canaries&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#23481;&#38169;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#24471;&#21040;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22810;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#31034;&#20363;(&#31216;&#20026;canaries)&#65292;&#26469;&#23457;&#35745;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#30340;&#31532;&#19968;&#21407;&#21017;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25193;&#23637;&#24046;&#20998;&#38544;&#31169;&#23450;&#20041;&#26469;&#22788;&#29702;&#38543;&#26426;&#25968;&#25454;&#38598;&#30340;Lifted Differential Privacy (LiDP)&#12290;&#36825;&#36171;&#20104;&#20102;&#25105;&#20204;&#35774;&#35745;&#38543;&#26426;&#21270;&#30340;canaries&#30340;&#33258;&#30001;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#23581;&#35797;&#21306;&#20998;&#35757;&#32451;&#26377;$K$&#20010;canaries&#21644;&#35757;&#32451;&#27809;&#26377;&#19968;&#20010;canary&#26102;&#30340;&#27169;&#22411;&#26469;&#23457;&#35745;LiDP&#65292;&#21363;&#30041;&#20986;&#19968;&#20010;canary&#12290;&#21033;&#29992;canaries&#30340;i.i.d&#65292;LiDP&#21487;&#20197;&#21033;&#29992;&#35774;&#35745;&#20013;&#30340;&#23545;&#31216;&#24615;&#24182;&#22797;&#29992;&#27599;&#20010;&#31169;&#26377;&#35757;&#32451;&#27169;&#22411;&#26469;&#36816;&#34892;&#22810;&#20010;&#32479;&#35745;&#27979;&#35797;&#65292;&#19968;&#20010;&#38024;&#23545;&#27599;&#20010;canary&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36890;&#36807;&#36866;&#24212;&#32463;&#39564;&#39640;&#38454;&#30456;&#20851;&#24615;&#26469;&#21033;&#29992;&#22810;&#20010;&#27979;&#35797;&#32479;&#35745;&#37327;&#12290;&#24635;&#20043;&#65292;&#36825;&#20010;&#26032;&#37197;&#26041;&#23637;&#31034;&#20102;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a rigorous methodology for auditing differentially private machine learning algorithms by adding multiple carefully designed examples called canaries. We take a first principles approach based on three key components. First, we introduce Lifted Differential Privacy (LiDP) that expands the definition of differential privacy to handle randomized datasets. This gives us the freedom to design randomized canaries. Second, we audit LiDP by trying to distinguish between the model trained with $K$ canaries versus $K - 1$ canaries in the dataset, leaving one canary out. By drawing the canaries i.i.d., LiDP can leverage the symmetry in the design and reuse each privately trained model to run multiple statistical tests, one for each canary. Third, we introduce novel confidence intervals that take advantage of the multiple test statistics by adapting to the empirical higher-order correlations. Together, this new recipe demonstrates significant improvements in sample complexity, both the
&lt;/p&gt;</description></item><item><title>Trompt&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20854;&#20013;&#20998;&#31163;&#20102;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#24335;&#26469;&#35843;&#25972;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#33021;&#22815;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18446</link><description>&lt;p&gt;
Trompt&#65306;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#26356;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Trompt: Towards a Better Deep Neural Network for Tabular Data. (arXiv:2305.18446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18446
&lt;/p&gt;
&lt;p&gt;
Trompt&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20854;&#20013;&#20998;&#31163;&#20102;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#24335;&#26469;&#35843;&#25972;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#33021;&#22815;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#21487;&#35859;&#26159;&#21508;&#31181;&#23454;&#38469;&#39046;&#22495;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#32467;&#26500;&#20043;&#19968;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#30005;&#23376;&#21830;&#21153;&#12290;&#20869;&#22312;&#30340;&#24322;&#36136;&#24615;&#20801;&#35768;&#34920;&#26684;&#25968;&#25454;&#23384;&#20648;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#26368;&#36817;&#21457;&#24067;&#30340;&#34920;&#26684;&#22522;&#20934;&#65292;&#25105;&#20204;&#21487;&#20197;&#30475;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#26641;&#29366;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Trompt&#65292;&#23427;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#23398;&#20064;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#19968;&#32452;&#27169;&#22411;&#22806;&#30340;&#25552;&#31034;&#26469;&#35843;&#25972;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20462;&#25913;&#27169;&#22411;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;Trompt&#23558;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#31574;&#30053;&#20998;&#20026;&#20004;&#37096;&#20998;&#12290;&#31532;&#19968;&#37096;&#20998;&#31867;&#20284;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#27880;&#37325;&#23398;&#20064;&#34920;&#26684;&#30340;&#20869;&#22312;&#20449;&#24687;&#12290;&#31532;&#20108;&#37096;&#20998;&#31867;&#20284;&#20110;&#25552;&#31034;&#65292;&#27880;&#37325;&#23398;&#20064;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290; Trompt&#22312;&#19978;&#36848;&#22522;&#20934;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is arguably one of the most commonly used data structures in various practical domains, including finance, healthcare and e-commerce. The inherent heterogeneity allows tabular data to store rich information. However, based on a recently published tabular benchmark, we can see deep neural networks still fall behind tree-based models on tabular datasets. In this paper, we propose Trompt--which stands for Tabular Prompt--a novel architecture inspired by prompt learning of language models. The essence of prompt learning is to adjust a large pre-trained model through a set of prompts outside the model without directly modifying the model. Based on this idea, Trompt separates the learning strategy of tabular data into two parts. The first part, analogous to pre-trained models, focus on learning the intrinsic information of a table. The second part, analogous to prompts, focus on learning the variations among samples. Trompt is evaluated with the benchmark mentioned above. The ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#30830;&#23450;&#26799;&#24230;&#25918;&#22823;&#23618;&#27425;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#21644;&#21152;&#36895;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18445</link><description>&lt;p&gt;
&#26234;&#33021;&#26799;&#24230;&#25918;&#22823;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Intelligent gradient amplification for deep neural networks. (arXiv:2305.18445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#30830;&#23450;&#26799;&#24230;&#25918;&#22823;&#23618;&#27425;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#21644;&#21152;&#36895;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#27169;&#22411;&#28145;&#24230;&#22686;&#21152;&#20250;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#22686;&#21152;&#65292;&#19988;&#20250;&#20986;&#29616;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#26377;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#21333;&#29420;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#32508;&#21512;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#21516;&#26102;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#21644;&#21152;&#36895;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#22320;&#30830;&#23450;&#20309;&#26102;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21738;&#20123;&#23618;&#24212;&#29992;&#26799;&#24230;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#20998;&#26512;&#35757;&#32451;&#26399;&#38388;&#23618;&#26799;&#24230;&#27874;&#21160;&#30340;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#12290;&#20998;&#21035;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#26234;&#33021;&#24230;&#37327;&#21644;&#20004;&#20010;&#19981;&#21516;&#30340;&#38408;&#20540;&#26469;&#30830;&#23450;&#25918;&#22823;&#30340;&#23618;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26234;&#33021;&#26799;&#24230;&#25918;&#22823;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models offer superior performance compared to other machine learning techniques for a variety of tasks and domains, but pose their own challenges. In particular, deep learning models require larger training times as the depth of a model increases, and suffer from vanishing gradients. Several solutions address these problems independently, but there have been minimal efforts to identify an integrated solution that improves the performance of a model by addressing vanishing gradients, as well as accelerates the training process to achieve higher performance at larger learning rates. In this work, we intelligently determine which layers of a deep learning model to apply gradient amplification to, using a formulated approach that analyzes gradient fluctuations of layers during training. Detailed experiments are performed for simpler and deeper neural networks using two different intelligent measures and two different thresholds that determine the amplification layers, and a t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;CoTASP&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20043;&#21069;&#20219;&#21153;&#30340;&#20849;&#21516;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.18444</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#25552;&#31034;&#30340;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#30340;&#25345;&#32493;&#20219;&#21153;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Continual Task Allocation in Meta-Policy Network via Sparse Prompting. (arXiv:2305.18444v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;CoTASP&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20043;&#21069;&#20219;&#21153;&#30340;&#20849;&#21516;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#19981;&#26029;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#26469;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#19968;&#33324;&#21270;&#33021;&#21147;&#30340;&#20803;&#31574;&#30053;&#65292;&#26159;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36830;&#32493;&#20219;&#21153;&#20998;&#37197;&#30340;&#31232;&#30095;&#25552;&#31034;&#65288;CoTASP&#65289;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#12290;&#36890;&#36807;&#20132;&#26367;&#20248;&#21270;&#23376;&#32593;&#32476;&#21644;&#25552;&#31034;&#65292;CoTASP&#26356;&#26032;&#20102;&#20803;&#31574;&#30053;&#65292;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#28982;&#21518;&#26356;&#26032;&#23383;&#20856;&#65292;&#20197;&#20351;&#20248;&#21270;&#21518;&#30340;&#25552;&#31034;&#19982;&#20219;&#21153;&#23884;&#20837;&#30456;&#21305;&#37197;&#65292;&#20174;&#32780;&#25429;&#25417;&#20854;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#30456;&#20851;&#20219;&#21153;&#36890;&#36807;&#30456;&#20284;&#30340;&#25552;&#31034;&#22312;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#20849;&#20139;&#26356;&#22810;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#36328;&#20219;&#21153;&#24178;&#25200;&#23548;&#33268;&#36951;&#24536;&#34987;&#26377;&#25928;&#22320;&#32422;&#26463;&#12290;&#32473;&#23450;&#32463;&#36807;&#35757;&#32451;&#30340;&#20803;&#31574;&#30053;&#21644;&#26356;&#26032;&#21518;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;&#25552;&#31034;&#26469;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#20013;&#25552;&#21462;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23548;&#33322;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;CoTASP&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20219;&#21153;&#23436;&#25104;&#24230;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by "Continual Task Allocation via Sparse Prompting (CoTASP)", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POBGA&#30340;&#25913;&#36827;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#21518;&#24724;&#24230;&#19978;&#38480;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20808;&#21069;&#31639;&#27861;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25237;&#24433;&#31639;&#27861;&#12289;&#38459;&#22622;&#25216;&#26415;&#21644;&#22312;&#32447;boosting&#26799;&#24230;&#19978;&#21319;&#25216;&#26415;&#30456;&#32467;&#21512;&#23454;&#29616;&#30340;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#20998;&#25955;&#24335;&#23454;&#29616;&#30340;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;&#20302;&#23616;&#37096;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18442</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#26080;&#25237;&#24433;&#22312;&#32447;&#36830;&#32493;&#27425;&#27169;&#26368;&#22823;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Projection-free Online Continuous Submodular Maximization. (arXiv:2305.18442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POBGA&#30340;&#25913;&#36827;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#21518;&#24724;&#24230;&#19978;&#38480;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20808;&#21069;&#31639;&#27861;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25237;&#24433;&#31639;&#27861;&#12289;&#38459;&#22622;&#25216;&#26415;&#21644;&#22312;&#32447;boosting&#26799;&#24230;&#19978;&#21319;&#25216;&#26415;&#30456;&#32467;&#21512;&#23454;&#29616;&#30340;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#20998;&#25955;&#24335;&#23454;&#29616;&#30340;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;&#20302;&#23616;&#37096;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#35843;&#21644;&#36830;&#32493;DR-&#23376;&#27169;&#22870;&#21169;&#20989;&#25968;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22797;&#26434;&#20915;&#31574;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Mono-Frank-Wolfe&#65288;Mono-FW&#65289;&#30340;&#39640;&#25928;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#24635;&#20849;&#38656;&#35201; $O(T)$ &#26799;&#24230;&#35780;&#20272;&#21644;&#32447;&#24615;&#20248;&#21270;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23427;&#21482;&#33021;&#23454;&#29616; $O(T^{4/5})$ &#30340;$(1-1/e)$-&#21518;&#24724;&#24230;&#19978;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#21363;POBGA&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23558;&#21518;&#24724;&#24230;&#19978;&#38480;&#38477;&#20302;&#21040; $O(T^{3/4})$&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;Mono-FW&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#19981;&#26159;&#20462;&#25913;Mono-FW&#65292;&#32780;&#26159;&#23558;&#19968;&#31181;&#31216;&#20026;&#22312;&#32447;boosting&#26799;&#24230;&#19978;&#21319;&#30340;&#25237;&#24433;&#31639;&#27861;&#12289;&#19968;&#31181;&#19981;&#21487;&#34892;&#30340;&#25237;&#24433;&#25216;&#26415;&#21644;&#19968;&#31181;&#38459;&#22622;&#25216;&#26415;&#24039;&#22937;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#21435;&#20013;&#24515;&#21270;&#35774;&#32622;&#24182;&#24320;&#21457;&#20102;POBGA&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#23427;&#19981;&#20165;&#20943;&#23569;&#20102;&#24403;&#21069;&#20195;&#30721;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#32780;&#19988;&#22312;&#20855;&#26377;&#20302;&#23616;&#37096;&#24615;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of online learning with monotone and continuous DR-submodular reward functions, which has received great attention recently. To efficiently handle this problem, especially in the case with complicated decision sets, previous studies have proposed an efficient projection-free algorithm called Mono-Frank-Wolfe (Mono-FW) using $O(T)$ gradient evaluations and linear optimization steps in total. However, it only attains a $(1-1/e)$-regret bound of $O(T^{4/5})$. In this paper, we propose an improved projection-free algorithm, namely POBGA, which reduces the regret bound to $O(T^{3/4})$ while keeping the same computational complexity as Mono-FW. Instead of modifying Mono-FW, our key idea is to make a novel combination of a projection-based algorithm called online boosting gradient ascent, an infeasible projection technique, and a blocking technique. Furthermore, we consider the decentralized setting and develop a variant of POBGA, which not only reduces the current 
&lt;/p&gt;</description></item><item><title>DeCoR&#26159;&#19968;&#31181;&#25345;&#32493;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26089;&#26399;&#38899;&#39057;&#32534;&#30721;&#20013;&#30340;&#37327;&#21270;&#32034;&#24341;&#65292;&#38388;&#25509;&#20174;&#26089;&#26399;&#27169;&#22411;&#21521;&#26368;&#26032;&#27169;&#22411;&#25552;&#21462;&#30693;&#35782;&#65292;&#36991;&#20813;&#22312;&#23398;&#20064;&#26032;&#25968;&#25454;&#26102;&#36951;&#24536;&#20197;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#19982;&#25345;&#32493;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#23567;&#65292;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.18441</link><description>&lt;p&gt;
DeCoR: &#36890;&#36807;&#39044;&#27979;&#26089;&#26399;&#38899;&#39057;&#32534;&#30721;&#26469;&#36991;&#20813;&#30693;&#35782;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes. (arXiv:2305.18441v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18441
&lt;/p&gt;
&lt;p&gt;
DeCoR&#26159;&#19968;&#31181;&#25345;&#32493;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26089;&#26399;&#38899;&#39057;&#32534;&#30721;&#20013;&#30340;&#37327;&#21270;&#32034;&#24341;&#65292;&#38388;&#25509;&#20174;&#26089;&#26399;&#27169;&#22411;&#21521;&#26368;&#26032;&#27169;&#22411;&#25552;&#21462;&#30693;&#35782;&#65292;&#36991;&#20813;&#22312;&#23398;&#20064;&#26032;&#25968;&#25454;&#26102;&#36951;&#24536;&#20197;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#19982;&#25345;&#32493;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#23567;&#65292;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#38899;&#39057;&#29305;&#24449;&#25552;&#21462;&#38656;&#35201;&#36880;&#27493;&#23398;&#20064;&#26032;&#30340;&#22768;&#38899;&#31867;&#21035;&#65292;&#20197;&#36866;&#24212;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26032;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#20165;&#22312;&#26032;&#25968;&#25454;&#19978;&#20248;&#21270;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#20250;&#30772;&#22351;&#27169;&#22411;&#22312;&#38271;&#26399;&#20869;&#30340;&#33391;&#22909;&#34920;&#29616;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoR&#30340;&#26032;&#30340;&#25345;&#32493;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#23384;&#20648;&#20808;&#21069;&#25968;&#25454;&#12289;&#29305;&#24449;&#25110;&#27169;&#22411;&#30340;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;DeCoR&#36890;&#36807;&#20174;&#24310;&#36831;&#30721;&#26412;&#39044;&#27979;&#37327;&#21270;&#32034;&#24341;&#38388;&#25509;&#20174;&#26089;&#26399;&#27169;&#22411;&#21521;&#26368;&#26032;&#27169;&#22411;&#25552;&#28860;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DeCoR&#25552;&#39640;&#20102;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#19982;&#25345;&#32493;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#25928;&#26524;&#33391;&#22909;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26497;&#23567;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#26159;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong audio feature extraction involves learning new sound classes incrementally, which is essential for adapting to new data distributions over time. However, optimizing the model only on new data can lead to catastrophic forgetting of previously learned tasks, which undermines the model's ability to perform well over the long term. This paper introduces a new approach to continual audio representation learning called DeCoR. Unlike other methods that store previous data, features, or models, DeCoR indirectly distills knowledge from an earlier model to the latest by predicting quantization indices from a delayed codebook. We demonstrate that DeCoR improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning. Our approach introduces minimal storage and computation overhead, making it a lightweight and efficient solution for continual learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21487;&#33021;&#24615;&#34917;&#20607; &#8221;&#30340;&#26032;&#39062;&#24402;&#22240;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#40657;&#30418;&#27169;&#22411;&#30340;&#24322;&#24120;&#24402;&#22240;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18440</link><description>&lt;p&gt;
&#40657;&#30418;&#23376;&#24322;&#24120;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Black-Box Anomaly Attribution. (arXiv:2305.18440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21487;&#33021;&#24615;&#34917;&#20607; &#8221;&#30340;&#26032;&#39062;&#24402;&#22240;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#40657;&#30418;&#27169;&#22411;&#30340;&#24322;&#24120;&#24402;&#22240;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#40657;&#30418;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20559;&#31163;&#30495;&#23454;&#35266;&#23519;&#32467;&#26524;&#26102;&#65292;&#22914;&#20309;&#21028;&#26029;&#20559;&#24046;&#32972;&#21518;&#30340;&#21407;&#22240;&#26159;&#19968;&#20010;&#22522;&#26412;&#32780;&#26222;&#36941;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#19994;&#21153;&#25110;&#24037;&#19994;AI&#24212;&#29992;&#30340;&#26368;&#32456;&#29992;&#25143;&#32463;&#24120;&#38382;&#30340;&#38382;&#39064;&#12290;&#20559;&#24046;&#21487;&#33021;&#26159;&#30001;&#20110;&#27425;&#20248;&#40657;&#30418;&#27169;&#22411;&#65292;&#25110;&#32773;&#20165;&#20165;&#22240;&#20026;&#26679;&#26412;&#26159;&#24322;&#24120;&#20540;&#12290;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#33719;&#24471;&#26576;&#31181;&#24418;&#24335;&#30340;&#24402;&#22240;&#20998;&#25968;&#65292;&#21363;&#25351;&#31034;&#36755;&#20837;&#21464;&#37327;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#31243;&#24230;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the prediction of a black-box machine learning model deviates from the true observation, what can be said about the reason behind that deviation? This is a fundamental and ubiquitous question that the end user in a business or industrial AI application often asks. The deviation may be due to a sub-optimal black-box model, or it may be simply because the sample in question is an outlier. In either case, one would ideally wish to obtain some form of attribution score -- a value indicative of the extent to which an input variable is responsible for the anomaly.  In the present paper we address this task of ``anomaly attribution,'' particularly in the setting in which the model is black-box and the training data are not available. Specifically, we propose a novel likelihood-based attribution framework we call the ``likelihood compensation (LC),'' in which the responsibility score is equated with the correction on each input variable needed to attain the highest possible likelihood. We
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25913;&#21160;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#28304;&#22836;&#24402;&#23646;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#24037;&#31243;&#20998;&#26512;&#29983;&#25104;&#22270;&#29255;&#30340;&#26469;&#28304;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#29305;&#23450;&#22411;&#21495;&#30340;&#20381;&#36182;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.18439</link><description>&lt;p&gt;
&#26080;&#25913;&#21160;&#19988;&#27169;&#22411;&#26080;&#20851;&#30340;&#29983;&#25104;&#22270;&#20687;&#28304;&#22836;&#24402;&#23646;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alteration-free and Model-agnostic Origin Attribution of Generated Images. (arXiv:2305.18439v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25913;&#21160;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#28304;&#22836;&#24402;&#23646;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#24037;&#31243;&#20998;&#26512;&#29983;&#25104;&#22270;&#29255;&#30340;&#26469;&#28304;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#29305;&#23450;&#22411;&#21495;&#30340;&#20381;&#36182;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#28508;&#22312;&#30340;&#28389;&#29992;&#21644;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#38382;&#39064;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#20998;&#26512;&#22270;&#20687;&#30340;&#26469;&#28304;&#65292;&#25512;&#26029;&#26576;&#20010;&#29305;&#23450;&#30340;&#27169;&#22411;&#26159;&#21542;&#29983;&#25104;&#20102;&#19968;&#24352;&#29305;&#23450;&#30340;&#22270;&#20687;&#65292;&#21363;&#21407;&#22987;&#24402;&#23646;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#36866;&#29992;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#26102;&#23384;&#22312;&#38480;&#21046;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#35757;&#32451;&#25110;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#39069;&#22806;&#30340;&#22788;&#29702;&#27493;&#39588;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#19982;&#32570;&#23569;&#36825;&#20123;&#29305;&#23450;&#25805;&#20316;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#36755;&#20837;&#21453;&#21521;&#24037;&#31243;&#26469;&#24320;&#21457;&#19968;&#31181;&#26080;&#25913;&#21160;&#19988;&#27169;&#22411;&#26080;&#20851;&#30340;&#28304;&#22836;&#24402;&#23646;&#26041;&#27861;&#65292;&#21363;&#23545;&#20110;&#29305;&#23450;&#22270;&#20687;&#21453;&#36716;&#26576;&#29305;&#23450;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#32473;&#23450;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#21453;&#21521;&#24037;&#31243;&#20219;&#21153;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#38590;&#24230;&#24046;&#24322;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22522;&#20110;&#36825;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#28304;&#22836;&#24402;&#23646;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#25913;&#21160;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing attention in image generation models. However, concerns have emerged regarding potential misuse and intellectual property (IP) infringement associated with these models. Therefore, it is necessary to analyze the origin of images by inferring if a specific image was generated by a particular model, i.e., origin attribution. Existing methods are limited in their applicability to specific types of generative models and require additional steps during training or generation. This restricts their use with pre-trained models that lack these specific operations and may compromise the quality of image generation. To overcome this problem, we first develop an alteration-free and model-agnostic origin attribution method via input reverse-engineering on image generation models, i.e., inverting the input of a particular model for a specific image. Given a particular model, we first analyze the differences in the hardness of reverse-engineering tasks for the gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25968;&#20540;&#32534;&#30721;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#21644;SRG&#31639;&#27861;&#26469;&#29983;&#25104;&#35299;&#37322;&#24615;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18437</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#22312;&#31867;&#21035;&#21644;&#28151;&#21512;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65306;&#26080;&#25439;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization. (arXiv:2305.18437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25968;&#20540;&#32534;&#30721;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#21644;SRG&#31639;&#27861;&#26469;&#29983;&#25104;&#35299;&#37322;&#24615;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#28151;&#21512;&#25968;&#25454;&#26500;&#24314;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19968;&#30452;&#26159;&#31639;&#27861;&#38754;&#23545;&#38750;&#25968;&#20540;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#20540;&#32534;&#30721;&#26041;&#26696;&#21644;&#26080;&#25439;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25903;&#25345;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#21644;&#28436;&#31034;&#20854;&#37325;&#35201;&#20316;&#29992;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#28151;&#21512;&#25968;&#25454;&#31867;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#21253;&#65292;&#20197;&#23545;&#28151;&#21512;&#25968;&#25454;&#30340;&#25152;&#26377;&#20869;&#37096;&#25805;&#20316;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#39034;&#24207;&#35268;&#21017;&#29983;&#25104;&#65288;SRG&#65289;&#8221;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#35745;&#31639;&#23454;&#39564;&#20013;&#25104;&#21151;&#35780;&#20272;&#35813;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building accurate and interpretable Machine Learning (ML) models for heterogeneous/mixed data is a long-standing challenge for algorithms designed for numeric data. This work focuses on developing numeric coding schemes for non-numeric attributes for ML algorithms to support accurate and explainable ML models, methods for lossless visualization of n-D non-numeric categorical data with visual rule discovery in these visualizations, and accurate and explainable ML models for categorical data. This study proposes a classification of mixed data types and analyzes their important role in Machine Learning. It presents a toolkit for enforcing interpretability of all internal operations of ML algorithms on mixed data with a visual data exploration on mixed data. A new Sequential Rule Generation (SRG) algorithm for explainable rule generation with categorical data is proposed and successfully evaluated in multiple computational experiments. This work is one of the steps to the full scope ML alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18436</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#23454;&#29616;&#26368;&#20248;K&#22343;&#20540;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K&#22343;&#20540;&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#26494;&#24347;&#26368;&#36817;&#34987;&#25552;&#20986;&#29992;&#20110;&#35299;&#20915;K&#22343;&#20540;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#20294;&#23454;&#29616;SDP&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#36825;&#20123;&#20445;&#35777;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#34987;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#20047;&#22362;&#23454;&#30340;&#32479;&#35745;&#22522;&#30784;&#25110;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;NMF&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#20984;Burer-Monteiro&#20998;&#35299;&#26041;&#27861;&#35299;&#20915;&#21322;&#23450;&#35268;&#21010;&#26494;&#24347;&#30340;K&#22343;&#20540;&#20844;&#24335;&#30340;&#38750;&#36127;&#20302;&#31209;&#38480;&#21046;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#65292;&#21516;&#26102;&#20063;&#20139;&#26377;&#19982;SDP&#30456;&#21516;&#30340;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;NMF&#31639;&#27861;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;SDP&#27714;&#35299;&#22120;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
$K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20132;&#21449;&#29109;&#20272;&#35745;&#22120;&#30340;&#22791;&#36873;&#19979;&#30028;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#20010;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27604;&#26679;&#26412;&#65292;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#39640;&#20449;&#24687;&#22686;&#30410;&#65292;&#20801;&#35768;&#23398;&#20064;&#26356;&#20248;&#31168;&#30340;&#35774;&#35745;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#38544;&#24335;&#27010;&#29575;&#27169;&#22411;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.18435</link><description>&lt;p&gt;
&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#30340;&#20132;&#21449;&#29109;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Entropy Estimators for Sequential Experiment Design with Reinforcement Learning. (arXiv:2305.18435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18435
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20132;&#21449;&#29109;&#20272;&#35745;&#22120;&#30340;&#22791;&#36873;&#19979;&#30028;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#20010;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27604;&#26679;&#26412;&#65292;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#39640;&#20449;&#24687;&#22686;&#30410;&#65292;&#20801;&#35768;&#23398;&#20064;&#26356;&#20248;&#31168;&#30340;&#35774;&#35745;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#38544;&#24335;&#27010;&#29575;&#27169;&#22411;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#26377;&#25928;&#22320;&#23398;&#20064;&#35774;&#35745;&#23454;&#39564;&#24207;&#21015;&#30340;&#25674;&#38144;&#35774;&#35745;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#30340;&#23545;&#27604;&#20272;&#35745;&#22120;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#23545;&#27604;&#26679;&#26412;&#26469;&#36798;&#21040;&#26080;&#20559;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#27169;&#22411;&#20998;&#24067;&#21644;&#28789;&#27963;&#30340;&#25552;&#35758;&#20998;&#24067;&#30340;&#22791;&#36873;&#19979;&#30028;&#20272;&#35745;&#22120;&#12290;&#25552;&#35758;&#20998;&#24067;&#36924;&#36817;&#27169;&#22411;&#21442;&#25968;&#22312;&#23454;&#39564;&#21382;&#21490;&#21644;&#35774;&#35745;&#31574;&#30053;&#26465;&#20214;&#19979;&#32473;&#23450;&#30340;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#19981;&#38656;&#35201;&#23545;&#27604;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39640;&#20449;&#24687;&#22686;&#30410;&#20272;&#35745;&#65292;&#20801;&#35768;&#23398;&#20064;&#26356;&#20248;&#31168;&#30340;&#35774;&#35745;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#38544;&#24335;&#27010;&#29575;&#27169;&#22411;&#20860;&#23481;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#36830;&#32493;&#21644;&#31163;&#25955;&#35774;&#35745;&#20197;&#21450;&#26174;&#24335;&#21644;&#38544;&#24335;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning can effectively learn amortised design policies for designing sequences of experiments. However, current methods rely on contrastive estimators of expected information gain, which require an exponential number of contrastive samples to achieve an unbiased estimation. We propose an alternative lower bound estimator, based on the cross-entropy of the joint model distribution and a flexible proposal distribution. This proposal distribution approximates the true posterior of the model parameters given the experimental history and the design policy. Our estimator requires no contrastive samples, can achieve more accurate estimates of high information gains, allows learning of superior design policies, and is compatible with implicit probabilistic models. We assess our algorithm's performance in various tasks, including continuous and discrete designs and explicit and implicit likelihoods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24179;&#34892;&#22352;&#26631;&#36827;&#34892;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20998;&#31867;&#22120;&#31639;&#27861;Hyper&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#32456;&#31471;&#29992;&#25143;&#26131;&#20110;&#29702;&#35299;&#30340;&#12289;&#29992;&#20110;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#28151;&#21512;&#21644;&#32431;&#31929;&#30340;&#36229;&#22359;&#12290;</title><link>http://arxiv.org/abs/2305.18434</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#34892;&#22352;&#26631;&#30340;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Parallel Coordinates for Discovery of Interpretable Machine Learning Models. (arXiv:2305.18434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24179;&#34892;&#22352;&#26631;&#36827;&#34892;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20998;&#31867;&#22120;&#31639;&#27861;Hyper&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#32456;&#31471;&#29992;&#25143;&#26131;&#20110;&#29702;&#35299;&#30340;&#12289;&#29992;&#20110;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#28151;&#21512;&#21644;&#32431;&#31929;&#30340;&#36229;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24179;&#34892;&#22352;&#26631;&#36827;&#34892;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#65292;&#25552;&#21319;&#20102;&#21487;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#24179;&#34892;&#22352;&#26631;&#20013;&#30340;&#22270;&#24418;&#25968;&#25454;&#34920;&#31034;&#20351;&#24471;&#32456;&#31471;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#36229;&#31435;&#26041;&#20307;&#21644;&#36229;&#22359;&#30340;&#27010;&#24565;&#12290;&#25991;&#31456;&#24314;&#35758;&#22312;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#20998;&#31867;&#22120;&#31639;&#27861;Hyper&#20013;&#21516;&#26102;&#20351;&#29992;&#28151;&#21512;&#21644;&#32431;&#31929;&#30340;&#36229;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Hyper&#27169;&#22411;&#20855;&#26377;&#20915;&#31574;&#26641;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#31639;&#27861;&#34987;&#29992;&#22312;&#20102;&#22810;&#31181;&#35774;&#32622;&#21644;&#36873;&#39033;&#20013;&#65292;&#20197;&#20132;&#20114;&#26041;&#24335;&#25110;&#33258;&#21160;&#26041;&#24335;&#21457;&#29616;&#37325;&#21472;&#25110;&#38750;&#37325;&#21472;&#30340;&#36229;&#22359;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#28436;&#31034;&#20102;&#20351;&#29992;&#36229;&#22359;&#21644;&#35270;&#35273;&#27169;&#24335;&#30340;&#35821;&#35328;&#25551;&#36848;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;UCI ML&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#35780;&#20272;Hyper&#31639;&#27861;&#12290;&#36890;&#36807;10&#25240;&#20132;&#21449;&#39564;&#35777;&#65292;Hyper&#31639;&#27861;&#33021;&#22815;&#21457;&#29616;&#28151;&#21512;&#21644;&#32431;&#31929;&#30340;&#36229;&#22359;&#12290;&#36229;&#22359;&#12289;&#38477;&#32500;&#21644;&#21487;&#35270;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#24050;&#32463;&#24314;&#31435;&#36215;&#26469;&#12290;&#26368;&#32456;&#29992;&#25143;&#33021;&#22815;&#25214;&#21040;&#21644;&#35266;&#23519;&#36825;&#20123;&#36229;&#22359;&#30340;&#33021;&#21147;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work uses visual knowledge discovery in parallel coordinates to advance methods of interpretable machine learning. The graphic data representation in parallel coordinates made the concepts of hypercubes and hyperblocks (HBs) simple to understand for end users. It is suggested to use mixed and pure hyperblocks in the proposed data classifier algorithm Hyper. It is shown that Hyper models generalize decision trees. The algorithm is presented in several settings and options to discover interactively or automatically overlapping or non-overlapping hyperblocks. Additionally, the use of hyperblocks in conjunction with language descriptions of visual patterns is demonstrated. The benchmark data from the UCI ML repository were used to evaluate the Hyper algorithm. It enabled the discovery of mixed and pure HBs evaluated using 10-fold cross validation. Connections among hyperblocks, dimension reduction and visualization have been established. The capability of end users to find and observe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#21644;&#37319;&#26679;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#36947;&#22270;&#20687;&#35843;&#33410;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#26465;&#20214;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.18433</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35748;&#30693;&#36328;&#27169;&#24577;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models. (arXiv:2305.18433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#21644;&#37319;&#26679;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#36947;&#22270;&#20687;&#35843;&#33410;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#26465;&#20214;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#26041;&#27861;&#20351;&#29992;&#25351;&#23548;&#26041;&#24335;&#22312;&#28508;&#22312;&#31354;&#38388;&#19978;&#25552;&#20379;&#25511;&#21046;&#65292;&#20197;&#23454;&#29616;&#19981;&#21516;&#27169;&#24577;&#30340;&#26465;&#20214;&#29983;&#25104;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20998;&#21035;&#35757;&#32451;&#27599;&#20010;&#27169;&#24577;&#30340;&#27169;&#22411;&#26469;&#25552;&#20379;&#25351;&#23548;&#65292;&#22240;&#27492;&#21463;&#21040;&#36328;&#27169;&#24577;&#20449;&#24687;&#20002;&#22833;&#30340;&#24433;&#21709;&#65292;&#19988;&#20165;&#33021;&#23454;&#29616;&#21333;&#21521;&#26465;&#20214;&#29983;&#25104;&#12290;&#26412;&#25991;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#21516;&#27493;&#33719;&#21462;&#22810;&#27169;&#24577;&#20449;&#24687;&#24182;&#23398;&#20064;&#27169;&#24577;&#38388;&#30456;&#20851;&#24615;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#36890;&#36947;&#22270;&#20687;&#35843;&#33410;&#30340;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#21644;&#37319;&#26679;&#26041;&#26696;&#65292;&#20197;&#22312;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#65292;&#26356;&#22909;&#22320;&#27169;&#20223;&#22823;&#33041;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#25152;&#26377;&#30456;&#20851;&#27169;&#24577;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing cross-modal generative methods based on diffusion models use guidance to provide control over the latent space to enable conditional generation across different modalities. Such methods focus on providing guidance through separately-trained models, each for one modality. As a result, these methods suffer from cross-modal information loss and are limited to unidirectional conditional generation. Inspired by how humans synchronously acquire multi-modal information and learn the correlation between modalities, we explore a multi-modal diffusion model training and sampling scheme that uses channel-wise image conditioning to learn cross-modality correlation during the training phase to better mimic the learning process in the brain. Our empirical results demonstrate that our approach can achieve data generation conditioned on all correlated modalities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#26041;&#27861;&#29992;&#20110;&#21019;&#24314;&#21644;&#22686;&#24378;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35270;&#21270;&#33021;&#21147;&#65292;&#20998;&#21035;&#20026;&#24367;&#26354;&#22352;&#26631;&#21644;&#31227;&#20301;&#37197;&#23545;&#22352;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.18432</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24314;&#27169;&#30340;&#23436;&#20840;&#21487;&#35270;&#21270;&#20132;&#20114;&#24335;&#20915;&#31574;&#26641;&#21019;&#24314;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Interactive Decision Tree Creation and Enhancement with Complete Visualization for Explainable Modeling. (arXiv:2305.18432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#26041;&#27861;&#29992;&#20110;&#21019;&#24314;&#21644;&#22686;&#24378;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35270;&#21270;&#33021;&#21147;&#65292;&#20998;&#21035;&#20026;&#24367;&#26354;&#22352;&#26631;&#21644;&#31227;&#20301;&#37197;&#23545;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#31934;&#24230;&#65292;ML&#27169;&#22411;&#21487;&#35270;&#21270;&#26159;ML&#36807;&#31243;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#20915;&#31574;&#26641;&#65288;DTs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#29992;&#20110;&#29702;&#35299;&#35768;&#22810;&#40657;&#21283;&#23376;ML&#27169;&#22411;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#21019;&#24314;&#21644;&#22686;&#24378;DT&#30340;&#23436;&#20840;&#21487;&#35270;&#21270;&#30340;&#21487;&#29702;&#35299;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#29256;&#26412;&#30340;&#36890;&#29992;&#32447;&#22352;&#26631;&#65288;GLC&#65289;&#65306;&#24367;&#26354;&#22352;&#26631;&#65288;BC&#65289;&#21644;&#31227;&#20301;&#37197;&#23545;&#22352;&#26631;&#65288;SPC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
To increase the interpretability and prediction accuracy of the Machine Learning (ML) models, visualization of ML models is a key part of the ML process. Decision Trees (DTs) are essential in machine learning (ML) because they are used to understand many black box ML models including Deep Learning models. In this research, two new methods for creation and enhancement with complete visualizing Decision Trees as understandable models are suggested. These methods use two versions of General Line Coordinates (GLC): Bended Coordinates (BC) and Shifted Paired Coordinates (SPC). The Bended Coordinates are a set of line coordinates, where each coordinate is bended in a threshold point of the respective DT node. In SPC, each n-D point is visualized in a set of shifted pairs of 2-D Cartesian coordinates as a directed graph. These new methods expand and complement the capabilities of existing methods to visualize DT models more completely. These capabilities allow us to observe and analyze: (1) r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;Journey Ranker&#65292;&#26469;&#35299;&#20915;Airbnb&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#21807;&#19968;&#25361;&#25112;&#65292;&#21363;&#23458;&#25143;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#65292;&#35813;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.18431</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20248;&#21270;Airbnb&#25628;&#32034;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
Optimizing Airbnb Search Journey with Multi-task Learning. (arXiv:2305.18431v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;Journey Ranker&#65292;&#26469;&#35299;&#20915;Airbnb&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#21807;&#19968;&#25361;&#25112;&#65292;&#21363;&#23458;&#25143;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#65292;&#35813;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Airbnb&#26159;&#19968;&#20010;&#22312;&#32447;&#20303;&#23487;&#21644;&#20307;&#39564;&#24066;&#22330;&#65292;&#23458;&#20154;&#36890;&#24120;&#38656;&#35201;&#33457;&#36153;&#25968;&#21608;&#26469;&#25506;&#32034;&#21644;&#27604;&#36739;&#22810;&#20010;&#29289;&#21697;&#65292;&#24182;&#22312;&#20570;&#20986;&#26368;&#21518;&#30340;&#39044;&#35746;&#35831;&#27714;&#20043;&#21069;&#24179;&#34913;&#23458;&#20154;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#12290;&#25628;&#32034;&#36807;&#31243;&#30340;&#38271;&#26399;&#24615;&#36136;&#20197;&#21450;&#38656;&#35201;&#24179;&#34913;&#23458;&#20154;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#65292;&#36825;&#20123;&#37117;&#20026;Airbnb&#30340;&#25628;&#32034;&#25490;&#21517;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;Journey Ranker&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;Journey Ranker&#21033;&#29992;&#20013;&#38388;&#30340;&#23458;&#25143;&#25805;&#20316;&#20316;&#20026;&#37324;&#31243;&#30865;&#65288;&#26080;&#35770;&#26159;&#31215;&#26497;&#30340;&#36824;&#26159;&#28040;&#26497;&#30340;&#65289;&#26469;&#26356;&#22909;&#22320;&#23558;&#23458;&#25143;&#25512;&#21521;&#25104;&#21151;&#30340;&#39044;&#35746;&#12290;&#23427;&#36824;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#23458;&#25143;&#29366;&#24577;&#21644;&#25628;&#32034;&#26597;&#35810;&#65289;&#26469;&#24179;&#34913;&#23458;&#20154;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#12290;&#20854;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35774;&#35745;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65292;&#20998;&#31163;&#26126;&#30830;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;Airbnb&#25628;&#32034;&#25490;&#21517;&#20197;&#22806;&#30340;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
At Airbnb, an online marketplace for stays and experiences, guests often spend weeks exploring and comparing multiple items before making a final reservation request. Each reservation request may then potentially be rejected or cancelled by the host prior to check-in. The long and exploratory nature of the search journey, as well as the need to balance both guest and host preferences, present unique challenges for Airbnb search ranking. In this paper, we present Journey Ranker, a new multi-task deep learning model architecture that addresses these challenges. Journey Ranker leverages intermediate guest actions as milestones, both positive and negative, to better progress the guest towards a successful booking. It also uses contextual information such as guest state and search query to balance guest and host preferences. Its modular and extensible design, consisting of four modules with clear separation of concerns, allows for easy application to use cases beyond the Airbnb search ranki
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38134;&#34892;&#20132;&#26131;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#24369;&#30417;&#30563;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#26368;&#23567;&#21270;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#33021;&#22815;&#24555;&#36895;&#25193;&#23637;&#21040;&#26032;&#30340;&#21644;&#32452;&#21512;&#29992;&#20363;&#65292;&#21487;&#29992;&#20110;&#36130;&#21153;&#20581;&#24247;&#25253;&#21578;&#21644;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#31561;&#37329;&#34701;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18430</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#24369;&#30417;&#30563;&#38134;&#34892;&#20132;&#26131;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable and Weakly Supervised Bank Transaction Classification. (arXiv:2305.18430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38134;&#34892;&#20132;&#26131;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#24369;&#30417;&#30563;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#26368;&#23567;&#21270;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#33021;&#22815;&#24555;&#36895;&#25193;&#23637;&#21040;&#26032;&#30340;&#21644;&#32452;&#21512;&#29992;&#20363;&#65292;&#21487;&#29992;&#20110;&#36130;&#21153;&#20581;&#24247;&#25253;&#21578;&#21644;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#31561;&#37329;&#34701;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#24369;&#30417;&#30563;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#23545;&#38134;&#34892;&#20132;&#26131;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21551;&#21457;&#24335;&#21644;&#39046;&#22495;&#30693;&#35782;&#26469;&#35757;&#32451;&#20934;&#30830;&#30340;&#20132;&#26131;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#23545;&#26114;&#36149;&#21644;&#38590;&#20197;&#33719;&#21462;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31471;&#21040;&#31471;&#25968;&#25454;&#31649;&#36947;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20132;&#26131;&#25991;&#26412;&#23884;&#20837;&#12289;&#38170;&#23450;&#12289;&#26631;&#31614;&#29983;&#25104;&#12289;&#21028;&#21035;&#24335;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20197;&#21450;&#31995;&#32479;&#26550;&#26500;&#27010;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;&#24066;&#22330;&#39046;&#20808;&#35299;&#20915;&#26041;&#26696;&#12289;&#23454;&#29616;&#20934;&#30830;&#20998;&#31867;&#24182;&#19988;&#21487;&#20197;&#24555;&#36895;&#25193;&#23637;&#21040;&#26032;&#30340;&#21644;&#32452;&#21512;&#29992;&#20363;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#21453;&#36807;&#26469;&#21487;&#20197;&#24320;&#21551;&#35768;&#22810;&#37329;&#34701;&#24212;&#29992;&#65292;&#20363;&#22914;&#36130;&#21153;&#20581;&#24247;&#25253;&#21578;&#21644;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to categorize bank transactions using weak supervision, natural language processing, and deep neural network techniques. Our approach minimizes the reliance on expensive and difficult-to-obtain manual annotations by leveraging heuristics and domain knowledge to train accurate transaction classifiers. We present an effective and scalable end-to-end data pipeline, including data preprocessing, transaction text embedding, anchoring, label generation, discriminative neural network training, and an overview of the system architecture. We demonstrate the effectiveness of our method by showing it outperforms existing market-leading solutions, achieves accurate categorization, and can be quickly extended to novel and composite use cases. This can in turn unlock many financial applications such as financial health reporting and credit risk assessment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;General Line Coordinates&#30340;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#12289;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21450;&#20854;&#35299;&#37322;&#35268;&#21017;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18429</link><description>&lt;p&gt;
&#22522;&#20110;General Line Coordinates&#30340;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Visual Knowledge Discovery with General Line Coordinates. (arXiv:2305.18429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;General Line Coordinates&#30340;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#12289;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21450;&#20854;&#35299;&#37322;&#35268;&#21017;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32500;&#25968;&#25454;&#19978;&#30340;&#40657;&#30418;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#29702;&#35299;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#35299;&#37322;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#31181;&#26080;&#25439;General Line Coordinates&#30340;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26159;&#20808;&#21069;&#24341;&#20837;&#30340;General Line Coordinates Linear&#21644;Dynamic Scaffolding Coordinates&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#29983;&#25104;&#12289;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21450;&#20854;&#35299;&#37322;&#35268;&#21017;&#12290;&#20026;&#30830;&#20445;&#36825;&#20123;&#38750;&#32447;&#24615;&#27169;&#22411;&#21644;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#65292;General Line Coordinates Linear&#36824;&#24320;&#21457;&#20102;&#26032;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#26597;&#25214;&#26368;&#22351;&#24773;&#20917;&#39564;&#35777;&#20998;&#35010;&#12290;&#36825;&#20123;&#25193;&#23637;&#21253;&#25324;General Line Coordinates&#38750;&#32447;&#24615;&#65292;&#20132;&#20114;&#24335;&#35268;&#21017;&#32447;&#24615;&#65292;&#36229;&#22359;&#35268;&#21017;&#32447;&#24615;&#21644;&#26368;&#22351;&#24773;&#20917;&#32447;&#24615;&#12290;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#26041;&#27861;&#21487;&#20197;&#22312;&#35299;&#37322;&#21644;&#39044;&#27979;&#26041;&#38754;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding black-box Machine Learning methods on multidimensional data is a key challenge in Machine Learning. While many powerful Machine Learning methods already exist, these methods are often unexplainable or perform poorly on complex data. This paper proposes visual knowledge discovery approaches based on several forms of lossless General Line Coordinates. These are an expansion of the previously introduced General Line Coordinates Linear and Dynamic Scaffolding Coordinates to produce, explain, and visualize non-linear classifiers with explanation rules. To ensure these non-linear models and rules are accurate, General Line Coordinates Linear also developed new interactive visual knowledge discovery algorithms for finding worst-case validation splits. These expansions are General Line Coordinates non-linear, interactive rules linear, hyperblock rules linear, and worst-case linear. Experiments across multiple benchmark datasets show that this visual knowledge discovery method can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#22870;&#21169;&#20877;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#36879;&#35270;&#24314;&#27169;&#29366;&#24577;&#21644;&#34892;&#21160;&#36129;&#29486;&#65292;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#36820;&#22238;&#20998;&#35299;&#12290;&#29983;&#25104;&#36820;&#22238;&#20998;&#35299;&#65288;GRD&#65289;&#26694;&#26550;&#29992;&#20110;&#24310;&#36831;&#22870;&#21169;&#22330;&#26223;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18427</link><description>&lt;p&gt;
GRD: &#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#21487;&#35299;&#37322;&#22870;&#21169;&#20877;&#20998;&#37197;&#30340;&#29983;&#25104;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning. (arXiv:2305.18427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#22870;&#21169;&#20877;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#36879;&#35270;&#24314;&#27169;&#29366;&#24577;&#21644;&#34892;&#21160;&#36129;&#29486;&#65292;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#36820;&#22238;&#20998;&#35299;&#12290;&#29983;&#25104;&#36820;&#22238;&#20998;&#35299;&#65288;GRD&#65289;&#26694;&#26550;&#29992;&#20110;&#24310;&#36831;&#22870;&#21169;&#22330;&#26223;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#21738;&#20123;&#29366;&#24577;-&#34892;&#21160;&#23545;&#24212;&#35813;&#23545;&#26410;&#26469;&#30340;&#20998;&#27493;&#22870;&#21169;&#36127;&#36131;&#12290;Return Decomposition&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#35266;&#27979;&#24207;&#21015;&#20013;&#30340;&#22870;&#21169;&#26469;&#20445;&#25345;&#31574;&#30053;&#19981;&#21464;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20197;&#19981;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26500;&#24314;&#22870;&#21169;&#20877;&#20998;&#37197;&#65292;&#20294;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#22240;&#26524;&#36879;&#35270;&#26469;&#26126;&#30830;&#24314;&#27169;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#36129;&#29486;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#36820;&#22238;&#20998;&#35299;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#22312;&#36820;&#22238;&#20998;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#25551;&#36848;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#21644;&#22522;&#20110;&#36712;&#36857;&#30340;&#38271;&#26399;&#22238;&#25253;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#36820;&#22238;&#20998;&#35299;&#65288;GRD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24310;&#36831;&#22870;&#21169;&#22330;&#26223;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GRD&#39318;&#20808;&#30830;&#23450;&#29983;&#25104;&#36807;&#31243;&#20013;&#19981;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#21644;&#22240;&#26524;&#20851;&#31995;&#65292;&#28982;&#21518;&#21033;&#29992;&#30830;&#23450;&#30340;&#22240;&#26524;&#27169;&#22411;&#35745;&#31639;&#21487;&#35266;&#27979;&#22870;&#21169;&#30340;&#26399;&#26395;&#65292;&#36827;&#32780;&#25552;&#39640;&#31574;&#30053;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Return Decomposition offers a solution by redistributing rewards from observed sequences while preserving policy invariance. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable return decomposition. In this paper, we start by studying the role of causal generative models in return decomposition by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#22686;&#26448;&#21046;&#36896;&#20013;&#36755;&#20837;&#21464;&#37327;&#21644;&#25289;&#20280;&#24378;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#21457;&#29616;Infill&#30334;&#20998;&#27604;&#21644;&#25380;&#20986;&#28201;&#24230;&#23545;&#20110;&#25289;&#20280;&#24378;&#24230;&#20855;&#26377;&#26368;&#39640;&#30340;&#27491;&#30456;&#20851;&#21644;&#36127;&#30456;&#20851;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18426</link><description>&lt;p&gt;
&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#25506;&#31350;&#22686;&#26448;&#21046;&#36896;&#26679;&#21697;&#20013;&#36755;&#20837;&#21464;&#37327;&#21644;&#25289;&#20280;&#24378;&#24230;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Employing Explainable Artificial Intelligence (XAI) Methodologies to Analyze the Correlation between Input Variables and Tensile Strength in Additively Manufactured Samples. (arXiv:2305.18426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#22686;&#26448;&#21046;&#36896;&#20013;&#36755;&#20837;&#21464;&#37327;&#21644;&#25289;&#20280;&#24378;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#21457;&#29616;Infill&#30334;&#20998;&#27604;&#21644;&#25380;&#20986;&#28201;&#24230;&#23545;&#20110;&#25289;&#20280;&#24378;&#24230;&#20855;&#26377;&#26368;&#39640;&#30340;&#27491;&#30456;&#20851;&#21644;&#36127;&#30456;&#20851;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21253;&#25324;Infill&#30334;&#20998;&#27604;&#12289;&#23618;&#39640;&#12289;&#25380;&#20986;&#28201;&#24230;&#21644;&#25171;&#21360;&#36895;&#24230;&#22312;&#20869;&#30340;&#21508;&#31181;&#36755;&#20837;&#21442;&#25968;&#23545;&#20110;&#20135;&#29983;&#30340;&#22686;&#26448;&#21046;&#36896;&#29289;&#20307;&#30340;&#25289;&#20280;&#24378;&#24230;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#25105;&#20204;&#23545;&#36755;&#20837;&#21442;&#25968;&#21644;&#25289;&#20280;&#24378;&#24230;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#29702;&#35299;&#65292;&#20197;&#21450;&#30830;&#23450;&#24433;&#21709;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;SHAP&#65288;Shapley Additive Explanations&#65289;&#26041;&#27861;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#34892;&#20026;&#65292;&#20197;&#20998;&#26512;&#25968;&#25454;&#24182;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;Infill&#30334;&#20998;&#27604;&#21644;&#25380;&#20986;&#28201;&#24230;&#19982;&#25289;&#20280;&#24378;&#24230;&#20855;&#26377;&#26368;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20998;&#21035;&#20026;&#27491;&#30456;&#20851;&#21644;&#36127;&#30456;&#20851;&#12290;&#20351;&#29992;XAI&#25216;&#26415;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36755;&#20837;&#21442;&#25968;&#21644;&#22686;&#26448;&#21046;&#36896;&#20013;&#25289;&#20280;&#24378;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper explores the impact of various input parameters, including Infill percentage, Layer Height, Extrusion Temperature, and Print Speed, on the resulting Tensile Strength in objects produced through additive manufacturing. The main objective of this study is to enhance our understanding of the correlation between the input parameters and Tensile Strength, as well as to identify the key factors influencing the performance of the additive manufacturing process. To achieve this objective, we introduced the utilization of Explainable Artificial Intelligence (XAI) techniques for the first time, which allowed us to analyze the data and gain valuable insights into the system's behavior. Specifically, we employed SHAP (SHapley Additive exPlanations), a widely adopted framework for interpreting machine learning model predictions, to provide explanations for the behavior of a machine learning model trained on the data. Our findings reveal that the Infill percentage and Extrusion T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#27531;&#24046;&#20302;&#31209;&#29305;&#24615;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#39640;&#25928;&#23384;&#20648;&#30340;&#26032;&#26041;&#27861;ERE&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#37327;&#21270;&#21644;&#20998;&#23618;&#31209;&#20998;&#37197;&#26469;&#25552;&#39640;&#23384;&#20648;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18425</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#26435;&#37325;&#27531;&#24046;&#30340;&#20302;&#31209;&#36924;&#36817;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#39640;&#25928;&#23384;&#20648;
&lt;/p&gt;
&lt;p&gt;
Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals. (arXiv:2305.18425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#27531;&#24046;&#20302;&#31209;&#29305;&#24615;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#39640;&#25928;&#23384;&#20648;&#30340;&#26032;&#26041;&#27861;ERE&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#37327;&#21270;&#21644;&#20998;&#23618;&#31209;&#20998;&#37197;&#26469;&#25552;&#39640;&#23384;&#20648;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#27531;&#24046;&#30340;&#20302;&#31209;&#29305;&#24615;&#26469;&#39640;&#25928;&#23384;&#20648;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#22823;&#22411;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#27531;&#24046;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#20302;&#31209;&#29305;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#25928;&#27531;&#24046;&#32534;&#30721;&#65288;ERE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#20302;&#31209;&#26435;&#37325;&#27531;&#24046;&#26469;&#23454;&#29616;&#23545;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#30340;&#39640;&#25928;&#23384;&#20648;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26435;&#37325;&#27531;&#24046;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#37327;&#21270;&#21644;&#20998;&#23618;&#31209;&#20998;&#37197;&#26469;&#25512;&#21160;&#23384;&#20648;&#25928;&#29575;&#30340;&#26497;&#38480;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#24577;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an efficient method for storing fine-tuned models by leveraging the low-rank properties of weight residuals. Our key observation is that weight residuals in large overparameterized models exhibit even stronger low-rank characteristics. Based on this insight, we propose Efficient Residual Encoding (ERE), a novel approach that achieves efficient storage of fine-tuned model weights by approximating the low-rank weight residuals. Furthermore, we analyze the robustness of weight residuals and push the limit of storage efficiency by utilizing additional quantization and layer-wise rank allocation. Our experimental results demonstrate that our method significantly reduces memory footprint while preserving performance in various tasks and modalities. We release our code.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28155;&#21152;&#22122;&#22768;&#30340;&#22810;&#23618;Sigmoid&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#24102;&#22122;&#22768;&#24773;&#20917;&#19979;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#29992;$\log(T/\sigma)$&#26469;&#30028;&#23450;&#65292;&#19981;&#23384;&#22312;&#22122;&#22768;&#26102;&#19979;&#30028;&#20026;$wT$&#65292;&#20004;&#32773;&#23384;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.18423</link><description>&lt;p&gt;
&#22122;&#38899;&#22312;&#23398;&#20064;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20013;&#30340;&#20316;&#29992;&#65306;&#38271;&#24207;&#21015;&#30340;&#25351;&#25968;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences. (arXiv:2305.18423v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28155;&#21152;&#22122;&#22768;&#30340;&#22810;&#23618;Sigmoid&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#24102;&#22122;&#22768;&#24773;&#20917;&#19979;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#29992;$\log(T/\sigma)$&#26469;&#30028;&#23450;&#65292;&#19981;&#23384;&#22312;&#22122;&#22768;&#26102;&#19979;&#30028;&#20026;$wT$&#65292;&#20004;&#32773;&#23384;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#28155;&#21152;&#29420;&#31435;&#22122;&#38899;&#30340;&#22810;&#23618;Sigmoid&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#31867;&#38271;&#24230;&#20026;T&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#31867;&#30340;PAC&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#34987;&#30028;&#23450;&#20026;$O (w\log(T/\sigma))$&#12290;&#23545;&#20110;&#30456;&#21516;&#31867;&#30340;&#38750;&#22122;&#22768;&#29256;&#26412;&#65288;&#21363;$\sigma=0$&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#20026;$\Omega (wT)$&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20986;&#22312;&#22122;&#22768;&#21644;&#38750;&#22122;&#22768;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23545;T&#30340;&#20381;&#36182;&#24615;&#20013;&#23384;&#22312;&#25351;&#25968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#19978;&#38480;&#23545;$1/\sigma$&#30340;&#23545;&#25968;&#20381;&#36182;&#24230;&#24456;&#23567;&#65292;&#21363;&#20351;&#38024;&#23545;&#25968;&#20540;&#19978;&#21487;&#20197;&#24573;&#30053;&#30340;$\sigma$&#65292;&#36825;&#20010;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the class of noisy multi-layered sigmoid recurrent neural networks with $w$ (unbounded) weights for classification of sequences of length $T$, where independent noise distributed according to $\mathcal{N}(0,\sigma^2)$ is added to the output of each neuron in the network. Our main result shows that the sample complexity of PAC learning this class can be bounded by $O (w\log(T/\sigma))$. For the non-noisy version of the same class (i.e., $\sigma=0$), we prove a lower bound of $\Omega (wT)$ for the sample complexity. Our results indicate an exponential gap in the dependence of sample complexity on $T$ for noisy versus non-noisy networks. Moreover, given the mild logarithmic dependence of the upper bound on $1/\sigma$, this gap still holds even for numerically negligible values of $\sigma$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperTime&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#26102;&#38388;&#19978;&#40065;&#26834;&#30340;&#39044;&#27979;&#24615;&#33021;&#36229;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#21382;&#21490;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#23545;&#24179;&#22343;&#39564;&#35777;&#25439;&#22833;&#21644;&#26368;&#22351;&#24773;&#20917;&#39564;&#35777;&#25439;&#22833;&#35774;&#32622;&#20102;&#35789;&#20856;&#20248;&#20808;&#32423;&#39034;&#24207;&#65292;&#24182;&#22312;&#22810;&#20010;&#24102;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#24378;&#21170;&#12290;</title><link>http://arxiv.org/abs/2305.18421</link><description>&lt;p&gt;
HyperTime: &#24212;&#23545;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperTime: Hyperparameter Optimization for Combating Temporal Distribution Shifts. (arXiv:2305.18421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperTime&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#26102;&#38388;&#19978;&#40065;&#26834;&#30340;&#39044;&#27979;&#24615;&#33021;&#36229;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#21382;&#21490;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#23545;&#24179;&#22343;&#39564;&#35777;&#25439;&#22833;&#21644;&#26368;&#22351;&#24773;&#20917;&#39564;&#35777;&#25439;&#22833;&#35774;&#32622;&#20102;&#35789;&#20856;&#20248;&#20808;&#32423;&#39034;&#24207;&#65292;&#24182;&#22312;&#22810;&#20010;&#24102;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#24378;&#21170;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;HyperTime&#8221;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#23547;&#25214;&#23545;&#26410;&#30693;&#27979;&#35797;&#25968;&#25454;&#30340;&#28508;&#22312;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24471;&#21040;&#20102;&#19968;&#20010;&#37325;&#35201;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#21363;&#36890;&#36807;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26102;&#38388;&#19978;&#30340;&#40065;&#26834;&#39044;&#27979;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#40065;&#26834;&#20248;&#21270;&#25991;&#29486;&#20013;&#30340;&#26368;&#22351;&#24773;&#20917;&#23548;&#21521;&#21746;&#23398;&#65292;&#24110;&#21161;&#25214;&#21040;&#36825;&#26679;&#30340;&#40065;&#26834;&#24615;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;HyperTime&#22312;&#21382;&#21490;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#23545;&#24179;&#22343;&#39564;&#35777;&#25439;&#22833;&#21644;&#26368;&#22351;&#24773;&#20917;&#39564;&#35777;&#25439;&#22833;&#35774;&#32622;&#20102;&#35789;&#20856;&#20248;&#20808;&#32423;&#39034;&#24207;&#12290;&#25105;&#20204;&#23545;&#39044;&#26399;&#27979;&#35797;&#25439;&#22833;&#30340;&#19978;&#38480;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#22312;&#22810;&#20010;&#20855;&#26377;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24378;&#22823;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a hyperparameter optimization method named \emph{HyperTime} to find hyperparameters robust to potential temporal distribution shifts in the unseen test data. Our work is motivated by an important observation that it is, in many cases, possible to achieve temporally robust predictive performance via hyperparameter optimization. Based on this observation, we leverage the `worst-case-oriented' philosophy from the robust optimization literature to help find such robust hyperparameter configurations. HyperTime imposes a lexicographic priority order on average validation loss and worst-case validation loss over chronological validation sets. We perform a theoretical analysis on the upper bound of the expected test loss, which reveals the unique advantages of our approach. We also demonstrate the strong empirical performance of the proposed method on multiple machine learning tasks with temporal distribution shifts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#20026;&#21160;&#24577;&#20915;&#31574;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#40065;&#26834;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23558;Q-learning&#19982;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.18420</link><description>&lt;p&gt;
&#26041;&#24046;&#20943;&#23569;&#30340;&#20998;&#24067;&#24335;&#40065;&#26834;Q-learning&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Variance-reduced Distributionally Robust Q-learning. (arXiv:2305.18420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#20026;&#21160;&#24577;&#20915;&#31574;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#40065;&#26834;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23558;Q-learning&#19982;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#21644;&#24212;&#29992;&#20013;&#65292;&#38754;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#21160;&#24577;&#20915;&#31574;&#26159;&#22522;&#26412;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#25910;&#38598;&#25152;&#22522;&#20110;&#30340;&#29615;&#22659;&#20998;&#24067;&#21487;&#33021;&#20250;&#19981;&#21516;&#20110;&#27169;&#22411;&#37096;&#32626;&#25152;&#22522;&#20110;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21363;&#20998;&#24067;&#24335;&#40065;&#26834;Q-learning&#21644;&#23427;&#30340;&#26041;&#24046;&#20943;&#23569;&#23545;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#40065;&#26834;&#31574;&#30053;&#65292;&#23613;&#31649;&#20250;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#20123;&#31639;&#27861;&#26088;&#22312;&#23558;&#24102;&#26377;Kullback-Leibler&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#26080;&#38480;&#26102;&#22495;$\gamma$-&#25240;&#25187;&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;$q$-&#20989;&#25968;&#20197;&#20803;&#32032;$\epsilon$-&#31934;&#24230;&#26377;&#25928;&#36924;&#36817;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#26041;&#24046;&#20943;&#23569;&#30340;&#20998;&#24067;&#24335;&#40065;&#26834;Q-learning&#23558;&#21516;&#27493;Q-learning&#19982;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#20854;&#24615;&#33021;&#65292;&#24182;&#19988;&#25105;&#20204;&#24314;&#31435;&#20102;&#23427;&#36798;&#21040;$ \tilde O(|S||A|(1-\gamma)^{-4}\epsilon^{-4}$&#30340;&#26368;&#23567;&#26368;&#22823;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic decision making under distributional shifts is of fundamental interest in theory and applications of reinforcement learning: The distribution of the environment on which the data is collected can differ from that of the environment on which the model is deployed. This paper presents two novel model-free algorithms, namely the distributionally robust Q-learning and its variance-reduced counterpart, that can effectively learn a robust policy despite distributional shifts. These algorithms are designed to efficiently approximate the $q$-function of an infinite-horizon $\gamma$-discounted robust Markov decision process with Kullback-Leibler uncertainty set to an entry-wise $\epsilon$-degree of precision. Further, the variance-reduced distributionally robust Q-learning combines the synchronous Q-learning with variance-reduction techniques to enhance its performance. Consequently, we establish that it attains a minmax sample complexity upper bound of $\tilde O(|S||A|(1-\gamma)^{-4}\e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#37319;&#29992;&#21452;&#21521;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38271;&#31687;&#38899;&#39057;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.18419</link><description>&lt;p&gt;
&#37319;&#29992;&#21452;&#21521;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#25552;&#39640;&#38271;&#31687;&#38899;&#39057;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR. (arXiv:2305.18419v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#37319;&#29992;&#21452;&#21521;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38271;&#31687;&#38899;&#39057;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#31163;&#21457;&#38899;&#20013;&#30340;&#35821;&#20041;&#23436;&#25972;&#21477;&#23376;&#23454;&#29616;&#38271;&#31687;&#38899;&#39057;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#38450;&#27490;ASR&#35299;&#30721;&#22120;&#22788;&#29702;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#30340;&#26080;&#25928;&#20449;&#24687;&#65292;&#21516;&#26102;&#36991;&#20813;&#23427;&#38169;&#36807;&#24403;&#21069;&#21477;&#23376;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#20070;&#38754;&#25991;&#26412;&#20013;&#65292;&#35821;&#20041;&#23436;&#25972;&#30340;&#21477;&#23376;&#36890;&#24120;&#30001;&#26631;&#28857;&#31526;&#21495;&#20998;&#38548;&#65307;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#23454;&#38469;&#21475;&#35821;&#20013;&#30340;&#21457;&#38899;&#24456;&#23569;&#21253;&#21547;&#26631;&#28857;&#31526;&#21495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#28860;&#20174;&#20070;&#38754;&#26631;&#28857;&#25991;&#26412;&#20013;&#35757;&#32451;&#30340;&#21452;&#21521;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20013;&#30340;&#26631;&#28857;&#31526;&#21495;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20174;LM&#25945;&#24072;&#25552;&#28860;&#30340;&#20998;&#21106;&#22120;&#19982;&#20197;&#20854;&#20182;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22768;&#23398;&#20572;&#39039;&#30340;&#25945;&#24072;&#25552;&#28860;&#30340;&#20998;&#21106;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#27969;&#23186;&#20307;ASR&#31649;&#36947;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#20998;&#21106;&#22120;&#22312;YouTube&#23383;&#24149;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;3.2%&#30340;&#30456;&#23545;WER&#22686;&#30410;&#20197;&#21450;60ms&#20013;&#20301;&#27573;&#26411;&#31471;&#24310;&#36831;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method of segmenting long-form speech by separating semantically complete sentences within the utterance. This prevents the ASR decoder from needlessly processing faraway context while also preventing it from missing relevant context within the current sentence. Semantically complete sentence boundaries are typically demarcated by punctuation in written text; but unfortunately, spoken real-world utterances rarely contain punctuation. We address this limitation by distilling punctuation knowledge from a bidirectional teacher language model (LM) trained on written, punctuated text. We compare our segmenter, which is distilled from the LM teacher, against a segmenter distilled from a acoustic-pause-based teacher used in other works, on a streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2% relative WER gain along with a 60 ms median end-of-segment latency reduction on a YouTube captioning task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20869;&#23384;&#21463;&#21040;&#26497;&#31471;&#38480;&#21046;&#26102;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.18418</link><description>&lt;p&gt;
&#19968;&#30629;&#65306;&#37325;&#26032;&#24605;&#32771;&#35270;&#39057;&#19981;&#26029;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Just a Glimpse: Rethinking Temporal Information for Video Continual Learning. (arXiv:2305.18418v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20869;&#23384;&#21463;&#21040;&#26497;&#31471;&#38480;&#21046;&#26102;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26159;&#36830;&#32493;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#37325;&#35201;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#23494;&#20999;&#30456;&#20851;&#12290;&#38543;&#30528;&#31867;&#21035;/&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#21463;&#21040;&#20869;&#23384;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#20250;&#20986;&#29616;&#12290;&#22312;&#35270;&#39057;&#39046;&#22495;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35270;&#39057;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#24103;&#65292;&#36825;&#20250;&#20351;&#22238;&#25918;&#35760;&#24518;&#36127;&#25285;&#26356;&#37325;&#12290;&#30446;&#21069;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#20174;&#35270;&#39057;&#27969;&#20013;&#23545;&#24103;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#22238;&#25918;&#35760;&#24518;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26497;&#31471;&#20869;&#23384;&#38480;&#21046;&#19979;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20174;&#20195;&#34920;&#22823;&#37327;&#29420;&#29305;&#35270;&#39057;&#30340;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#35270;&#39057;&#25968;&#25454;&#38598;Kin&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning is one of the most important settings for the study of Continual Learning, as it closely resembles real-world application scenarios. With constrained memory sizes, catastrophic forgetting arises as the number of classes/tasks increases. Studying continual learning in the video domain poses even more challenges, as video data contains a large number of frames, which places a higher burden on the replay memory. The current common practice is to sub-sample frames from the video stream and store them in the replay memory. In this paper, we propose SMILE a novel replay mechanism for effective video continual learning based on individual/single frames. Through extensive experimentation, we show that under extreme memory constraints, video diversity plays a more significant role than temporal information. Therefore, our method focuses on learning from a small number of frames that represent a large number of unique videos. On three representative video datasets, Kin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#36807;&#31243;&#27880;&#24847;&#21147;&#21644;&#32593;&#26684;&#32534;&#30721;&#30340;&#31639;&#27861;&#65292;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#29702;&#35299;&#22823;&#33041;&#24378;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18417</link><description>&lt;p&gt;
&#28857;&#36807;&#31243;&#27880;&#24847;&#21147;&#25903;&#25345;&#32593;&#26684;&#32534;&#30721;&#20197;&#23454;&#29616;&#20998;&#24067;&#22806;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Determinantal Point Process Attention Over Grid Codes Supports Out of Distribution Generalization. (arXiv:2305.18417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#36807;&#31243;&#27880;&#24847;&#21147;&#21644;&#32593;&#26684;&#32534;&#30721;&#30340;&#31639;&#27861;&#65292;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#29702;&#35299;&#22823;&#33041;&#24378;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#20223;&#31867;&#20154;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#26469;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#35299;&#20915;&#22797;&#26434;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#33021;&#25552;&#20379;&#20851;&#20110;&#22823;&#33041;&#22914;&#20309;&#25903;&#25345;&#20154;&#31867;&#33021;&#22815;&#23454;&#29616;&#30340;&#24378;&#24418;&#24335;&#27867;&#21270;&#30340;&#35265;&#35299;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#27867;&#21270;&#8212;&#8212;&#22312;&#35757;&#32451;&#38598;&#20998;&#24067;&#20043;&#22806;&#30340;&#27979;&#35797;&#26679;&#20363;&#19978;&#25104;&#21151;&#25191;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#22823;&#33041;&#22788;&#29702;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#26377;&#21161;&#20110;&#23454;&#29616;&#36825;&#31181;&#33021;&#21147;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#31639;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#35745;&#31639;&#30340;&#29305;&#23450;&#29305;&#24449;&#23454;&#29616;OOD&#27867;&#21270;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35748;&#30693;&#20219;&#21153;&#30340;&#34920;&#29616;&#26469;&#25552;&#20379;&#27010;&#24565;&#39564;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21754;&#20083;&#21160;&#29289;&#22823;&#33041;&#20351;&#29992;&#31867;&#20284;&#32593;&#26684;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#22312;&#20869;&#21957;&#30382;&#23618;&#20013;&#65289;&#26469;&#34920;&#31034;&#24230;&#37327;&#31354;&#38388;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have made tremendous gains in emulating human-like intelligence, and have been used increasingly as ways of understanding how the brain may solve the complex computational problems on which this relies. However, these still fall short of, and therefore fail to provide insight into how the brain supports strong forms of generalization of which humans are capable. One such case is out-of-distribution (OOD) generalization -successful performance on test examples that lie outside the distribution of the training set. Here, we identify properties of processing in the brain that may contribute to this ability. We describe a two-part algorithm that draws on specific features of neural computation to achieve OOD generalization, and provide a proof of concept by evaluating performance on two challenging cognitive tasks. First we draw on the fact that the mammalian brain represents metric spaces using grid-like representations (e.g., in entorhinal cortex): abstract represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#25311;&#20132;&#21449;&#26639;&#30340;&#38750;&#29702;&#24819;&#24615;&#20197;&#21450;&#23427;&#20204;&#23545;&#28857;&#31215;&#25805;&#20316;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#26102;&#20132;&#21449;&#26639;&#24863;&#30693;&#31934;&#32454;&#35843;&#25972;&#25209;&#37327;&#24402;&#19968;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#24433;&#21709;&#65292;&#20174;&#32780;&#38477;&#20302;&#20869;&#23384;&#35745;&#31639;&#30340;&#37325;&#26032;&#35757;&#32451;&#30340;&#30828;&#20214;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.18416</link><description>&lt;p&gt;
&#26816;&#26597;&#25209;&#37327;&#24402;&#19968;&#21270;&#20248;&#21270;&#22312;&#20869;&#23384;&#35745;&#31639;&#20013;&#20943;&#36731;&#19981;&#21516;&#30828;&#20214;&#22122;&#22768;&#30340;&#20316;&#29992;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Examining the Role and Limits of Batchnorm Optimization to Mitigate Diverse Hardware-noise in In-memory Computing. (arXiv:2305.18416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#25311;&#20132;&#21449;&#26639;&#30340;&#38750;&#29702;&#24819;&#24615;&#20197;&#21450;&#23427;&#20204;&#23545;&#28857;&#31215;&#25805;&#20316;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#26102;&#20132;&#21449;&#26639;&#24863;&#30693;&#31934;&#32454;&#35843;&#25972;&#25209;&#37327;&#24402;&#19968;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#24433;&#21709;&#65292;&#20174;&#32780;&#38477;&#20302;&#20869;&#23384;&#35745;&#31639;&#30340;&#37325;&#26032;&#35757;&#32451;&#30340;&#30828;&#20214;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23384;&#35745;&#31639;&#65288;IMC&#65289;&#24179;&#21488;&#22914;&#27169;&#25311;&#20132;&#21449;&#26639;&#27491;&#22312;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#20419;&#36827;&#20102;&#20302;&#31934;&#24230;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21152;&#36895;&#65292;&#24182;&#20855;&#26377;&#39640;&#38754;&#31215;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20132;&#21449;&#26639;&#20013;&#30340;&#22266;&#26377;&#38750;&#29702;&#24819;&#24615;&#20351;&#24471;&#37096;&#32626;&#30340;DNN&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#38500;&#20102;&#37327;&#21270;&#35823;&#24046;&#20043;&#22806;&#65292;&#25512;&#29702;&#36807;&#31243;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#38750;&#29702;&#24819;&#24615;&#21253;&#25324;&#20132;&#21449;&#26639;&#30005;&#36335;&#32423;&#23492;&#29983;&#30005;&#38459;&#21644;&#35774;&#22791;&#32423;&#38750;&#29702;&#24819;&#24615;&#65292;&#22914;&#38543;&#26426;&#35835;&#22122;&#22768;&#21644;&#26102;&#38388;&#28418;&#31227;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23494;&#20999;&#26816;&#26597;&#36825;&#20123;&#38750;&#29702;&#24819;&#24615;&#23545;&#27169;&#25311;&#20132;&#21449;&#26639;&#20013;&#30340;&#28857;&#31215;&#25805;&#20316;&#36896;&#25104;&#30340;&#25197;&#26354;&#65292;&#24182;&#25506;&#32034;&#23454;&#26102;&#36890;&#36807;&#24863;&#30693;&#20132;&#21449;&#26639;&#31934;&#32454;&#35843;&#25972;&#25209;&#37327;&#24402;&#19968;&#21270;&#21442;&#25968;&#26469;&#20943;&#36731;&#38750;&#29702;&#24819;&#24615;&#24433;&#21709;&#30340;&#20960;&#20046;&#26080;&#38656;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;IMC&#22122;&#22768;&#24863;&#30693;&#37325;&#26032;&#35757;&#32451;&#30340;&#30828;&#20214;&#25104;&#26412;&#65292;&#21253;&#25324;&#20869;&#23384;&#21644;&#35757;&#32451;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Memory Computing (IMC) platforms such as analog crossbars are gaining focus as they facilitate the acceleration of low-precision Deep Neural Networks (DNNs) with high area- &amp; compute-efficiencies. However, the intrinsic non-idealities in crossbars, which are often non-deterministic and non-linear, degrade the performance of the deployed DNNs. In addition to quantization errors, most frequently encountered non-idealities during inference include crossbar circuit-level parasitic resistances and device-level non-idealities such as stochastic read noise and temporal drift. In this work, our goal is to closely examine the distortions caused by these non-idealities on the dot-product operations in analog crossbars and explore the feasibility of a nearly training-less solution via crossbar-aware fine-tuning of batchnorm parameters in real-time to mitigate the impact of the non-idealities. This enables reduction in hardware costs in terms of memory and training energy for IMC noise-aware re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26550;&#26500;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#25968;&#25454;&#38382;&#39064;&#12290;GATr&#20351;&#29992;&#25237;&#24433;&#20960;&#20309;&#20195;&#25968;&#34920;&#31034;&#36755;&#20837;&#36755;&#20986;&#21644;&#29366;&#24577;&#65292;&#20855;&#26377;&#21487;&#32553;&#25918;&#24615;&#12289;&#34920;&#36798;&#24615;&#12289;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;n&#20307;&#24314;&#27169;&#21644;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23454;&#39564;&#20013;&#65292;GATr&#30456;&#23545;&#20110;&#38750;&#20960;&#20309;&#22522;&#32447;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.18415</link><description>&lt;p&gt;
&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Geometric Algebra Transformers. (arXiv:2305.18415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26550;&#26500;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#25968;&#25454;&#38382;&#39064;&#12290;GATr&#20351;&#29992;&#25237;&#24433;&#20960;&#20309;&#20195;&#25968;&#34920;&#31034;&#36755;&#20837;&#36755;&#20986;&#21644;&#29366;&#24577;&#65292;&#20855;&#26377;&#21487;&#32553;&#25918;&#24615;&#12289;&#34920;&#36798;&#24615;&#12289;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;n&#20307;&#24314;&#27169;&#21644;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23454;&#39564;&#20013;&#65292;GATr&#30456;&#23545;&#20110;&#38750;&#20960;&#20309;&#22522;&#32447;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#25968;&#25454;&#38382;&#39064;&#28041;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#20154;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#39046;&#22495;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;&#35768;&#22810;&#24418;&#24335;&#65292;&#20363;&#22914;&#28857;&#12289;&#26041;&#21521;&#21521;&#37327;&#12289;&#24179;&#38754;&#25110;&#21464;&#25442;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22914;&#27492;&#22810;&#31181;&#20960;&#20309;&#31867;&#22411;, &#21516;&#26102;&#23562;&#37325;&#23427;&#20204;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#20960;&#20309;&#25968;&#25454;&#30340;&#36890;&#29992;&#26550;&#26500;&#12290;GATr&#20351;&#29992;&#25237;&#24433;&#20960;&#20309;&#20195;&#25968;&#26469;&#34920;&#31034;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#20854;&#25552;&#20379;&#24120;&#35265;&#20960;&#20309;&#23545;&#35937;&#30340;&#39640;&#25928;16&#32500;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#20197;&#21450;&#20316;&#29992;&#20110;&#23427;&#20204;&#30340;&#36816;&#31639;&#31526;&#12290;GATr&#26159;&#30456;&#23545;&#20110;E(3)&#65288;3D&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#23545;&#31216;&#32676;&#65289;&#31561;&#21464;&#30340;&#12290;&#20316;&#20026;&#21464;&#25442;&#22120;&#65292;GATr&#21487;&#25193;&#23637;&#12289;&#34920;&#36798;&#20016;&#23500;&#19988;&#22810;&#21151;&#33021;&#12290;&#22312;n&#20307;&#24314;&#27169;&#21644;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23454;&#39564;&#20013;&#65292;GATr&#30456;&#23545;&#20110;&#38750;&#20960;&#20309;&#22522;&#32447;&#22343;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Problems involving geometric data arise in a variety of fields, including computer vision, robotics, chemistry, and physics. Such data can take numerous forms, such as points, direction vectors, planes, or transformations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric algebra, which offers an efficient 16-dimensional vector space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a transformer, GATr is scalable, expressive, and versatile. In experiments with n-body modeling and robotic planning, GATr shows strong improvements over non-geometric baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.18413</link><description>&lt;p&gt;
&#20174;API&#23398;&#20064;&#23398;&#20064;&#65306;&#40657;&#30418;&#25968;&#25454;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#65288;DFML&#65289;&#26088;&#22312;&#36890;&#36807;&#20174;&#19968;&#32452;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20803;&#23398;&#20064;&#32780;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;DFML&#24037;&#20316;&#20165;&#33021;&#20174;&#65288;i&#65289;&#30333;&#30418;&#21644;&#65288;ii&#65289;&#23567;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;iii&#65289;&#30456;&#21516;&#30340;&#26550;&#26500;&#20013;&#20803;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#21363;&#29992;&#25143;&#20165;&#33021;&#36890;&#36807;&#20219;&#24847;&#27169;&#22411;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;API&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25968;&#25454;&#26080;&#20851;&#20803;&#30693;&#35782;&#33976;&#39311;&#65288;BiDf-MKD&#65289;&#26694;&#26550;&#65292;&#23558;&#26356;&#36890;&#29992;&#30340;&#20803;&#30693;&#35782;&#20174;&#19968;&#32452;&#40657;&#30418;API&#36716;&#31227;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;Hawkes&#36807;&#31243;&#19979;&#24322;&#36136;&#20107;&#20214;&#21160;&#24577;&#30340;&#30701;&#26102;&#24207;&#20381;&#36182;&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;MHP&#21644;&#33258;&#28608;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18412</link><description>&lt;p&gt;
Hawkes&#36807;&#31243;&#19979;&#24322;&#36136;&#20107;&#20214;&#21160;&#24577;&#30340;&#30701;&#26102;&#24207;&#20381;&#36182;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Short-term Temporal Dependency Detection under Heterogeneous Event Dynamic with Hawkes Processes. (arXiv:2305.18412v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;Hawkes&#36807;&#31243;&#19979;&#24322;&#36136;&#20107;&#20214;&#21160;&#24577;&#30340;&#30701;&#26102;&#24207;&#20381;&#36182;&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;MHP&#21644;&#33258;&#28608;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#34920;&#29616;&#20986;&#30456;&#20114;&#28608;&#21169;&#25110;&#25233;&#21046;&#30340;&#27169;&#24335;&#12290;&#21487;&#38752;&#22320;&#26816;&#27979;&#36825;&#31181;&#26102;&#38388;&#20381;&#36182;&#24615;&#23545;&#31185;&#23398;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#20013;&#26368;&#24120;&#29992;&#30340;&#26159;&#22810;&#20803;Hawkes&#36807;&#31243;(MHP)&#65292;&#20854;&#24433;&#21709;&#20989;&#25968;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;Granger&#22240;&#26524;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#32477;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#26631;&#20934;MHP&#24378;&#24230;&#30340;&#30452;&#25509;&#25110;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#19982;&#30495;&#23454;&#25968;&#25454;&#19981;&#19968;&#33268;&#12290;&#22312;&#19981;&#35268;&#21017;&#21644;&#26410;&#30693;&#30340;&#24322;&#36136;&#24378;&#24230;&#19979;&#65292;&#25429;&#25417;&#26102;&#24207;&#20381;&#36182;&#24615;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#20154;&#20204;&#38590;&#20197;&#21306;&#20998;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#21644;&#24378;&#24230;&#27874;&#21160;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#38024;&#23545;&#30701;&#26102;&#24207;&#20381;&#36182;&#24615;&#26816;&#27979;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26469;&#33258;MHP&#30340;&#20132;&#21449;&#24433;&#21709;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;(MLE)&#23384;&#22312;&#26080;&#27861;&#28040;&#38500;&#20294;&#21487;&#20197;&#36890;&#36807;&#25968;&#37327;&#32423;&#20943;&#23569;&#30340;&#35823;&#24046;&#65292;&#20351;&#29992;&#30340;&#26159;&#20114;&#21160;HP&#30340;&#24322;&#36136;&#24378;&#24230;&#32780;&#19981;&#26159;&#30446;&#26631;HP&#30340;&#24322;&#36136;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;MHP&#21644;&#33258;&#28608;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30701;&#26102;&#24207;&#20381;&#36182;&#26816;&#27979;&#30340;&#40065;&#26834;&#26694;&#26550;&#12290;&#22522;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many event sequence data exhibit mutually exciting or inhibiting patterns. Reliable detection of such temporal dependency is crucial for scientific investigation. The de facto model is the Multivariate Hawkes Process (MHP), whose impact function naturally encodes a causal structure in Granger causality. However, the vast majority of existing methods use direct or nonlinear transform of standard MHP intensity with constant baseline, inconsistent with real-world data. Under irregular and unknown heterogeneous intensity, capturing temporal dependency is hard as one struggles to distinguish the effect of mutual interaction from that of intensity fluctuation. In this paper, we address the short-term temporal dependency detection issue. We show the maximum likelihood estimation (MLE) for cross-impact from MHP has an error that can not be eliminated but may be reduced by order of magnitude, using heterogeneous intensity not of the target HP but of the interacting HP. Then we proposed a robust
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23485;&#24230;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#27809;&#26377;&#24433;&#21709;&#65292;&#32593;&#32476;&#22312;&#26089;&#26399;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#24615;&#65292;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#26469;&#35828;&#36825;&#19968;&#19968;&#33268;&#24615;&#36143;&#31359;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#19988;&#22823;&#23485;&#24230;&#19979;&#30340;&#32467;&#26500;&#29305;&#24615;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#34920;&#26126;&#29305;&#24449;&#23398;&#20064;&#26497;&#38480;&#21487;&#20197;&#25429;&#25417;&#21040;&#29616;&#23454;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.18411</link><description>&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#32593;&#32476;&#22312;&#23454;&#38469;&#35268;&#27169;&#19979;&#20855;&#26377;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Feature-Learning Networks Are Consistent Across Widths At Realistic Scales. (arXiv:2305.18411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23485;&#24230;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#27809;&#26377;&#24433;&#21709;&#65292;&#32593;&#32476;&#22312;&#26089;&#26399;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#24615;&#65292;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#26469;&#35828;&#36825;&#19968;&#19968;&#33268;&#24615;&#36143;&#31359;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#19988;&#22823;&#23485;&#24230;&#19979;&#30340;&#32467;&#26500;&#29305;&#24615;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#34920;&#26126;&#29305;&#24449;&#23398;&#20064;&#26497;&#38480;&#21487;&#20197;&#25429;&#25417;&#21040;&#29616;&#23454;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#32593;&#32476;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#23545;&#32593;&#32476;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#35757;&#32451;&#26089;&#26399;&#65292;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#23485;&#32593;&#32476;&#19981;&#20165;&#20855;&#26377;&#30456;&#21516;&#30340;&#25439;&#22833;&#26354;&#32447;&#65292;&#32780;&#19988;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27979;&#35797;&#39044;&#27979;&#20063;&#26159;&#19968;&#33268;&#30340;&#12290;&#23545;&#20110;&#20687;CIFAR-5m&#36825;&#26679;&#30340;&#31616;&#21333;&#20219;&#21153;&#65292;&#36825;&#36866;&#29992;&#20110;&#20855;&#26377;&#23454;&#38469;&#23485;&#24230;&#30340;&#32593;&#32476;&#30340;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#21253;&#25324;&#20869;&#37096;&#34920;&#31034;&#12289;&#39044;&#28608;&#27963;&#20998;&#24067;&#12289;&#31283;&#23450;&#24615;&#36793;&#32536;&#29616;&#35937;&#21644;&#22823;&#23398;&#20064;&#29575;&#25928;&#24212;&#65292;&#22312;&#22823;&#23485;&#24230;&#19979;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#21551;&#21457;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#22312;&#29305;&#24449;&#23398;&#20064;&#26497;&#38480;&#19979;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#29616;&#23454;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#29616;&#35937;&#12290;&#23545;&#20110;&#26356;&#38590;&#30340;&#20219;&#21153;&#65288;&#22914;ImageNet&#21644;&#35821;&#35328;&#24314;&#27169;&#65289;&#21644;&#26356;&#26202;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#26377;&#38480;&#23485;&#24230;&#30340;&#20559;&#24046;&#20250;&#31995;&#32479;&#22320;&#22686;&#38271;&#12290;&#36825;&#20123;&#20559;&#24046;&#26159;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#25928;&#24212;&#24341;&#36215;&#30340;&#12290;&#39318;&#20808;&#65292;&#32593;&#32476;&#36755;&#20986;&#20855;&#26377;&#21021;&#22987;&#21270;&#30456;&#20851;&#30340;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
We study the effect of width on the dynamics of feature-learning neural networks across a variety of architectures and datasets. Early in training, wide neural networks trained on online data have not only identical loss curves but also agree in their point-wise test predictions throughout training. For simple tasks such as CIFAR-5m this holds throughout training for networks of realistic widths. We also show that structural properties of the models, including internal representations, preactivation distributions, edge of stability phenomena, and large learning rate effects are consistent across large widths. This motivates the hypothesis that phenomena seen in realistic models can be captured by infinite-width, feature-learning limits. For harder tasks (such as ImageNet and language modeling), and later training times, finite-width deviations grow systematically. Two distinct effects cause these deviations across widths. First, the network output has initialization-dependent variance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#22810;&#32452;&#23398;&#25968;&#25454;&#22312;&#22240;&#26524;&#25512;&#26029;&#12289;&#22522;&#22240;&#32452;&#23398;&#21644;&#20083;&#33146;&#30284;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#20998;&#26512;&#22522;&#22240;&#32452;&#25200;&#21160;&#23545;&#20083;&#33146;&#30284;&#24739;&#32773;&#29983;&#23384;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18410</link><description>&lt;p&gt;
&#29702;&#35299;&#20083;&#33146;&#30284;&#29983;&#23384;&#65306;&#22312;&#22810;&#32452;&#23398;&#25968;&#25454;&#19978;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#21644;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data. (arXiv:2305.18410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#22810;&#32452;&#23398;&#25968;&#25454;&#22312;&#22240;&#26524;&#25512;&#26029;&#12289;&#22522;&#22240;&#32452;&#23398;&#21644;&#20083;&#33146;&#30284;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#20998;&#26512;&#22522;&#22240;&#32452;&#25200;&#21160;&#23545;&#20083;&#33146;&#30284;&#24739;&#32773;&#29983;&#23384;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23545;&#26356;&#26131;&#29992;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#21457;&#23637;&#21644;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#20197;&#36890;&#36807;&#20998;&#26512;&#35266;&#27979;&#25968;&#25454;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#20020;&#24202;&#21307;&#29983;&#21644;&#29983;&#29289;&#23398;&#23478;&#39044;&#27979;&#30142;&#30149;&#39044;&#21518;&#24182;&#24314;&#35758;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#22240;&#26524;&#25512;&#26029;&#12289;&#22522;&#22240;&#32452;&#23398;&#21644;&#20083;&#33146;&#30284;&#20132;&#21449;&#39046;&#22495;&#19978;&#26497;&#23569;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#30495;&#23454;&#25968;&#25454;&#19978;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#35780;&#20272;&#26222;&#36941;&#26497;&#20026;&#22256;&#38590;&#65292;&#22240;&#20026;&#22320;&#38754;&#30495;&#23454;&#30340;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#35780;&#20272;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#21512;&#36866;&#30340;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#26469;&#30740;&#31350;&#22522;&#22240;&#32452;&#20013;&#21508;&#31181;&#25200;&#21160;&#22914;&#20309;&#24433;&#21709;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for more usable and explainable machine learning models in healthcare increases the importance of developing and utilizing causal discovery algorithms, which aim to discover causal relations by analyzing observational data. Explainable approaches aid clinicians and biologists in predicting the prognosis of diseases and suggesting proper treatments. However, very little research has been conducted at the crossroads between causal discovery, genomics, and breast cancer, and we aim to bridge this gap. Moreover, evaluation of causal discovery methods on real data is in general notoriously difficult because ground-truth causal relations are usually unknown, and accordingly, in this paper, we also propose to address the evaluation problem with large language models. In particular, we exploit suitable causal discovery algorithms to investigate how various perturbations in the genome can affect the survival of patients diagnosed with breast cancer. We used three main causal discovery 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29702;&#35770;&#19978;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.18409</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#23398;&#20064;&#65306;&#31616;&#21333;&#19988;&#21487;&#35777;&#26126;&#30340;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms. (arXiv:2305.18409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29702;&#35770;&#19978;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#19982;&#22810;&#20010;&#30446;&#26631;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65288;&#22914;&#22810;&#26631;&#20934;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#65289;&#20013;&#19968;&#20010;&#26377;&#24433;&#21709;&#21147;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#26041;&#21521;&#30340;&#37051;&#22495;&#20869;&#38480;&#21046;&#20844;&#20849;&#19979;&#38477;&#26041;&#21521;&#26469;&#35268;&#33539;&#32447;&#24615;&#32452;&#21512;&#30446;&#26631;&#30340;&#26368;&#20248;&#26041;&#21521;&#65292;&#20363;&#22914;MTL&#20013;&#30340;&#24179;&#22343;&#25439;&#22833;&#12290; &#36825;&#20010;&#20844;&#24335;&#21253;&#25324;GD&#21644;MGDA&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#20139;&#21463;&#20687;CAGrad&#20013;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#26377;&#21033;&#20110;&#38543;&#26426;&#31639;&#27861;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#26041;&#21521;&#23548;&#21521;&#22810;&#30446;&#26631;&#26799;&#24230;&#19979;&#38477;&#65288;SDMGrad&#65289;&#65292;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;SGD&#31867;&#22411;&#30340;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#21450;&#22312;&#30446;&#26631;&#25968;&#37327;&#36739;&#22810;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#30446;&#26631;&#37319;&#26679;&#30340;SDMGrad-OS&#31639;&#27861;&#12290; &#23545;&#20110;&#24658;&#23450;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#955;&#65292;&#25105;&#20204;&#35777;&#26126;SDMGrad&#21644;SDMGrad-OS&#30830;&#23454;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective problem by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling in the setting where the number of objectives is large. For a constant-level regularization parameter $\lambda$, we show that SDMGrad and SDMGrad-OS provably converge to a Pareto stationary poin
&lt;/p&gt;</description></item><item><title>MoleculeSDE&#26159;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20960;&#20309;&#19982;2D&#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.18407</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining. (arXiv:2305.18407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18407
&lt;/p&gt;
&lt;p&gt;
MoleculeSDE&#26159;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20960;&#20309;&#19982;2D&#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#25552;&#39640;&#22522;&#20110; AI &#30340;&#33647;&#29289;&#21457;&#29616;&#24615;&#33021;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#20851;&#27880;&#21333;&#19968;&#30340;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#22823;&#21270;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#21487;&#20197;&#22686;&#24378;&#20998;&#23376;&#34920;&#31034;&#33021;&#21147;&#12290;&#32780;&#29616;&#26377;&#30340;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20174;&#25299;&#25169;&#21644;&#20960;&#20309;&#32534;&#30721;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#20272;&#35745; MI&#65292;&#22240;&#27492;&#20002;&#22833;&#20102;&#20998;&#23376;&#30340;&#20851;&#38190;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MoleculeSDE&#12290;MoleculeSDE&#21033;&#29992;&#32676;&#23545;&#31216;&#65288;&#22914; SE&#65288;3&#65289;-&#31561;&#21464;&#21644;&#21453;&#23556;-&#21453;&#23545;&#31216;&#65289;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104; 3D &#20960;&#20309;&#24418;&#29366;&#19982; 2D &#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#23427;&#19981;&#20165;&#33719;&#24471;&#26356;&#32039;&#30340;MI&#30028;&#38480;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecule pretraining has quickly become the go-to schema to boost the performance of AI-based drug discovery. Naturally, molecules can be represented as 2D topological graphs or 3D geometric point clouds. Although most existing pertaining methods focus on merely the single modality, recent research has shown that maximizing the mutual information (MI) between such two modalities enhances the molecule representation ability. Meanwhile, existing molecule multi-modal pretraining approaches approximate MI based on the representation space encoded from the topology and geometry, thus resulting in the loss of critical structural information of molecules. To address this issue, we propose MoleculeSDE. MoleculeSDE leverages group symmetric (e.g., SE(3)-equivariant and reflection-antisymmetric) stochastic differential equation models to generate the 3D geometries from 2D topologies, and vice versa, directly in the input space. It not only obtains tighter MI bound but also enables prosperous dow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#24494;&#36890;&#36947;&#20013;&#30340;&#20256;&#28909;&#31995;&#25968;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#32463;&#39564;&#30456;&#20851;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.18406</link><description>&lt;p&gt;
&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24494;&#36890;&#36947;&#20013;&#30340;&#20256;&#28909;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
A machine learning approach to the prediction of heat-transfer coefficients in micro-channels. (arXiv:2305.18406v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#24494;&#36890;&#36947;&#20013;&#30340;&#20256;&#28909;&#31995;&#25968;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#32463;&#39564;&#30456;&#20851;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#21452;&#30456;&#20256;&#28909;&#31995;&#25968;&#23545;&#21387;&#32553;&#24335;&#25442;&#28909;&#22120;&#30340;&#20248;&#21270;&#35774;&#35745;&#21644;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#36827;&#27493;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24212;&#29992;&#26377;&#25152;&#25552;&#21319;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#26469;&#20272;&#35745;&#24494;&#36890;&#36947;&#20013;&#30340;&#20256;&#28909;&#31995;&#25968;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#36136;&#37327;&#27969;&#37327;&#12289;&#28909;&#27969;&#37327;&#12289;&#31995;&#32479;&#21387;&#21147;&#21644;&#36890;&#36947;&#30452;&#24452;&#21644;&#38271;&#24230;&#30340;&#20989;&#25968;&#26469;&#35757;&#32451;&#21644;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#32463;&#39564;&#30456;&#20851;&#24335;&#65292;&#20294;&#23384;&#22312;&#36807;&#25311;&#21512;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#32467;&#26524;&#35299;&#37322;&#31561;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate prediction of the two-phase heat transfer coefficient (HTC) as a function of working fluids, channel geometries and process conditions is key to the optimal design and operation of compact heat exchangers. Advances in artificial intelligence research have recently boosted the application of machine learning (ML) algorithms to obtain data-driven surrogate models for the HTC. For most supervised learning algorithms, the task is that of a nonlinear regression problem. Despite the fact that these models have been proven capable of outperforming traditional empirical correlations, they have key limitations such as overfitting the data, the lack of uncertainty estimation, and interpretability of the results. To address these limitations, in this paper, we use a multi-output Gaussian process regression (GPR) to estimate the HTC in microchannels as a function of the mass flow rate, heat flux, system pressure and channel diameter and length. The model is trained using the Brunel Tw
&lt;/p&gt;</description></item><item><title>Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18405</link><description>&lt;p&gt;
Dink-Net: &#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18405
&lt;/p&gt;
&lt;p&gt;
Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#24418;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#32452;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Dink-Net&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21306;&#20998;&#24102;&#22686;&#24378;&#30340;&#36319;&#19981;&#24102;&#22686;&#24378;&#30340;&#33410;&#28857;&#65292;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#24418;&#24335;&#12290;&#21516;&#26102;&#65292;&#23558;&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#26041;&#24335;&#26368;&#23567;&#21270;&#25552;&#20986;&#30340;&#38598;&#32676;&#33192;&#32960;&#25439;&#22833;&#21644;&#38598;&#32676;&#25910;&#32553;&#25439;&#22833;&#65292;&#20248;&#21270;&#32858;&#31867;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#23558;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#20004;&#20010;&#27493;&#39588;&#32479;&#19968;&#20026;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24341;&#23548;&#32593;&#32476;&#23398;&#20064;&#32858;&#31867;&#21451;&#22909;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;Dink-Net&#33021;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#22270;&#24418;&#19978;&#65292;&#22240;&#20026;&#35774;&#35745;&#30340;&#33192;&#32960;&#25910;&#32553;&#25805;&#20316;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Dink-Net&#22312;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#22270;&#24418;&#30340;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#22270;&#32858;&#31867;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.18404</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#26696;&#30830;&#35748;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24320;&#21457;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20581;&#22766;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#23558;&#25104;&#20026;&#23427;&#20204;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#19979;&#23433;&#20840;&#37096;&#32626;&#30340;&#20851;&#38190;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#31181;&#35266;&#23519;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#39044;&#27979;&#65292;&#21487;&#33021;&#20250;&#26377;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#23545;&#20110;&#36229;&#20986;&#20027;&#39064;&#30340;&#38382;&#39064;&#30340;&#20132;&#25442;&#24615;&#20551;&#35774;&#65292;&#36825;&#21487;&#33021;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#22330;&#26223;&#12290;&#26412;&#30740;&#31350;&#20026;&#22312;&#38656;&#35201;&#21487;&#38752;&#20445;&#35777;&#38169;&#35823;&#29575;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#26356;&#21152;&#20540;&#24471;&#20449;&#36182;&#21644;&#21487;&#38752;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18403</link><description>&lt;p&gt;
&#21098;&#26525;&#19982;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26469;&#20415;&#23452;&#22320;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#24040;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21046;&#32422;&#12290;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#36890;&#36807;&#21024;&#38500;&#20887;&#20313;&#21442;&#25968;&#26469;&#25552;&#20379;&#27169;&#22411;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LPM&#32780;&#35328;&#65292;&#33719;&#24471;&#26799;&#24230;&#26159;&#35745;&#31639;&#19978;&#31105;&#27490;&#30340;&#65292;&#36825;&#38656;&#35201;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LPM&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31216;&#20026;LoRAPrune&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#21033;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#20540;&#21644;&#26799;&#24230;&#65292;&#32780;&#19981;&#26159;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#22522;&#20110;&#21098;&#26525;&#20934;&#21017;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#12290;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38613;&#22609;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#21098;&#21644;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#24418;&#32467;&#26500;&#26469;&#25581;&#31034;&#20219;&#21153;&#30340;&#23376;&#20989;&#25968;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#24067;&#23572;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18402</link><description>&lt;p&gt;
&#31070;&#32463;&#38613;&#22609;&#65306;&#36890;&#36807;&#20462;&#21098;&#21644;&#32593;&#32476;&#20998;&#26512;&#25581;&#31034;&#20998;&#23618;&#27169;&#22359;&#21270;&#20219;&#21153;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis. (arXiv:2305.18402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38613;&#22609;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#21098;&#21644;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#24418;&#32467;&#26500;&#26469;&#25581;&#31034;&#20219;&#21153;&#30340;&#23376;&#20989;&#25968;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#24067;&#23572;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30446;&#26631;&#20989;&#25968;&#21644;&#20219;&#21153;&#36890;&#24120;&#34920;&#29616;&#20026;&#20998;&#23618;&#27169;&#22359;&#21270;&#65292;&#21487;&#20197;&#23558;&#20854;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#23376;&#20989;&#25968;&#20197;&#20998;&#23618;&#32452;&#32455;&#12290;&#36825;&#20123;&#23376;&#20989;&#25968;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#29305;&#24449;&#65306;&#23427;&#20204;&#26377;&#19968;&#32452;&#19981;&#21516;&#30340;&#36755;&#20837;&#65288;&#36755;&#20837;&#21487;&#20998;&#31163;&#24615;&#65289;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#23618;&#27425;&#20013;&#20316;&#20026;&#36755;&#20837;&#34987;&#37325;&#29992;&#65288;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#65289;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#31435;&#20102;&#20998;&#23618;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#23398;&#20064;&#25928;&#29575;&#12289;&#27867;&#21270;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#22914;&#20309;&#35782;&#21035;&#28508;&#22312;&#30340;&#23376;&#20989;&#25968;&#21450;&#20854;&#20998;&#23618;&#32467;&#26500;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38613;&#22609;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#21098;&#21644;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#20197;&#25581;&#31034;&#23376;&#20989;&#25968;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#24067;&#23572;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#31070;&#32463;&#38613;&#22609;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20219;&#21153;&#30340;&#28508;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26356;&#23481;&#26131;&#34987;&#20154;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#25193;&#23637;&#21040;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#30340;&#28508;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#25552;&#20379;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural target functions and tasks typically exhibit hierarchical modularity - they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (input-separability) and they are reused as inputs higher in the hierarchy (reusability). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transferability. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an ap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.18400</link><description>&lt;p&gt;
&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18400
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#65288;TFL&#65289;&#36890;&#24120;&#21033;&#29992;&#20445;&#25252;&#26426;&#21046;&#26469;&#20445;&#35777;&#38544;&#31169;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20445;&#25252;&#26426;&#21046;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24341;&#20837;&#25928;&#29992;&#25439;&#22833;&#25110;&#25928;&#29575;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#20445;&#25252;&#26426;&#21046;&#21450;&#20854;&#21442;&#25968;&#24212;&#35813;&#20180;&#32454;&#36873;&#25321;&#65292;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#20174;&#19994;&#32773;&#38656;&#35201;&#24037;&#20855;&#26469;&#34913;&#37327;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#24182;&#20248;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#25163;&#22836;&#24212;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20010;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;(1)&#23558;TFL&#23450;&#20041;&#20026;&#25214;&#21040;&#20445;&#25252;&#26426;&#21046;&#26469;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#19977;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#38382;&#39064;&#65307;(2)&#27491;&#24335;&#23450;&#20041;&#20102;&#36825;&#19977;&#20010;&#22240;&#32032;&#30340;&#26377;&#30028;&#27979;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#27492;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.18399</link><description>&lt;p&gt;
&#20851;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#35268;&#33539;&#21270;&#23545;&#21021;&#22987;&#21270;&#31561;&#36317;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010; Gram &#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#35813;&#30697;&#38453;&#21253;&#21547;&#19982;&#19968;&#25209;&#36755;&#20837;&#23545;&#24212;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#25104;&#23545;&#20869;&#31215;&#12290;&#22312;&#20960;&#31181;&#26550;&#26500;&#20013;&#65292;&#35266;&#23519;&#21040;&#22312;&#21021;&#22987;&#21270;&#26102;&#35813; Gram &#30697;&#38453;&#20250;&#38543;&#30528;&#28145;&#24230;&#21464;&#24471;&#36864;&#21270;&#65292;&#20174;&#32780;&#20005;&#37325;&#20943;&#32531;&#35757;&#32451;&#36895;&#24230;&#12290;&#35268;&#33539;&#21270;&#23618;&#22914;&#25209;&#22788;&#29702;&#35268;&#33539;&#21270;&#25110;&#23618;&#35268;&#33539;&#21270;&#65292;&#22312;&#38450;&#27490;&#31209;&#23849;&#28291;&#38382;&#39064;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#26080;&#27861;&#20840;&#38754;&#35206;&#30422;&#24191;&#27867;&#29992;&#20110; transformer &#20013;&#30340;&#23618;&#35268;&#33539;&#21270;&#21644;&#26377;&#38480;&#28145;&#24230;&#19979;&#35268;&#33539;&#21270;&#30340;&#37327;&#21270;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#32467;&#21512;&#28608;&#27963;&#20989;&#25968;&#23618;&#20351;&#29992;&#30340;&#23618;&#35268;&#33539;&#21270;&#21487;&#20197;&#20351;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340; Gram &#30697;&#38453;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#24182;&#20351;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340; Hermite &#23637;&#24320;&#26469;&#37327;&#21270;&#36825;&#20010;&#36895;&#24230;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22797;&#21046;&#19981;&#36866;&#24403;&#20154;&#31867;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25233;&#21046;&#29983;&#25104;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#27169;&#22411;&#23545;&#19990;&#30028;&#19985;&#38475;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.18398</link><description>&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#19981;&#24403;&#34892;&#20026;&#32531;&#35299;&#65306;&#21453;&#26144;&#19990;&#30028;&#19985;&#38475;&#26159;&#21542;&#26377;&#20215;&#20540;&#65311;
&lt;/p&gt;
&lt;p&gt;
Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?. (arXiv:2305.18398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22797;&#21046;&#19981;&#36866;&#24403;&#20154;&#31867;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25233;&#21046;&#29983;&#25104;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#27169;&#22411;&#23545;&#19990;&#30028;&#19985;&#38475;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#32593;&#32476;&#19978;&#38543;&#26426;&#25235;&#21462;&#30340;&#25968;&#21313;&#20159;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#36824;&#20250;&#22797;&#21046;&#19981;&#36866;&#24403;&#30340;&#20154;&#31867;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#19981;&#24403;&#36864;&#21270;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#37096;&#32626;&#26102;&#23545;&#20854;&#36827;&#34892;&#30417;&#35270;&#21644;&#35843;&#33410;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25512;&#29702;&#26102;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#25233;&#21046;&#19981;&#21512;&#36866;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27169;&#22411;&#23545;&#19990;&#30028;&#19985;&#38475;&#30340;&#34920;&#29616;&#26469;&#23558;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also reproduce inappropriate human behavior. Specifically, we demonstrate inappropriate degeneration on a large-scale for various generative text-to-image models, thus motivating the need for monitoring and moderating them at deployment. To this end, we evaluate mitigation strategies at inference to suppress the generation of inappropriate content. Our findings show that we can use models' representations of the world's ugliness to align them with human preferences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#20256;&#32479;&#27665;&#35843;&#25968;&#25454;&#39044;&#27979;2023&#24180;&#22303;&#32819;&#20854;&#24635;&#32479;&#36873;&#20030;&#32467;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#20132;&#20114;&#25968;&#37327;&#30340;ARIMAX&#27169;&#22411;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.18397</link><description>&lt;p&gt;
&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#39044;&#27979;2023&#24180;&#22303;&#32819;&#20854;&#24635;&#32479;&#36873;&#20030;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Prediction of the 2023 Turkish Presidential Election Results Using Social Media Data. (arXiv:2305.18397v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#20256;&#32479;&#27665;&#35843;&#25968;&#25454;&#39044;&#27979;2023&#24180;&#22303;&#32819;&#20854;&#24635;&#32479;&#36873;&#20030;&#32467;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#20132;&#20114;&#25968;&#37327;&#30340;ARIMAX&#27169;&#22411;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24433;&#21709;&#20102;&#25919;&#27835;&#31454;&#36873;&#30340;&#26041;&#24335;&#65292;&#22240;&#27492;&#23427;&#20204;&#24050;&#25104;&#20026;&#25919;&#27835;&#23478;&#30452;&#25509;&#19982;&#20844;&#27665;&#20114;&#21160;&#30340;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#24037;&#20855;&#12290;&#35768;&#22810;&#22269;&#23478;&#30340;&#21069;&#20960;&#27425;&#36873;&#20030;&#34920;&#26126;&#65292;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21487;&#33021;&#20250;&#23545;&#36873;&#20030;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#24179;&#21488;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19982;&#20256;&#32479;&#27665;&#35843;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#21442;&#21152;2023&#24180;&#22303;&#32819;&#20854;&#36873;&#20030;&#30340;&#25919;&#20826;&#30340;&#24471;&#31080;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#30340;&#26159;&#22522;&#20110;&#20307;&#31215;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#26469;&#32771;&#34385;&#31038;&#20132;&#23186;&#20307;&#20132;&#20114;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#20010;&#19981;&#21516;&#26102;&#38388;&#31383;&#21475;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#26102;&#38388;&#31383;&#21475;&#20013;&#65292;ARIMAX&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms influence the way political campaigns are run and therefore they have become an increasingly important tool for politicians to directly interact with citizens. Previous elections in various countries have shown that social media data may significantly impact election results. In this study, we aim to predict the vote shares of parties participating in the 2023 elections in Turkey by combining social media data from various platforms together with traditional polling data. Our approach is a volume-based approach that considers the number of social media interactions rather than content. We compare several prediction models across varying time windows. Our results show that for all time windows, the ARIMAX model outperforms the other algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.18396</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#29702;&#35299;&#21152;&#23494;&#25552;&#31034;&#65306;&#38754;&#21521;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;Transformers
&lt;/p&gt;
&lt;p&gt;
LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#26381;&#21153;&#22120;&#23458;&#25143;&#31471;&#29615;&#22659;&#20013;&#20026;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26500;&#24314;&#31169;&#26377;&#25512;&#26029;&#26694;&#26550;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#25345;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#23458;&#25143;&#31471;&#36755;&#20837;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#24403;&#31169;&#26377;&#36755;&#20837;&#36890;&#36807;&#21407;&#22987;LLMs&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#26102;&#65292;&#36825;&#20123;&#26694;&#26550;&#20250;&#20135;&#29983;&#26174;&#30528;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#65292;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#19982;&#26368;&#26032;&#30340;Iron&#65288;NeurIPS 2022&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#27169;&#22411;&#25512;&#26029;&#31649;&#36947;&#22312;&#35745;&#31639;&#19978;&#23454;&#29616;&#20102;$5 \times$&#30340;&#21152;&#36895;&#65292;&#22312;&#36890;&#20449;&#24320;&#38144;&#19978;&#23454;&#29616;&#20102;80\%&#30340;&#38477;&#20302;&#65292;&#21516;&#26102;&#20960;&#20046;&#20445;&#25345;&#20102;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation- and communication-heavy operators in the transformer architecture with privacy-computing friendly approximations can greatly reduce the private inference costs with minor impact on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80\% reduction in communication overhead, while retaining nearly identical accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18395</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311;&#65306;&#38754;&#21521;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#21512;&#30693;&#35782;&#29702;&#35299;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#39640;&#19988;&#28041;&#21450;&#25968;&#25454;&#38544;&#31169;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;&#26631;&#35760;&#25968;&#25454;&#25110;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#26159;&#30001;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#25152;&#38656;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311; (KARD) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#30340;&#20381;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#37325;&#25490;&#22120;&#65292;&#29992;&#20110;&#33719;&#24471;&#19982;&#20381;&#25454;&#29983;&#25104;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;KARD&#22312;&#19977;&#39033;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;LLMs&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.18394</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23618;&#23398;&#20064;&#30340;&#26368;&#20248;&#27491;&#21017;&#21270;&#21442;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27491;&#21017;&#21270;&#24120;&#29992;&#20110;&#35299;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#39640;&#20808;&#39564;&#20449;&#24687;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#21442;&#25968;&#21152;&#20197;&#26435;&#34913;&#65292;&#32780;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#20363;&#22914;&#24046;&#24322;&#21407;&#21017;&#21644;L-&#26354;&#32447;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#21512;&#36866;&#30340;&#21442;&#25968;&#20540;&#65292;&#20294;&#26159;&#36817;&#24180;&#26469;&#65292;&#19968;&#31181;&#21483;&#20570;&#21452;&#23618;&#23398;&#20064;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#29992;&#20110;&#30830;&#23450;&#26368;&#20248;&#21442;&#25968;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#31574;&#30053;&#26377;&#21508;&#31181;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21452;&#23618;&#23398;&#20064;&#30340;&#33391;&#22909;&#24615;&#36136;&#20173;&#28982;&#26159;&#19968;&#20010;&#21457;&#23637;&#20013;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#26465;&#20214;&#26469;&#34920;&#24449;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#27491;&#20540;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21457;&#38899;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#35821;&#38899;&#28165;&#26224;&#24230;&#65292;&#20854;&#20013;&#65292;&#21033;&#29992;&#20808;&#39564;&#35268;&#33539;&#21270;&#30340;&#26368;&#22823;&#23545;&#25968;&#20960;&#29575;&#21457;&#38899;&#20934;&#30830;&#24230;&#65288;maxlogit GoP&#65289;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18392</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21457;&#38899;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#23545;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#35821;&#38899;&#28165;&#26224;&#24230;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification. (arXiv:2305.18392v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21457;&#38899;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#35821;&#38899;&#28165;&#26224;&#24230;&#65292;&#20854;&#20013;&#65292;&#21033;&#29992;&#20808;&#39564;&#35268;&#33539;&#21270;&#30340;&#26368;&#22823;&#23545;&#25968;&#20960;&#29575;&#21457;&#38899;&#20934;&#30830;&#24230;&#65288;maxlogit GoP&#65289;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21457;&#38899;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26469;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#35821;&#38899;&#28165;&#26224;&#24230;&#12290;&#24403;&#21069;&#30340;&#21457;&#38899;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#33258;&#36127;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#35821;&#38899;&#26469;&#35828;&#19981;&#22826;&#36866;&#29992;&#65292;&#22240;&#20026;&#20854;&#19982;&#20581;&#24247;&#20154;&#30340;&#35821;&#38899;&#23384;&#22312;&#26174;&#33879;&#30340;&#22768;&#23398;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an improved Goodness of Pronunciation (GoP) that utilizes Uncertainty Quantification (UQ) for automatic speech intelligibility assessment for dysarthric speech. Current GoP methods rely heavily on neural network-driven overconfident predictions, which is unsuitable for assessing dysarthric speech due to its significant acoustic differences from healthy speech. To alleviate the problem, UQ techniques were used on GoP by 1) normalizing the phoneme prediction (entropy, margin, maxlogit, logit-margin) and 2) modifying the scoring function (scaling, prior normalization). As a result, prior-normalized maxlogit GoP achieves the best performance, with a relative increase of 5.66%, 3.91%, and 23.65% compared to the baseline GoP for English, Korean, and Tamil, respectively. Furthermore, phoneme analysis is conducted to identify which phoneme scores significantly correlate with intelligibility scores in each language.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22330;&#26223;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21270;&#34920;&#36798;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#31867;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#27604;&#20351;&#29992;&#23398;&#20064;&#34920;&#36798;&#24335;&#30340;&#27169;&#22411;&#26377;&#25152;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.18391</link><description>&lt;p&gt;
MemeGraphs: &#23558;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#36830;
&lt;/p&gt;
&lt;p&gt;
MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18391
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22330;&#26223;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21270;&#34920;&#36798;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#31867;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#27604;&#20351;&#29992;&#23398;&#20064;&#34920;&#36798;&#24335;&#30340;&#27169;&#22411;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#26159;&#19968;&#31181;&#22312;&#31038;&#20132;&#23186;&#20307;&#21644;&#20114;&#32852;&#32593;&#19978;&#27969;&#34892;&#30340;&#20256;&#25773;&#36235;&#21183;&#21644;&#35266;&#28857;&#30340;&#24418;&#24335;&#65292;&#32467;&#21512;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#27169;&#24335;&#12290;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#24189;&#40664;&#21644;&#35773;&#21050;&#65292;&#20294;&#20063;&#21487;&#33021;&#21547;&#26377;&#20882;&#29359;&#24615;&#30340;&#20869;&#23481;&#12290;&#33258;&#21160;&#20998;&#26512;&#21644;&#20998;&#31867;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20854;&#35299;&#37322;&#20381;&#36182;&#20110;&#23545;&#35270;&#35273;&#20803;&#32032;&#12289;&#35821;&#35328;&#21644;&#32972;&#26223;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#26377;&#24847;&#20041;&#22320;&#34920;&#31034;&#36825;&#20123;&#26469;&#28304;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#20415;&#23558;&#34920;&#24773;&#21253;&#20316;&#20026;&#25972;&#20307;&#20998;&#31867;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22330;&#26223;&#22270;&#20316;&#20026;&#34920;&#31034;&#22270;&#20687;&#20013;&#29289;&#20307;&#21450;&#20854;&#35270;&#35273;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#36798;&#26041;&#24335;&#65292;&#24182;&#23558;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#20998;&#31867;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;ImgBERT&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#20351;&#29992;&#20165;&#23398;&#20064;&#65288;&#32780;&#19981;&#26159;&#32467;&#26500;&#21270;&#65289;&#30340;&#34920;&#36798;&#24335;&#36827;&#34892;&#22810;&#27169;&#24335;&#24314;&#27169;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22987;&#32456;&#26377;&#25152;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#20154;&#24037;&#22270;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20379;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are a popular form of communicating trends and ideas in social media and on the internet in general, combining the modalities of images and text. They can express humor and sarcasm but can also have offensive content. Analyzing and classifying memes automatically is challenging since their interpretation relies on the understanding of visual elements, language, and background knowledge. Thus, it is important to meaningfully represent these sources and the interaction between them in order to classify a meme as a whole. In this work, we propose to use scene graphs, that express images in terms of objects and their visual relations, and knowledge graphs as structured representations for meme classification with a Transformer-based architecture. We compare our approach with ImgBERT, a multimodal model that uses only learned (instead of structured) representations of the meme, and observe consistent improvements. We further provide a dataset with human graph annotations that we compa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;&#29616;&#35937;&#65292;&#21457;&#29616;&#31070;&#32463;&#20803;&#21487;&#20197;&#36827;&#34892;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24314;&#31435;&#36215;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27492;&#32467;&#26500;&#21487;&#34987;&#26377;&#25928;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.18390</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;
&lt;/p&gt;
&lt;p&gt;
Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;&#29616;&#35937;&#65292;&#21457;&#29616;&#31070;&#32463;&#20803;&#21487;&#20197;&#36827;&#34892;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24314;&#31435;&#36215;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27492;&#32467;&#26500;&#21487;&#34987;&#26377;&#25928;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#36825;&#26159;&#20154;&#33041;&#20013;&#24120;&#35265;&#30340;&#29305;&#28857;&#65292;&#34987;&#35748;&#20026;&#23545;&#20110;&#26222;&#36941;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20027;&#35201;&#32771;&#34385;&#20102;&#27169;&#22359;&#21270;&#30340;&#20004;&#20010;&#20027;&#35201;&#29305;&#24449;&#65306;&#65288;1&#65289;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#19987;&#19994;&#21270;&#65306;&#25105;&#20204;&#35780;&#20272;&#20102;&#27599;&#20010;&#31070;&#32463;&#20803;&#26159;&#21542;&#20027;&#35201;&#19987;&#19994;&#21270;&#20110;&#26576;&#19968;&#21151;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#26159;&#30340;&#12290;&#65288;2&#65289;&#22522;&#20110;&#21151;&#33021;&#32858;&#31867;&#30340;&#31070;&#32463;&#20803;&#20998;&#32452;&#65306;&#25105;&#20204;&#25506;&#31350;&#20102;&#23558;&#31070;&#32463;&#20803;&#25353;&#21151;&#33021;&#20998;&#32452;&#30340;&#32467;&#26500;&#23547;&#25214;&#26041;&#27861;&#65292;&#27599;&#20010;&#27169;&#22359;&#22343;&#20026;&#20854;&#30456;&#24212;&#21151;&#33021;&#24037;&#20316;&#12290;&#37492;&#20110;&#21487;&#33021;&#23384;&#22312;&#30340;&#22823;&#37327;&#32467;&#26500;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#20998;&#23618;&#19987;&#23478;&#27169;&#22411;&#36523;&#19978;&#65292;&#24182;&#23558;&#31070;&#32463;&#20803;&#21010;&#20998;&#20026;&#19987;&#23478;&#65292;&#36890;&#24120;&#20026;&#19981;&#21516;&#30340;&#36755;&#20837;&#28608;&#27963;&#19981;&#21516;&#30340;&#19987;&#23478;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21151;&#33021;&#19987;&#23478;&#65292;&#32858;&#38598;&#20102;&#26576;&#19968;&#21151;&#33021;&#30340;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#25200;&#21160;&#21151;&#33021;&#19987;&#23478;&#30340;&#28608;&#27963;&#26174;&#33879;&#24433;&#21709;&#20102;&#30456;&#24212;&#30340;f&#38190;
&lt;/p&gt;
&lt;p&gt;
This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnoRand&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#26550;&#26500;&#21644;&#38543;&#26426;&#21512;&#25104;&#26631;&#31614;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#23613;&#21487;&#33021;&#22320;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.18389</link><description>&lt;p&gt;
AnoRand: &#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
AnoRand: A Semi Supervised Deep Learning Anomaly Detection Method by Random Labeling. (arXiv:2305.18389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnoRand&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#26550;&#26500;&#21644;&#38543;&#26426;&#21512;&#25104;&#26631;&#31614;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#23613;&#21487;&#33021;&#22320;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#25110;&#32773;&#26356;&#19968;&#33324;&#30340;&#31163;&#32676;&#20540;&#26816;&#27979;&#26159;&#29702;&#35770;&#21644;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#20043;&#19968;&#12290;&#20854;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#36890;&#24120;&#35775;&#38382;&#38750;&#24120;&#23569;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#25110;&#26681;&#26412;&#27809;&#26377;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#19982;&#38543;&#26426;&#21512;&#25104;&#26631;&#31614;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861; \textbf{AnoRand}&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20855;&#26377;&#20004;&#20010;&#26500;&#24314;&#22359;&#65306;(1) &#30001;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;(feed forward ferceptron)&#32452;&#25104;&#30340;&#22122;&#22768;&#26816;&#27979;(ND)&#22359;&#21644;(2) &#33258;&#32534;&#30721;&#22120;(AE)&#22359;&#12290;&#36825;&#31181;&#26032;&#26550;&#26500;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#22312;&#28508;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22312;&#25968;&#25454;&#39640;&#24230;&#19981;&#24179;&#34913;&#26102;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#30340;&#33021;&#21147;&#26469;&#23613;&#21487;&#33021;&#22320;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035; (&#20363;&#22914;&#65292;&#22312;&#24322;&#24120;&#26816;&#27979;&#30340;&#24773;&#20917;&#19979;&#26159;&#22823;&#22810;&#25968;&#31867;&#21035;)&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#25200;&#21160; (&#28155;&#21152;&#22122;&#22768;) &#23569;&#37327;&#26679;&#26412; (&#20363;&#22914;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection or more generally outliers detection is one of the most popular and challenging subject in theoretical and applied machine learning. The main challenge is that in general we have access to very few labeled data or no labels at all. In this paper, we present a new semi-supervised anomaly detection method called \textbf{AnoRand} by combining a deep learning architecture with random synthetic label generation. The proposed architecture has two building blocks: (1) a noise detection (ND) block composed of feed forward ferceptron and (2) an autoencoder (AE) block. The main idea of this new architecture is to learn one class (e.g. the majority class in case of anomaly detection) as well as possible by taking advantage of the ability of auto encoders to represent data in a latent space and the ability of Feed Forward Perceptron (FFP) to learn one class when the data is highly imbalanced. First, we create synthetic anomalies by randomly disturbing (add noise) few samples (e.g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#24046;&#20998;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#31639;&#27861;&#22312;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20174;&#23454;&#36341;&#32773;&#27809;&#26377;&#36229;&#36807;&#24179;&#22343;&#22238;&#25253;&#20043;&#22806;&#30340;&#22238;&#25253;&#20998;&#24067;&#30340;&#20852;&#36259;&#20043;&#22788;&#65292;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;QTD&#20063;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;TD&#23398;&#20064;&#31561;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18388</link><description>&lt;p&gt;
&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#20215;&#20540;&#20272;&#35745;&#20013;&#30340;&#32479;&#35745;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation. (arXiv:2305.18388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#24046;&#20998;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#31639;&#27861;&#22312;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20174;&#23454;&#36341;&#32773;&#27809;&#26377;&#36229;&#36807;&#24179;&#22343;&#22238;&#25253;&#20043;&#22806;&#30340;&#22238;&#25253;&#20998;&#24067;&#30340;&#20852;&#36259;&#20043;&#22788;&#65292;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;QTD&#20063;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;TD&#23398;&#20064;&#31561;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#26102;&#38388;&#24046;&#20998;&#30340;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#20998;&#26512;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65288;QTD&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#32467;&#35770;&#65306;&#21363;&#20351;&#20174;&#23454;&#36341;&#32773;&#27809;&#26377;&#36229;&#36807;&#24179;&#22343;&#22238;&#25253;&#20043;&#22806;&#30340;&#22238;&#25253;&#20998;&#24067;&#30340;&#20852;&#36259;&#20043;&#22788;&#65292;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;QTD&#65288;&#23398;&#20064;&#20851;&#20110;&#20840;&#37096;&#22238;&#25253;&#20998;&#24067;&#30340;&#39044;&#27979;&#65289;&#20063;&#21487;&#20197;&#25552;&#20379;&#27604;&#35832;&#22914;&#20256;&#32479;TD&#23398;&#20064;&#65288;&#20165;&#39044;&#27979;&#24179;&#22343;&#22238;&#25253;&#65289;&#31561;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of temporal-difference-based policy evaluation in reinforcement learning. In particular, we analyse the use of a distributional reinforcement learning algorithm, quantile temporal-difference learning (QTD), for this task. We reach the surprising conclusion that even if a practitioner has no interest in the return distribution beyond the mean, QTD (which learns predictions about the full distribution of returns) may offer performance superior to approaches such as classical TD learning, which predict only the mean return, even in the tabular setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#22686;&#24378;&#35282;&#33394;&#35774;&#35745;&#24072;&#30340;&#21019;&#36896;&#21147;&#65292;&#22312;&#22810;&#23186;&#20307;&#39033;&#30446;&#20013;&#26500;&#24605;&#26032;&#30340;&#35282;&#33394;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;GAN&#21487;&#20197;&#25552;&#20379;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#36873;&#39033;&#65292;&#21516;&#26102;&#20943;&#23569;&#35774;&#35745;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.18387</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22686;&#24378;&#35282;&#33394;&#35774;&#35745;&#24072;&#30340;&#21019;&#36896;&#21147;
&lt;/p&gt;
&lt;p&gt;
Augmenting Character Designers Creativity Using Generative Adversarial Networks. (arXiv:2305.18387v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#22686;&#24378;&#35282;&#33394;&#35774;&#35745;&#24072;&#30340;&#21019;&#36896;&#21147;&#65292;&#22312;&#22810;&#23186;&#20307;&#39033;&#30446;&#20013;&#26500;&#24605;&#26032;&#30340;&#35282;&#33394;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;GAN&#21487;&#20197;&#25552;&#20379;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#36873;&#39033;&#65292;&#21516;&#26102;&#20943;&#23569;&#35774;&#35745;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#30001;&#20110;&#20854;&#20851;&#38190;&#29305;&#24449;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21560;&#24341;&#20102;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;GAN&#19987;&#27880;&#20110;&#29616;&#23454;&#20027;&#20041;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#39046;&#22495;&#65292;&#22914;&#26412;&#39033;&#24037;&#20316;&#65292;&#29983;&#25104;&#36229;&#29616;&#23454;&#30340;&#36755;&#20986;&#24182;&#19981;&#26159;&#39318;&#35201;&#20219;&#21153;&#12290;&#22312;&#27492;&#65292;&#29983;&#25104;&#30340;&#32467;&#26524;&#34987;&#29992;&#20316;&#35748;&#30693;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#22686;&#24378;&#35282;&#33394;&#35774;&#35745;&#24072;&#30340;&#21019;&#36896;&#21147;&#65292;&#20026;&#19981;&#21516;&#30340;&#22810;&#23186;&#20307;&#39033;&#30446;&#26500;&#24605;&#26032;&#30340;&#35282;&#33394;&#12290;&#20026;&#20102;&#36873;&#25321;&#26368;&#36866;&#21512;&#36825;&#31181;&#21019;&#24847;&#29615;&#22659;&#30340;GAN&#65292;&#25105;&#20204;&#39318;&#20808;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;GAN&#26550;&#26500;&#20197;&#21450;&#23427;&#20204;&#22312;&#20351;&#29992;&#21333;&#20010;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#23545;&#26032;&#30340;&#35270;&#35273;&#35282;&#33394;&#25968;&#25454;&#38598;&#36827;&#34892;&#20174;&#22836;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#26367;&#20195;&#25216;&#26415;&#65292;&#22914;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#20811;&#26381;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65292;&#36825;&#26159;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#25163;&#24037;&#33609;&#22270;&#19982;GAN&#29983;&#25104;&#30340;&#36755;&#20986;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#21019;&#24847;&#25506;&#32034;&#65292;&#20805;&#20998;&#21033;&#29992;&#20154;&#31867;&#35774;&#35745;&#24072;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#20248;&#21183;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;GAN&#30830;&#23454;&#21487;&#20197;&#22686;&#24378;&#35282;&#33394;&#35774;&#35745;&#24072;&#30340;&#21019;&#36896;&#21147;&#65292;&#25552;&#20379;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#36873;&#39033;&#65292;&#21516;&#26102;&#20943;&#23569;&#35774;&#35745;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Generative Adversarial Networks (GANs) continue to attract the attention of researchers in different fields due to the wide range of applications devised to take advantage of their key features. Most recent GANs are focused on realism, however, generating hyper-realistic output is not a priority for some domains, as in the case of this work. The generated outcomes are used here as cognitive components to augment character designers creativity while conceptualizing new characters for different multimedia projects. To select the best-suited GANs for such a creative context, we first present a comparison between different GAN architectures and their performance when trained from scratch on a new visual characters dataset using a single Graphics Processing Unit. We also explore alternative techniques, such as transfer learning and data augmentation, to overcome computational resource limitations, a challenge faced by many researchers in the domain. Additionally, mixed me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21327;&#21516;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;CFD&#32467;&#26524;&#65292;&#20197;&#20943;&#23569;&#27668;&#21160;&#39044;&#27979;&#20013;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#28145;&#36828;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18386</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21327;&#21516;&#26694;&#26550;&#21512;&#25104;&#26426;&#32764;&#31354;&#27668;&#21160;&#21147;&#23398;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
A Synergistic Framework Leveraging Autoencoders and Generative Adversarial Networks for the Synthesis of Computational Fluid Dynamics Results in Aerofoil Aerodynamics. (arXiv:2305.18386v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21327;&#21516;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;CFD&#32467;&#26524;&#65292;&#20197;&#20943;&#23569;&#27668;&#21160;&#39044;&#27979;&#20013;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#28145;&#36828;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#39046;&#22495;&#65292;&#20934;&#30830;&#39044;&#27979;&#27668;&#21160;&#34892;&#20026;&#22312;&#26426;&#32764;&#35774;&#35745;&#21644;&#20248;&#21270;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21327;&#21516;&#22320;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;CFD&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26694;&#26550;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#20869;&#22312;&#33021;&#21147;&#23558;&#26426;&#32764;&#20960;&#20309;&#24418;&#29366;&#32534;&#30721;&#25104;&#19968;&#20010;&#20855;&#26377;&#21387;&#32553;&#24615;&#21644;&#20449;&#24687;&#20215;&#20540;&#30340;20&#38271;&#24230;&#21521;&#37327;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#26377;&#26465;&#20214;&#30340;GAN&#32593;&#32476;&#23558;&#20854;&#32763;&#35793;&#25104;&#31934;&#30830;&#30340;&#21387;&#21147;&#20998;&#24067;&#22270;&#65292;&#32771;&#34385;&#21040;&#22266;&#23450;&#30340;&#39118;&#36895;&#12289;&#25915;&#35282;&#21644;&#28237;&#27969;&#27700;&#24179;&#35268;&#33539;&#12290;&#35757;&#32451;&#36807;&#31243;&#21033;&#29992;&#20174;JavaFoil&#36719;&#20214;&#33719;&#24471;&#30340;&#32454;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#26426;&#32764;&#20960;&#20309;&#24418;&#29366;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#27668;&#21160;&#39044;&#27979;&#20013;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#28145;&#36828;&#30340;&#28508;&#21147;&#65292;&#20351;&#24471;&#35780;&#20272;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of computational fluid dynamics (CFD), accurate prediction of aerodynamic behaviour plays a pivotal role in aerofoil design and optimization. This study proposes a novel approach that synergistically combines autoencoders and Generative Adversarial Networks (GANs) for the purpose of generating CFD results. Our innovative framework harnesses the intrinsic capabilities of autoencoders to encode aerofoil geometries into a compressed and informative 20-length vector representation. Subsequently, a conditional GAN network adeptly translates this vector into precise pressure-distribution plots, accounting for fixed wind velocity, angle of attack, and turbulence level specifications. The training process utilizes a meticulously curated dataset acquired from JavaFoil software, encompassing a comprehensive range of aerofoil geometries. The proposed approach exhibits profound potential in reducing the time and costs associated with aerodynamic prediction, enabling efficient evaluati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#24322;&#36136;&#24615;&#22270;&#21644;&#21516;&#36136;&#24615;&#22270;&#65292;&#24182;&#22312;&#35768;&#22810;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18385</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#21452;&#37325;&#23884;&#20837;&#65306;&#36866;&#29992;&#20110;&#24322;&#36136;&#24615;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Self-attention Dual Embedding for Graphs with Heterophily. (arXiv:2305.18385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#24322;&#36136;&#24615;&#22270;&#21644;&#21516;&#36136;&#24615;&#22270;&#65292;&#24182;&#22312;&#35768;&#22810;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#12290;GNNs&#36890;&#24120;&#20551;&#35774;&#22270;&#26159;&#21516;&#36136;&#30340;&#65292;&#21363;&#30456;&#37051;&#33410;&#28857;&#24456;&#21487;&#33021;&#23646;&#20110;&#30456;&#21516;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#37117;&#26159;&#24322;&#36136;&#30340;&#65292;&#36825;&#23548;&#33268;&#20351;&#29992;&#26631;&#20934;&#30340;GNNs&#26102;&#20998;&#31867;&#31934;&#24230;&#35201;&#20302;&#24471;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#65292;&#23427;&#23545;&#24322;&#36136;&#24615;&#21644;&#21516;&#36136;&#24615;&#22270;&#37117;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#19977;&#20010;&#20027;&#35201;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#20013;&#65292;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#25299;&#25169;&#25552;&#20379;&#19981;&#21516;&#25968;&#37327;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#24212;&#35813;&#29420;&#31435;&#32534;&#30721;&#24182;&#20197;&#33258;&#36866;&#24212;&#26041;&#24335;&#20248;&#20808;&#32423;&#21270;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20256;&#25773;&#22270;&#25299;&#25169;&#20449;&#24687;&#26102;&#20801;&#35768;&#36127;&#30340;&#27880;&#24847;&#26435;&#37325;&#21487;&#20197;&#25552;&#39640;&#31934;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33410;&#28857;&#20043;&#38388;&#19981;&#23545;&#31216;&#30340;&#27880;&#24847;&#26435;&#37325;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;GNN&#65292;&#21033;&#29992;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20123;&#26631;&#20934;&#30340;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been highly successful for the node classification task. GNNs typically assume graphs are homophilic, i.e. neighboring nodes are likely to belong to the same class. However, a number of real-world graphs are heterophilic, and this leads to much lower classification accuracy using standard GNNs. In this work, we design a novel GNN which is effective for both heterophilic and homophilic graphs. Our work is based on three main observations. First, we show that node features and graph topology provide different amounts of informativeness in different graphs, and therefore they should be encoded independently and prioritized in an adaptive manner. Second, we show that allowing negative attention weights when propagating graph topology information improves accuracy. Finally, we show that asymmetric attention weights between nodes are helpful. We design a GNN which makes use of these observations through a novel self-attention mechanism. We evaluate our algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#37327;&#23398;&#20064;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#21487;&#33021;&#23384;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#23454;&#35777;&#25581;&#31034;&#20102;11&#20010;&#20856;&#22411;&#30340;&#22686;&#37327;&#23398;&#20064;&#22120;&#23545;&#22522;&#20110;&#20013;&#27602;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#24230;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18384</link><description>&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#65306;&#19968;&#39033;&#23454;&#35777;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study. (arXiv:2305.18384v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#37327;&#23398;&#20064;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#21487;&#33021;&#23384;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#23454;&#35777;&#25581;&#31034;&#20102;11&#20010;&#20856;&#22411;&#30340;&#22686;&#37327;&#23398;&#20064;&#22120;&#23545;&#22522;&#20110;&#20013;&#27602;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#24230;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#32531;&#35299;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#22788;&#29702;&#39034;&#24207;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22686;&#37327;&#23398;&#20064;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#65292;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#22522;&#20110;&#20013;&#27602;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#25105;&#20204;&#35748;&#20026;&#22686;&#37327;&#23398;&#20064;&#20013;&#27969;&#24335;&#25968;&#25454;&#30340;&#26412;&#36136;&#20026;&#23545;&#25163;&#25552;&#20379;&#20102;&#24456;&#22823;&#30340;&#20415;&#21033;&#65292;&#36890;&#36807;&#25968;&#25454;&#27745;&#26579;&#65292;&#23545;&#25163;&#21487;&#20197;&#22312;&#20219;&#20309;&#26102;&#38388;&#25110;&#26102;&#38388;&#24207;&#21015;&#19978;&#21019;&#24314;&#20998;&#24067;&#24335;&#21644;&#36328;&#20219;&#21153;&#25915;&#20987;&#65292;&#20174;&#32780;&#24433;&#21709;&#20219;&#20309;&#26410;&#30693;&#30340;&#20808;&#21069;&#25110;&#21518;&#32493;&#20219;&#21153;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#27880;&#20837;&#26497;&#23567;&#25968;&#37327;&#30340;&#21518;&#38376;&#26679;&#26412; (&#20363;&#22914;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#20165;&#38656;&#35201;&#27880;&#20837;0.1%&#65289;&#12290;&#20026;&#20102;&#21560;&#24341;&#30740;&#31350;&#31038;&#21306;&#30340;&#20851;&#27880;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#23398;&#20064;&#22330;&#26223;&#20986;&#21457;&#65292;&#23454;&#35777;&#25581;&#31034;&#20102;11&#20010;&#20856;&#22411;&#30340;&#22686;&#37327;&#23398;&#20064;&#22120;&#23545;&#22522;&#20110;&#20013;&#27602;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#24230;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#36328;&#20219;&#21153;&#27867;&#21270;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large amounts of incremental learning algorithms have been proposed to alleviate the catastrophic forgetting issue arises while dealing with sequential data on a time series. However, the adversarial robustness of incremental learners has not been widely verified, leaving potential security risks. Specifically, for poisoning-based backdoor attacks, we argue that the nature of streaming data in IL provides great convenience to the adversary by creating the possibility of distributed and cross-task attacks -- an adversary can affect \textbf{any unknown} previous or subsequent task by data poisoning \textbf{at any time or time series} with extremely small amount of backdoor samples injected (e.g., $0.1\%$ based on our observations). To attract the attention of the research community, in this paper, we empirically reveal the high vulnerability of 11 typical incremental learners against poisoning-based backdoor attack on 3 learning scenarios, especially the cross-task generalization effect 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#32479;&#35745;&#21147;&#23398;&#30340;&#29616;&#35937;&#23398;&#27169;&#22411;&#65292;&#20351;&#29992;&#31867;&#20284;&#28201;&#24230;&#21644;&#36127;&#36733;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#24314;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36229;&#21442;&#25968;&#23545;&#20462;&#21098;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#20998;&#31867;&#20462;&#21098;&#21518;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#30340;&#20840;&#23616;&#32467;&#26500;&#26500;&#24314;&#20102;&#19968;&#20010;&#19977;&#37325;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20462;&#21098;&#30340;&#20248;&#21270;&#36807;&#31243;&#20197;&#21450;&#23545;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#30340;&#21464;&#21270;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2305.18383</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#30340;&#19977;&#37325;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Three-regime Model of Network Pruning. (arXiv:2305.18383v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18383
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#32479;&#35745;&#21147;&#23398;&#30340;&#29616;&#35937;&#23398;&#27169;&#22411;&#65292;&#20351;&#29992;&#31867;&#20284;&#28201;&#24230;&#21644;&#36127;&#36733;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#24314;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36229;&#21442;&#25968;&#23545;&#20462;&#21098;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#20998;&#31867;&#20462;&#21098;&#21518;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#30340;&#20840;&#23616;&#32467;&#26500;&#26500;&#24314;&#20102;&#19968;&#20010;&#19977;&#37325;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20462;&#21098;&#30340;&#20248;&#21270;&#36807;&#31243;&#20197;&#21450;&#23545;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#30340;&#21464;&#21270;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#35757;&#32451;&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;&#35757;&#32451;&#36718;&#25968;&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20462;&#21098;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#22914;&#20309;&#31934;&#30830;&#39044;&#27979;&#35843;&#25972;&#26576;&#19968;&#29305;&#23450;&#36229;&#21442;&#25968;&#23545;&#20462;&#21098;&#30340;&#24433;&#21709;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#32479;&#35745;&#21147;&#23398;&#30340;&#29616;&#35937;&#23398;&#27169;&#22411;&#65292;&#20351;&#29992;&#31867;&#20284;&#28201;&#24230;&#21644;&#36127;&#36733;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#24314;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36229;&#21442;&#25968;&#23545;&#20462;&#21098;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#23454;&#35777;&#32467;&#26524;&#65306;&#26681;&#25454;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#36127;&#36733;&#31867;&#21442;&#25968;&#30340;&#20540;&#65292;&#24403;&#22686;&#21152;&#20462;&#21098;&#21069;&#27169;&#22411;&#20013;&#19968;&#31181;&#31867;&#20284;&#28201;&#24230;&#30340;&#21442;&#25968;&#30340;&#20540;&#26102;&#65292;&#20462;&#21098;&#24615;&#33021;&#21487;&#33021;&#20250;&#24471;&#21040;&#20248;&#21270;&#25110;&#25439;&#23475;&#12290;&#22522;&#20110;&#36825;&#31181;&#36716;&#21464;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#31867;&#20462;&#21098;&#21518;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#30340;&#20840;&#23616;&#32467;&#26500;&#26500;&#24314;&#20102;&#19968;&#20010;&#19977;&#37325;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#25581;&#31034;&#20102;&#20462;&#21098;&#30340;&#20248;&#21270;&#36807;&#31243;&#20197;&#21450;&#19982;&#20462;&#21098;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#30340;&#21464;&#21270;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has highlighted the complex influence training hyperparameters, e.g., the number of training epochs, can have on the prunability of machine learning models. Perhaps surprisingly, a systematic approach to predict precisely how adjusting a specific hyperparameter will affect prunability remains elusive. To address this gap, we introduce a phenomenological model grounded in the statistical mechanics of learning. Our approach uses temperature-like and load-like parameters to model the impact of neural network (NN) training hyperparameters on pruning performance. A key empirical result we identify is a sharp transition phenomenon: depending on the value of a load-like parameter in the pruned model, increasing the value of a temperature-like parameter in the pre-pruned model may either enhance or impair subsequent pruning performance. Based on this transition, we build a three-regime model by taxonomizing the global structure of the pruned NN loss landscape. Our model reveals tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24230;&#32423;&#21035;&#30340;&#20462;&#21098;&#8221;(PALS), &#36890;&#36807;&#31232;&#30095;&#35757;&#32451;&#21644;&#35757;&#32451;&#26399;&#38388;&#26041;&#27861;&#20013;&#30340;&#8220;&#25193;&#24352;&#8221;&#26426;&#21046;&#65292;&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.18382</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#31232;&#30095;&#24230;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21464;&#21270;&#65292;&#29992;&#20110;&#21033;&#29992;Transformer&#36827;&#34892;&#39640;&#25928;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers. (arXiv:2305.18382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24230;&#32423;&#21035;&#30340;&#20462;&#21098;&#8221;(PALS), &#36890;&#36807;&#31232;&#30095;&#35757;&#32451;&#21644;&#35757;&#32451;&#26399;&#38388;&#26041;&#27861;&#20013;&#30340;&#8220;&#25193;&#24352;&#8221;&#26426;&#21046;&#65292;&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#31232;&#30095;&#36830;&#25509;&#21644;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#65292;&#21487;&#20197;&#23454;&#29616;DNN&#30340;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#30830;&#23450;&#31232;&#30095;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#25439;&#22833;&#31232;&#30095;&#24230;&#26435;&#34913;&#26159;&#24322;&#26500;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24230;&#32423;&#21035;&#30340;&#20462;&#21098;&#8221;(PALS)&#65292;&#26469;&#33258;&#21160;&#23547;&#27714;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26368;&#20339;&#24179;&#34913;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#31232;&#30095;&#27700;&#24179;&#12290;PALS&#20174;&#31232;&#30095;&#35757;&#32451;&#21644;&#35757;&#32451;&#26399;&#38388;&#26041;&#27861;&#20013;&#21560;&#21462;&#28789;&#24863;&#12290;&#23427;&#22312;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#8220;&#25193;&#24352;&#8221;&#26426;&#21046;&#65292;&#20801;&#35768;&#27169;&#22411;&#21160;&#24577;&#25910;&#32553;&#12289;&#25193;&#24352;&#25110;&#20445;&#25345;&#31283;&#23450;&#65292;&#20197;&#25214;&#21040;&#36866;&#24403;&#30340;&#31232;&#30095;&#24230;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#22312;&#20197;Transformer&#33879;&#31216;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#25928;&#29575;, &#35813;&#27169;&#22411;&#20197;&#20854;&#20986;&#33394;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#34920;&#29616;&#32780;&#38395;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient time series forecasting has become critical for real-world applications, particularly with deep neural networks (DNNs). Efficiency in DNNs can be achieved through sparse connectivity and reducing the model size. However, finding the sparsity level automatically during training remains a challenging task due to the heterogeneity in the loss-sparsity tradeoffs across the datasets. In this paper, we propose \enquote{\textbf{P}runing with \textbf{A}daptive \textbf{S}parsity \textbf{L}evel} (\textbf{PALS}), to automatically seek an optimal balance between loss and sparsity, all without the need for a predefined sparsity level. PALS draws inspiration from both sparse training and during-training methods. It introduces the novel "expand" mechanism in training sparse neural networks, allowing the model to dynamically shrink, expand, or remain stable to find a proper sparsity level. In this paper, we focus on achieving efficiency in transformers known for their excellent time series f
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#30719;&#30707;&#20013;&#25552;&#28860;&#40644;&#37329;: &#22522;&#20110;&#20851;&#38190;&#26679;&#26412;&#36873;&#25321;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#22810;&#27169;&#22411;&#30340;&#29616;&#22312;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#20173;&#28982;&#38750;&#24120;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#29992;&#20449;&#24687;&#29702;&#35770;&#26469;&#24314;&#27169;&#33976;&#39311;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#31181;&#26032;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#25193;&#22823;&#29616;&#26377;&#31639;&#27861;&#33539;&#22260;&#20197;&#23545;&#26356;&#24222;&#22823;&#21644;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;0.04&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#20445;&#25345;&#21487;&#27604;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20854;&#36129;&#29486;&#21487;&#33021;&#20026;&#33976;&#39311;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#22609;&#24418;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#22810;&#20010;&#20195;&#29702;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18380</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#21147;&#30340;&#20449;&#29992;&#20998;&#37197;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Potential-based Credit Assignment for Cooperative RL-based Testing of Autonomous Vehicles. (arXiv:2305.18380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#22609;&#24418;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#22810;&#20010;&#20195;&#29702;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;(AVs)&#22312;&#26222;&#36890;&#23454;&#38469;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#19968;&#20123;&#24847;&#22806;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#19981;&#29702;&#24615;&#34892;&#20026;&#24341;&#21457;&#20005;&#37325;&#23433;&#20840;&#20851;&#20999;&#12290;&#26412;&#25991;&#24341;&#20837;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;(RL)&#30340;&#27010;&#24565;&#65292;&#20026;AV&#35268;&#21010;&#21644;&#20915;&#31574;&#27169;&#22359;&#29983;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#21327;&#20316;RL&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#26159;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#65292;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#20026;&#22810;&#20010;&#20195;&#29702;&#20998;&#37197;&#22870;&#21169;&#65292;&#32771;&#34385;&#25152;&#26377;&#21442;&#25968;&#21644;&#26102;&#24207;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#20511;&#37492;&#20102;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#29992;&#20110;&#35299;&#20915;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20351;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#22870;&#21169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While autonomous vehicles (AVs) may perform remarkably well in generic real-life cases, their irrational action in some unforeseen cases leads to critical safety concerns. This paper introduces the concept of collaborative reinforcement learning (RL) to generate challenging test cases for AV planning and decision-making module. One of the critical challenges for collaborative RL is the credit assignment problem, where a proper assignment of rewards to multiple agents interacting in the traffic scenario, considering all parameters and timing, turns out to be non-trivial. In order to address this challenge, we propose a novel potential-based reward-shaping approach inspired by counterfactual analysis for solving the credit-assignment problem. The evaluation in a simulated environment demonstrates the superiority of our proposed approach against other methods using local and global rewards.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#19981;&#31934;&#30830;&#29275;&#39039;&#27861;&#26469;&#27714;&#35299;&#31561;&#24335;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#12289;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38543;&#26426;&#36845;&#20195;&#33609;&#22270;&#27714;&#35299;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#29275;&#39039;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#22312;&#31934;&#30830;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#20248;&#21183;&#20989;&#25968;&#19978;&#25191;&#34892;&#32447;&#25628;&#32034;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#27493;&#38271;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#12289;&#40065;&#26834;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.18379</link><description>&lt;p&gt;
&#31934;&#30830;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#21644;&#38543;&#26426;&#36845;&#20195;&#33609;&#22270;&#31639;&#27861;&#27714;&#35299;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Constrained Optimization via Exact Augmented Lagrangian and Randomized Iterative Sketching. (arXiv:2305.18379v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#19981;&#31934;&#30830;&#29275;&#39039;&#27861;&#26469;&#27714;&#35299;&#31561;&#24335;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#12289;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38543;&#26426;&#36845;&#20195;&#33609;&#22270;&#27714;&#35299;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#29275;&#39039;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#22312;&#31934;&#30830;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#20248;&#21183;&#20989;&#25968;&#19978;&#25191;&#34892;&#32447;&#25628;&#32034;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#27493;&#38271;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#12289;&#40065;&#26834;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35299;&#20915;&#31561;&#24335;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#12289;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#20986;&#29616;&#65292;&#21253;&#25324;&#21463;&#32422;&#26463;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;PDE&#32422;&#26463;&#20248;&#21270;&#12290;&#25105;&#20204;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#19981;&#31934;&#30830;&#29275;&#39039;&#27861;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#36845;&#20195;&#33609;&#22270;&#27714;&#35299;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#29275;&#39039;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#22312;&#31934;&#30830;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#20248;&#21183;&#20989;&#25968;&#19978;&#25191;&#34892;&#32447;&#25628;&#32034;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#27493;&#38271;&#12290;&#24403;&#37197;&#22791;&#36866;&#24403;&#30340;&#33609;&#22270;&#30697;&#38453;&#26102;&#65292;&#38543;&#26426;&#27714;&#35299;&#22120;&#30456;&#23545;&#20110;&#30830;&#23450;&#24615;&#32447;&#24615;&#31995;&#32479;&#27714;&#35299;&#22120;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27599;&#27425;&#36845;&#20195;&#30340;&#28014;&#28857;&#36816;&#31639;&#22797;&#26434;&#24230;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#38543;&#26426;&#27714;&#35299;&#22120;&#30340;&#31934;&#24230;&#21644;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#30340;&#24809;&#32602;&#21442;&#25968;&#65292;&#20197;&#30830;&#20445;&#19981;&#31934;&#30830;&#30340;&#29275;&#39039;&#26041;&#21521;&#26159;&#31934;&#30830;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#20989;&#25968;&#30340;&#19979;&#38477;&#26041;&#21521;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#22343;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider solving equality-constrained nonlinear, nonconvex optimization problems. This class of problems appears widely in a variety of applications in machine learning and engineering, ranging from constrained deep neural networks, to optimal control, to PDE-constrained optimization. We develop an adaptive inexact Newton method for this problem class. In each iteration, we solve the Lagrangian Newton system inexactly via a randomized iterative sketching solver, and select a suitable stepsize by performing line search on an exact augmented Lagrangian merit function. The randomized solvers have advantages over deterministic linear system solvers by significantly reducing per-iteration flops complexity and storage cost, when equipped with suitable sketching matrices. Our method adaptively controls the accuracy of the randomized solver and the penalty parameters of the exact augmented Lagrangian, to ensure that the inexact Newton direction is a descent direction of the exact augmented 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#25104;&#21151;&#23558;&#25968;&#25454;&#36827;&#34892;&#20102;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#26368;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18378</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#36827;&#34892;&#35299;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#25104;&#21151;&#23558;&#25968;&#25454;&#36827;&#34892;&#20102;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#26368;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#38656;&#35201;&#23558;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#24182;&#29420;&#31435;&#22320;&#34920;&#31034;&#20986;&#26469;&#65292;&#32780;&#27169;&#22411;&#24182;&#27809;&#26377;&#25552;&#20379;&#26377;&#20851;&#36825;&#20123;&#22240;&#32032;&#30340;&#30495;&#23454;&#20449;&#24687;&#65292;&#24402;&#32435;&#20559;&#35265;&#22312;&#23454;&#29616;&#35299;&#32544;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26045;&#21152;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#26397;&#30528;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#25968;&#25454;&#30340;&#24402;&#32435;&#20559;&#35265;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#28508;&#22312;&#32500;&#24230;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#31163;&#25955;&#32534;&#30721;&#65292;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#24212;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;&#26631;&#37327;&#30721;&#20070;&#12290;&#28508;&#22312;&#37327;&#21270;&#36843;&#20351;&#32534;&#30721;&#22120;&#22312;&#35768;&#22810;&#25968;&#25454;&#28857;&#19978;&#20351;&#29992;&#23569;&#37327;&#28508;&#22312;&#20540;&#65292;&#20174;&#32780;&#20351;&#35299;&#30721;&#22120;&#33021;&#22815;&#20026;&#27599;&#20010;&#20540;&#20998;&#37197;&#19968;&#33268;&#30340;&#21547;&#20041;&#12290;&#35268;&#33539;&#21270;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#24341;&#21521;&#36825;&#31181;&#31616;&#26126;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24191;&#27867;&#24212;&#29992;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#19968;&#31995;&#21015;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#22122;&#22768;&#31867;&#22411;BadLabel&#65292;&#23427;&#36890;&#36807;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#65288;LNL&#65289;&#31639;&#27861;&#24615;&#33021;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;LNL&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18377</link><description>&lt;p&gt;
BadLabel: &#35780;&#20272;&#19982;&#22686;&#24378;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning. (arXiv:2305.18377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#22122;&#22768;&#31867;&#22411;BadLabel&#65292;&#23427;&#36890;&#36807;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#65288;LNL&#65289;&#31639;&#27861;&#24615;&#33021;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;LNL&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25512;&#36827;&#23454;&#29992;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26631;&#31614;&#22122;&#22768;&#31867;&#22411;&#65292;&#20174;&#26465;&#20214;&#31867;&#22122;&#22768;&#21040;&#23454;&#20363;&#20381;&#36182;&#22122;&#22768;&#19981;&#31561;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#22122;&#22768;&#31867;&#22411;BadLabel&#65292;&#21487;&#20197;&#36890;&#36807;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;LNL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;BadLabel&#22522;&#20110;&#26631;&#20934;&#20998;&#31867;&#30340;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#21046;&#20316;&#65292;&#36873;&#25321;&#29305;&#23450;&#26679;&#26412;&#24182;&#23558;&#20854;&#26631;&#31614;&#32763;&#36716;&#20026;&#20854;&#20182;&#26631;&#31614;&#65292;&#20351;&#24471;&#24178;&#20928;&#26631;&#31614;&#21644;&#22122;&#22768;&#26631;&#31614;&#30340;&#25439;&#22833;&#20540;&#21464;&#24471;&#26080;&#27861;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;BadLabel&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;LNL&#26041;&#27861;&#65292;&#27599;&#20010;epoch&#23545;&#26631;&#31614;&#36827;&#34892;&#25932;&#23545;&#25200;&#21160;&#65292;&#20351;&#24178;&#20928;&#26631;&#31614;&#21644;&#22122;&#22768;&#26631;&#31614;&#30340;&#25439;&#22833;&#20540;&#20877;&#27425;&#21487;&#21306;&#20998;&#12290;&#19968;&#26086;&#36873;&#25321;&#20102;&#19968;&#23567;&#37096;&#20998;&#65288;&#22823;&#22810;&#25968;&#65289;&#24178;&#20928;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#21322;&#30417;&#30563;&#32763;&#35793;&#25216;&#26415;&#24212;&#29992;&#21040;LNL&#20013;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;BadLabel&#31867;&#22411;&#21644;&#25552;&#20986;&#30340;&#40065;&#26834;LNL&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;BadLabel&#21487;&#20197;&#23558;&#19971;&#20010;&#29616;&#26377;LNL&#31639;&#27861;&#30340;&#24615;&#33021;&#38477;&#20302;&#39640;&#36798;60&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#27604;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#19971;&#20010;&#29616;&#26377;LNL&#31639;&#27861;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798;17&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label-noise learning (LNL) aims to increase the model's generalization given training data with noisy labels. To facilitate practical LNL algorithms, researchers have proposed different label noise types, ranging from class-conditional to instance-dependent noises. In this paper, we introduce a novel label noise type called BadLabel, which can significantly degrade the performance of existing LNL algorithms by a large margin. BadLabel is crafted based on the label-flipping attack against standard classification, where specific samples are selected and their labels are flipped to other labels so that the loss values of clean and noisy labels become indistinguishable. To address the challenge posed by BadLabel, we further propose a robust LNL method that perturbs the labels in an adversarial manner at each epoch to make the loss values of clean and noisy labels again distinguishable. Once we select a small set of (mostly) clean labeled data, we can apply the techniques of semi-supervised
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dash&#30340;&#26377;&#25928;&#32780;&#20934;&#30830;&#30340;PARAFAC2&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#37319;&#29992;&#20004;&#38454;&#27573;ALS&#31639;&#27861;&#65292;&#22312;&#21452;&#21521;&#27969;&#22788;&#29702;&#38750;&#35268;&#21017;&#24352;&#37327;&#26102;&#39640;&#25928;&#22320;&#22788;&#29702;&#26032;&#34892;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22312;&#35813;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#24230;&#37327;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Dash&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18376</link><description>&lt;p&gt;
&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;PARAFAC2&#21452;&#21521;&#27969;&#31639;&#27861;&#21450;&#20854;&#22312;&#38750;&#35268;&#21017;&#24352;&#37327;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fast and Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors -- Algorithm and Application. (arXiv:2305.18376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dash&#30340;&#26377;&#25928;&#32780;&#20934;&#30830;&#30340;PARAFAC2&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#37319;&#29992;&#20004;&#38454;&#27573;ALS&#31639;&#27861;&#65292;&#22312;&#21452;&#21521;&#27969;&#22788;&#29702;&#38750;&#35268;&#21017;&#24352;&#37327;&#26102;&#39640;&#25928;&#22320;&#22788;&#29702;&#26032;&#34892;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22312;&#35813;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#24230;&#37327;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Dash&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31181;&#38750;&#35268;&#21017;&#24352;&#37327;&#30340;&#21452;&#21521;&#27969;&#22788;&#29702;&#65292;&#20854;&#20013;&#24352;&#37327;&#30340;&#20004;&#20010;&#32500;&#24230;&#22312;&#26102;&#38388;&#19978;&#22686;&#21152;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#30340;PARAFAC2&#20998;&#35299;&#26041;&#27861;Dash&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;ALS&#31639;&#27861;&#65292;&#22312;&#22788;&#29702;&#26032;&#34892;&#26102;&#25928;&#29575;&#39640;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22312;&#21452;&#21521;&#27969;&#19978;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Dash&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#26816;&#27979;&#25928;&#26524;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#38745;&#24577;&#21644;&#27969;&#24335;PARAFAC2&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we efficiently and accurately analyze an irregular tensor in a dual-way streaming setting where the sizes of two dimensions of the tensor increase over time? What types of anomalies are there in the dual-way streaming setting? An irregular tensor is a collection of matrices whose column lengths are the same while their row lengths are different. In a dual-way streaming setting, both new rows of existing matrices and new matrices arrive over time. PARAFAC2 decomposition is a crucial tool for analyzing irregular tensors. Although real-time analysis is necessary in the dual-way streaming, static PARAFAC2 decomposition methods fail to efficiently work in this setting since they perform PARAFAC2 decomposition for accumulated tensors whenever new data arrive. Existing streaming PARAFAC2 decomposition methods work in a limited setting and fail to handle new rows of matrices efficiently. In this paper, we propose Dash, an efficient and accurate PARAFAC2 decomposition method working in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#23398;&#20064;&#36339;&#36291;&#26041;&#27861;&#26469;&#29983;&#25104;&#24314;&#27169;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35745;&#25968;&#21644;&#38750;&#36127;&#36830;&#32493;&#25968;&#25454;&#31561;&#39640;&#31232;&#30095;&#24230;&#12289;&#20542;&#26012;&#24230;&#12289;&#37325;&#23614;&#24230;&#25110;&#36807;&#24230;&#20998;&#25955;&#24230;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#23398;&#20064;&#36339;&#36291;&#30456;&#27604;&#20110;&#23398;&#20064;&#21435;&#22122;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18375</link><description>&lt;p&gt;
&#23398;&#20064;&#36339;&#36291;: &#34180;&#21270;&#21644;&#21152;&#21402;&#28508;&#22312;&#35745;&#25968;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling. (arXiv:2305.18375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#23398;&#20064;&#36339;&#36291;&#26041;&#27861;&#26469;&#29983;&#25104;&#24314;&#27169;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35745;&#25968;&#21644;&#38750;&#36127;&#36830;&#32493;&#25968;&#25454;&#31561;&#39640;&#31232;&#30095;&#24230;&#12289;&#20542;&#26012;&#24230;&#12289;&#37325;&#23614;&#24230;&#25110;&#36807;&#24230;&#20998;&#25955;&#24230;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#23398;&#20064;&#36339;&#36291;&#30456;&#27604;&#20110;&#23398;&#20064;&#21435;&#22122;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21435;&#22122;&#24050;&#25104;&#20026;&#35774;&#35745;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#30340;&#37325;&#35201;&#33539;&#24335;&#65292;&#29992;&#20110;&#24314;&#27169;&#36830;&#32493;&#30340;&#23454;&#20540;&#25968;&#25454;&#21644;&#20998;&#31867;&#25968;&#25454;&#24050;&#32463;&#26377;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#21457;&#29616;&#23398;&#20064;&#21435;&#22122;&#22312;&#24314;&#27169;&#26576;&#20123;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#65288;&#22914;&#35745;&#25968;&#21644;&#38750;&#36127;&#36830;&#32493;&#25968;&#25454;&#65289;&#26102;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#20123;&#25968;&#25454;&#32463;&#24120;&#26159;&#39640;&#24230;&#31232;&#30095;&#12289;&#20542;&#26012;&#12289;&#37325;&#23614;&#25110;&#36807;&#24230;&#20998;&#25955;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#36339;&#36291;&#20316;&#20026;&#21508;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#20351;&#29992;&#27491;&#21521;&#35745;&#25968;&#31232;&#21270;&#26041;&#27861;&#26500;&#24314;&#23398;&#20064;&#30446;&#26631;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36870;&#21521;&#35745;&#25968;&#21152;&#21402;&#36807;&#31243;&#36845;&#20195;&#22320;&#25913;&#36827;&#20854;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#20160;&#20040;&#24773;&#20917;&#19979;&#23398;&#20064;&#36339;&#36291;&#19982;&#23398;&#20064;&#21435;&#22122;&#34920;&#29616;&#30456;&#24403;&#65292;&#24182;&#19988;&#20160;&#20040;&#24773;&#20917;&#19979;&#23398;&#20064;&#36339;&#36291;&#34920;&#29616;&#26356;&#22909;&#12290;&#20363;&#22914;&#65292;&#24314;&#35758;&#22312;&#24314;&#27169;&#35745;&#25968;&#21644;&#38750;&#36127;&#36830;&#32493;&#25968;&#25454;&#26102;&#20351;&#29992;&#23398;&#20064;&#36339;&#36291;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20855;&#26377;&#31232;&#30095;&#24615;&#12289;&#20542;&#26012;&#24615;&#12289;&#37325;&#23614;&#24615;&#25110;&#36807;&#24230;&#20998;&#25955;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to denoise has emerged as a prominent paradigm to design state-of-the-art deep generative models for natural images. How to use it to model the distributions of both continuous real-valued data and categorical data has been well studied in recently proposed diffusion models. However, it is found in this paper to have limited ability in modeling some other types of data, such as count and non-negative continuous data, that are often highly sparse, skewed, heavy-tailed, and/or overdispersed. To this end, we propose learning to jump as a general recipe for generative modeling of various types of data. Using a forward count thinning process to construct learning objectives to train a deep neural network, it employs a reverse count thickening process to iteratively refine its generation through that network. We demonstrate when learning to jump is expected to perform comparably to learning to denoise, and when it is expected to perform better. For example, learning to jump is recom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#35889;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;Top-N&#25512;&#33616;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18374</link><description>&lt;p&gt;
&#32431;&#35889;&#22270;&#23884;&#20837;&#65306;&#23558;&#22270;&#21367;&#31215;&#37325;&#26032;&#35299;&#37322;&#20026;Top-N&#25512;&#33616;&#20013;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pure Spectral Graph Embeddings: Reinterpreting Graph Convolution for Top-N Recommendation. (arXiv:2305.18374v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#35889;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;Top-N&#25512;&#33616;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21327;&#21516;&#36807;&#28388;&#20219;&#21153;&#65288;CF&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#30340;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22270;&#21367;&#31215;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#65292;&#22270;&#21367;&#31215;&#25805;&#20316;&#19982;&#22270;&#35889;&#22495;&#19978;&#30340;&#36807;&#28388;&#25805;&#20316;&#26377;&#20851;&#65292;&#20294;&#20026;&#20160;&#20040;&#36825;&#20250;&#23548;&#33268;&#21327;&#21516;&#36807;&#28388;&#38382;&#39064;&#30340;&#26356;&#39640;&#24615;&#33021;&#30340;&#29702;&#35770;&#22522;&#30784;&#20173;&#19981;&#20026;&#20154;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#21367;&#31215;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#29305;&#24449;&#22914;&#20309;&#20174;&#36807;&#28388;&#25805;&#20316;&#25512;&#36827;&#21040;&#30001;&#24402;&#19968;&#21270;&#37051;&#25509;&#30697;&#38453;&#30340;&#26368;&#39640;&#29305;&#24449;&#20540;&#23545;&#24212;&#30340;&#29305;&#24449;&#21521;&#37327;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#19988;&#35813;&#23376;&#31354;&#38388;&#19978;&#30340;&#21521;&#37327;&#26159;&#19982;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20989;&#25968;&#30340;&#27714;&#21644;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22270;&#21367;&#31215;&#25805;&#20316;&#37325;&#26032;&#35299;&#37322;&#20026;&#32431;&#35889;&#23884;&#20837;&#65292;&#23558;&#20854;&#19982;&#35889;&#26041;&#27861;&#25991;&#29486;&#23545;&#40784;&#65292;&#24182;&#31361;&#20986;&#20854;&#19982;Laplacian Eigenmaps&#21644;Common Neighbour Ranking Mechanism&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;Laplacian&#30340;&#35889;&#29305;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;Top-N&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of graph convolution in the development of recommender system algorithms has recently achieved state-of-the-art results in the collaborative filtering task (CF). While it has been demonstrated that the graph convolution operation is connected to a filtering operation on the graph spectral domain, the theoretical rationale for why this leads to higher performance on the collaborative filtering problem remains unknown. The presented work makes two contributions. First, we investigate the effect of using graph convolution throughout the user and item representation learning processes, demonstrating how the latent features learned are pushed from the filtering operation into the subspace spanned by the eigenvectors associated with the highest eigenvalues of the normalised adjacency matrix, and how vectors lying on this subspace are the optimal solutions for an objective function related to the sum of the prediction function over the training data. Then, we present an approach that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#23433;&#20840;&#20445;&#35777;&#25552;&#20379;&#20551;&#35774;&#30340;&#20570;&#27861;&#65292;&#20197;&#29992;&#20110;&#39564;&#35777;&#20855;&#26377;&#22797;&#26434;&#29615;&#22659;&#21644;&#23398;&#20064;&#22686;&#24378;&#32452;&#20214;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;DNN&#34892;&#20026;&#20551;&#35774;&#65292;&#26469;&#28385;&#36275;&#35201;&#27714;&#30340;&#23433;&#20840;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18372</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#33258;&#20027;&#31995;&#32479;&#39564;&#35777;&#20013;&#30340;&#20551;&#35774;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Assumption Generation for the Verification of Learning-Enabled Autonomous Systems. (arXiv:2305.18372v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#23433;&#20840;&#20445;&#35777;&#25552;&#20379;&#20551;&#35774;&#30340;&#20570;&#27861;&#65292;&#20197;&#29992;&#20110;&#39564;&#35777;&#20855;&#26377;&#22797;&#26434;&#29615;&#22659;&#21644;&#23398;&#20064;&#22686;&#24378;&#32452;&#20214;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;DNN&#34892;&#20026;&#20551;&#35774;&#65292;&#26469;&#28385;&#36275;&#35201;&#27714;&#30340;&#23433;&#20840;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#30340;&#23433;&#20840;&#20445;&#35777;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#22312;&#38656;&#35201;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#31561;&#23398;&#20064;&#22686;&#24378;&#32452;&#20214;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#29992;&#20110;&#35270;&#35273;&#24863;&#30693;&#12290; DNN&#30001;&#20110;&#35268;&#27169;&#22823;&#65288;&#21487;&#33021;&#20855;&#26377;&#25104;&#21315;&#19978;&#19975;&#20010;&#21442;&#25968;&#65289;&#12289;&#32570;&#20047;&#27491;&#24335;&#35268;&#33539;&#65288;DNN&#36890;&#24120;&#26159;&#20174;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#32570;&#20047;&#20219;&#20309;&#27491;&#24335;&#35201;&#27714;&#65289;&#20197;&#21450;&#23545;&#29615;&#22659;&#20013;&#24494;&#23567;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#32780;&#38590;&#20197;&#20998;&#26512;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#20445;&#35777;&#24335;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#36825;&#31181;&#33258;&#20027;&#31995;&#32479;&#30340;&#31995;&#32479;&#32423;&#23433;&#20840;&#23646;&#24615;&#12290; &#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#22312;&#20110;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21512;&#25104;&#26377;&#20445;&#35777;&#30340;DNN&#34892;&#20026;&#30340;&#20551;&#35774;&#65292;&#22312;&#27809;&#26377;DNN&#24863;&#30693;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#20998;&#26512;&#31995;&#32479;&#65292;&#20197;&#20445;&#35777;&#28385;&#36275;&#25152;&#38656;&#30340;&#23433;&#20840;&#23646;&#24615;&#12290; &#21512;&#25104;&#30340;&#20551;&#35774;&#26159;&#26368;&#24369;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#24449;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;DNN&#30340;&#36755;&#20986;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing safety guarantees for autonomous systems is difficult as these systems operate in complex environments that require the use of learning-enabled components, such as deep neural networks (DNNs) for visual perception. DNNs are hard to analyze due to their size (they can have thousands or millions of parameters), lack of formal specifications (DNNs are typically learnt from labeled data, in the absence of any formal requirements), and sensitivity to small changes in the environment. We present an assume-guarantee style compositional approach for the formal verification of system-level safety properties of such autonomous systems. Our insight is that we can analyze the system in the absence of the DNN perception components by automatically synthesizing assumptions on the DNN behaviour that guarantee the satisfaction of the required safety properties. The synthesized assumptions are the weakest in the sense that they characterize the output sequences of all the possible DNNs that, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33041;&#40836;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#30382;&#36136;&#21402;&#24230;&#29305;&#24449;&#25429;&#25417;&#21152;&#36895;&#32769;&#21270;&#65292;&#24182;&#21453;&#26144;&#20986;&#22686;&#21152;&#30340;&#31070;&#32463;&#30142;&#30149;&#25110;&#35748;&#30693;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.18370</link><description>&lt;p&gt;
&#20351;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33041;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Brain Age Prediction using coVariance Neural Networks. (arXiv:2305.18370v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33041;&#40836;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#30382;&#36136;&#21402;&#24230;&#29305;&#24449;&#25429;&#25417;&#21152;&#36895;&#32769;&#21270;&#65292;&#24182;&#21453;&#26144;&#20986;&#22686;&#21152;&#30340;&#31070;&#32463;&#30142;&#30149;&#25110;&#35748;&#30693;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34987;&#29992;&#20110;&#21033;&#29992;&#33041;&#25104;&#20687;&#25968;&#25454;&#20026;&#20010;&#20307;&#25552;&#20379;&#8220;&#33041;&#40836;&#8221;&#20272;&#35745;&#12290;&#30001;&#20110;&#33041;&#40836;&#19982;&#23454;&#38469;&#24180;&#40836;&#23384;&#22312;&#24046;&#24322;&#65288;&#31216;&#20026;&#8220;&#33041;&#40836;&#24046;&#8221;&#65289;&#65292;&#22240;&#27492;&#21487;&#20197;&#25429;&#25417;&#30001;&#20110;&#19981;&#33391;&#20581;&#24247;&#29366;&#20917;&#23548;&#33268;&#30340;&#21152;&#36895;&#32769;&#21270;&#65292;&#24182;&#22240;&#27492;&#21453;&#26144;&#20986;&#22686;&#21152;&#30340;&#31070;&#32463;&#30142;&#30149;&#25110;&#35748;&#30693;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#33041;&#40836;&#39044;&#27979;&#31639;&#27861;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#26041;&#27861;&#35770;&#20381;&#25454;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476; (VNN)&#26469;&#25552;&#20986;&#19968;&#31181;&#35299;&#21078;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#30382;&#36136;&#21402;&#24230;&#29305;&#24449;&#36827;&#34892;&#33041;&#40836;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#33041;&#40836;&#39044;&#27979;&#26694;&#26550;&#19981;&#20165;&#25193;&#23637;&#21040;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149; (AD) &#20013;&#33041;&#40836;&#24046;&#30340;&#31895;&#30053;&#25351;&#26631;&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of "brain age" for an individual. Importantly, the discordance between brain age and chronological age (referred to as "brain age gap") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer's disease (AD) and we make two important obser
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;VGG16&#31639;&#27861;&#30340;&#32954;&#30284;&#32467;&#33410;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#23558;&#32467;&#33410;&#20998;&#31867;&#20026;&#24694;&#24615;&#12289;&#33391;&#24615;&#21644;&#20581;&#24247;&#24739;&#32773;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18367</link><description>&lt;p&gt;
&#20351;&#29992;VGG16&#31639;&#27861;&#23545;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#32954;&#30284;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Using VGG16 Algorithms for classification of lung cancer in CT scans Image. (arXiv:2305.18367v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18367
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;VGG16&#31639;&#27861;&#30340;&#32954;&#30284;&#32467;&#33410;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#23558;&#32467;&#33410;&#20998;&#31867;&#20026;&#24694;&#24615;&#12289;&#33391;&#24615;&#21644;&#20581;&#24247;&#24739;&#32773;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#20840;&#29699;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26089;&#26399;&#26816;&#27979;&#32954;&#32467;&#33410;&#23545;&#20110;&#25552;&#39640;&#30284;&#30151;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#21307;&#29983;&#24517;&#39035;&#25163;&#21160;&#35782;&#21035;&#21487;&#33021;&#26377;&#32954;&#30284;&#30340;&#21306;&#22495;&#12290;&#22312;&#24320;&#21457;&#36825;&#20123;&#26816;&#27979;&#31995;&#32479;&#26102;&#65292;&#32954;&#32467;&#33410;&#30340;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#36136;&#22320;&#30340;&#20219;&#24847;&#24615;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#22312;&#32954;&#32467;&#33410;&#30340;&#20934;&#30830;&#35786;&#26029;&#21644;&#20998;&#31867;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;VGG16&#65292;&#20197;&#24110;&#21161;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#35786;&#26029;&#21644;&#20998;&#31867;&#32954;&#30284;&#32467;&#33410;&#12290;VGG16&#21487;&#23558;&#32954;&#30284;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20026;&#24694;&#24615;&#12289;&#33391;&#24615;&#21644;&#20581;&#24247;&#24739;&#32773;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#33410;&#26816;&#27979;&#20855;&#26377;92.08&#65285;&#30340;&#25935;&#24863;&#24615;&#65292;91&#65285;&#30340;&#20934;&#30830;&#24615;&#21644;93&#65285;&#30340;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is the leading reason behind cancer-related deaths within the world. Early detection of lung nodules is vital for increasing the survival rate of cancer patients. Traditionally, physicians should manually identify the world suspected of getting carcinoma. When developing these detection systems, the arbitrariness of lung nodules' shape, size, and texture could be a challenge. Many studies showed the applied of computer vision algorithms to accurate diagnosis and classification of lung nodules. A deep learning algorithm called the VGG16 was developed during this paper to help medical professionals diagnose and classify carcinoma nodules. VGG16 can classify medical images of carcinoma in malignant, benign, and healthy patients. This paper showed that nodule detection using this single neural network had 92.08% sensitivity, 91% accuracy, and an AUC of 93%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#25913;&#21892;&#35270;&#35273;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.18357</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepSI: Interactive Deep Learning for Semantic Interaction. (arXiv:2305.18357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#25913;&#21892;&#35270;&#35273;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#35270;&#35273;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#20132;&#20114;&#12290;&#35821;&#20041;&#20132;&#20114;&#25512;&#26029;&#20998;&#26512;&#20154;&#21592;&#22312;&#24863;&#30693;&#36807;&#31243;&#20013;&#30340;&#31934;&#30830;&#24847;&#22270;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\text{DeepSI}_{\text{finetune}}$&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#21040;&#20154;&#22312;&#20132;&#20114;&#24335;&#24863;&#30693;&#31649;&#36947;&#20013;&#65292;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#28145;&#24230;&#23398;&#20064;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#35821;&#20041;&#20132;&#20114;&#25512;&#26029;&#30340;&#36136;&#37327;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#35821;&#20041;&#20132;&#20114;&#26469;&#24494;&#35843;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35821;&#20041;&#20132;&#20114;&#25512;&#26029;&#30340;&#36136;&#37327;&#12290;&#20154;&#26426;&#20132;&#20114;&#21644;&#28145;&#24230;&#23398;&#20064;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#20351;&#24471;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35780;&#20272;&#23558;&#28145;&#24230;&#23398;&#20064;&#23884;&#20837;&#21040;&#35821;&#20041;&#20132;&#20114;&#24490;&#29615;&#20013;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;$\text{DeepSI}_{\te
&lt;/p&gt;
&lt;p&gt;
In this paper, we design novel interactive deep learning methods to improve semantic interactions in visual analytics applications. The ability of semantic interaction to infer analysts' precise intents during sensemaking is dependent on the quality of the underlying data representation. We propose the $\text{DeepSI}_{\text{finetune}}$ framework that integrates deep learning into the human-in-the-loop interactive sensemaking pipeline, with two important properties. First, deep learning extracts meaningful representations from raw data, which improves semantic interaction inference. Second, semantic interactions are exploited to fine-tune the deep learning representations, which then further improves semantic interaction inference. This feedback loop between human interaction and deep learning enables efficient learning of userand task-specific representations. To evaluate the advantage of embedding the deep learning within the semantic interaction loop, we compare $\text{DeepSI}_{\te
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Forward-Forward&#31639;&#27861;&#35757;&#32451;&#30340;&#32593;&#32476;&#20869;&#37096;&#34920;&#24449;&#20855;&#26377;&#39640;&#31232;&#30095;&#24230;&#65292;&#31867;&#21035;&#29305;&#23450;&#30340;&#38598;&#21512;&#65292;&#36825;&#19982;&#29983;&#29289;&#23398;&#35266;&#23519;&#21040;&#30340;&#30382;&#23618;&#34920;&#24449;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.18353</link><description>&lt;p&gt;
Forward-Forward&#31639;&#27861;&#35757;&#32451;&#30340;&#32593;&#32476;&#20013;&#30340;&#31361;&#29616;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Emergent representations in networks trained with the Forward-Forward algorithm. (arXiv:2305.18353v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18353
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Forward-Forward&#31639;&#27861;&#35757;&#32451;&#30340;&#32593;&#32476;&#20869;&#37096;&#34920;&#24449;&#20855;&#26377;&#39640;&#31232;&#30095;&#24230;&#65292;&#31867;&#21035;&#29305;&#23450;&#30340;&#38598;&#21512;&#65292;&#36825;&#19982;&#29983;&#29289;&#23398;&#35266;&#23519;&#21040;&#30340;&#30382;&#23618;&#34920;&#24449;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Backpropagation&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#20854;&#32570;&#20047;&#29983;&#29289;&#23398;&#19978;&#30340;&#29616;&#23454;&#24615;&#12290;&#20026;&#20102;&#23547;&#25214;&#19968;&#31181;&#26356;&#20855;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#36991;&#20813;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#65292;&#32780;&#26159;&#20351;&#29992;&#26412;&#22320;&#23398;&#20064;&#35268;&#21017;&#65292;&#26368;&#36817;&#20171;&#32461;&#30340;Forward-Forward&#31639;&#27861;&#23558;Backpropagation&#30340;&#20256;&#36882;&#26367;&#25442;&#20026;&#20004;&#20010;&#21069;&#21521;&#20256;&#36882;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;Forward-Forward&#31639;&#27861;&#33719;&#24471;&#30340;&#20869;&#37096;&#34920;&#24449;&#32452;&#32455;&#20026;&#31283;&#20581;&#30340;&#65292;&#31867;&#21035;&#29305;&#23450;&#30340;&#38598;&#21512;&#65292;&#30001;&#26497;&#23569;&#37327;&#30340;&#26377;&#25928;&#21333;&#20803;(&#39640;&#31232;&#30095;&#24230;)&#32452;&#25104;&#12290;&#36825;&#19982;&#24863;&#35273;&#22788;&#29702;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#30382;&#23618;&#34920;&#24449;&#38750;&#24120;&#30456;&#20284;&#12290;&#34429;&#28982;&#22312;&#20351;&#29992;&#26631;&#20934;Backpropagation&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#27809;&#26377;&#21457;&#29616;&#65292;&#20294;&#26159;&#22312;&#20351;&#29992;&#19982;Forward-Forward&#30456;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#32593;&#32476;&#20013;&#20063;&#20986;&#29616;&#20102;&#31232;&#30095;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;Forward-Forward&#25552;&#35758;&#30340;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#26356;&#25509;&#36817;&#29983;&#29289;&#23398;&#23398;&#20064;&#30340;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Backpropagation algorithm, widely used to train neural networks, has often been criticised for its lack of biological realism. In an attempt to find a more biologically plausible alternative, and avoid to back-propagate gradients in favour of using local learning rules, the recently introduced Forward-Forward algorithm replaces the traditional forward and backward passes of Backpropagation with two forward passes. In this work, we show that internal representations obtained with the Forward-Forward algorithm organize into robust, category-specific ensembles, composed by an extremely low number of active units (high sparsity). This is remarkably similar to what is observed in cortical representations during sensory processing. While not found in models trained with standard Backpropagation, sparsity emerges also in networks optimized by Backpropagation, on the same training objective of Forward-Forward. These results suggest that the learning procedure proposed by Forward-Forward ma
&lt;/p&gt;</description></item><item><title>&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#39640;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#20351;&#24471;&#39640;&#32500;&#25968;&#25454;&#22686;&#21152;&#65292;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#65292;&#29992;&#20110;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18352</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#22810;&#35270;&#35282;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Genetic Algorithm for Multi-View Feature Selection. (arXiv:2305.18352v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18352
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#39640;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#20351;&#24471;&#39640;&#32500;&#25968;&#25454;&#22686;&#21152;&#65292;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#65292;&#29992;&#20110;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#34917;&#20805;&#20449;&#24687;&#26469;&#22686;&#24378;&#39044;&#27979;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20250;&#23548;&#33268;&#39640;&#32500;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;&#36825;&#23545;&#21487;&#20197;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#39044;&#27979;&#27169;&#22411;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#19981;&#20165;&#21487;&#20197;&#35299;&#20915;&#19981;&#33391;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23613;&#31649;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#21033;&#29992;&#36328;&#27169;&#24577;&#30340;&#20869;&#22312;&#20449;&#24687;&#12289;&#32570;&#20047;&#27867;&#21270;&#24615;&#21644;&#36866;&#29992;&#20110;&#29305;&#23450;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#19978;&#30340;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view datasets offer diverse forms of data that can enhance prediction models by providing complementary information. However, the use of multi-view data leads to an increase in high-dimensional data, which poses significant challenges for the prediction models that can lead to poor generalization. Therefore, relevant feature selection from multi-view datasets is important as it not only addresses the poor generalization but also enhances the interpretability of the models. Despite the success of traditional feature selection methods, they have limitations in leveraging intrinsic information across modalities, lacking generalizability, and being tailored to specific classification tasks. We propose a novel genetic algorithm strategy to overcome these limitations of traditional feature selection methods for multi-view data. Our proposed approach, called the multi-view multi-objective feature selection genetic algorithm (MMFS-GA), simultaneously selects the optimal subset of feature
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#21487;&#20197;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#31181;&#23376;&#23646;&#24615;&#38598;&#21512;&#36731;&#24230;&#30417;&#30563;&#24182;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#23646;&#24615;&#31867;&#22411;&#12290;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#21551;&#21457;&#24335;&#21644;&#26080;&#30417;&#30563;&#28508;&#22312;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#39069;&#22806;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#65292;&#23558;&#29616;&#26377;&#31867;&#22411;&#30340;&#23646;&#24615;&#25193;&#23637;&#26368;&#22810;12&#20493;&#65292;&#24182;&#25104;&#21151;&#21457;&#25496;&#20102;39&#65285;&#30340;&#26032;&#23646;&#24615;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.18350</link><description>&lt;p&gt;
&#23454;&#29616;&#24320;&#25918;&#19990;&#30028;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#65306;&#22522;&#20110;&#36731;&#24230;&#30417;&#30563;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach. (arXiv:2305.18350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#21487;&#20197;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#31181;&#23376;&#23646;&#24615;&#38598;&#21512;&#36731;&#24230;&#30417;&#30563;&#24182;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#23646;&#24615;&#31867;&#22411;&#12290;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#21551;&#21457;&#24335;&#21644;&#26080;&#30417;&#30563;&#28508;&#22312;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#39069;&#22806;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#65292;&#23558;&#29616;&#26377;&#31867;&#22411;&#30340;&#23646;&#24615;&#25193;&#23637;&#26368;&#22810;12&#20493;&#65292;&#24182;&#25104;&#21151;&#21457;&#25496;&#20102;39&#65285;&#30340;&#26032;&#23646;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#20219;&#21153;&#35774;&#32622;&#65292;&#29992;&#20110;&#25552;&#21462;&#24320;&#25918;&#19990;&#30028;&#23646;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#26469;&#33258;&#20110;&#29616;&#26377;&#36164;&#28304;&#20013;&#24341;&#23548;&#30340;&#39640;&#36136;&#37327;&#31181;&#23376;&#23646;&#24615;&#38598;&#21512;&#65292;&#26088;&#22312;&#25193;&#23637;&#29616;&#26377;&#31181;&#23376;&#31867;&#22411;&#30340;&#23646;&#24615;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#26041;&#24335;&#21457;&#29616;&#20219;&#20309;&#26032;&#30340;&#23646;&#24615;&#31867;&#22411;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#29305;&#23450;&#20110;&#21463;&#38480;&#30417;&#30563;&#30340;Amacer&#26041;&#27861;&#12290;&#23588;&#20854;&#26159;&#65292;&#30001;&#20110;&#37027;&#20123;&#26410;&#35265;&#36807;&#30340;&#26032;&#23646;&#24615;&#27809;&#26377;&#30452;&#25509;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#20844;&#24335;&#21033;&#29992;&#20102;&#33258;&#25105;&#30417;&#30563;&#21551;&#21457;&#24335;&#21644;&#26080;&#30417;&#30563;&#28508;&#22312;&#23646;&#24615;&#65292;&#21033;&#29992;&#20135;&#21697;&#19978;&#19979;&#25991;&#33719;&#24471;&#39069;&#22806;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#20540;&#19978;&#36229;&#36807;&#20102;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;12&#20010;&#30334;&#20998;&#28857;&#65292;&#20351;&#29616;&#26377;&#31867;&#22411;&#30340;&#23646;&#24615;&#22823;&#22823;&#25193;&#23637;&#20102;&#26368;&#22810;12&#20493;&#65292;&#24182;&#19988;&#21457;&#29616;&#26032;&#23646;&#24615;&#20540;&#30340;&#33021;&#21147;&#36798;&#21040;&#20102;39&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new task setting for attribute mining on e-commerce products, serving as a practical solution to extract open-world attributes without extensive human intervention. Our supervision comes from a high-quality seed attribute set bootstrapped from existing resources, and we aim to expand the attribute vocabulary of existing seed types, and also to discover any new attribute types automatically. A new dataset is created to support our setting, and our approach Amacer is proposed specifically to tackle the limited supervision. Especially, given that no direct supervision is available for those unseen new attributes, our novel formulation exploits self-supervised heuristic and unsupervised latent attributes, which attains implicit semantic signals as additional supervision by leveraging product context. Experiments suggest that our approach surpasses various baselines by 12 F1, expanding attributes of existing types significantly by up to 12 times, and discovering values from 39%
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18342</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#32534;&#31243;&#20013;&#31070;&#32463;&#20219;&#21153;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#19979;&#32473;&#23450;&#30340;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20687; GPT-4 &#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21512;&#25104;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#36923;&#36753;&#21644;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415; NeurTaskSyn&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#21512;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;NeurTaskSyn &#30001;&#20004;&#20010;&#37096;&#20998;&#26500;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#25351;&#23548;&#24213;&#23618;&#31526;&#21495;&#25191;&#34892;&#24341;&#25806;&#29983;&#25104;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RLCF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18341</link><description>&lt;p&gt;
&#20351;&#29992;&#32534;&#35793;&#22120;&#29983;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#35843;&#25972;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback. (arXiv:2305.18341v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RLCF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20195;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31243;&#24207;&#21512;&#25104;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#21487;&#33021;&#36829;&#21453;&#22522;&#26412;&#30340;&#35821;&#35328;&#32423;&#21035;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RLCF&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#30340;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290; RLCF&#23558;LLM&#35270;&#20026;&#36890;&#36807;RL&#20195;&#29702;&#36880;&#27493;&#29983;&#25104;&#20195;&#30721;&#65292;&#24182;&#25509;&#25910;&#20197;&#19979;&#21453;&#39304;&#65306;&#65288;i&#65289;&#32534;&#35793;&#22120;&#27966;&#29983;&#30340;&#21453;&#39304;&#19982;&#25152;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#36890;&#36807;&#19968;&#32452;&#27491;&#30830;&#24615;&#26816;&#26597;&#26377;&#20851;; &#65288;ii&#65289;&#19981;&#21516;LLM&#30340;&#21453;&#39304;&#65292;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#19968;&#32452;&#21442;&#32771;&#31243;&#24207;&#30456;&#20284;&#12290;&#36825;&#20123;&#21453;&#39304;&#26426;&#21046;&#24110;&#21161;&#25152;&#29983;&#25104;&#30340;&#20195;&#30721;&#22312;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#30340;&#21516;&#26102;&#20445;&#25345;&#22312;&#30446;&#26631;&#20998;&#24067;&#20013;&#12290;RLCF&#26159;&#27169;&#22411;&#21644;&#35821;&#35328;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#22312;Java&#30340;MBJP&#21644;MathQA&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLCF&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, the code that these models produce can violate basic language-level invariants, leading to lower performance in downstream tasks. We address this issue through an approach, called RLCF, that further trains a pre-trained LLM using feedback from a code compiler. RLCF views the LLM as an RL agent that generates code step by step and receives: (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM on whether the generated code is similar to a set of reference programs in the training corpus. Together, these feedback mechanisms help the generated code remain within the target distribution while passing all static correctness checks. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF significantly raise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20135;&#21697;&#37327;&#21270;&#65288;PQ&#65289;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26367;&#20195;&#20256;&#32479;&#20056;&#21152;&#65288;MAC&#65289;&#36816;&#31639;&#30340;&#25928;&#26524;&#12290;&#20316;&#32773;&#21457;&#29616;FLOP&#21644;&#21442;&#25968;&#25968;&#37327;&#31561;&#25351;&#26631;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;PQ&#23450;&#21046;&#30828;&#20214;&#21152;&#36895;&#22120;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18334</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;&#65306;&#25105;&#20204;&#21040;&#20102;&#21527;&#65311;&#20135;&#21697;&#37327;&#21270;&#21450;&#20854;&#30828;&#20214;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are We There Yet? Product Quantization and its Hardware Acceleration. (arXiv:2305.18334v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20135;&#21697;&#37327;&#21270;&#65288;PQ&#65289;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26367;&#20195;&#20256;&#32479;&#20056;&#21152;&#65288;MAC&#65289;&#36816;&#31639;&#30340;&#25928;&#26524;&#12290;&#20316;&#32773;&#21457;&#29616;FLOP&#21644;&#21442;&#25968;&#25968;&#37327;&#31561;&#25351;&#26631;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;PQ&#23450;&#21046;&#30828;&#20214;&#21152;&#36895;&#22120;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20056;&#21152;&#65288;MAC&#65289;&#36816;&#31639;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#20027;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#26368;&#36817;&#65292;&#20135;&#21697;&#37327;&#21270;&#65288;PQ&#65289;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#36825;&#20123;&#24037;&#20316;&#36127;&#36733;&#65292;&#29992;&#39044;&#20808;&#35745;&#31639;&#30340;&#28857;&#31215;&#30340;&#20869;&#23384;&#26597;&#25214;&#26367;&#25442;&#20102;MAC&#12290;&#34429;&#28982;&#36825;&#20010;&#23646;&#24615;&#20351;PQ&#25104;&#20026;&#27169;&#22411;&#21152;&#36895;&#30340;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20154;&#20204;&#24456;&#23569;&#20102;&#35299;&#19982;&#35745;&#31639;&#21644;&#23384;&#20648;&#22120;&#21344;&#29992;&#30456;&#20851;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;PQ&#35774;&#32622;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#36880;&#23618;&#37325;&#24314;&#35823;&#24046;&#21644;&#31471;&#21040;&#31471;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#30740;&#31350;&#37096;&#32626;PQ DNN&#30340;&#25928;&#29575;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;FLOP&#12289;&#21442;&#25968;&#25968;&#37327;&#29978;&#33267;CPU/GPU&#24615;&#33021;&#31561;&#25351;&#26631;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26356;&#20844;&#24179;&#22320;&#35780;&#20272;PQ&#30340;&#30828;&#20214;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#23450;&#21046;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#36816;&#34892;PQ&#27169;&#22411;&#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;PQ&#37197;&#32622;&#30340;&#30828;&#20214;&#24615;&#33021;&#21644;&#23384;&#20648;&#35201;&#27714;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional multiply-accumulate (MAC) operations have long dominated computation time for deep neural networks (DNNs). Recently, product quantization (PQ) has been successfully applied to these workloads, replacing MACs with memory lookups to pre-computed dot products. While this property makes PQ an attractive solution for model acceleration, little is understood about the associated trade-offs in terms of compute and memory footprint, and the impact on accuracy. Our empirical study investigates the impact of different PQ settings and training methods on layerwise reconstruction error and end-to-end model accuracy. When studying the efficiency of deploying PQ DNNs, we find that metrics such as FLOPs, number of parameters, and even CPU/GPU performance, can be misleading. To address this issue, and to more fairly assess PQ in terms of hardware efficiency, we design the first custom hardware accelerator to evaluate the speed and efficiency of running PQ models. We identify PQ configurat
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18333</link><description>&lt;p&gt;
&#20855;&#26377;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#25490;&#21517;&#65306;&#33258;&#22686;&#24378;&#21160;&#24577;&#19979;&#30340;&#29992;&#25143;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics. (arXiv:2305.18333v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18333
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#30830;&#35748;&#27969;&#34892;&#24230;&#20559;&#35265;&#22312;&#25512;&#33616;&#65288;&#21644;&#20854;&#20182;&#22522;&#20110;&#25490;&#21517;&#30340;&#65289;&#31995;&#32479;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#20854;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#30340;&#35814;&#32454;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#36890;&#36807;&#23427;&#65292;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#21487;&#20197;&#24433;&#21709;&#29992;&#25143;&#36873;&#25321;&#65292;&#24182;&#19988;&#21487;&#20197;&#36127;&#38754;&#24433;&#21709;&#21508;&#31181;&#25512;&#33616;&#31574;&#30053;&#30340;&#38598;&#20307;&#29992;&#25143;&#25928;&#29992;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#38750;&#24179;&#31283;&#19978;&#19979;&#25991;&#33073;&#38774;&#26426;&#65292;&#24378;&#35843;&#19981;&#26159;&#20026;&#20102;&#28040;&#38500;&#27969;&#34892;&#24230;&#20559;&#35265;&#32780;&#26159;&#20026;&#20102;&#20943;&#36731;&#20854;&#36127;&#38754;&#24433;&#21709;&#32780;&#36827;&#34892;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#39318;&#20808;&#65292;&#26222;&#36890;&#30340;&#26377;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#25512;&#33616;&#31995;&#32479;&#20250;&#36890;&#36807;&#28151;&#28102;&#29289;&#21697;&#36136;&#37327;&#21644;&#27969;&#34892;&#24230;&#32780;&#24341;&#21457;&#32447;&#24615;&#36951;&#25022;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#32447;&#24615;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#29289;&#21697;&#36136;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#20063;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#36275;&#22815;&#21464;&#24322;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31867;UCB&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#23454;&#20102;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While popularity bias is recognized to play a role in recommmender (and other ranking-based) systems, detailed analyses of its impact on user welfare have largely been lacking. We propose a general mechanism by which item popularity, item quality, and position bias can impact user choice, and how it can negatively impact the collective user utility of various recommender policies. Formulating the problem as a non-stationary contextual bandit, we highlight the importance of exploration, not to eliminate popularity bias, but to mitigate its negative effects. First, naive popularity-biased recommenders are shown to induce linear regret by conflating item quality and popularity. More generally, we show that, even in linear settings, identifiability of item quality may not be possible due to the confounding effects of popularity bias. However, under sufficient variability assumptions, we develop an efficient UCB-style algorithm and prove efficient regret guarantees. We complement our analys
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;FPGA&#38598;&#32676;&#30340;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#37197;&#32622;&#20013;&#35780;&#20272;&#21644;&#31649;&#29702;&#31070;&#32463;&#32593;&#32476;&#24037;&#20316;&#36127;&#36733;&#65292;&#23454;&#29616;&#26368;&#20339;&#30340;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18332</link><description>&lt;p&gt;
&#21487;&#37325;&#26500;&#20998;&#24067;&#24335;FPGA&#38598;&#32676;&#35774;&#35745;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Reconfigurable Distributed FPGA Cluster Design for Deep Learning Accelerators. (arXiv:2305.18332v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18332
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;FPGA&#38598;&#32676;&#30340;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#37197;&#32622;&#20013;&#35780;&#20272;&#21644;&#31649;&#29702;&#31070;&#32463;&#32593;&#32476;&#24037;&#20316;&#36127;&#36733;&#65292;&#23454;&#29616;&#26368;&#20339;&#30340;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#21151;&#32791;&#23884;&#20837;&#24335;FPGA&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#25506;&#32034;&#20998;&#24067;&#24335;&#35843;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#65292;&#20197;&#33719;&#24471;&#26368;&#20339;&#30340;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#38598;&#32676;&#22312;&#23454;&#39564;&#36807;&#31243;&#20013;&#26159;&#27169;&#22359;&#21270;&#30340;&#65292;&#25105;&#20204;&#24050;&#32463;&#23454;&#29616;&#20102;&#39640;&#36798;12&#20010;&#22522;&#20110;Zynq-7020&#33455;&#29255;&#30340;&#26495;&#21644;5&#20010;UltraScale+ MPSoC FPGA&#26495;&#36890;&#36807;&#20197;&#22826;&#32593;&#20132;&#25442;&#26426;&#36830;&#25509;&#65292;&#24182;&#19988;&#38598;&#32676;&#23558;&#35780;&#20272;&#21487;&#37197;&#32622;&#30340;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#65288;DLA&#65289;Versatile Tensor Accelerator&#65288;VTA&#65289;&#12290;&#36825;&#31181;&#21487;&#36866;&#24212;&#30340;&#20998;&#24067;&#24335;&#26550;&#26500;&#30340;&#20248;&#28857;&#22312;&#20110;&#20854;&#33021;&#22815;&#22312;&#35768;&#22810;&#37197;&#32622;&#20013;&#35780;&#20272;&#21644;&#31649;&#29702;&#31070;&#32463;&#32593;&#32476;&#24037;&#20316;&#36127;&#36733;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#20854;&#29305;&#23450;&#30340;&#24212;&#29992;&#31243;&#24207;&#38656;&#27714;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#35745;&#31639;&#22270;&#25490;&#21015;&#25104;&#31649;&#36947;&#32467;&#26500;&#65292;&#24182;&#25163;&#21160;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a distributed system based on lowpower embedded FPGAs designed for edge computing applications focused on exploring distributing scheduling optimizations for Deep Learning (DL) workloads to obtain the best performance regarding latency and power efficiency. Our cluster was modular throughout the experiment, and we have implementations that consist of up to 12 Zynq-7020 chip-based boards as well as 5 UltraScale+ MPSoC FPGA boards connected through an ethernet switch, and the cluster will evaluate configurable Deep Learning Accelerator (DLA) Versatile Tensor Accelerator (VTA). This adaptable distributed architecture is distinguished by its capacity to evaluate and manage neural network workloads in numerous configurations which enables users to conduct multiple experiments tailored to their specific application needs. The proposed system can simultaneously execute diverse Neural Network (NN) models, arrange the computation graph in a pipeline structure, and manually allocate g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24179;&#34913;&#20256;&#25773;&#31639;&#27861;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#25104;&#21151;&#35757;&#32451;&#20102;&#20234;&#36763;&#26426;&#22120;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#22312; MNIST &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#36719;&#20214;&#23454;&#29616;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20234;&#36763;&#26426;&#22120;&#30340;&#36830;&#25509;&#36824;&#25903;&#25345;&#21367;&#31215;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#21367;&#31215;&#32593;&#32476;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#26368;&#23569;&#20351;&#29992;&#33258;&#26059;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.18321</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#34913;&#20256;&#25773;&#31639;&#27861;&#35757;&#32451;&#20234;&#36763;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Training an Ising Machine with Equilibrium Propagation. (arXiv:2305.18321v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24179;&#34913;&#20256;&#25773;&#31639;&#27861;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#25104;&#21151;&#35757;&#32451;&#20102;&#20234;&#36763;&#26426;&#22120;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#22312; MNIST &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#36719;&#20214;&#23454;&#29616;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20234;&#36763;&#26426;&#22120;&#30340;&#36830;&#25509;&#36824;&#25903;&#25345;&#21367;&#31215;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#21367;&#31215;&#32593;&#32476;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#26368;&#23569;&#20351;&#29992;&#33258;&#26059;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20234;&#36763;&#26426;&#22120;&#26159;&#20234;&#36763;&#27169;&#22411;&#32806;&#21512;&#33258;&#26059;&#30340;&#30828;&#20214;&#23454;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36215;&#28304;&#20013;&#23545;&#20110;&#38750;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#19982;&#20234;&#36763;&#26426;&#22120;&#29289;&#29702;&#29305;&#24615;&#20043;&#38388;&#30340;&#22797;&#26434;&#21305;&#37197;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#33719;&#24471;&#39640;&#31934;&#24230;&#38750;&#24120;&#20851;&#38190;&#65292;&#20294;&#20854;&#24212;&#29992;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#24179;&#34913;&#20256;&#25773;&#31639;&#27861;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#20234;&#36763;&#26426;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312; MNIST &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#36719;&#20214;&#23454;&#29616;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#37319;&#29992; D-Wave &#20234;&#36763;&#26426;&#22120;&#30340;&#37327;&#23376;&#36864;&#28779;&#36807;&#31243;&#26469;&#35757;&#32451;&#19968;&#20010;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26426;&#22120;&#30340;&#36830;&#25509;&#25903;&#25345;&#21367;&#31215;&#25805;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#21367;&#31215;&#32593;&#32476;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#26368;&#23569;&#20351;&#29992;&#33258;&#26059;&#25968;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#20234;&#36763;&#26426;&#22120;&#20316;&#20026;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ising machines, which are hardware implementations of the Ising model of coupled spins, have been influential in the development of unsupervised learning algorithms at the origins of Artificial Intelligence (AI). However, their application to AI has been limited due to the complexities in matching supervised training methods with Ising machine physics, even though these methods are essential for achieving high accuracy. In this study, we demonstrate a novel approach to train Ising machines in a supervised way through the Equilibrium Propagation algorithm, achieving comparable results to software-based implementations. We employ the quantum annealing procedure of the D-Wave Ising machine to train a fully-connected neural network on the MNIST dataset. Furthermore, we demonstrate that the machine's connectivity supports convolution operations, enabling the training of a compact convolutional network with minimal spins per neuron. Our findings establish Ising machines as a promising traina
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#65292;&#23545;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#25688;&#35201;&#32451;&#20064;&#30340;&#31572;&#26696;&#32467;&#26500;&#36827;&#34892;&#21453;&#39304;&#65292;&#35813;&#27169;&#22411;&#22312;&#23398;&#29983;&#25552;&#20132;&#30340;&#21477;&#23376;&#20013;&#23558;&#20854;&#24402;&#20026;&#19977;&#31867;&#65292;&#21363;&#32972;&#26223;&#12289;&#25216;&#26415;&#21644;&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#21270;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2305.18319</link><description>&lt;p&gt;
&#21270;&#23398;&#25968;&#25454;&#24211;&#21644;&#25688;&#35201;&#32451;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Automated Feedback Generation for a Chemistry Database and Abstracting Exercise. (arXiv:2305.18319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#65292;&#23545;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#25688;&#35201;&#32451;&#20064;&#30340;&#31572;&#26696;&#32467;&#26500;&#36827;&#34892;&#21453;&#39304;&#65292;&#35813;&#27169;&#22411;&#22312;&#23398;&#29983;&#25552;&#20132;&#30340;&#21477;&#23376;&#20013;&#23558;&#20854;&#24402;&#20026;&#19977;&#31867;&#65292;&#21363;&#32972;&#26223;&#12289;&#25216;&#26415;&#21644;&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#21270;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#30340;&#21453;&#39304;&#23545;&#20110;&#25945;&#23398;&#21644;&#23398;&#20064;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#29616;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#21464;&#25442;&#22120;&#65288;&#26426;&#22120;&#23398;&#20064;&#65289;&#27169;&#22411;&#65288;BERT&#65289;&#26469;&#23545;&#25688;&#35201;&#32451;&#20064;&#30340;&#31572;&#26696;&#32467;&#26500;&#36827;&#34892;&#21453;&#39304;&#12290;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#65292;&#35201;&#27714;&#23398;&#29983;&#20204;&#20174;&#20986;&#29256;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#19968;&#31687;&#25991;&#31456;&#24182;&#23545;&#20854;&#20869;&#23481;&#36827;&#34892;&#24635;&#32467;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;207&#20010;&#25552;&#20132;&#21697;&#65292;&#24635;&#20849;&#25688;&#35201;&#20102;21&#31687;&#26469;&#33258;&#20027;&#35201;&#25991;&#29486;&#30340;&#25991;&#31456;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65288;&#32422;15,000&#20010;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;80%&#30340;&#24050;&#25552;&#20132;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#36825;&#19968;&#27493;&#39588;&#34987;&#35748;&#20026;&#26159;&#37325;&#35201;&#30340;&#12290;&#23398;&#29983;&#25552;&#20132;&#30340;&#21477;&#23376;&#34987;&#24402;&#20026;&#19977;&#31867;&#8212;&#8212;&#32972;&#26223;&#12289;&#25216;&#26415;&#21644;&#35266;&#23519;&#8212;&#8212;&#36825;&#20351;&#24471;&#21487;&#20197;&#23558;&#27599;&#20010;&#25552;&#20132;&#21697;&#30340;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#23398;&#29983;&#30340;&#25688;&#35201;&#32467;&#26500;&#20197;&#21450;&#26469;&#33258;PubMed&#25968;&#25454;&#24211;&#30340;&#22823;&#37327;&#25688;&#35201;&#65292;&#21487;&#20197;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Timely feedback is an important part of teaching and learning. Here we describe how a readily available neural network transformer (machine-learning) model (BERT) can be used to give feedback on the structure of the response to an abstracting exercise where students are asked to summarise the contents of a published article after finding it from a publication database. The dataset contained 207 submissions from two consecutive years of the course, summarising a total of 21 different papers from the primary literature. The model was pre-trained using an available dataset (approx. 15,000 samples) and then fine-tuned on 80% of the submitted dataset. This fine tuning was seen to be important. The sentences in the student submissions are characterised into three classes - background, technique and observation - which allows a comparison of how each submission is structured. Comparing the structure of the students' abstract a large collection of those from the PubMed database shows that stud
&lt;/p&gt;</description></item><item><title>CDJUR-BR&#26159;&#19968;&#20221;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#21253;&#21547;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#20013;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#65292;&#35813;&#25910;&#34255;&#28085;&#30422;&#21508;&#31181;&#27861;&#24459;&#31243;&#24207;&#25991;&#20214;&#65292;&#24182;&#26377;&#21161;&#20110;&#35299;&#20915;&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26080;&#27861;&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#27861;&#24459;&#23454;&#36341;&#25991;&#26412;&#20013;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18315</link><description>&lt;p&gt;
CDJUR-BR -- &#24102;&#26377;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#30340;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#40644;&#37329;&#25910;&#34255;
&lt;/p&gt;
&lt;p&gt;
CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities. (arXiv:2305.18315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18315
&lt;/p&gt;
&lt;p&gt;
CDJUR-BR&#26159;&#19968;&#20221;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#21253;&#21547;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#20013;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#65292;&#35813;&#25910;&#34255;&#28085;&#30422;&#21508;&#31181;&#27861;&#24459;&#31243;&#24207;&#25991;&#20214;&#65292;&#24182;&#26377;&#21161;&#20110;&#35299;&#20915;&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26080;&#27861;&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#27861;&#24459;&#23454;&#36341;&#25991;&#26412;&#20013;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22810;&#25968;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#65288;Legal AI&#65289;&#24212;&#29992;&#31243;&#24207;&#32780;&#35328;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27861;&#24459;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#25991;&#26412;&#28041;&#21450;&#21040;&#30340;&#23454;&#20307;&#24182;&#38750;&#24403;&#21069;&#21487;&#29992;&#30340;NER&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#12290;&#32570;&#20047;&#27861;&#35268;&#12289;&#21028;&#20363;&#12289;&#35777;&#25454;&#12289;&#24809;&#32602;&#12289;&#27861;&#24459;&#31243;&#24207;&#20013;&#20154;&#20204;&#30340;&#35282;&#33394;&#65288;&#27861;&#23448;&#12289;&#24459;&#24072;&#12289;&#21463;&#23475;&#32773;&#12289;&#34987;&#21578;&#12289;&#35777;&#20154;&#65289;&#12289;&#20301;&#32622;&#31867;&#22411;&#65288;&#29359;&#32618;&#22320;&#28857;&#12289;&#34987;&#21578;&#22320;&#22336;&#65289;&#31561;&#30340;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#20173;&#38656;&#35201;&#19968;&#20010;&#29992;&#27861;&#24459;&#39046;&#22495;&#30340;&#31934;&#32454;&#23454;&#20307;&#36827;&#34892;&#27880;&#37322;&#30340;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#28085;&#30422;&#27861;&#24459;&#31243;&#24207;&#30340;&#21508;&#31181;&#25991;&#20214;&#65292;&#20363;&#22914;&#35831;&#24895;&#20070;&#12289;&#35843;&#26597;&#12289;&#25237;&#35785;&#12289;&#20915;&#23450;&#21644;&#21028;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24052;&#35199;&#21496;&#27861;&#40644;&#37329;&#25910;&#34255;&#65288;CDJUR-BR&#65289;&#30340;&#24320;&#21457;&#65292;&#35813;&#25910;&#34255;&#21253;&#21547;&#19968;&#32452;&#30001;&#27861;&#24459;&#25991;&#29486;&#19987;&#23478;&#27880;&#37322;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#12290;&#21019;&#24314;CDJUR-BR&#36981;&#24490;&#20102;&#33258;&#24049;&#30340;
&lt;/p&gt;
&lt;p&gt;
A basic task for most Legal Artificial Intelligence (Legal AI) applications is Named Entity Recognition (NER). However, texts produced in the context of legal practice make references to entities that are not trivially recognized by the currently available NERs. There is a lack of categorization of legislation, jurisprudence, evidence, penalties, the roles of people in a legal process (judge, lawyer, victim, defendant, witness), types of locations (crime location, defendant's address), etc. In this sense, there is still a need for a robust golden collection, annotated with fine-grained entities of the legal domain, and which covers various documents of a legal process, such as petitions, inquiries, complaints, decisions and sentences. In this article, we describe the development of the Golden Collection of the Brazilian Judiciary (CDJUR-BR) contemplating a set of fine-grained named entities that have been annotated by experts in legal documents. The creation of CDJUR-BR followed its ow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;(CAT)&#26694;&#26550;&#30340;&#32422;&#26463;&#29256;&#26412;C-BOBCAT&#65292;&#36890;&#36807;&#26435;&#34913;&#27979;&#35797;&#20934;&#30830;&#24615;&#21644;&#38382;&#39064;&#26292;&#38706;&#29575;&#21450;&#27979;&#35797;&#37325;&#21472;&#29575;&#65292;&#35299;&#20915;&#20102;BOBCAT&#23384;&#22312;&#30340;&#39640;&#38382;&#39064;&#26292;&#38706;&#29575;&#21644;&#27979;&#35797;&#37325;&#21472;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18312</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#20013;&#24179;&#34913;&#27979;&#35797;&#20934;&#30830;&#24615;&#19982;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Balancing Test Accuracy and Security in Computerized Adaptive Testing. (arXiv:2305.18312v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;(CAT)&#26694;&#26550;&#30340;&#32422;&#26463;&#29256;&#26412;C-BOBCAT&#65292;&#36890;&#36807;&#26435;&#34913;&#27979;&#35797;&#20934;&#30830;&#24615;&#21644;&#38382;&#39064;&#26292;&#38706;&#29575;&#21450;&#27979;&#35797;&#37325;&#21472;&#29575;&#65292;&#35299;&#20915;&#20102;BOBCAT&#23384;&#22312;&#30340;&#39640;&#38382;&#39064;&#26292;&#38706;&#29575;&#21644;&#27979;&#35797;&#37325;&#21472;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;(CAT)&#26159;&#19968;&#31181;&#21487;&#20197;&#20934;&#30830;&#27979;&#37327;&#23398;&#29983;&#30693;&#35782;&#27700;&#24179;&#19988;&#32553;&#30701;&#27979;&#35797;&#26102;&#38388;&#30340;&#20010;&#24615;&#21270;&#27979;&#35797;&#24418;&#24335;&#12290;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;CAT(BOBCAT)&#26159;&#19968;&#20010;&#26368;&#36817;&#30340;&#26694;&#26550;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#32553;&#30701;&#20102;&#27979;&#35797;&#26102;&#38388;&#24182;&#25552;&#39640;&#20102;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#23384;&#22312;&#39640;&#38382;&#39064;&#26292;&#38706;&#29575;&#21644;&#27979;&#35797;&#37325;&#21472;&#29575;&#30340;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#27979;&#35797;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BOBCAT&#30340;&#19968;&#31181;&#32422;&#26463;&#29256;&#26412;&#65292;&#36890;&#36807;&#26356;&#25913;&#20854;&#20248;&#21270;&#35774;&#32622;&#20351;&#25105;&#20204;&#33021;&#22815;&#26435;&#34913;&#27979;&#35797;&#20934;&#30830;&#24615;&#21644;&#38382;&#39064;&#26292;&#38706;&#29575;&#21450;&#27979;&#35797;&#37325;&#21472;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;&#25104;&#20154;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computerized adaptive testing (CAT) is a form of personalized testing that accurately measures students' knowledge levels while reducing test length. Bilevel optimization-based CAT (BOBCAT) is a recent framework that learns a data-driven question selection algorithm to effectively reduce test length and improve test accuracy. However, it suffers from high question exposure and test overlap rates, which potentially affects test security. This paper introduces a constrained version of BOBCAT to address these problems by changing its optimization setup and enabling us to trade off test accuracy for question exposure and test overlap rates. We show that C-BOBCAT is effective through extensive experiments on two real-world adult testing datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#35270;&#35282;&#20132;&#20114;&#20027;&#39064;&#22238;&#24402;&#31639;&#27861;&#65288;MV-ICTR&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#21516;&#26102;&#32435;&#20837;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#29289;&#21697;&#29305;&#23450;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#30340;&#20010;&#20154;&#20559;&#22909;&#65292;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#31574;&#30053;&#36827;&#34892;&#25345;&#32493;&#30340;&#22312;&#32447;&#20010;&#24615;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18306</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20132;&#20114;&#24335;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Multi-View Interactive Collaborative Filtering. (arXiv:2305.18306v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#35270;&#35282;&#20132;&#20114;&#20027;&#39064;&#22238;&#24402;&#31639;&#27861;&#65288;MV-ICTR&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#21516;&#26102;&#32435;&#20837;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#29289;&#21697;&#29305;&#23450;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#30340;&#20010;&#20154;&#20559;&#22909;&#65292;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#31574;&#30053;&#36827;&#34892;&#25345;&#32493;&#30340;&#22312;&#32447;&#20010;&#24615;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65288;&#22914;&#28857;&#20987;&#25110;&#35780;&#20998;&#65289;&#24448;&#24448;&#24456;&#23569;&#65292;&#29289;&#21697;&#30340;&#25442;&#25163;&#29575;&#65288;&#20363;&#22914;&#26032;&#25991;&#31456;&#12289;&#25307;&#32856;&#20449;&#24687;&#65289;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#29992;&#25143;-&#29289;&#21697;&#35780;&#20998;&#22806;&#65292;&#38598;&#25104;&#19978;&#19979;&#25991;&#8220;&#36793;&#8221;&#20449;&#24687;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#12290;&#34429;&#28982;&#23384;&#22312;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20165;&#33021;&#36827;&#34892;&#26679;&#26412;&#20869;&#25512;&#33616;&#65292;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#38480;&#21046;&#65292;&#24182;&#19981;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#31574;&#30053;&#36827;&#34892;&#38271;&#26399;&#32047;&#31215;&#25910;&#30410;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#20132;&#20114;&#20027;&#39064;&#22238;&#24402;&#65288;MV-ICTR&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#37096;&#20998;&#22312;&#32447;&#28508;&#22312;&#22240;&#23376;&#25512;&#33616;&#31639;&#27861;&#65292;&#21516;&#26102;&#32435;&#20837;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#29289;&#21697;&#29305;&#23450;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#30340;&#20010;&#20154;&#20559;&#22909;&#65292;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#31574;&#30053;&#36827;&#34892;&#25345;&#32493;&#22312;&#32447;&#20010;&#24615;&#21270;&#12290;&#35813;&#31639;&#27861;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, recommender system user interaction data such as clicks or ratings is sparse, and item turnover rates (e.g., new articles, job postings) high. Given this, the integration of contextual "side" information in addition to user-item ratings is highly desirable. Whilst there are algorithms that can handle both rating and contextual data simultaneously, these algorithms are typically limited to making only in-sample recommendations, suffer from the curse of dimensionality, and do not incorporate multi-armed bandit (MAB) policies for long-term cumulative reward optimization. We propose multi-view interactive topic regression (MV-ICTR) a novel partially online latent factor recommender algorithm that incorporates both rating and contextual information to model item-specific feature dependencies and users' personal preferences simultaneously, with multi-armed bandit policies for continued online personalization. The result is significantly increased performance on datasets wi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28508;&#22312;Bandits&#31639;&#27861;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2305.18305</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;Bandits&#30340;&#39640;&#20934;&#30830;&#24230;&#21644;&#20302;&#36951;&#25022;&#29992;&#25143;&#20919;&#21551;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
High Accuracy and Low Regret for User-Cold-Start Using Latent Bandits. (arXiv:2305.18305v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18305
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;Bandits&#31639;&#27861;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28508;&#22312;Bandits&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26032;&#29992;&#25143;&#21152;&#20837;&#25512;&#33616;&#31995;&#32479;&#26102;&#38754;&#20020;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#31639;&#27861;&#22312;&#21516;&#26102;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36951;&#25022;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel latent-bandit algorithm for tackling the cold-start problem for new users joining a recommender system. This new algorithm significantly outperforms the state of the art, simultaneously achieving both higher accuracy and lower regret.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#19968;&#22823;&#32452;&#20505;&#36873;&#20989;&#25968;&#65292;&#20351;&#29992; $\ell_1-\ell_2$ &#31232;&#30095;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#32467;&#26500;&#27169;&#22411;&#36873;&#25321;&#65292;&#23454;&#29616;&#20174;&#19981;&#20805;&#20998;&#19988;&#22024;&#26434;&#30340;&#26102;&#31354;&#25968;&#25454;&#20013;&#35782;&#21035;&#32467;&#26500;&#21270;&#21160;&#24577;&#31995;&#32479;&#65307;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17467</link><description>&lt;p&gt;
&#36890;&#36807; $\ell_1-\ell_2$ &#20248;&#21270;&#36827;&#34892;&#32467;&#26500;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Structured model selection via $\ell_1-\ell_2$ optimization. (arXiv:2305.17467v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17467
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#19968;&#22823;&#32452;&#20505;&#36873;&#20989;&#25968;&#65292;&#20351;&#29992; $\ell_1-\ell_2$ &#31232;&#30095;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#32467;&#26500;&#27169;&#22411;&#36873;&#25321;&#65292;&#23454;&#29616;&#20174;&#19981;&#20805;&#20998;&#19988;&#22024;&#26434;&#30340;&#26102;&#31354;&#25968;&#25454;&#20013;&#35782;&#21035;&#32467;&#26500;&#21270;&#21160;&#24577;&#31995;&#32479;&#65307;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#19968;&#22823;&#32452;&#20505;&#36873;&#20989;&#25968;&#65292;&#29992;&#19968;&#31181;&#38750;&#20984; $\ell_1-\ell_2$ &#31232;&#30095;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#65292;&#36890;&#36807;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#30340;&#26041;&#27861;&#36827;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#20505;&#36873;&#20989;&#25968;&#38598;&#21512;&#24418;&#25104;&#36793;&#30028;&#27491;&#20132;&#31995;&#32479;&#30340;&#32467;&#26500;&#38543;&#26426;&#37319;&#26679;&#30697;&#38453;&#65292;&#23601;&#21487;&#20197;&#36890;&#36807;&#20271;&#24681;&#26031;&#22374;&#26679;&#24335;&#30340;&#19981;&#31561;&#24335;&#21644;&#19968;&#33268;&#24615;&#26465;&#20214;&#31283;&#23450;&#24674;&#22797;&#65292;&#24182;&#19988;&#35823;&#24046;&#26377;&#30028;&#12290;&#35813;&#23398;&#20064;&#26041;&#27861;&#22312;&#30001;&#31896;&#24615;Burgers'&#26041;&#31243;&#21644;&#20004;&#20010;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#35745;&#31639;&#32467;&#26524;&#35777;&#26126;&#20102;&#25104;&#21151;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#30456;&#23545;&#20110;&#29615;&#22659;&#32500;&#25968;&#21644;&#20505;&#36873;&#20989;&#25968;&#25968;&#37327;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated model selection is an important application in science and engineering. In this work, we develop a learning approach for identifying structured dynamical systems from undersampled and noisy spatiotemporal data. The learning is performed by a sparse least-squares fitting over a large set of candidate functions via a nonconvex $\ell_1-\ell_2$ sparse optimization solved by the alternating direction method of multipliers. Using a Bernstein-like inequality with a coherence condition, we show that if the set of candidate functions forms a structured random sampling matrix of a bounded orthogonal system, the recovery is stable and the error is bounded. The learning approach is validated on synthetic data generated by the viscous Burgers' equation and two reaction-diffusion equations. The computational results demonstrate the theoretical guarantees of success and the efficiency with respect to the ambient dimension and the number of candidate functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#36716;&#25442;&#65292;&#19988;&#21518;&#24724;&#36880;&#28176;&#22686;&#21152;&#19982;&#23545;&#25163;&#30340;&#24694;&#24847;&#31243;&#24230;&#25104;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.17380</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#36716;&#25442;&#30340;&#26080;&#36951;&#25022;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions. (arXiv:2305.17380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#36716;&#25442;&#65292;&#19988;&#21518;&#24724;&#36880;&#28176;&#22686;&#21152;&#19982;&#23545;&#25163;&#30340;&#24694;&#24847;&#31243;&#24230;&#25104;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22312;&#19982;&#23545;&#25163;&#30340;$ T $&#36718;&#20132;&#20114;&#20043;&#21518;&#23454;&#29616;${ O}(\sqrt{T})$&#30340;&#21518;&#24724;&#65292;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#26159;&#30001;&#23545;&#25163;&#20219;&#24847;&#36873;&#25321;&#30340;&#65292;&#20294;&#21069;&#25552;&#26159;&#36716;&#31227;&#20989;&#25968;&#24517;&#39035;&#22266;&#23450;&#12290;&#36825;&#26159;&#22240;&#20026;&#24050;&#32463;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#36716;&#31227;&#20989;&#25968;&#20351;&#26080;&#24724;&#23398;&#20064;&#21464;&#24471;&#19981;&#21487;&#33021;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#31181;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#36716;&#25442;&#30340;&#31639;&#27861;&#65292;&#21518;&#24724;&#36880;&#28176;&#22686;&#21152;&#19982;&#23545;&#25163;&#30340;&#24694;&#24847;&#31243;&#24230;&#25104;&#27604;&#20363;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23427;&#30340;&#21518;&#24724;&#20026;$\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$&#65292;&#20854;&#20013;$C^{\textsf{P}}$&#34920;&#31034;&#36716;&#25442;&#20989;&#25968;&#30340;&#23545;&#25239;&#24615;&#65292;&#26368;&#22810;&#21487;&#20197;&#20026;${O}(T)$&#12290;&#34429;&#28982;&#27492;&#31639;&#27861;&#26412;&#36523;&#38656;&#35201;$C^{\textsf{P}}$&#30340;&#30693;&#35782;&#65292;&#20294;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#40657;&#30418;&#32553;&#20943;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#36827;&#19968;&#27493;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#38170;&#23450;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing online learning algorithms for adversarial Markov Decision Processes achieve ${O}(\sqrt{T})$ regret after $T$ rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys $\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$ regret where $C^{\textsf{P}}$ measures how adversarial the transition functions are and can be at most ${O}(T)$. While this algorithm itself requires knowledge of $C^{\textsf{P}}$, we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>mldr.resampling&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#26088;&#22312;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17152</link><description>&lt;p&gt;
mldr.resampling: &#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#31639;&#27861;&#26377;&#25928;&#30340;&#21442;&#32771;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms. (arXiv:2305.17152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17152
&lt;/p&gt;
&lt;p&gt;
mldr.resampling&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#26088;&#22312;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#37319;&#26679;&#31639;&#27861;&#26159;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#24773;&#20917;&#30340;&#26377;&#29992;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#24517;&#39035;&#22788;&#29702;&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#30340;&#22855;&#24322;&#24615;&#65292;&#20363;&#22914;&#21516;&#19968;&#23454;&#20363;&#20013;&#39057;&#32321;&#21644;&#19981;&#39057;&#32321;&#26631;&#31614;&#30340;&#20986;&#29616;&#12290;&#36825;&#31687;&#21407;&#21019;&#36719;&#20214;&#21457;&#34920;&#20171;&#32461;&#20102; mldr.resampling&#65292;&#36825;&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;&#20102;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#24378;&#35843;&#25928;&#29575;&#65292;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#32791;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resampling algorithms are a useful approach to deal with imbalanced learning in multilabel scenarios. These methods have to deal with singularities in the multilabel data, such as the occurrence of frequent and infrequent labels in the same instance. Implementations of these methods are sometimes limited to the pseudocode provided by their authors in a paper. This Original Software Publication presents mldr.resampling, a software package that provides reference implementations for eleven multilabel resampling methods, with an emphasis on efficiency since these algorithms are usually time-consuming.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#25240;&#25187;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#21512;&#25104;&#65292;&#24182;&#25506;&#35752;&#22914;&#20309;&#20943;&#23569;&#20854;&#23545;&#29366;&#24577;&#36716;&#31227;&#30340;&#24494;&#23567;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17115</link><description>&lt;p&gt;
&#22522;&#20110;&#25240;&#25187;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#30340;&#25919;&#31574;&#21512;&#25104;&#21644;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Policy Synthesis and Reinforcement Learning for Discounted LTL. (arXiv:2305.17115v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#25240;&#25187;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#21512;&#25104;&#65292;&#24182;&#25506;&#35752;&#22914;&#20309;&#20943;&#23569;&#20854;&#23545;&#29366;&#24577;&#36716;&#31227;&#30340;&#24494;&#23567;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#22256;&#38590;&#24615;&#20351;&#24471;&#20351;&#29992;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#26469;&#34920;&#36798;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30446;&#26631;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#12290;&#28982;&#32780;&#65292;LTL &#30340;&#32570;&#28857;&#26159;&#23427;&#23545;&#29366;&#24577;&#36716;&#31227;&#30340;&#24494;&#23567;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#65292;&#36825;&#38459;&#30861;&#20102;&#22522;&#20110;&#27010;&#29575;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#30340;&#23398;&#20064;&#65292;&#38500;&#38750;&#37319;&#29992;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;&#26102;&#38388;&#25240;&#25187;&#20026;&#28040;&#38500;&#36825;&#31181;&#25935;&#24863;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#24335;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#35813;&#36923;&#36753;&#30340;&#39640;&#34920;&#36798;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#26410;&#30693;&#29366;&#24577;&#36716;&#25442;&#27010;&#29575;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20351;&#29992;&#25240;&#25187; LTL &#36827;&#34892;&#25919;&#31574;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22870;&#21169;&#26426;&#22120;&#23558;&#25240;&#25187; LTL &#38477;&#35299;&#20026;&#25240;&#25187;&#21644;&#22870;&#21169;&#20043;&#21644;&#65292;&#24403;&#25152;&#26377;&#25240;&#25187;&#22240;&#23376;&#30456;&#21516;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of manually specifying reward functions has led to an interest in using linear temporal logic (LTL) to express objectives for reinforcement learning (RL). However, LTL has the downside that it is sensitive to small perturbations in the transition probabilities, which prevents probably approximately correct (PAC) learning without additional assumptions. Time discounting provides a way of removing this sensitivity, while retaining the high expressivity of the logic. We study the use of discounted LTL for policy synthesis in Markov decision processes with unknown transition probabilities, and show how to reduce discounted LTL to discounted-sum reward via a reward machine when all discount factors are identical.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SVD&#26080;&#30417;&#30563;&#19977;&#32500;&#24179;&#38754;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21644;&#35821;&#20041;&#22320;&#22270;&#21363;&#21487;&#25913;&#21892;NeRF&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35757;&#32451;&#35270;&#22270;&#30340;&#36807;&#25311;&#21512;&#23548;&#33268;&#20302;&#32441;&#29702;&#21306;&#22495;&#30340;&#20960;&#20309;&#37325;&#24314;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16914</link><description>&lt;p&gt;
PlaNeRF&#65306;SVD&#26080;&#30417;&#30563;&#19977;&#32500;&#24179;&#38754;&#27491;&#21017;&#21270;&#29992;&#20110;NeRF&#22823;&#35268;&#27169;&#22330;&#26223;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction. (arXiv:2305.16914v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SVD&#26080;&#30417;&#30563;&#19977;&#32500;&#24179;&#38754;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21644;&#35821;&#20041;&#22320;&#22270;&#21363;&#21487;&#25913;&#21892;NeRF&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35757;&#32451;&#35270;&#22270;&#30340;&#36807;&#25311;&#21512;&#23548;&#33268;&#20302;&#32441;&#29702;&#21306;&#22495;&#30340;&#20960;&#20309;&#37325;&#24314;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21033;&#29992;2D&#22270;&#20687;&#21644;&#30456;&#26426;&#23039;&#24577;&#36827;&#34892;3D&#22330;&#26223;&#37325;&#24314;&#20197;&#36827;&#34892;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#12290;&#23613;&#31649;NeRF&#33021;&#20135;&#29983;&#36924;&#30495;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#32463;&#24120;&#36973;&#21463;&#36807;&#25311;&#21512;&#20110;&#35757;&#32451;&#35270;&#22270;&#30340;&#22256;&#25200;&#65292;&#23548;&#33268;&#20960;&#20309;&#37325;&#24314;&#19981;&#20339;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#32441;&#29702;&#21306;&#22495;&#12290;&#36825;&#31181;&#38480;&#21046;&#38480;&#21046;&#20102;&#35768;&#22810;&#38656;&#35201;&#20934;&#30830;&#20960;&#20309;&#24418;&#24577;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#20363;&#22914;&#22806;&#25512;NVS&#65292;&#39640;&#28165;&#26144;&#23556;&#21644;&#22330;&#26223;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21644;&#35821;&#20041;&#22320;&#22270;&#21363;&#21487;&#25913;&#21892;NeRF&#30340;&#19977;&#32500;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#26032;&#39062;&#24179;&#38754;&#27491;&#21017;&#21270;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25439;&#22833;&#35774;&#35745;&#20013;&#21033;&#29992;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#27979;&#37327;&#65288;SSIM&#65289;&#26469;&#27491;&#30830;&#21021;&#22987;&#21270;NeRF&#30340;&#20307;&#31215;&#34920;&#31034;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#27969;&#34892;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20960;&#20309;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images and camera poses for Novel View Synthesis (NVS). Although NeRF can produce photorealistic results, it often suffers from overfitting to training views, leading to poor geometry reconstruction, especially in low-texture areas. This limitation restricts many important applications which require accurate geometry, such as extrapolated NVS, HD mapping and scene editing. To address this limitation, we propose a new method to improve NeRF's 3D structure using only RGB images and semantic maps. Our approach introduces a novel plane regularization based on Singular Value Decomposition (SVD), that does not rely on any geometric prior. In addition, we leverage the Structural Similarity Index Measure (SSIM) in our loss design to properly initialize the volumetric representation of NeRF. Quantitative and qualitative results show that our method outperforms popular regularization approaches in accurate geometry reconstructi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20102;&#24322;&#36136;&#24615;&#21407;&#21017;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#23545;&#33410;&#28857;&#19978;&#30340;&#20449;&#24687;&#27969;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#22522;&#20110;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#22312;&#22788;&#29702;&#24322;&#36136;&#24615;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16780</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#31070;&#32463;&#23545;&#27969;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Convection-Diffusion with Heterophily. (arXiv:2305.16780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20102;&#24322;&#36136;&#24615;&#21407;&#21017;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#23545;&#33410;&#28857;&#19978;&#30340;&#20449;&#24687;&#27969;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#22522;&#20110;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#22312;&#22788;&#29702;&#24322;&#36136;&#24615;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#21516;&#36136;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#24322;&#36136;&#24615;&#22270;&#30340;&#24615;&#33021;&#34920;&#29616;&#36739;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#65288;CDE&#65289;&#23545;&#33410;&#28857;&#19978;&#30340;&#20449;&#24687;&#27969;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#24322;&#36136;&#24615;&#21407;&#21017;&#12290;&#36825;&#20351;&#24471;CDE&#33021;&#22815;&#32771;&#34385;&#21040;&#22522;&#20110;&#21516;&#36136;&#24615;&#30340;&#20449;&#24687;&#25193;&#25955;&#21644;&#22522;&#20110;&#24322;&#36136;&#24615;&#30340;&#20449;&#24687;&#8220;&#23545;&#27969;&#8221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#38024;&#23545;&#24322;&#36136;&#24615;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#33021;&#22815;&#23454;&#29616;&#31454;&#20105;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zknus/Graph-Diffusion-CDE} &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown promising results across various graph learning tasks, but they often assume homophily, which can result in poor performance on heterophilic graphs. The connected nodes are likely to be from different classes or have dissimilar features on heterophilic graphs. In this paper, we propose a novel GNN that incorporates the principle of heterophily by modeling the flow of information on nodes using the convection-diffusion equation (CDE). This allows the CDE to take into account both the diffusion of information due to homophily and the ``convection'' of information due to heterophily. We conduct extensive experiments, which suggest that our framework can achieve competitive performance on node classification tasks for heterophilic graphs, compared to the state-of-the-art methods. The code is available at \url{https://github.com/zknus/Graph-Diffusion-CDE}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;HumBert&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#23545;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#30340;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.16756</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#23454;&#29616;&#21253;&#23481;&#21644;&#20559;&#35265;&#24863;&#30693;&#30340;&#20154;&#36947;&#20027;&#20041;&#21709;&#24212;&#20837;&#21475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;HumBert&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#23545;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#30340;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#36947;&#20027;&#20041;&#21361;&#26426;&#26399;&#38388;&#65292;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#24773;&#20917;&#20998;&#26512;&#23545;&#20110;&#39640;&#25928;&#22320;&#25552;&#20379;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20154;&#36947;&#20027;&#20041;&#21407;&#21017;&#21644;&#19981;&#30041;&#20219;&#20309;&#20154;&#33853;&#21518;&#21407;&#21017;&#30340;&#22522;&#30784;&#12290;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#21487;&#20197;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#36825;&#31181;&#25968;&#25454;&#20998;&#26512;&#65292;&#20363;&#22914;&#65292;&#25353;&#29031;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#23545;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#24494;&#35843;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#23454;&#29616;&#65292;&#28041;&#21450;&#19968;&#20123;&#23454;&#36341;&#21644;&#36947;&#24503;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#21644;&#22797;&#26434;&#23376;&#39046;&#22495;&#19978;&#30340;&#25928;&#26524;&#19981;&#20339;&#20197;&#21450;&#31038;&#20250;&#20559;&#35265;&#21644;&#19981;&#33391;&#20851;&#32852;&#30340;&#32534;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807; (1) &#24341;&#20837;&#19968;&#20010;&#36866;&#21512;&#20154;&#36947;&#20027;&#20041;&#20998;&#26512;&#26694;&#26550;&#30340;&#26032;&#26550;&#26500;&#65292;(2) &#21019;&#24314;&#21644;&#21457;&#24067;&#19968;&#20010;&#26032;&#30340;&#20154;&#36947;&#20027;&#20041;&#29305;&#23450; LLM&#65292;&#31216;&#20026; HumBert&#65292;&#24182;&#19988; (3) &#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the Leave No One Behind (LNOB) principle. This data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. However, approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. In this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. We approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific LLM called HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38543;&#26426;&#24120;&#25968;&#28145;&#24230;&#32593;&#32476;&#30340;PTAS&#26041;&#27861;&#65292;&#23545;&#20110;&#20219;&#20309;&#22266;&#23450;&#35823;&#24046;&#21644;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16508</link><description>&lt;p&gt;
&#22823;&#37096;&#20998;&#31070;&#32463;&#32593;&#32476;&#20960;&#20046;&#26159;&#21487;&#23398;&#20064;&#30340;
&lt;/p&gt;
&lt;p&gt;
Most Neural Networks Are Almost Learnable. (arXiv:2305.16508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38543;&#26426;&#24120;&#25968;&#28145;&#24230;&#32593;&#32476;&#30340;PTAS&#26041;&#27861;&#65292;&#23545;&#20110;&#20219;&#20309;&#22266;&#23450;&#35823;&#24046;&#21644;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;PTAS&#26469;&#23398;&#20064;&#38543;&#26426;&#24120;&#25968;&#28145;&#24230;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#22266;&#23450;&#30340;$\epsilon&gt;0$&#21644;&#28145;&#24230;$i$&#65292;&#23384;&#22312;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#23545;&#20110;$\sqrt{d} \cdot \mathbb{S}^{d-1}$&#19978;&#30340;&#20219;&#20309;&#20998;&#24067;&#65292;&#23398;&#20064;&#38543;&#26426;Xavier&#32593;&#32476;&#30340;&#28145;&#24230;$i$&#65292;&#35823;&#24046;&#20026;$\epsilon$&#12290;&#35813;&#31639;&#27861;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$&#65292;&#20854;&#20013;$\bar d$&#26159;&#32593;&#32476;&#30340;&#22823;&#23567;&#12290;&#23545;&#20110;&#26576;&#20123;&#31867;&#20284;&#20110;Sigmoid&#21644;ReLU&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20197;&#23558;&#35823;&#24046;&#30028;&#38480;&#25913;&#36827;&#20026;$(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#31181;&#20960;&#20046;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#23398;&#20064;&#24120;&#25968;&#28145;&#24230;&#38543;&#26426;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a PTAS for learning random constant-depth networks. We show that for any fixed $\epsilon&gt;0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\epsilon$. The algorithm runs in time and sample complexity of $(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16358</link><description>&lt;p&gt;
&#24102;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16358
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26435;&#37325;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#26641;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#22810;&#20010;&#36830;&#36890;&#20998;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20197;&#23454;&#29616;&#24179;&#28369;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#27969;&#27700;&#32447;&#20013;&#21253;&#21547;&#32858;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20960;&#20309;&#29615;&#22659;&#19979;&#20063;&#33021;&#33391;&#22909;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#21046;&#23450;&#20102;&#19968;&#20010;&#29305;&#21035;&#30340;&#25439;&#22833;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#37096;&#20998;&#32858;&#31867;&#25968;&#25454;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15703</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#22909;&#22788;&#65306;&#23567;&#25439;&#22833;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15703
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#26524;&#65292;&#20294;&#20854;&#20309;&#26102;&#20309;&#22320;&#26377;&#30410;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#22238;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#25105;&#20204;&#30340;&#36793;&#30028;&#20250;&#27604;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#26356;&#24378;&#12290;&#20316;&#20026;&#28909;&#36523;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#25104;&#26412;&#20998;&#24067;&#20250;&#22312;&#24773;&#22659;&#23637;&#24320;&#65288;CB&#65289;&#20013;&#23548;&#33268;&#23567;&#25439;&#22833;&#21518;&#24724;&#36793;&#30028;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#24335;CB&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#22312;&#23454;&#35777;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#22312;&#32447;RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#29256;&#26412;&#31354;&#38388;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#34920;&#26684;MDP&#20013;&#23454;&#29616;&#20102;&#23567;&#25439;&#22833;&#21518;&#24724;&#65292;&#21516;&#26102;&#22312;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#20139;&#26377;&#23567;&#25439;&#22833;PAC&#36793;&#30028;&#12290;&#20197;&#31867;&#20284;&#30340;&#35265;&#35299;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#31163;&#32447;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
&lt;/p&gt;</description></item><item><title>SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;</title><link>http://arxiv.org/abs/2305.15486</link><description>&lt;p&gt;
SPRING: GPT-4&#36890;&#36807;&#23398;&#20064;&#35770;&#25991;&#21644;&#25512;&#29702;&#22312;&#28216;&#25103;&#20013;&#34920;&#29616;&#36229;&#36807;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. (arXiv:2305.15486v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15486
&lt;/p&gt;
&lt;p&gt;
SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#30001;&#20110;&#20854;&#22810;&#20219;&#21153;&#12289;&#28145;&#24230;&#25506;&#32034;&#21644;&#30446;&#26631;&#20248;&#20808;&#32423;&#35201;&#27714;&#65292;&#23545;AI&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#28216;&#25103;&#26041;&#38754;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#22312;&#20687;Crafter&#25110;Minecraft&#36825;&#26679;&#22797;&#26434;&#30340;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SPRING&#65292;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;LaTeX&#28304;&#20316;&#20026;&#28216;&#25103;&#35821;&#22659;&#21644;&#20195;&#29702;&#24403;&#21069;&#35266;&#23519;&#30340;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;SPRING&#26694;&#26550;&#21033;&#29992;&#20855;&#26377;&#28216;&#25103;&#30456;&#20851;&#38382;&#39064;&#30340;&#23450;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20316;&#20026;&#33410;&#28857;&#21644;&#20381;&#36182;&#20851;&#31995;&#20316;&#20026;&#36793;&#12290;&#36890;&#36807;&#25353;&#25299;&#25169;&#39034;&#24207;&#36941;&#21382;DAG&#24182;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#30340;LLM&#21709;&#24212;&#26469;&#30830;&#23450;&#22312;&#29615;&#22659;&#20013;&#37319;&#21462;&#30340;&#26368;&#20248;&#34892;&#21160;&#65292;LLM&#23545;&#26368;&#32456;&#33410;&#28857;&#30340;&#31572;&#26696;&#30452;&#25509;&#36716;&#21270;&#20026;&#29615;&#22659;&#34892;&#21160;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;
&lt;/p&gt;
&lt;p&gt;
Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.15151</link><description>&lt;p&gt;
&#30693;&#35782;&#35774;&#35745;&#65306;&#36890;&#36807;&#30693;&#35782;&#25552;&#28860;&#25512;&#21160;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#65292;&#23547;&#25214;&#25240;&#21472;&#20026;&#25152;&#26399;&#26395;&#32467;&#26500;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#24050;&#32463;&#21462;&#24471;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#30053;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#37325;&#35201;&#24615;&#65292;&#26410;&#33021;&#35206;&#30422;&#24191;&#27867;&#30340;&#34507;&#30333;&#36136;&#31354;&#38388;&#65292;&#24182;&#19988;&#27809;&#26377;&#34701;&#20837;&#24120;&#35265;&#30340;&#34507;&#30333;&#36136;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#26469;&#33410;&#30465;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;CATH&#12289;TS50&#21644;TS500&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#30693;&#35782;&#35774;&#35745;&#26041;&#27861;&#22312;CATH&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;PiFold&#26041;&#27861;&#32422;9&#65285;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30693;&#35782;&#35774;&#35745;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20219;&#24847;&#20998;&#36776;&#29575;&#27668;&#20505;&#25968;&#25454;&#38477;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#37319;&#26679;&#35757;&#32451;&#65292;&#24182;&#33021;&#23558;&#20854;&#36755;&#20837;&#38646;&#26679;&#26412;&#38477;&#23610;&#24230;&#21040;&#20219;&#24847;&#26410;&#35265;&#39640;&#20998;&#36776;&#29575;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#38477;&#23610;&#24230;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14452</link><description>&lt;p&gt;
&#20219;&#24847;&#20998;&#36776;&#29575;&#27668;&#20505;&#25968;&#25454;&#38477;&#23610;&#24230;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operators for Arbitrary Resolution Climate Data Downscaling. (arXiv:2305.14452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20219;&#24847;&#20998;&#36776;&#29575;&#27668;&#20505;&#25968;&#25454;&#38477;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#37319;&#26679;&#35757;&#32451;&#65292;&#24182;&#33021;&#23558;&#20854;&#36755;&#20837;&#38646;&#26679;&#26412;&#38477;&#23610;&#24230;&#21040;&#20219;&#24847;&#26410;&#35265;&#39640;&#20998;&#36776;&#29575;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#38477;&#23610;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#27169;&#25311;&#22312;&#24341;&#23548;&#25105;&#20204;&#20102;&#35299;&#27668;&#20505;&#21464;&#21270;&#21644;&#24212;&#23545;&#20854;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20197;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#26469;&#35299;&#26512;&#22797;&#26434;&#30340;&#27668;&#20505;&#36807;&#31243;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#21152;&#36895;&#27668;&#20505;&#27169;&#25311;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#29992;&#20110;&#20174;&#24555;&#36895;&#36816;&#34892;&#30340;&#20302;&#20998;&#36776;&#29575;&#27169;&#25311;&#20013;&#38477;&#23610;&#24230;&#27668;&#20505;&#21464;&#37327;&#65292;&#20294;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#38590;&#20197;&#33719;&#24471;&#25110;&#32570;&#20047;&#65292;&#22823;&#22823;&#38480;&#21046;&#20102;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#38477;&#23610;&#24230;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#23567;&#30340;&#19978;&#37319;&#26679;&#22240;&#23376;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#21487;&#20197;&#23558;&#20854;&#36755;&#20837;&#38646;&#26679;&#26412;&#38477;&#23610;&#24230;&#21040;&#20219;&#24847;&#26410;&#35265;&#39640;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#38477;&#23610;&#24230;&#27169;&#22411;&#22312;ERA5&#27668;&#20505;&#27169;&#22411;&#25968;&#25454;&#21644;Navier-Stokes&#26041;&#31243;&#35299;&#27861;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#26631;&#20934;&#21333;&#20998;&#36776;&#29575;&#38477;&#23610;&#24230;&#21644;&#38646;&#26679;&#26412;&#25512;&#24191;&#21040;&#26356;&#39640;&#19978;&#37319;&#26679;&#22240;&#23376;&#26041;&#38754;&#65292;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21367;&#31215;&#21644;&#29983;&#25104;&#23545;&#25239;&#24335;&#38477;&#23610;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate simulations are essential in guiding our understanding of climate change and responding to its effects. However, it is computationally expensive to resolve complex climate processes at high spatial resolution. As one way to speed up climate simulations, neural networks have been used to downscale climate variables from fast-running low-resolution simulations, but high-resolution training data are often unobtainable or scarce, greatly limiting accuracy. In this work, we propose a downscaling method based on the Fourier neural operator. It trains with data of a small upsampling factor and then can zero-shot downscale its input to arbitrary unseen high resolution. Evaluated both on ERA5 climate model data and on the Navier-Stokes equation solution data, our downscaling model significantly outperforms state-of-the-art convolutional and generative adversarial downscaling models, both in standard single-resolution downscaling and in zero-shot generalization to higher upsampling facto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VIPER&#31639;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#30340;&#19987;&#23478;&#32423;&#25511;&#21046;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14343</link><description>&lt;p&gt;
&#20316;&#20026;&#22870;&#21169;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Video Prediction Models as Rewards for Reinforcement Learning. (arXiv:2305.14343v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VIPER&#31639;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#30340;&#19987;&#23478;&#32423;&#25511;&#21046;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21046;&#23450;&#35753;&#20195;&#29702;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#30340;&#22870;&#21169;&#20449;&#21495;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#20174;&#24191;&#27867;&#21487;&#29992;&#20110;&#20114;&#32852;&#32593;&#19978;&#30340;&#26080;&#26631;&#27880;&#35270;&#39057;&#20013;&#25552;&#21462;&#34892;&#20026;&#20559;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Video Prediction Rewards (VIPER)&#65292;&#36825;&#31181;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20316;&#20026;&#19981;&#38656;&#35201;&#34892;&#20026;&#24178;&#39044;&#30340;&#24378;&#21270;&#23398;&#20064;&#22870;&#21169;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#19987;&#23478;&#35270;&#39057;&#19978;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;Transformer&#65292;&#28982;&#21518;&#23558;&#35270;&#39057;&#39044;&#27979;&#21487;&#33021;&#24615;&#29992;&#20316;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;VIPER&#20351;&#24471;&#22312;DMC&#12289;Atari&#21644;RLBench&#20219;&#21153;&#31561;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#20869;&#65292;&#22312;&#27809;&#26377;&#32534;&#31243;&#20219;&#21153;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19987;&#23478;&#32423;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20026;&#27809;&#26377;&#19987;&#23478;&#25968;&#25454;&#21487;&#29992;&#30340;&#20998;&#24067;&#22806;&#29615;&#22659;&#23548;&#20986;&#22870;&#21169;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#26700;&#38754;&#25805;&#32437;&#30340;&#36328;&#20307;&#29616;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#20855;&#26377;&#20280;&#32553;&#24615;&#30340;&#22870;&#21169;&#21046;&#23450;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning. A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#39640;&#26031;-&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#24615;&#12290;&#23545;&#20110;&#20174;&#39640;&#26031;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#21452;&#32447;&#24615;&#26680;&#30340;SVGD&#21160;&#24577;&#23558;&#20445;&#25345;&#39640;&#26031;&#29366;&#24577;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21576;&#29616;&#20986;&#24378;&#23545;&#25968;&#20985;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#22343;&#22330;&#39640;&#26031;-SVGD&#21160;&#24577;&#20250;&#32447;&#24615;&#25910;&#25947;&#20110;KL&#25955;&#24230;&#19979;&#26368;&#25509;&#36817;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#26377;&#38480;&#31890;&#23376;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#23545;&#22343;&#22330;&#26497;&#38480;&#30340;&#26102;&#38388;&#24494;&#27493;&#19968;&#33268;&#25910;&#25947;&#20197;&#21450;&#32447;&#24615;&#25910;&#25947;&#33267;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.14076</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#26031;-&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#24615;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent. (arXiv:2305.14076v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#39640;&#26031;-&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#24615;&#12290;&#23545;&#20110;&#20174;&#39640;&#26031;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#21452;&#32447;&#24615;&#26680;&#30340;SVGD&#21160;&#24577;&#23558;&#20445;&#25345;&#39640;&#26031;&#29366;&#24577;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21576;&#29616;&#20986;&#24378;&#23545;&#25968;&#20985;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#22343;&#22330;&#39640;&#26031;-SVGD&#21160;&#24577;&#20250;&#32447;&#24615;&#25910;&#25947;&#20110;KL&#25955;&#24230;&#19979;&#26368;&#25509;&#36817;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#26377;&#38480;&#31890;&#23376;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#23545;&#22343;&#22330;&#26497;&#38480;&#30340;&#26102;&#38388;&#24494;&#27493;&#19968;&#33268;&#25910;&#25947;&#20197;&#21450;&#32447;&#24615;&#25910;&#25947;&#33267;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD)&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#22522;&#20110;&#31890;&#23376;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#12290;&#23613;&#31649;&#20854;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#29702;&#35299;SVGD&#30340;&#29702;&#35770;&#23646;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#20174;&#39640;&#26031;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#21452;&#32447;&#24615;&#26680;&#30340;SVGD&#21160;&#24577;&#23558;&#20445;&#25345;&#39640;&#26031;&#29366;&#24577;&#12290;&#21463;&#27492;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#21452;&#32447;&#24615;&#26680;&#23558;SVGD&#25237;&#24433;&#21040;&#39640;&#26031;&#20998;&#24067;&#26063;&#20013;&#65292;&#21363;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029; (GVI) &#19982; SVGD&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#22343;&#22330; PDE &#21644;&#31163;&#25955;&#31890;&#23376;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22270;&#20687;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21576;&#29616;&#20986;&#24378;&#23545;&#25968;&#20985;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#22343;&#22330;&#39640;&#26031;-SVGD&#21160;&#24577;&#20250;&#32447;&#24615;&#25910;&#25947;&#20110;KL&#25955;&#24230;&#19979;&#26368;&#25509;&#36817;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#26377;&#38480;&#31890;&#23376;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#23545;&#22343;&#22330;&#26497;&#38480;&#30340;&#26102;&#38388;&#24494;&#27493;&#19968;&#33268;&#25910;&#25947;&#20197;&#21450;&#32447;&#24615;&#25910;&#25947;&#33267;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#19968;&#20010;&#26032;&#30340;&#20195;&#25968;&#24658;&#31561;&#24335;&#65292;&#35813;&#31561;&#24335;&#23558;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#30340;&#36153;&#24076;&#23572;&#20449;&#24687;&#30697;&#38453;&#19982;&#31890;&#23376;&#22343;&#21248;&#20998;&#24067;&#30340;&#36153;&#24076;&#23572;&#20449;&#24687;&#30697;&#38453;&#30456;&#20851;&#32852;&#12290;&#36825;&#20010;&#31561;&#24335;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#36879;&#35270; GVI with SVGD &#22312;&#22343;&#22330;&#21644;&#31890;&#23376;&#35774;&#32622;&#20013;&#30340;&#21160;&#24577;&#24615;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) is a nonparametric particle-based deterministic sampling algorithm. Despite its wide usage, understanding the theoretical properties of SVGD has remained a challenging problem. For sampling from a Gaussian target, the SVGD dynamics with a bilinear kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, i.e., SVGD projected to the family of Gaussian distributions via the bilinear kernel, or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete picture by considering both the mean-field PDE and discrete particle systems. When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics is proven to converge linearly to the Gaussian distribution closest to the target in KL divergence. In the finite-particle setting, there is both uniform in time convergence to the mean-field limit and linear convergence in ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#22312;&#19981;&#24433;&#21709;&#20581;&#24247;&#24739;&#32773;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13108</link><description>&lt;p&gt;
&#36890;&#36807;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#36827;&#34892;&#22833;&#35821;&#30151;&#35821;&#38899;&#30340;&#26080;&#20559;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test. (arXiv:2305.13108v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#22312;&#19981;&#24433;&#21709;&#20581;&#24247;&#24739;&#32773;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#26159;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#30001;&#20110;ERM&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#30340;&#24179;&#22343;&#34920;&#29616;&#32780;&#19981;&#32771;&#34385;&#19968;&#20010;&#32676;&#20307;&#65292;&#20363;&#22914;&#20581;&#24247;&#25110;&#22833;&#35821;&#30151;&#24739;&#32773;&#65292;&#22240;&#27492;ASR&#31995;&#32479;&#26080;&#27861;&#35782;&#21035;&#36328;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#23548;&#33268;ASR&#31995;&#32479;&#23384;&#22312;&#20559;&#24046;&#19988;&#20854;&#32676;&#20307;&#24615;&#33021;&#24046;&#24322;&#20005;&#37325;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#32676;&#20307;&#31283;&#20581;&#24615;&#65292;&#38024;&#23545;&#22833;&#35821;&#30151;&#24739;&#32773;&#36827;&#34892;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#12290; Re-SAT&#31995;&#32479;&#22320;&#34913;&#37327;&#25152;&#32473;&#25968;&#25454;&#26679;&#26412;&#30340;&#21435;&#20559;&#24110;&#21161;&#24615;&#65292;&#24182;&#36890;&#36807;&#21435;&#20559;&#24110;&#21161;&#24615;&#21152;&#26435;&#26469;&#32531;&#35299;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292; Re-SAT&#26377;&#21161;&#20110;&#25913;&#21892;&#22833;&#35821;&#30151;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20581;&#24247;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition systems based on deep learning are mainly trained under empirical risk minimization (ERM). Since ERM utilizes the averaged performance on the data samples regardless of a group such as healthy or dysarthric speakers, ASR systems are unaware of the performance disparities across the groups. This results in biased ASR systems whose performance differences among groups are severe. In this study, we aim to improve the ASR system in terms of group robustness for dysarthric speakers. To achieve our goal, we present a novel approach, sample reweighting with sample affinity test (Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given data sample and then mitigates the bias by debiasing helpfulness-based sample reweighting. Experimental results demonstrate that Re-SAT contributes to improved ASR performance on dysarthric speech without performance degradation on healthy speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20999;&#32447;&#31354;&#38388;&#20013;&#36827;&#34892;&#20219;&#21153;&#31639;&#26415;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#26435;&#37325;&#20998;&#31163;&#26159;&#20854;&#26377;&#25928;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#20219;&#21153;&#31639;&#26415;&#25216;&#26415;Tan&#65292;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12827</link><description>&lt;p&gt;
&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#20219;&#21153;&#31639;&#26415;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#36827;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. (arXiv:2305.12827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20999;&#32447;&#31354;&#38388;&#20013;&#36827;&#34892;&#20219;&#21153;&#31639;&#26415;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#26435;&#37325;&#20998;&#31163;&#26159;&#20854;&#26377;&#25928;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#20219;&#21153;&#31639;&#26415;&#25216;&#26415;Tan&#65292;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20219;&#21153;&#31639;&#26415;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#32534;&#36753;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;&#36890;&#36807;&#28155;&#21152;&#19981;&#21516;&#20219;&#21153;&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#32780;&#25269;&#28040;&#23427;&#20204;&#21017;&#20250;&#23548;&#33268;&#20219;&#21153;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20219;&#21153;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20854;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#20219;&#21153;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#34920;&#26126;&#26435;&#37325;&#20998;&#31163;&#26159;&#20351;&#20854;&#26377;&#25928;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36825;&#31181;&#23646;&#24615;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20986;&#29616;&#65292;&#24182;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#19981;&#21516;&#26041;&#21521;&#19978;&#20135;&#29983;&#65292;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#27835;&#29702;&#29420;&#31435;&#30340;&#23616;&#37096;&#21306;&#22495;&#26102;&#20307;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#23558;&#27169;&#22411;&#32447;&#24615;&#21270;&#20197;&#22312;&#20999;&#32447;&#31354;&#38388;&#20013;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#25918;&#22823;&#26435;&#37325;&#20998;&#31163;&#12290;&#36825;&#23548;&#33268;&#22312;&#22810;&#20010;&#20219;&#21153;&#31639;&#27861;&#22522;&#20934;&#21644;&#19981;&#21516;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#20219;&#21153;&#31639;&#26415;&#25216;&#26415;Tan&#65292;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#23558;&#20219;&#21153;&#26435;&#37325;&#22686;&#37327;&#25237;&#24433;&#21040;&#20999;&#32447;&#31354;&#38388;&#19978;&#30340;&#26032;&#25237;&#24433;&#65292;&#30830;&#20445;&#32534;&#36753;&#30340;&#26435;&#37325;&#20445;&#25345;&#25509;&#36817;&#39044;&#35757;&#32451;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20219;&#21153;&#31639;&#26415;&#30340;&#24037;&#20316;&#21407;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#25351;&#20986;&#26435;&#37325;&#20998;&#31163;&#26159;&#20351;&#20854;&#25104;&#20026;&#21487;&#33021;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task arithmetic has recently emerged as a cost-effective and scalable approach to edit pre-trained models directly in weight space: By adding the fine-tuned weights of different tasks, the model's performance can be improved on these tasks, while negating them leads to task forgetting. Yet, our understanding of the effectiveness of task arithmetic and its underlying principles remains limited. We present a comprehensive study of task arithmetic in vision-language models and show that weight disentanglement is the crucial factor that makes it effective. This property arises during pre-training and manifests when distinct directions in weight space govern separate, localized regions in function space associated with the tasks. Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement. This leads to substantial performance improvements across multiple task arithmetic benchmarks and diverse models. Building on these findings, we pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#26681;&#25454;cGAN&#30340;&#21028;&#21035;&#22120;&#25968;&#25454;&#35782;&#21035;&#20986;&#26368;&#25509;&#36817;&#30446;&#26631;&#30340;&#29616;&#26377;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#22238;&#25918;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#30446;&#26631;&#27169;&#24335;&#30340;cGAN&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11400</link><description>&lt;p&gt;
&#38754;&#21521;&#26377;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#26681;&#25454;cGAN&#30340;&#21028;&#21035;&#22120;&#25968;&#25454;&#35782;&#21035;&#20986;&#26368;&#25509;&#36817;&#30446;&#26631;&#30340;&#29616;&#26377;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#22238;&#25918;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#30446;&#26631;&#27169;&#24335;&#30340;cGAN&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#24517;&#39035;&#23398;&#20064;&#30446;&#26631;&#27169;&#24335;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#20165;&#20351;&#29992;&#26377;&#38480;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#38024;&#23545;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#27169;&#24335;&#20146;&#21644;&#21147;&#37327;&#24230;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#23436;&#20840;&#22522;&#20110;cGAN&#30340;&#21028;&#21035;&#22120;&#65292;&#21487;&#20197;&#35782;&#21035;&#26368;&#25509;&#36817;&#30446;&#26631;&#30340;&#29616;&#26377;&#27169;&#24335;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;&#22522;&#20110;&#26368;&#25509;&#36817;&#27169;&#24335;&#30340;&#21152;&#26435;&#26631;&#31614;&#26469;&#25193;&#23637;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#39044;&#38450;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;cGAN&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#28982;&#21518;&#36890;&#36807;&#22238;&#25918;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#30446;&#26631;&#27169;&#24335;&#30340;cGAN&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36234;&#20102;&#21508;&#31181;&#26631;&#20934;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In few-shot continual learning for generative models, a target mode must be learned with limited samples without adversely affecting the previously learned modes. In this paper, we propose a new continual learning approach for conditional generative adversarial networks (cGAN) based on a new mode-affinity measure for generative modeling. Our measure is entirely based on the cGAN's discriminator and can identify the existing modes that are most similar to the target. Subsequently, we expand the continual learning model by including the target mode using a weighted label derived from those of the closest modes. To prevent catastrophic forgetting, we first generate labeled data samples using the cGAN's generator, and then train the cGAN model for the target mode while memory replaying with the generated data. Our experimental results demonstrate the efficacy of our approach in improving the generation performance over the baselines and the state-of-the-art approaches for various standard 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10769</link><description>&lt;p&gt;
&#36861;&#36214;&#33976;&#39311;&#65306;&#21152;&#36895;&#37319;&#26679;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling. (arXiv:2305.10769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#36890;&#24120;&#38656;&#35201;&#25191;&#34892;&#22823;&#37327;&#30340;&#37319;&#26679;&#27493;&#39588;&#65292;&#36825;&#38459;&#30861;&#20102;&#23454;&#26102;&#26679;&#26412;&#21512;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;&#20256;&#32479;&#30340;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21152;&#36895;&#37319;&#26679;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#31163;&#25955;&#26102;&#38388;&#27493;&#39588;&#22330;&#26223;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#35838;&#31243;&#25165;&#33021;&#23454;&#29616;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36861;&#36214;&#33976;&#39311;&#65288;CUD&#65289;&#65292;&#23427;&#40723;&#21169;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#8220;&#36861;&#36214;&#8221;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CUD&#35843;&#25972;&#20102;&#21407;&#22987;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#20351;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#21644;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#23545;&#40784;&#65292;&#21033;&#29992;&#22522;&#20110;&#40857;&#26684;-&#24211;&#22612;&#30340;&#22810;&#27493;&#23545;&#40784;&#33976;&#39311;&#36827;&#34892;&#31934;&#30830;&#30340;ODE&#20272;&#35745;&#65292;&#21516;&#26102;&#38450;&#27490;&#24322;&#27493;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probability Models (DPMs) have made impressive advancements in various machine learning domains. However, achieving high-quality synthetic samples typically involves performing a large number of sampling steps, which impedes the possibility of real-time sample synthesis. Traditional accelerated sampling algorithms via knowledge distillation rely on pre-trained model weights and discrete time step scenarios, necessitating additional training sessions to achieve their goals. To address these issues, we propose the Catch-Up Distillation (CUD), which encourages the current moment output of the velocity estimation model ``catch up'' with its previous moment output. Specifically, CUD adjusts the original Ordinary Differential Equation (ODE) training objective to align the current moment output with both the ground truth label and the previous moment output, utilizing Runge-Kutta-based multi-step alignment distillation for precise ODE estimation while preventing asynchronous updates
&lt;/p&gt;</description></item><item><title>RelationMatch&#26159;&#19968;&#31181;&#21033;&#29992;&#30697;&#38453;&#20132;&#21449;&#29109;&#65288;MCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21305;&#37197;&#25209;&#20869;&#20851;&#31995;&#65292;&#26377;&#25928;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10397</link><description>&lt;p&gt;
RelationMatch&#65306;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#25209;&#20869;&#20851;&#31995;&#21305;&#37197;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
RelationMatch: Matching In-batch Relationships for Semi-supervised Learning. (arXiv:2305.10397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10397
&lt;/p&gt;
&lt;p&gt;
RelationMatch&#26159;&#19968;&#31181;&#21033;&#29992;&#30697;&#38453;&#20132;&#21449;&#29109;&#65288;MCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21305;&#37197;&#25209;&#20869;&#20851;&#31995;&#65292;&#26377;&#25928;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#38598;&#20013;&#22312;&#26469;&#33258;&#30456;&#21516;&#26469;&#28304;&#30340;&#25104;&#23545;&#25968;&#25454;&#28857;&#30340;&#39044;&#27979;&#23545;&#20934;&#19978;&#65292;&#24182;&#24573;&#30053;&#20102;&#27599;&#20010;&#25209;&#27425;&#20869;&#30340;&#28857;&#38388;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;RelationMatch&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#30697;&#38453;&#20132;&#21449;&#29109;&#65288;MCE&#65289;&#25439;&#22833;&#20989;&#25968;&#26469;&#21457;&#25496;&#25209;&#20869;&#20851;&#31995;&#12290;&#36890;&#36807;&#24212;&#29992;MCE&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;FixMatch&#21644;FlexMatch&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20165;&#20351;&#29992;40&#20010;&#26631;&#31614;&#30340;STL-10&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30456;&#23545;&#20110;FlexMatch&#26377;15.21&#65285;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;MCE&#24212;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning has achieved notable success by leveraging very few labeled data and exploiting the wealth of information derived from unlabeled data. However, existing algorithms usually focus on aligning predictions on paired data points augmented from an identical source, and overlook the inter-point relationships within each batch. This paper introduces a novel method, RelationMatch, which exploits in-batch relationships with a matrix cross-entropy (MCE) loss function. Through the application of MCE, our proposed method consistently surpasses the performance of established state-of-the-art methods, such as FixMatch and FlexMatch, across a variety of vision datasets. Notably, we observed a substantial enhancement of 15.21% in accuracy over FlexMatch on the STL-10 dataset using only 40 labels. Moreover, we apply MCE to supervised learning scenarios, and observe consistent improvements as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.09782</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35270;&#35273;&#38382;&#31572;&#31639;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29702;&#35299;&#24182;&#22238;&#31572;&#38382;&#39064;&#12290; VQA &#23545;&#35270;&#35273;&#21463;&#25439;&#32773;&#26377;&#24110;&#21161;&#65292;&#21487;&#29992;&#20110;&#23433;&#20840;&#30417;&#25511;&#31995;&#32479;&#21644;&#20174;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#22312;&#32447;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290; &#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23398;&#20064;&#38382;&#39064;&#30340;&#35821;&#20041;&#24182;&#25552;&#21462;&#25991;&#26412;&#29305;&#24449;&#12290; &#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#29992;&#20110;&#20197;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25152;&#38382;&#38382;&#39064;&#28041;&#21450;&#30340;&#29289;&#20307;&#30340;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#34920;&#31034;&#12290; &#27880;&#24847;&#21147;&#27169;&#22411;&#35797;&#22270;&#27169;&#20223;&#20154;&#31867;&#26681;&#25454;&#35821;&#22659;&#20851;&#27880;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#34892;&#20026;&#12290; &#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340; VQA &#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#29983;&#25104;&#25991;&#26412;&#35821;&#20041;&#65292;&#35782;&#21035;&#23545;&#35937;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) usesimage processing algorithms to process the image and natural language processing methods to understand and answer the question. VQA is helpful to a visually impaired person, can be used for the security surveillance system and online chatbots that learn from the web. It uses NLP methods to learn the semantic of the question and to derive the textual features. Computer vision techniques are used for generating image representation in such a way that they can identify the objects about which question is asked. The Attention model tries to mimic the human behavior of giving attention to a different region of an image according to our understanding of its context. This paper critically examines and reviews methods of VQA algorithm such as generation of semantics of text, identification of objects and answer classification techniques that use the co-attention approach.
&lt;/p&gt;</description></item><item><title>DUDE&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#22522;&#20934;&#27979;&#35797;&#24182;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;</title><link>http://arxiv.org/abs/2305.08455</link><description>&lt;p&gt;
&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#65288;DUDE&#65289;
&lt;/p&gt;
&lt;p&gt;
Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08455
&lt;/p&gt;
&lt;p&gt;
DUDE&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#22522;&#20934;&#27979;&#35797;&#24182;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21628;&#21505;&#25991;&#26723;AI&#31038;&#21306;&#37325;&#26032;&#35780;&#20272;&#24403;&#21069;&#30340;&#26041;&#27861;&#35770;&#65292;&#25317;&#25265;&#21019;&#24314;&#26356;&#23454;&#38469;&#21462;&#21521;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#65288;DUDE&#65289;&#26088;&#22312;&#32416;&#27491;&#22312;&#29702;&#35299;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#65288;VRD&#65289;&#26041;&#38754;&#30340;&#30740;&#31350;&#36827;&#23637;&#20572;&#28382;&#19981;&#21069;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#22810;&#34892;&#19994;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#39029;VRD&#30456;&#20851;&#30340;&#38382;&#39064;&#31867;&#22411;&#12289;&#31572;&#26696;&#21644;&#25991;&#26723;&#24067;&#23616;&#30340;&#21019;&#26032;&#65292;&#20855;&#26377;&#21508;&#31181;&#26469;&#28304;&#21644;&#26085;&#26399;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#35780;&#20272;&#35774;&#32622;&#26469;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#36825;&#20123;&#35774;&#32622;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#38656;&#35201;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#24378;&#22823;&#30340;&#27867;&#21270;&#21644;&#36866;&#24212;&#12290;DUDE&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#26356;&#23454;&#38469;&#12289;&#26356;&#38271;&#26399;&#30340;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#65292;&#24182;&#24076;&#26395;&#23427;&#20250;&#24341;&#39046;&#26410;&#26469;&#30340;&#25193;&#23637;&#21644;&#36129;&#29486;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35828;&#26126;&#20102;&#20197;&#19979;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05402</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20998;&#31867;&#26159;&#19968;&#39033;&#20851;&#38190;&#30340;&#12289;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#19968;&#23478;&#20027;&#35201;&#32593;&#32476;&#20844;&#21496;&#24050;&#32463;&#22312;&#20351;&#29992;&#30340;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#12290;&#22312;&#35813;&#27169;&#22411;&#26680;&#24515;&#20013;&#65292;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#25509;&#21463;&#20135;&#21697;&#26631;&#39064;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20174;&#25968;&#21315;&#20010;&#21487;&#29992;&#20505;&#36873;&#39033;&#20013;&#36755;&#20986;&#26368;&#21512;&#36866;&#30340;&#31867;&#21035;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31867;&#20284;&#29289;&#21697;&#26631;&#31614;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292;&#26631;&#39064;&#20013;&#20851;&#20110;&#39068;&#33394;&#25110;&#23610;&#23544;&#30340;&#23567;&#21464;&#21270;&#65292;&#20250;&#23545;&#27169;&#22411;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#20250;&#23545;&#19979;&#28216;&#30340;&#25512;&#33616;&#25110;&#25628;&#32034;&#24212;&#29992;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.  To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05368</link><description>&lt;p&gt;
GNNs: &#21487;&#20197;&#26356;&#24378;&#12289;&#26356;&#26032;&#12289;&#26356;&#24555;
&lt;/p&gt;
&lt;p&gt;
GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#20174;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#36890;&#36807;&#38598;&#25104;&#37051;&#23621;&#33410;&#28857;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#34920;&#29616;&#20986;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#24615;&#33021;&#20250;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#32780;&#36880;&#28176;&#38477;&#20302;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;k&#36339;&#23376;&#22270;&#32858;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;&#28145;&#23618;GNN&#34920;&#29616;&#36880;&#28176;&#36864;&#21270;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#32858;&#21512;&#23376;&#22270;&#30340;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#27531;&#24046;&#30340;GNN&#23454;&#38469;&#19978;&#21033;&#29992;&#20102;1&#21040;k&#36339;&#23376;&#22270;&#32858;&#21512;&#32467;&#26524;&#26469;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#35777;&#26126;&#20854;&#27604;&#20043;&#21069;&#30340;&#27531;&#24046;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#21487;&#20197;&#21033;&#29992;1&#21040;k&#36339;&#36291;&#23376;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#21152;&#26435;&#22240;&#26524; DAGs&#30340;&#26032;&#24230;&#37327;&#21644;&#25628;&#32034;&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#30340;&#22240;&#26524;&#22270;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#25429;&#25417;&#25628;&#32034;&#31639;&#27861;&#30340;&#26368;&#22351;&#24178;&#39044;&#25104;&#26412;&#65292;&#24182;&#25552;&#20379;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#23454;&#29616;&#23545;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2305.04445</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#26435;&#22240;&#26524; DAG &#30340;&#26032;&#24230;&#37327;&#21644;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
New metrics and search algorithms for weighted causal DAGs. (arXiv:2305.04445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#21152;&#26435;&#22240;&#26524; DAGs&#30340;&#26032;&#24230;&#37327;&#21644;&#25628;&#32034;&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#30340;&#22240;&#26524;&#22270;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#25429;&#25417;&#25628;&#32034;&#31639;&#27861;&#30340;&#26368;&#22351;&#24178;&#39044;&#25104;&#26412;&#65292;&#24182;&#25552;&#20379;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#23454;&#29616;&#23545;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#26102;&#65292;&#21482;&#33021;&#24674;&#22797;&#21040;&#19968;&#20010;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#30340;&#22240;&#26524;&#22270;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#20551;&#35774;&#25110;&#24178;&#39044;&#25968;&#25454;&#26469;&#23436;&#25104;&#24674;&#22797;&#12290;&#26412;&#25991;&#22312;&#19968;&#20123;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#36890;&#36807;&#33410;&#28857;&#30456;&#20851;&#24178;&#39044;&#25104;&#26412;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#65292;&#30740;&#31350;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#27809;&#26377;&#31639;&#27861;&#33021;&#22815;&#27604;&#39564;&#35777;&#27425;&#25968;&#30340;&#39034;&#24207;&#26356;&#22909;&#22320;&#23454;&#29616;&#28176;&#36817;&#20445;&#35777;&#65292;&#39564;&#35777;&#27425;&#25968;&#26159;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#30340;&#19968;&#20010;&#25104;&#29087;&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#36127;&#38754;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#25429;&#25417;&#20219;&#20309;&#25628;&#32034;&#31639;&#27861;&#26368;&#22351;&#24178;&#39044;&#25104;&#26412;&#30340;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#36825;&#20010;&#26032;&#22522;&#20934;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#37117;&#33021;&#23454;&#29616;&#23545;&#25968;&#36924;&#36817;&#65306;&#21407;&#23376;&#12289;&#26377;&#30028;&#22823;&#23567;&#30340;&#24178;&#39044;&#21644;&#24191;&#20041;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recovering causal relationships from data is an important problem. Using observational data, one can typically only recover causal graphs up to a Markov equivalence class and additional assumptions or interventional data are needed for complete recovery. In this work, under some standard assumptions, we study causal graph discovery via adaptive interventions with node-dependent interventional costs. For this setting, we show that no algorithm can achieve an approximation guarantee that is asymptotically better than linear in the number of vertices with respect to the verification number; a well-established benchmark for adaptive search algorithms. Motivated by this negative result, we define a new benchmark that captures the worst-case interventional cost for any search algorithm. Furthermore, with respect to this new benchmark, we provide adaptive search algorithms that achieve logarithmic approximations under various settings: atomic, bounded size interventions and generalized cost o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.04111</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#24314;&#27169;&#19979;&#30340;&#39640;&#25928;&#21644;&#24230;&#25968;&#24341;&#23548;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23567;&#22270;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#29983;&#25104;&#21253;&#21547;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22270;&#24182;&#28385;&#36275;&#22270;&#32479;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#38543;&#26426;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#26368;&#32456;&#33719;&#24471;&#19968;&#24352;&#31354;&#30333;&#22270;&#12290;EDGE&#20165;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#12290;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#26356;&#23569;&#22320;&#36827;&#34892;&#36793;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;EDGE&#26126;&#30830;&#22320;&#20801;&#35768;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;&#23427;&#36824;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24322;&#27493;&#32858;&#21512;FEEL&#26426;&#21046;PAOTA&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#21644;&#35774;&#22791;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;FEEL&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#36890;&#36807;&#35843;&#25972;&#36793;&#32536;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#21151;&#29575;&#26469;&#26368;&#23567;&#21270;FEEL&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#19978;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#31934;&#24230;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#24555;&#20110;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#20256;&#32479;&#21516;&#27493;FEEL&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.04066</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#30340;&#21322;&#24322;&#27493;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Semi-Asynchronous Federated Edge Learning Mechanism via Over-the-air Computation. (arXiv:2305.04066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24322;&#27493;&#32858;&#21512;FEEL&#26426;&#21046;PAOTA&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#21644;&#35774;&#22791;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;FEEL&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#36890;&#36807;&#35843;&#25972;&#36793;&#32536;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#21151;&#29575;&#26469;&#26368;&#23567;&#21270;FEEL&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#19978;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#31934;&#24230;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#24555;&#20110;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#20256;&#32479;&#21516;&#27493;FEEL&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#35745;&#31639;&#26159;&#25552;&#39640;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#25928;&#29575;&#30340;&#26377;&#25928;&#20256;&#36755;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;FEEL&#31995;&#32479;&#36890;&#24120;&#22312;&#27599;&#20010;&#20840;&#23616;&#36718;&#27425;&#20013;&#37319;&#29992;&#20256;&#32479;&#30340;&#21516;&#27493;&#32858;&#21512;&#26426;&#21046;&#65292;&#32780;&#36825;&#20123;&#26426;&#21046;&#23481;&#26131;&#21463;&#21040;&#28382;&#21518;&#32773;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#21322;&#24322;&#27493;&#32858;&#21512;FEEL&#26426;&#21046;&#65288;PAOTA&#65289;&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#21644;&#35774;&#22791;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;FEEL&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#32771;&#34385;&#21040;&#26469;&#33258;&#36793;&#32536;&#35774;&#22791;&#27169;&#22411;&#26356;&#26032;&#30340;&#38472;&#26087;&#24615;&#21644;&#21457;&#25955;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#32858;&#21512;&#26399;&#35843;&#25972;&#36793;&#32536;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#21151;&#29575;&#26469;&#26368;&#23567;&#21270;FEEL&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#19978;&#30028;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#25509;&#36817;&#29702;&#24819;&#30340;&#23616;&#37096;SGD&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#30456;&#21516;&#30340;&#30446;&#26631;&#20934;&#30830;&#24230;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#30340;&#35757;&#32451;&#36895;&#24230;&#26174;&#30528;&#24555;&#20110;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#20256;&#32479;&#21516;&#27493;FEEL&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-the-air Computation (AirComp) has been demonstrated as an effective transmission scheme to boost the efficiency of federated edge learning (FEEL). However, existing FEEL systems with AirComp scheme often employ traditional synchronous aggregation mechanisms for local model aggregation in each global round, which suffer from the stragglers issues. In this paper, we propose a semi-asynchronous aggregation FEEL mechanism with AirComp scheme (PAOTA) to improve the training efficiency of the FEEL system in the case of significant heterogeneity in data and devices. Taking the staleness and divergence of model updates from edge devices into consideration, we minimize the convergence upper bound of the FEEL global model by adjusting the uplink transmit power of edge devices at each aggregation period. The simulation results demonstrate that our proposed algorithm achieves convergence performance close to that of the ideal Local SGD. Furthermore, with the same target accuracy, the training
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00909</link><description>&lt;p&gt;
&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;&#65306;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#22797;&#26434;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#20570;&#27861;&#36890;&#24120;&#26159;&#20808;&#27010;&#36848;&#19968;&#19979;&#25511;&#21046;&#27969;&#31243;&#65292;&#28982;&#21518;&#36845;&#20195;&#36827;&#34892;&#20016;&#23500;&#65292;&#26368;&#32456;&#29983;&#25104;&#19968;&#20123;&#31934;&#24515;&#21152;&#24037;&#30340;&#35821;&#27861;&#32467;&#26500;&#21644;&#23618;&#27425;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#20195;&#30721;&#65292;&#27809;&#26377;&#20013;&#38388;&#29615;&#33410;&#65292;&#20197;&#21453;&#26144;"&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;"&#30340;&#32467;&#26500;&#21270;&#24605;&#32500;&#36807;&#31243;&#12290;&#21463;&#21040;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChainCoder&#65292;&#36825;&#26159;&#19968;&#31181;&#31243;&#24207;&#32508;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36880;&#27493;&#29983;&#25104;Python&#20195;&#30721;&#65292;&#21363;&#20174;&#31895;&#21040;&#32454;&#36827;&#34892;&#22810;&#27425;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#35821;&#27861;&#26641;&#35299;&#26512;&#23558;&#28304;&#20195;&#30721;&#20998;&#35299;&#20026;&#24067;&#23616;&#26694;&#26550;&#32452;&#20214;&#21644;&#38468;&#20214;&#32452;&#20214;&#65292;&#20197;&#26500;&#24314;&#23618;&#27425;&#34920;&#31034;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#39044;&#27979;&#30446;&#26631;&#37325;&#26032;&#21551;&#21160;&#65292;&#24418;&#25104;&#22810;&#27425;&#36890;&#36807;&#30446;&#26631;&#65292;&#27599;&#27425;&#29983;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#36825;&#20123;&#23376;&#24207;&#21015;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#20018;&#32852;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39640;&#31354;&#24179;&#21488;&#21644;&#26080;&#20154;&#26426;&#26500;&#25104;&#30340;&#20998;&#23618;&#24335;&#31354;&#20013;&#35745;&#31639;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35843;&#25972;&#33322;&#36857;&#21644;&#36164;&#28304;&#20998;&#37197;&#26469;&#26368;&#23567;&#21270;&#29992;&#25143;&#30340;&#20449;&#24687;&#24180;&#40836;&#65292;&#24182;&#21463;&#21040;&#22810;&#37325;&#36164;&#28304;&#32422;&#26463;&#21644;&#20449;&#36947;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20449;&#24687;&#24180;&#40836;&#12290;</title><link>http://arxiv.org/abs/2305.00780</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;NOMA NTNs&#20013;&#30340;&#26080;&#32447;&#30005;&#21644;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#65306;&#22312;CSI&#19981;&#30830;&#23450;&#24615;&#19979;&#26368;&#23567;&#21270;AoI
&lt;/p&gt;
&lt;p&gt;
AI-based Radio and Computing Resource Allocation and Path Planning in NOMA NTNs: AoI Minimization under CSI Uncertainty. (arXiv:2305.00780v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39640;&#31354;&#24179;&#21488;&#21644;&#26080;&#20154;&#26426;&#26500;&#25104;&#30340;&#20998;&#23618;&#24335;&#31354;&#20013;&#35745;&#31639;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35843;&#25972;&#33322;&#36857;&#21644;&#36164;&#28304;&#20998;&#37197;&#26469;&#26368;&#23567;&#21270;&#29992;&#25143;&#30340;&#20449;&#24687;&#24180;&#40836;&#65292;&#24182;&#21463;&#21040;&#22810;&#37325;&#36164;&#28304;&#32422;&#26463;&#21644;&#20449;&#36947;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20449;&#24687;&#24180;&#40836;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#30340;&#31354;&#20013;&#35745;&#31639;&#26694;&#26550;&#65292;&#30001;&#39640;&#31354;&#24179;&#21488;&#65288;HAP&#65289;&#21644;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#32452;&#25104;&#65292;&#29992;&#20110;&#35745;&#31639;&#36890;&#36807;&#19978;&#34892;&#38750;&#27491;&#20132;&#22810;&#22336;&#65288;UL-NOMA&#65289;&#36830;&#25509;&#30340;&#38470;&#22320;&#31227;&#21160;&#29992;&#25143;&#30340;&#23436;&#20840;&#21368;&#36733;&#20219;&#21153;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#35745;&#31639;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#20013;&#20449;&#24687;&#30340;&#26032;&#40092;&#24230;&#65292;&#32771;&#34385;&#20102;&#20449;&#24687;&#24180;&#40836;&#65288;AoI&#65289;&#30340;&#26631;&#20934;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#35843;&#25972;UAV&#21644;HAP&#19978;&#30340;&#36164;&#28304;&#20998;&#37197;&#21644;UAV&#30340;&#36712;&#36857;&#65292;&#20197;&#26368;&#23567;&#21270;&#20855;&#26377;&#24377;&#24615;&#20219;&#21153;&#30340;&#29992;&#25143;&#30340;&#24179;&#22343;AoI&#65292;&#24182;&#21463;&#21040;CSI&#19981;&#30830;&#23450;&#24615;&#21644;UAV&#21644;HAP&#30340;&#22810;&#37325;&#36164;&#28304;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MADDPG&#65289;&#21644;&#32852;&#21512;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#26469;&#35774;&#35745;UAV&#30340;&#36712;&#36857;&#65292;&#24182;&#33719;&#24471;&#36890;&#36947;&#12289;&#21151;&#29575;&#21644;CPU&#20998;&#37197;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;AoI&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a hierarchical aerial computing framework composed of high altitude platform (HAP) and unmanned aerial vehicles (UAVs) to compute the fully offloaded tasks of terrestrial mobile users which are connected through an uplink non-orthogonal multiple access (UL-NOMA). To better assess the freshness of information in computation-intensive applications the criterion of age of information (AoI) is considered. In particular, the problem is formulated to minimize the average AoI of users with elastic tasks, by adjusting UAVs trajectory and resource allocation on both UAVs and HAP, which is restricted by the channel state information (CSI) uncertainty and multiple resource constraints of UAVs and HAP. In order to solve this non-convex optimization problem, two methods of multi-agent deep deterministic policy gradient (MADDPG) and federated reinforcement learning (FRL) are proposed to design the UAVs trajectory, and obtain channel, power, and CPU allocations. It is shown 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;DynaVol&#65292;&#21487;&#20197;&#22312;&#22810;&#23454;&#20307;&#21160;&#24577;&#22330;&#26223;&#20013;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#21644;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#22686;&#24378;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00393</link><description>&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#23545;&#21160;&#24577;&#22330;&#26223;&#36827;&#34892;&#29289;&#20307;&#20013;&#24515;&#20307;&#32032;&#21270;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering. (arXiv:2305.00393v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;DynaVol&#65292;&#21487;&#20197;&#22312;&#22810;&#23454;&#20307;&#21160;&#24577;&#22330;&#26223;&#20013;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#21644;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#22686;&#24378;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#30340;3D&#22330;&#26223;&#20013;&#29702;&#35299;&#19990;&#30028;&#30340;&#32452;&#25104;&#21160;&#24577;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#26102;&#38388;&#32447;&#32034;&#65292;&#35201;&#20040;&#24573;&#30053;&#20102;&#22330;&#26223;&#20998;&#35299;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DynaVol&#65292;&#19968;&#31181;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#20026;&#22810;&#23454;&#20307;&#65288;&#22914;&#29289;&#20307;&#65289;&#30340;&#21160;&#24577;&#22330;&#26223;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#23427;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#65292;&#21160;&#24577;&#32780;&#28789;&#27963;&#22320;&#23558;&#31354;&#38388;&#20301;&#32622;&#32465;&#23450;&#21040;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#20174;&#32780;&#22312;&#20195;&#34920;&#24615;&#27700;&#24179;&#19978;&#40723;&#21169;&#20449;&#24687;&#30340;&#20998;&#31163;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;DynaVol&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its ef
&lt;/p&gt;</description></item><item><title>"&#27169;&#25311;&#32593;&#32476;"&#27169;&#22411;&#22312;3D&#29289;&#20307;&#22330;&#26223;&#20998;&#21106;&#20013;&#37319;&#29992;&#31867;&#27604;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#24182;&#39044;&#27979;&#31867;&#20284;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#33021;&#22815;&#22312;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#20013;&#24471;&#20986;&#30456;&#20284;&#30340;&#35299;&#26512;&#65292;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2304.14382</link><description>&lt;p&gt;
&#27169;&#25311;&#24418;&#24335;&#36716;&#25442;&#22120;&#29992;&#20110;&#23569;&#26679;&#26412;3D&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analogy-Forming Transformers for Few-Shot 3D Parsing. (arXiv:2304.14382v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14382
&lt;/p&gt;
&lt;p&gt;
"&#27169;&#25311;&#32593;&#32476;"&#27169;&#22411;&#22312;3D&#29289;&#20307;&#22330;&#26223;&#20998;&#21106;&#20013;&#37319;&#29992;&#31867;&#27604;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#24182;&#39044;&#27979;&#31867;&#20284;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#33021;&#22815;&#22312;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#20013;&#24471;&#20986;&#30456;&#20284;&#30340;&#35299;&#26512;&#65292;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#19968;&#32452;&#26377;&#26631;&#35760;&#30340;&#32467;&#26500;&#21270;3D&#22330;&#26223;&#20013;&#26174;&#24335;&#22320;&#32534;&#30721;&#39046;&#22495;&#30693;&#35782;&#65288;&#20316;&#20026;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#37096;&#20998;&#65289;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#23545;3D&#29289;&#20307;&#22330;&#26223;&#36827;&#34892;&#20998;&#21106;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20174;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#21450;&#20854;&#30456;&#24212;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#35843;&#21046;&#26426;&#21046;&#20026;&#36755;&#20837;&#22330;&#26223;&#39044;&#27979;&#31867;&#20284;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23558;&#22330;&#26223;&#26144;&#23556;&#21040;&#37096;&#20998;&#20998;&#21106;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26816;&#32034;&#30340;&#35760;&#24518;&#36827;&#34892;&#26465;&#20214;&#25511;&#21046;&#65292;&#39044;&#27979;&#28151;&#21512;&#21305;&#37197;&#26816;&#32034;&#35760;&#24518;&#30340;&#32467;&#26500;&#21512;&#25104;&#12290;&#22312;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#20013;&#65292;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#34987;&#19968;&#33268;&#22320;&#22788;&#29702;&#65292;&#36890;&#36807;&#23545;&#36866;&#24403;&#30340;&#35760;&#24518;&#38598;&#36827;&#34892;&#26465;&#20214;&#35859;&#35789;&#65292;&#26080;&#35770;&#26159;&#20174;&#21333;&#20010;&#12289;&#23569;&#25968;&#36824;&#26159;&#35768;&#22810;&#23384;&#20648;&#23454;&#20363;&#20013;&#32487;&#25215;&#30456;&#20284;&#30340;&#35299;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#22312;&#35768;&#22810;&#26679;&#26412;&#24773;&#20917;&#19979;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Analogical Networks, a model that encodes domain knowledge explicitly, in a collection of structured labelled 3D scenes, in addition to implicitly, as model parameters, and segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures for the input scene, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformers in many-shot settings, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#33021;&#37327;&#39044;&#27979;&#30340;&#30830;&#20999;&#33021;&#37327;&#24341;&#23548;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#20013;&#38388;&#24341;&#23548;&#20272;&#35745;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12824</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#33021;&#37327;&#39044;&#27979;&#30340;&#30830;&#20999;&#33021;&#37327;&#24341;&#23548;&#25193;&#25955;&#37319;&#26679;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning. (arXiv:2304.12824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#33021;&#37327;&#39044;&#27979;&#30340;&#30830;&#20999;&#33021;&#37327;&#24341;&#23548;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#20013;&#38388;&#24341;&#23548;&#20272;&#35745;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#23548;&#37319;&#26679;&#26159;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#20219;&#21153;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#20854;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#23884;&#20837;&#20154;&#31867;&#23450;&#20041;&#30340;&#25351;&#23548;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#33324;&#35774;&#32622;&#65292;&#20854;&#20013;&#25351;&#23548;&#26159;&#30001;&#19968;&#20010;&#65288;&#38750;&#26631;&#20934;&#21270;&#65289;&#33021;&#37327;&#20989;&#25968;&#23450;&#20041;&#30340;&#12290;&#36825;&#31181;&#35774;&#32622;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#22312;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#20013;&#38388;&#25351;&#23548;&#26159;&#30001;&#37319;&#26679;&#20998;&#24067;&#21644;&#33021;&#37327;&#20989;&#25968;&#20849;&#21516;&#23450;&#20041;&#30340;&#65292;&#24456;&#38590;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#20013;&#38388;&#25351;&#23548;&#37197;&#26041;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#33021;&#37327;&#39044;&#27979;&#65288;CEP&#65289;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#26469;&#23398;&#20064;&#31934;&#30830;&#30340;&#20013;&#38388;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26080;&#38480;&#21046;&#30340;&#27169;&#22411;&#33021;&#21147;&#21644;&#25968;&#25454;&#26679;&#26412;&#19979;&#20445;&#35777;&#25910;&#25947;&#21040;&#31934;&#30830;&#25351;&#23548;&#65292;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#20570;&#21040;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Guided sampling is a vital approach for applying diffusion models in real-world tasks that embeds human-defined guidance during the sampling procedure. This paper considers a general setting where the guidance is defined by an (unnormalized) energy function. The main challenge for this setting is that the intermediate guidance during the diffusion sampling procedure, which is jointly defined by the sampling distribution and the energy function, is unknown and is hard to estimate. To address this challenge, we propose an exact formulation of the intermediate guidance as well as a novel training objective named contrastive energy prediction (CEP) to learn the exact guidance. Our method is guaranteed to converge to the exact guidance under unlimited model capacity and data samples, while previous methods can not. We demonstrate the effectiveness of our method by applying it to offline reinforcement learning (RL). Extensive experiments on D4RL benchmarks demonstrate that our method outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09010</link><description>&lt;p&gt;
CF-VAE&#65306;&#22522;&#20110;VAE&#21644;&#22240;&#26524;&#27969;&#30340;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#23398;&#20064;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20854;&#20013;&#27599;&#20010;&#32500;&#24230;&#23545;&#24212;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#22240;&#32032;&#12290;&#30001;&#20110;&#29983;&#25104;&#22240;&#32032;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#65292;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20197;&#23558;&#22240;&#26524;&#32467;&#26500;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#20013;&#30340;&#27969;&#65292;&#31216;&#20026;&#22240;&#26524;&#27969;&#12290;&#22522;&#20110;&#24191;&#27867;&#29992;&#20110;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#22522;&#20934;&#22240;&#32032;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20998;&#31163;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CF-VAE&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25193;&#23637;&#30340; Wasserstein PAC-Bayes &#26694;&#26550;&#65292;&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#20960;&#20309;&#20551;&#35774;&#25552;&#20379;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102; \cite{lambert2022variational} &#20013;&#31639;&#27861;&#30340;&#36755;&#20986;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#24314;&#31435;&#20102; PAC-Bayes &#21644;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;</title><link>http://arxiv.org/abs/2304.07048</link><description>&lt;p&gt;
Wasserstein PAC-Bayes &#23398;&#20064;&#65306;&#27867;&#21270;&#19982;&#20248;&#21270;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation. (arXiv:2304.07048v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25193;&#23637;&#30340; Wasserstein PAC-Bayes &#26694;&#26550;&#65292;&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#20960;&#20309;&#20551;&#35774;&#25552;&#20379;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102; \cite{lambert2022variational} &#20013;&#31639;&#27861;&#30340;&#36755;&#20986;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#24314;&#31435;&#20102; PAC-Bayes &#21644;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PAC-Bayes &#23398;&#20064;&#26159;&#19968;&#31181;&#24050;&#24314;&#31435;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#38454;&#27573;&#35780;&#20272;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#24324;&#28165;&#26970;&#20026;&#20160;&#20040;&#30693;&#21517;&#31639;&#27861;&#30340;&#36755;&#20986;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#29305;&#24615;&#32780; PAC-Bayes &#26159;&#21542;&#26377;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#31616;&#35201;&#20171;&#32461;&#22312;&#25991;&#29486; \cite{amit2022ipm} &#20013;&#25552;&#20986;&#30340; \emph{Wasserstein PAC-Bayes} &#26694;&#26550;&#26469;&#31215;&#26497;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#20960;&#20309;&#20551;&#35774;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#20219;&#20309;&#35757;&#32451;&#20043;&#21069;&#23601;&#35777;&#26126;&#20102; \cite{lambert2022variational} &#20013;&#31639;&#27861;&#30340;&#36755;&#20986;&#20855;&#26377;&#24378;&#22823;&#30340;&#28176;&#36817;&#27867;&#21270;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27867;&#21270;&#26694;&#26550;&#20013;&#23558;&#20248;&#21270;&#32467;&#26524;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102; PAC-Bayes &#21644;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes learning is an established framework to assess the generalisation ability of learning algorithm during the training phase. However, it remains challenging to know whether PAC-Bayes is useful to understand, before training, why the output of well-known algorithms generalise well. We positively answer this question by expanding the \emph{Wasserstein PAC-Bayes} framework, briefly introduced in \cite{amit2022ipm}. We provide new generalisation bounds exploiting geometric assumptions on the loss function. Using our framework, we prove, before any training, that the output of an algorithm from \citet{lambert2022variational} has a strong asymptotic generalisation ability. More precisely, we show that it is possible to incorporate optimisation results within a generalisation framework, building a bridge between PAC-Bayes and optimisation algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24207;&#21015;&#36716;&#23548;&#26550;&#26500;TDT&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#27604;&#20256;&#32479;Transducers&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06795</link><description>&lt;p&gt;
Token-and-Duration Transducer&#26550;&#26500;&#65306;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#19982;&#26102;&#38271;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24207;&#21015;&#36716;&#23548;&#26550;&#26500;TDT&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#27604;&#20256;&#32479;Transducers&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#30340;&#26032;&#22411;Token-and-Duration Transducer(TDT)&#26550;&#26500;&#12290;TDT&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#21363;&#21457;&#23556;&#30340;&#26631;&#35760;&#35206;&#30422;&#30340;&#36755;&#20837;&#24103;&#30340;&#25968;&#37327;&#65292;&#26469;&#25193;&#23637;&#20256;&#32479;&#30340;RNN-Transducer&#26550;&#26500;&#12290;&#23427;&#20351;&#29992;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#26631;&#20934;&#21270;&#36755;&#20986;&#30340;&#32852;&#21512;&#32593;&#32476;&#26469;&#29983;&#25104;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#20998;&#24067;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;TDT&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#30340;&#25345;&#32493;&#26102;&#38388;&#36755;&#20986;&#36339;&#36807;&#36755;&#20837;&#24103;&#65292;&#20351;&#20854;&#27604;&#36880;&#24103;&#22788;&#29702;&#32534;&#30721;&#22120;&#36755;&#20986;&#30340;&#20256;&#32479;Transducers&#26174;&#30528;&#26356;&#24555;&#12290;&#22312;&#19981;&#21516;&#30340;&#24207;&#21015;&#36716;&#23548;&#20219;&#21153;&#19978;&#65292;TDT&#27169;&#22411;&#22343;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35821;&#38899;&#35782;&#21035;&#30340;TDT&#27169;&#22411;&#27604;RNN-Transducers&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#39640;&#36798;2.82&#20493;&#12290;&#35821;&#38899;&#32763;&#35793;&#30340;TDT&#27169;&#22411;&#19982;MUST-C&#27979;&#35797;&#30456;&#27604;&#25552;&#39640;&#20102;1&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than RNN-Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2304.05365</link><description>&lt;p&gt;
&#25105;&#20204;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#21527;&#65311;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20010;&#24615;&#21270;&#27835;&#30103;&#24207;&#21015;&#20197;&#25903;&#25345;&#29992;&#25143;&#37319;&#21462;&#26356;&#20581;&#24247;&#30340;&#34892;&#20026;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#36830;&#32493;&#20915;&#31574;&#38382;&#39064;&#28041;&#21450;&#21040;&#22522;&#20110;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#20808;&#21069;&#30340;&#27963;&#21160;&#27700;&#24179;&#12289;&#20301;&#32622;&#31561;&#65289;&#22312;&#20309;&#26102;&#27835;&#30103;&#20197;&#21450;&#22914;&#20309;&#27835;&#30103;&#30340;&#20915;&#23450;&#12290;&#22312;&#32447;RL&#31639;&#27861;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#21382;&#21490;&#21453;&#39304;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20010;&#24615;&#21270;&#36825;&#20123;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#35201;&#20915;&#23450;&#26159;&#21542;&#24212;&#22312;&#23454;&#38469;&#37096;&#32626;&#30340;&#8220;&#20248;&#21270;&#8221;&#24178;&#39044;&#20013;&#21253;&#21547;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25968;&#25454;&#35777;&#25454;&#65292;&#34920;&#26126;RL&#31639;&#27861;&#23454;&#38469;&#19978;&#27491;&#22312;&#23558;&#27835;&#30103;&#20010;&#24615;&#21270;&#36866;&#24212;&#20854;&#29992;&#25143;&#12290;&#30001;&#20110;RL&#31639;&#27861;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#23545;&#20854;&#22312;&#26576;&#20123;&#29366;&#24577;&#19979;&#30340;&#23398;&#20064;&#24182;&#20351;&#29992;&#27492;&#23398;&#20064;&#26469;&#25552;&#20379;&#29305;&#23450;&#27835;&#30103;&#30340;&#33021;&#21147;&#20135;&#29983;&#35823;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#23450;&#20041;&#30340;&#20010;&#24615;&#21270;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#37325;&#22797;&#37319;&#26679;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#22312;&#32447;RL&#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#22270;&#20449;&#21495;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20026;&#22270;&#20449;&#21495;&#22788;&#29702;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#37322;&#21644;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.00474</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#20449;&#21495;&#26368;&#20248;&#24674;&#22797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Optimal Recovery of Graph Signals. (arXiv:2304.00474v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#22270;&#20449;&#21495;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20026;&#22270;&#20449;&#21495;&#22788;&#29702;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#37322;&#21644;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20174;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#24179;&#28369;&#30340;&#22270;&#20449;&#21495;&#26159;&#19968;&#20010;&#30740;&#31350;&#24050;&#20037;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20174;&#26368;&#20248;&#24674;&#22797;&#30340;&#35282;&#24230;&#32771;&#34385;&#36825;&#20010;&#20219;&#21153;&#65292;&#36825;&#26159;&#20174;&#23454;&#38469;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#20989;&#25968;&#30340;&#19968;&#31181;&#25968;&#23398;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#19982;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#27169;&#22411;&#26377;&#20851;&#30340;&#26368;&#22351;&#24773;&#24418;&#35266;&#28857;&#12290;&#20808;&#21069;&#30340;&#26368;&#20248;&#24674;&#22797;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#27491;&#21017;&#21270;&#30446;&#26631;&#22312;&#19968;&#33324;&#38382;&#39064;&#31867;&#21035;&#19978;&#20135;&#29983;&#26368;&#20248;&#35299;&#65292;&#20294;&#26410;&#23436;&#20840;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35745;&#31639;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#36825;&#20123;&#21442;&#25968;&#23545;&#20110;&#22270;&#20449;&#21495;&#22788;&#29702;&#38382;&#39064;&#26159;&#26368;&#20248;&#30340;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#65288;&#21462;&#20915;&#20110;&#35774;&#32622;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#20013;&#30340;&#32463;&#20856;&#20248;&#21270;&#25216;&#26415;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#37322;&#65292;&#24182;&#29992;&#20110;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21322;&#21512;&#25104;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a smooth graph signal from partially observed data is a well-studied task in graph-based machine learning. We consider this task from the perspective of optimal recovery, a mathematical framework for learning a function from observational data that adopts a worst-case perspective tied to model assumptions on the function to be learned. Earlier work in the optimal recovery literature has shown that minimizing a regularized objective produces optimal solutions for a general class of problems, but did not fully identify the regularization parameter. Our main contribution provides a way to compute regularization parameters that are optimal or near-optimal (depending on the setting), specifically for graph signal processing problems. Our results offer a new interpretation for classical optimization techniques in graph-based learning and also come with new insights for hyperparameter selection. We illustrate the potential of our methods in numerical experiments on several semi-synth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.15103</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#30456;&#20284;&#24615;&#22270;&#35889;&#19978;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#36816;&#20316;&#21407;&#29702;&#21644;&#21407;&#22240;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#12290;&#21033;&#29992;&#36825;&#31181;&#31561;&#20215;&#24615;&#20316;&#20026;&#22522;&#30707;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#24182;&#20005;&#26684;&#25551;&#36848;&#22810;&#27169;&#24577;&#23545;&#35937;&#22914;&#20309;&#34987;&#23884;&#20837;&#21040;&#19968;&#36215;&#12290;&#22312;&#29702;&#35770;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#28151;&#21512;&#25439;&#22833;&#65292;&#32467;&#21512;&#26032;&#39062;&#30340;&#26680;&#20989;&#25968;&#65292;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26631;&#20934;&#39640;&#26031;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#24605;&#24819;&#30340;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;Barrett&#39135;&#31649;&#30142;&#30149;&#36827;&#31243;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2303.12711</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#24314;&#27169;Barrett&#39135;&#31649;&#30142;&#30149;&#36827;&#31243;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Latent Representation Learning for Modeling Disease Progression of Barrett's Esophagus. (arXiv:2303.12711v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#24605;&#24819;&#30340;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;Barrett&#39135;&#31649;&#30142;&#30149;&#36827;&#31243;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Barrett&#39135;&#31649;&#26159;&#39135;&#31649;&#33146;&#30284;&#30340;&#21807;&#19968;&#20808;&#39537;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#35786;&#26029;&#26102;&#39044;&#21518;&#19981;&#33391;&#30340;&#39135;&#31649;&#30284;&#30151;&#12290;&#22240;&#27492;&#65292;&#35786;&#26029;Barrett&#39135;&#31649;&#23545;&#20110;&#39044;&#38450;&#21644;&#27835;&#30103;&#39135;&#31649;&#30284;&#33267;&#20851;&#37325;&#35201;&#12290;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;Barrett&#39135;&#31649;&#35786;&#26029;&#65292;&#20294;&#32452;&#32455;&#30149;&#29702;&#23398;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#35266;&#23519;&#32773;&#21464;&#24322;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAEs)&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26174;&#31034;&#20986;&#28508;&#22312;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#36755;&#20837;&#25968;&#25454;&#26144;&#23556;&#21040;&#20855;&#26377;&#20165;&#26377;&#29992;&#29305;&#24449;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#20026;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#21644;&#35265;&#35299;&#23558;Barrett&#39135;&#31649;&#30149;&#31243;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;VAE&#30340;&#27431;&#20960;&#37324;&#24471;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#20102;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#12290;&#20960;&#20309;VAEs&#20026;&#28508;&#22312;&#31354;&#38388;&#25552;&#20379;&#38468;&#21152;&#20960;&#20309;&#32467;&#26500;&#65292;RHVAE&#20551;&#35774;&#20026;&#40654;&#26364;&#27969;&#24418;&#65292;$\mathcal{S}$-VAE&#20551;&#35774;&#20026;&#36229;&#29699;&#38754;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;$\mathcal{S}$-VAE&#20248;&#20110;&#24120;&#35268;VAE&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Barrett's Esophagus (BE) is the only precursor known to Esophageal Adenocarcinoma (EAC), a type of esophageal cancer with poor prognosis upon diagnosis. Therefore, diagnosing BE is crucial in preventing and treating esophageal cancer. While supervised machine learning supports BE diagnosis, high interobserver variability in histopathological training data limits these methods. Unsupervised representation learning via Variational Autoencoders (VAEs) shows promise, as they map input data to a lower-dimensional manifold with only useful features, characterizing BE progression for improved downstream tasks and insights. However, the VAE's Euclidean latent space distorts point relationships, hindering disease progression modeling. Geometric VAEs provide additional geometric structure to the latent space, with RHVAE assuming a Riemannian manifold and $\mathcal{S}$-VAE a hyperspherical manifold. Our study shows that $\mathcal{S}$-VAE outperforms vanilla VAE with better reconstruction losses, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#21270;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#24341;&#36215;&#30340;Hamiltonian&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#27969;&#21487;&#20197;&#36880;&#28176;&#36924;&#36817;&#32039;&#33268;&#22495;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#65292;&#20026;&#23454;&#38469;&#20351;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.12147</link><description>&lt;p&gt;
Hamiltonian&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19975;&#33021;&#36924;&#36817;&#24615;&#36136;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation Property of Hamiltonian Deep Neural Networks. (arXiv:2303.12147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#21270;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#24341;&#36215;&#30340;Hamiltonian&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#27969;&#21487;&#20197;&#36880;&#28176;&#36924;&#36817;&#32039;&#33268;&#22495;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#65292;&#20026;&#23454;&#38469;&#20351;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30001;&#31163;&#25955;&#21270;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#24341;&#36215;&#30340;Hamiltonian&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;HDNN&#65289;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;HDNN&#22240;&#35774;&#35745;&#32780;&#20855;&#26377;&#38750;&#28040;&#22833;&#26799;&#24230;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20960;&#20010;&#24212;&#29992;&#20013;HDNN&#24050;&#32463;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#32570;&#23569;&#37327;&#21270;&#20854;&#34920;&#29616;&#21147;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;HDNN&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#35777;&#26126;&#20102;HDNN&#30340;&#19968;&#37096;&#20998;&#27969;&#21487;&#20197;&#36880;&#28176;&#36924;&#36817;&#32039;&#33268;&#22495;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#27492;&#32467;&#26524;&#20026;&#23454;&#38469;&#20351;&#29992;HDNN&#25552;&#20379;&#20102;&#29282;&#22266;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the universal approximation capabilities of Hamiltonian Deep Neural Networks (HDNNs) that arise from the discretization of Hamiltonian Neural Ordinary Differential Equations. Recently, it has been shown that HDNNs enjoy, by design, non-vanishing gradients, which provide numerical stability during training. However, although HDNNs have demonstrated state-of-the-art performance in several applications, a comprehensive study to quantify their expressivity is missing. In this regard, we provide a universal approximation theorem for HDNNs and prove that a portion of the flow of HDNNs can approximate arbitrary well any continuous function over a compact domain. This result provides a solid theoretical foundation for the practical use of HDNNs.
&lt;/p&gt;</description></item><item><title>LNO&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#31639;&#27861;&#65288;&#22914;FNO&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#24182;&#36866;&#29992;&#20110;&#38750;&#21608;&#26399;&#24615;&#20449;&#21495;&#21644;&#30636;&#24577;&#21709;&#24212;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#37322;&#27169;&#22411;&#24182;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10528</link><description>&lt;p&gt;
LNO: &#29992;&#20110;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#25289;&#26222;&#25289;&#26031;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
LNO: Laplace Neural Operator for Solving Differential Equations. (arXiv:2303.10528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10528
&lt;/p&gt;
&lt;p&gt;
LNO&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#31639;&#27861;&#65288;&#22914;FNO&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#24182;&#36866;&#29992;&#20110;&#38750;&#21608;&#26399;&#24615;&#20449;&#21495;&#21644;&#30636;&#24577;&#21709;&#24212;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#37322;&#27169;&#22411;&#24182;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25289;&#26222;&#25289;&#26031;&#31070;&#32463;&#31639;&#23376;&#65288;LNO&#65289;&#65292;&#21033;&#29992;&#25289;&#26222;&#25289;&#26031;&#21464;&#25442;&#23545;&#36755;&#20837;&#31354;&#38388;&#36827;&#34892;&#20998;&#35299;&#12290;&#19982;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#19981;&#21516;&#65292;LNO&#21487;&#20197;&#22788;&#29702;&#38750;&#21608;&#26399;&#24615;&#20449;&#21495;&#65292;&#32771;&#34385;&#30636;&#24577;&#21709;&#24212;&#65292;&#24182;&#21576;&#25351;&#25968;&#25910;&#25947;&#12290;LNO&#32467;&#21512;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#26497;&#28857;-&#27531;&#24046;&#20851;&#31995;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21333;&#20010;&#25289;&#26222;&#25289;&#26031;&#23618;&#22312;&#36924;&#36817;&#19977;&#20010;ODE&#65288;Duffing&#25391;&#23376;&#12289;&#39537;&#21160;&#24341;&#21147;&#25670;&#21644;Lorenz&#31995;&#32479;&#65289;&#21644;&#19977;&#20010;PDE&#65288;Euler-Bernoulli&#26753;&#12289;&#25193;&#25955;&#26041;&#31243;&#21644;&#21453;&#24212;&#25193;&#25955;&#31995;&#32479;&#65289;&#30340;&#35299;&#26102;&#65292;&#27604;FNO&#20013;&#30340;&#22235;&#20010;&#20613;&#37324;&#21494;&#27169;&#22359;&#20855;&#26377;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#26410;&#38459;&#23612;&#24773;&#20917;&#19979;&#65292;LNO&#22312;&#25429;&#25417;&#30636;&#24577;&#21709;&#24212;&#26041;&#38754;&#20248;&#20110;FNO&#12290;&#23545;&#20110;&#32447;&#24615;&#27431;&#25289;-&#20271;&#21162;&#21033;&#26753;&#21644;&#25193;&#25955;&#26041;&#31243;&#65292; LNO&#23545;&#26497;&#28857;-&#27531;&#24046;&#20844;&#24335;&#30340;&#31934;&#30830;&#34920;&#31034;&#27604;FNO&#20135;&#29983;&#20102;&#26174;&#30528;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Laplace neural operator (LNO), which leverages the Laplace transform to decompose the input space. Unlike the Fourier Neural Operator (FNO), LNO can handle non-periodic signals, account for transient responses, and exhibit exponential convergence. LNO incorporates the pole-residue relationship between the input and the output space, enabling greater interpretability and improved generalization ability. Herein, we demonstrate the superior approximation accuracy of a single Laplace layer in LNO over four Fourier modules in FNO in approximating the solutions of three ODEs (Duffing oscillator, driven gravity pendulum, and Lorenz system) and three PDEs (Euler-Bernoulli beam, diffusion equation, and reaction-diffusion system). Notably, LNO outperforms FNO in capturing transient responses in undamped scenarios. For the linear Euler-Bernoulli beam and diffusion equation, LNO's exact representation of the pole-residue formulation yields significantly better results than FNO. Fo
&lt;/p&gt;</description></item><item><title>CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08127</link><description>&lt;p&gt;
CB2&#65306;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08127
&lt;/p&gt;
&lt;p&gt;
CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CB2 &#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010; 3D &#28216;&#25103;&#29615;&#22659;&#12289;&#19968;&#20010;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#21487;&#20026;&#20154;&#31867;&#26234;&#33021;&#20307;&#25552;&#20379;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312; https://cb2.ai &#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#31995;&#32479;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06833</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#20989;&#25968;&#20540;&#26597;&#25214;&#20854;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;&#20110;&#29983;&#25104;&#26041;&#31243;&#24207;&#21015;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#33719;&#30410;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#27604;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20851;&#27880;&#30340;&#26159;&#20511;&#37492;&#25991;&#26412;&#29983;&#25104;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#24573;&#30053;&#20102;&#26041;&#31243;&#30340;&#29305;&#23450;&#30446;&#26631;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TPSR&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#34701;&#20837;&#21040;Transformer&#35299;&#30721;&#36807;&#31243;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#21516;&#65292;TPSR&#20801;&#35768;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#65288;&#22914;&#25311;&#21512;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65289;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;UniDiffuser&#26694;&#26550;&#65292;&#37319;&#29992;&#19968;&#20010;Transformer&#27169;&#22411;&#26469;&#32479;&#19968;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20998;&#24067;&#25311;&#21512;&#38382;&#39064;&#65292;&#35206;&#30422;&#20102;&#36793;&#32536;&#12289;&#26465;&#20214;&#21644;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#19978;&#23454;&#29616;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.06555</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25193;&#25955;&#20013;&#30340;&#19968;&#20010;Transformer&#36866;&#24212;&#25152;&#26377;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. (arXiv:2303.06555v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;UniDiffuser&#26694;&#26550;&#65292;&#37319;&#29992;&#19968;&#20010;Transformer&#27169;&#22411;&#26469;&#32479;&#19968;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20998;&#24067;&#25311;&#21512;&#38382;&#39064;&#65292;&#35206;&#30422;&#20102;&#36793;&#32536;&#12289;&#26465;&#20214;&#21644;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#19978;&#23454;&#29616;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25193;&#25955;&#26694;&#26550;(UniDiffuser)&#65292;&#29992;&#20110;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#25311;&#21512;&#19982;&#19968;&#32452;&#22810;&#27169;&#24577;&#25968;&#25454;&#30456;&#20851;&#30340;&#25152;&#26377;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65306;&#38024;&#23545;&#36793;&#32536;&#12289;&#26465;&#20214;&#21644;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#34987;&#32479;&#19968;&#20026;&#39044;&#27979;&#25200;&#21160;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#65292;&#20854;&#20013;&#23545;&#19981;&#21516;&#27169;&#24577;&#30340;&#25200;&#21160;&#32423;&#21035;&#65288;&#21363;&#26102;&#38388;&#27493;&#38271;&#65289;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#12290;&#22312;&#32479;&#19968;&#35266;&#28857;&#30340;&#21551;&#21457;&#19979;&#65292;UniDiffuser&#20351;&#29992;&#20102;&#26368;&#23567;&#30340;&#20462;&#25913;&#26469;&#21516;&#26102;&#23398;&#20064;&#25152;&#26377;&#20998;&#24067;&#65292;&#21363;&#23545;&#25152;&#26377;&#27169;&#24577;&#25200;&#21160;&#25968;&#25454;&#65292;&#36755;&#20837;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#21333;&#29420;&#26102;&#38388;&#27493;&#38271;&#65292;&#24182;&#39044;&#27979;&#25152;&#26377;&#27169;&#24577;&#30340;&#22122;&#22768;&#32780;&#19981;&#26159;&#21333;&#19968;&#27169;&#24577;&#30340;&#22122;&#22768;&#12290;UniDiffuser&#26159;&#20351;&#29992;Transformer&#21442;&#25968;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#19981;&#21516;&#27169;&#24577;&#30340;&#36755;&#20837;&#31867;&#22411;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#19978;&#23454;&#29616;&#65292;UniDiffuser&#33021;&#22815;&#25191;&#34892;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#22270;&#20687;&#25991;&#26412;&#23545;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is -- learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model -perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. UniDiffuser is parameterized by a transformer for diffusion models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24067;&#26391;&#26725;&#30340;&#21069;&#21521;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#25193;&#25955;&#22411;&#35821;&#38899;&#22686;&#24378;&#20013;&#30001;&#37325;&#24314;&#36807;&#31243;&#21644;&#20808;&#39564;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#30340;&#26041;&#27861;&#21482;&#38656;&#19968;&#21322;&#30340;&#36845;&#20195;&#27425;&#25968;&#21644;&#19968;&#20010;&#21487;&#35843;&#30340;&#36229;&#21442;&#25968;&#21363;&#21487;&#25913;&#36827;&#22788;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.14748</link><description>&lt;p&gt;
&#20943;&#23569;&#25193;&#25955;&#22411;&#35821;&#38899;&#22686;&#24378;&#20013;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20808;&#39564;&#19981;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement. (arXiv:2302.14748v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14748
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24067;&#26391;&#26725;&#30340;&#21069;&#21521;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#25193;&#25955;&#22411;&#35821;&#38899;&#22686;&#24378;&#20013;&#30001;&#37325;&#24314;&#36807;&#31243;&#21644;&#20808;&#39564;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#30340;&#26041;&#27861;&#21482;&#38656;&#19968;&#21322;&#30340;&#36845;&#20195;&#27425;&#25968;&#21644;&#19968;&#20010;&#21487;&#35843;&#30340;&#36229;&#21442;&#25968;&#21363;&#21487;&#25913;&#36827;&#22788;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#34987;&#25104;&#21151;&#22320;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#12290;&#19968;&#31181;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#29992;&#20110;&#24314;&#27169;&#36845;&#20195;&#21521;&#21069;&#36807;&#31243;&#65292;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#29615;&#22659;&#22122;&#22768;&#21644;&#30333;&#22122;&#22768;&#34987;&#28155;&#21152;&#21040;&#24178;&#20928;&#30340;&#35821;&#38899;&#20449;&#21495;&#20013;&#12290;&#34429;&#28982;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#65292;&#21521;&#21069;&#36807;&#31243;&#30340;&#22343;&#20540;&#20250;&#32467;&#26463;&#20110;&#22024;&#26434;&#28151;&#21512;&#20449;&#21495;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20250;&#25552;&#21069;&#32467;&#26463;&#24182;&#20197;&#22024;&#26434;&#28151;&#21512;&#20449;&#21495;&#30340;&#36817;&#20284;&#20540;&#32467;&#26463;&#12290;&#36825;&#23548;&#33268;&#21521;&#21069;&#36807;&#31243;&#30340;&#32456;&#27490;&#20998;&#24067;&#19982;&#29992;&#20110;&#25512;&#29702;&#21453;&#21521;&#36807;&#31243;&#30340;&#20808;&#39564;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24067;&#26391;&#26725;&#30340;&#21069;&#21521;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#36807;&#31243;&#19982;&#20197;&#21069;&#30340;&#25193;&#25955;&#36807;&#31243;&#30456;&#27604;&#65292;&#21487;&#20197;&#20943;&#23569;&#19981;&#21305;&#37197;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#22788;&#29702;&#36807;&#31243;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23458;&#35266;&#25351;&#26631;&#19978;&#25552;&#20379;&#20102;&#25913;&#36827;&#65292;&#20165;&#20351;&#29992;&#19968;&#21322;&#30340;&#36845;&#20195;&#27493;&#39588;&#24182;&#20855;&#26377;&#19968;&#20010;&#21487;&#35843;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, score-based generative models have been successfully employed for the task of speech enhancement. A stochastic differential equation is used to model the iterative forward process, where at each step environmental noise and white Gaussian noise are added to the clean speech signal. While in limit the mean of the forward process ends at the noisy mixture, in practice it stops earlier and thus only at an approximation of the noisy mixture. This results in a discrepancy between the terminating distribution of the forward process and the prior used for solving the reverse process at inference. In this paper, we address this discrepancy and propose a forward process based on a Brownian bridge. We show that such a process leads to a reduction of the mismatch compared to previous diffusion processes. More importantly, we show that our approach improves in objective metrics over the baseline process with only half of the iteration steps and having one hyperparameter less to tune.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#30340;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13221</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65306;&#36890;&#36807;&#31163;&#25955;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#23454;&#29616;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Data-Centric AI: Deep Generative Differentiable Feature Selection via Discrete Subsetting as Continuous Embedding Space Optimization. (arXiv:2302.13221v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#30340;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#65288;FS&#65289;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#23376;&#38598;&#65292;&#20363;&#22914;&#36807;&#28388;&#22120;&#12289;&#21253;&#35013;&#22120;&#21644;&#23884;&#20837;&#24335;&#26041;&#27861;&#12290;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;FS&#30340;&#26631;&#20934;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21464;&#21270;&#65292;&#24182;&#19988;&#24403;&#25968;&#25454;&#26159;&#39640;&#32500;&#21644;&#23567;&#26679;&#26412;&#26102;&#65292;FS&#23481;&#26131;&#20986;&#29616;&#38382;&#39064;&#12290;&#36873;&#25321;&#30340;&#29305;&#24449;&#23376;&#38598;&#26159;&#21542;&#21487;&#20197;&#26356;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#27867;&#21270;&#20026;&#19968;&#20010;&#28145;&#24230;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65306;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#21644;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#28145;&#24230;&#29305;&#24449;&#23376;&#38598;&#32534;&#30721;&#22120;&#12289;&#20934;&#30830;&#24615;&#35780;&#20272;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#26799;&#24230;&#19978;&#21319;&#20248;&#21270;&#22120;&#12290;&#36825;&#20010;&#26694;&#26550;&#23454;&#29616;&#20102;&#22235;&#20010;&#27493;&#39588;&#65306;1) &#29305;&#24449;-&#20934;&#30830;&#24615;&#35757;&#32451;&#25968;&#25454;&#20934;&#22791;&#65307;2) &#28145;&#24230;&#29305;&#24449;&#23376;&#38598;&#23884;&#20837;&#65307;3) &#26799;&#24230;&#20248;&#21270;&#25628;&#32034;&#65307;4) &#29305;&#24449;&#23376;&#38598;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#27934;&#35265;&#65306;&#23558;&#24378;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#27169;&#22411;&#35270;&#20026;&#25628;&#32034;&#21152;&#36895;&#22120;&#12289;&#22810;&#23610;&#24230;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#36880;&#28176;&#22686;&#24378;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Feature Selection (FS), such as filter, wrapper, and embedded methods, aims to find the optimal feature subset for a given downstream task. However, in many real-world practices, 1) the criteria of FS vary across domains; 2) FS is brittle when data is a high-dimensional and small sample size. Can selected feature subsets be more generalized, accurate, and input dimensionality agnostic? We generalize this problem into a deep differentiable feature selection task and propose a new perspective: discrete feature subsetting as continuous embedding space optimization. We develop a generic and principled framework including a deep feature subset encoder, accuracy evaluator, decoder, and gradient ascent optimizer. This framework implements four steps: 1) features-accuracy training data preparation; 2) deep feature subset embedding; 3) gradient-optimized search; 4) feature subset reconstruction. We develop new technical insights: reinforcement as a training data generator, ensembles of diverse 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#37322;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#20013;&#20851;&#38190;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65306;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#19981;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.12537</link><description>&lt;p&gt;
&#30446;&#26631;&#32593;&#32476;&#22914;&#20309;&#31283;&#23450;&#26102;&#38388;&#24046;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Why Target Networks Stabilise Temporal Difference Methods. (arXiv:2302.12537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#37322;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#20013;&#20851;&#38190;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65306;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#19981;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36817;&#26399;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31867;&#20351;&#29992;&#19981;&#39057;&#32321;&#26356;&#26032;&#30446;&#26631;&#20540;&#36827;&#34892;&#31574;&#30053;&#35780;&#20272;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;&#30446;&#26631;&#32593;&#32476;&#26377;&#25928;&#24615;&#30340;&#23436;&#25972;&#29702;&#35770;&#35299;&#37322;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31181;&#27969;&#34892;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26368;&#32456;&#22238;&#31572;&#20102;&#8220;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#21487;&#20197;&#31283;&#23450;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#37096;&#20998;&#25311;&#21512;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#25324;&#30446;&#26631;&#32593;&#32476;&#30340;&#20351;&#29992;&#65292;&#24182;&#19988;&#22635;&#34917;&#20102;&#25311;&#21512;&#26041;&#27861;&#21644;&#21322;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#29420;&#29305;&#22320;&#25551;&#36848;&#25152;&#35859;&#30340;&#33268;&#21629;&#19977;&#20803;&#32452;&#65292;&#21363;&#20351;&#29992;&#26102;&#24207;&#24046;&#20998;&#26356;&#26032;&#65292;&#32467;&#21512;&#65288;&#38750;&#32447;&#24615;&#65289;&#20989;&#25968;&#36924;&#36817;&#21644;&#22788;&#20110;&#31163;&#32447;&#29366;&#24577;&#30340;&#25968;&#25454;&#65292;&#36825;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#25910;&#25947;&#30340;&#31639;&#27861;&#12290;&#36825;&#19968;&#35748;&#35782;&#20351;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#30446;&#26631;&#32593;&#32476;&#30340;&#20351;&#29992;&#21487;&#20197;&#20943;&#36731;&#26465;&#20214;&#24046;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integral to recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. Yet a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: `why do target networks stabilise TD learning'? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad - the use of TD updates with (nonlinear) function approximation and off-policy data - which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditionin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20989;&#25968;&#30340;Lipschitz&#34892;&#20026;&#65292;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;Lipschitz&#36830;&#32493;&#24615;&#30340;&#22522;&#26412;&#21644;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#22312;Lipschitz&#24120;&#25968;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#20013;&#35782;&#21035;&#20986;&#20102;&#26126;&#26174;&#30340;&#21452;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.10886</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;Lipschitz&#36830;&#32493;&#24615;&#30340;&#19968;&#20123;&#22522;&#26412;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions. (arXiv:2302.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20989;&#25968;&#30340;Lipschitz&#34892;&#20026;&#65292;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;Lipschitz&#36830;&#32493;&#24615;&#30340;&#22522;&#26412;&#21644;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#22312;Lipschitz&#24120;&#25968;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#20013;&#35782;&#21035;&#20986;&#20102;&#26126;&#26174;&#30340;&#21452;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#36830;&#32493;&#24615;&#26159;&#20219;&#20309;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#31616;&#21333;&#20294;&#20851;&#38190;&#30340;&#21151;&#33021;&#24615;&#36136;&#65292;&#23427;&#22788;&#20110;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20989;&#25968;&#30340;Lipschitz&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#32791;&#23613;&#26368;&#31616;&#21333;&#21644;&#26368;&#19968;&#33324;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#30340;&#26497;&#38480;&#65292;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65288;&#21363;&#65292;&#20307;&#31995;&#32467;&#26500;&#12289;&#25439;&#22833;&#12289;&#20248;&#21270;&#22120;&#12289;&#26631;&#31614;&#22122;&#38899;&#31561;&#65289;&#65292;&#34429;&#28982;&#36825;&#19968;&#36873;&#25321;&#20027;&#35201;&#26159;&#21463;&#35745;&#31639;&#38590;&#24230;&#32467;&#26524;&#30340;&#39537;&#21160;&#65292;&#20294;&#23427;&#20063;&#38750;&#24120;&#20016;&#23500;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20960;&#20010;&#22522;&#26412;&#21644;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#36824;&#34917;&#20805;&#20102;&#36866;&#24403;&#30340;&#29702;&#35770;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lipschitz continuity is a simple yet crucial functional property of any predictive model for it lies at the core of the model's robustness, generalisation, as well as adversarial vulnerability. Our aim is to thoroughly investigate and characterise the Lipschitz behaviour of the functions realised by neural networks. Thus, we carry out an empirical investigation in a range of different settings (namely, architectures, losses, optimisers, label noise, and more) by exhausting the limits of the simplest and the most general lower and upper bounds. Although motivated primarily by computational hardness results, this choice nevertheless turns out to be rather resourceful and sheds light on several fundamental and intriguing traits of the Lipschitz continuity of neural network functions, which we also supplement with suitable theoretical arguments. As a highlight of this investigation, we identify a striking double descent trend in both upper and lower bounds to the Lipschitz constant with in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#24615;&#22810;&#35270;&#35282;&#32858;&#31867;&#65288;IMVC&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32422;&#26463;&#20849;&#21516;&#34920;&#31034;&#21644;&#35270;&#22270;&#29305;&#23450;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#23567;&#65292;&#25552;&#39640;&#22810;&#35270;&#35282;&#25968;&#25454;&#32858;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.08743</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20114;&#20449;&#24687;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-View Clustering from the Perspective of Mutual Information. (arXiv:2302.08743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#24615;&#22810;&#35270;&#35282;&#32858;&#31867;&#65288;IMVC&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32422;&#26463;&#20849;&#21516;&#34920;&#31034;&#21644;&#35270;&#22270;&#29305;&#23450;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#23567;&#65292;&#25552;&#39640;&#22810;&#35270;&#35282;&#25968;&#25454;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#22810;&#35270;&#35282;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#20197;&#25552;&#39640;&#32858;&#31867;&#25928;&#26524;&#26159;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#26032;&#27169;&#22411;&#8212;&#8212;&#20449;&#24687;&#24615;&#22810;&#35270;&#35282;&#32858;&#31867;&#65288;IMVC&#65289;&#65292;&#23427;&#25552;&#21462;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20849;&#21516;&#20449;&#24687;&#21644;&#29305;&#23450;&#35270;&#35282;&#20449;&#24687;&#65292;&#24182;&#26500;&#24314;&#38754;&#21521;&#32858;&#31867;&#30340;&#32508;&#21512;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22810;&#20010;&#29305;&#24449;&#36830;&#25509;&#25104;&#19968;&#20010;&#32479;&#19968;&#29305;&#24449;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#32534;&#30721;&#22120;&#23558;&#20854;&#20256;&#36882;&#20197;&#26816;&#32034;&#36328;&#35270;&#22270;&#30340;&#20849;&#21516;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#27599;&#20010;&#35270;&#22270;&#30340;&#29305;&#24449;&#34987;&#21457;&#36865;&#21040;&#32534;&#30721;&#22120;&#20197;&#20135;&#29983;&#32039;&#20945;&#30340;&#35270;&#22270;&#29305;&#23450;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32422;&#26463;&#20849;&#21516;&#34920;&#31034;&#21644;&#35270;&#22270;&#29305;&#23450;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#23567;&#65292;&#20197;&#33719;&#24471;&#22810;&#23618;&#27425;&#20449;&#24687;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#23558;&#20849;&#21516;&#34920;&#31034;&#21644;&#35270;&#22270;&#29305;&#23450;&#34920;&#31034;&#25340;&#25509;&#36215;&#26469;&#65292;&#24314;&#27169;&#27599;&#20010;&#35270;&#22270;&#30340;&#31934;&#32454;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the complementary information of multi-view data to improve clustering effects is a crucial issue in multi-view clustering. In this paper, we propose a novel model based on information theory termed Informative Multi-View Clustering (IMVC), which extracts the common and view-specific information hidden in multi-view data and constructs a clustering-oriented comprehensive representation. More specifically, we concatenate multiple features into a unified feature representation, then pass it through a encoder to retrieve the common representation across views. Simultaneously, the features of each view are sent to a encoder to produce a compact view-specific representation, respectively. Thus, we constrain the mutual information between the common representation and view-specific representations to be minimal for obtaining multi-level information. Further, the common representation and view-specific representation are spliced to model the refined representation of each view, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;</title><link>http://arxiv.org/abs/2302.07849</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36866;&#24212;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#35843;&#25972;&#65292;&#29305;&#21035;&#26159;&#24403;&#27809;&#26377;&#38024;&#23545;&#8220;&#26032;&#27491;&#24120;&#8221;&#36827;&#34892;&#35757;&#32451;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23548;&#33268;&#20135;&#29983;&#20102;&#38646;&#26679;&#26412;AD&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#65288;ACR&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;AD&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;SVDD&#65289;&#26469;&#36866;&#24212;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#21152;&#20803;&#35757;&#32451;&#65292;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#21644;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#38646;&#26679;&#26412;AD&#32467;&#26524;&#65292;&#24182;&#22312;&#26469;&#33258;&#19987;&#19994;&#39046;&#22495;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#27573;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32431;&#25506;&#32034;&#35774;&#32622;&#19979;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#25152;&#26377;&#20219;&#21153;&#20043;&#38388;&#30340;&#20844;&#20849;&#20302;&#32500;&#32447;&#24615;&#34920;&#31034;&#65292;&#35774;&#35745;&#20102;&#35745;&#31639;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#21152;&#36895;&#26368;&#20339;&#33218;&#25110;&#31574;&#30053;&#35782;&#21035;&#36807;&#31243;&#65292;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.04441</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#33218;&#31639;&#27861;&#20013;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-task Representation Learning for Pure Exploration in Linear Bandits. (arXiv:2302.04441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32431;&#25506;&#32034;&#35774;&#32622;&#19979;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#25152;&#26377;&#20219;&#21153;&#20043;&#38388;&#30340;&#20844;&#20849;&#20302;&#32500;&#32447;&#24615;&#34920;&#31034;&#65292;&#35774;&#35745;&#20102;&#35745;&#31639;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#21152;&#36895;&#26368;&#20339;&#33218;&#25110;&#31574;&#30053;&#35782;&#21035;&#36807;&#31243;&#65292;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24207;&#21015;&#20915;&#31574;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#36817;&#25104;&#21151;, &#20294;&#22312;&#32431;&#25506;&#32034;&#22330;&#26223; (&#21363;, &#25214;&#21040;&#26368;&#20339;&#36873;&#39033;&#21644;&#26368;&#23567;&#21270;&#26679;&#26412;&#22797;&#26434;&#24230;) &#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;. &#22312;&#26412;&#25991;&#20013;, &#25105;&#20204;&#30740;&#31350;&#20102;&#32431;&#25506;&#32034;&#35774;&#32622;&#20013;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;, &#21253;&#25324;&#32447;&#24615;&#36172;&#33218;&#20013;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035; (RepBAI-LB) &#21644;&#19978;&#19979;&#25991;&#32447;&#24615;&#36172;&#33218;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035; (RepBPI-CLB). &#22312;&#36825;&#20004;&#20010;&#38382;&#39064;&#20013;, &#25152;&#26377;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#20302;&#32500;&#32447;&#24615;&#34920;&#31034;, &#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#36825;&#20010;&#29305;&#24615;&#21152;&#36895;&#25152;&#26377;&#20219;&#21153;&#30340;&#26368;&#20339;&#33218; (&#31574;&#30053;) &#35782;&#21035;&#36807;&#31243;. &#25105;&#20204;&#35774;&#35745;&#20102;&#35745;&#31639;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#31639;&#27861; DouExpDes and C-DouExpDes &#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;, &#23427;&#20204;&#25191;&#34892;&#21452;&#37325;&#23454;&#39564;&#35774;&#35745;&#26469;&#35268;&#21010;&#23398;&#20064;&#20840;&#23616;&#34920;&#31034;&#30340;&#26368;&#20339;&#26679;&#26412;&#20998;&#37197;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#21508;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#20844;&#20849;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of representation learning in sequential decision making, the study of the pure exploration scenario (i.e., identify the best option and minimize the sample complexity) is still limited. In this paper, we study multi-task representation learning for best arm identification in linear bandits (RepBAI-LB) and best policy identification in contextual linear bandits (RepBPI-CLB), two popular pure exploration settings with wide applications, e.g., clinical trials and web content optimization. In these two problems, all tasks share a common low-dimensional linear representation, and our goal is to leverage this feature to accelerate the best arm (policy) identification process for all tasks. For these problems, we design computationally and sample efficient algorithms DouExpDes and C-DouExpDes, which perform double experimental designs to plan optimal sample allocations for learning the global representation. We show that by learning the common representation among 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#20284;&#28982;&#20998;&#25968;&#65288;FLS&#65289;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#26679;&#26412;&#30340;&#26032;&#39062;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04440</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples. (arXiv:2302.04440v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#20284;&#28982;&#20998;&#25968;&#65288;FLS&#65289;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#26679;&#26412;&#30340;&#26032;&#39062;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#32500;&#12289;&#22797;&#26434;&#21644;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#20173;&#28982;&#19981;&#23436;&#20840;&#65306;&#26631;&#20934;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#36866;&#29992;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#20063;&#24456;&#23569;&#19982;&#24863;&#30693;&#20445;&#30495;&#24230;&#30456;&#20851;&#65292;&#32780;&#22522;&#20110;&#26679;&#26412;&#30340;&#25351;&#26631;&#65288;&#22914;FID&#65289;&#23545;&#36807;&#25311;&#21512;&#19981;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#29305;&#24449;&#20284;&#28982;&#20998;&#25968;&#65288;FLS&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#20998;&#25968;&#65292;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#26469;&#25552;&#20379;&#20840;&#38754;&#30340;&#19977;&#30456;&#35780;&#20272;&#65292;&#32771;&#34385;&#29983;&#25104;&#26679;&#26412;&#30340;&#26032;&#39062;&#24615;&#65288;&#21363;&#19982;&#35757;&#32451;&#26679;&#26412;&#19981;&#21516;&#65289;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;FLS&#22312;&#26816;&#27979;&#36807;&#25311;&#21512;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#20808;&#21069;&#25552;&#20986;&#30340;&#24230;&#37327;&#25351;&#26631;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#31867;&#21035;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;FLS&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Score (FLS), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLS to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLS on various image datasets and model classes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#37325;&#35201;&#24615;&#37319;&#26679;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#22312;&#20998;&#24067;&#20559;&#24046;&#32422;&#26463;&#19979;&#26399;&#26395;&#22833;&#30495;&#30340;&#26368;&#22351;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2302.04421</link><description>&lt;p&gt;
&#20449;&#24687;&#29702;&#35770;&#37325;&#35201;&#24615;&#37319;&#26679;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Information Theoretical Importance Sampling Clustering. (arXiv:2302.04421v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#37325;&#35201;&#24615;&#37319;&#26679;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#22312;&#20998;&#24067;&#20559;&#24046;&#32422;&#26463;&#19979;&#26399;&#26395;&#22833;&#30495;&#30340;&#26368;&#22351;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#32858;&#31867;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26469;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#30495;&#23454;&#22330;&#26223;&#19979;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;ITISC&#65289;&#65292;&#20854;&#22312;&#20998;&#24067;&#20559;&#24046;&#30340;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#26399;&#26395;&#22833;&#30495;&#30340;&#26368;&#22351;&#24773;&#20917;&#12290;&#20998;&#24067;&#20559;&#24046;&#32422;&#26463;&#21487;&#20197;&#36716;&#25442;&#20026;&#20197;&#22522;&#20110;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#22343;&#21248;&#20998;&#24067;&#20026;&#20013;&#24515;&#30340;&#19968;&#32452;&#26435;&#37325;&#20998;&#24067;&#30340;&#32422;&#26463;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#22312;&#26368;&#22823;&#38477;&#32423;&#19979;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#22240;&#27492;&#24471;&#21040;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#24102;&#26377;&#32422;&#26463;&#30340;&#26368;&#23567;&#26368;&#22823;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23558;&#20854;&#37325;&#26032;&#34920;&#36798;&#20026;&#26080;&#32422;&#26463;&#38382;&#39064;&#12290;&#35813;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20132;&#26367;&#20248;&#21270;&#31639;&#27861;&#25110;&#20351;&#29992;&#21830;&#19994;&#21487;&#29992;&#36719;&#20214;&#30340;&#36890;&#29992;&#20248;&#21270;&#20363;&#31243;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
A current assumption of most clustering methods is that the training data and future data are taken from the same distribution. However, this assumption may not hold in most real-world scenarios. In this paper, we propose an information theoretical importance sampling based approach for clustering problems (ITISC) which minimizes the worst case of expected distortions under the constraint of distribution deviation. The distribution deviation constraint can be converted to the constraint over a set of weight distributions centered on the uniform distribution derived from importance sampling. The objective of the proposed approach is to minimize the loss under maximum degradation hence the resulting problem is a constrained minimax optimization problem which can be reformulated to an unconstrained problem using the Lagrange method. The optimization problem can be solved by both an alternative optimization algorithm or a general optimization routine by commercially available software. Exp
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;IB-UQ&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#22238;&#24402;&#21644;&#25805;&#20316;&#22120;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.03271</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;&#31070;&#32463;&#20989;&#25968;&#22238;&#24402;&#21644;&#31070;&#32463;&#25805;&#20316;&#22120;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
IB-UQ: Information bottleneck based uncertainty quantification for neural function regression and neural operator learning. (arXiv:2302.03271v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03271
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;IB-UQ&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#22238;&#24402;&#21644;&#25805;&#20316;&#22120;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;IB-UQ&#65292;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22238;&#24402;&#21644;&#31070;&#32463;&#25805;&#20316;&#22120;&#23398;&#20064;&#65288;DeepONet&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#20449;&#24863;&#30693;&#30340;&#32534;&#30721;&#22120;&#21152;&#20837;&#20102;&#29942;&#39048;&#65292;&#35813;&#32534;&#30721;&#22120;&#26681;&#25454;&#36755;&#20837;&#25968;&#25454;&#23646;&#20110;&#35757;&#32451;&#25968;&#25454;&#25152;&#22312;&#21306;&#22495;&#30340;&#21487;&#20449;&#24230;&#23558;&#36755;&#20837;&#32534;&#30721;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39640;&#26031;&#35299;&#30721;&#22120;&#26465;&#20214;&#22320;&#39044;&#27979;&#36755;&#20986;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#65292;&#21487;&#20197;&#22686;&#24378;&#22806;&#25512;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#36136;&#37327;&#65292;&#24182;&#19988;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#37117;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#35813;&#30446;&#26631;&#30340;&#21487;&#35745;&#31639;&#21464;&#20998;&#19979;&#30028;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#19982;&#20381;&#36182;&#20110;&#21704;&#23494;&#23572;&#39039;&#33945;&#29305;&#21345;&#32599;&#21518;&#39564;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;IB-UQ&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#20855;&#26377;&#21487;&#27604;&#36739;&#20934;&#30830;&#24230;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for uncertainty quantification via information bottleneck (IB-UQ) for scientific machine learning tasks, including deep neural network (DNN) regression and neural operator learning (DeepONet). Specifically, we incorporate the bottleneck by a confidence-aware encoder, which encodes inputs into latent representations according to the confidence of the input data belonging to the region where training data is located, and utilize a Gaussian decoder to predict means and variances of outputs conditional on representation variables. Furthermore, we propose a data augmentation based information bottleneck objective which can enhance the quantification quality of the extrapolation uncertainty, and the encoder and decoder can be both trained by minimizing a tractable variational bound of the objective. In comparison to uncertainty quantification (UQ) methods for scientific learning tasks that rely on Bayesian neural networks with Hamiltonian Monte Carlo posterior es
&lt;/p&gt;</description></item><item><title>V1T&#26159;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#36328;&#21160;&#29289;&#23398;&#20064;&#20849;&#20139;&#30340;&#35270;&#35273;&#21644;&#34892;&#20026;&#34920;&#31034;&#65292;&#23545;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#19979;&#30340;&#35270;&#35273;&#30382;&#23618;&#31070;&#32463;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#20043;&#21069;&#22522;&#20110;&#21367;&#31215;&#30340;&#27169;&#22411;&#36229;&#36807;12.7&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Transformer&#23398;&#20064;&#30340;&#33258;&#25105;&#20851;&#27880;&#26435;&#37325;&#36824;&#33021;&#22815;&#23637;&#31034;&#19982;&#32676;&#20307;&#24863;&#21463;&#37326;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03023</link><description>&lt;p&gt;
V1T&#65306;&#20351;&#29992;Vision Transformer&#36827;&#34892;&#22823;&#35268;&#27169;&#23567;&#40736;V1&#21709;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
V1T: large-scale mouse V1 response prediction using a Vision Transformer. (arXiv:2302.03023v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03023
&lt;/p&gt;
&lt;p&gt;
V1T&#26159;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#36328;&#21160;&#29289;&#23398;&#20064;&#20849;&#20139;&#30340;&#35270;&#35273;&#21644;&#34892;&#20026;&#34920;&#31034;&#65292;&#23545;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#19979;&#30340;&#35270;&#35273;&#30382;&#23618;&#31070;&#32463;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#20043;&#21069;&#22522;&#20110;&#21367;&#31215;&#30340;&#27169;&#22411;&#36229;&#36807;12.7&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Transformer&#23398;&#20064;&#30340;&#33258;&#25105;&#20851;&#27880;&#26435;&#37325;&#36824;&#33021;&#22815;&#23637;&#31034;&#19982;&#32676;&#20307;&#24863;&#21463;&#37326;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#23545;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#19979;&#30340;&#35270;&#35273;&#30382;&#23618;&#31070;&#32463;&#21709;&#24212;&#30340;&#31934;&#30830;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;V1T&#65292;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#36328;&#21160;&#29289;&#23398;&#20064;&#20849;&#20139;&#30340;&#35270;&#35273;&#21644;&#34892;&#20026;&#34920;&#31034;&#12290;&#25105;&#20204;&#23545;&#35760;&#24405;&#20110;&#23567;&#40736;&#21407;&#22987;&#35270;&#35273;&#30382;&#23618;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#20043;&#21069;&#22522;&#20110;&#21367;&#31215;&#30340;&#27169;&#22411;&#36229;&#36807;12.7&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#23398;&#20064;&#30340;&#33258;&#25105;&#20851;&#27880;&#26435;&#37325;&#19982;&#32676;&#20307;&#24863;&#21463;&#37326;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#31070;&#32463;&#21709;&#24212;&#39044;&#27979;&#35774;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#65292;&#24182;&#21487;&#19982;&#34892;&#20026;&#21644;&#31070;&#32463;&#35760;&#24405;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#25581;&#31034;&#35270;&#35273;&#30382;&#23618;&#30340;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce V1T, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#38899;&#20048;&#21512;&#25104;&#21644;&#28304;&#20998;&#31163;&#65292;&#24182;&#20110;&#37096;&#20998;&#29983;&#25104;&#21644;&#20998;&#31163;&#20219;&#21153;&#19978;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#23450;&#37327;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.02257</link><description>&lt;p&gt;
&#22810;&#28304;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#38899;&#20048;&#29983;&#25104;&#21644;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Diffusion Models for Simultaneous Music Generation and Separation. (arXiv:2302.02257v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#38899;&#20048;&#21512;&#25104;&#21644;&#28304;&#20998;&#31163;&#65292;&#24182;&#20110;&#37096;&#20998;&#29983;&#25104;&#21644;&#20998;&#31163;&#20219;&#21153;&#19978;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#23450;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#25193;&#25955;&#22522;&#20110;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#19978;&#19979;&#25991;&#28304;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#30340;&#24471;&#20998;&#26469;&#36827;&#34892;&#38899;&#20048;&#21512;&#25104;&#21644;&#28304;&#20998;&#31163;&#12290;&#38500;&#20102;&#32463;&#20856;&#30340;&#24635;&#25512;&#29702;&#20219;&#21153;&#65288;&#21363;&#29983;&#25104;&#28151;&#21512;&#29289;&#20307;&#65292;&#20998;&#31163;&#28304;&#65289;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#24182;&#22312;&#28304;&#22635;&#20805;&#30340;&#37096;&#20998;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20123;&#28304;&#32473;&#20854;&#20182;&#20154;&#65288;&#20363;&#22914;&#65292;&#28436;&#22863;&#19968;&#26465;&#19982;&#40723;&#30456;&#37197;&#30340;&#38050;&#29748;&#26354;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dirac&#20284;&#28982;&#20989;&#25968;&#30340;&#20998;&#31163;&#20219;&#21153;&#30340;&#26032;&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Slakh2100&#36825;&#20010;&#26631;&#20934;&#30340;&#38899;&#20048;&#28304;&#20998;&#31163;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#35774;&#32622;&#20013;&#25552;&#20379;&#23450;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#22312;&#28304;&#20998;&#31163;&#35774;&#32622;&#20013;&#30340;&#26377;&#31454;&#20105;&#21147;&#30340;&#23450;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#29983;&#25104;&#21644;&#20998;&#31163;&#20219;&#21153;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#20363;&#23376;&#65292;&#22240;&#27492;&#20195;&#34920;&#20102;&#36890;&#29992;&#38899;&#39057;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we define a diffusion-based generative model capable of both music synthesis and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we also introduce and experiment on the partial generation task of source imputation, where we generate a subset of the sources given the others (e.g., play a piano track that goes well with the drums). Additionally, we introduce a novel inference method for the separation task based on Dirac likelihood functions. We train our model on Slakh2100, a standard dataset for musical source separation, provide qualitative results in the generation settings, and showcase competitive quantitative results in the source separation setting. Our method is the first example of a single model that can handle both generation and separation tasks, thus representing a step toward general audio models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Koopman&#31639;&#23376;&#30340;&#38750;&#28176;&#36827;&#23398;&#20064;&#30028;&#38480;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#26102;&#38388;&#21487;&#36870;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#21160;&#24577;&#27169;&#20998;&#35299;&#65288;EDMD&#65289;&#21644;&#38477;&#20302;&#31209;&#22238;&#24402;&#65288;RRR&#65289;&#20004;&#31181;&#27969;&#34892;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.02004</link><description>&lt;p&gt;
Koopman&#31639;&#23376;&#23398;&#20064;&#30340;&#23574;&#38160;&#35889;&#29575;
&lt;/p&gt;
&lt;p&gt;
Sharp Spectral Rates for Koopman Operator Learning. (arXiv:2302.02004v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Koopman&#31639;&#23376;&#30340;&#38750;&#28176;&#36827;&#23398;&#20064;&#30028;&#38480;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#26102;&#38388;&#21487;&#36870;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#21160;&#24577;&#27169;&#20998;&#35299;&#65288;EDMD&#65289;&#21644;&#38477;&#20302;&#31209;&#22238;&#24402;&#65288;RRR&#65289;&#20004;&#31181;&#27969;&#34892;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#21487;&#20197;&#26041;&#20415;&#22320;&#25551;&#36848;&#20026;&#30456;&#20851;&#30340;Koopman&#31639;&#23376;&#65292;&#20854;&#20316;&#29992;&#20351;&#31995;&#32479;&#30340;&#27599;&#20010;&#21487;&#35266;&#27979;&#37327;&#38543;&#26102;&#38388;&#21521;&#21069;&#28436;&#21270;&#12290;&#36890;&#36807;&#35768;&#22810;&#31639;&#27861;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;Koopman&#31639;&#23376;&#21450;&#20854;&#35889;&#20998;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;Koopman&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#30340;&#38750;&#28176;&#36827;&#23398;&#20064;&#30028;&#38480;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26102;&#38388;&#21487;&#36870;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#65292;&#21253;&#25324;&#37325;&#35201;&#30340;Langevin&#21160;&#21147;&#23398;&#31034;&#20363;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#20272;&#35745;&#22120;&#65306;&#25193;&#23637;&#21160;&#24577;&#27169;&#20998;&#35299;&#65288;EDMD&#65289;&#21644;&#38477;&#20302;&#31209;&#22238;&#24402;&#65288;RRR&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20851;&#38190;&#20381;&#36182;&#20110;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#35823;&#24046;&#30340;&#26032;&#22855;&#26497;&#23567;&#21270;&#30028;&#38480;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24037;&#20316;&#30340;&#37325;&#35201;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#35889;&#23398;&#20064;&#36793;&#30028;&#21463;&#21040;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#21644;&#20272;&#35745;&#29305;&#24449;&#20989;&#25968;&#30340;&#26032;&#22411;&#24230;&#37327;&#22833;&#30495;&#20989;&#25968;&#30340;&#21516;&#26102;&#25511;&#21046;&#39537;&#21160;&#12290;&#36793;&#30028;&#34920;&#26126;&#65292;EDMD&#21644;RRR&#30340;&#26041;&#24046;&#30456;&#20284;&#65292;&#20294;EDMD&#20248;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Non-linear dynamical systems can be handily described by the associated Koopman operator, whose action evolves every observable of the system forward in time. Learning the Koopman operator and its spectral decomposition from data is enabled by a number of algorithms. In this work we present for the first time non-asymptotic learning bounds for the Koopman eigenvalues and eigenfunctions. We focus on time-reversal-invariant stochastic dynamical systems, including the important example of Langevin dynamics. We analyze two popular estimators: Extended Dynamic Mode Decomposition (EDMD) and Reduced Rank Regression (RRR). Our results critically hinge on novel minimax estimation bounds for the operator norm error, that may be of independent interest. Our spectral learning bounds are driven by the simultaneous control of the operator norm error and a novel metric distortion functional of the estimated eigenfunctions. The bounds indicates that both EDMD and RRR have similar variance, but EDMD su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26131;&#21463;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#29616;&#26377;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#23545;&#25193;&#25955;&#27169;&#22411;&#22522;&#26412;&#26080;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.01316</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26131;&#21463;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Diffusion Models Vulnerable to Membership Inference Attacks?. (arXiv:2302.01316v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26131;&#21463;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#29616;&#26377;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#23545;&#25193;&#25955;&#27169;&#22411;&#22522;&#26412;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#23427;&#20204;&#21487;&#33021;&#24102;&#26469;&#23433;&#20840;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;GANs&#25110;VAE&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#26469;&#35828;&#22522;&#26412;&#26080;&#25928;&#65292;&#35201;&#20040;&#26159;&#22240;&#20026;&#36866;&#29992;&#22330;&#26223;&#19981;&#21516;&#65288;&#20363;&#22914;&#38656;&#35201;GANs&#30340;&#21028;&#21035;&#22120;&#65289;&#65292;&#35201;&#20040;&#26159;&#22240;&#20026;&#20551;&#35774;&#19981;&#24403;&#65288;&#20363;&#22914;&#65292;&#21512;&#25104;&#26679;&#26412;&#21644;&#25104;&#21592;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#26356;&#36817;&#65289;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;Step-wise Error Comparing Membership Inference&#65288;SecMI&#65289;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#20013;&#21069;&#21521;&#36807;&#31243;&#21518;&#39564;&#20272;&#35745;&#30340;&#21305;&#37197;&#24773;&#20917;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;SecMI&#36981;&#24490;MIA&#20013;&#30340;&#24120;&#35265;&#36807;&#25311;&#21512;&#20551;&#35774;&#65292;&#21363;&#25104;&#21592;&#26679;&#26412;&#36890;&#24120;&#20855;&#26377;&#36739;&#23567;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#32780;&#27604;&#36739;&#20445;&#25345;&#26679;&#26412;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;&#26631;&#20934;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#20999;&#27604;&#38634;&#22827;&#25193;&#23637;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21367;&#31215;&#31070;&#32463;&#31639;&#23376;&#65288;CNO&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#19978;&#19979;&#25991;&#20013;&#22788;&#29702;&#20989;&#25968;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102; CNOs &#21487;&#20197;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20986;&#29616;&#30340;&#31639;&#23376;&#21040;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;CNOs &#26174;&#30528;&#20248;&#20110;&#22522;&#32447;&#65292;&#36825;&#20026;&#40065;&#26834;&#20934;&#30830;&#25805;&#20316;&#23398;&#20064;&#30340;&#21478;&#19968;&#31181;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2302.01178</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#40065;&#26834;&#20934;&#30830;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Operators for robust and accurate learning of PDEs. (arXiv:2302.01178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21367;&#31215;&#31070;&#32463;&#31639;&#23376;&#65288;CNO&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#19978;&#19979;&#25991;&#20013;&#22788;&#29702;&#20989;&#25968;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102; CNOs &#21487;&#20197;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20986;&#29616;&#30340;&#31639;&#23376;&#21040;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;CNOs &#26174;&#30528;&#20248;&#20110;&#22522;&#32447;&#65292;&#36825;&#20026;&#40065;&#26834;&#20934;&#30830;&#25805;&#20316;&#23398;&#20064;&#30340;&#21478;&#19968;&#31181;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#25104;&#21151;&#65292;&#22522;&#20110;&#21367;&#31215;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#22240;&#27492;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#30340;&#19978;&#19979;&#25991;&#20013;&#34987;&#24191;&#27867;&#24573;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30830;&#23454;&#33021;&#22815;&#22788;&#29702;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20989;&#25968;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340;&#26550;&#26500;&#31216;&#20026;&#21367;&#31215;&#31070;&#32463;&#31639;&#23376;&#65288;CNO&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#22312;&#35745;&#31639;&#26426;&#19978;&#20197;&#31163;&#25955;&#24418;&#24335;&#23454;&#29616;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#30340;&#36830;&#32493;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26222;&#36866;&#23450;&#29702;&#65292;&#20197;&#23637;&#31034;CNOs&#21487;&#20197;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20986;&#29616;&#30340;&#31639;&#23376;&#21040;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290; CNOs&#22312;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#20855;&#26377;&#21487;&#33021;&#20855;&#26377;&#22810;&#23610;&#24230;&#35299;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26032;&#22871;&#20214;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#34987;&#35266;&#23519;&#21040;&#26174;&#30528;&#20248;&#20110;&#22522;&#32447;&#65292;&#20026;&#40065;&#26834;&#20934;&#30830;&#25805;&#20316;&#23398;&#20064;&#30340;&#21478;&#19968;&#31181;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with possibly multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36895;&#24230;&#36951;&#24536;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#26426;&#22120;&#19978;&#36827;&#34892;&#22312;&#32447;&#35843;&#24230;&#65292;&#31639;&#27861;&#19981;&#38656;&#35201;&#20934;&#30830;&#30340;&#20316;&#19994;&#22788;&#29702;&#36895;&#24230;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.00985</link><description>&lt;p&gt;
&#26080;&#38656;&#31934;&#30830;&#20102;&#35299;&#36895;&#24230;&#30340;&#24555;&#36895;&#22312;&#32447;&#35843;&#24230;&#65306;Speed-Oblivious Online Scheduling
&lt;/p&gt;
&lt;p&gt;
Speed-Oblivious Online Scheduling: Knowing (Precise) Speeds is not Necessary. (arXiv:2302.00985v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36895;&#24230;&#36951;&#24536;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#26426;&#22120;&#19978;&#36827;&#34892;&#22312;&#32447;&#35843;&#24230;&#65292;&#31639;&#27861;&#19981;&#38656;&#35201;&#20934;&#30830;&#30340;&#20316;&#19994;&#22788;&#29702;&#36895;&#24230;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#24322;&#26500;&#26426;&#22120;&#19978;&#36827;&#34892;&#22312;&#32447;&#35843;&#24230;&#65292;&#24182;&#37319;&#29992;&#36895;&#24230;&#36951;&#24536;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#31639;&#27861;&#19981;&#30693;&#36947;&#20934;&#30830;&#30340;&#20316;&#19994;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#23545;&#39044;&#35328;&#31639;&#27861;&#21644;&#38750;&#39044;&#35328;&#31639;&#27861;&#36827;&#34892;&#20102;&#24378;&#22823;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#24182;&#22312;&#21463;&#23454;&#38469;&#29615;&#22659;&#21551;&#21457;&#30340;&#27169;&#22411;&#20013;&#20811;&#26381;&#20102;&#36825;&#20123;&#38382;&#39064;&#65306;(i)&#25105;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#31454;&#20105;&#23398;&#20064;&#24615;&#30340;&#31639;&#27861;&#65292;&#20551;&#35774;&#32473;&#20986;&#20102;&#65288;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65289;&#36895;&#24230;&#39044;&#27979;&#65307;(ii)&#25105;&#20204;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#36895;&#24230;&#25490;&#24207;&#27169;&#22411;&#30340;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#26681;&#25454;&#20854;&#26410;&#30693;&#30340;&#20316;&#19994;&#22788;&#29702;&#36895;&#24230;&#65292;&#24050;&#30693;&#19968;&#31181;&#21333;&#19968;&#20840;&#23616;&#25490;&#24207;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24378;&#26377;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#23545;&#20195;&#34920;&#24615;&#30340;&#24322;&#26500;&#22810;&#26680;&#22788;&#29702;&#22120;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20284;&#20046;&#26159;&#31532;&#19968;&#27425;&#22312;&#38750;&#21512;&#25104;&#30828;&#20214;&#29615;&#22659;&#20013;&#35780;&#20272;&#20855;&#26377;&#39044;&#27979;&#30340;&#35843;&#24230;&#31639;&#27861;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online scheduling on unrelated (heterogeneous) machines in a speed-oblivious setting, where an algorithm is unaware of the exact job-dependent processing speeds. We show strong impossibility results for clairvoyant and non-clairvoyant algorithms and overcome them in models inspired by practical settings: (i) we provide competitive learning-augmented algorithms, assuming that (possibly erroneous) predictions on the speeds are given, and (ii) we provide competitive algorithms for the speed-ordered model, where a single global order of machines according to their unknown job-dependent speeds is known. We prove strong theoretical guarantees and evaluate our findings on a representative heterogeneous multi-core processor. These seem to be the first empirical results for scheduling algorithms with predictions that are evaluated in a non-synthetic hardware environment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CD-GraB&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#20998;&#24067;&#24335;&#31034;&#20363;&#39034;&#24207;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;CD-GraB&#23637;&#29616;&#20986;&#32447;&#24615;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#24182;&#19988;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.00845</link><description>&lt;p&gt;
CD-GraB&#65306;&#21327;&#35843;&#20998;&#24067;&#24335;&#31034;&#20363;&#39034;&#24207;&#20197;&#35777;&#26126;&#21152;&#36895;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training. (arXiv:2302.00845v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CD-GraB&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#20998;&#24067;&#24335;&#31034;&#20363;&#39034;&#24207;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;CD-GraB&#23637;&#29616;&#20986;&#32447;&#24615;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#24182;&#19988;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#22312;&#32447;&#26799;&#24230;&#24179;&#34913;&#65288;GraB&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23384;&#22312;&#22522;&#20110;&#32622;&#25442;&#30340;&#31034;&#20363;&#25490;&#24207;&#21487;&#20197;&#20445;&#35777;&#20248;&#20110;&#38543;&#26426;&#37325;&#25490;&#65288;RR&#65289;&#12290;&#32780;RR&#20250;&#20219;&#24847;&#25490;&#21015;&#35757;&#32451;&#31034;&#20363;&#65292;GraB&#21033;&#29992;&#20808;&#21069;&#26102;&#26399;&#30340;&#38472;&#26087;&#26799;&#24230;&#23545;&#31034;&#20363;&#36827;&#34892;&#25490;&#24207;--&#23454;&#29616;&#27604;RR&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#20294;&#26159;&#65292;GraB&#22312;&#35774;&#35745;&#19978;&#23384;&#22312;&#38480;&#21046;&#65306;&#34429;&#28982;&#23427;&#23637;&#31034;&#20102;&#22312;&#38598;&#20013;&#25968;&#25454;&#19978;&#25193;&#23637;&#35757;&#32451;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#20294;&#24182;&#19981;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#29616;&#20195;&#20998;&#24067;&#24335;ML&#24037;&#20316;&#36127;&#36733;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#35843;&#20998;&#24067;&#24335;GraB&#65288;CD-GraB&#65289;&#65292;&#23427;&#21033;&#29992;&#20808;&#21069;&#20851;&#20110;&#20869;&#26680;&#31232;&#30095;&#21270;&#24037;&#20316;&#30340;&#27934;&#23519;&#21147;&#65292;&#23558;&#32622;&#25442;&#25490;&#24207;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#30340;&#20248;&#21183;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#35774;&#32622;&#12290;CD-GraB&#20855;&#26377;&#21487;&#24573;&#30053;&#30340;&#24320;&#38144;&#65292;&#22312;&#20013;&#22830;&#38598;&#26435;GraB&#19978;&#20855;&#26377;&#32447;&#24615;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#19978;&#32463;&#39564;&#24615;&#22320;&#20248;&#20110;&#20998;&#24067;&#24335;RR&#31561;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on online Gradient Balancing (GraB) has revealed that there exist permutation-based example orderings that are guaranteed to outperform random reshuffling (RR). Whereas RR arbitrarily permutes training examples, GraB leverages stale gradients from prior epochs to order examples -- achieving a provably faster convergence rate than RR. However, GraB is limited by design: While it demonstrates an impressive ability to scale-up training on centralized data, it does not naturally extend to modern distributed ML workloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which uses insights from prior work on kernel thinning to translate the benefits of provably faster permutation-based example ordering to distributed settings. With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate over centralized GraB and outperforms baselines empirically, including distributed RR, on a variety of benchmark tasks.
&lt;/p&gt;</description></item><item><title>dyMEAN&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#20840;&#21407;&#23376;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#34920;&#20301;&#21644;&#19981;&#23436;&#25972;&#30340;&#25239;&#20307;&#24207;&#21015;&#36827;&#34892;&#25239;&#20307;&#35774;&#35745;&#65292;&#22312;&#22788;&#29702;&#20840;&#21407;&#23376;&#26102;&#33021;&#22815;&#22788;&#29702;&#21487;&#21464;&#22823;&#23567;&#30340;&#34507;&#30333;&#27531;&#22522;&#12290;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#21482;&#22788;&#29702;&#25239;&#20307;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#26576;&#20010;&#23376;&#20219;&#21153;&#21644;&#26080;&#27861;&#25429;&#25417;&#20840;&#21407;&#23376;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2302.00203</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#20840;&#21407;&#23376;&#25239;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
End-to-End Full-Atom Antibody Design. (arXiv:2302.00203v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00203
&lt;/p&gt;
&lt;p&gt;
dyMEAN&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#20840;&#21407;&#23376;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#34920;&#20301;&#21644;&#19981;&#23436;&#25972;&#30340;&#25239;&#20307;&#24207;&#21015;&#36827;&#34892;&#25239;&#20307;&#35774;&#35745;&#65292;&#22312;&#22788;&#29702;&#20840;&#21407;&#23376;&#26102;&#33021;&#22815;&#22788;&#29702;&#21487;&#21464;&#22823;&#23567;&#30340;&#34507;&#30333;&#27531;&#22522;&#12290;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#21482;&#22788;&#29702;&#25239;&#20307;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#26576;&#20010;&#23376;&#20219;&#21153;&#21644;&#26080;&#27861;&#25429;&#25417;&#20840;&#21407;&#23376;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#35774;&#35745;&#26159;&#33647;&#29289;&#27835;&#30103;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24403;&#21069;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#32570;&#38519;&#65306;1&#65289;&#21482;&#22788;&#29702;&#25972;&#20010;&#25239;&#20307;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#26576;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#20854;&#27425;&#20248;&#25110;&#36164;&#28304;&#23494;&#38598;&#12290;2&#65289;&#24573;&#30053;&#26694;&#26550;&#21306;&#22495;&#25110;&#20391;&#38142;&#20013;&#30340;&#20219;&#19968;&#37096;&#20998;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#20840;&#21407;&#23376;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#22810;&#36890;&#36947;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;dyMEAN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#20840;&#21407;&#23376;&#27169;&#22411;&#65292;&#29992;&#20110;&#26681;&#25454;&#34920;&#20301;&#21644;&#19981;&#23436;&#25972;&#30340;&#25239;&#20307;&#24207;&#21015;&#36827;&#34892; E&#65288;3&#65289;&#31561;&#21464;&#25239;&#20307;&#35774;&#35745;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#32467;&#26500;&#21021;&#22987;&#21270;&#20316;&#20026;&#25239;&#20307;&#32467;&#26500;&#30340;&#26377;&#30693;&#35782;&#29468;&#27979;&#65292;&#28982;&#21518;&#25552;&#20986;&#24433;&#21709;&#34920;&#20301;-&#25239;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#8220;shadow paratope&#8221;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#22810;&#36890;&#36947;&#31561;&#21464;&#32534;&#30721;&#22120;&#26469;&#26356;&#26032;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35813;&#32534;&#30721;&#22120;&#21487;&#22312;&#32771;&#34385;&#20840;&#21407;&#23376;&#26102;&#22788;&#29702;&#21487;&#21464;&#22823;&#23567;&#30340;&#34507;&#30333;&#27531;&#22522;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;dyMEAN&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20026;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25239;&#20307;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibody design is an essential yet challenging task in various domains like therapeutics and biology. There are two major defects in current learning-based methods: 1) tackling only a certain subtask of the whole antibody design pipeline, making them suboptimal or resource-intensive. 2) omitting either the framework regions or side chains, thus incapable of capturing the full-atom geometry. To address these pitfalls, we propose dynamic Multi-channel Equivariant grAph Network (dyMEAN), an end-to-end full-atom model for E(3)-equivariant antibody design given the epitope and the incomplete sequence of the antibody. Specifically, we first explore structural initialization as a knowledgeable guess of the antibody structure and then propose shadow paratope to bridge the epitope-antibody connections. Both 1D sequences and 3D structures are updated via an adaptive multi-channel equivariant encoder that is able to process protein residues of variable sizes when considering full atoms. Finally,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#38598;&#22823;&#23567;P&#21644;&#21021;&#22987;&#21270;&#35268;&#27169;a&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;SGD&#22122;&#22768;&#22823;&#23567;T&#23545;&#20110;MNIST&#21644;CIFAR10&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24471;&#21040;SGD&#22122;&#22768;&#21487;&#33021;&#26377;&#23475;&#25110;&#26377;&#30410;&#30340;&#30456;&#22270;&#65292;&#21516;&#26102;&#21457;&#29616;&#29305;&#24449;&#28201;&#24230;Tc&#23384;&#22312;&#30456;&#20284;&#30340;&#23610;&#24230;&#20851;&#31995;&#65292;&#22312;&#35757;&#32451;&#38598;&#20026;1000&#65374;10000&#20043;&#38388;&#26102;&#36798;&#21040;&#26368;&#20339;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2301.13703</link><description>&lt;p&gt;
&#12298;&#25581;&#31034;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22122;&#22768;&#22312;&#28145;&#24230;&#23398;&#20064;&#19981;&#21516;&#27169;&#24335;&#19979;&#30340;&#20316;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
Dissecting the Effects of SGD Noise in Distinct Regimes of Deep Learning. (arXiv:2301.13703v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#38598;&#22823;&#23567;P&#21644;&#21021;&#22987;&#21270;&#35268;&#27169;a&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;SGD&#22122;&#22768;&#22823;&#23567;T&#23545;&#20110;MNIST&#21644;CIFAR10&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24471;&#21040;SGD&#22122;&#22768;&#21487;&#33021;&#26377;&#23475;&#25110;&#26377;&#30410;&#30340;&#30456;&#22270;&#65292;&#21516;&#26102;&#21457;&#29616;&#29305;&#24449;&#28201;&#24230;Tc&#23384;&#22312;&#30456;&#20284;&#30340;&#23610;&#24230;&#20851;&#31995;&#65292;&#22312;&#35757;&#32451;&#38598;&#20026;1000&#65374;10000&#20043;&#38388;&#26102;&#36798;&#21040;&#26368;&#20339;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22122;&#22768;&#20309;&#26102;&#20250;&#24433;&#21709;&#21040;&#20854;&#27867;&#21270;&#25928;&#26524;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#27169;&#24335;&#19979;&#36816;&#34892;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#38598;&#22823;&#23567;P&#21644;&#21021;&#22987;&#21270;&#35268;&#27169;a&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;SGD&#22122;&#22768;&#22823;&#23567;T&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;MNIST&#21644;CIFAR10&#22270;&#20687;&#20998;&#31867;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#65306;&#65288;i&#65289;&#22312;&#65288;a&#65292;T&#65289;&#24179;&#38754;&#19978;&#24471;&#21040;&#24615;&#33021;&#30340;&#30456;&#22270;&#12290;&#23427;&#20204;&#34920;&#26126;&#65292;&#26681;&#25454;&#35757;&#32451;&#27169;&#24335;&#65292;SGD&#22122;&#22768;&#21487;&#33021;&#26377;&#23475;&#25110;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22686;&#21152;T&#25110;&#20943;&#23567;&#945;&#37117;&#33021;&#20351;&#32593;&#32476;&#36339;&#20986;&#24816;&#24615;&#27169;&#24335;&#65292;&#20294;&#36825;&#20123;&#21464;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#21487;&#33021;&#26159;&#30456;&#21453;&#30340;&#12290; &#65288;ii&#65289;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#29305;&#24449;&#28201;&#24230;Tc&#19982;&#35757;&#32451;&#38598;&#22823;&#23567;&#23384;&#22312;&#30456;&#20284;&#30340;&#23610;&#24230;&#20851;&#31995;&#65292;&#22312;&#35757;&#32451;&#38598;&#20026;1000&#65374;10000&#20043;&#38388;&#65288;&#36825;&#26159;&#35768;&#22810;&#24212;&#29992;&#30340;&#23454;&#38469;&#35757;&#32451;&#38598;&#22823;&#23567;&#65289;&#26102;&#65292;Tc&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#24615;&#33021;&#36798;&#21040;&#26368;&#20339;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding when the noise in stochastic gradient descent (SGD) affects generalization of deep neural networks remains a challenge, complicated by the fact that networks can operate in distinct training regimes. Here we study how the magnitude of this noise $T$ affects performance as the size of the training set $P$ and the scale of initialization $\alpha$ are varied. For gradient descent, $\alpha$ is a key parameter that controls if the network is `lazy'($\alpha\gg1$) or instead learns features ($\alpha\ll1$). For classification of MNIST and CIFAR10 images, our central results are: (i) obtaining phase diagrams for performance in the $(\alpha,T)$ plane. They show that SGD noise can be detrimental or instead useful depending on the training regime. Moreover, although increasing $T$ or decreasing $\alpha$ both allow the net to escape the lazy regime, these changes can have opposite effects on performance. (ii) Most importantly, we find that the characteristic temperature $T_c$ where th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;PAC-Bayesian bound&#20316;&#20026;Soft Actor-Critic (SAC)&#31639;&#27861;&#35780;&#35770;&#23478;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#35770;&#23478;&#24341;&#23548;&#30340;&#38543;&#26426;&#25628;&#32034;&#25506;&#32034;&#22810;&#20010;&#26410;&#26469;&#26469;&#25552;&#39640;&#22312;&#32447;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#32463;&#20856;&#25511;&#21046;&#21644;&#36816;&#21160;&#20219;&#21153;&#20013;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.12776</link><description>&lt;p&gt;
PAC-Bayesian&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Soft Actor-Critic Learning. (arXiv:2301.12776v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;PAC-Bayesian bound&#20316;&#20026;Soft Actor-Critic (SAC)&#31639;&#27861;&#35780;&#35770;&#23478;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#35770;&#23478;&#24341;&#23548;&#30340;&#38543;&#26426;&#25628;&#32034;&#25506;&#32034;&#22810;&#20010;&#26410;&#26469;&#26469;&#25552;&#39640;&#22312;&#32447;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#32463;&#20856;&#25511;&#21046;&#21644;&#36816;&#21160;&#20219;&#21153;&#20013;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36890;&#36807;&#20004;&#20010;&#20998;&#21035;&#20316;&#31574;&#30053;&#35780;&#20272;&#21644;&#25913;&#36827;&#30340;&#21151;&#33021;&#36924;&#36817;&#22120;&#26469;&#35299;&#20915;&#22686;&#24378;&#23398;&#20064;(RL)&#30340;&#21452;&#37325;&#30446;&#26631;&#12290;&#27492;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#26159;&#20197;&#35757;&#32451;&#19981;&#31283;&#23450;&#20026;&#20195;&#20215;&#30340;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#35780;&#35770;&#23478;&#36924;&#36817;&#35823;&#24046;&#23545;&#28436;&#21592;&#30340;&#30772;&#22351;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#27425;&#37319;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;(PAC)Bayesian&#30028;&#38480;&#20316;&#20026;Soft Actor-Critic (SAC)&#31639;&#27861;&#30340;&#35780;&#35770;&#23478;&#35757;&#32451;&#30446;&#26631;&#26469;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#24403;&#38543;&#26426;&#28436;&#21592;&#36890;&#36807;&#35780;&#35770;&#23478;&#24341;&#23548;&#30340;&#38543;&#26426;&#25628;&#32034;&#25506;&#32034;&#22810;&#20010;&#26410;&#26469;&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#32463;&#20856;&#25511;&#21046;&#21644;&#36816;&#21160;&#20219;&#21153;&#20013;&#65292;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-critic algorithms address the dual goals of reinforcement learning (RL), policy evaluation and improvement, via two separate function approximators. The practicality of this approach comes at the expense of training instability, caused mainly by the destructive effect of the approximation errors of the critic on the actor. We tackle this bottleneck by employing an existing Probably Approximately Correct (PAC) Bayesian bound for the first time as the critic training objective of the Soft Actor-Critic (SAC) algorithm. We further demonstrate that online learning performance improves significantly when a stochastic actor explores multiple futures by critic-guided random search. We observe our resulting algorithm to compare favorably to the state of the art on multiple classical control and locomotion tasks in terms of both sample efficiency and regret minimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#21033;&#29992;DNN&#23545;Banach&#31354;&#38388;&#19978;&#30340;Lipschitz&#31639;&#23376;&#36827;&#34892;&#28145;&#24230;&#31639;&#23376;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;PDE&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#65292;&#21253;&#25324;&#26925;&#22278;&#26041;&#31243;&#12289;&#25243;&#29289;&#32447;&#26041;&#31243;&#21644;Burgers&#26041;&#31243;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#20379;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21270;&#19981;&#21464;&#24615;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.12227</link><description>&lt;p&gt;
&#28145;&#24230;&#31639;&#23376;&#23398;&#20064;&#20943;&#36731;&#20102;PDE&#20013;&#32500;&#25968;&#28798;&#38590;
&lt;/p&gt;
&lt;p&gt;
Deep Operator Learning Lessens the Curse of Dimensionality for PDEs. (arXiv:2301.12227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#21033;&#29992;DNN&#23545;Banach&#31354;&#38388;&#19978;&#30340;Lipschitz&#31639;&#23376;&#36827;&#34892;&#28145;&#24230;&#31639;&#23376;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;PDE&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#65292;&#21253;&#25324;&#26925;&#22278;&#26041;&#31243;&#12289;&#25243;&#29289;&#32447;&#26041;&#31243;&#21644;Burgers&#26041;&#31243;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#20379;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21270;&#19981;&#21464;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#22343;&#24050;&#33719;&#24471;&#26174;&#33879;&#25104;&#21151;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;PDE&#30456;&#20851;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#26412;&#25991;&#21033;&#29992;DNNs&#20272;&#35745;&#20102;&#22312;Banach&#31354;&#38388;&#19978;&#23398;&#20064;Lipschitz&#31639;&#23376;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;PDE&#35299;&#31639;&#23376;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25351;&#23450;DNN&#30340;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#38656;&#35201;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#65292;&#20197;&#20445;&#35777;&#19968;&#23450;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;&#22312;&#23545;&#25968;&#25454;&#20998;&#24067;&#25110;&#31639;&#23376;&#32467;&#26500;&#36827;&#34892;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#31639;&#23376;&#23398;&#20064;&#21487;&#20197;&#25918;&#26494;&#23545;PDE&#31163;&#25955;&#21270;&#20998;&#36776;&#29575;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#20943;&#36731;&#35768;&#22810;&#19982;PDE&#30456;&#20851;&#30340;&#38382;&#39064;&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#65292;&#21253;&#25324;&#26925;&#22278;&#26041;&#31243;&#12289;&#25243;&#29289;&#32447;&#26041;&#31243;&#21644;Burgers&#26041;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#24212;&#29992;&#20110;&#25552;&#20379;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21270;&#19981;&#21464;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have achieved remarkable success in numerous domains, and their application to PDE-related problems has been rapidly advancing. This paper provides an estimate for the generalization error of learning Lipschitz operators over Banach spaces using DNNs with applications to various PDE solution operators. The goal is to specify DNN width, depth, and the number of training samples needed to guarantee a certain testing error. Under mild assumptions on data distributions or operator structures, our analysis shows that deep operator learning can have a relaxed dependence on the discretization resolution of PDEs and, hence, lessen the curse of dimensionality in many PDE-related problems including elliptic equations, parabolic equations, and Burgers equations. Our results are also applied to give insights about discretization-invariant in operator learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#20998;&#21106;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2301.12171</link><description>&lt;p&gt;
ZegOT:&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#30340;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#38646;&#26679;&#26412;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts. (arXiv:2301.12171v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12171
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#20998;&#21106;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#23545;&#27604;&#24615;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#30340;&#25104;&#21151;&#20026;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#24076;&#26395;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#25110;&#23545;CLIP&#27169;&#22359;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ZegOT&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23558;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#19982;&#20923;&#32467;&#30340;&#22270;&#20687;&#23884;&#20837;&#21305;&#37197;&#65292;&#20174;&#32780;&#23454;&#29616;&#38646;&#26679;&#26412;&#20998;&#21106;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22810;&#25552;&#31034;&#26368;&#20248;&#20256;&#36755;&#27714;&#35299;&#22120;&#65288;MPOT&#65289;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#19982;&#20923;&#32467;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#34255;&#23618;&#30340;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#20043;&#38388;&#23398;&#20064;&#20102;&#19968;&#31181;&#26368;&#20248;&#26144;&#23556;&#12290;&#36825;&#31181;&#29420;&#29305;&#30340;&#26144;&#23556;&#26041;&#27861;&#26377;&#25928;&#22320;&#20351;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#20851;&#27880;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#20041;&#23646;&#24615;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent success of large-scale Contrastive Language-Image Pre-training (CLIP) has led to great promise in zero-shot semantic segmentation by transferring image-text aligned knowledge to pixel-level classification. However, existing methods usually require an additional image encoder or retraining/tuning the CLIP module. Here, we propose a novel Zero-shot segmentation with Optimal Transport (ZegOT) method that matches multiple text prompts with frozen image embeddings through optimal transport. In particular, we introduce a novel Multiple Prompt Optimal Transport Solver (MPOT), which is designed to learn an optimal mapping between multiple text prompts and visual feature maps of the frozen image encoder hidden layers. This unique mapping method facilitates each of the multiple text prompts to effectively focus on distinct visual semantic attributes. Through extensive experiments on benchmark datasets, we show that our method achieves the state-of-the-art (SOTA) performance over existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30828;&#24065;&#25237;&#27880;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#23436;&#20840;&#19981;&#38656;&#35201;&#23398;&#20064;&#36895;&#29575;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11294</link><description>&lt;p&gt;
&#22522;&#20110;&#30828;&#24065;&#37319;&#26679;&#30340;&#26080;&#38656;&#23398;&#20064;&#36895;&#29575;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates. (arXiv:2301.11294v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30828;&#24065;&#25237;&#27880;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#23436;&#20840;&#19981;&#38656;&#35201;&#23398;&#20064;&#36895;&#29575;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#65288;ParVI&#65289;&#26041;&#27861;&#22914;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#22240;&#21487;&#25193;&#23637;&#24615;&#22312;&#36125;&#21494;&#26031;&#25512;&#29702;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#36136;&#19981;&#21487;&#36991;&#20813;&#22320;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#65288;&#22914;&#23398;&#20064;&#36895;&#29575;&#65289;&#65292;&#24517;&#39035;&#30001;&#20174;&#19994;&#32773;&#20180;&#32454;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#20197;&#21512;&#36866;&#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#30446;&#26631;&#27979;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#22522;&#20110;&#30828;&#24065;&#25237;&#27880;&#30340;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23436;&#20840;&#19981;&#38656;&#35201;&#23398;&#20064;&#36895;&#29575;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#20540;&#20363;&#23376;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20960;&#20010;&#39640;&#32500;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#19982;&#20854;&#20182;ParVI&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#23398;&#20064;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32422;&#26463;&#21644;&#26080;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#24182;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#26469;&#23637;&#31034;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10932</link><description>&lt;p&gt;
&#20851;&#20110;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected Conditional Risk Measures. (arXiv:2301.10932v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32422;&#26463;&#21644;&#26080;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#24182;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#26469;&#23637;&#31034;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#25511;&#21046;&#19981;&#30830;&#23450;&#32467;&#26524;&#21644;&#30830;&#20445;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#21487;&#38752;&#24615;&#33021;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#34429;&#28982;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#20855;&#26377;&#19982;&#39118;&#38505;&#20013;&#24615;&#24773;&#20917;&#19979;&#30456;&#21516;&#30340;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#21160;&#24577;&#26102;&#38388;&#19968;&#33268;&#39118;&#38505;&#24230;&#37327;&#65292;&#31216;&#20026;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#65288;ECRM&#65289;&#65292;&#24182;&#20026;&#22522;&#20110;ECRM&#30340;&#30446;&#26631;&#20989;&#25968;&#25512;&#23548;&#20986;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#12290;&#22312;&#32422;&#26463;&#30452;&#25509;&#21442;&#25968;&#21270;&#21644;&#26080;&#32422;&#26463;softmax&#21442;&#25968;&#21270;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Risk-sensitive reinforcement learning (RL) has become a popular tool to control the risk of uncertain outcomes and ensure reliable performance in various sequential decision-making problems. While policy gradient methods have been developed for risk-sensitive RL, it remains unclear if these methods enjoy the same global convergence guarantees as in the risk-neutral case. In this paper, we consider a class of dynamic time-consistent risk measures, called Expected Conditional Risk Measures (ECRMs), and derive policy gradient updates for ECRM-based objective functions. Under both constrained direct parameterization and unconstrained softmax parameterization, we provide global convergence and iteration complexities of the corresponding risk-averse policy gradient algorithms. We further test risk-averse variants of REINFORCE and actor-critic algorithms to demonstrate the efficacy of our method and the importance of risk control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#26356;&#32039;&#23494;&#22320;&#30028;&#23450;Transformer&#32534;&#30721;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21516;&#26102;&#26159;&#19979;&#38480;&#21644;&#19978;&#38480;&#30340;&#19968;&#38454;&#36923;&#36753;&#21464;&#20307;&#30340;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#20934;&#30830;&#21051;&#30011;Transformer&#32534;&#30721;&#22120;&#21487;&#35782;&#21035;&#35821;&#35328;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2301.10743</link><description>&lt;p&gt;
&#23545;Transformer&#32534;&#30721;&#22120;&#34920;&#36798;&#33021;&#21147;&#30340;&#26356;&#32039;&#23494;&#30028;&#23450;
&lt;/p&gt;
&lt;p&gt;
Tighter Bounds on the Expressivity of Transformer Encoders. (arXiv:2301.10743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26356;&#32039;&#23494;&#22320;&#30028;&#23450;Transformer&#32534;&#30721;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21516;&#26102;&#26159;&#19979;&#38480;&#21644;&#19978;&#38480;&#30340;&#19968;&#38454;&#36923;&#36753;&#21464;&#20307;&#30340;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#20934;&#30830;&#21051;&#30011;Transformer&#32534;&#30721;&#22120;&#21487;&#35782;&#21035;&#35821;&#35328;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#31995;&#32479;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#26377;&#28508;&#21147;&#25581;&#31034;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#28982;&#32780;&#23545;&#20110;Transformer&#26469;&#35828;&#36825;&#20173;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;Bhattamishra&#31561;&#20154;&#24050;&#32463;&#34920;&#26126;&#65292;Transformer&#32534;&#30721;&#22120;&#33267;&#23569;&#19982;&#19968;&#31181;&#29305;&#23450;&#30340;&#35745;&#25968;&#26426;&#21516;&#31561;&#34920;&#36798;&#33021;&#21147;&#65292;&#32780;Merrill&#21644;Sabharwal&#21017;&#34920;&#26126;&#22266;&#23450;&#31934;&#24230;&#30340;Transformer&#32534;&#30721;&#22120;&#21482;&#33021;&#35782;&#21035;&#32479;&#19968;&#30340;$TC^0$&#35821;&#35328;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#35748;&#20855;&#26377;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#21464;&#20307;&#65292;&#26082;&#26159;&#22266;&#23450;&#31934;&#24230;Transformer&#32534;&#30721;&#22120;&#30340;&#19978;&#30028;&#65292;&#20063;&#26159;Transformer&#32534;&#30721;&#22120;&#30340;&#19979;&#30028;&#65292;&#20174;&#32780;&#23558;&#36825;&#20123;&#32467;&#26524;&#32852;&#31995;&#36215;&#26469;&#24182;&#21152;&#20197;&#21152;&#24378;&#12290;&#36825;&#20351;&#25105;&#20204;&#27604;&#20197;&#21069;&#26356;&#25509;&#36817;&#20934;&#30830;&#21051;&#30011;Transformer&#32534;&#30721;&#22120;&#21487;&#35782;&#21035;&#30340;&#35821;&#35328;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Characterizing neural networks in terms of better-understood formal systems has the potential to yield new insights into the power and limitations of these networks. Doing so for transformers remains an active area of research. Bhattamishra and others have shown that transformer encoders are at least as expressive as a certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision transformer encoders recognize only languages in uniform $TC^0$. We connect and strengthen these results by identifying a variant of first-order logic with counting quantifiers that is simultaneously an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders. This brings us much closer than before to an exact characterization of the languages that transformer encoders recognize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#24490;&#29615;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (CycleGAN) &#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30495;&#23454;&#30340;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#26469;&#25193;&#20805;&#27169;&#25311;&#25968;&#25454;&#65292;&#29983;&#25104;&#20960;&#20046;&#26080;&#27861;&#19982;&#30495;&#23454;&#25968;&#25454;&#21306;&#20998;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#24182;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#26631;&#31614;&#65292;&#36825;&#23637;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#38761;&#21629;&#24615;&#36716;&#21464;&#26448;&#26009;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.07743</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21019;&#24314;&#36924;&#30495;&#30340;&#25195;&#25551;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Leveraging generative adversarial networks to create realistic scanning transmission electron microscopy images. (arXiv:2301.07743v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#24490;&#29615;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (CycleGAN) &#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30495;&#23454;&#30340;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#26469;&#25193;&#20805;&#27169;&#25311;&#25968;&#25454;&#65292;&#29983;&#25104;&#20960;&#20046;&#26080;&#27861;&#19982;&#30495;&#23454;&#25968;&#25454;&#21306;&#20998;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#24182;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#26631;&#31614;&#65292;&#36825;&#23637;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#38761;&#21629;&#24615;&#36716;&#21464;&#26448;&#26009;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#30005;&#23376;&#26174;&#24494;&#23398;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#36890;&#36807;&#33258;&#21160;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#26469;&#38761;&#21629;&#24615;&#36716;&#21464;&#26448;&#26009;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#22914;&#20309;&#24320;&#21457;&#20986;&#24555;&#36895;&#25512;&#24191;&#21040;&#19981;&#21516;&#23454;&#39564;&#26465;&#20214;&#19979;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#37319;&#29992;&#24490;&#29615;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (CycleGAN) &#21644;&#19968;&#20010;&#20114;&#34917;&#31354;&#38388;&#21028;&#21035;&#22120;&#65292;&#36890;&#36807;&#22686;&#21152;&#30495;&#23454;&#30340;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#26469;&#25193;&#20805;&#27169;&#25311;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;CycleGAN&#33021;&#22815;&#29983;&#25104;&#20960;&#20046;&#26080;&#27861;&#19982;&#30495;&#23454;&#25968;&#25454;&#21306;&#20998;&#30340;&#22270;&#20687;&#65292;&#24182;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#26631;&#31614;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476; (FCN) &#22312;&#19968;&#20010;&#30001;450&#19975;&#20010;&#21407;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21333;&#20010;&#21407;&#23376;&#32570;&#38519;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#36866;&#24212;&#24615;&#24378;&#30340;FCN&#65292;&#24182;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#24178;&#39044;&#19979;&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#30340;&#23454;&#39564;&#21464;&#37327;&#65292;&#23637;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#21033;&#29992;&#39640;&#36890;&#37327;&#30005;&#23376;&#26174;&#24494;&#23398;&#38761;&#21629;&#26448;&#26009;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of automation and machine learning (ML) in electron microscopy has the potential to revolutionize materials research through autonomous data collection and processing. A significant challenge lies in developing ML models that rapidly generalize to large data sets under varying experimental conditions. We address this by employing a cycle generative adversarial network (CycleGAN) with a reciprocal space discriminator, which augments simulated data with realistic spatial frequency information. This allows the CycleGAN to generate images nearly indistinguishable from real data and provide labels for ML applications. We showcase our approach by training a fully convolutional network (FCN) to identify single atom defects in a 4.5 million atom data set, collected using automated acquisition in an aberration-corrected scanning transmission electron microscope (STEM). Our method produces adaptable FCNs that can adjust to dynamically changing experimental variables with minimal interve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;Boltzmann&#23494;&#24230;&#21464;&#24418;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#25554;&#20540;&#33021;&#37327;&#20989;&#25968;&#31561;&#23454;&#29616;Boltzmann&#23494;&#24230;&#30340;&#21464;&#24418;&#65292;&#28982;&#21518;&#25214;&#21040;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;&#65292;&#23558;&#26679;&#26412;&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#34920;&#29616;&#22312;&#39640;&#26031;&#28151;&#21512;&#21644;&#37327;&#23376;&#21147;&#23398;&#31890;&#23376;&#30340;Boltzmann&#23494;&#24230;&#19978;&#27604;KL-&#21453;&#25955;&#24230;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.07388</link><description>&lt;p&gt;
&#23398;&#20064;Boltzmann&#23494;&#24230;&#30340;&#21464;&#24418;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Learning Deformation Trajectories of Boltzmann Densities. (arXiv:2301.07388v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;Boltzmann&#23494;&#24230;&#21464;&#24418;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#25554;&#20540;&#33021;&#37327;&#20989;&#25968;&#31561;&#23454;&#29616;Boltzmann&#23494;&#24230;&#30340;&#21464;&#24418;&#65292;&#28982;&#21518;&#25214;&#21040;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;&#65292;&#23558;&#26679;&#26412;&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#34920;&#29616;&#22312;&#39640;&#26031;&#28151;&#21512;&#21644;&#37327;&#23376;&#21147;&#23398;&#31890;&#23376;&#30340;Boltzmann&#23494;&#24230;&#19978;&#27604;KL-&#21453;&#25955;&#24230;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26679;&#26412;&#20294;&#23384;&#22312;&#33021;&#37327;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#33021;&#37327;&#20989;&#25968;$f_1$&#21644;&#24191;&#20041;&#39640;&#26031;&#20989;&#25968;$f_0$&#20043;&#38388;&#30340;&#39044;&#23450;&#25110;&#23398;&#20064;&#25554;&#20540;$f_t$&#12290;&#33021;&#37327;&#20989;&#25968;&#30340;&#25554;&#20540;&#24341;&#36215;Boltzmann&#23494;&#24230;$p_t\propto e^{-f_t}$&#30340;&#25554;&#20540;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#27839;&#30528;&#26063;$p_t$&#30340;&#26102;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;$V_t$&#65292;&#23558;&#26679;&#26412;&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#12290;&#23558;&#26679;&#26412;&#27839;&#30528;&#26063;$p_t$&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#30340;&#26465;&#20214;&#21487;&#20197;&#36716;&#21270;&#20026;$V_t$&#21644;$f_t$&#20043;&#38388;&#30340;PDE&#65292;&#25105;&#20204;&#20248;&#21270;$V_t$&#21644;$f_t$&#20197;&#28385;&#36275;&#27492;PDE&#12290;&#25105;&#20204;&#22312;&#39640;&#26031;&#28151;&#21512;&#21644;&#21452;&#20117;&#21183;&#30340;&#37327;&#23376;&#21147;&#23398;&#31890;&#23376;&#30340;Boltzmann&#23494;&#24230;&#19978;&#23454;&#39564;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#30446;&#26631;&#19982;KL-&#21453;&#25955;&#24230;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a training objective for continuous normalizing flows that can be used in the absence of samples but in the presence of an energy function. Our method relies on either a prescribed or a learnt interpolation $f_t$ of energy functions between the target energy $f_1$ and the energy function of a generalized Gaussian $f_0(x) = ||x/\sigma||_p^p$. The interpolation of energy functions induces an interpolation of Boltzmann densities $p_t \propto e^{-f_t}$ and we aim to find a time-dependent vector field $V_t$ that transports samples along the family $p_t$ of densities. The condition of transporting samples along the family $p_t$ can be translated to a PDE between $V_t$ and $f_t$ and we optimize $V_t$ and $f_t$ to satisfy this PDE. We experimentally compare the proposed training objective to the reverse KL-divergence on Gaussian mixtures and on the Boltzmann density of a quantum mechanical particle in a double-well potential.
&lt;/p&gt;</description></item><item><title>&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#38656;&#27714;&#27169;&#22411;&#30340;&#22825;&#28982;&#20559;&#35265;&#23548;&#33268;&#20102;&#35266;&#27979;&#21040;&#30340;&#38656;&#27714;&#19982;&#23454;&#38469;&#38656;&#27714;&#26377;&#24046;&#24322;&#65292;&#20351;&#29992;&#26377;&#20851;&#23457;&#26597;&#30340;&#27169;&#22411;&#26469;&#27169;&#25311;&#20805;&#30005;&#38656;&#27714;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#30495;&#23454;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2301.06418</link><description>&lt;p&gt;
&#27880;&#24847;&#24046;&#36317;&#65306;&#24314;&#27169;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#38656;&#27714;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mind the Gap: Modelling Difference Between Censored and Uncensored Electric Vehicle Charging Demand. (arXiv:2301.06418v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06418
&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#38656;&#27714;&#27169;&#22411;&#30340;&#22825;&#28982;&#20559;&#35265;&#23548;&#33268;&#20102;&#35266;&#27979;&#21040;&#30340;&#38656;&#27714;&#19982;&#23454;&#38469;&#38656;&#27714;&#26377;&#24046;&#24322;&#65292;&#20351;&#29992;&#26377;&#20851;&#23457;&#26597;&#30340;&#27169;&#22411;&#26469;&#27169;&#25311;&#20805;&#30005;&#38656;&#27714;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#30495;&#23454;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20805;&#30005;&#35760;&#24405;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#38656;&#27714;&#27169;&#22411;&#20250;&#22825;&#28982;&#22320;&#23545;&#21487;&#29992;&#20805;&#30005;&#22120;&#30340;&#20379;&#24212;&#20135;&#29983;&#20559;&#35265;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#32771;&#34385;&#21040;&#34987;&#21344;&#29992;&#20805;&#30005;&#31449;&#21644;&#31454;&#20105;&#23545;&#20805;&#30005;&#38656;&#27714;&#30340;&#25439;&#22833;&#12290;&#36825;&#20123;&#25439;&#22833;&#34920;&#26126;&#23454;&#38469;&#38656;&#27714;&#24456;&#21487;&#33021;&#27604;&#20805;&#30005;&#35760;&#24405;&#21453;&#26144;&#30340;&#38656;&#27714;&#26356;&#39640;&#65292;&#21363;&#30495;&#23454;&#38656;&#27714;&#26159;&#28508;&#22312;&#30340;&#65288;&#26410;&#35266;&#23519;&#21040;&#30340;&#65289;&#65292;&#32780;&#35266;&#23519;&#32467;&#26524;&#21017;&#26159;&#34987;&#23457;&#26597;&#30340;&#12290;&#22240;&#27492;&#65292;&#20381;&#36182;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#20805;&#30005;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#22312;&#26410;&#26469;&#22522;&#30784;&#35774;&#26045;&#25193;&#23637;&#21644;&#20379;&#24212;&#31649;&#29702;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#20272;&#35745;&#20805;&#30005;&#30340;&#30495;&#23454;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26377;&#20851;&#23457;&#26597;&#30340;&#27169;&#22411;&#26469;&#27169;&#25311;&#20805;&#30005;&#38656;&#27714;&#20197;&#35299;&#20915;&#27492;&#38480;&#21046;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#23457;&#26597;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#24182;&#20174;&#35266;&#23519;&#21040;&#30340;&#20805;&#30005;&#35760;&#24405;&#20013;&#23398;&#20064;&#30495;&#23454;&#30340;&#28508;&#22312;&#38656;&#27714;&#20998;&#24067;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#34987;&#21344;&#29992;&#20805;&#30005;&#31449;&#21644;&#31454;&#20105;&#26381;&#21153;&#22914;&#20309;&#20351;&#29992;GPS&#36712;&#36857;&#23457;&#26597;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electric vehicle charging demand models, with charging records as input, will inherently be biased toward the supply of available chargers. These models often fail to account for demand lost from occupied charging stations and competitors. The lost demand suggests that the actual demand is likely higher than the charging records reflect, i.e., the true demand is latent (unobserved), and the observations are censored. As a result, machine learning models that rely on these observed records for forecasting charging demand may be limited in their application in future infrastructure expansion and supply management, as they do not estimate the true demand for charging. We propose using censorship-aware models to model charging demand to address this limitation. These models incorporate censorship in their loss functions and learn the true latent demand distribution from observed charging records. We study how occupied charging stations and competing services censor demand using GPS traject
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#39044;&#27979;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#33073;&#21457;&#21644;&#22836;&#30382;&#30456;&#20851;&#30142;&#30149;&#65306;&#26001;&#31171;&#12289;&#38134;&#23633;&#30149;&#21644;&#27611;&#22218;&#28814;&#12290;</title><link>http://arxiv.org/abs/2301.00122</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#36827;&#34892;&#22836;&#21457;&#21644;&#22836;&#30382;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hair and Scalp Disease Detection using Machine Learning and Image Processing. (arXiv:2301.00122v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#39044;&#27979;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#33073;&#21457;&#21644;&#22836;&#30382;&#30456;&#20851;&#30142;&#30149;&#65306;&#26001;&#31171;&#12289;&#38134;&#23633;&#30149;&#21644;&#27611;&#22218;&#28814;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26377;8000&#19975;&#32654;&#22269;&#20154;&#22240;&#34928;&#32769;&#12289;&#21387;&#21147;&#12289;&#33647;&#29289;&#25110;&#22522;&#22240;&#32536;&#30001;&#32780;&#24739;&#19978;&#33073;&#21457;&#30151;&#12290;&#22836;&#21457;&#21644;&#22836;&#30382;&#30456;&#20851;&#30340;&#30142;&#30149;&#24448;&#24448;&#22312;&#26089;&#26399;&#26102;&#34987;&#24573;&#35270;&#12290;&#26377;&#26102;&#65292;&#24739;&#32773;&#26080;&#27861;&#21306;&#20998;&#33073;&#21457;&#21644;&#27491;&#24120;&#30340;&#25481;&#21457;&#12290;&#35786;&#26029;&#19982;&#22836;&#21457;&#26377;&#20851;&#30340;&#30142;&#30149;&#24456;&#32791;&#26102;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#19987;&#19994;&#30340;&#30382;&#32932;&#31185;&#21307;&#29983;&#36827;&#34892;&#35270;&#35273;&#21644;&#21307;&#23398;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;&#24635;&#20307;&#35786;&#26029;&#26102;&#38388;&#30340;&#24310;&#36831;&#21152;&#21095;&#20102;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#30001;&#20110;&#20855;&#26377;&#22270;&#20687;&#22788;&#29702;&#33021;&#21147;&#65292;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#21307;&#30103;&#20445;&#20581;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#65292;&#21487;&#20197;&#39044;&#27979;&#20687;&#30284;&#30151;&#21644;&#32959;&#30244;&#31561;&#33268;&#21629;&#30142;&#30149;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#21644;&#24739;&#32773;&#65292;&#25552;&#20379;&#26089;&#26399;&#30151;&#29366;&#30340;&#21021;&#27493;&#35265;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#39044;&#27979;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#33073;&#21457;&#21644;&#22836;&#30382;&#30456;&#20851;&#30142;&#30149;&#65306;&#26001;&#31171;&#12289;&#38134;&#23633;&#30149;&#21644;&#27611;&#22218;&#28814;&#12290;&#28982;&#32780;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Almost 80 million Americans suffer from hair loss due to aging, stress, medication, or genetic makeup. Hair and scalp-related diseases often go unnoticed in the beginning. Sometimes, a patient cannot differentiate between hair loss and regular hair fall. Diagnosing hair-related diseases is time-consuming as it requires professional dermatologists to perform visual and medical tests. Because of that, the overall diagnosis gets delayed, which worsens the severity of the illness. Due to the image-processing ability, neural network-based applications are used in various sectors, especially healthcare and health informatics, to predict deadly diseases like cancers and tumors. These applications assist clinicians and patients and provide an initial insight into early-stage symptoms. In this study, we used a deep learning approach that successfully predicts three main types of hair loss and scalp-related diseases: alopecia, psoriasis, and folliculitis. However, limited study in this area, una
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10823</link><description>&lt;p&gt;
&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#20381;&#36182;&#32467;&#26500;&#21270;&#27880;&#37322;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#26399;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#30340;RE&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;RE&#30446;&#26631;&#39044;&#35757;&#32451;&#20851;&#31995;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23427;&#38459;&#27490;RE&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20851;&#31995;&#21487;&#33021;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36731;&#26494;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#65292;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#25991;&#26723;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#26174;&#30528;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10511</link><description>&lt;p&gt;
&#20309;&#26102;&#19981;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;&#65306;&#25506;&#32034;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#35760;&#24518;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#26263;&#31034;&#20102;&#20165;&#20381;&#38752;&#20854;&#21442;&#25968;&#26469;&#32534;&#30721;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#22312;PopQA&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30693;&#35782;&#25506;&#27979;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38271;&#23614;&#20013;&#65292;&#25193;&#23637;&#35268;&#27169;&#26080;&#27861;&#26126;&#26174;&#25913;&#21892;&#35760;&#24518;&#23454;&#38469;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32988;&#36807;&#32423;&#21035;&#22823;&#24471;&#22810;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26410;&#32463;&#21327;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#39640;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#38656;&#35201;&#26102;&#26816;&#32034;&#38750;&#21442;&#25968;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#26102;&#24207;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#20272;&#35745;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#32622;&#20449;&#39044;&#27979;&#31639;&#27861;SPCI&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#65292;SPCI&#22312;&#25152;&#38656;&#32463;&#39564;&#35206;&#30422;&#19979;&#30340;&#21306;&#38388;&#23485;&#24230;&#26174;&#33879;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2212.03463</link><description>&lt;p&gt;
&#26102;&#24207;&#25968;&#25454;&#30340;&#24207;&#21015;&#39044;&#27979;&#32622;&#20449;&#25512;&#26029;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sequential Predictive Conformal Inference for Time Series. (arXiv:2212.03463v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03463
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#26102;&#24207;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#20272;&#35745;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#32622;&#20449;&#39044;&#27979;&#31639;&#27861;SPCI&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#65292;SPCI&#22312;&#25152;&#38656;&#32463;&#39564;&#35206;&#30422;&#19979;&#30340;&#21306;&#38388;&#23485;&#24230;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#33258;&#30001;&#30340;&#24207;&#21015;&#25968;&#25454;&#65288;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#65289;&#32622;&#20449;&#39044;&#27979;&#31639;&#27861;&#65292;&#31216;&#20026;&#8220;&#24207;&#21015;&#39044;&#27979;&#32622;&#20449;&#25512;&#26029;&#8221;&#65288;SPCI&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#19981;&#21487;&#20132;&#25442;&#30340;&#24615;&#36136;&#65292;&#22240;&#27492;&#35768;&#22810;&#29616;&#26377;&#30340;&#32622;&#20449;&#39044;&#27979;&#31639;&#27861;&#19981;&#36866;&#29992;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#22312;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#26102;&#65292;&#33258;&#36866;&#24212;&#37325;&#26032;&#20272;&#35745;&#38750;&#19968;&#33268;&#24615;&#20998;&#25968;&#65288;&#20363;&#22914;&#39044;&#27979;&#27531;&#24046;&#65289;&#30340;&#26465;&#20214;&#20998;&#20301;&#25968;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#32622;&#20449;&#39044;&#27979;&#21306;&#38388;&#30340;&#38382;&#39064;&#35270;&#20026;&#39044;&#27979;&#26410;&#26469;&#27531;&#24046;&#30340;&#20998;&#20301;&#25968;&#65292;&#32473;&#23450;&#29992;&#25143;&#25351;&#23450;&#30340;&#28857;&#39044;&#27979;&#31639;&#27861;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#22312;&#25193;&#23637;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#19968;&#33268;&#24615;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#26465;&#20214;&#35206;&#30422;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SPCI&#30456;&#23545;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#22312;&#25152;&#38656;&#32463;&#39564;&#35206;&#30422;&#19979;&#30340;&#21306;&#38388;&#23485;&#24230;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the \textit{sequential predictive conformal inference} (\texttt{SPCI}). We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction residuals), upon exploiting the temporal dependence among them. More precisely, we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a user-specified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real-data experiments, we demonstrate a significant reduction in interval width of \texttt{SPCI} compared to other existing methods under the desired empirical cover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#32447;&#24615;&#33218;&#26426;&#65288;DLB&#65289;&#36825;&#19968;&#27010;&#24565;&#65292;&#36825;&#26159;&#32447;&#24615;&#33218;&#26426;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#38544;&#34255;&#29366;&#24577;&#12290;&#36825;&#19968;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#22312;&#23454;&#38469;&#20915;&#31574;&#20013;&#34892;&#21160;&#19981;&#20250;&#31435;&#21363;&#21453;&#26144;&#22312;&#21453;&#39304;&#19978;&#65292;&#24182;&#22312;&#36739;&#38271;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#25193;&#25955;&#20854;&#24433;&#21709;&#25152;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.08997</link><description>&lt;p&gt;
&#21160;&#24577;&#32447;&#24615;&#33218;&#26426;&#65288;Dynamical Linear Bandits&#65289;
&lt;/p&gt;
&lt;p&gt;
Dynamical Linear Bandits. (arXiv:2211.08997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#32447;&#24615;&#33218;&#26426;&#65288;DLB&#65289;&#36825;&#19968;&#27010;&#24565;&#65292;&#36825;&#26159;&#32447;&#24615;&#33218;&#26426;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#38544;&#34255;&#29366;&#24577;&#12290;&#36825;&#19968;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#22312;&#23454;&#38469;&#20915;&#31574;&#20013;&#34892;&#21160;&#19981;&#20250;&#31435;&#21363;&#21453;&#26144;&#22312;&#21453;&#39304;&#19978;&#65292;&#24182;&#22312;&#36739;&#38271;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#25193;&#25955;&#20854;&#24433;&#21709;&#25152;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#34892;&#21160;&#19981;&#20250;&#31435;&#21363;&#21453;&#26144;&#22312;&#21453;&#39304;&#19978;&#65292;&#24182;&#22312;&#36739;&#38271;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#25193;&#25955;&#20854;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#22312;&#22312;&#32447;&#24191;&#21578;&#20013;&#65292;&#25237;&#36164;&#20110;&#26576;&#20010;&#24179;&#21488;&#20250;&#20135;&#29983;&#21363;&#26102;&#30340;&#24847;&#35782;&#22686;&#38271;&#65292;&#20294;&#23454;&#38469;&#22238;&#25253;&#65292;&#21363;&#36716;&#21270;&#65292;&#21487;&#33021;&#21457;&#29983;&#22312;&#26410;&#26469;&#36739;&#36828;&#30340;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#36716;&#21270;&#26159;&#21542;&#21457;&#29983;&#21462;&#20915;&#20110;&#24847;&#35782;&#30340;&#22686;&#38271;&#36895;&#24230;&#12289;&#20854;&#28040;&#22833;&#25928;&#24212;&#20197;&#21450;&#19982;&#20854;&#20182;&#24191;&#21578;&#24179;&#21488;&#30340;&#21327;&#21516;&#25110;&#24178;&#25200;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#26377;&#24310;&#36831;&#21644;&#32858;&#21512;&#21453;&#39304;&#21487;&#33021;&#24615;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#65292;&#27809;&#26377;&#20851;&#20110;&#19968;&#20010;&#34892;&#21160;&#22312;&#26410;&#26469;&#22914;&#20309;&#20256;&#25773;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#24573;&#30053;&#20102;&#21487;&#33021;&#30340;&#21160;&#24577;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#21363;&#21160;&#24577;&#32447;&#24615;&#33218;&#26426;&#65288;DLB&#65289;&#65292;&#36825;&#26159;&#32447;&#24615;&#33218;&#26426;&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#20854;&#29305;&#24449;&#26159;&#20855;&#26377;&#38544;&#34255;&#29366;&#24577;&#12290;&#24403;&#25191;&#34892;&#19968;&#20010;&#34892;&#21160;&#26102;&#65292;&#23398;&#20064;&#32773;&#35266;&#23519;&#21040;&#19968;&#20010;&#22122;&#22768;&#22238;&#25253;&#65292;&#20854;&#22343;&#20540;&#26159;&#19968;&#20010;&#32447;&#24615;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
In many real-world sequential decision-making problems, an action does not immediately reflect on the feedback and spreads its effects over a long time frame. For instance, in online advertising, investing in a platform produces an instantaneous increase of awareness, but the actual reward, i.e., a conversion, might occur far in the future. Furthermore, whether a conversion takes place depends on: how fast the awareness grows, its vanishing effects, and the synergy or interference with other advertising platforms. Previous work has investigated the Multi-Armed Bandit framework with the possibility of delayed and aggregated feedback, without a particular structure on how an action propagates in the future, disregarding possible dynamical effects. In this paper, we introduce a novel setting, the Dynamical Linear Bandits (DLB), an extension of the linear bandits characterized by a hidden state. When an action is performed, the learner observes a noisy reward whose mean is a linear functio
&lt;/p&gt;</description></item><item><title>FedGen&#20026;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#21487;&#25512;&#24191;&#24615;&#65292;&#20801;&#35768;&#20998;&#24067;&#24335;&#35774;&#22791;&#20849;&#21516;&#35782;&#21035;&#21644;&#21306;&#20998;&#20266;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#24067;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2211.01914</link><description>&lt;p&gt;
FedGen: &#36866;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#30340;&#21487;&#25512;&#24191;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedGen: Generalizable Federated Learning for Sequential Data. (arXiv:2211.01914v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01914
&lt;/p&gt;
&lt;p&gt;
FedGen&#20026;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#21487;&#25512;&#24191;&#24615;&#65292;&#20801;&#35768;&#20998;&#24067;&#24335;&#35774;&#22791;&#20849;&#21516;&#35782;&#21035;&#21644;&#21306;&#20998;&#20266;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#24067;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36981;&#24490;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#23384;&#22312;&#20266;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#24448;&#24448;&#38590;&#20197;&#25512;&#24191;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20998;&#24067;&#24335;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#20998;&#24067;&#24335;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#30340;&#20559;&#24046;&#21644;&#25968;&#25454;&#37319;&#26679;&#38382;&#39064;&#65292;&#20250;&#20135;&#29983;&#20266;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#38169;&#35823;&#22320;&#24433;&#21709;&#27169;&#22411;&#12290;&#24403;&#21069;&#30340;&#25512;&#24191;&#26041;&#27861;&#26159;&#20026;&#38598;&#20013;&#24335;&#35757;&#32451;&#35774;&#35745;&#30340;&#65292;&#35797;&#22270;&#35782;&#21035;&#20855;&#26377;&#19982;&#30446;&#26631;&#19981;&#21464;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20943;&#23569;&#20266;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38590;&#20197;&#33719;&#24471;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedGen&#30340;&#21487;&#25512;&#24191;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20801;&#35768;&#23458;&#25143;&#31471;&#20197;&#21327;&#20316;&#30340;&#26041;&#24335;&#35782;&#21035;&#21644;&#21306;&#20998;&#20266;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#24067;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing federated learning models that follow the standard risk minimization paradigm of machine learning often fail to generalize in the presence of spurious correlations in the training data. In many real-world distributed settings, spurious correlations exist due to biases and data sampling issues on distributed devices or clients that can erroneously influence models. Current generalization approaches are designed for centralized training and attempt to identify features that have an invariant causal relationship with the target, thereby reducing the effect of spurious features. However, such invariant risk minimization approaches rely on apriori knowledge of training data distributions which is hard to obtain in many applications. In this work, we present a generalizable federated learning framework called FedGen, which allows clients to identify and distinguish between spurious and invariant features in a collaborative manner without prior knowledge of training distributions. We
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#21453;&#28436;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#26080;&#36755;&#20837;&#36755;&#20986;&#23545;&#30340;&#21453;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#23398;&#20064;&#26399;&#26395;&#36755;&#20986;&#20998;&#24067;&#65292;&#21487;&#20197;&#35299;&#20915;&#25152;&#26399;&#26395;&#36755;&#20986;&#19982;&#21021;&#22987;&#38543;&#26426;&#29468;&#27979;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#30340;&#20559;&#31227;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.01724</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#36845;&#20195;&#21453;&#28436;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Control by Iterative Inversion. (arXiv:2211.01724v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01724
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#21453;&#28436;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#26080;&#36755;&#20837;&#36755;&#20986;&#23545;&#30340;&#21453;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#23398;&#20064;&#26399;&#26395;&#36755;&#20986;&#20998;&#24067;&#65292;&#21487;&#20197;&#35299;&#20915;&#25152;&#26399;&#26395;&#36755;&#20986;&#19982;&#21021;&#22987;&#38543;&#26426;&#29468;&#27979;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#30340;&#20559;&#31227;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36845;&#20195;&#21453;&#28436;&#8221;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26080;&#36755;&#20837;&#36755;&#20986;&#23545;&#30340;&#21453;&#20989;&#25968;&#65292;&#21482;&#20351;&#29992;&#25152;&#38656;&#36755;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#21644;&#23545;&#21069;&#21521;&#20989;&#25968;&#30340;&#35775;&#38382;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#25152;&#26399;&#26395;&#36755;&#20986;&#19982;&#21021;&#22987;&#38543;&#26426;&#29468;&#27979;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20989;&#25968;&#30340;&#30456;&#24403;&#20005;&#26684;&#30340;&#26465;&#20214;&#19979;&#65292;&#36845;&#20195;&#21453;&#28436;&#21487;&#20197;&#27491;&#30830;&#22320;&#24341;&#23548;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;&#36845;&#20195;&#21453;&#28436;&#24212;&#29992;&#20110;&#23398;&#20064;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#36755;&#20837;&#26159;&#19968;&#32452;&#25152;&#38656;&#34892;&#20026;&#30340;&#28436;&#31034;&#65292;&#20197;&#36712;&#36857;&#30340;&#35270;&#39057;&#23884;&#20837;&#65288;&#27809;&#26377;&#21160;&#20316;&#65289;&#30340;&#24418;&#24335;&#32473;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36845;&#20195;&#22320;&#23398;&#20064;&#27169;&#20223;&#30001;&#24403;&#21069;&#31574;&#30053;&#29983;&#25104;&#30340;&#36712;&#36857;&#65292;&#21463;&#21040;&#38543;&#26426;&#25506;&#32034;&#22122;&#22768;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22870;&#21169;&#65292;&#20165;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#22823;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36712;&#36857;&#23884;&#20837;&#25216;&#26415;&#21644;&#31574;&#30053;&#34920;&#31034;&#12290;&#20107;&#23454;&#19978;&#65292;&#20351;&#29992;VQ-VAE&#23884;&#20837;&#21644;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31574;&#30053;&#34920;&#31034;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#25511;&#21046;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose $\textit{iterative inversion}$ -- an algorithm for learning an inverse function without input-output pairs, but only with samples from the desired output distribution and access to the forward function. The key challenge is a $\textit{distribution shift}$ between the desired outputs and the outputs of an initial random guess, and we prove that iterative inversion can steer the learning correctly, under rather strict conditions on the function. We apply iterative inversion to learn control. Our input is a set of demonstrations of desired behavior, given as video embeddings of trajectories (without actions), and our method iteratively learns to imitate trajectories generated by the current policy, perturbed by random exploration noise. Our approach does not require rewards, and only employs supervised learning, which can be easily scaled to use state-of-the-art trajectory embedding techniques and policy representations. Indeed, with a VQ-VAE embedding, and a transformer-based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#20256;&#32479;&#30340;&#24425;&#31080;&#20551;&#35828;&#65288;LTH&#65289;&#25193;&#23637;&#21040;&#30001;&#22270;&#20687;&#34917;&#19969;&#32452;&#25104;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#35777;&#26126;&#23384;&#22312;&#19968;&#20010;&#23376;&#38598;&#30340;&#36755;&#20837;&#22270;&#20687;&#34917;&#19969;&#20351;&#24471;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#65292;&#24182;&#19988;&#36798;&#21040;&#19982;&#20351;&#29992;&#25152;&#26377;&#22270;&#20687;&#34917;&#19969;&#35757;&#32451;&#30340;ViTs&#30456;&#20284;&#30340;&#31934;&#24230;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#37117;&#26159;&#21487;&#34892;&#30340;&#21644;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.01484</link><description>&lt;p&gt;
&#38754;&#21521;&#35270;&#35273;Transformer&#30340;&#25968;&#25454;&#32423;&#24425;&#31080;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Data Level Lottery Ticket Hypothesis for Vision Transformers. (arXiv:2211.01484v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20256;&#32479;&#30340;&#24425;&#31080;&#20551;&#35828;&#65288;LTH&#65289;&#25193;&#23637;&#21040;&#30001;&#22270;&#20687;&#34917;&#19969;&#32452;&#25104;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#35777;&#26126;&#23384;&#22312;&#19968;&#20010;&#23376;&#38598;&#30340;&#36755;&#20837;&#22270;&#20687;&#34917;&#19969;&#20351;&#24471;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#65292;&#24182;&#19988;&#36798;&#21040;&#19982;&#20351;&#29992;&#25152;&#26377;&#22270;&#20687;&#34917;&#19969;&#35757;&#32451;&#30340;ViTs&#30456;&#20284;&#30340;&#31934;&#24230;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#37117;&#26159;&#21487;&#34892;&#30340;&#21644;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24425;&#31080;&#20551;&#35828;&#65288;LTH&#65289;&#22768;&#31216;&#22312;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#30528;&#19968;&#20010;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#21644;&#19968;&#20010;&#31216;&#20026;&#8220;&#33719;&#22870;&#24425;&#31080;&#8221;&#30340;&#36866;&#24403;&#38543;&#26426;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#20415;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23427;&#65292;&#20351;&#20854;&#20960;&#20046;&#20687;&#23494;&#38598;&#32593;&#32476;&#19968;&#26679;&#22909;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23545;&#20110;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#20013;LTH&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#34987;&#35780;&#20272;&#12290;&#26412;&#25991;&#39318;&#20808;&#34920;&#26126;&#20102;&#22312;&#29616;&#26377;&#26041;&#27861;&#19979;&#65292;&#22312;ViTs&#30340;&#26435;&#37325;&#32423;&#21035;&#19978;&#23547;&#25214;&#20256;&#32479;&#30340;&#33719;&#22870;&#24425;&#31080;&#26159;&#22256;&#38590;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;ViTs&#30340;LTH&#25512;&#24191;&#21040;&#30001;&#22270;&#20687;&#34917;&#19969;&#32452;&#25104;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#21463;ViTs&#36755;&#20837;&#20381;&#36182;&#21551;&#21457;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23384;&#22312;&#19968;&#20010;&#36755;&#20837;&#22270;&#20687;&#34917;&#19969;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#36890;&#36807;&#20165;&#20351;&#29992;&#35813;&#23376;&#38598;&#65292;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;ViT&#65292;&#24182;&#36798;&#21040;&#19982;&#20351;&#29992;&#25152;&#26377;&#22270;&#20687;&#34917;&#19969;&#35757;&#32451;&#30340;ViTs&#30456;&#20284;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36755;&#20837;&#34917;&#19969;&#23376;&#38598;&#31216;&#20026;em&#33719;&#22870;&#24425;&#31080;&#65292;&#23427;&#20195;&#34920;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#31080;&#36873;&#25321;&#22120;&#29983;&#25104;&#24102;&#26377;em&#33719;&#22870;&#24425;&#31080;&#30340;&#26435;&#37325;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;EMD&#35757;&#32451;&#36825;&#20123;&#8220;&#33719;&#22870;&#8221;&#23376;&#38598;&#65292;&#21487;&#20197;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#37117;&#26159;&#21487;&#34892;&#30340;&#21644;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional lottery ticket hypothesis (LTH) claims that there exists a sparse subnetwork within a dense neural network and a proper random initialization method called the winning ticket, such that it can be trained from scratch to almost as good as the dense counterpart. Meanwhile, the research of LTH in vision transformers (ViTs) is scarcely evaluated. In this paper, we first show that the conventional winning ticket is hard to find at the weight level of ViTs by existing methods. Then, we generalize the LTH for ViTs to input data consisting of image patches inspired by the input dependence of ViTs. That is, there exists a subset of input image patches such that a ViT can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches. We call this subset of input patches the em winning tickets, which represent a significant amount of information in the input data. We use a ticket selector to generate the w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#24418;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#33258;&#28982;&#21644;&#30452;&#35266;&#30340;&#25512;&#29702;&#36807;&#21435;&#26102;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00472</link><description>&lt;p&gt;
&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Backtracking Counterfactuals. (arXiv:2211.00472v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#24418;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#33258;&#28982;&#21644;&#30452;&#35266;&#30340;&#25512;&#29702;&#36807;&#21435;&#26102;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#26159;&#20154;&#31867;&#24605;&#32500;&#20013;&#24191;&#27867;&#23384;&#22312;&#30340;&#19968;&#31181;&#25512;&#29702;&#26041;&#24335;&#8212;&#8212;&#35774;&#24819;&#19968;&#20123;&#20551;&#35774;&#22330;&#26223;&#25110;&#21487;&#33021;&#23384;&#22312;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#24773;&#20917;&#19982;&#23454;&#38469;&#24773;&#20917;&#19981;&#21516;&#12290;&#20256;&#32479;&#19978;&#65292;&#21453;&#20107;&#23454;&#24773;&#26223;&#34987;&#35270;&#20026;&#23616;&#37096;&#36829;&#21453;&#33258;&#28982;&#35268;&#24459;&#30340;&#8220;&#23567;&#22855;&#36857;&#8221;&#65292;&#20294;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#32780;&#22312;Pearl&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#26694;&#26550;&#20013;&#65292;&#36825;&#36890;&#36807;&#20462;&#25913;&#22240;&#26524;&#23450;&#24459;&#30340;&#24178;&#39044;&#32780;&#20351;&#24471;&#22806;&#29983;&#21464;&#37327;&#30340;&#20540;&#20849;&#20139;&#24471;&#21040;&#20102;&#25968;&#23398;&#19978;&#30340;&#20005;&#26684;&#21270;&#12290;&#20294;&#36817;&#24180;&#26469;&#65292;&#21746;&#23398;&#23478;&#21644;&#24515;&#29702;&#23398;&#23478;&#23545;&#36825;&#31181;&#21333;&#32431;&#30340;&#24178;&#39044;&#20027;&#20041;&#21453;&#20107;&#23454;&#35266;&#28857;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#36136;&#30097;&#12290;&#30456;&#21453;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#35266;&#28857;&#65292;&#21363;&#22312;&#21453;&#20107;&#23454;&#19990;&#30028;&#20013;&#22240;&#26524;&#23450;&#24459;&#20445;&#25345;&#19981;&#21464;&#65292;&#23558;&#19982;&#23454;&#38469;&#24773;&#20917;&#30340;&#24046;&#24322;&#8220;&#22238;&#28335;&#8221;&#21040;&#25913;&#21464;&#30340;&#21021;&#22987;&#26465;&#20214;(&#22806;&#29983;&#21464;&#37327;)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#20294;&#28789;&#27963;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#29702;&#36807;&#21435;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#12289;&#26356;&#30452;&#35266;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning -- envisioning hypothetical scenarios, or possible worlds, where some circumstances are different from what (f)actually occurred (counter-to-fact) -- is ubiquitous in human cognition. Conventionally, counterfactually-altered circumstances have been treated as "small miracles" that locally violate the laws of nature while sharing the same initial conditions. In Pearl's structural causal model (SCM) framework this is made mathematically rigorous via interventions that modify the causal laws while the values of exogenous variables are shared. In recent years, however, this purely interventionist account of counterfactuals has increasingly come under scrutiny from both philosophers and psychologists. Instead, they suggest a backtracking account of counterfactuals, according to which the causal laws remain unchanged in the counterfactual world; differences to the factual world are instead "backtracked" to altered initial conditions (exogenous variables). In the pres
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.17426</link><description>&lt;p&gt;
&#20613;&#31435;&#21494;&#20998;&#26512;&#23454;&#29616;&#19968;&#33268;&#19988;&#30495;&#23454;&#30340;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#36328;&#23398;&#31185;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#37322;&#38656;&#35201;&#19982;&#24403;&#21069;&#26696;&#20363;&#30456;&#20851;&#30340;&#20551;&#35774;&#24773;&#26223;&#19968;&#33268;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#22240;&#32032;&#25913;&#21464;&#65292;&#27169;&#22411;&#20250;&#22914;&#20309;&#21453;&#24212;&#65311;&#23613;&#31649;&#24402;&#22240;&#26041;&#27861;&#30001;&#20248;&#38597;&#30340;&#20844;&#29702;&#31995;&#32479;&#25903;&#25345;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20837;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#19968;&#33268;&#12290;&#20026;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#24067;&#23572;&#20989;&#25968;&#30340;&#20613;&#31435;&#21494;&#20998;&#26512;&#26469;&#33719;&#24471;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#21508;&#31181;&#21322;&#24452;&#30340;&#37051;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#23454;&#29616;2&#20493;&#33267;50&#20493;&#26356;&#20302;&#30340;&#35299;&#37322;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17312</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#24207;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#31361;&#21464;&#20998;&#24067;&#36716;&#25442;&#65292;&#21363;&#25152;&#35859;&#30340;&#21464;&#28857;&#26816;&#27979;&#65292;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#65292;&#24403;&#21457;&#29983;&#21464;&#28857;&#26102;&#65292;&#35813;&#37327;&#20250;&#26174;&#33879;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#21464;&#28857;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22810;&#23646;&#24615;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#24230;&#37327;&#21487;&#33021;&#20250;&#24573;&#30053;&#22810;&#20010;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21487;&#33021;&#38169;&#35823;&#22320;&#32473;&#20986;&#24230;&#37327;&#32467;&#26524;&#65292;&#20351;&#24471;&#26368;&#23567;&#25110;&#27809;&#26377;&#20559;&#35265;&#25918;&#22823;&#30340;&#24773;&#20917;&#34987;&#35823;&#21028;&#12290;</title><link>http://arxiv.org/abs/2210.11924</link><description>&lt;p&gt;
&#30007;&#24615;&#20063;&#27927;&#34915;&#26381;&#65306;&#22810;&#23646;&#24615;&#20559;&#35265;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Men Also Do Laundry: Multi-Attribute Bias Amplification. (arXiv:2210.11924v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22810;&#23646;&#24615;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#24230;&#37327;&#21487;&#33021;&#20250;&#24573;&#30053;&#22810;&#20010;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21487;&#33021;&#38169;&#35823;&#22320;&#32473;&#20986;&#24230;&#37327;&#32467;&#26524;&#65292;&#20351;&#24471;&#26368;&#23567;&#25110;&#27809;&#26377;&#20559;&#35265;&#25918;&#22823;&#30340;&#24773;&#20917;&#34987;&#35823;&#21028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#30740;&#31350;&#30028;&#21644;&#20844;&#20247;&#23545;&#36825;&#20123;&#31995;&#32479;&#19981;&#20165;&#20250;&#22797;&#21046;&#65292;&#32780;&#19988;&#20250;&#25918;&#22823;&#26377;&#23475;&#31038;&#20250;&#20559;&#35265;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#25991;&#25152;&#20851;&#27880;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26159;&#25351;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#25918;&#22823;&#22266;&#26377;&#30340;&#35757;&#32451;&#38598;&#20559;&#35265;&#12290;&#29616;&#26377;&#30340;&#25351;&#26631;&#38024;&#23545;&#21333;&#20010;&#27880;&#37322;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#8220;&#30005;&#33041;&#8221;&#65289;&#27979;&#37327;&#20559;&#35265;&#25918;&#22823;&#12290;&#28982;&#32780;&#65292;&#20960;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#30001;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#27880;&#37322;&#30340;&#22270;&#20687;&#32452;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21033;&#29992;&#22810;&#20010;&#23646;&#24615;&#65288;&#20363;&#22914;{&#8220;&#30005;&#33041;&#8221;&#65292;&#8220;&#38190;&#30424;&#8221;}&#65289;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#36825;&#20123;&#30456;&#20851;&#24615;&#19981;&#34987;&#24403;&#21069;&#30340;&#24230;&#37327;&#25152;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#21069;&#30340;&#24230;&#37327;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#32473;&#20986;&#26368;&#23567;&#25110;&#27809;&#26377;&#20559;&#35265;&#25918;&#22823;&#30340;&#21360;&#35937;&#65292;&#22240;&#20026;&#23427;&#20204;&#28041;&#21450;&#23545;&#27491;&#20540;&#21644;&#36127;&#20540;&#36827;&#34892;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25351;&#26631;&#32570;&#20047;&#26126;&#30830;&#30340;&#26399;&#26395;&#20540;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
As computer vision systems become more widely deployed, there is increasing concern from both the research community and the public that these systems are not only reproducing but amplifying harmful social biases. The phenomenon of bias amplification, which is the focus of this work, refers to models amplifying inherent training set biases at test time. Existing metrics measure bias amplification with respect to single annotated attributes (e.g., $\texttt{computer}$). However, several visual datasets consist of images with multiple attribute annotations. We show models can learn to exploit correlations with respect to multiple attributes (e.g., {$\texttt{computer}$, $\texttt{keyboard}$}), which are not accounted for by current metrics. In addition, we show current metrics can give the erroneous impression that minimal or no bias amplification has occurred as they involve aggregating over positive and negative values. Further, these metrics lack a clear desired value, making them diffic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Call Graph Evolution Analytics&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36719;&#20214;&#31995;&#32479;&#29256;&#26412;&#24207;&#21015;&#30340;&#28436;&#21270;&#35843;&#29992;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36890;&#36807;&#20998;&#26512;&#28436;&#21270;&#35843;&#29992;&#22270;&#30340;&#20381;&#36182;&#20851;&#31995;&#27169;&#24335;&#30340;&#28436;&#21270;&#65292;&#24110;&#21161;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#29256;&#26412;&#28436;&#21270;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2210.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#36719;&#20214;&#31995;&#32479;&#29256;&#26412;&#24207;&#21015;&#19978;&#30340;&#35843;&#29992;&#22270;&#28436;&#21270;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Call Graph Evolution Analytics over a Version Series of an Evolving Software System. (arXiv:2210.08316v1 [cs.SE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08316
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Call Graph Evolution Analytics&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36719;&#20214;&#31995;&#32479;&#29256;&#26412;&#24207;&#21015;&#30340;&#28436;&#21270;&#35843;&#29992;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36890;&#36807;&#20998;&#26512;&#28436;&#21270;&#35843;&#29992;&#22270;&#30340;&#20381;&#36182;&#20851;&#31995;&#27169;&#24335;&#30340;&#28436;&#21270;&#65292;&#24110;&#21161;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#29256;&#26412;&#28436;&#21270;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#29992;&#22270;&#28436;&#21270;&#20998;&#26512;&#21487;&#20197;&#24110;&#21161;&#36719;&#20214;&#24037;&#31243;&#24072;&#22312;&#32500;&#25252;&#25110;&#28436;&#36827;&#36719;&#20214;&#31995;&#32479;&#26102;&#36827;&#34892;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Call Graph Evolution Analytics&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36719;&#20214;&#31995;&#32479;&#29256;&#26412;&#24207;&#21015; VS = V_1, V_2, &#8230; V_N &#30340;&#28436;&#21270;&#35843;&#29992;&#22270; ECG = CG_1, CG_2, &#8230; CG_N &#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;Call Graph Evolution Rules&#65288;CGERs&#65289;&#21644;Call Graph Evolution Subgraphs&#65288;CGESs&#65289;&#23436;&#25104;&#30340;&#12290;&#31867;&#20284;&#20110;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#65292;CGERs&#29992;&#20110;&#25429;&#33719;&#31995;&#32479;&#20013;&#20381;&#36182;&#20851;&#31995;&#30340;&#20849;&#29616;&#12290;&#19982;&#35843;&#29992;&#22270;&#20013;&#30340;&#23376;&#22270;&#27169;&#24335;&#31867;&#20284;&#65292;CGESs&#29992;&#20110;&#25429;&#33719;&#28436;&#21270;&#35843;&#29992;&#22270;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#27169;&#24335;&#30340;&#28436;&#21270;&#12290;&#23545;&#36825;&#20123;&#27169;&#24335;&#30340;&#28436;&#21270;&#36827;&#34892;&#35843;&#29992;&#22270;&#20998;&#26512;&#21487;&#20197;&#35782;&#21035;&#20986;&#38656;&#35201;&#20851;&#27880;&#30340;&#28508;&#22312;&#21463;&#24433;&#21709;&#30340;&#20381;&#36182;&#20851;&#31995;&#65288;&#25110;&#36807;&#31243;&#35843;&#29992;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Call Graph evolution analytics can aid a software engineer when maintaining or evolving a software system. This paper proposes Call Graph Evolution Analytics to extract information from an evolving call graph ECG = CG_1, CG_2,... CG_N for their version series VS = V_1, V_2, ... V_N of an evolving software system. This is done using Call Graph Evolution Rules (CGERs) and Call Graph Evolution Subgraphs (CGESs). Similar to association rule mining, the CGERs are used to capture co-occurrences of dependencies in the system. Like subgraph patterns in a call graph, the CGESs are used to capture evolution of dependency patterns in evolving call graphs. Call graph analytics on the evolution in these patterns can identify potentially affected dependencies (or procedure calls) that need attention. The experiments are done on the evolving call graphs of 10 large evolving systems to support dependency evolution management. We also consider results from a detailed study for evolving call graphs of M
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.07290</link><description>&lt;p&gt;
&#21452;&#25511;&#21046;&#21464;&#37327;&#21152;&#36895;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Dual control variate for faster black-box variational inference. (arXiv:2210.07290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#25512;&#26029;&#26694;&#26550;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#20272;&#35745;&#20013;&#30340;&#39640;&#26041;&#24046;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#24046;&#26469;&#33258;&#20004;&#20010;&#38543;&#26426;&#28304;&#65306;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#12290;&#29616;&#26377;&#30340;&#25511;&#21046;&#21464;&#37327;&#20165;&#35299;&#20915;&#33945;&#29305;&#21345;&#32599;&#22122;&#22768;&#65292;&#32780;&#22686;&#37327;&#26799;&#24230;&#26041;&#27861;&#36890;&#24120;&#20165;&#35299;&#20915;&#25968;&#25454;&#23376;&#25277;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#8221;&#25511;&#21046;&#21464;&#37327;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#20004;&#31181;&#22122;&#22768;&#28304;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30830;&#35748;&#36825;&#23548;&#33268;&#20102;&#20943;&#23569;&#26041;&#24046;&#21644;&#22312;&#22810;&#20010;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box variational inference is a widely-used framework for Bayesian posterior inference, but in some cases suffers from high variance in gradient estimates, harming accuracy and efficiency. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. Whereas existing control variates only address Monte Carlo noise and incremental gradient methods typically only address data subsampling, we propose a new "dual" control variate capable of jointly reducing variance from both sources of noise. We confirm that this leads to reduced variance and improved optimization in several real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#36827;&#34892;&#24341;&#21147;&#27874;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#24471;&#21040;&#19981;&#21463;&#32593;&#32476;&#19981;&#20934;&#30830;&#24615;&#24433;&#21709;&#30340;&#26657;&#27491;&#21518;&#39564;&#65292;&#36824;&#21487;&#20197;&#35780;&#20272;&#24314;&#35758;&#21644;&#35782;&#21035;&#22833;&#36133;&#24773;&#20917;&#30340;&#24615;&#33021;&#35786;&#26029;&#20197;&#21450;&#24471;&#21040;&#36125;&#21494;&#26031;&#35777;&#25454;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.05686</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#24555;&#36895;&#21487;&#38752;&#24341;&#21147;&#27874;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Neural Importance Sampling for Rapid and Reliable Gravitational-Wave Inference. (arXiv:2210.05686v2 [gr-qc] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#36827;&#34892;&#24341;&#21147;&#27874;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#24471;&#21040;&#19981;&#21463;&#32593;&#32476;&#19981;&#20934;&#30830;&#24615;&#24433;&#21709;&#30340;&#26657;&#27491;&#21518;&#39564;&#65292;&#36824;&#21487;&#20197;&#35780;&#20272;&#24314;&#35758;&#21644;&#35782;&#21035;&#22833;&#36133;&#24773;&#20917;&#30340;&#24615;&#33021;&#35786;&#26029;&#20197;&#21450;&#24471;&#21040;&#36125;&#21494;&#26031;&#35777;&#25454;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25674;&#20313;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#19982;&#37325;&#35201;&#24615;&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#24341;&#21147;&#27874;&#25512;&#26029;&#12290;&#39318;&#20808;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#24555;&#36895;&#24314;&#35758;&#65292;&#28982;&#21518;&#26681;&#25454;&#22522;&#26412;&#20284;&#28982;&#21644;&#20808;&#39564;&#35745;&#31639;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#36825;&#25552;&#20379;&#20102;&#65288;1&#65289;&#19968;&#20010;&#19981;&#21463;&#32593;&#32476;&#19981;&#20934;&#30830;&#24615;&#24433;&#21709;&#30340;&#26657;&#27491;&#21518;&#39564;&#12289;&#65288;2&#65289;&#35780;&#20272;&#24314;&#35758;&#21644;&#35782;&#21035;&#22833;&#36133;&#24773;&#20917;&#30340;&#24615;&#33021;&#35786;&#26029;&#65288;&#26679;&#26412;&#25928;&#29575;&#65289;&#20197;&#21450;&#65288;3&#65289;&#36125;&#21494;&#26031;&#35777;&#25454;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#36890;&#36807;&#24314;&#31435;&#36825;&#31181;&#29420;&#31435;&#30340;&#39564;&#35777;&#21644;&#26657;&#27491;&#26426;&#21046;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20123;&#38024;&#23545;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#31185;&#23398;&#25512;&#26029;&#30340;&#26368;&#39057;&#32321;&#25209;&#35780;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#22411;&#30740;&#31350;&#65292;&#20351;&#29992;SEOBv4PHM&#21644;IMRPhenomXPHM&#27874;&#24418;&#27169;&#22411;&#20998;&#26512;&#20102;LIGO&#21644;Virgo&#35266;&#23519;&#21040;&#30340;42&#20010;&#21452;&#40657;&#27934;&#21512;&#24182;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20013;&#20301;&#26679;&#26412;&#25928;&#29575;&#32422;&#20026;10&#65285;&#65288;&#27604;&#26631;&#20934;&#37319;&#26679;&#22120;&#39640;&#20004;&#20010;&#25968;&#37327;&#32423;&#65289;&#65292;&#21516;&#26102;&#29366;&#24577;&#31354;&#38388;&#20307;&#31215;&#20943;&#23567;&#20102;&#21313;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We combine amortized neural posterior estimation with importance sampling for fast and accurate gravitational-wave inference. We first generate a rapid proposal for the Bayesian posterior using neural networks, and then attach importance weights based on the underlying likelihood and prior. This provides (1) a corrected posterior free from network inaccuracies, (2) a performance diagnostic (the sample efficiency) for assessing the proposal and identifying failure cases, and (3) an unbiased estimate of the Bayesian evidence. By establishing this independent verification and correction mechanism we address some of the most frequent criticisms against deep learning for scientific inference. We carry out a large study analyzing 42 binary black hole mergers observed by LIGO and Virgo with the SEOBNRv4PHM and IMRPhenomXPHM waveform models. This shows a median sample efficiency of $\approx 10\%$ (two orders-of-magnitude better than standard samplers) as well as a ten-fold reduction in the sta
&lt;/p&gt;</description></item><item><title>ParaDime &#26159;&#19968;&#20010;&#21442;&#25968;&#21270;&#38477;&#32500;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#36716;&#25442;&#65292;&#20174;&#32780;&#23558;&#22810;&#20010;&#29616;&#20195; DR &#25216;&#26415;&#32479;&#19968;&#36215;&#26469;&#12290;&#20511;&#21161; ParaDime&#65292;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#22320;&#23454;&#39564;&#21508;&#31181; DR &#25216;&#26415;&#65292;&#22914;&#28151;&#21512;&#20998;&#31867; / &#23884;&#20837;&#27169;&#22411;&#21644;&#30417;&#30563; DR&#65292;&#20026;&#39640;&#32500;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#25171;&#24320;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04582</link><description>&lt;p&gt;
ParaDime&#65306;&#21442;&#25968;&#21270;&#38477;&#32500;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ParaDime: A Framework for Parametric Dimensionality Reduction. (arXiv:2210.04582v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04582
&lt;/p&gt;
&lt;p&gt;
ParaDime &#26159;&#19968;&#20010;&#21442;&#25968;&#21270;&#38477;&#32500;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#36716;&#25442;&#65292;&#20174;&#32780;&#23558;&#22810;&#20010;&#29616;&#20195; DR &#25216;&#26415;&#32479;&#19968;&#36215;&#26469;&#12290;&#20511;&#21161; ParaDime&#65292;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#22320;&#23454;&#39564;&#21508;&#31181; DR &#25216;&#26415;&#65292;&#22914;&#28151;&#21512;&#20998;&#31867; / &#23884;&#20837;&#27169;&#22411;&#21644;&#30417;&#30563; DR&#65292;&#20026;&#39640;&#32500;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#25171;&#24320;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ParaDime &#26159;&#19968;&#20010;&#21442;&#25968;&#21270;&#38477;&#32500;&#65288;DR&#65289;&#26694;&#26550;&#12290;&#22312;&#21442;&#25968;&#21270; DR &#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#25104;&#23558;&#39640;&#32500;&#25968;&#25454;&#28857;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#19968;&#20010;&#30446;&#26631;&#20989;&#25968;&#12290;ParaDime &#24314;&#31435;&#22312;&#19968;&#20123;&#29616;&#20195; DR &#25216;&#26415;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;&#22522;&#20110;&#36716;&#25442;&#36807;&#30340;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#24605;&#24819;&#19978;&#65292;&#20026;&#36825;&#20123;&#20851;&#31995;&#21644;&#36716;&#25442;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#23450;&#20041;&#20102;&#23427;&#20204;&#22914;&#20309;&#34987;&#29992;&#20110;&#35268;&#33539;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25439;&#22833;&#12290;&#36890;&#36807;&#36825;&#20010;&#25509;&#21475;&#65292;ParaDime &#32479;&#19968;&#20102;&#24230;&#37327; MDS&#12289;t-SNE &#21644; UMAP &#31561; DR &#25216;&#26415;&#30340;&#21442;&#25968;&#21270;&#29256;&#26412;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#23436;&#20840;&#33258;&#23450;&#20041; DR &#36807;&#31243;&#30340;&#25152;&#26377;&#26041;&#38754;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26131;&#20110;&#23450;&#21046;&#30340;&#29305;&#28857;&#20351; ParaDime &#36866;&#29992;&#20110;&#23454;&#39564;&#24615;&#25216;&#26415;&#65292;&#20363;&#22914;&#28151;&#21512;&#20998;&#31867; / &#23884;&#20837;&#27169;&#22411;&#21644;&#30417;&#30563; DR&#12290;&#36890;&#36807; ParaDime&#65292;&#25171;&#24320;&#20102;&#21487;&#35270;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ParaDime is a framework for parametric dimensionality reduction (DR). In parametric DR, neural networks are trained to embed high-dimensional data items in a low-dimensional space while minimizing an objective function. ParaDime builds on the idea that the objective functions of several modern DR techniques result from transformed inter-item relationships. It provides a common interface for specifying these relations and transformations and for defining how they are used within the losses that govern the training process. Through this interface, ParaDime unifies parametric versions of DR techniques such as metric MDS, t-SNE, and UMAP. It allows users to fully customize all aspects of the DR process. We show how this ease of customization makes ParaDime suitable for experimenting with interesting techniques such as hybrid classification/embedding models and supervised DR. This way, ParaDime opens up new possibilities for visualizing high-dimensional data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;InfoOT&#65292;&#23427;&#26159;&#19968;&#31181;&#20449;&#24687;&#35770;&#25193;&#23637;&#30340;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26368;&#20248;&#36755;&#36816;&#24573;&#30053;&#20102;&#25968;&#25454;&#20013;&#30456;&#24178;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#33021;&#22815;&#22788;&#29702;&#31163;&#32676;&#20540;&#21644;&#38598;&#25104;&#26032;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#12289;&#36328;&#22495;&#26816;&#32034;&#21644;&#21333;&#32454;&#32990;&#23545;&#40784;&#31561;&#20219;&#21153;&#30340;&#23545;&#40784;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.03164</link><description>&lt;p&gt;
InfoOT: &#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
InfoOT: Information Maximizing Optimal Transport. (arXiv:2210.03164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03164
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;InfoOT&#65292;&#23427;&#26159;&#19968;&#31181;&#20449;&#24687;&#35770;&#25193;&#23637;&#30340;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26368;&#20248;&#36755;&#36816;&#24573;&#30053;&#20102;&#25968;&#25454;&#20013;&#30456;&#24178;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#33021;&#22815;&#22788;&#29702;&#31163;&#32676;&#20540;&#21644;&#38598;&#25104;&#26032;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#12289;&#36328;&#22495;&#26816;&#32034;&#21644;&#21333;&#32454;&#32990;&#23545;&#40784;&#31561;&#20219;&#21153;&#30340;&#23545;&#40784;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#36890;&#36807;&#26368;&#23567;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#36816;&#36755;&#25104;&#26412;&#65288;&#20363;&#22914;&#20960;&#20309;&#36317;&#31163;&#65289;&#23545;&#20998;&#24067;&#20013;&#30340;&#26679;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#24573;&#30053;&#20102;&#25968;&#25454;&#20013;&#30340;&#30456;&#24178;&#32467;&#26500;&#65292;&#20363;&#22914;&#31751;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#31163;&#32676;&#20540;&#65292;&#20063;&#19981;&#33021;&#38598;&#25104;&#26032;&#25968;&#25454;&#28857;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InfoOT&#65292;&#23427;&#26159;&#26368;&#20248;&#36755;&#36816;&#30340;&#20449;&#24687;&#35770;&#25193;&#23637;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#21270;&#20960;&#20309;&#36317;&#31163;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#22495;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#26368;&#32456;&#30340;&#30446;&#26631;&#20173;&#28982;&#21487;&#20197;&#34987;&#20844;&#24335;&#21270;&#20026;&#65288;&#24191;&#20041;&#30340;&#65289;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26377;&#25928;&#22320;&#27714;&#35299;&#12290;&#36825;&#31181;&#20844;&#24335;&#21270;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#30340;&#25237;&#24433;&#26041;&#27861;&#65292;&#23427;&#23545;&#31163;&#32676;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#26679;&#26412;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22495;&#33258;&#36866;&#24212;&#12289;&#36328;&#22495;&#26816;&#32034;&#21644;&#21333;&#32454;&#32990;&#23545;&#40784;&#31561;&#22522;&#20934;&#20013;&#65292;InfoOT&#21487;&#20197;&#25552;&#39640;&#23545;&#40784;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport aligns samples across distributions by minimizing the transportation cost between them, e.g., the geometric distances. Yet, it ignores coherence structure in the data such as clusters, does not handle outliers well, and cannot integrate new data points. To address these drawbacks, we propose InfoOT, an information-theoretic extension of optimal transport that maximizes the mutual information between domains while minimizing geometric distances. The resulting objective can still be formulated as a (generalized) optimal transport problem, and can be efficiently solved by projected gradient descent. This formulation yields a new projection method that is robust to outliers and generalizes to unseen samples. Empirically, InfoOT improves the quality of alignments across benchmarks in domain adaptation, cross-domain retrieval, and single-cell alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26680;&#22343;&#20540;&#27744;&#65288;KAP&#65289;&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#33258;&#28982;&#22320;&#20135;&#29983;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#26680;&#38598;&#21512;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00062</link><description>&lt;p&gt;
&#20351;&#29992;&#26680;&#22343;&#20540;&#27744;&#21270;&#23398;&#20064;&#40065;&#26834;&#30340;&#26680;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Kernel Ensembles with Kernel Average Pooling. (arXiv:2210.00062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26680;&#22343;&#20540;&#27744;&#65288;KAP&#65289;&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#33258;&#28982;&#22320;&#20135;&#29983;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#26680;&#38598;&#21512;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#38598;&#25104;&#19968;&#30452;&#34987;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20197;&#20943;&#23569;&#20010;&#21035;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#24046;&#65292;&#20351;&#20854;&#23545;&#36755;&#20837;&#25200;&#21160;&#26356;&#21152;&#40065;&#26834;&#12290;&#20551;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;dropout&#65289;&#20063;&#24120;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#22343;&#20540;&#27744;&#65288;KAP&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27169;&#22359;&#65292;&#21487;&#27839;&#30528;&#23618;&#28608;&#27963;&#24352;&#37327;&#30340;&#26680;&#32500;&#24230;&#24212;&#29992;&#22343;&#20540;&#28388;&#27874;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;KAP&#20197;&#21450;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#26680;&#38598;&#21512;&#33258;&#28982;&#22320;&#20135;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#30340;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;KAP&#27169;&#22411;&#23545;&#21508;&#31181;&#24418;&#24335;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;CIFAR10&#12289;CIFAR100&#12289;TinyImagenet&#21644;Imagenet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce Kernel Average Pooling (KAP), a neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when trained on inputs perturbed with additive Gaussian noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#25968;&#25454;&#28857;&#20381;&#36182;&#20851;&#31995;&#30340;&#24402;&#19968;&#21270;&#27969;&#20284;&#28982;&#30446;&#26631;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#32463;&#39564;&#32467;&#26524;&#21644;&#26356;&#39640;&#30340;&#32479;&#35745;&#21151;&#25928;&#12290;</title><link>http://arxiv.org/abs/2209.14933</link><description>&lt;p&gt;
&#20174;&#30456;&#20851;&#25968;&#25454;&#20013;&#35757;&#32451;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Training Normalizing Flows from Dependent Data. (arXiv:2209.14933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#25968;&#25454;&#28857;&#20381;&#36182;&#20851;&#31995;&#30340;&#24402;&#19968;&#21270;&#27969;&#20284;&#28982;&#30446;&#26631;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#32463;&#39564;&#32467;&#26524;&#21644;&#26356;&#39640;&#30340;&#32479;&#35745;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#19968;&#31181;&#21151;&#33021;&#24378;&#22823;&#30340;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#65292;&#23427;&#26159;&#23494;&#24230;&#20272;&#35745;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#28151;&#21512;&#20307;&#12290;&#30446;&#21069;&#30340;&#24402;&#19968;&#21270;&#27969;&#23398;&#20064;&#31639;&#27861;&#20551;&#23450;&#25968;&#25454;&#28857;&#26159;&#29420;&#31435;&#37319;&#26679;&#30340;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#21487;&#33021;&#23548;&#33268;&#23494;&#24230;&#20272;&#35745;&#21644;&#25968;&#25454;&#29983;&#25104;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#25968;&#25454;&#28857;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#24402;&#19968;&#21270;&#27969;&#20284;&#28982;&#30446;&#26631;&#65292;&#24182;&#20026;&#27492;&#25512;&#23548;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#20381;&#36182;&#32467;&#26500;&#30340;&#28789;&#27963;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23562;&#37325;&#35266;&#23519;&#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#25913;&#21892;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#32463;&#39564;&#32467;&#26524;&#65292;&#24182;&#22312;&#23545;&#20840;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#23548;&#33268;&#26356;&#39640;&#30340;&#32479;&#35745;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows are powerful non-parametric statistical models that function as a hybrid between density estimators and generative models. Current learning algorithms for normalizing flows assume that data points are sampled independently, an assumption that is frequently violated in practice, which may lead to erroneous density estimation and data generation. We propose a likelihood objective of normalizing flows incorporating dependencies between the data points, for which we derive a flexible and efficient learning algorithm suitable for different dependency structures. We show that respecting dependencies between observations can improve empirical results on both synthetic and real-world data, and leads to higher statistical power in a downstream application to genome-wide association studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#20248;&#21270;&#20013;&#30340;&#32463;&#20856;&#24037;&#20855;&#36817;&#31471;&#28857;&#27861;&#21644;&#23545;&#20598;&#24179;&#28369;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#27604;&#20043;&#21069;&#26356;&#22909;&#30340;&#25928;&#29575;&#20445;&#35777;&#12290;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#21333;&#20010;&#30340;&#20984;&#24179;&#28369;&#30446;&#26631;&#26469;&#21516;&#26102;&#26356;&#26032;&#25104;&#26412;&#21644;Q&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#23884;&#22871;&#31574;&#30053;&#35780;&#20272;&#21644;&#25104;&#26412;&#26356;&#26032;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2209.10968</link><description>&lt;p&gt;
&#36817;&#31471;&#28857;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proximal Point Imitation Learning. (arXiv:2209.10968v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#20248;&#21270;&#20013;&#30340;&#32463;&#20856;&#24037;&#20855;&#36817;&#31471;&#28857;&#27861;&#21644;&#23545;&#20598;&#24179;&#28369;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#27604;&#20043;&#21069;&#26356;&#22909;&#30340;&#25928;&#29575;&#20445;&#35777;&#12290;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#21333;&#20010;&#30340;&#20984;&#24179;&#28369;&#30446;&#26631;&#26469;&#21516;&#26102;&#26356;&#26032;&#25104;&#26412;&#21644;Q&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#23884;&#22871;&#31574;&#30053;&#35780;&#20272;&#21644;&#25104;&#26412;&#26356;&#26032;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#20294;&#27809;&#26377;&#38480;&#21046;&#19968;&#33268;&#24615;&#20551;&#35774;&#30340;&#26080;&#38480;&#26102;&#22495;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#20005;&#26684;&#30340;&#25928;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#20174;&#38382;&#39064;&#30340;&#26497;&#23567;&#21270;-&#26497;&#22823;&#21270;&#24418;&#24335;&#24320;&#22987;&#65292;&#24182;&#27010;&#36848;&#22914;&#20309;&#21033;&#29992;&#20248;&#21270;&#20013;&#30340;&#32463;&#20856;&#24037;&#20855;-&#36817;&#31471;&#28857;&#27861;&#65288;PPM&#65289;&#21644;&#23545;&#20598;&#24179;&#28369;&#26469;&#36827;&#34892;&#22312;&#32447;&#21644;&#31163;&#32447;IL&#12290;&#30001;&#20110;PPM&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#20808;&#21069;&#25991;&#29486;&#20013;&#20986;&#29616;&#30340;&#22312;&#32447;IL&#20013;&#20986;&#29616;&#30340;&#23884;&#22871;&#31574;&#30053;&#35780;&#20272;&#21644;&#25104;&#26412;&#26356;&#26032;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#21333;&#20010;&#30340;&#20984;&#24179;&#28369;&#30446;&#26631;&#26469;&#21516;&#26102;&#26356;&#26032;&#25104;&#26412;&#21644;Q&#20989;&#25968;&#65292;&#25670;&#33073;&#20102;&#24120;&#35268;&#30340;&#20132;&#26367;&#26356;&#26032;&#12290;&#24403;&#19981;&#31934;&#30830;&#22320;&#27714;&#35299;&#26102;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#35823;&#24046;&#19982;&#24674;&#22797;&#31574;&#30053;&#30340;&#27425;&#20248;&#24615;&#30456;&#20851;&#32852;&#12290;&#20316;&#20026;&#39069;&#22806;&#30340;&#22870;&#21169;&#65292;&#36890;&#36807;&#23558;PPM&#37325;&#26032;&#35299;&#37322;&#20026;&#20197;&#19987;&#23478;&#31574;&#30053;&#20026;&#20013;&#24515;&#28857;&#30340;&#23545;&#20598;&#24179;&#28369;&#65292;&#25105;&#20204;&#36824;&#33719;&#24471;&#20102;&#31163;&#32447;IL&#31639;&#27861;&#65292;&#20139;&#26377;&#20851;&#20110;&#25152;&#38656;&#19987;&#23478;&#25152;&#20855;&#26377;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work develops new algorithms with rigorous efficiency guarantees for infinite horizon imitation learning (IL) with linear function approximation without restrictive coherence assumptions. We begin with the minimax formulation of the problem and then outline how to leverage classical tools from optimization, in particular, the proximal-point method (PPM) and dual smoothing, for online and offline IL, respectively. Thanks to PPM, we avoid nested policy evaluation and cost updates for online IL appearing in the prior literature. In particular, we do away with the conventional alternating updates by the optimization of a single convex and smooth objective over both cost and Q-functions. When solved inexactly, we relate the optimization errors to the suboptimality of the recovered policy. As an added bonus, by re-interpreting PPM as dual smoothing with the expert policy as a center point, we also obtain an offline IL algorithm enjoying theoretical guarantees in terms of required expert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;IDIA&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#65292;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.07341</link><description>&lt;p&gt;
CLIP&#26159;&#21542;&#30693;&#36947;&#25105;&#30340;&#33080;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does CLIP Know My Face?. (arXiv:2209.07341v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;IDIA&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#65292;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#24212;&#29992;&#20013;&#30340;&#26222;&#21450;&#65292;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#22810;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#29305;&#21035;&#26159;&#20687;CLIP&#36825;&#26679;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;(IDIA)&#36890;&#36807;&#29992;&#21516;&#19968;&#20154;&#30340;&#22270;&#29255;&#21521;&#27169;&#22411;&#26597;&#35810;&#65292;&#20174;&#32780;&#25581;&#31034;&#35813;&#20010;&#20154;&#26159;&#21542;&#34987;&#21253;&#21547;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#35753;&#27169;&#22411;&#20174;&#21508;&#31181;&#21487;&#33021;&#30340;&#25991;&#26412;&#26631;&#31614;&#20013;&#36873;&#25321;&#65292;&#27169;&#22411;&#20250;&#36879;&#38706;&#26159;&#21542;&#35782;&#21035;&#35813;&#20154;&#29289;&#65292;&#20174;&#32780;&#34920;&#26126;&#20854;&#34987;&#29992;&#20110;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;CLIP&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#12290;&#25105;&#20204;&#30830;&#35748;&#35813;&#27169;&#22411;&#24050;&#32463;&#23398;&#20250;&#23558;&#21517;&#31216;&#19982;&#25551;&#32472;&#30340;&#20010;&#20154;&#30456;&#20851;&#32852;&#65292;&#36825;&#24847;&#21619;&#30528;&#25935;&#24863;&#20449;&#24687;&#23384;&#22312;&#20110;&#20854;&#20013;&#65292;&#21487;&#20197;&#34987;&#23545;&#25163;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#38656;&#35201;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#26356;&#22909;&#22320;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of deep learning in various applications, privacy concerns around the protection of training data has become a critical area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible text labels, the model reveals whether it recognizes the person and, therefore, was used for training. Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. Our results highlight the need for 
&lt;/p&gt;</description></item><item><title>PD-MORL&#26159;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#37319;&#29992;&#20559;&#22909;&#20316;&#20026;&#25351;&#23548;&#65292;&#36866;&#24212;&#20110;&#21508;&#31181;&#20559;&#22909;&#31354;&#38388;&#30340;&#30446;&#26631;&#65292;&#21487;&#25193;&#23637;&#21040;&#36830;&#32493;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;MORL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.07914</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;PD-MORL
&lt;/p&gt;
&lt;p&gt;
PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm. (arXiv:2208.07914v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07914
&lt;/p&gt;
&lt;p&gt;
PD-MORL&#26159;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#37319;&#29992;&#20559;&#22909;&#20316;&#20026;&#25351;&#23548;&#65292;&#36866;&#24212;&#20110;&#21508;&#31181;&#20559;&#22909;&#31354;&#38388;&#30340;&#30446;&#26631;&#65292;&#21487;&#25193;&#23637;&#21040;&#36830;&#32493;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;MORL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#30001;&#20559;&#22909;&#21521;&#37327;&#21152;&#26435;&#30340;&#32852;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#38024;&#23545;&#22810;&#20010;&#20914;&#31361;&#30446;&#26631;&#30340;&#23454;&#38469;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#35774;&#35745;&#32422;&#26463;&#21644;&#30446;&#26631;&#36890;&#24120;&#20250;&#21160;&#24577;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#23384;&#20648;&#27599;&#20010;&#28508;&#22312;&#20559;&#22909;&#30340;&#31574;&#30053;&#26159;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#32473;&#23450;&#22495;&#20013;&#20351;&#29992;&#21333;&#27425;&#35757;&#32451;&#33719;&#21462;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#35299;&#38598;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MORL&#31639;&#27861;&#65292;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#36890;&#29992;&#32593;&#32476;&#20197;&#35206;&#30422;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#36830;&#32493;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#8220;&#22522;&#20110;&#20559;&#22909;&#30340;MORL&#65288;PD-MORL&#65289;&#8221;&#21033;&#29992;&#20559;&#22909;&#20197;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#65292;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#21270;&#26041;&#27861;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#36866;&#24212;&#24403;&#21069;&#24773;&#20917;&#19979;&#30340;&#21508;&#31181;&#20559;&#22909;&#31354;&#38388;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#35780;&#20272;PD-MORL&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#24085;&#32047;&#25176;&#21069;&#27839;&#35206;&#30422;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;MORL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective reinforcement learning (MORL) approaches have emerged to tackle many real-world problems with multiple conflicting objectives by maximizing a joint objective function weighted by a preference vector. These approaches find fixed customized policies corresponding to preference vectors specified during training. However, the design constraints and objectives typically change dynamically in real-life scenarios. Furthermore, storing a policy for each potential preference is not scalable. Hence, obtaining a set of Pareto front solutions for the entire preference space in a given domain with a single training is critical. To this end, we propose a novel MORL algorithm that trains a single universal network to cover the entire preference space scalable to continuous robotic tasks. The proposed approach, Preference-Driven MORL (PD-MORL), utilizes the preferences as guidance to update the network parameters. It also employs a novel parallelization approach to increase sample effi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#25955;&#24335;&#25512;&#29702;&#25152;&#21487;&#33021;&#23548;&#33268;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#24230;&#37327;&#21644;&#26032;&#35774;&#35745;&#30340;&#38544;&#31169;&#20445;&#25252;&#20449;&#21495;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#22686;&#24378;&#36879;&#26126;&#24230;&#65292;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#38544;&#31169;&#20445;&#25252;&#36890;&#20449;&#21644;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2208.06963</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Decentralized Inference with Graph Neural Networks in Wireless Networks. (arXiv:2208.06963v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#25955;&#24335;&#25512;&#29702;&#25152;&#21487;&#33021;&#23548;&#33268;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#24230;&#37327;&#21644;&#26032;&#35774;&#35745;&#30340;&#38544;&#31169;&#20445;&#25252;&#20449;&#21495;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#22686;&#24378;&#36879;&#26126;&#24230;&#65292;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#38544;&#31169;&#20445;&#25252;&#36890;&#20449;&#21644;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#22270;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#26368;&#36817;&#22312;&#21508;&#31181;&#26080;&#32447;&#20248;&#21270;&#38382;&#39064;&#20013;&#24471;&#21040;&#25104;&#21151;&#24212;&#29992;&#12290;&#37492;&#20110;GNN&#30340;&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#33258;&#28982;&#22320;&#23454;&#29616;&#20998;&#25955;&#24335;&#65292;&#22240;&#27492;GNN&#26159;&#27425;&#19990;&#20195;&#26080;&#32447;&#36890;&#20449;&#30340;&#20998;&#25955;&#24335;&#25511;&#21046;/&#31649;&#29702;&#30340;&#28508;&#22312;&#25512;&#25163;&#12290;&#28982;&#32780;&#65292;&#22312;GNN&#30340;&#20998;&#25955;&#25512;&#29702;&#20013;&#65292;&#30001;&#20110;&#37051;&#23621;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#21487;&#33021;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20998;&#26512;&#24182;&#22686;&#24378;&#20102;&#26080;&#32447;&#32593;&#32476;&#20013;&#22522;&#20110;GNN&#30340;&#20998;&#25955;&#25512;&#29702;&#30340;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#20449;&#21495;&#20197;&#21450;&#20445;&#35777;&#38544;&#31169;&#30340;&#35757;&#32451;&#31639;&#27861;&#26469;&#23454;&#29616;&#20445;&#25252;&#38544;&#31169;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;SNR-&#38544;&#31169;&#26435;&#34913;&#20989;&#25968;&#65292;&#20197;&#20998;&#26512;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#37319;&#29992;GNN&#30340;&#20998;&#25955;&#25512;&#29702;&#30340;&#24615;&#33021;&#19978;&#38480;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#36879;&#26126;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#23637;&#31034;&#20102;&#22522;&#20110;GNN&#30340;&#38544;&#31169;&#20445;&#25252;&#36890;&#20449;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an efficient neural network model for graph data, graph neural networks (GNNs) recently find successful applications for various wireless optimization problems. Given that the inference stage of GNNs can be naturally implemented in a decentralized manner, GNN is a potential enabler for decentralized control/management in the next-generation wireless communications. Privacy leakage, however, may occur due to the information exchanges among neighbors during decentralized inference with GNNs. To deal with this issue, in this paper, we analyze and enhance the privacy of decentralized inference with GNNs in wireless networks. Specifically, we adopt local differential privacy as the metric, and design novel privacy-preserving signals as well as privacy-guaranteed training algorithms to achieve privacy-preserving inference. We also define the SNR-privacy trade-off function to analyze the performance upper bound of decentralized inference with GNNs in wireless networks. To further enhance t
&lt;/p&gt;</description></item><item><title>RLang&#26159;&#19968;&#31181;&#22768;&#26126;&#24615;&#35821;&#35328;&#65292;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#37096;&#20998;&#19990;&#30028;&#27169;&#22411;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#34920;&#26684;&#31639;&#27861;&#12289;&#31574;&#30053;&#26799;&#24230;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#12289;&#20998;&#23618;&#26041;&#27861;&#21644;&#28145;&#24230;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.06448</link><description>&lt;p&gt;
RLang&#65306;&#19968;&#31181;&#25551;&#36848;&#37096;&#20998;&#39046;&#22495;&#30693;&#35782;&#32473;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#22768;&#26126;&#24615;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents. (arXiv:2208.06448v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06448
&lt;/p&gt;
&lt;p&gt;
RLang&#26159;&#19968;&#31181;&#22768;&#26126;&#24615;&#35821;&#35328;&#65292;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#37096;&#20998;&#19990;&#30028;&#27169;&#22411;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#34920;&#26684;&#31639;&#27861;&#12289;&#31574;&#30053;&#26799;&#24230;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#12289;&#20998;&#23618;&#26041;&#27861;&#21644;&#28145;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; RLang &#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;(DSL)&#65292;&#29992;&#20110;&#19982;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#36890;&#20449;&#12290;&#19982;&#29616;&#26377;&#30340;&#21482;&#19982;&#20915;&#31574;&#21046;&#23450;&#24418;&#24335;&#20013;&#30340;&#19968;&#20010;&#20803;&#32032;(&#22914;&#22870;&#21169;&#20989;&#25968;&#25110;&#31574;&#30053;)&#30456;&#20851;&#30340; DSL &#19981;&#21516;&#65292;RLang &#21487;&#20197;&#25351;&#23450;&#20851;&#20110;Markov&#20915;&#31574;&#36807;&#31243;&#30340;&#27599;&#20010;&#20803;&#32032;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20026;RLang&#23450;&#20041;&#20102;&#31934;&#30830;&#30340;&#35821;&#27861;&#21644;&#22522;&#30784;&#35821;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#26512;&#22120;&#65292;&#23558;RLang&#31243;&#24207;&#22522;&#20110;&#31639;&#27861;&#26102;&#19981;&#21464;&#30340;&#37096;&#20998;&#19990;&#30028;&#27169;&#22411;&#21644;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28436;&#31034;&#19981;&#21516;RL&#26041;&#27861;&#22914;&#20309;&#21033;&#29992;&#25152;&#24471;&#30693;&#35782;&#30340;RLang&#31243;&#24207;&#65292;&#21253;&#25324;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#34920;&#26684;&#31639;&#27861;&#12289;&#31574;&#30053;&#26799;&#24230;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#12289;&#20998;&#23618;&#26041;&#27861;&#21644;&#28145;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RLang, a domain-specific language (DSL) for communicating domain knowledge to an RL agent. Unlike existing RL DSLs that ground to \textit{single} elements of a decision-making formalism (e.g., the reward function or policy), RLang can specify information about every element of a Markov decision process. We define precise syntax and grounding semantics for RLang, and provide a parser that grounds RLang programs to an algorithm-agnostic \textit{partial} world model and policy that can be exploited by an RL agent. We provide a series of example RLang programs demonstrating how different RL methods can exploit the resulting knowledge, encompassing model-free and model-based tabular algorithms, policy gradient and value-based methods, hierarchical approaches, and deep methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#21576;&#29616;&#31070;&#32463;&#29616;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32441;&#29702;&#22810;&#36793;&#24418;&#32780;&#19981;&#26159;&#22522;&#20110;&#23556;&#32447;&#34892;&#36827;&#30340;&#20307;&#31215;&#28210;&#26579;&#31639;&#27861;&#65292;&#24182;&#19988;&#21033;&#29992;&#20256;&#32479;&#28210;&#26579;&#31649;&#32447;&#20013;&#30340; z-&#32531;&#20914;&#22120;&#65292;&#20351;&#24471; NeRF &#21487;&#20197;&#36890;&#36807;&#20687;&#32032;&#32423;&#24182;&#34892;&#24615;&#26469;&#23454;&#29616;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#20132;&#20114;&#24335;&#24103;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2208.00277</link><description>&lt;p&gt;
MobileNeRF&#65306;&#21033;&#29992;&#22810;&#36793;&#24418;&#20809;&#26629;&#21270;&#31649;&#32447;&#22312;&#31227;&#21160;&#26550;&#26500;&#19978;&#39640;&#25928;&#21576;&#29616;&#31070;&#32463;&#29616;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures. (arXiv:2208.00277v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#21576;&#29616;&#31070;&#32463;&#29616;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32441;&#29702;&#22810;&#36793;&#24418;&#32780;&#19981;&#26159;&#22522;&#20110;&#23556;&#32447;&#34892;&#36827;&#30340;&#20307;&#31215;&#28210;&#26579;&#31639;&#27861;&#65292;&#24182;&#19988;&#21033;&#29992;&#20256;&#32479;&#28210;&#26579;&#31649;&#32447;&#20013;&#30340; z-&#32531;&#20914;&#22120;&#65292;&#20351;&#24471; NeRF &#21487;&#20197;&#36890;&#36807;&#20687;&#32032;&#32423;&#24182;&#34892;&#24615;&#26469;&#23454;&#29616;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#20132;&#20114;&#24335;&#24103;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#23637;&#31034;&#20102;&#20174;&#26032;&#39062;&#30340;&#35270;&#35282;&#21512;&#25104;3D&#22330;&#26223;&#22270;&#20687;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#22522;&#20110;&#23556;&#32447;&#34892;&#36827;&#30340;&#19987;&#19994;&#20307;&#31215;&#28210;&#26579;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#19982;&#24191;&#27867;&#37096;&#32626;&#30340;&#22270;&#24418;&#30828;&#20214;&#30340;&#33021;&#21147;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32441;&#29702;&#22810;&#36793;&#24418;&#30340;&#26032;&#22411;NeRF&#34920;&#31034;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#28210;&#26579;&#31649;&#32447;&#39640;&#25928;&#22320;&#21512;&#25104;&#26032;&#30340;&#22270;&#20687;&#12290;&#23558;NeRF&#34920;&#31034;&#20026;&#19968;&#32452;&#22810;&#36793;&#24418;&#65292;&#32441;&#29702;&#34920;&#31034;&#20108;&#20803;&#19981;&#36879;&#26126;&#24230;&#21644;&#29305;&#24449;&#21521;&#37327;&#12290;&#20351;&#29992;z-&#32531;&#20914;&#22120;&#20256;&#32479;&#21576;&#29616;&#22810;&#36793;&#24418;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#20687;&#32032;&#22788;&#33719;&#24471;&#29305;&#24449;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#29305;&#24449;&#30001;&#23567;&#22411;&#12289;&#35270;&#22270;&#30456;&#20851;&#30340;MLP&#22312;&#29255;&#27573;&#30528;&#33394;&#22120;&#20013;&#35299;&#37322;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#20687;&#32032;&#39068;&#33394;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;NeRF&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#22810;&#36793;&#24418;&#20809;&#26629;&#21270;&#31649;&#32447;&#36827;&#34892;&#21576;&#29616;&#65292;&#36825;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#20687;&#32032;&#32423;&#24182;&#34892;&#24615;&#65292;&#22312;&#21253;&#25324;&#31227;&#21160;&#30005;&#35805;&#22312;&#20869;&#30340;&#21508;&#31181;&#35745;&#31639;&#24179;&#21488;&#19978;&#23454;&#29616;&#20132;&#20114;&#24335;&#24103;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumetric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graphics hardware. This paper introduces a new NeRF representation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines. The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Traditional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables NeRFs to be rendered with the traditional polygon rasterization pipeline, which provides massive pixel-level parallelism, achieving interactive frame rates on a wide range of compute platforms, including mobile pho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;LDA&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#65292;&#38024;&#23545;&#19982;&#22522;&#22240;&#21644;&#30142;&#30149;&#30456;&#20851;&#30340;&#21307;&#23398;&#31185;&#23398;&#26399;&#21002;&#25991;&#31456;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#24182;&#20445;&#25345;&#21407;&#22987;&#24847;&#20041;&#30340;&#21387;&#32553;&#29256;&#26412;&#65292;&#24182;&#20351;&#29992;PyLDAvis&#36827;&#34892;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2207.14687</link><description>&lt;p&gt;
&#22522;&#20110;LDA&#20027;&#39064;&#24314;&#27169;&#30340;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#25968;&#25454;&#39537;&#21160;&#28508;&#22312;&#35821;&#24847;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Data-driven Latent Semantic Analysis for Automatic Text Summarization using LDA Topic Modelling. (arXiv:2207.14687v7 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;LDA&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#65292;&#38024;&#23545;&#19982;&#22522;&#22240;&#21644;&#30142;&#30149;&#30456;&#20851;&#30340;&#21307;&#23398;&#31185;&#23398;&#26399;&#21002;&#25991;&#31456;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#24182;&#20445;&#25345;&#21407;&#22987;&#24847;&#20041;&#30340;&#21387;&#32553;&#29256;&#26412;&#65292;&#24182;&#20351;&#29992;PyLDAvis&#36827;&#34892;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#26102;&#20195;&#22823;&#25968;&#25454;&#25366;&#25496;&#21644;&#22823;&#25991;&#26412;&#20998;&#26512;&#30340;&#21040;&#26469;&#21644;&#26222;&#21450;&#65292;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#25104;&#20026;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#21644;&#26816;&#32034;&#37325;&#35201;&#20449;&#24687;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#26412;&#30740;&#31350;&#20174;&#21333;&#20010;&#21644;&#22810;&#20010;&#25991;&#26723;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#25688;&#35201;&#26159;&#23558;&#24222;&#22823;&#30340;&#25991;&#26412;&#25991;&#31456;&#21387;&#32553;&#20026;&#30701;&#23567;&#27719;&#24635;&#29256;&#26412;&#30340;&#20219;&#21153;&#12290;&#30446;&#30340;&#26159;&#32553;&#23567;&#25991;&#26412;&#30340;&#22823;&#23567;&#65292;&#20294;&#35201;&#20445;&#30041;&#20851;&#38190;&#37325;&#35201;&#20449;&#24687;&#24182;&#20445;&#25345;&#21407;&#22987;&#25991;&#26723;&#30340;&#24847;&#20041;&#12290;&#8220;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#8221;&#65288;LDA&#65289;&#26041;&#27861;&#34987;&#29992;&#20110;&#20174;&#19982;&#22522;&#22240;&#21644;&#30142;&#30149;&#30456;&#20851;&#30340;&#21307;&#23398;&#31185;&#23398;&#26399;&#21002;&#25991;&#31456;&#20013;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;PyLDAvis&#30340;&#22522;&#20110;&#32593;&#39029;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#21487;&#35270;&#21270;&#25152;&#36873;&#20027;&#39064;&#12290;&#36825;&#31181;&#21487;&#35270;&#21270;&#25552;&#20379;&#20102;&#20027;&#35201;&#20027;&#39064;&#30340;&#24635;&#20307;&#35270;&#22270;&#65292;&#21516;&#26102;&#20801;&#35768;&#28145;&#20837;&#29702;&#35299;&#20010;&#20307;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent and popularity of big data mining and huge text analysis in modern times, automated text summarization became prominent for extracting and retrieving important information from documents. This research investigates aspects of automatic text summarization from the perspectives of single and multiple documents. Summarization is a task of condensing huge text articles into short, summarized versions. The text is reduced in size for summarization purpose but preserving key vital information and retaining the meaning of the original document. This study presents the Latent Dirichlet Allocation (LDA) approach used to perform topic modelling from summarised medical science journal articles with topics related to genes and diseases. In this study, PyLDAvis web-based interactive visualization tool was used to visualise the selected topics. The visualisation provides an overarching view of the main topics while allowing and attributing deep meaning to the prevalence individual to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#33647;&#29289;&#27835;&#30103;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#23519;&#30149;&#20154;&#20010;&#20307;&#30340;&#21382;&#21490;&#35760;&#24405;&#12289;&#20351;&#29992; Transformer &#27169;&#22411;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#23458;&#35266;&#39044;&#27979;&#32467;&#26524;&#65292;&#20026;&#20010;&#24615;&#21270;&#36828;&#31243;&#20581;&#24247;&#20256;&#24863;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.13700</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36828;&#31243;&#29992;&#33647;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Remote Medication Status Prediction for Individuals with Parkinson's Disease using Time-series Data from Smartphones. (arXiv:2207.13700v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#33647;&#29289;&#27835;&#30103;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#23519;&#30149;&#20154;&#20010;&#20307;&#30340;&#21382;&#21490;&#35760;&#24405;&#12289;&#20351;&#29992; Transformer &#27169;&#22411;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#23458;&#35266;&#39044;&#27979;&#32467;&#26524;&#65292;&#20026;&#20010;&#24615;&#21270;&#36828;&#31243;&#20581;&#24247;&#20256;&#24863;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#30340;&#33647;&#29289;&#27835;&#30103;&#36890;&#24120;&#26159;&#36828;&#31243;&#36827;&#34892;&#30340;&#65292;&#36828;&#31163;&#21307;&#38498;&#30340;&#29615;&#22659;&#23545;&#21450;&#26102;&#20934;&#30830;&#22320;&#25910;&#38598;&#20581;&#24247;&#29366;&#24577;&#25968;&#25454;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#31359;&#25140;&#24335;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#34892;&#20026;&#20449;&#21495;&#30340;&#20010;&#20307;&#24046;&#24322;&#20063;&#23548;&#33268;&#37319;&#29992;&#24403;&#21069;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#27969;&#31243;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20844;&#20849; mPower &#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258; 487 &#21517;&#24739;&#32773;&#30340;&#26234;&#33021;&#25163;&#26426;&#25910;&#38598;&#30340;62,182&#20010;&#36828;&#31243;&#22810;&#27169;&#24335;&#27979;&#35797;&#35760;&#24405;&#65292;&#29992;&#20110;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#29992;&#33647;&#29366;&#24577;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#23519;&#30149;&#20154;&#20010;&#20307;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#36890;&#36807; Transformer &#27169;&#22411;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#65292;&#26377;&#26395;&#22312;&#23458;&#35266;&#19978;&#39044;&#27979;&#19977;&#31181;&#19981;&#21516;&#30340;&#29992;&#33647;&#29366;&#24577;&#65306;&#29992;&#33647;&#21069;&#65288;AUC = 0.95&#65289;&#12289;&#29992;&#33647;&#21518;&#65288;AUC = 0.958&#65289;&#21644;&#20854;&#20182;&#26102;&#21051;&#65288;AUC = 0.976&#65289;&#12290;&#26412;&#26041;&#27861;&#20026;&#20010;&#24615;&#21270;&#36828;&#31243;&#20581;&#24247;&#20256;&#24863;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication for neurological diseases such as the Parkinson's disease usually happens remotely away from hospitals. Such out-of-lab environments pose challenges in collecting timely and accurate health status data. Individual differences in behavioral signals collected from wearable sensors also lead to difficulties in adopting current general machine learning analysis pipelines. To address these challenges, we present a method for predicting the medication status of Parkinson's disease patients using the public mPower dataset, which contains 62,182 remote multi-modal test records collected on smartphones from 487 patients. The proposed method shows promising results in predicting three medication statuses objectively: Before Medication (AUC=0.95), After Medication (AUC=0.958), and Another Time (AUC=0.976) by examining patient-wise historical records with the attention weights learned through a Transformer model. Our method provides an innovative way for personalized remote health sensi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#21322;&#38544;&#31169;&#22330;&#26223;&#19979;&#22914;&#20309;&#36890;&#36807;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#29992;&#25143;&#25935;&#24863;&#23646;&#24615;&#26102;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.08336</link><description>&lt;p&gt;
&#24403;&#20844;&#24179;&#36935;&#21040;&#38544;&#31169;&#65306;&#21322;&#38544;&#31169;&#25935;&#24863;&#23646;&#24615;&#19979;&#30340;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes. (arXiv:2207.08336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08336
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#21322;&#38544;&#31169;&#22330;&#26223;&#19979;&#22914;&#20309;&#36890;&#36807;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#29992;&#25143;&#25935;&#24863;&#23646;&#24615;&#26102;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#24050;&#32463;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#38459;&#30861;&#23427;&#20204;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30830;&#20445;&#20844;&#24179;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#21162;&#21147;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#20197;&#20943;&#36731;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#23545;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#38544;&#31169;&#30340;&#25285;&#24551;&#65292;&#36890;&#24120;&#38590;&#20197;&#33719;&#21462;&#22823;&#35268;&#27169;&#29992;&#25143;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;&#30001;&#20110;&#27861;&#24459;&#21512;&#35268;&#24615;&#21644;&#20154;&#20204;&#23545;&#38544;&#31169;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#38544;&#31169;&#26426;&#21046;&#20363;&#22914;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#34987;&#24191;&#27867;&#24378;&#21046;&#25191;&#34892;&#20110;&#25935;&#24863;&#20449;&#24687;&#30340;&#25968;&#25454;&#25910;&#38598;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#22312;&#38544;&#31169;&#20445;&#25252;&#19979;&#36827;&#34892;&#20844;&#24179;&#39044;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21322;&#38544;&#31169;&#22330;&#26223;&#19979;&#30340;&#20844;&#24179;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#25935;&#24863;&#23646;&#24615;&#26159;&#31169;&#26377;&#30340;&#65292;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#26159;&#21487;&#29992;&#30340;&#25935;&#24863;&#23646;&#24615;&#26159;&#24178;&#20928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models have demonstrated promising performance in many areas. However, the concerns that they can be biased against specific demographic groups hinder their adoption in high-stake applications. Thus, it is essential to ensure fairness in machine learning models. Most previous efforts require direct access to sensitive attributes for mitigating bias. Nonetheless, it is often infeasible to obtain large-scale users' sensitive attributes considering users' concerns about privacy in the data collection process. Privacy mechanisms such as local differential privacy (LDP) are widely enforced on sensitive information in the data collection stage due to legal compliance and people's increasing awareness of privacy. Therefore, a critical problem is how to make fair predictions under privacy. We study a novel and practical problem of fair classification in a semi-private setting, where most of the sensitive attributes are private and only a small amount of clean ones are availabl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21487;&#35270;&#21270;&#26041;&#27861;&#26816;&#27979;&#27169;&#22411;&#24322;&#24120;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#35782;&#21035;&#24494;&#22937;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#26080;&#27861;&#35782;&#21035;&#23548;&#33268;&#24322;&#24120;&#34892;&#20026;&#30340;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#36879;&#26126;&#24230;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.13498</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#23457;&#35745;&#65306;&#36879;&#26126;&#26041;&#27861;&#38590;&#20197;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior. (arXiv:2206.13498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21487;&#35270;&#21270;&#26041;&#27861;&#26816;&#27979;&#27169;&#22411;&#24322;&#24120;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#35782;&#21035;&#24494;&#22937;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#26080;&#27861;&#35782;&#21035;&#23548;&#33268;&#24322;&#24120;&#34892;&#20026;&#30340;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#36879;&#26126;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21487;&#35270;&#21270;&#25552;&#20379;&#20102;&#20165;&#26377;&#36755;&#20986;&#21487;&#33021;&#20250;&#24573;&#30053;&#30340;&#20449;&#24687;&#12290;&#20294;&#25105;&#20204;&#33021;&#30456;&#20449;&#27169;&#22411;&#21487;&#35270;&#21270;&#21453;&#26144;&#20102;&#27169;&#22411;&#34892;&#20026;&#21527;&#65311;&#20363;&#22914;&#65292;&#23427;&#20204;&#33021;&#21542;&#35786;&#26029;&#20986;&#31181;&#26893;&#30340;&#21518;&#38376;&#25110;&#36807;&#24230;&#27491;&#21017;&#21270;&#31561;&#24322;&#24120;&#34892;&#20026;&#65311;&#20026;&#20102;&#35780;&#20272;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#23427;&#20204;&#26159;&#21542;&#23558;&#19981;&#27491;&#24120;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#27491;&#24120;&#27169;&#22411;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21040;&#26126;&#26174;&#24322;&#24120;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#35782;&#21035;&#26356;&#24494;&#22937;&#30340;&#24322;&#24120;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#35782;&#21035;&#23548;&#33268;&#24322;&#24120;&#34892;&#20026;&#30340;&#36755;&#20837;&#65292;&#20363;&#22914;&#21253;&#21547;&#34394;&#20551;&#25552;&#31034;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20123;&#27969;&#34892;&#27169;&#22411;&#21487;&#35270;&#21270;&#30340;&#30450;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#36879;&#26126;&#24230;&#26041;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model visualizations provide information that outputs alone might miss. But can we trust that model visualizations reflect model behavior? For instance, can they diagnose abnormal behavior such as planted backdoors or overregularization? To evaluate visualization methods, we test whether they assign different visualizations to anomalously trained models and normal models. We find that while existing methods can detect models with starkly anomalous behavior, they struggle to identify more subtle anomalies. Moreover, they often fail to recognize the inputs that induce anomalous behavior, e.g. images containing a spurious cue. These results reveal blind spots and limitations of some popular model visualizations. By introducing a novel evaluation framework for visualizations, our work paves the way for developing more reliable model transparency methods in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#33410;&#28857;&#20998;&#31867;&#20013;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GNN&#32467;&#26500;&#20197;&#30456;&#21516;&#30340;&#31934;&#24230;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#25152;&#38656;&#30340;&#26435;&#37325;&#27604;&#20351;&#29992;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#23569;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2206.05904</link><description>&lt;p&gt;
GNN&#22312;&#25512;&#24191;&#24102;&#38480;&#20989;&#25968;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#27604;NN&#26356;&#21152;&#26126;&#26174;
&lt;/p&gt;
&lt;p&gt;
Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#33410;&#28857;&#20998;&#31867;&#20013;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GNN&#32467;&#26500;&#20197;&#30456;&#21516;&#30340;&#31934;&#24230;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#25152;&#38656;&#30340;&#26435;&#37325;&#27604;&#20351;&#29992;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#23569;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20197;&#20854;&#25972;&#21512;&#22270;&#24418;&#20449;&#24687;&#30340;&#33021;&#21147;&#34987;&#24191;&#27867;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#20165;&#38024;&#23545;&#22270;&#32423;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#33410;&#28857;&#32423;&#20219;&#21153;&#65292;&#20363;&#22914;&#33410;&#28857;&#20998;&#31867;&#65292;&#20854;&#20013;&#35797;&#22270;&#20174;&#35266;&#23519;&#21040;&#30340;&#33410;&#28857;&#26631;&#31614;&#20013;&#25554;&#20540;&#20986;&#32570;&#22833;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#25152;&#36848;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23427;&#23454;&#36136;&#19978;&#26159;&#19968;&#20010;&#20989;&#25968;&#25554;&#20540;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;GNN&#25554;&#20540;$\mathbb{R}^d$&#20013;&#24102;&#38480;&#20989;&#25968;&#25152;&#38656;&#30340;&#26435;&#37325;&#21644;&#23618;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;GNN&#26550;&#26500;&#20197;$\epsilon$-&#36817;&#20284;&#31163;&#25955;&#24102;&#38480;&#20449;&#21495;&#20165;&#38656;&#35201;$O((\log \epsilon^{-1})^{d})$&#20010;&#26435;&#37325;&#65292;&#36825;&#27604;&#20351;&#29992;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24471;&#21040;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#25152;&#38656;&#26435;&#37325;&#23569;&#24471;&#22810; - &#29305;&#21035;&#22320;&#65292;&#20351;&#29992;&#20351;&#29992;$O((\log \epsilon^{-1})^{d})$&#20010;&#26679;&#26412;&#26469;&#35757;&#32451;GNN&#20197;$\epsilon$-&#36924;&#36817;&#24102;&#38480;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\mathbb{R}^d$. Our result shows that, the number of weights needed to $\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\log \epsilon^{-1})^{d})$ weights using a GNN trained by $O((\log \epsilon^{-1})^{d})$ samples to $\epsilon$-approximate a discretized bandlimited signal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#24577;&#23398;&#20064;&#20013;&#30340;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#26174;&#31034;&#20986;&#20219;&#20309;&#20351;&#29992;&#38750;&#30456;&#24178;&#27979;&#37327;&#30340;&#21327;&#35758;&#65292;&#21363;&#20351;&#26159;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#65292;&#37117;&#38656;&#35201;&#937;(d3/&#949;2)&#20010;&#22797;&#21046;&#21697;&#30340;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.05265</link><description>&lt;p&gt;
&#20309;&#26102;&#36866;&#24212;&#24615;&#26377;&#21161;&#20110;&#37327;&#23376;&#29366;&#24577;&#23398;&#20064;?
&lt;/p&gt;
&lt;p&gt;
When Does Adaptivity Help for Quantum State Learning?. (arXiv:2206.05265v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#24577;&#23398;&#20064;&#20013;&#30340;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#26174;&#31034;&#20986;&#20219;&#20309;&#20351;&#29992;&#38750;&#30456;&#24178;&#27979;&#37327;&#30340;&#21327;&#35758;&#65292;&#21363;&#20351;&#26159;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#65292;&#37117;&#38656;&#35201;&#937;(d3/&#949;2)&#20010;&#22797;&#21046;&#21697;&#30340;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20851;&#20110;&#8220;&#24577;&#37325;&#26500;&#8221;&#30340;&#32463;&#20856;&#38382;&#39064;&#65306;&#22312;&#30693;&#36947;&#19968;&#20010;&#26410;&#30693;&#37327;&#23376;&#24577;$\rho\in\mathbb{C^{d x d}}$&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#20986;&#19968;&#20010;&#36817;&#20284;&#30340;$\widehat{\rho}$&#65292;&#20351;&#24471;&#22312;&#26576;&#31181;&#24847;&#20041;&#19979;&#65292;&#20363;&#22914;&#36857;&#36317;&#31163;&#25110;&#20445;&#30495;&#24230;&#65292;&#23427;&#25509;&#36817;&#20110;$\rho$&#12290;&#24403;&#20801;&#35768;&#36827;&#34892;&#32416;&#32544;&#30340;&#30456;&#24178;&#27979;&#37327;&#26102;&#65292;&#38656;&#35201;$\Theta(d^2/\epsilon^2)$&#20010;&#22797;&#21046;&#20307;&#25165;&#33021;&#33719;&#24471;&#36857;&#36317;&#31163;$\epsilon$&#30340;&#36817;&#20284;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23454;&#29616;&#36825;&#31181;&#36895;&#29575;&#30340;&#21327;&#35758;&#38656;&#35201;&#22823;&#37327;&#30340;&#37327;&#23376;&#20869;&#23384;&#24320;&#38144;&#65292;&#22240;&#27492;&#19981;&#33021;&#22312;&#36817;&#26399;&#35774;&#22791;&#19978;&#23454;&#29616;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20351;&#29992;&#38750;&#30456;&#24178;&#65288;&#21333;&#20010;&#22797;&#21046;&#65289;&#27979;&#37327;&#30340;&#21327;&#35758;&#25152;&#38656;&#30340;&#22797;&#21046;&#21697;&#25968;&#37327;&#20026;$O(d^3/\epsilon^2)$&#65292;&#22810;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22914;&#20309;&#29702;&#35299;&#36825;&#20010;&#36895;&#29575;&#26159;&#19978;&#38480;&#36824;&#26159;&#19979;&#38480;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#23637;&#31034;&#20219;&#20309;&#20351;&#29992;&#38750;&#30456;&#24178;&#27979;&#37327;&#30340;&#21327;&#35758;&#65292;&#21363;&#20351;&#26159;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#65292;&#37117;&#38656;&#35201;$\Omega(d^3/\epsilon^2)$&#20010;&#22797;&#21046;&#21697;&#30340;&#37327;&#65292;&#20174;&#32780;&#21305;&#37197;&#24050;&#30693;&#30340;&#26368;&#22909;&#30340;&#19978;&#38480;&#65292;&#25105;&#20204;&#23436;&#20840;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;P...
&lt;/p&gt;
&lt;p&gt;
We consider the classic question of state tomography: given copies of an unknown quantum state $\rho\in\mathbb{C}^{d\times d}$, output $\widehat{\rho}$ which is close to $\rho$ in some sense, e.g. trace distance or fidelity. When one is allowed to make coherent measurements entangled across all copies, $\Theta(d^2/\epsilon^2)$ copies are necessary and sufficient to get trace distance $\epsilon$. Unfortunately, the protocols achieving this rate incur large quantum memory overheads that preclude implementation on near-term devices. On the other hand, the best known protocol using incoherent (single-copy) measurements uses $O(d^3/\epsilon^2)$ copies, and multiple papers have posed it as an open question to understand whether or not this rate is tight. In this work, we fully resolve this question, by showing that any protocol using incoherent measurements, even if they are chosen adaptively, requires $\Omega(d^3/\epsilon^2)$ copies, matching the best known upper bound.  We do so by a new p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;Doob h&#21464;&#25442;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#31163;&#25955;&#35266;&#27979;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;&#22312;&#32447;&#28388;&#27874;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39640;&#24230;&#20449;&#24687;&#21270;&#12289;&#35266;&#27979;&#20540;&#22312;&#27169;&#22411;&#19979;&#26497;&#31471;&#25110;&#29366;&#24577;&#32500;&#25968;&#36739;&#22823;&#26102;&#27604;&#26368;&#20808;&#36827;&#30340;&#31890;&#23376;&#28388;&#27874;&#22120;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.03369</link><description>&lt;p&gt;
&#31163;&#25955;&#35266;&#27979;&#25193;&#25955;&#36807;&#31243;&#30340;&#35745;&#31639;Doob h&#21464;&#25442;&#22312;&#32447;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Computational Doob's h-transforms for Online Filtering of Discretely Observed Diffusions. (arXiv:2206.03369v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;Doob h&#21464;&#25442;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#31163;&#25955;&#35266;&#27979;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;&#22312;&#32447;&#28388;&#27874;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39640;&#24230;&#20449;&#24687;&#21270;&#12289;&#35266;&#27979;&#20540;&#22312;&#27169;&#22411;&#19979;&#26497;&#31471;&#25110;&#29366;&#24577;&#32500;&#25968;&#36739;&#22823;&#26102;&#27604;&#26368;&#20808;&#36827;&#30340;&#31890;&#23376;&#28388;&#27874;&#22120;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#31163;&#25955;&#35266;&#27979;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;&#22312;&#32447;&#28388;&#27874;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23436;&#20840;&#36866;&#24212;&#30340;&#36741;&#21161;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#30340;Doob h&#21464;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#32447;&#24615;Feynman-Kac&#20844;&#24335;&#21644;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#28508;&#22312;&#30340;&#21453;&#21521;Kolmogorov&#26041;&#31243;&#26469;&#36817;&#20284;&#36825;&#20123;h&#21464;&#25442;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#22312;&#25968;&#25454;&#21516;&#21270;&#36807;&#31243;&#20043;&#21069;&#35757;&#32451;&#23616;&#37096;&#26368;&#20248;&#30340;&#31890;&#23376;&#28388;&#27874;&#22120;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#35266;&#27979;&#20540;&#39640;&#24230;&#20449;&#24687;&#21270;&#65292;&#35266;&#27979;&#20540;&#22312;&#27169;&#22411;&#19979;&#26497;&#31471;&#65292;&#25110;&#29366;&#24577;&#32500;&#25968;&#36739;&#22823;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#31890;&#23376;&#28388;&#27874;&#22120;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with online filtering of discretely observed nonlinear diffusion processes. Our approach is based on the fully adapted auxiliary particle filter, which involves Doob's $h$-transforms that are typically intractable. We propose a computational framework to approximate these $h$-transforms by solving the underlying backward Kolmogorov equations using nonlinear Feynman-Kac formulas and neural networks. The methodology allows one to train a locally optimal particle filter prior to the data-assimilation procedure. Numerical experiments illustrate that the proposed approach can be orders of magnitude more efficient than state-of-the-art particle filters in the regime of highly informative observations, when the observations are extreme under the model, or if the state dimension is large.
&lt;/p&gt;</description></item><item><title>InstaAug&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36755;&#20837;&#29305;&#23450;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#19981;&#21464;&#24615;&#27169;&#22359;&#20174;&#36755;&#20837;&#20013;&#26144;&#23556;&#21040;&#37327;&#36523;&#23450;&#21046;&#30340;&#21464;&#25442;&#21442;&#25968;&#65292;&#25429;&#25417;&#23616;&#37096;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30417;&#30563;&#21644;&#33258;&#25105;&#30417;&#30563;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.00051</link><description>&lt;p&gt;
&#25429;&#25417;&#23616;&#37096;&#19981;&#21464;&#24615;&#30340;&#23398;&#20064;&#23454;&#20363;&#29305;&#23450;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Instance-Specific Augmentations by Capturing Local Invariances. (arXiv:2206.00051v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00051
&lt;/p&gt;
&lt;p&gt;
InstaAug&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36755;&#20837;&#29305;&#23450;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#19981;&#21464;&#24615;&#27169;&#22359;&#20174;&#36755;&#20837;&#20013;&#26144;&#23556;&#21040;&#37327;&#36523;&#23450;&#21046;&#30340;&#21464;&#25442;&#21442;&#25968;&#65292;&#25429;&#25417;&#23616;&#37096;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30417;&#30563;&#21644;&#33258;&#25105;&#30417;&#30563;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;InstaAug&#65292;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;&#20197;&#24448;&#30340;&#23398;&#20064;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21407;&#22987;&#36755;&#20837;&#21644;&#24212;&#29992;&#20110;&#35813;&#36755;&#20837;&#30340;&#21464;&#25442;&#20043;&#38388;&#23384;&#22312;&#29420;&#31435;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#38750;&#24120;&#38480;&#21046;&#65292;&#22240;&#20026;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#22686;&#24378;&#25429;&#25417;&#30340;&#19981;&#21464;&#24615;&#26412;&#36523;&#36890;&#24120;&#39640;&#24230;&#20381;&#36182;&#20110;&#36755;&#20837;&#12290;InstaAug&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#19981;&#21464;&#24615;&#27169;&#22359;&#65292;&#20174;&#36755;&#20837;&#20013;&#26144;&#23556;&#21040;&#37327;&#36523;&#23450;&#21046;&#30340;&#21464;&#25442;&#21442;&#25968;&#65292;&#20197;&#25429;&#25417;&#23616;&#37096;&#19981;&#21464;&#24615;&#12290;&#36825;&#21487;&#20197;&#21516;&#26102;&#19982;&#19979;&#28216;&#27169;&#22411;&#23436;&#20840;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#25110;&#32773;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#21333;&#29420;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;InstaAug&#23398;&#20064;&#20102;&#24191;&#27867;&#30340;&#21464;&#25442;&#31867;&#21035;&#30340;&#26377;&#24847;&#20041;&#30340;&#36755;&#20837;&#30456;&#20851;&#22686;&#24378;&#65292;&#36825;&#36827;&#32780;&#25552;&#39640;&#20102;&#30417;&#30563;&#21644;&#33258;&#25105;&#30417;&#30563;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce InstaAug, a method for automatically learning input-specific augmentations from data. Previous methods for learning augmentations have typically assumed independence between the original input and the transformation applied to that input. This can be highly restrictive, as the invariances we hope our augmentation will capture are themselves often highly input dependent. InstaAug instead introduces a learnable invariance module that maps from inputs to tailored transformation parameters, allowing local invariances to be captured. This can be simultaneously trained alongside the downstream model in a fully end-to-end manner, or separately learned for a pre-trained model. We empirically demonstrate that InstaAug learns meaningful input-dependent augmentations for a wide range of transformation classes, which in turn provides better performance on both supervised and self-supervised tasks.
&lt;/p&gt;</description></item><item><title>BrainIB&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#24320;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#20998;&#26512;fMRI&#22270;&#20687;&#20013;&#30340;&#21151;&#33021;&#36830;&#25509;&#26102;&#33021;&#22815;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36793;&#32536;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#26410;&#35265;&#26679;&#26412;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2205.03612</link><description>&lt;p&gt;
BrainIB&#65306;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#30340;&#21487;&#35299;&#37322;&#24615;&#33041;&#32593;&#32476;&#31934;&#31070;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck. (arXiv:2205.03612v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03612
&lt;/p&gt;
&lt;p&gt;
BrainIB&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#24320;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#20998;&#26512;fMRI&#22270;&#20687;&#20013;&#30340;&#21151;&#33021;&#36830;&#25509;&#26102;&#33021;&#22815;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36793;&#32536;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#26410;&#35265;&#26679;&#26412;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#29983;&#29289;&#26426;&#21046;&#32780;&#38750;&#20027;&#35266;&#30151;&#29366;&#23545;&#31934;&#31070;&#38556;&#30861;&#36827;&#34892;&#35786;&#26029;&#30340;&#26032;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#20849;&#35782;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#21151;&#33021;&#36830;&#25509;&#65288;FC&#65289;&#36827;&#34892;&#31934;&#31070;&#38556;&#30861;&#21644;&#20581;&#24247;&#23545;&#29031;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#29992;&#20110;&#30830;&#23450;&#22823;&#33041;&#26631;&#35760;&#29289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35786;&#26029;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#30340;&#24773;&#20917;&#65288;&#30001;&#20110;&#35757;&#32451;&#26679;&#26412;&#19981;&#36275;&#65289;&#65292;&#22312;&#26032;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#34920;&#29616;&#24046;&#12290;&#27492;&#22806;&#65292;&#38590;&#20197;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#12289;&#21487;&#38752;&#30340;&#22823;&#33041;&#29983;&#29289;&#26631;&#35760;&#29289;&#26469;&#35299;&#37322;&#28508;&#22312;&#30340;&#35786;&#26029;&#20915;&#31574;&#12290;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#20854;&#21487;&#33021;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BrainIB&#65292;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#33879;&#21517;&#30340;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#26469;&#20998;&#26512;&#21151;&#33021;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;fMRI&#65289;&#12290;BrainIB&#33021;&#22815;&#35782;&#21035;&#22823;&#33041;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36793;&#32536;&#65288;&#21363;&#23376;&#22270;&#65289;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#26410;&#35265;&#26679;&#26412;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing a new diagnostic models based on the underlying biological mechanisms rather than subjective symptoms for psychiatric disorders is an emerging consensus. Recently, machine learning-based classifiers using functional connectivity (FC) for psychiatric disorders and healthy controls are developed to identify brain markers. However, existing machine learningbased diagnostic models are prone to over-fitting (due to insufficient training samples) and perform poorly in new test environment. Furthermore, it is difficult to obtain explainable and reliable brain biomarkers elucidating the underlying diagnostic decisions. These issues hinder their possible clinical applications. In this work, we propose BrainIB, a new graph neural network (GNN) framework to analyze functional magnetic resonance images (fMRI), by leveraging the famed Information Bottleneck (IB) principle. BrainIB is able to identify the most informative edges in the brain (i.e., subgraph) and generalizes well to unseen 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#65292;&#29992;&#20110;&#39034;&#24207;&#36873;&#25321;TLM&#39044;&#35757;&#32451;&#36229;&#21442;&#25968;&#65292;&#26088;&#22312;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;Thompson&#25277;&#26679;&#65288;GP-TS&#65289;&#31639;&#27861;&#65292;&#21152;&#36895;Pre-training&#36807;&#31243;&#24182;&#38477;&#20302;MLM&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2203.13151</link><description>&lt;p&gt;
&#22810;&#33218;&#32769;&#34382;&#26426;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#36164;&#28304;&#39640;&#25928;&#12289;&#22312;&#32447;&#20248;&#21270;&#65306;&#21160;&#24577;&#36974;&#30422;&#30340;&#20351;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#65292;&#29992;&#20110;&#39034;&#24207;&#36873;&#25321;TLM&#39044;&#35757;&#32451;&#36229;&#21442;&#25968;&#65292;&#26088;&#22312;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;Thompson&#25277;&#26679;&#65288;GP-TS&#65289;&#31639;&#27861;&#65292;&#21152;&#36895;Pre-training&#36807;&#31243;&#24182;&#38477;&#20302;MLM&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#39044;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;TLM&#65289;&#12290; TLM&#39044;&#35757;&#32451;&#38656;&#35201;&#39640;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24341;&#20837;&#35768;&#22810;&#26410;&#35299;&#20915;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#20363;&#22914;&#36873;&#25321;&#20854;&#39044;&#35757;&#32451;&#36229;&#21442;&#25968;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#65292;&#29992;&#20110;&#39034;&#24207;&#36873;&#25321;TLM&#39044;&#35757;&#32451;&#36229;&#21442;&#25968;&#65292;&#26088;&#22312;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#20854;&#39034;&#24207;&#26368;&#23567;&#21270;&#30340;&#24102;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#20195;&#29702;&#39640;&#26031;&#36807;&#31243;&#22870;&#21169;&#27169;&#22411;&#12290; &#25552;&#20986;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;Thompson&#25277;&#26679;&#65288;GP-TS&#65289;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#25513;&#30721;&#27010;&#29575;&#36827;&#34892;MLM&#39044;&#35757;&#32451;&#65292;&#32780;&#26159;&#36890;&#36807;&#39034;&#24207;&#36873;&#25321;&#25913;&#21892;&#24615;&#33021;&#30340;&#25513;&#30721;&#36229;&#21442;&#25968;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#12290; &#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;GP-TS&#22914;&#20309;&#39640;&#25928;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#23569;&#37327;&#36845;&#20195;&#20013;&#23454;&#29616;&#26356;&#20302;&#30340;MLM&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters. We propose a multi-armed bandit framework for the sequential selection of TLM pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization. Instead of MLM pre-training with fixed masking probabilities, the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training by sequentially selecting masking hyperparameters that improve performance. We empirically demonstrate how GP-TS pre-trains language models efficiently, i.e., it achieves lower MLM loss in fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19968;&#31867;&#21452;&#23618;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#38656;warm-start&#20063;&#21487;&#23454;&#29616;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.03397</link><description>&lt;p&gt;
&#26377;&#19979;&#23618;&#21387;&#32553;&#30340;&#21452;&#23618;&#20248;&#21270;: &#26080;warm-start&#24773;&#20917;&#19979;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bilevel Optimization with a Lower-level Contraction: Optimal Sample Complexity without Warm-Start. (arXiv:2202.03397v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31867;&#21452;&#23618;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#38656;warm-start&#20063;&#21487;&#23454;&#29616;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31867;&#19968;&#33324;&#30340;&#21452;&#23618;&#38382;&#39064;&#65292;&#20854;&#20013;&#19978;&#23618;&#38382;&#39064;&#26159;&#23558;&#19968;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#26368;&#23567;&#21270;&#65292;&#19979;&#23618;&#38382;&#39064;&#26159;&#23547;&#25214;&#19968;&#20809;&#28369;&#25910;&#32553;&#26144;&#23556;&#30340;&#19981;&#21160;&#28857;&#12290;&#36825;&#31867;&#38382;&#39064;&#21253;&#25324;&#20803;&#23398;&#20064;&#12289;&#22343;&#34913;&#27169;&#22411;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#25968;&#25454;&#27745;&#26579;&#23545;&#25239;&#25915;&#20987;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#21363;&#20351;&#27809;&#26377;warm-start&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22914;&#20803;&#23398;&#20064;&#21644;&#22343;&#34913;&#27169;&#22411;&#65292;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#39034;&#24207;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyse a general class of bilevel problems, in which the upper-level problem consists in the minimization of a smooth objective function and the lower-level problem is to find the fixed point of a smooth contraction map. This type of problems include instances of meta-learning, equilibrium models, hyperparameter optimization and data poisoning adversarial attacks. Several recent works have proposed algorithms which warm-start the lower-level problem, i.e. they use the previous lower-level approximate solution as a staring point for the lower-level solver. This warm-start procedure allows one to improve the sample complexity in both the stochastic and deterministic settings, achieving in some cases the order-wise optimal sample complexity. However, there are situations, e.g., meta learning and equilibrium models, in which the warm-start procedure is not well-suited or ineffective. In this work we show that without warm-start, it is still possible to achieve order-wise (near) optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#39640;&#25928;&#30340;&#30452;&#36830;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#33410;&#28857;&#24310;&#36831;&#21644;&#24102;&#23485;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2202.03356</link><description>&lt;p&gt;
&#39640;&#25928;&#30452;&#36830;&#25299;&#25169;&#32467;&#26500;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Efficient Direct-Connect Topologies for Collective Communications. (arXiv:2202.03356v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#39640;&#25928;&#30340;&#30452;&#36830;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#33410;&#28857;&#24310;&#36831;&#21644;&#24102;&#23485;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26500;&#24314;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#30340;&#39640;&#25928;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#38024;&#23545;&#33410;&#28857;&#24310;&#36831;&#19982;&#24102;&#23485;&#26435;&#34913;&#20248;&#21270;&#30340;&#30452;&#36830;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20010;&#31639;&#27861;&#26694;&#26550;&#20174;&#23567;&#30340;&#22522;&#30784;&#25299;&#25169;&#32467;&#26500;&#21644;&#30456;&#20851;&#30340;&#36890;&#20449;&#36827;&#24230;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#21487;&#20197;&#36845;&#20195;&#24212;&#29992;&#30340;&#25216;&#26415;&#26469;&#27966;&#29983;&#26356;&#22823;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20123;&#34893;&#29983;&#30340;&#25299;&#25169;&#32467;&#26500;&#30340;&#26102;&#38388;&#34920;&#21487;&#20197;&#19982;&#25193;&#23637;&#19968;&#36215;&#21512;&#25104;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#20248;&#21270;&#20844;&#24335;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#20026;&#32473;&#23450;&#30340;&#38598;&#32676;&#22823;&#23567;&#21644;&#24230;&#25968;&#21512;&#25104;&#35768;&#22810;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#26102;&#38388;&#34920;&#65292;&#28982;&#21518;&#20026;&#32473;&#23450;&#30340;&#24037;&#20316;&#36127;&#36733;&#30830;&#23450;&#36866;&#24403;&#30340;&#25299;&#25169;&#21644;&#26102;&#38388;&#34920;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#34917;&#19969;&#38754;&#26495;&#37197;&#32622;&#25152;&#38656;&#25299;&#25169;&#32467;&#26500;&#30340;12&#33410;&#28857;&#20809;&#23398;&#23454;&#39564;&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22686;&#21152;&#20102;&#22522;&#20110;&#20998;&#26512;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#29992;&#20110;&#26356;&#22823;&#30340;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of distilling efficient network topologies for collective communications. We provide an algorithmic framework for constructing direct-connect topologies optimized for the node latency vs bandwidth trade-off given a collective communication workload. Our algorithmic framework allows us to start from small base topologies and associated communication schedules and use a set of techniques that can be iteratively applied to derive much larger topologies. The schedules for these derived topologies are either synthesized along with the expansions or computed using an optimization formulation. Our approach allows us to synthesize many different topologies and schedules for a given cluster size and degree, and then identify the appropriate topology and schedule for a given workload. We evaluate our approach on a 12-node optical testbed that uses patch panels for configuring the desired topology and augment it with an analytical-model-based evaluation for larger deployme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25104;&#21592;&#26080;&#20998;&#24067;&#27169;&#22411;&#65292;&#29992;&#20110;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#65292;&#25903;&#25345;&#33410;&#28857;&#25152;&#23646;&#22810;&#20010;&#31038;&#32676;&#21644;&#26377;&#38480;&#23454;&#25968;&#26435;&#20540;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#28508;&#22312;&#31038;&#32676;&#32467;&#26500;&#30340;&#37325;&#21472;&#31526;&#21495;&#32593;&#32476;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#35889;&#31639;&#27861;&#20272;&#35745;&#27169;&#22411;&#19979;&#30340;&#31038;&#32676;&#25104;&#21592;&#36164;&#26684;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#31946;&#21152;&#26435;&#27169;&#22359;&#24230;&#26469;&#35780;&#20272;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#36136;&#37327;&#24182;&#30830;&#23450;&#21152;&#26435;&#32593;&#32476;&#31038;&#32676;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2112.04389</link><description>&lt;p&gt;
&#28151;&#21512;&#25104;&#21592;&#26080;&#20998;&#24067;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed Membership Distribution-Free Model. (arXiv:2112.04389v4 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.04389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25104;&#21592;&#26080;&#20998;&#24067;&#27169;&#22411;&#65292;&#29992;&#20110;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#65292;&#25903;&#25345;&#33410;&#28857;&#25152;&#23646;&#22810;&#20010;&#31038;&#32676;&#21644;&#26377;&#38480;&#23454;&#25968;&#26435;&#20540;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#28508;&#22312;&#31038;&#32676;&#32467;&#26500;&#30340;&#37325;&#21472;&#31526;&#21495;&#32593;&#32476;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#35889;&#31639;&#27861;&#20272;&#35745;&#27169;&#22411;&#19979;&#30340;&#31038;&#32676;&#25104;&#21592;&#36164;&#26684;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#31946;&#21152;&#26435;&#27169;&#22359;&#24230;&#26469;&#35780;&#20272;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#36136;&#37327;&#24182;&#30830;&#23450;&#21152;&#26435;&#32593;&#32476;&#31038;&#32676;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#20855;&#26377;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#20013;&#36827;&#34892;&#31038;&#32676;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#33410;&#28857;&#21487;&#20197;&#23646;&#20110;&#22810;&#20010;&#31038;&#32676;&#65292;&#36793;&#26435;&#21487;&#20197;&#26159;&#26377;&#38480;&#23454;&#25968;&#12290;&#20026;&#20102;&#23545;&#36825;&#26679;&#30340;&#22797;&#26434;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#8212;&#8212;&#28151;&#21512;&#25104;&#21592;&#26080;&#20998;&#24067;&#65288;MMDF&#65289;&#27169;&#22411;&#12290;MMDF&#27809;&#26377;&#23545;&#36793;&#26435;&#30340;&#20998;&#24067;&#32422;&#26463;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20123;&#20808;&#21069;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21253;&#25324;&#33879;&#21517;&#30340;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#20855;&#26377;&#28508;&#22312;&#31038;&#32676;&#32467;&#26500;&#30340;&#37325;&#21472;&#31526;&#21495;&#32593;&#32476;&#20063;&#21487;&#20197;&#20174;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#25910;&#25947;&#29575;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#35889;&#31639;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#19979;&#30340;&#31038;&#32676;&#25104;&#21592;&#36164;&#26684;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#27169;&#31946;&#21152;&#26435;&#27169;&#22359;&#24230;&#26469;&#35780;&#20272;&#20855;&#26377;&#27491;&#36127;&#36793;&#26435;&#30340;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#25105;&#20204;&#30340;fuzzy weighted modularity&#26469;&#30830;&#23450;&#21152;&#26435;&#32593;&#32476;&#31038;&#32676;&#25968;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of community detection in overlapping weighted networks, where nodes can belong to multiple communities and edge weights can be finite real numbers. To model such complex networks, we propose a general framework - the mixed membership distribution-free (MMDF) model. MMDF has no distribution constraints of edge weights and can be viewed as generalizations of some previous models, including the well-known mixed membership stochastic blockmodels. Especially, overlapping signed networks with latent community structures can also be generated from our model. We use an efficient spectral algorithm with a theoretical guarantee of convergence rate to estimate community memberships under the model. We also propose fuzzy weighted modularity to evaluate the quality of community detection for overlapping weighted networks with positive and negative edge weights. We then provide a method to determine the number of communities for weighted networks by taking advantage of our f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#31561;&#21464;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65288;GNPE&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21442;&#25968;&#21644;&#25968;&#25454;&#32852;&#21512;&#21464;&#25442;&#19979;&#25972;&#21512;&#31561;&#21464;&#24615;&#65292;&#29992;&#20110;&#20174;&#24341;&#21147;&#27874;&#35266;&#27979;&#20013;&#23545;&#21452;&#40657;&#27934;&#31995;&#32479;&#36827;&#34892;&#25674;&#38144;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2111.13139</link><description>&lt;p&gt;
&#32676;&#31561;&#21464;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Group equivariant neural posterior estimation. (arXiv:2111.13139v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#31561;&#21464;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65288;GNPE&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21442;&#25968;&#21644;&#25968;&#25454;&#32852;&#21512;&#21464;&#25442;&#19979;&#25972;&#21512;&#31561;&#21464;&#24615;&#65292;&#29992;&#20110;&#20174;&#24341;&#21147;&#27874;&#35266;&#27979;&#20013;&#23545;&#21452;&#40657;&#27934;&#31995;&#32479;&#36827;&#34892;&#25674;&#38144;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#30340;&#20223;&#30495;&#25512;&#29702;&#26159;&#35299;&#20915;&#31185;&#23398;&#39046;&#22495;&#21453;&#38382;&#39064;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#22522;&#30784;&#27491;&#21521;&#27169;&#22411;&#35270;&#20026;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#19981;&#20250;&#21033;&#29992;&#31561;&#21464;&#24615;&#31561;&#20960;&#20309;&#24615;&#36136;&#12290;&#31561;&#21464;&#24615;&#22312;&#31185;&#23398;&#27169;&#22411;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#23558;&#20854;&#30452;&#25509;&#25972;&#21512;&#21040;&#34920;&#36798;&#24335;&#25512;&#29702;&#32593;&#32476;&#65288;&#22914;&#24402;&#19968;&#21270;&#27969;&#65289;&#20013;&#24182;&#19981;&#31616;&#21333;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21442;&#25968;&#21644;&#25968;&#25454;&#30340;&#32852;&#21512;&#21464;&#25442;&#19979;&#25972;&#21512;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#31216;&#20026;&#32676;&#31561;&#21464;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65288;GNPE&#65289;&#8212;&#8212;&#22522;&#20110;&#33258;&#27965;&#22320;&#26631;&#20934;&#21270;&#25968;&#25454;&#30340;&#8220;&#23039;&#24577;&#8221;&#65292;&#21516;&#26102;&#20272;&#35745;&#21442;&#25968;&#21518;&#39564;&#12290;&#23427;&#26159;&#29420;&#31435;&#20110;&#20307;&#31995;&#32467;&#26500;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#31934;&#30830;&#21644;&#36817;&#20284;&#31561;&#21464;&#24615;&#12290;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#21033;&#29992;GNPE&#20174;&#24341;&#21147;&#27874;&#35266;&#27979;&#20013;&#23545;&#21452;&#40657;&#27934;&#31995;&#32479;&#36827;&#34892;&#20102;&#25674;&#38144;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based inference with conditional neural density estimators is a powerful approach to solving inverse problems in science. However, these methods typically treat the underlying forward model as a black box, with no way to exploit geometric properties such as equivariances. Equivariances are common in scientific models, however integrating them directly into expressive inference networks (such as normalizing flows) is not straightforward. We here describe an alternative method to incorporate equivariances under joint transformations of parameters and data. Our method -- called group equivariant neural posterior estimation (GNPE) -- is based on self-consistently standardizing the "pose" of the data while estimating the posterior over parameters. It is architecture-independent, and applies both to exact and approximate equivariances. As a real-world application, we use GNPE for amortized inference of astrophysical binary black hole systems from gravitational-wave observations. W
&lt;/p&gt;</description></item><item><title>&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#22797;&#26434; AI &#27169;&#22411;&#38656;&#35201;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#35774;&#35745;&#20915;&#31574;&#23545;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2109.15284</link><description>&lt;p&gt;
AI-Enabled &#31227;&#21160;&#24212;&#29992;&#20013;&#30340;&#21738;&#20123;&#35774;&#35745;&#20915;&#31574;&#26377;&#21161;&#20110;&#26356;&#29615;&#20445;&#30340; AI&#65311;
&lt;/p&gt;
&lt;p&gt;
Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener AI?. (arXiv:2109.15284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.15284
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#22797;&#26434; AI &#27169;&#22411;&#38656;&#35201;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#35774;&#35745;&#20915;&#31574;&#23545;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26500;&#24314;&#12289;&#21457;&#23637;&#21644;&#20351;&#29992;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#38656;&#35201;&#26114;&#36149;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#34429;&#28982;&#24403;&#21069;&#21487;&#29992;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#29615;&#22659;&#25903;&#25345;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20294;&#23558; AI &#27169;&#22411;&#37096;&#32626;&#21040;&#31227;&#21160;&#35774;&#22791;&#19978;&#65288;&#36825;&#26159;&#19968;&#31181;&#19981;&#26029;&#22686;&#38271;&#30340;&#36235;&#21183;&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#21253;&#25324;&#35745;&#31639;&#36164;&#28304;&#32570;&#20047;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#24847;&#21619;&#30528;&#22312; AI &#21551;&#29992;&#36719;&#20214;&#24037;&#31243;&#29983;&#21629;&#21608;&#26399;&#20013;&#36827;&#34892;&#35774;&#35745;&#20915;&#31574;&#30340;&#26102;&#20505;&#38656;&#35201;&#24179;&#34913;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#26435;&#34913;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#31995;&#32479;&#22320;&#35780;&#20272;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#22797;&#26434;&#30340; AI &#27169;&#22411;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#26102;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23427;&#20204;&#20855;&#26377;&#38544;&#24335;&#30340;&#36164;&#28304;&#38480;&#21046;&#12290;&#25105;&#20204;&#26088;&#22312;&#28085;&#30422;&#65288;i&#65289;&#35774;&#35745;&#20915;&#31574;&#23545;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#39564;&#35777;&#21078;&#26512;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#38024;&#23545;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340; AI&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The construction, evolution and usage of complex artificial intelligence (AI) models demand expensive computational resources. While currently available high-performance computing environments support well this complexity, the deployment of AI models in mobile devices, which is an increasing trend, is challenging. Mobile applications consist of environments with low computational resources and hence imply limitations in the design decisions during the AI-enabled software engineering lifecycle that balance the trade-off between the accuracy and the complexity of the mobile applications.  Objective: Our objective is to systematically assess the trade-off between accuracy and complexity when deploying complex AI models (e.g. neural networks) to mobile devices, which have an implicit resource limitation. We aim to cover (i) the impact of the design decisions on the achievement of high-accuracy and low resource-consumption implementations; and (ii) the validation of profiling to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#20351;&#29992;&#20248;&#21270;&#35774;&#35745;&#30340;&#36172;&#21338;&#31639;&#27861;&#36951;&#25022;&#20998;&#24067;&#20855;&#26377;&#38750;&#24120;&#37325;&#30340;&#23614;&#37096;&#65292;&#23545;&#20110;$p&gt;1$&#65292;&#36951;&#25022;&#20998;&#24067;&#30340;$p$'th&#30697;&#22686;&#38271;&#35201;&#27604;&#22810;&#23545;&#25968;&#32423;&#21035;&#24555;&#24471;&#22810;&#65292;&#24403;&#38382;&#39064;&#30053;&#24494;&#38169;&#35823;&#26102;&#65292;&#20248;&#21270;UCB&#36172;&#21338;&#35774;&#35745;&#30340;&#36951;&#25022;&#21487;&#20197;&#27604;&#20256;&#32479;&#29702;&#35770;&#24314;&#35758;&#30340;&#22686;&#38271;&#24471;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2109.13595</link><description>&lt;p&gt;
&#20248;&#21270;&#36172;&#21338;&#31639;&#27861;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Fragility of Optimized Bandit Algorithms. (arXiv:2109.13595v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.13595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#20351;&#29992;&#20248;&#21270;&#35774;&#35745;&#30340;&#36172;&#21338;&#31639;&#27861;&#36951;&#25022;&#20998;&#24067;&#20855;&#26377;&#38750;&#24120;&#37325;&#30340;&#23614;&#37096;&#65292;&#23545;&#20110;$p&gt;1$&#65292;&#36951;&#25022;&#20998;&#24067;&#30340;$p$'th&#30697;&#22686;&#38271;&#35201;&#27604;&#22810;&#23545;&#25968;&#32423;&#21035;&#24555;&#24471;&#22810;&#65292;&#24403;&#38382;&#39064;&#30053;&#24494;&#38169;&#35823;&#26102;&#65292;&#20248;&#21270;UCB&#36172;&#21338;&#35774;&#35745;&#30340;&#36951;&#25022;&#21487;&#20197;&#27604;&#20256;&#32479;&#29702;&#35770;&#24314;&#35758;&#30340;&#22686;&#38271;&#24471;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20851;&#20110;&#36172;&#21338;&#31639;&#27861;&#26368;&#20248;&#35774;&#35745;&#30340;&#25991;&#29486;&#37117;&#26159;&#22522;&#20110;&#26399;&#26395;&#36951;&#25022;&#30340;&#26368;&#23567;&#21270;&#12290;&#24050;&#30693;&#23545;&#20110;&#26576;&#20123;&#25351;&#25968;&#26063;&#65292;&#26368;&#20248;&#35774;&#35745;&#22312;&#25289;&#20381;-&#32599;&#23486;&#26031;&#19979;&#30028;&#25351;&#23548;&#19979;&#65292;&#21487;&#23454;&#29616;&#26399;&#26395;&#36951;&#25022;&#20197;&#23545;&#25968;&#32423;&#21035;&#22686;&#38271;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#36825;&#31181;&#26368;&#20248;&#35774;&#35745;&#26102;&#65292;&#20851;&#32852;&#31639;&#27861;&#30340;&#36951;&#25022;&#20998;&#24067;&#24517;&#28982;&#20855;&#26377;&#19968;&#20010;&#38750;&#24120;&#37325;&#30340;&#23614;&#37096;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#25130;&#26029;&#26607;&#35199;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;$p&gt;1$&#65292;&#36951;&#25022;&#20998;&#24067;&#30340;$p$'th&#30697;&#22686;&#38271;&#35201;&#27604;&#22810;&#23545;&#25968;&#32423;&#21035;&#24555;&#24471;&#22810;&#65292;&#29305;&#21035;&#26159;&#20316;&#20026;&#34915;&#34966;&#25968;&#30340;&#24130;&#20989;&#25968;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20248;&#21270;UCB&#36172;&#21338;&#35774;&#35745;&#22312;&#21478;&#19968;&#20010;&#26041;&#38754;&#20063;&#24456;&#33030;&#24369;&#65292;&#21363;&#24403;&#38382;&#39064;&#30053;&#24494;&#38169;&#35823;&#26102;&#65292;&#36951;&#25022;&#21487;&#20197;&#27604;&#20256;&#32479;&#29702;&#35770;&#24314;&#35758;&#30340;&#22686;&#38271;&#24471;&#26356;&#24555;&#12290;&#25105;&#20204;&#30340;&#35770;&#28857;&#22522;&#20110;&#26631;&#20934;&#30340;&#25514;&#26045;&#25913;&#21464;&#24605;&#24819;&#65292;&#24182;&#34920;&#26126;&#26368;&#20248;&#35774;&#35745;&#21487;&#33021;&#23548;&#33268;&#19968;&#20123;&#19981;&#22826;&#21487;&#33021;&#21457;&#29983;&#30340;&#24773;&#20917;&#65292;&#22240;&#27492;&#24212;&#35813;&#20197;&#35880;&#24910;&#30340;&#26041;&#24335;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of the literature on optimal design of bandit algorithms is based on minimization of expected regret. It is well known that designs that are optimal over certain exponential families can achieve expected regret that grows logarithmically in the number of arm plays, at a rate governed by the Lai-Robbins lower bound. In this paper, we show that when one uses such optimized designs, the regret distribution of the associated algorithms necessarily has a very heavy tail, specifically, that of a truncated Cauchy distribution. Furthermore, for $p&gt;1$, the $p$'th moment of the regret distribution grows much faster than poly-logarithmically, in particular as a power of the total number of arm plays. We show that optimized UCB bandit designs are also fragile in an additional sense, namely when the problem is even slightly mis-specified, the regret can grow much faster than the conventional theory suggests. Our arguments are based on standard change-of-measure ideas, and indicate that the mos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bipartite Distribution-Free&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#21644;&#25506;&#27979;&#21152;&#26435;&#20108;&#20998;&#32593;&#32476;&#30340;&#31038;&#21306;&#32467;&#26500;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#33410;&#28857;&#24230;&#25968;&#30340;&#21464;&#21270;&#20197;&#21450;&#26399;&#26395;&#30340;&#22359;&#32467;&#26500;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35889;&#31639;&#27861;&#29992;&#20110;&#35782;&#21035;&#31038;&#21306;&#12290;</title><link>http://arxiv.org/abs/2109.10319</link><description>&lt;p&gt;
&#21152;&#26435;&#20108;&#20998;&#32593;&#32476;&#30340;&#31038;&#21306;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Community detection for weighted bipartite networks. (arXiv:2109.10319v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.10319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bipartite Distribution-Free&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#21644;&#25506;&#27979;&#21152;&#26435;&#20108;&#20998;&#32593;&#32476;&#30340;&#31038;&#21306;&#32467;&#26500;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#33410;&#28857;&#24230;&#25968;&#30340;&#21464;&#21270;&#20197;&#21450;&#26399;&#26395;&#30340;&#22359;&#32467;&#26500;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35889;&#31639;&#27861;&#29992;&#20110;&#35782;&#21035;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20998;&#32593;&#32476;&#20986;&#29616;&#22312;&#22810;&#20010;&#39046;&#22495;&#65292;&#20363;&#22914;&#29983;&#29289;&#23398;&#12289;&#31038;&#20250;&#23398;&#12289;&#29983;&#29702;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#38543;&#26426;&#20849;&#21516;&#22359;&#27169;&#22411;&#65288;ScBM&#65289;&#26469;&#26816;&#27979;&#20108;&#20998;&#22270;&#25968;&#25454;&#30340;&#31038;&#21306;&#32467;&#26500;&#65292;&#20294;&#26159;ScBM&#23436;&#20840;&#24573;&#30053;&#36793;&#26435;&#24182;&#19988;&#26080;&#27861;&#35299;&#37322;&#21152;&#26435;&#20108;&#20998;&#32593;&#32476;&#30340;&#22359;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25918;&#23485;ScBM&#30340;&#20998;&#24067;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Bipartite Distribution-Free&#30340;&#27169;&#22411;&#26469;&#24314;&#27169;&#21152;&#26435;&#20108;&#20998;&#32593;&#32476;&#65292;&#24182;&#32771;&#34385;&#33410;&#28857;&#24230;&#25968;&#30340;&#21464;&#21270;&#26469;&#26500;&#24314;&#27169;&#22411;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#38656;&#35201;&#22312;&#37051;&#25509;&#30697;&#38453;&#30340;&#29983;&#25104;&#20803;&#19978;&#25351;&#23450;&#29305;&#23450;&#30340;&#20998;&#24067;&#65292;&#32780;&#21482;&#38656;&#35201;&#22312;&#26399;&#26395;&#37051;&#25509;&#30697;&#38453;&#19978;&#25351;&#23450;&#22359;&#32467;&#26500;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#35889;&#31639;&#27861;&#65292;&#26469;&#35782;&#21035;&#31038;&#21306;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#35777;&#20363;&#23376;&#26469;&#23637;&#31034;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bipartite network appears in various areas, such as biology, sociology, physiology, and computer science. \cite{rohe2016co} proposed Stochastic co-Blockmodel (ScBM) as a tool for detecting community structure of binary bipartite graph data in network studies. However, ScBM completely ignores edge weight and is unable to explain the block structure of a weighted bipartite network. Here, to model a weighted bipartite network, we introduce a Bipartite Distribution-Free model by releasing ScBM's distribution restriction. We also build an extension of the proposed model by considering the variation of node degree. Our models do not require a specific distribution on generating elements of the adjacency matrix but only a block structure on the expected adjacency matrix. Spectral algorithms with theoretical guarantees on the consistent estimation of node labels are presented to identify communities. Our proposed methods are illustrated by simulated and empirical examples.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26497;&#23567;&#21270;&#20998;&#26512;&#65292;&#26088;&#22312;&#22238;&#31572;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36716;&#31227;&#31283;&#23450;&#20449;&#24687;&#26102;&#24212;&#35813;&#36716;&#31227;&#21738;&#20010;&#23376;&#38598;&#20174;&#32780;&#36798;&#21040;&#26368;&#20339;&#30340;&#27867;&#21270;&#33021;&#21147;&#36825;&#19968;&#38382;&#39064;</title><link>http://arxiv.org/abs/2107.01876</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#36716;&#31227;&#21738;&#31181;&#19981;&#21464;&#24615;&#65311;&#19968;&#31181;&#22240;&#26524;&#26497;&#23567;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Which Invariance Should We Transfer? A Causal Minimax Learning Approach. (arXiv:2107.01876v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26497;&#23567;&#21270;&#20998;&#26512;&#65292;&#26088;&#22312;&#22238;&#31572;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36716;&#31227;&#31283;&#23450;&#20449;&#24687;&#26102;&#24212;&#35813;&#36716;&#31227;&#21738;&#20010;&#23376;&#38598;&#20174;&#32780;&#36798;&#21040;&#26368;&#20339;&#30340;&#27867;&#21270;&#33021;&#21147;&#36825;&#19968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#21487;&#38752;&#24212;&#23545;&#25968;&#25454;&#38598;&#21464;&#21270;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#35797;&#22270;&#23558;&#31283;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;&#30475;&#19981;&#35265;&#30340;&#29615;&#22659;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#22522;&#20110;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#36890;&#36807;do-operator&#28040;&#38500;&#21487;&#21464;&#30340;&#22240;&#26524;&#26426;&#21046;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#24471;&#21040;&#30340;&#31283;&#23450;&#39044;&#27979;&#22240;&#20026;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#31283;&#23450;&#20449;&#24687;&#32780;&#26356;&#21152;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24212;&#35813;&#36716;&#31227;&#36825;&#25972;&#20010;&#31283;&#23450;&#20449;&#24687;&#20013;&#30340;&#21738;&#20010;&#23376;&#38598;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26497;&#23567;&#21270;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21028;&#26029;&#25972;&#20010;&#31283;&#23450;&#38598;&#26159;&#21542;&#26368;&#20248;&#30340;&#22270;&#24418;&#26465;&#20214;&#12290;&#24403;&#36825;&#20010;&#26465;&#20214;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;&#36890;&#36807;&#19968;&#20010;&#20363;&#23376;&#65292;&#36825;&#20010;&#25972;&#20010;&#31283;&#23450;&#38598;&#34429;&#28982;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#31283;&#23450;&#20449;&#24687;&#65292;&#20294;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#36716;&#31227;&#38598;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#20248;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#26368;&#23567;&#21547;&#20041;&#30340;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20223;&#30495;&#21644;&#23454;&#38469;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major barrier to deploying current machine learning models lies in their non-reliability to dataset shifts. To resolve this problem, most existing studies attempted to transfer stable information to unseen environments. Particularly, independent causal mechanisms-based methods proposed to remove mutable causal mechanisms via the do-operator. Compared to previous methods, the obtained stable predictors are more effective in identifying stable information. However, a key question remains: which subset of this whole stable information should the model transfer, in order to achieve optimal generalization ability? To answer this question, we present a comprehensive minimax analysis from a causal perspective. Specifically, we first provide a graphical condition for the whole stable set to be optimal. When this condition fails, we surprisingly find with an example that this whole stable set, although can fully exploit stable information, is not the optimal one to transfer. To identify the o
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#30340;&#26367;&#20195;&#21697;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861; "DINGO" &#33021;&#22815;&#23454;&#29616;&#23545;&#26816;&#27979;&#21040;&#30340;&#24341;&#21147;&#27874;&#20107;&#20214;&#29289;&#29702;&#21442;&#25968;&#30340;&#24555;&#36895;&#20934;&#30830;&#25512;&#26029;&#65292;&#21487;&#22312;&#19981;&#25439;&#22833;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2106.12594</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#36827;&#34892;&#23454;&#26102;&#24341;&#21147;&#27874;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Real-time gravitational-wave science with neural posterior estimation. (arXiv:2106.12594v2 [gr-qc] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.12594
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#30340;&#26367;&#20195;&#21697;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861; "DINGO" &#33021;&#22815;&#23454;&#29616;&#23545;&#26816;&#27979;&#21040;&#30340;&#24341;&#21147;&#27874;&#20107;&#20214;&#29289;&#29702;&#21442;&#25968;&#30340;&#24555;&#36895;&#20934;&#30830;&#25512;&#26029;&#65292;&#21487;&#22312;&#19981;&#25439;&#22833;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24341;&#21147;&#27874;&#21442;&#25968;&#35780;&#20272;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#30340;&#26367;&#20195;&#21697;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#31532;&#19968;&#20010;LIGO-Virgo&#24341;&#21147;&#27874;&#30636;&#21464;&#30446;&#24405;&#20013;&#30340;8&#20010;&#24341;&#21147;&#27874;&#20107;&#20214;&#65292;&#24182;&#21457;&#29616;&#19982;&#26631;&#20934;&#25512;&#26029;&#20195;&#30721;&#38750;&#24120;&#25509;&#36817;&#65292;&#20294;&#25512;&#26029;&#26102;&#38388;&#20174; O(day) &#38477;&#20302;&#21040;&#27599;&#20010;&#20107;&#20214;&#30340;&#19968;&#20998;&#38047;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#20107;&#20214;&#38468;&#36817;&#30340;&#25506;&#27979;&#22120;&#22122;&#22768;&#29305;&#24449;&#20272;&#35745;&#12290;&#36825;&#26679;&#21487;&#20197;&#23558;&#20449;&#21495;&#21644;&#22122;&#22768;&#27169;&#22411;&#32534;&#30721;&#22312;&#25968;&#30334;&#19975;&#20010;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#65292;&#24182;&#33021;&#22815;&#38024;&#23545;&#19982;&#35757;&#32451;&#20998;&#24067;&#19968;&#33268;&#30340;&#20219;&#20309;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#65292;&#32771;&#34385;&#20174;&#20107;&#20214;&#21040;&#20107;&#20214;&#30340;&#22122;&#22768;&#38750;&#31283;&#24577;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861; -- &#21517;&#20026; "DINGO" -- &#35774;&#23450;&#20102;&#26816;&#27979;&#21040;&#30340;&#24341;&#21147;&#27874;&#20107;&#20214;&#29289;&#29702;&#21442;&#25968;&#24555;&#36895;&#20934;&#30830;&#25512;&#26029;&#30340;&#26032;&#26631;&#20934;&#65292;&#36825;&#24212;&#35813;&#20351;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#19981;&#25439;&#22833;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate unprecedented accuracy for rapid gravitational-wave parameter estimation with deep learning. Using neural networks as surrogates for Bayesian posterior distributions, we analyze eight gravitational-wave events from the first LIGO-Virgo Gravitational-Wave Transient Catalog and find very close quantitative agreement with standard inference codes, but with inference times reduced from O(day) to a minute per event. Our networks are trained using simulated data, including an estimate of the detector-noise characteristics near the event. This encodes the signal and noise models within millions of neural-network parameters, and enables inference for any observed data consistent with the training distribution, accounting for noise nonstationarity from event to event. Our algorithm -- called "DINGO" -- sets a new standard in fast-and-accurate inference of physical parameters of detected gravitational-wave events, which should enable real-time data analysis without sacrificing acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#38656;&#20449;&#20219;&#26381;&#21153;&#22120;&#25110;&#20854;&#20182;&#25968;&#25454;&#28304;&#30340;&#36328; silo &#32852;&#37030;&#23398;&#20064;&#65292;&#32771;&#34385;&#20102;&#36328; silo &#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169; ISRL-DP&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30830;&#20445;&#26469;&#33258;&#27599;&#20010;&#20154;&#30340;&#25968;&#25454;&#37117;&#19981;&#20250;&#34987;&#27844;&#28431;&#12290;</title><link>http://arxiv.org/abs/2106.09779</link><description>&lt;p&gt;
&#26080;&#38656;&#20449;&#20219;&#30340;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65306;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses. (arXiv:2106.09779v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#38656;&#20449;&#20219;&#26381;&#21153;&#22120;&#25110;&#20854;&#20182;&#25968;&#25454;&#28304;&#30340;&#36328; silo &#32852;&#37030;&#23398;&#20064;&#65292;&#32771;&#34385;&#20102;&#36328; silo &#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169; ISRL-DP&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30830;&#20445;&#26469;&#33258;&#27599;&#20010;&#20154;&#30340;&#25968;&#25454;&#37117;&#19981;&#20250;&#34987;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#36328;&#25968;&#25454;&#28304;&#65288;&#36328; silo&#65289;FL&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#20027;&#20154;&#37117;&#19981;&#20449;&#20219;&#26381;&#21153;&#22120;&#25110;&#20854;&#20182; silos&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#25968;&#25454;&#28304;&#65288;&#20363;&#22914;&#21307;&#38498;&#65289;&#37117;&#26377;&#26469;&#33258;&#19981;&#21516;&#20154;&#65288;&#20363;&#22914;&#24739;&#32773;&#65289;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#24517;&#39035;&#32500;&#25252;&#27599;&#20010;&#20154;&#65288;&#20363;&#22914;&#21307;&#30103;&#35760;&#24405;&#65289;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#21363;&#20351;&#26381;&#21153;&#22120;&#25110;&#20854;&#20182;&#25968;&#25454;&#28304;&#26159;&#24694;&#24847;&#30417;&#21548;&#32773;&#12290;&#36825;&#31181;&#35201;&#27714;&#20419;&#36827;&#20102;&#23545;&#36328; silo &#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;ISRL-DP&#65289;&#30340;&#30740;&#31350;&#65292;&#23427;&#35201;&#27714; silo i &#30340;&#36890;&#20449;&#28385;&#36275;&#35760;&#24405; / &#39033;&#30446;&#32423;&#24046;&#20998;&#38544;&#31169; (DP)&#12290;ISRL-DP &#30830;&#20445; silo i &#20013;&#27599;&#20010;&#20154;&#65288;&#20363;&#22914;&#24739;&#32773;&#65289;&#30340;&#25968;&#25454;&#37117;&#19981;&#20250;&#27844;&#28431;&#12290;ISRL-DP &#19981;&#21516;&#20110;&#21508;&#31181;&#24050;&#26377;&#30340;&#38544;&#31169;&#27010;&#24565;&#12290;&#20013;&#24515;&#21644;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#20551;&#23450;&#20154;&#20204;&#20449;&#20219;&#26381;&#21153;&#22120;/&#20854;&#20182;&#25968;&#25454;&#28304;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#26412;&#22320;DP &#20551;&#23450;&#20154;&#20204;&#26681;&#26412;&#19981;&#20449;&#20219;&#20219;&#20309;&#20154;&#65288;&#29978;&#33267;&#26159;&#20182;&#20204;&#33258;&#24049;&#30340;&#25968;&#25454;&#28304;&#65289;&#12290;ISRL-DP &#22788;&#20110;&#20013;&#24515;&#21644;&#26412;&#22320;DP &#20043;&#38388;&#65292;&#20351;&#24471;&#22312;&#36328; silo &#30340;&#30495;&#23454;&#24773;&#20917;&#19979;&#20855;&#26377;&#29616;&#23454;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies federated learning (FL)--especially cross-silo FL--with data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) has data from different people (e.g. patients) and must maintain the privacy of each person's data (e.g. medical record), even if the server or other silos act as adversarial eavesdroppers. This requirement motivates the study of Inter-Silo Record-Level Differential Privacy (ISRL-DP), which requires silo i's communications to satisfy record/item-level differential privacy (DP). ISRL-DP ensures that the data of each person (e.g. patient) in silo i (e.g. hospital i) cannot be leaked. ISRL-DP is different from well-studied privacy notions. Central and user-level DP assume that people trust the server/other silos. On the other end of the spectrum, local DP assumes that people do not trust anyone at all (even their own silo). Sitting between central and local DP, ISRL-DP makes the realistic assumption (in cross-sil
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25913;&#36827;Moment Accountant&#26041;&#27861;&#65292;DP-SGD&#20855;&#26377;&#21487;&#20851;&#38381;&#24418;&#24335;&#30340;$(\epsilon&#65292;\delta)$-DP&#20445;&#35777;&#65292;&#24182;&#19988;&#20854;&#20445;&#35777;&#25509;&#36817;&#26159;&#32039;&#23494;&#30340;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2102.09030</link><description>&lt;p&gt;
&#35770;DP-SGD&#30340;Moment Accountant&#26041;&#27861;&#30340;&#32039;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Tightness of the Moment Accountant for DP-SGD. (arXiv:2102.09030v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.09030
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;Moment Accountant&#26041;&#27861;&#65292;DP-SGD&#20855;&#26377;&#21487;&#20851;&#38381;&#24418;&#24335;&#30340;$(\epsilon&#65292;\delta)$-DP&#20445;&#35777;&#65292;&#24182;&#19988;&#20854;&#20445;&#35777;&#25509;&#36817;&#26159;&#32039;&#23494;&#30340;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;SGD&#65288;DP-SGD&#65289;&#20013;&#65292;&#22312;&#25191;&#34892;&#21098;&#20999;&#25805;&#20316;&#21518;&#65292;&#21521;&#26412;&#22320;SGD&#26356;&#26032;&#28155;&#21152;&#26631;&#20934;&#24046;&#20026;$ \sigma $&#30340;&#39640;&#26031;&#22122;&#22768;&#12290;&#36890;&#36807;&#38750;&#24179;&#20961;&#22320;&#25913;&#36827;Moment Accountant&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;$(\epsilon&#65292;\delta)$-DP&#20445;&#35777;&#65306;&#22914;&#26524;$ \sigma=\sqrt{ 2(\epsilon+\ln(1/\delta))/\epsilon} $&#65292;&#21017;DP-SGD&#26159;$ (\epsilon \leq 1/2&#65292;\delta = 1 / N) $-DP&#65292;&#20854;&#20013;$T$&#33267;&#23569;&#20026;$ \approx 2k^2/\epsilon$&#65292; $(2/e)^2k^2-1/2\geq \ln(N)$&#65292;&#20854;&#20013;$T$&#26159;&#22238;&#21512;&#30340;&#24635;&#25968;&#65292;$ K = kN $&#26159;&#26799;&#24230;&#35745;&#31639;&#30340;&#24635;&#25968;&#65292;&#20854;&#20013;$ k $&#29992;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;$N$&#30340;&#26102;&#20195;&#25968;&#37327;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#34920;&#36798;&#24335;&#25509;&#36817;&#32039;&#65292;&#22312;$T$&#23567;&#20110;&#32422;&#20026;$ 8 $&#20493;&#20110;&#19979;&#30028;$ \approx 2k^2/\epsilon$&#30340;&#24120;&#25968;&#22240;&#23376;&#26102;&#65292;$(\epsilon&#65292;\delta)$-DP&#20445;&#35777;&#23558;&#34987;&#36829;&#21453;&#12290;&#36873;&#25321;&#26368;&#23567;&#21487;&#33021;&#20540;&#30340;$T \approx 2k^2/\epsilon$&#19981;&#20165;&#20250;&#23548;&#33268;&#25509;&#36817;&#23494;&#38598;&#30340;DP&#20445;&#35777;&#65292;&#32780;&#19988;&#36824;&#20250;&#26368;&#23567;&#21270;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to provide differential privacy, Gaussian noise with standard deviation $\sigma$ is added to local SGD updates after performing a clipping operation in Differential Private SGD (DP-SGD). By non-trivially improving the moment account method we prove a closed form $(\epsilon,\delta)$-DP guarantee: DP-SGD is $(\epsilon\leq 1/2,\delta=1/N)$-DP if $\sigma=\sqrt{2(\epsilon +\ln(1/\delta))/\epsilon}$ with $T$ at least $\approx 2k^2/\epsilon$ and $(2/e)^2k^2-1/2\geq \ln(N)$, where $T$ is the total number of rounds, and $K=kN$ is the total number of gradient computations where $k$ measures $K$ in number of epochs of size $N$ of the local data set. We prove that our expression is close to tight in that if $T$ is more than a constant factor $\approx 8$ smaller than the lower bound $\approx 2k^2/\epsilon$, then the $(\epsilon,\delta)$-DP guarantee is violated. Choosing the smallest possible value $T\approx 2k^2/\epsilon$ not only leads to a close to tight DP guarantee, but also minimizes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20026;2&#30340;&#24102;&#38480;&#21046;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#23450;&#20102;&#24403;&#38544;&#34255;&#21442;&#25968;&#20998;&#24067;&#20110;&#26377;&#30028;&#22495;&#26102;&#65292;&#32593;&#32476;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#38646;&#36924;&#36817;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2008.08427</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#24102;&#38480;&#21046;&#30340;&#38543;&#26426;&#26435;&#37325;&#26377;&#22810;&#22823;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Powerful are Shallow Neural Networks with Bandlimited Random Weights?. (arXiv:2008.08427v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.08427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20026;2&#30340;&#24102;&#38480;&#21046;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#23450;&#20102;&#24403;&#38544;&#34255;&#21442;&#25968;&#20998;&#24067;&#20110;&#26377;&#30028;&#22495;&#26102;&#65292;&#32593;&#32476;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#38646;&#36924;&#36817;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#20026;2&#30340;&#24102;&#38480;&#21046;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#38543;&#26426;&#32593;&#32476;&#26159;&#25351;&#38544;&#34255;&#23618;&#21442;&#25968;&#34987;&#20923;&#32467;&#24182;&#36171;&#20104;&#38543;&#26426;&#20998;&#37197;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21482;&#26377;&#36755;&#20986;&#23618;&#21442;&#25968;&#36890;&#36807;&#25439;&#22833;&#26368;&#23567;&#21270;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;&#38543;&#26426;&#26435;&#37325;&#30340;&#38544;&#34255;&#23618;&#26159;&#36991;&#20813;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#20248;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#24050;&#34987;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#25152;&#37319;&#29992;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#26159;&#26222;&#36866;&#36924;&#36817;&#22120;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#20107;&#23454;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#24403;&#38544;&#34255;&#21442;&#25968;&#20998;&#24067;&#20110;&#26377;&#30028;&#22495;&#26102;&#65292;&#32593;&#32476;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#38646;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#29305;&#21035;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#24179;&#20961;&#36924;&#36817;&#35823;&#24046;&#19979;&#30028;&#12290;&#35777;&#26126;&#21033;&#29992;&#20102;Ridgelet&#20998;&#26512;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#35856;&#27874;&#20998;&#26512;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#20102;&#32463;&#20856;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#29305;&#21035;&#26159;&#20449;&#21495;&#22312;&#26576;&#31181;&#38480;&#21046;&#19979;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the expressive power of depth-2 bandlimited random neural networks. A random net is a neural network where the hidden layer parameters are frozen with random assignment, and only the output layer parameters are trained by loss minimization. Using random weights for a hidden layer is an effective method to avoid non-convex optimization in standard gradient descent learning. It has also been adopted in recent deep learning theories. Despite the well-known fact that a neural network is a universal approximator, in this study, we mathematically show that when hidden parameters are distributed in a bounded domain, the network may not achieve zero approximation error. In particular, we derive a new nontrivial approximation error lower bound. The proof utilizes the technique of ridgelet analysis, a harmonic analysis method designed for neural networks. This method is inspired by fundamental principles in classical signal processing, specifically the idea that signals with limit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#26368;&#36817;&#37051;&#30340;&#28857;&#38598;&#25277;&#26679;&#65292;&#25152;&#28041;&#21450;&#30340;RaySense&#33609;&#22270;&#21487;&#20197;&#25429;&#25417;&#28857;&#30340;&#22522;&#26412;&#20960;&#20309;&#24418;&#24577;&#20197;&#21450;&#25552;&#21462;&#19982;&#20043;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#19988;&#21487;&#39640;&#25928;&#22320;&#36827;&#34892;&#28857;&#38598;&#19978;&#30340;&#32447;&#31215;&#20998;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/1911.10737</link><description>&lt;p&gt;
&#29992;&#23556;&#32447;&#36827;&#34892;&#26368;&#36817;&#37051;&#28857;&#38598;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbor Sampling of Point Sets using Rays. (arXiv:1911.10737v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.10737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#26368;&#36817;&#37051;&#30340;&#28857;&#38598;&#25277;&#26679;&#65292;&#25152;&#28041;&#21450;&#30340;RaySense&#33609;&#22270;&#21487;&#20197;&#25429;&#25417;&#28857;&#30340;&#22522;&#26412;&#20960;&#20309;&#24418;&#24577;&#20197;&#21450;&#25552;&#21462;&#19982;&#20043;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#19988;&#21487;&#39640;&#25928;&#22320;&#36827;&#34892;&#28857;&#38598;&#19978;&#30340;&#32447;&#31215;&#20998;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25277;&#26679;&#12289;&#21387;&#32553;&#21644;&#20998;&#26512;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#28857;&#38598;&#21644;&#20854;&#20182;&#20960;&#20309;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#31181;&#31216;&#20026;RaySense&#33609;&#22270;&#30340;&#24352;&#37327;&#65292;&#35813;&#24352;&#37327;&#25429;&#25417;&#27839;&#30528;&#19968;&#32452;&#23556;&#32447;&#30340;&#28857;&#30340;&#22522;&#26412;&#20960;&#20309;&#24418;&#24577;&#30340;&#26368;&#36817;&#37051;&#23621;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21487;&#20197;&#22312;RaySense&#33609;&#22270;&#19978;&#25191;&#34892;&#30340;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21516;&#30340;&#23646;&#24615;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#23556;&#32447;&#38598;&#30340;&#24773;&#20917;&#19979;&#20174;&#33609;&#22270;&#20013;&#25552;&#21462;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#20351;&#29992;&#33609;&#22270;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#28857;&#38598;&#19978;&#30340;&#32447;&#31215;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20960;&#20010;&#31034;&#20363;&#65292;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#31574;&#30053;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new framework for the sampling, compression, and analysis of distributions of point sets and other geometric objects embedded in Euclidean spaces. Our approach involves the construction of a tensor called the RaySense sketch, which captures the nearest neighbors from the underlying geometry of points along a set of rays. We explore various operations that can be performed on the RaySense sketch, leading to different properties and potential applications. Statistical information about the data set can be extracted from the sketch, independent of the ray set. Line integrals on point sets can be efficiently computed using the sketch. We also present several examples illustrating applications of the proposed strategy in practical scenarios.
&lt;/p&gt;</description></item></channel></rss>