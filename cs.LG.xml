<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;kNN&#31639;&#27861;&#36827;&#34892;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#21160;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21464;&#37327;&#36873;&#25321;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01635</link><description>&lt;p&gt;
kNN&#31639;&#27861;&#29992;&#20110;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#20272;&#35745;&#65292;&#20855;&#26377;&#33258;&#21160;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21464;&#37327;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
kNN Algorithm for Conditional Mean and Variance Estimation with Automated Uncertainty Quantification and Variable Selection
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;kNN&#31639;&#27861;&#36827;&#34892;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#21160;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21464;&#37327;&#36873;&#25321;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;kNN&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;kNN&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#19982;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#36873;&#25321;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#30446;&#26631;&#26159;&#20934;&#30830;&#20272;&#35745;&#38543;&#26426;&#21709;&#24212;&#21464;&#37327;&#30340;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25551;&#36848;&#21508;&#31181;&#24773;&#26223;&#19979;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#20102;&#19968;&#20010;&#20581;&#22766;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26426;&#21046;&#65292;&#21033;&#29992;&#25105;&#20204;&#20043;&#21069;&#20851;&#20110;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#30340;&#20272;&#35745;&#24037;&#20316;&#12290; kNN&#30340;&#24212;&#29992;&#30830;&#20445;&#20102;&#22312;&#39044;&#27979;&#21306;&#38388;&#26102;&#21487;&#25193;&#23637;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#19982;&#26368;&#20248;&#38750;&#21442;&#25968;&#36895;&#29575;&#30456;&#19968;&#33268;&#30340;&#32479;&#35745;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;kNN&#21322;&#21442;&#25968;&#31639;&#27861;&#26469;&#20272;&#35745;&#32771;&#34385;&#21327;&#21464;&#37327;&#30340;ROC&#26354;&#32447;&#12290;&#23545;&#20110;&#36873;&#25321;&#24179;&#28369;&#21442;&#25968;k&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#21464;&#37327;&#36873;&#25321;&#30340;&#24341;&#20837;&#26174;&#33879;&#25552;&#39640;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a kNN-based regression method that synergizes the scalability and adaptability of traditional non-parametric kNN models with a novel variable selection technique. This method focuses on accurately estimating the conditional mean and variance of random response variables, thereby effectively characterizing conditional distributions across diverse scenarios.Our approach incorporates a robust uncertainty quantification mechanism, leveraging our prior estimation work on conditional mean and variance. The employment of kNN ensures scalable computational efficiency in predicting intervals and statistical accuracy in line with optimal non-parametric rates. Additionally, we introduce a new kNN semi-parametric algorithm for estimating ROC curves, accounting for covariates. For selecting the smoothing parameter k, we propose an algorithm with theoretical guarantees.Incorporation of variable selection enhances the performance of the method significantly over convention
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01632</link><description>&lt;p&gt;
&#36229;&#36234;&#23610;&#24230;&#65306;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#26080;&#36951;&#25022;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01632
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#38656;&#35201;&#25311;&#21512;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#32780;&#25311;&#21512;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#38656;&#35201;&#25351;&#23450;&#36229;&#21442;&#25968; - &#22823;&#37096;&#20998;&#29702;&#35770;&#25991;&#29486;&#20551;&#35774;&#36825;&#20123;&#36229;&#21442;&#25968;&#26159;&#24050;&#30693;&#30340;&#12290;&#20043;&#21069;&#30340;&#29702;&#35770;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#22312;&#31354;&#38388;&#20013;&#22343;&#21248;&#22635;&#20805;&#65292;&#32780;&#24120;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#21482;&#26377;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25165;&#26159;&#19968;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#25968;&#25454;&#19981;&#19968;&#23450;&#28385;&#36275;&#36825;&#31181;&#22343;&#21248;&#22635;&#20805;&#30340;&#26465;&#20214;&#12290;&#30001;&#20110;&#26080;&#27861;&#20445;&#35777;&#36229;&#21442;&#25968;&#20272;&#35745;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#19988;&#36825;&#20123;&#36229;&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#39640;&#26031;&#36807;&#31243;&#25311;&#21512;&#65292;&#22240;&#27492;&#23545;&#20855;&#26377;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20043;&#21069;&#25552;&#20986;&#30340;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#30340;&#31639;&#27861;&#20165;&#33021;&#22788;&#29702;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#26410;&#30693;&#38271;&#24230;&#23610;&#24230;&#12289;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#33539;&#25968;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#39057;&#29575;&#27966;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;HE-GP-UCB&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#30340;&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparame
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#24191;&#20041;&#35821;&#27861;&#35268;&#21017;&#65288;GGRs&#65289;&#26469;&#23454;&#29616;&#32452;&#21512;&#19968;&#33324;&#21270;&#65292;&#23558;&#20854;&#35270;&#20026;&#36716;&#23548;&#20219;&#21153;&#20013;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#24418;&#24335;&#21270;&#20102;&#35821;&#35328;&#36716;&#23548;&#30340;&#24191;&#20041;&#23545;&#31216;&#24615;&#27010;&#24565;&#65292;&#36824;&#19982;&#24378;&#21270;&#23398;&#20064;&#21644;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#26377;&#20851;&#32852;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01629</link><description>&lt;p&gt;
&#12298;&#35770;&#25991;&#26631;&#39064;&#65306;&#24191;&#20041;&#35821;&#27861;&#35268;&#21017;&#21644;&#22522;&#20110;&#32467;&#26500;&#30340;&#19968;&#33324;&#21270;&#8212;&#8212;&#23545;&#20110;&#35789;&#27719;&#20219;&#21153;&#21644;&#36716;&#23548;&#30340;&#32463;&#20856;&#31561;&#21464;&#24615;&#20043;&#22806;&#30340;&#19968;&#33324;&#21270;&#30340;&#31435;&#22330;&#35770;&#25991;&#12299;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#24191;&#20041;&#35821;&#27861;&#35268;&#21017;&#65288;GGRs&#65289;&#26469;&#23454;&#29616;&#32452;&#21512;&#19968;&#33324;&#21270;&#65292;&#23558;&#20854;&#35270;&#20026;&#36716;&#23548;&#20219;&#21153;&#20013;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#24418;&#24335;&#21270;&#20102;&#35821;&#35328;&#36716;&#23548;&#30340;&#24191;&#20041;&#23545;&#31216;&#24615;&#27010;&#24565;&#65292;&#36824;&#19982;&#24378;&#21270;&#23398;&#20064;&#21644;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#26377;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#19968;&#33324;&#21270;&#26159;&#20154;&#31867;&#23398;&#20064;&#35789;&#27719;&#19982;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#20027;&#35201;&#24046;&#24322;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#33021;&#22815;&#20351;&#29992;&#24191;&#20041;&#35821;&#27861;&#35268;&#21017;&#65288;GGRs&#65289;&#36827;&#34892;&#32452;&#21512;&#19968;&#33324;&#21270;&#30340;&#27169;&#22411;&#65292;GGRs&#26159;&#19968;&#31867;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#36716;&#23548;&#20219;&#21153;&#30340;&#32452;&#21512;&#32422;&#26463;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#21463;&#29289;&#29702;&#23398;&#20219;&#21153;&#20013;&#31561;&#21464;&#24615;&#32422;&#26463;&#21551;&#21457;&#30340;&#36716;&#23548;&#31867;&#27604;&#12290;&#38500;&#20102;&#20026;&#35821;&#35328;&#36716;&#23548;&#24418;&#24335;&#21270;&#24191;&#20041;&#30340;&#23545;&#31216;&#24615;&#27010;&#24565;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#20197;&#21253;&#21547;&#35768;&#22810;&#29616;&#26377;&#24037;&#20316;&#20316;&#20026;&#29305;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;GGRs&#30340;&#24819;&#27861;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#19982;&#24378;&#21270;&#23398;&#20064;&#21644;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization is one of the main properties which differentiates lexical learning in humans from state-of-art neural networks. We propose a general framework for building models that can generalize compositionally using the concept of Generalized Grammar Rules (GGRs), a class of symmetry-based compositional constraints for transduction tasks, which we view as a transduction analogue of equivariance constraints in physics-inspired tasks. Besides formalizing generalized notions of symmetry for language transduction, our framework is general enough to contain many existing works as special cases. We present ideas on how GGRs might be implemented, and in the process draw connections to reinforcement learning and other areas of research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#20004;&#28857;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30340;&#39640;&#25928;&#24615;&#24182;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01621</link><description>&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#27169;&#22411;&#38646;&#38454;&#20248;&#21270;&#30340;&#38543;&#26426;&#20004;&#28857;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Two Points Method for Deep Model Zeroth-order Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#20004;&#28857;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30340;&#39640;&#25928;&#24615;&#24182;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#30001;&#20110;&#30828;&#20214;&#39044;&#31639;&#25110;&#32570;&#20047;&#21453;&#21521;&#20256;&#25773;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#26500;&#24314;&#25110;&#23436;&#20840;&#24494;&#35843;&#36825;&#26679;&#30340;&#22823;&#27169;&#22411;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#38646;&#38454;&#26041;&#27861;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#23427;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#36882;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#26412;&#25991;&#22312;&#26080;&#26799;&#24230;&#24773;&#24418;&#19979;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38543;&#26426;&#20004;&#28857;&#65288;S2P&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#33324;&#21644;&#25918;&#26494;&#30340;&#24179;&#28369;&#24615;&#20551;&#35774;&#19979;&#25552;&#20986;&#20102;S2P&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#36136;&#12290;&#29702;&#35770;&#24615;&#36136;&#36824;&#25581;&#31034;&#20102;&#26356;&#24555;&#12289;&#26356;&#31283;&#23450;&#30340;S2P&#21464;&#20307;&#8212;&#8212;&#21152;&#36895;S2P&#65288;AS2P&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#26032;&#25910;&#25947;&#24615;&#36136;&#65292;&#26356;&#22909;&#22320;&#34920;&#31034;&#20102;&#28145;&#24230;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;AS2P&#22312;&#20248;&#21270;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;&#65288;&#21253;&#25324;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#30446;&#26631;&#19978;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#40065;&#26834;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#25935;&#25463;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#21644;&#24674;&#22797;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#20027;&#21160;&#39044;&#27979;&#35268;&#21010;&#22833;&#36133;&#30340;&#39118;&#38505;&#24182;&#36827;&#34892;&#24674;&#22797;&#65292;&#21516;&#26102;&#33021;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#20135;&#29983;&#28789;&#27963;&#30340;&#36816;&#21160;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01617</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#40065;&#26834;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26410;&#30693;&#29615;&#22659;&#20013;&#25935;&#25463;&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#21644;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
A GP-based Robust Motion Planning Framework for Agile Autonomous Robot Navigation and Recovery in Unknown Environments
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#40065;&#26834;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#25935;&#25463;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#21644;&#24674;&#22797;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#20027;&#21160;&#39044;&#27979;&#35268;&#21010;&#22833;&#36133;&#30340;&#39118;&#38505;&#24182;&#36827;&#34892;&#24674;&#22797;&#65292;&#21516;&#26102;&#33021;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#20135;&#29983;&#28789;&#27963;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#26469;&#35828;&#65292;&#29615;&#22659;&#21644;&#31995;&#32479;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#23548;&#33268;&#36816;&#21160;&#35268;&#21010;&#30340;&#22833;&#36133;&#65292;&#36827;&#32780;&#36896;&#25104;&#28508;&#22312;&#30340;&#30896;&#25758;&#39118;&#38505;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#24230;&#40065;&#26834;&#30340;&#33258;&#20027;&#24615;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#24212;&#35813;&#33021;&#22815;&#20027;&#21160;&#39044;&#27979;&#21644;&#24674;&#22797;&#36825;&#31181;&#22833;&#36133;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20027;&#21160;&#26816;&#27979;&#26410;&#26469;&#36816;&#21160;&#35268;&#21010;&#22833;&#36133;&#30340;&#39118;&#38505;&#12290;&#24403;&#27492;&#39118;&#38505;&#36229;&#36807;&#19968;&#23450;&#38408;&#20540;&#26102;&#65292;&#20250;&#35302;&#21457;&#19968;&#31181;&#24674;&#22797;&#34892;&#20026;&#65292;&#21033;&#29992;&#21516;&#19968;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#23547;&#25214;&#19968;&#20010;&#23433;&#20840;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#32487;&#32493;&#26397;&#30528;&#30446;&#26631;&#21069;&#36827;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20165;&#22312;&#20223;&#30495;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#12290;&#20223;&#30495;&#21644;&#29289;&#29702;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#39044;&#27979;&#35268;&#21010;&#22120;&#22833;&#36133;&#24182;&#23558;&#26426;&#22120;&#20154;&#24674;&#22797;&#21040;&#21487;&#33021;&#25104;&#21151;&#35268;&#21010;&#30340;&#29366;&#24577;&#65292;&#21516;&#26102;&#23454;&#29616;&#28789;&#27963;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
For autonomous mobile robots, uncertainties in the environment and system model can lead to failure in the motion planning pipeline, resulting in potential collisions. In order to achieve a high level of robust autonomy, these robots should be able to proactively predict and recover from such failures. To this end, we propose a Gaussian Process (GP) based model for proactively detecting the risk of future motion planning failure. When this risk exceeds a certain threshold, a recovery behavior is triggered that leverages the same GP model to find a safe state from which the robot may continue towards the goal. The proposed approach is trained in simulation only and can generalize to real world environments on different robotic platforms. Simulations and physical experiments demonstrate that our framework is capable of both predicting planner failures and recovering the robot to states where planner success is likely, all while producing agile motion.
&lt;/p&gt;</description></item><item><title>L2G2G&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21516;&#27493;&#28508;&#22312;&#33410;&#28857;&#34920;&#31034;&#20197;&#25552;&#39640;GAE&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#35745;&#31639;&#21482;&#26377;&#26412;&#22320;&#22270;&#22359;&#25439;&#22833;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#20102;&#22270;&#30340;&#20449;&#24687;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01614</link><description>&lt;p&gt;
L2G2G: &#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#22522;&#20110;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01614
&lt;/p&gt;
&lt;p&gt;
L2G2G&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21516;&#27493;&#28508;&#22312;&#33410;&#28857;&#34920;&#31034;&#20197;&#25552;&#39640;GAE&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#35745;&#31639;&#21482;&#26377;&#26412;&#22320;&#22270;&#22359;&#25439;&#22833;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#20102;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20998;&#26512;&#23454;&#38469;&#32593;&#32476;&#65292;&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#29992;&#24037;&#20855;&#12290;&#36825;&#20123;&#26041;&#27861;&#65292;&#22914;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;(GAE)&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20302;&#32500;&#34920;&#31034;&#65292;&#20063;&#31216;&#20026;&#23884;&#20837;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#33719;&#24471;;&#36825;&#20123;&#23884;&#20837;&#19982;&#35299;&#30721;&#22120;&#19968;&#36215;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#21644;&#36793;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;GAE&#24448;&#24448;&#30456;&#24403;&#20934;&#30830;&#65292;&#20294;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#25913;&#21892;&#36895;&#24230;&#65292;Local2Global&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#21521;&#37327;&#21516;&#27493;&#30340;&#22270;&#22359;&#23884;&#20837;&#30456;&#32467;&#21512;&#65292;&#26174;&#31034;&#20986;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2G2G&#65292;&#19968;&#31181;_Local2Global&#26041;&#27861;&#65292;&#23427;&#22312;&#19981;&#29306;&#29298;&#21487;&#25193;&#23637;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;GAE&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;GAE&#26399;&#38388;&#21160;&#24577;&#21516;&#27493;&#28508;&#22312;&#33410;&#28857;&#34920;&#31034;&#26469;&#23454;&#29616;&#30340;&#12290;&#23427;&#36824;&#21463;&#30410;&#20110;&#35299;&#30721;&#22120;&#35745;&#31639;&#21482;&#26377;&#26412;&#22320;&#22270;&#22359;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010;&#26102;&#20195;&#20013;&#30340;&#26412;&#22320;&#23884;&#20837;&#23545;&#40784;&#21033;&#29992;&#20102;&#26356;&#22810;&#26469;&#33258;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
For analysing real-world networks, graph representation learning is a popular tool. These methods, such as a graph autoencoder (GAE), typically rely on low-dimensional representations, also called embeddings, which are obtained through minimising a loss function; these embeddings are used with a decoder for downstream tasks such as node classification and edge prediction. While GAEs tend to be fairly accurate, they suffer from scalability issues. For improved speed, a Local2Global approach, which combines graph patch embeddings based on eigenvector synchronisation, was shown to be fast and achieve good accuracy. Here we propose L2G2G, a Local2Global method which improves GAE accuracy without sacrificing scalability. This improvement is achieved by dynamically synchronising the latent node representations, while training the GAEs. It also benefits from the decoder computing an only local patch loss. Hence, aligning the local embeddings in each epoch utilises more information from the gr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24037;&#19994;&#24494;&#30005;&#32593;&#30340;&#39640;&#25928;&#25511;&#21046;&#26041;&#26696;&#65292;&#36890;&#36807;&#36830;&#25509;&#30340;&#30005;&#21160;&#27773;&#36710;&#32593;&#26684;&#23454;&#29616;&#20027;&#39057;&#29575;&#25511;&#21046;&#21644;&#25317;&#22622;&#31649;&#29702;&#12290;&#36825;&#39033;&#25216;&#26415;&#36890;&#36807;&#23558;&#30005;&#21160;&#27773;&#36710;&#29992;&#20316;&#36127;&#33655;&#21644;&#28304;&#65292;&#21033;&#29992;&#20854;&#24555;&#36895;&#35843;&#33410;&#33021;&#21147;&#26469;&#25552;&#39640;&#30005;&#32593;&#21487;&#38752;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01608</link><description>&lt;p&gt;
&#36830;&#25509;&#30340;&#30005;&#21160;&#27773;&#36710;&#32593;&#26684;&#30340;&#23481;&#38169;&#20998;&#26512;&#22312;&#24037;&#19994;&#24494;&#30005;&#32593;&#20013;&#20351;&#29992;&#39640;&#25928;&#25511;&#21046;&#26041;&#26696;&#36827;&#34892;&#20027;&#39057;&#29575;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Contingency Analysis of a Grid of Connected EVs for Primary Frequency Control of an Industrial Microgrid Using Efficient Control Scheme
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24037;&#19994;&#24494;&#30005;&#32593;&#30340;&#39640;&#25928;&#25511;&#21046;&#26041;&#26696;&#65292;&#36890;&#36807;&#36830;&#25509;&#30340;&#30005;&#21160;&#27773;&#36710;&#32593;&#26684;&#23454;&#29616;&#20027;&#39057;&#29575;&#25511;&#21046;&#21644;&#25317;&#22622;&#31649;&#29702;&#12290;&#36825;&#39033;&#25216;&#26415;&#36890;&#36807;&#23558;&#30005;&#21160;&#27773;&#36710;&#29992;&#20316;&#36127;&#33655;&#21644;&#28304;&#65292;&#21033;&#29992;&#20854;&#24555;&#36895;&#35843;&#33410;&#33021;&#21147;&#26469;&#25552;&#39640;&#30005;&#32593;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29123;&#27833;&#27773;&#36710;&#32479;&#27835;&#20132;&#36890;&#39046;&#22495;&#19968;&#20010;&#22810;&#19990;&#32426;&#21518;&#65292;&#30001;&#20110;&#19968;&#31995;&#21015;&#20248;&#21183;&#65292;&#21253;&#25324;&#26356;&#20302;&#36816;&#33829;&#25104;&#26412;&#21644;&#26356;&#20302;&#30340;CO2&#25490;&#25918;&#65292;&#30005;&#21160;&#27773;&#36710;&#20284;&#20046;&#21363;&#23558;&#24471;&#21040;&#25512;&#24191;&#12290;&#36890;&#36807;&#20351;&#29992;&#36710;&#36742;&#21040;&#30005;&#32593;&#65288;&#22914;&#26524;&#20351;&#29992;&#30005;&#21160;&#27773;&#36710;&#20316;&#20026;&#36127;&#33655;&#65292;&#21017;&#20026;&#30005;&#32593;&#21040;&#36710;&#36742;&#65289;&#30340;&#26041;&#27861;&#65292;&#30005;&#21160;&#27773;&#36710;&#21487;&#20197;&#21516;&#26102;&#20316;&#20026;&#36127;&#33655;&#21644;&#28304;&#36827;&#34892;&#36816;&#34892;&#12290;&#20027;&#39057;&#29575;&#35843;&#33410;&#21644;&#25317;&#22622;&#31649;&#29702;&#26159;&#35813;&#25216;&#26415;&#28155;&#21152;&#21040;&#24037;&#19994;&#24494;&#30005;&#32593;&#20013;&#30340;&#20004;&#20010;&#37325;&#35201;&#29305;&#24615;&#12290;&#24037;&#19994;&#24494;&#30005;&#32593;&#30001;&#19981;&#21516;&#30340;&#33021;&#28304;&#28304;&#65292;&#22914;&#39118;&#30005;&#22330;&#21644;&#20809;&#20239;&#30005;&#22330;&#65292;&#20648;&#33021;&#31995;&#32479;&#21644;&#36127;&#36733;&#32452;&#25104;&#12290;&#30005;&#21160;&#27773;&#36710;&#30001;&#20110;&#20854;&#24555;&#36895;&#35843;&#33410;&#33021;&#21147;&#32780;&#20316;&#20026;&#39057;&#29575;&#31649;&#29702;&#25216;&#26415;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#12290;&#30005;&#32593;&#21487;&#38752;&#24615;&#21462;&#20915;&#20110;&#36825;&#31181;&#24555;&#36895;&#21453;&#24212;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#20107;&#25925;&#65292;&#30005;&#21160;&#27773;&#36710;&#30340;&#30005;&#27744;&#29366;&#24577;&#21644;&#30005;&#21160;&#27773;&#36710;&#38431;&#20013;&#30005;&#21160;&#27773;&#36710;&#30340;&#25968;&#37327;&#30340;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20027;&#39057;&#29575;&#25511;&#21046;&#30340;&#25511;&#21046;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
After over a century of internal combustion engines ruling the transport sector, electric vehicles appear to be on the verge of gaining traction due to a slew of advantages, including lower operating costs and lower CO2 emissions. By using the Vehicle-to-Grid (or Grid-to-Vehicle if Electric vehicles (EVs) are utilized as load) approach, EVs can operate as both a load and a source. Primary frequency regulation and congestion management are two essential characteristics of this technology that are added to an industrial microgrid. Industrial Microgrids are made up of different energy sources such as wind farms and PV farms, storage systems, and loads. EVs have gained a lot of interest as a technique for frequency management because of their ability to regulate quickly. Grid reliability depends on this quick reaction. Different contingency, state of charge of the electric vehicles, and a varying number of EVs in an EV fleet are considered in this work, and a proposed control scheme for fr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01607</link><description>&lt;p&gt;
&#20855;&#26377;&#24517;&#35201;&#22238;&#28335;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
Natural Counterfactuals With Necessary Backtracking
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#25552;&#20379;&#35299;&#37322;&#21644;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;Judea Pearl&#30340;&#30740;&#31350;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24456;&#20248;&#38597;&#65292;&#20294;&#20854;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#26223;&#24448;&#24448;&#38656;&#35201;&#36807;&#20110;&#33073;&#31163;&#23454;&#38469;&#24773;&#26223;&#30340;&#24178;&#39044;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#21644;&#19968;&#31181;&#26681;&#25454;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20801;&#35768;&#23545;&#22240;&#26524;&#21069;&#32622;&#21464;&#37327;&#36827;&#34892;&#25913;&#21464;&#20197;&#26368;&#23567;&#21270;&#19982;&#23454;&#38469;&#24773;&#26223;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#24615;&#20934;&#21017;&#20801;&#35768;&#20294;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20004;&#20010;&#21313;&#24180;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#22522;&#20110;&#24615;&#21035;&#30340;&#34880;&#21387;&#21464;&#21270;&#19981;&#26174;&#33879;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#20551;&#35774;&#65307;&#21516;&#26102;&#65292;&#33298;&#24352;&#21387;&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#25910;&#32553;&#21387;&#22312;&#22235;&#21313;&#22810;&#23681;&#30340;&#24180;&#40836;&#32452;&#26174;&#31034;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#23792;&#20540;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01598</link><description>&lt;p&gt;
&#20174;&#20004;&#20010;&#21313;&#24180;&#30340;&#34880;&#21387;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#36328;&#36234;7500&#19975;&#24739;&#32773;&#23601;&#35786;&#30340;&#19981;&#21516;&#20154;&#32676;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning from Two Decades of Blood Pressure Data: Demography-Specific Patterns Across 75 Million Patient Encounters
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01598
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20004;&#20010;&#21313;&#24180;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#22522;&#20110;&#24615;&#21035;&#30340;&#34880;&#21387;&#21464;&#21270;&#19981;&#26174;&#33879;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#20551;&#35774;&#65307;&#21516;&#26102;&#65292;&#33298;&#24352;&#21387;&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#25910;&#32553;&#21387;&#22312;&#22235;&#21313;&#22810;&#23681;&#30340;&#24180;&#40836;&#32452;&#26174;&#31034;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#23792;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#34880;&#21387;&#20173;&#28982;&#26159;&#20840;&#29699;&#20851;&#27880;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#20854;&#24739;&#30149;&#29575;&#19981;&#26029;&#19978;&#21319;&#65292;&#22240;&#27492;&#38656;&#35201;&#26377;&#25928;&#30340;&#30417;&#27979;&#21644;&#29702;&#35299;&#34880;&#21387;&#21160;&#24577;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20174;&#34880;&#21387;&#27979;&#37327;&#20013;&#33719;&#24471;&#30340;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#26159;&#20102;&#35299;&#39640;&#34880;&#21387;&#36235;&#21183;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25253;&#36947;&#20102;&#34880;&#21387;&#21464;&#21270;&#19982;&#21508;&#31181;&#22240;&#32032;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20221;&#28085;&#30422;&#20102;&#20004;&#20010;&#21313;&#24180;&#30340;7500&#19975;&#35760;&#24405;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#20026;&#25506;&#32034;&#21644;&#20998;&#26512;&#19981;&#21516;&#20154;&#32676;&#29305;&#24449;&#65292;&#22914;&#24180;&#40836;&#12289;&#31181;&#26063;&#21644;&#24615;&#21035;&#20043;&#38388;&#30340;&#34880;&#21387;&#21464;&#21270;&#25552;&#20379;&#20102;&#29420;&#29305;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#24615;&#21035;&#30340;&#34880;&#21387;&#21464;&#21270;&#22312;&#32479;&#35745;&#19978;&#24182;&#19981;&#26174;&#33879;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#20551;&#35774;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#33298;&#24352;&#21387;&#65288;SBP&#65289;&#38543;&#30528;&#24180;&#40836;&#30340;&#22686;&#38271;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#33298;&#24352;&#21387;&#65288;DBP&#65289;&#22312;&#22235;&#21313;&#22810;&#23681;&#30340;&#24180;&#40836;&#32452;&#26174;&#31034;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#23792;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#20998;&#24067;&#27169;&#24335;&#20013;&#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypertension remains a global health concern with a rising prevalence, necessitating effective monitoring and understanding of blood pressure (BP) dynamics. This study delves into the wealth of information derived from BP measurement, a crucial approach in informing our understanding of hypertensive trends. Numerous studies have reported on the relationship between BP variation and various factors. In this research, we leveraged an extensive dataset comprising 75 million records spanning two decades, offering a unique opportunity to explore and analyze BP variations across demographic features such as age, race, and gender. Our findings revealed that gender-based BP variation was not statistically significant, challenging conventional assumptions. Interestingly, systolic blood pressure (SBP) consistently increased with age, while diastolic blood pressure (DBP) displayed a distinctive peak in the forties age group. Moreover, our analysis uncovered intriguing similarities in the distribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#38899;&#39057;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#31232;&#30095;&#29366;&#24577;&#19979;&#20855;&#26377;&#36739;&#39640;&#25928;&#29575;&#21644;&#33258;&#20027;&#20135;&#29983;&#30340;&#36873;&#25321;&#24615;&#21644;&#21516;&#27493;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01571</link><description>&lt;p&gt;
Spiking Music: &#22522;&#20110;&#20107;&#20214;&#30340;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#38899;&#39057;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Spiking Music: Audio Compression with Event Based Auto-encoders
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#38899;&#39057;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#31232;&#30095;&#29366;&#24577;&#19979;&#20855;&#26377;&#36739;&#39640;&#25928;&#29575;&#21644;&#33258;&#20027;&#20135;&#29983;&#30340;&#36873;&#25321;&#24615;&#21644;&#21516;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#20013;&#30340;&#31070;&#32463;&#20803;&#36890;&#36807;&#31216;&#20026;&#33033;&#20914;&#30340;&#28857;&#26102;&#38388;&#20107;&#20214;&#20256;&#36882;&#20449;&#24687;&#12290;&#33033;&#20914;&#30340;&#26102;&#24207;&#34987;&#35748;&#20026;&#21253;&#21547;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20294;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#22312;&#25968;&#23383;&#31995;&#32479;&#20013;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#32534;&#30721;&#23545;&#38899;&#39057;&#21387;&#32553;&#26159;&#26377;&#25928;&#30340;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#28145;&#24230;&#20108;&#36827;&#21046;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#39640;&#31232;&#30095;&#32422;&#26463;&#19979;&#65292;&#27169;&#22411;&#36827;&#20837;&#19968;&#31181;&#31232;&#30095;&#30697;&#38453;&#23384;&#20648;&#31639;&#27861;&#26356;&#39640;&#25928;&#23384;&#20648;&#20108;&#36827;&#21046;&#20107;&#20214;&#30697;&#38453;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#22823;&#22411;MAESTRO&#38050;&#29748;&#24405;&#38899;&#25968;&#25454;&#38598;&#19978;&#23558;&#20854;&#19982;&#30690;&#37327;&#37327;&#21270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#8220;Spiking Music&#21387;&#32553;&#8221;&#31639;&#27861;&#19981;&#20165;&#22312;&#21387;&#32553;/&#37325;&#26500;&#30340;&#26435;&#34913;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#32780;&#19988;&#22312;&#31232;&#30095;&#29366;&#24577;&#19979;&#65292;&#22312;&#32534;&#30721;&#30340;&#20107;&#20214;&#21644;&#38050;&#29748;&#25353;&#38190;&#20987;&#25171;&#20043;&#38388;&#20986;&#29616;&#20102;&#33258;&#20027;&#20135;&#29983;&#30340;&#36873;&#25321;&#24615;&#21644;&#21516;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurons in the brain communicate information via punctual events called spikes. The timing of spikes is thought to carry rich information, but it is not clear how to leverage this in digital systems. We demonstrate that event-based encoding is efficient for audio compression. To build this event-based representation we use a deep binary auto-encoder, and under high sparsity pressure, the model enters a regime where the binary event matrix is stored more efficiently with sparse matrix storage algorithms. We test this on the large MAESTRO dataset of piano recordings against vector quantized auto-encoders. Not only does our "Spiking Music compression" algorithm achieve a competitive compression/reconstruction trade-off, but selectivity and synchrony between encoded events and piano key strikes emerge without supervision in the sparse regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;Adam&#20248;&#21270;&#22120;&#30340;&#26500;&#25104;&#21644;&#31639;&#27861;&#32452;&#25104;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;Adam&#23454;&#38469;&#19978;&#26159;&#20266;&#35013;&#25104;FTRL&#30340;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#20854;&#31639;&#27861;&#32452;&#25104;&#30340;&#22909;&#22788;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01567</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#26356;&#26032;&#29702;&#35299;Adam&#20248;&#21270;&#22120;&#65306;Adam&#26159;&#20266;&#35013;&#25104;FTRL&#30340;
&lt;/p&gt;
&lt;p&gt;
Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;Adam&#20248;&#21270;&#22120;&#30340;&#26500;&#25104;&#21644;&#31639;&#27861;&#32452;&#25104;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;Adam&#23454;&#38469;&#19978;&#26159;&#20266;&#35013;&#25104;FTRL&#30340;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#20854;&#31639;&#27861;&#32452;&#25104;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Adam&#20248;&#21270;&#22120;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#31639;&#27861;&#32452;&#25104;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23545;Adam&#30340;&#20998;&#26512;&#20165;&#26174;&#31034;&#20102;&#21487;&#20197;&#31616;&#21333;&#22320;&#36890;&#36807;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#22914;SGD&#23454;&#29616;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#19981;&#21516;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102;Adam&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#21463;Cutkosky&#31561;&#20154;&#65288;2023&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#31216;&#20026;&#22312;&#32447;&#23398;&#20064;&#26356;&#26032;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#26681;&#25454;&#22312;&#32447;&#23398;&#20064;&#32773;&#36873;&#25321;&#20248;&#21270;&#22120;&#30340;&#26356;&#26032;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#35774;&#35745;&#19968;&#20010;&#22909;&#30340;&#20248;&#21270;&#22120;&#23601;&#31561;&#21516;&#20110;&#35774;&#35745;&#19968;&#20010;&#22909;&#30340;&#22312;&#32447;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;Adam&#23545;&#24212;&#20110;&#19968;&#31181;&#34987;&#31216;&#20026;Follow-the-Regularized-Leader (FTRL)&#30340;&#21407;&#21017;&#24615;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20854;&#31639;&#27861;&#32452;&#25104;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited. In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD. In this work, we provide a different perspective based on online learning that underscores the importance of Adam's algorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates, where we choose the updates of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20303;&#23429;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23433;&#20840;&#32858;&#21512;&#31639;&#27861;&#21644;&#22810;&#26041;&#35745;&#31639;&#23494;&#30721;&#23398;&#25216;&#26415;&#26469;&#20943;&#36731;&#26799;&#24230;&#27844;&#28431;&#30340;&#39118;&#38505;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21644;&#23545;&#26032;&#20852;&#25915;&#20987;&#25216;&#26415;&#30340;&#23481;&#26131;&#21463;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01546</link><description>&lt;p&gt;
&#38754;&#21521;&#20303;&#23429;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Distributed Learning for Residential Short-Term Load Forecasting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01546
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20303;&#23429;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23433;&#20840;&#32858;&#21512;&#31639;&#27861;&#21644;&#22810;&#26041;&#35745;&#31639;&#23494;&#30721;&#23398;&#25216;&#26415;&#26469;&#20943;&#36731;&#26799;&#24230;&#27844;&#28431;&#30340;&#39118;&#38505;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21644;&#23545;&#26032;&#20852;&#25915;&#20987;&#25216;&#26415;&#30340;&#23481;&#26131;&#21463;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21147;&#31995;&#32479;&#39046;&#22495;&#65292;&#20303;&#23429;&#29992;&#25143;&#22312;&#36127;&#33655;&#39044;&#27979;&#24212;&#29992;&#20013;&#30340;&#26085;&#30410;&#22686;&#21152;&#20351;&#24471;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#26085;&#36235;&#20851;&#27880;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36127;&#33655;&#25968;&#25454;&#21487;&#33021;&#24847;&#22806;&#22320;&#27844;&#38706;&#20303;&#23429;&#29992;&#25143;&#30340;&#26085;&#24120;&#29983;&#27963;&#20064;&#24815;&#65292;&#20174;&#32780;&#23545;&#20854;&#36130;&#20135;&#23433;&#20840;&#26500;&#25104;&#39118;&#38505;&#12290;&#34429;&#28982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#34987;&#29992;&#26469;&#36890;&#36807;&#22312;&#19981;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#20294;&#36825;&#20123;FL&#27169;&#22411;&#23545;&#26032;&#20852;&#30340;&#25915;&#20987;&#25216;&#26415;&#65288;&#22914;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#21644;&#27602;&#21270;&#25915;&#20987;&#65289;&#26174;&#31034;&#20986;&#20102;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#25269;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#20102;&#19968;&#31181;&#23433;&#20840;&#32858;&#21512;&#65288;SecAgg&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#22810;&#26041;&#35745;&#31639;&#23494;&#30721;&#23398;&#25216;&#26415;&#26469;&#20943;&#36731;&#26799;&#24230;&#27844;&#28431;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;SecAgg&#30340;&#24341;&#20837;&#38656;&#35201;&#37096;&#32626;&#39069;&#22806;&#30340;&#23376;&#20013;&#24515;&#26381;&#21153;&#22120;&#26469;&#25191;&#34892;&#22810;&#26041;&#35745;&#31639;&#21327;&#35758;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#38477;&#20302;&#20102;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of power systems, the increasing involvement of residential users in load forecasting applications has heightened concerns about data privacy. Specifically, the load data can inadvertently reveal the daily routines of residential users, thereby posing a risk to their property security. While federated learning (FL) has been employed to safeguard user privacy by enabling model training without the exchange of raw data, these FL models have shown vulnerabilities to emerging attack techniques, such as Deep Leakage from Gradients and poisoning attacks. To counteract these, we initially employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty computation cryptographic techniques to mitigate the risk of gradient leakage. However, the introduction of SecAgg necessitates the deployment of additional sub-center servers for executing the multiparty computation protocol, thereby escalating computational complexity and reducing system robustness, especially in scenario
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#36866;&#24212;&#35266;&#27979;&#29305;&#24449;&#38598;&#65292;&#24182;&#23558;&#22635;&#20805;&#35268;&#21017;&#21644;&#22238;&#24402;&#27169;&#22411;&#21516;&#26102;&#23398;&#20064;&#65292;&#30456;&#27604;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#38750;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#24773;&#20917;&#19979;&#65292;&#26041;&#27861;&#23454;&#29616;&#20102;2-10%&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01543</link><description>&lt;p&gt;
&#38024;&#23545;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Optimization for Prediction with Missing Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#36866;&#24212;&#35266;&#27979;&#29305;&#24449;&#38598;&#65292;&#24182;&#23558;&#22635;&#20805;&#35268;&#21017;&#21644;&#22238;&#24402;&#27169;&#22411;&#21516;&#26102;&#23398;&#20064;&#65292;&#30456;&#27604;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#38750;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#24773;&#20917;&#19979;&#65292;&#26041;&#27861;&#23454;&#29616;&#20102;2-10%&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20855;&#26377;&#32570;&#22833;&#26465;&#30446;&#30340;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#26368;&#24120;&#29992;&#21644;&#22810;&#21151;&#33021;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#27700;&#32447;&#25216;&#26415;&#65292;&#39318;&#20808;&#22635;&#20805;&#32570;&#22833;&#26465;&#30446;&#65292;&#28982;&#21518;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#12290;&#26412;&#25991;&#23558;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;&#35270;&#20026;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#20854;&#20013;&#22238;&#24402;&#31995;&#25968;&#33021;&#22815;&#36866;&#24212;&#35266;&#27979;&#29305;&#24449;&#38598;&#12290;&#25105;&#20204;&#34920;&#26126;&#19968;&#20123;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#31561;&#21516;&#20110;&#21516;&#26102;&#23398;&#20064;&#22635;&#20805;&#35268;&#21017;&#21644;&#19979;&#28216;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#32780;&#19981;&#26159;&#39034;&#24207;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32852;&#21512;&#22635;&#20805;-&#22238;&#24402;&#30340;&#35299;&#37322;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#22312;&#25968;&#25454;&#38750;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#22806;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;2-10%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01542</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#30340;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning Collective Variables for Protein Folding with Labeled Data Augmentation through Geodesic Interpolation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#26469;&#30740;&#31350;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#32597;&#35265;&#20107;&#20214;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#27839;&#30528;&#21152;&#36895;&#21457;&#29983;&#30340;&#38598;&#20307;&#21464;&#37327;&#65288;CV&#65289;&#30340;&#23450;&#20041;&#12290;&#33719;&#24471;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;CV&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#20851;&#20110;&#29305;&#23450;&#20107;&#20214;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#38459;&#30861;&#65292;&#20363;&#22914;&#20174;&#26410;&#25240;&#21472;&#21040;&#25240;&#21472;&#26500;&#35937;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#26080;&#20851;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#24230;&#37327;&#26469;&#29983;&#25104;&#31867;&#20284;&#34507;&#30333;&#36136;&#25240;&#21472;&#36716;&#21464;&#30340;&#27979;&#22320;&#25554;&#20540;&#65292;&#20174;&#32780;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#30340;&#36807;&#28193;&#24577;&#26679;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#25554;&#20540;&#36827;&#24230;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22238;&#24402;&#30340;&#23398;&#20064;&#26041;&#26696;&#26469;&#26500;&#24314;CV&#27169;&#22411;&#65292;&#24403;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In molecular dynamics (MD) simulations, rare events, such as protein folding, are typically studied by means of enhanced sampling techniques, most of which rely on the definition of a collective variable (CV) along which the acceleration occurs. Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation. We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples. Leveraging interpolation progress parameters, we introduce a regression-based learning scheme for CV models, which outperforms classifier-based methods when transition state data is limited and noisy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#36890;&#36807;&#32508;&#21512;RGB&#12289;&#28909;&#20687;&#21644;&#28145;&#24230;&#19977;&#20010;&#27169;&#24577;&#30340;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;HBA&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#25972;&#21512;&#22810;&#27169;&#24577;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01537</link><description>&lt;p&gt;
&#32553;&#23567;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#30340;&#24046;&#36317;&#65306;&#19968;&#31181;&#32508;&#21512;&#19977;&#27169;&#24577;&#25968;&#25454;&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing Trimodal Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#36890;&#36807;&#32508;&#21512;RGB&#12289;&#28909;&#20687;&#21644;&#28145;&#24230;&#19977;&#20010;&#27169;&#24577;&#30340;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;HBA&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#25972;&#21512;&#22810;&#27169;&#24577;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26222;&#36866;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#65288;HBA&#65289;&#39046;&#22495;&#65292;RGB&#19968;&#30452;&#26159;&#39318;&#36873;&#30340;&#27169;&#24577;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#33719;&#21462;&#24182;&#19988;&#20449;&#24687;&#20016;&#23500;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20248;&#28857;&#30456;&#36830;&#30340;&#26159;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#20809;&#29031;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#28431;&#27934;&#30340;&#19968;&#20010;&#21487;&#33021;&#24615;&#26159;&#21033;&#29992;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#20363;&#22914;&#65292;&#28909;&#20687;&#32032;&#33021;&#22815;&#31361;&#20986;&#20154;&#20307;&#24418;&#24577;&#65292;&#32780;&#28145;&#24230;&#27169;&#24577;&#21017;&#22686;&#21152;&#20102;&#37325;&#35201;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#24577;&#24102;&#26469;&#20102;&#30456;&#20851;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#24456;&#23569;&#19968;&#37096;&#20998;&#29305;&#23450;&#20110;HBA&#30340;&#25968;&#25454;&#38598;&#38598;&#25104;&#20102;&#36825;&#20123;&#27169;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#21019;&#24314;&#19977;&#27169;&#24577;&#65288;&#21363;RGB&#12289;&#28909;&#20687;&#21644;&#28145;&#24230;&#65289;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25216;&#26415;&#21033;&#29992;&#20174;RGB&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#20154;&#20307;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#19982;&#33258;&#21160;&#33719;&#21462;&#30340;&#28909;&#20687;&#21644;&#28145;&#24230;&#32972;&#26223;&#32467;&#21512;&#12290;&#26377;&#20102;&#36825;&#20004;&#20010;&#35201;&#32032;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#29616;&#26377;&#30340;RGB&#25968;&#25454;&#21512;&#25104;&#28145;&#24230;&#21644;&#28909;&#20687;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
In pervasive machine learning, especially in Human Behavior Analysis (HBA), RGB has been the primary modality due to its accessibility and richness of information. However, linked with its benefits are challenges, including sensitivity to lighting conditions and privacy concerns. One possibility to overcome these vulnerabilities is to resort to different modalities. For instance, thermal is particularly adept at accentuating human forms, while depth adds crucial contextual layers. Despite their known benefits, only a few HBA-specific datasets that integrate these modalities exist. To address this shortage, our research introduces a novel generative technique for creating trimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique capitalizes on human segmentation masks derived from RGB images, combined with thermal and depth backgrounds that are sourced automatically. With these two ingredients, we synthesize depth and thermal counterparts from existing RGB data uti
&lt;/p&gt;</description></item><item><title>&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01528</link><description>&lt;p&gt;
&#35299;&#30721;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01528
&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#32467;&#26524;&#12290;&#22312;&#23545;LLM&#36827;&#34892;&#25512;&#26029;&#26102;&#65292;&#25512;&#27979;&#35299;&#30721;&#20351;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#25512;&#27979;&#20196;&#29260;&#65292;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;LLM&#39564;&#35777;&#36825;&#20123;&#33609;&#31295;&#20196;&#29260;&#12290;&#25512;&#27979;&#35299;&#30721;&#25552;&#20379;&#30340;&#21152;&#36895;&#21462;&#20915;&#20110;&#33609;&#31295;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#26222;&#36941;&#24314;&#35758;&#36873;&#25321;&#19968;&#20010;&#33609;&#31295;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;LLM&#25509;&#21463;&#30340;&#27010;&#29575;&#24456;&#39640;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#20043;&#30456;&#21453;&#65292;&#38543;&#30528;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#21534;&#21520;&#37327;&#20943;&#23569;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23545;&#24433;&#21709;&#25512;&#27979;&#35299;&#30721;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#21644;&#24433;&#21709;&#21152;&#36895;&#25928;&#26524;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#35813;&#27169;&#22411;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#25552;&#39640;&#25512;&#27979;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#29983;&#25104;&#36890;&#29992;&#26435;&#37325;&#30340;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20174;&#23569;&#37327;&#22270;&#20687;&#20013;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#19977;&#32500;&#29289;&#20307;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01524</link><description>&lt;p&gt;
&#36229;&#24179;&#38754;&#65306;&#24555;&#36895; NeRF &#33258;&#36866;&#24212;&#30340;&#36229;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#29983;&#25104;&#36890;&#29992;&#26435;&#37325;&#30340;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20174;&#23569;&#37327;&#22270;&#20687;&#20013;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#19977;&#32500;&#29289;&#20307;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;Neural radiance fields&#65292;NeRF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#65292;&#29992;&#20110;&#20174;&#23569;&#37327;&#22522;&#30784;&#22270;&#20687;&#21512;&#25104;&#26032;&#30340;&#19977;&#32500;&#29289;&#20307;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;NeRF &#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#35757;&#32451;&#27599;&#20010;&#35201;&#34920;&#31034;&#30340;&#29289;&#20307;&#30340;&#29420;&#31435;&#26550;&#26500;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#33539;&#24335;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#26799;&#24230;&#20248;&#21270;&#12290;&#36229;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25910;&#38598;&#20449;&#24687;&#24182;&#20026;&#36890;&#29992;&#26435;&#37325;&#29983;&#25104;&#26356;&#26032;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#27493;&#39588;&#20174;&#23569;&#37327;&#22270;&#20687;&#29983;&#25104;&#39640;&#36136;&#37327;&#19977;&#32500;&#29289;&#20307;&#34920;&#31034;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#21644;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images. However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent. To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference. The hypernetwork gathers information from the training data and generates an update for universal weights. As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step. This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#36328;&#39046;&#22495;&#27468;&#22768;&#21512;&#25104;&#27169;&#22411;Karaoker-SSL&#65292;&#36890;&#36807;&#20943;&#23569;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#21644;&#24341;&#20837;Conformer&#27169;&#22359;&#26469;&#23454;&#29616;&#26080;&#38656;&#20351;&#29992;&#27468;&#21809;&#25968;&#25454;&#21644;&#25163;&#24037;&#29305;&#24449;&#30340;&#21512;&#25104;&#36807;&#31243;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01520</link><description>&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#26469;&#23454;&#29616;&#20302;&#36164;&#28304;&#36328;&#39046;&#22495;&#27468;&#22768;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Cross-Domain Singing Voice Synthesis via Reduced Self-Supervised Speech Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01520
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#36328;&#39046;&#22495;&#27468;&#22768;&#21512;&#25104;&#27169;&#22411;Karaoker-SSL&#65292;&#36890;&#36807;&#20943;&#23569;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#21644;&#24341;&#20837;Conformer&#27169;&#22359;&#26469;&#23454;&#29616;&#26080;&#38656;&#20351;&#29992;&#27468;&#21809;&#25968;&#25454;&#21644;&#25163;&#24037;&#29305;&#24449;&#30340;&#21512;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Karaoker-SSL&#30340;&#27468;&#22768;&#21512;&#25104;&#27169;&#22411;&#65292;&#23427;&#21482;&#36890;&#36807;&#25991;&#26412;&#21644;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20316;&#20026;&#19968;&#20010;&#20856;&#22411;&#30340;&#22810;&#20154;&#35821;&#38899;&#27169;&#22411;&#12290;&#36825;&#26159;&#19968;&#20010;&#20302;&#36164;&#28304;&#30340;&#27969;&#31243;&#65292;&#19981;&#38656;&#35201;&#31471;&#21040;&#31471;&#22320;&#20351;&#29992;&#20219;&#20309;&#27468;&#21809;&#25968;&#25454;&#65292;&#22240;&#20026;&#20854;&#22768;&#30721;&#22120;&#20063;&#26159;&#22312;&#35821;&#38899;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;Karaoker-SSL&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#36890;&#36807;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#36827;&#34892;&#26465;&#20214;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#32500;&#24230;&#30340;&#23376;&#38598;&#26469;&#39044;&#22788;&#29702;&#36825;&#20123;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26465;&#20214;&#27169;&#22359;&#38388;&#25509;&#22320;&#36890;&#36807;&#22810;&#20219;&#21153;&#35774;&#32622;&#26469;&#25351;&#23548;&#25429;&#25417;&#39118;&#26684;&#20449;&#24687;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;Conformer&#30340;&#27169;&#22359;&#23454;&#29616;&#30340;&#65292;&#35813;&#27169;&#22359;&#20174;&#22768;&#23398;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#39044;&#27979;&#38899;&#39640;&#12290;&#22240;&#27492;&#65292;Karaoker-SSL&#20801;&#35768;&#36827;&#34892;&#27468;&#22768;&#21512;&#25104;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;&#20063;&#19981;&#38656;&#35201;&#25991;&#26412;&#23545;&#40784;&#25110;&#27468;&#35789;&#26102;&#38388;&#25139;&#12290;&#20026;&#20102;&#25913;&#21892;&#22768;&#38899;&#36136;&#37327;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20197;&#30446;&#26631;&#35828;&#35805;&#32773;&#20026;&#26465;&#20214;&#30340;U-Net&#37492;&#21035;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a singing voice synthesis model, Karaoker-SSL, that is trained only on text and speech data as a typical multi-speaker acoustic model. It is a low-resource pipeline that does not utilize any singing data end-to-end, since its vocoder is also trained on speech data. Karaoker-SSL is conditioned by self-supervised speech representations in an unsupervised manner. We preprocess these representations by selecting only a subset of their task-correlated dimensions. The conditioning module is indirectly guided to capture style information during training by multi-tasking. This is achieved with a Conformer-based module, which predicts the pitch from the acoustic model's output. Thus, Karaoker-SSL allows singing voice synthesis without reliance on hand-crafted and domain-specific features. There are also no requirements for text alignments or lyrics timestamps. To refine the voice quality, we employ a U-Net discriminator that is conditioned on the target speaker and fol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01515</link><description>&lt;p&gt;
&#25552;&#21319;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65306;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#21644;&#29992;&#20110;&#26356;&#24555;&#25910;&#25947;&#30340;&#26032;&#22411;&#21152;&#36895;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Stochastic Gradient Descent: A Unified Framework and Novel Acceleration Methods for Faster Convergence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SGD&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#25913;&#36827;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#20363;&#22914;SGDm&#65292;AdaGrad&#65292;Adam&#31561;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#65292;&#23427;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;&#20219;&#20309;&#19968;&#38454;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#26356;&#26032;&#26041;&#21521;$g_t$&#35299;&#37322;&#20026;&#38543;&#26426;&#27425;&#26799;&#24230;$\nabla f_t(x_t)$&#21644;&#38468;&#21152;&#30340;&#21152;&#36895;&#39033;$\frac{2|\langle v_t, \nabla f_t(x_t) \rangle|}{\|v_t\|_2^2} v_t$&#30340;&#21644;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;$\langle v_t, \nabla f_t(x_t) \rangle$&#26469;&#35752;&#35770;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21152;&#36895;&#26041;&#27861;&#65306;\textbf{&#25298;&#32477;&#21152;&#36895;}&#21644;\textbf{&#38543;&#26426;&#21521;&#37327;&#21152;&#36895;}&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on SGD, previous works have proposed many algorithms that have improved convergence speed and generalization in stochastic optimization, such as SGDm, AdaGrad, Adam, etc. However, their convergence analysis under non-convex conditions is challenging. In this work, we propose a unified framework to address this issue. For any first-order methods, we interpret the updated direction $g_t$ as the sum of the stochastic subgradient $\nabla f_t(x_t)$ and an additional acceleration term $\frac{2|\langle v_t, \nabla f_t(x_t) \rangle|}{\|v_t\|_2^2} v_t$, thus we can discuss the convergence by analyzing $\langle v_t, \nabla f_t(x_t) \rangle$. Through our framework, we have discovered two plug-and-play acceleration methods: \textbf{Reject Accelerating} and \textbf{Random Vector Accelerating}, we theoretically demonstrate that these two methods can directly lead to an improvement in convergence rate.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRESTO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26144;&#23556;&#20381;&#36182;&#20110;&#28508;&#22312;&#34920;&#31034;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#20803;&#23431;&#23449;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25345;&#32493;&#21516;&#35843;&#26469;&#27979;&#37327;&#28508;&#22312;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#32479;&#35745;&#25512;&#29702;&#23427;&#20204;&#30340;&#20998;&#24067;&#12290;&#21487;&#20197;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#26816;&#27979;&#24322;&#24120;&#23884;&#20837;&#21644;&#39640;&#25928;&#23548;&#33322;&#36229;&#21442;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01514</link><description>&lt;p&gt;
&#26144;&#23556;&#28508;&#22312;&#34920;&#31034;&#30340;&#22810;&#20803;&#23431;&#23449;
&lt;/p&gt;
&lt;p&gt;
Mapping the Multiverse of Latent Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRESTO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26144;&#23556;&#20381;&#36182;&#20110;&#28508;&#22312;&#34920;&#31034;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#20803;&#23431;&#23449;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25345;&#32493;&#21516;&#35843;&#26469;&#27979;&#37327;&#28508;&#22312;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#32479;&#35745;&#25512;&#29702;&#23427;&#20204;&#30340;&#20998;&#24067;&#12290;&#21487;&#20197;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#26816;&#27979;&#24322;&#24120;&#23884;&#20837;&#21644;&#39640;&#25928;&#23548;&#33322;&#36229;&#21442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21709;&#24212;&#26368;&#36817;&#23545;&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#26469;&#24212;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#38382;&#39064;&#30340;&#21628;&#21505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PRESTO&#65292;&#19968;&#31181;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26144;&#23556;&#20381;&#36182;&#20110;&#28508;&#22312;&#34920;&#31034;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#20803;&#23431;&#23449;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#23545;&#23427;&#20204;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#20173;&#28982;&#19981;&#34987;&#20805;&#20998;&#29702;&#35299;&#65292;&#23548;&#33268;&#20102;&#19981;&#24517;&#35201;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#21487;&#38752;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#25345;&#32493;&#21516;&#35843;&#26469;&#34920;&#24449;&#19981;&#21516;&#32452;&#21512;&#30340;&#22810;&#26679;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;(&#36229;)&#21442;&#25968;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#25152;&#20135;&#29983;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#27979;&#37327;&#23427;&#20204;&#20043;&#38388;&#30340;&#25104;&#23545;(&#38750;)&#30456;&#20284;&#24615;&#24182;&#23545;&#20854;&#20998;&#24067;&#36827;&#34892;&#32479;&#35745;&#25512;&#29702;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#20445;&#25345;&#20102;&#28508;&#22312;&#34920;&#31034;&#38598;&#21512;&#30340;&#29702;&#24819;&#29305;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#26816;&#27979;&#24322;&#24120;&#23884;&#20837;&#25110;&#39640;&#25928;&#26377;&#25928;&#22320;&#23548;&#33322;&#36229;&#21442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present PRESTO, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations. Although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations. Our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions. As we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperpa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#33041;&#32959;&#30244;&#30340;&#20462;&#34917;&#65292;&#36890;&#36807;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#35299;&#20915;&#20102;&#24120;&#35268;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#38024;&#23545;MRI&#30340;&#38656;&#27714;&#36827;&#34892;&#20102;&#30456;&#20851;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#20462;&#34917;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01509</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#25512;&#36827;&#33041;&#32959;&#30244;&#20462;&#34917;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Advancing Brain Tumor Inpainting with Generative Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#33041;&#32959;&#30244;&#30340;&#20462;&#34917;&#65292;&#36890;&#36807;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#35299;&#20915;&#20102;&#24120;&#35268;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#38024;&#23545;MRI&#30340;&#38656;&#27714;&#36827;&#34892;&#20102;&#30456;&#20851;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#20462;&#34917;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30149;&#24577;&#33041;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#26159;&#35299;&#20915;&#24120;&#35268;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#30340;&#28508;&#22312;&#26041;&#26696;&#12290;&#20363;&#22914;&#65292;&#32452;&#32455;&#20998;&#21106;&#21644;&#33041;&#25552;&#21462;&#31639;&#27861;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#30149;&#24577;&#22270;&#20687;&#12290;&#25105;&#20204;&#23558;&#27492;&#35270;&#20026;3D&#20462;&#34917;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;2D&#20462;&#34917;&#26041;&#27861;&#25913;&#36827;&#20197;&#28385;&#36275;3D&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#25968;&#25454;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#21253;&#25324;&#38024;&#23545;MRI&#29305;&#23450;&#38656;&#27714;&#30340;&#28508;&#22312;&#20462;&#25913;&#65292;&#24182;&#20351;&#29992;BraTS2023&#20462;&#34917;&#25968;&#25454;&#38598;&#23545;&#22810;&#31181;&#20462;&#34917;&#25216;&#26415;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#21151;&#25928;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing healthy brain scans from diseased brain scans offers a potential solution to address the limitations of general-purpose algorithms, such as tissue segmentation and brain extraction algorithms, which may not effectively handle diseased images. We consider this a 3D inpainting task and investigate the adaptation of 2D inpainting methods to meet the requirements of 3D magnetic resonance imaging(MRI) data. Our contributions encompass potential modifications tailored to MRI-specific needs, and we conducted evaluations of multiple inpainting techniques using the BraTS2023 Inpainting datasets to assess their efficacy and limitations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26641;&#38598;&#25104;&#35299;&#37322;&#20026;&#33258;&#36866;&#24212;&#30340;&#33258;&#27491;&#21017;&#21270;&#24179;&#28369;&#22120;&#65292;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#30340;&#24179;&#28369;&#31243;&#24230;&#24182;&#26681;&#25454;&#27979;&#35797;&#21644;&#35757;&#32451;&#36755;&#20837;&#30340;&#24046;&#24322;&#35843;&#33410;&#24179;&#28369;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#26641;&#38598;&#25104;&#25104;&#21151;&#39537;&#21160;&#22240;&#32032;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01502</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#38543;&#26426;&#26862;&#26519;&#26377;&#25928;&#65311;&#23558;&#26641;&#38598;&#25104;&#35299;&#37322;&#20026;&#33258;&#36866;&#24212;&#30340;&#33258;&#27491;&#21017;&#21270;&#24179;&#28369;&#22120;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26641;&#38598;&#25104;&#35299;&#37322;&#20026;&#33258;&#36866;&#24212;&#30340;&#33258;&#27491;&#21017;&#21270;&#24179;&#28369;&#22120;&#65292;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#30340;&#24179;&#28369;&#31243;&#24230;&#24182;&#26681;&#25454;&#27979;&#35797;&#21644;&#35757;&#32451;&#36755;&#20837;&#30340;&#24046;&#24322;&#35843;&#33410;&#24179;&#28369;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#26641;&#38598;&#25104;&#25104;&#21151;&#39537;&#21160;&#22240;&#32032;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26641;&#38598;&#25104;&#22312;&#25928;&#26524;&#21644;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#25104;&#21151;&#30340;&#39537;&#21160;&#22240;&#32032;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#23558;&#26641;&#38598;&#25104;&#35299;&#37322;&#20026;&#33258;&#36866;&#24212;&#30340;&#33258;&#27491;&#21017;&#21270;&#24179;&#28369;&#22120;&#21487;&#20197;&#25552;&#20379;&#26032;&#30340;&#30452;&#35273;&#21644;&#26356;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#35266;&#28857;&#26469;&#23637;&#31034;&#65292;&#24403;&#23558;&#38543;&#26426;&#21270;&#26641;&#38598;&#25104;&#35270;&#20026;&#24179;&#28369;&#22120;&#26102;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#19981;&#20165;&#27604;&#23427;&#20204;&#25152;&#21253;&#21547;&#30340;&#21333;&#20010;&#26641;&#30340;&#39044;&#27979;&#26356;&#21152;&#24179;&#28369;&#65292;&#32780;&#19988;&#36824;&#26681;&#25454;&#27979;&#35797;&#21644;&#35757;&#32451;&#36755;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#22312;&#27979;&#35797;&#26102;&#36827;&#19968;&#27493;&#35843;&#33410;&#23427;&#20204;&#30340;&#24179;&#28369;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27934;&#23519;&#21147;&#37325;&#26032;&#23457;&#35270;&#12289;&#31934;&#28860;&#21644;&#21327;&#35843;&#20102;&#26368;&#36817;&#20004;&#20010;&#23545;&#26862;&#26519;&#25104;&#21151;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#27979;&#37327;&#25152;&#26263;&#31034;&#30340;&#24179;&#28369;&#31243;&#24230;&#26469;&#23458;&#35266;&#22320;&#37327;&#21270;&#26641;&#38598;&#25104;&#30340;&#29468;&#24819;&#34892;&#20026;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#20851;&#20110;&#26641;&#38598;&#25104;&#25913;&#36827;&#26426;&#21046;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Despite their remarkable effectiveness and broad application, the drivers of success underlying ensembles of trees are still not fully understood. In this paper, we highlight how interpreting tree ensembles as adaptive and self-regularizing smoothers can provide new intuition and deeper insight to this topic. We use this perspective to show that, when studied as smoothers, randomized tree ensembles not only make predictions that are quantifiably more smooth than the predictions of the individual trees they consist of, but also further regulate their smoothness at test-time based on the dissimilarity between testing and training inputs. First, we use this insight to revisit, refine and reconcile two recent explanations of forest success by providing a new way of quantifying the conjectured behaviors of tree ensembles objectively by measuring the effective degree of smoothing they imply. Then, we move beyond existing explanations for the mechanisms by which tree ensembles improve upon in
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#20351;&#29992;&#29699;&#35856;&#20989;&#25968;&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#26469;&#36817;&#20284;&#35745;&#31639;&#20999;&#29255;&#21326;&#29791;&#26031;&#22374;&#36317;&#31163;&#12290;&#19982;&#33945;&#29305;&#21345;&#32599;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#29702;&#35770;&#24615;&#36136;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01493</link><description>&lt;p&gt;
&#20351;&#29992;&#29699;&#35856;&#20989;&#25968;&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#30340;&#20999;&#29255;&#21326;&#29791;&#26031;&#22374;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01493
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#20351;&#29992;&#29699;&#35856;&#20989;&#25968;&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#26469;&#36817;&#20284;&#35745;&#31639;&#20999;&#29255;&#21326;&#29791;&#26031;&#22374;&#36317;&#31163;&#12290;&#19982;&#33945;&#29305;&#21345;&#32599;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#29702;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;&#21326;&#29791;&#26031;&#22374;&#65288;SW&#65289;&#36317;&#31163;&#26159;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#21326;&#29791;&#26031;&#22374;&#36317;&#31163;&#30340;&#24179;&#22343;&#20540;&#65292;&#32467;&#26524;&#20026;&#30456;&#20851;&#30340;&#19968;&#32500;&#25237;&#24433;&#30340;&#21326;&#29791;&#26031;&#22374;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;SW&#36317;&#31163;&#21487;&#20197;&#20889;&#25104;&#23545;&#29699;&#38754;&#19978;&#22343;&#21248;&#27979;&#24230;&#30340;&#31215;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26694;&#26550;&#26469;&#35745;&#31639;SW&#36317;&#31163;&#12290;&#29699;&#35856;&#20989;&#25968;&#26159;&#29699;&#38754;&#19978;&#30340;&#22810;&#39033;&#24335;&#65292;&#23427;&#20204;&#26500;&#25104;&#20102;&#29699;&#38754;&#19978;&#21487;&#31215;&#20989;&#25968;&#38598;&#21512;&#30340;&#27491;&#20132;&#22522;&#12290;&#23558;&#36825;&#20004;&#20010;&#20107;&#23454;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#31216;&#20026;&#29699;&#35856;&#25511;&#21046;&#21464;&#37327;&#65288;SHCV&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#29699;&#35856;&#20989;&#25968;&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#36817;&#20284;&#35745;&#31639;SW&#36317;&#31163;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#20363;&#22914;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#19968;&#23450;&#24418;&#24335;&#30340;&#32447;&#24615;&#20381;&#36182;&#26102;&#65292;&#28151;&#21512;&#39640;&#26031;&#27979;&#24230;&#30340;&#26080;&#35823;&#24046;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#33945;&#29305;&#21345;&#32599;&#30456;&#27604;&#65292;&#24471;&#21040;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sliced-Wasserstein (SW) distance between probability measures is defined as the average of the Wasserstein distances resulting for the associated one-dimensional projections. As a consequence, the SW distance can be written as an integral with respect to the uniform measure on the sphere and the Monte Carlo framework can be employed for calculating the SW distance. Spherical harmonics are polynomials on the sphere that form an orthonormal basis of the set of square-integrable functions on the sphere. Putting these two facts together, a new Monte Carlo method, hereby referred to as Spherical Harmonics Control Variates (SHCV), is proposed for approximating the SW distance using spherical harmonics as control variates. The resulting approach is shown to have good theoretical properties, e.g., a no-error property for Gaussian measures under a certain form of linear dependency between the variables. Moreover, an improved rate of convergence, compared to Monte Carlo, is established for g
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25581;&#31034;&#26435;&#37325;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#30340;&#22522;&#20110;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#37319;&#26679;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01484</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;&#65306;&#27169;&#24335;&#36830;&#25509;&#26159;&#21542;&#26159;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21487;&#34892;&#30340;&#22522;&#20110;&#26679;&#26412;&#25512;&#29702;&#30340;&#20851;&#38190;&#65311;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01484
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25581;&#31034;&#26435;&#37325;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#30340;&#22522;&#20110;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#37319;&#26679;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#26679;&#26412;&#25512;&#29702;&#65288;SBI&#65289;&#20013;&#65292;&#32593;&#32476;&#21442;&#25968;&#31354;&#38388;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25509;&#21463;&#26435;&#37325;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#29305;&#24449;&#20851;&#31995;&#65292;&#25104;&#21151;&#23454;&#29616;SBI&#26159;&#21487;&#33021;&#30340;&#65292;&#25581;&#31034;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#37319;&#26679;&#38382;&#39064;&#22256;&#38590;&#20043;&#38388;&#30340;&#31995;&#32479;&#32852;&#31995;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#37319;&#26679;&#21644;&#25910;&#25947;&#35786;&#26029;&#30340;&#23454;&#38469;&#25351;&#21335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in sample-based inference (SBI) for Bayesian neural networks is the size and structure of the networks' parameter space. Our work shows that successful SBI is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem. Through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis. As a result, we present a Bayesian deep ensemble approach as an effective solution with competitive performance and uncertainty quantification.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01481</link><description>&lt;p&gt;
&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Multi-level protein pre-training with Vabs-Net
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#19977;&#32500;&#32467;&#26500;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#21363;&#945;&#30899;&#21407;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21407;&#23376;&#65292;&#22914;&#20391;&#38142;&#21407;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#19978;&#23545;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20391;&#38142;&#21407;&#23376;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#23376;&#23545;&#25509;&#65289;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#22825;&#30495;&#22320;&#32452;&#21512;&#27531;&#22522;&#21644;&#21407;&#23376;&#20449;&#24687;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20449;&#24687;&#27844;&#28431;&#26159;&#21253;&#21547;&#21407;&#23376;&#32467;&#26500;&#30340;&#36755;&#20837;&#23548;&#33268;&#27531;&#22522;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#21464;&#24471;&#29712;&#30862;&#24182;&#23548;&#33268;&#27531;&#22522;&#34920;&#31034;&#19981;&#22815;&#20805;&#20998;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25513;&#30721;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01476</link><description>&lt;p&gt;
&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20063;&#21487;&#33021;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#24182;&#38656;&#35201;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#35299;&#20915;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23558;&#23545;&#31216;&#26680;&#24212;&#29992;&#20110;&#21464;&#20998;&#25512;&#26029;&#19979;&#30340;&#27880;&#24847;&#21147;&#26680;&#65307;&#28982;&#32780;&#65292;&#24573;&#30053;&#20102;&#27880;&#24847;&#21147;&#26680;&#26412;&#36136;&#19978;&#26159;&#19981;&#23545;&#31216;&#30340;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25512;&#23548;&#20986;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;GP&#21518;&#39564;&#30340;&#22797;&#26434;&#24230;&#20173;&#28982;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#30340;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464; &#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;KEP-SVGP&#65292;i&#65289;&#30001;&#20110;&#19982;&#27880;&#24847;&#21147;&#26680;&#30340;KSVD&#30456;&#23545;&#24212;&#30340;&#20004;&#32452;&#22855;&#24322;&#21521;&#37327;&#24341;&#23548;&#30340;SVGP&#23545;&#23436;&#20840;&#34920;&#24449;&#20102;&#19981;&#23545;&#31216;&#24615;&#65307;ii&#65289;&#20165;&#20351;&#29992;&#23569;&#37327;&#19982;KSVD&#30456;&#23545;&#24212;&#30340;&#20276;&#38543;&#29305;&#24449;&#20989;&#25968;&#65292;&#25512;&#23548;SVGP&#21518;&#39564;&#27010;&#29575;&#23494;&#24230;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;Follmer&#27969;&#26469;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#36716;&#21270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Wasserstein&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#35823;&#24046;&#20998;&#26512;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01460</link><description>&lt;p&gt;
&#28145;&#24230;&#26465;&#20214;&#29983;&#25104;&#23398;&#20064;&#65306;&#27169;&#22411;&#19982;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep Conditional Generative Learning: Model and Error Analysis
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;Follmer&#27969;&#26469;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#36716;&#21270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Wasserstein&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#35823;&#24046;&#20998;&#26512;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#31216;&#20026;&#26465;&#20214;Follmer&#27969;&#12290;&#20174;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#24320;&#22987;&#65292;&#25152;&#25552;&#20986;&#30340;&#27969;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;&#20854;&#36716;&#21270;&#20026;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#65292;&#22312;&#26102;&#38388;1&#22788;&#36798;&#21040;&#31283;&#23450;&#12290;&#20026;&#20102;&#26377;&#25928;&#23454;&#29616;&#65292;&#25105;&#20204;&#20351;&#29992;&#27431;&#25289;&#26041;&#27861;&#23545;&#27969;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38750;&#21442;&#25968;&#21270;&#20272;&#35745;&#36895;&#24230;&#22330;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23398;&#20064;&#26679;&#26412;&#30340;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#22312;&#26465;&#20214;&#20998;&#24067;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35823;&#24046;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#23427;&#22312;&#19968;&#31995;&#21015;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#26631;&#20934;&#30340;&#38750;&#21442;&#25968;&#21270;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#21040;&#28041;&#21450;&#22270;&#20687;&#25968;&#25454;&#30340;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#35828;&#26126;&#23427;&#20248;&#20110;&#21508;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an Ordinary Differential Equation (ODE) based deep generative method for learning a conditional distribution, named the Conditional Follmer Flow. Starting from a standard Gaussian distribution, the proposed flow could efficiently transform it into the target conditional distribution at time 1. For effective implementation, we discretize the flow with Euler's method where we estimate the velocity field nonparametrically using a deep neural network. Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein distance between the distribution of the learned samples and the target distribution, providing the first comprehensive end-to-end error analysis for conditional distribution learning via ODE flow. Our numerical experiments showcase its effectiveness across a range of scenarios, from standard nonparametric conditional density estimation problems to more intricate challenges involving image data, illustrating its superiority over various existing condition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01454</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#19968;&#31181;&#32479;&#35745;&#22240;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#65288;SCD&#65289;&#20013;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#23884;&#20837;&#21040;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#21019;&#24314;&#19968;&#33268;&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#27169;&#22411;&#26159;&#37325;&#35201;&#30340;&#65292;&#23613;&#31649;&#35782;&#21035;&#32972;&#26223;&#30693;&#35782;&#30340;&#25361;&#25112;&#34987;&#35748;&#21487;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;LLM&#30340;&#8220;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#65288;SCP&#65289;&#8221;&#19982;SCD&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#22240;&#26524;&#25512;&#26029;&#65288;KBCI&#65289;&#30456;&#32467;&#21512;&#65292;&#23545;SCD&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4&#21487;&#20197;&#20351;LLM-KBCI&#30340;&#36755;&#20986;&#19982;&#24102;&#26377;LLM-KBCI&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;SCD&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#65292;&#22914;&#26524;GPT-4&#32463;&#21382;&#20102;SCP&#65292;&#37027;&#20040;SCD&#30340;&#32467;&#26524;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;LLM&#19981;&#21547;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;LLM&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20854;&#32972;&#26223;&#30693;&#35782;&#26469;&#25913;&#36827;SCD&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#20013;&#25913;&#36827;&#37325;&#35201;&#24615;&#20272;&#35745;&#20197;&#25552;&#39640;&#39044;&#27979;&#35823;&#24046;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01450</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#20013;&#25913;&#36827;&#37325;&#35201;&#24615;&#20272;&#35745;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Improving importance estimation in covariate shift for providing accurate prediction error
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#20013;&#25913;&#36827;&#37325;&#35201;&#24615;&#20272;&#35745;&#20197;&#25552;&#39640;&#39044;&#27979;&#35823;&#24046;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#31639;&#27861;&#30340;&#39044;&#27979;&#22522;&#20110;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#25968;&#25454;&#36981;&#24490;&#30456;&#21516;&#30340;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#65292;&#36825;&#20010;&#26465;&#20214;&#24182;&#19981;&#25104;&#31435;&#65292;&#20363;&#22914;&#65292;&#21327;&#21464;&#37327;&#30340;&#20998;&#24067;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#32780;&#30446;&#26631;&#30340;&#26465;&#20214;&#20998;&#24067;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#24773;&#20917;&#34987;&#31216;&#20026;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#65292;&#26631;&#20934;&#35823;&#24046;&#20272;&#35745;&#21487;&#33021;&#19981;&#20877;&#20934;&#30830;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#37325;&#35201;&#24615;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#20943;&#36731;&#21327;&#21464;&#37327;&#20559;&#31227;&#23545;&#35823;&#24046;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#30340;&#32570;&#28857;&#26159;&#23427;&#19981;&#23481;&#26131;&#35745;&#31639;&#12290;Kullback-Leibler&#37325;&#35201;&#24615;&#20272;&#35745;&#36807;&#31243;&#65288;KLIEP&#65289;&#33021;&#22815;&#20197;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#24335;&#20272;&#35745;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#23427;&#30340;&#24615;&#33021;&#24456;&#22909;&#65292;&#20294;&#23427;&#26080;&#27861;&#24573;&#30053;&#30446;&#26631;&#20449;&#24687;&#65292;&#22240;&#20026;&#23427;&#21482;&#21253;&#25324;&#29992;&#20110;&#35745;&#31639;&#37325;&#35201;&#24615;&#30340;&#21327;&#21464;&#37327;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#26524;&#22312;&#35745;&#31639;&#37325;&#35201;&#24615;&#26102;&#21253;&#25324;&#30446;&#26631;&#20449;&#24687;&#30340;&#28508;&#22312;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional Machine Learning, the algorithms predictions are based on the assumption that the data follows the same distribution in both the training and the test datasets. However, in real world data this condition does not hold and, for instance, the distribution of the covariates changes whereas the conditional distribution of the targets remains unchanged. This situation is called covariate shift problem where standard error estimation may be no longer accurate. In this context, the importance is a measure commonly used to alleviate the influence of covariate shift on error estimations. The main drawback is that it is not easy to compute. The Kullback-Leibler Importance Estimation Procedure (KLIEP) is capable of estimating importance in a promising way. Despite its good performance, it fails to ignore target information, since it only includes the covariates information for computing the importance. In this direction, this paper explores the potential performance improvement if 
&lt;/p&gt;</description></item><item><title>&#21355;&#26143;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29420;&#29305;&#27169;&#24577;&#65292;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#29616;&#26377;&#30340;&#23454;&#36341;&#24182;&#21457;&#36215;&#19968;&#20010;&#20197;&#21355;&#26143;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#25361;&#25112;&#20026;&#20013;&#24515;&#30340;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20197;&#25512;&#21160;SatML&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01444</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#38190; -- &#21355;&#26143;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29420;&#29305;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Mission Critical -- Satellite Data is a Distinct Modality in Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01444
&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29420;&#29305;&#27169;&#24577;&#65292;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#29616;&#26377;&#30340;&#23454;&#36341;&#24182;&#21457;&#36215;&#19968;&#20010;&#20197;&#21355;&#26143;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#25361;&#25112;&#20026;&#20013;&#24515;&#30340;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20197;&#25512;&#21160;SatML&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#25968;&#25454;&#26377;&#28508;&#21147;&#20026;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#19968;&#27425;&#37325;&#22823;&#30340;&#25913;&#21464;&#65292;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#38024;&#23545;&#20256;&#32479;&#25968;&#25454;&#27169;&#24577;&#35774;&#35745;&#30340;&#29616;&#26377;&#23454;&#36341;&#12290;&#38543;&#30528;&#21355;&#26143;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;SatML&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#39046;&#22495;&#27491;&#22788;&#20110;&#19968;&#20010;&#21313;&#23383;&#36335;&#21475;&#12290;&#25105;&#20204;&#21487;&#20197;&#32487;&#32493;&#24212;&#29992;&#19981;&#36866;&#21512;&#30340;&#26041;&#27861;&#65292;&#25110;&#32773;&#25105;&#20204;&#21487;&#20197;&#21457;&#36215;&#19968;&#20010;&#20197;&#21355;&#26143;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#21644;&#25361;&#25112;&#20026;&#20013;&#24515;&#30340;&#26032;&#30740;&#31350;&#35758;&#31243;&#12290;&#26412;&#25991;&#35748;&#20026;&#21355;&#26143;&#25968;&#25454;&#26500;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#31181;&#29420;&#29305;&#27169;&#24577;&#65292;&#32780;&#25105;&#20204;&#24517;&#39035;&#25215;&#35748;&#36825;&#19968;&#28857;&#65292;&#20197;&#25512;&#21160;SatML&#22312;&#29702;&#35770;&#12289;&#26041;&#27861;&#21644;&#37096;&#32626;&#26041;&#38754;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20851;&#38190;&#24615;&#30340;&#35752;&#35770;&#38382;&#39064;&#21644;&#21487;&#34892;&#24615;&#24314;&#35758;&#65292;&#23558;SatML&#20174;&#20165;&#20165;&#19968;&#20010;&#26377;&#36259;&#30340;&#24212;&#29992;&#39046;&#22495;&#36716;&#21464;&#20026;&#19968;&#20010;&#33268;&#21147;&#20110;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#21644;&#31038;&#20250;&#37325;&#22823;&#25361;&#25112;&#30340;&#19987;&#38376;&#30740;&#31350;&#23398;&#31185;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satellite data has the potential to inspire a seismic shift for machine learning -- one in which we rethink existing practices designed for traditional data modalities. As machine learning for satellite data (SatML) gains traction for its real-world impact, our field is at a crossroads. We can either continue applying ill-suited approaches, or we can initiate a new research agenda that centers around the unique characteristics and challenges of satellite data. This position paper argues that satellite data constitutes a distinct modality for machine learning research and that we must recognize it as such to advance the quality and impact of SatML research across theory, methods, and deployment. We outline critical discussion questions and actionable suggestions to transform SatML from merely an intriguing application area to a dedicated research discipline that helps move the needle on big challenges for machine learning and society.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#24773;&#24863;&#20998;&#26512;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#31639;&#27861;&#24212;&#29992;&#20110;&#32929;&#31080;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#20197;&#26681;&#25454;&#24066;&#22330;&#24773;&#32490;&#21160;&#24577;&#35843;&#25972;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#38598;&#25104;&#31574;&#30053;&#12289;&#21333;&#19968;&#26234;&#33021;&#20307;&#31639;&#27861;&#21644;&#24066;&#22330;&#25351;&#26631;&#26356;&#20855;&#30408;&#21033;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#30456;&#20851;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#22266;&#23450;&#26356;&#25442;&#38598;&#25104;&#26234;&#33021;&#20307;&#30340;&#20570;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#22522;&#20110;&#24773;&#24863;&#30340;&#21160;&#24577;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20132;&#26131;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01441</link><description>&lt;p&gt;
&#23398;&#20064;&#24066;&#22330;&#65306;&#22522;&#20110;&#24773;&#24863;&#30340;&#38598;&#25104;&#20132;&#26131;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Learning the Market: Sentiment-Based Ensemble Trading Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#24773;&#24863;&#20998;&#26512;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#31639;&#27861;&#24212;&#29992;&#20110;&#32929;&#31080;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#20197;&#26681;&#25454;&#24066;&#22330;&#24773;&#32490;&#21160;&#24577;&#35843;&#25972;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#38598;&#25104;&#31574;&#30053;&#12289;&#21333;&#19968;&#26234;&#33021;&#20307;&#31639;&#27861;&#21644;&#24066;&#22330;&#25351;&#26631;&#26356;&#20855;&#30408;&#21033;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#30456;&#20851;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#22266;&#23450;&#26356;&#25442;&#38598;&#25104;&#26234;&#33021;&#20307;&#30340;&#20570;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#22522;&#20110;&#24773;&#24863;&#30340;&#21160;&#24577;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20132;&#26131;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24773;&#24863;&#20998;&#26512;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#31639;&#27861;&#24212;&#29992;&#20110;&#32929;&#31080;&#20132;&#26131;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#24066;&#22330;&#24773;&#32490;&#21160;&#24577;&#35843;&#25972;&#25152;&#20351;&#29992;&#30340;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#26032;&#38395;&#24773;&#24863;&#65292;&#24182;&#23558;&#20854;&#19982;&#23545;&#29616;&#26377;&#20316;&#21697;&#30340;&#19968;&#33324;&#25913;&#36827;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#24471;&#21040;&#26377;&#25928;&#32771;&#34385;&#23450;&#24615;&#24066;&#22330;&#22240;&#32032;&#21644;&#23450;&#37327;&#32929;&#31080;&#25968;&#25454;&#30340;&#33258;&#21160;&#20132;&#26131;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#19968;&#31181;&#30408;&#21033;&#12289;&#31283;&#20581;&#19988;&#39118;&#38505;&#26368;&#23567;&#30340;&#31574;&#30053;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#31574;&#30053;&#20197;&#21450;&#21333;&#19968;&#26234;&#33021;&#20307;&#31639;&#27861;&#21644;&#24066;&#22330;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#27599;&#38548;&#22266;&#23450;&#26376;&#20221;&#26356;&#25442;&#38598;&#25104;&#26234;&#33021;&#20307;&#30340;&#20570;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22522;&#20110;&#24773;&#24863;&#30340;&#21160;&#24577;&#26694;&#26550;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#36825;&#20123;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#31639;&#27861;&#35774;&#35745;&#31616;&#21333;&#19988;...
&lt;/p&gt;
&lt;p&gt;
We propose the integration of sentiment analysis and deep-reinforcement learning ensemble algorithms for stock trading, and design a strategy capable of dynamically altering its employed agent given concurrent market sentiment. In particular, we create a simple-yet-effective method for extracting news sentiment and combine this with general improvements upon existing works, resulting in automated trading agents that effectively consider both qualitative market factors and quantitative stock data. We show that our approach results in a strategy that is profitable, robust, and risk-minimal -- outperforming the traditional ensemble strategy as well as single agent algorithms and market metrics. Our findings determine that the conventional practice of switching ensemble agents every fixed-number of months is sub-optimal, and that a dynamic sentiment-based framework greatly unlocks additional performance within these agents. Furthermore, as we have designed our algorithm with simplicity and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01440</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65306;&#20174;&#20803;&#23398;&#20064;&#21040;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#22270;&#20013;&#24515;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26089;&#26399;&#30340;&#25216;&#26415;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#20013;&#36816;&#34892;&#65292;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20805;&#36275;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20010;&#38480;&#21046;&#24341;&#21457;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#21482;&#26377;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#21487;&#29992;&#12290;&#37492;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#25991;&#29486;&#65292;&#26412;&#32508;&#36848;&#35797;&#22270;&#32508;&#21512;&#26368;&#36817;&#30340;&#21457;&#23637;&#65292;&#25552;&#20379;&#27604;&#36739;&#24615;&#30340;&#35265;&#35299;&#65292;&#24182;&#30830;&#23450;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;&#35835;&#32773;&#36827;&#34892;&#26041;&#27861;&#36873;&#25321;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#20869;&#23481;&#28085;&#30422;&#20102;&#20998;&#23376;&#20449;&#24687;&#36755;&#20837;LLMs&#30340;&#34920;&#31034;&#21644;&#26631;&#35760;&#21270;&#26041;&#27861;&#12289;&#21270;&#23398;LLMs&#30340;&#19981;&#21516;&#32452;&#32676;&#21450;&#20854;&#25972;&#21512;&#26041;&#27861;&#12289;&#36866;&#29992;&#20110;&#21270;&#23398;LLMs&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#31561;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;LLMs&#22312;&#21270;&#23398;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01439</link><description>&lt;p&gt;
&#20174;&#35789;&#35821;&#21040;&#20998;&#23376;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Words to Molecules: A Survey of Large Language Models in Chemistry
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#20869;&#23481;&#28085;&#30422;&#20102;&#20998;&#23376;&#20449;&#24687;&#36755;&#20837;LLMs&#30340;&#34920;&#31034;&#21644;&#26631;&#35760;&#21270;&#26041;&#27861;&#12289;&#21270;&#23398;LLMs&#30340;&#19981;&#21516;&#32452;&#32676;&#21450;&#20854;&#25972;&#21512;&#26041;&#27861;&#12289;&#36866;&#29992;&#20110;&#21270;&#23398;LLMs&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#31561;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;LLMs&#22312;&#21270;&#23398;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#21508;&#31181;&#36328;&#23398;&#31185;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#21270;&#23398;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#23545;&#23558;LLMs&#25972;&#21512;&#21040;&#21270;&#23398;&#39046;&#22495;&#20013;&#25152;&#37319;&#29992;&#30340;&#24494;&#22937;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25506;&#32034;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20010;&#36328;&#23398;&#31185;&#20132;&#27719;&#28857;&#30340;&#22797;&#26434;&#24615;&#21644;&#21019;&#26032;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#20174;&#36890;&#36807;&#21508;&#31181;&#34920;&#31034;&#21644;&#26631;&#35760;&#21270;&#26041;&#27861;&#23558;&#20998;&#23376;&#20449;&#24687;&#36755;&#20837;LLMs&#24320;&#22987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#36755;&#20837;&#25968;&#25454;&#30340;&#39046;&#22495;&#21644;&#27169;&#24577;&#23558;&#21270;&#23398;LLMs&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#32452;&#65292;&#24182;&#35752;&#35770;&#23558;&#36825;&#20123;&#36755;&#20837;&#19982;LLMs&#25972;&#21512;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#36866;&#29992;&#20110;&#21270;&#23398;LLMs&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#22312;&#21270;&#23398;&#20013;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26032;&#33539;&#24335;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas. However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge. This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture. Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods. We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs. Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs. After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their applica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#23545;&#38750;&#32447;&#24615;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#36827;&#21270;&#29983;&#29289;&#23398;&#20013;&#30340;&#29983;&#29289;&#24418;&#24577;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01434</link><description>&lt;p&gt;
&#38543;&#26426;&#38750;&#32447;&#24615;&#19982;&#26080;&#31351;&#32500;&#25193;&#25955;&#36807;&#31243;&#30340;&#26465;&#20214;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Conditioning non-linear and infinite-dimensional diffusion processes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#23545;&#38750;&#32447;&#24615;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#36827;&#21270;&#29983;&#29289;&#23398;&#20013;&#30340;&#29983;&#29289;&#24418;&#24577;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#38543;&#26426;&#27169;&#22411;&#22312;&#31163;&#25955;&#21270;&#20043;&#21069;&#33258;&#28982;&#22320;&#23384;&#22312;&#20110;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#23558;&#35266;&#27979;&#25968;&#25454;&#32435;&#20837;&#32479;&#35745;&#21644;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38656;&#35201;&#23545;&#35266;&#27979;&#20540;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#22788;&#29702;&#20102;&#22312;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#23545;&#32447;&#24615;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#22312;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#23545;&#38750;&#32447;&#24615;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#20808;&#39564;&#31163;&#25955;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;&#20989;&#25968;&#20540;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Girsanov&#23450;&#29702;&#30340;&#26080;&#31351;&#32500;&#29256;&#26412;&#26469;&#23545;&#20989;&#25968;&#20540;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#28041;&#21450;&#24471;&#20998;&#30340;&#26465;&#20214;&#36807;&#31243;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25216;&#26415;&#24212;&#29992;&#20110;&#36827;&#21270;&#29983;&#29289;&#23398;&#20013;&#30340;&#29983;&#29289;&#24418;&#24577;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#36890;&#36807;Fourier&#22522;&#20989;&#25968;&#31163;&#25955;&#21270;&#65292;&#28982;&#21518;&#21033;&#29992;&#24471;&#20998;&#21305;&#37197;&#26041;&#27861;&#23398;&#20064;&#24471;&#20998;&#20989;&#25968;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. To incorporate observed data for statistical and learning tasks, one needs to condition on observations. While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. This paper conditions function valued stochastic processes without prior discretisation. To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score. We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#37096;&#20998;&#21487;&#35266;&#23519;&#31995;&#32479;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#36817;&#20284;&#26041;&#27861;&#23454;&#29616;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#29366;&#24577;&#31354;&#38388;&#30340;&#36807;&#28388;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01431</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;POMDP&#30340;&#36817;&#20284;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Approximate Control for Continuous-Time POMDPs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#37096;&#20998;&#21487;&#35266;&#23519;&#31995;&#32479;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#36817;&#20284;&#26041;&#27861;&#23454;&#29616;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#29366;&#24577;&#31354;&#38388;&#30340;&#36807;&#28388;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20855;&#26377;&#31163;&#25955;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#36830;&#32493;&#26102;&#38388;&#37096;&#20998;&#21487;&#35266;&#23519;&#31995;&#32479;&#30340;&#20915;&#31574;&#26694;&#26550;&#12290;&#30001;&#20110;&#22823;&#35268;&#27169;&#29366;&#24577;&#31354;&#38388;&#30340;&#26368;&#20248;&#20915;&#31574;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#26029;&#22686;&#21152;&#30340;&#29366;&#24577;&#25968;&#37327;&#30340;&#36807;&#28388;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39640;&#32500;&#36807;&#28388;&#20998;&#24067;&#25237;&#24433;&#21040;&#21442;&#25968;&#20998;&#24067;&#26063;&#19978;&#26469;&#36817;&#20284;&#23427;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#22522;&#20110;&#23436;&#20840;&#21487;&#35266;&#23519;&#31995;&#32479;&#30340;&#25511;&#21046;&#21551;&#21457;&#24335;&#20013;&#65292;&#20197;&#33719;&#24471;&#21487;&#25193;&#23637;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#37096;&#20998;&#35266;&#23519;&#31995;&#32479;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25490;&#38431;&#31995;&#32479;&#21644;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a decision-making framework for partially observable systems in continuous time with discrete state and action spaces. As optimal decision-making becomes intractable for large state spaces we employ approximation methods for the filtering and the control problem that scale well with an increasing number of states. Specifically, we approximate the high-dimensional filtering distribution by projecting it onto a parametric family of distributions, and integrate it into a control heuristic based on the fully observable system to obtain a scalable policy. We demonstrate the effectiveness of our approach on several partially observed systems, including queueing systems and chemical reaction networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#21160;&#38050;&#29748;&#36716;&#24405;&#31995;&#32479;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#22768;&#23398;&#29305;&#24615;&#26041;&#38754;&#20005;&#37325;&#36807;&#25311;&#21512;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#26597;&#30475;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;88.4&#30340;F1&#24471;&#20998;&#30340;&#26368;&#26032;&#38899;&#31526;&#36215;&#22987;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01424</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#40065;&#26834;&#33258;&#21160;&#38050;&#29748;&#36716;&#24405;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Analysis of Robust Automatic Piano Transcription
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#21160;&#38050;&#29748;&#36716;&#24405;&#31995;&#32479;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#22768;&#23398;&#29305;&#24615;&#26041;&#38754;&#20005;&#37325;&#36807;&#25311;&#21512;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#26597;&#30475;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;88.4&#30340;F1&#24471;&#20998;&#30340;&#26368;&#26032;&#38899;&#31526;&#36215;&#22987;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#27169;&#25216;&#26415;&#65292;&#33258;&#21160;&#38050;&#29748;&#36716;&#24405;&#31639;&#27861;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#37319;&#29992;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;Transformer&#21644;Perceiver&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#31995;&#32479;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36716;&#24405;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#20998;&#24067;&#22806;&#30340;&#24102;&#27880;&#37322;&#38050;&#29748;&#25968;&#25454;&#19978;&#34913;&#37327;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#20005;&#37325;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#22768;&#23398;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;MAESTRO&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#20102;&#19968;&#32452;&#26032;&#30340;&#38899;&#39057;&#25968;&#25454;&#65292;&#22312;&#19987;&#19994;&#24405;&#38899;&#29615;&#22659;&#20013;&#36890;&#36807;Yamaha Disklavier&#25773;&#25918;&#36827;&#34892;&#20102;&#33258;&#21160;&#25429;&#33719;&#12290;&#22312;&#20351;&#29992;&#21407;&#22987;&#21644;&#37325;&#26032;&#28436;&#22863;&#29256;&#26412;&#30340;MAESTRO&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#37319;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;MAPS&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;88.4&#30340;F1&#24471;&#20998;&#30340;&#26368;&#26032;&#38899;&#31526;&#36215;&#22987;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#20854;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for automatic piano transcription have improved dramatically in recent years due to new datasets and modeling techniques. Recent developments have focused primarily on adapting new neural network architectures, such as the Transformer and Perceiver, in order to yield more accurate systems. In this work, we study transcription systems from the perspective of their training data. By measuring their performance on out-of-distribution annotated piano data, we show how these models can severely overfit to acoustic properties of the training data. We create a new set of audio for the MAESTRO dataset, captured automatically in a professional studio recording environment via Yamaha Disklavier playback. Using various data augmentation techniques when training with the original and re-performed versions of the MAESTRO dataset, we achieve state-of-the-art note-onset accuracy of 88.4 F1-score on the MAPS dataset, without seeing any of its training data. We subsequently analyze these dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#24207;&#21015;&#32553;&#30701;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#65292;&#22312;&#23545;&#27604;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#26032;&#30340;&#28508;&#22312;&#20998;&#32452;&#21644;&#28508;&#22312;&#36873;&#25321;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01416</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#24207;&#21015;&#32553;&#30701;
&lt;/p&gt;
&lt;p&gt;
Sequence Shortening for Context-Aware Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#24207;&#21015;&#32553;&#30701;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#65292;&#22312;&#23545;&#27604;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#26032;&#30340;&#28508;&#22312;&#20998;&#32452;&#21644;&#28508;&#22312;&#36873;&#25321;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#26088;&#22312;&#36890;&#36807;&#23558;&#21608;&#22260;&#30340;&#21477;&#23376;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#25913;&#36827;&#21477;&#23376;&#30340;&#32763;&#35793;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24050;&#32463;&#24212;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#26550;&#26500;&#65292;&#21363;&#22522;&#20110;&#20018;&#32852;&#30340;&#21333;&#32534;&#30721;&#22120;&#21644;&#22810;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#22312;&#19979;&#19968;&#27493;&#20013;&#37325;&#29992;&#28304;&#21477;&#23376;&#30340;&#28508;&#22312;&#34920;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#22312;&#23545;&#27604;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65288;&#27169;&#22411;&#24517;&#39035;&#23545;&#25552;&#20379;&#30340;&#21477;&#23376;&#20013;&#30340;&#27491;&#30830;&#32763;&#35793;&#36827;&#34892;&#25490;&#24207;&#65289;&#65292;&#24182;&#19988;&#19982;&#21333;&#32534;&#30721;&#22120;&#21644;&#22810;&#32534;&#30721;&#22120;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;BLEU&#21644;COMET&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#32531;&#23384;&#34920;&#31034;&#24212;&#29992;&#20110;&#24207;&#21015;&#32553;&#30701;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19977;&#31181;&#22522;&#20110;&#27719;&#32858;&#30340;&#32553;&#30701;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#28508;&#22312;&#20998;&#32452;&#21644;&#28508;&#22312;&#36873;&#25321;&#65292;&#20854;&#20013;&#32593;&#32476;&#23398;&#20064;&#23558;&#20196;&#29260;&#20998;&#32452;&#25110;&#36873;&#25321;&#35201;&#32531;&#23384;&#20026;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32531;&#23384;&#34920;&#31034;&#30340;&#24207;&#21015;&#32553;&#30701;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that th
&lt;/p&gt;</description></item><item><title>SMLP&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#26679;&#26412;&#30340;&#31995;&#32479;&#25506;&#32034;&#24037;&#20855;&#65292;&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28784;&#30418;&#26041;&#27861;&#65292;&#21487;&#20197;&#25506;&#32034;&#31995;&#32479;&#24182;&#20248;&#21270;&#30828;&#20214;&#35774;&#35745;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01415</link><description>&lt;p&gt;
SMLP: &#31526;&#21495;&#26426;&#22120;&#23398;&#20064;&#35777;&#26126;&#22120;
&lt;/p&gt;
&lt;p&gt;
SMLP: Symbolic Machine Learning Prover
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01415
&lt;/p&gt;
&lt;p&gt;
SMLP&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#26679;&#26412;&#30340;&#31995;&#32479;&#25506;&#32034;&#24037;&#20855;&#65292;&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28784;&#30418;&#26041;&#27861;&#65292;&#21487;&#20197;&#25506;&#32034;&#31995;&#32479;&#24182;&#20248;&#21270;&#30828;&#20214;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#26426;&#22120;&#23398;&#20064;&#35777;&#26126;&#22120;&#65288;SMLP&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#36807;&#27169;&#25311;&#25110;&#25191;&#34892;&#31995;&#32479;&#24471;&#21040;&#30340;&#19968;&#31995;&#21015;&#36755;&#20837;&#21521;&#37327;&#26679;&#26412;&#30340;&#31995;&#32479;&#25506;&#32034;&#24037;&#20855;&#21644;&#24211;&#12290;SMLP&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#28784;&#30418;&#26041;&#27861;&#65292;&#21363;&#23558;&#25968;&#25454;&#25506;&#32034;&#30340;&#32479;&#35745;&#26041;&#27861;&#19982;&#26500;&#24314;&#21644;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#27010;&#29575;&#21644;&#24418;&#24335;&#26041;&#27861;&#26469;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#65292;&#20174;&#32780;&#25506;&#32034;&#31995;&#32479;&#12290;SMLP&#24050;&#32463;&#24212;&#29992;&#20110;&#33521;&#29305;&#23572;&#30340;&#24037;&#19994;&#29615;&#22659;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#20248;&#21270;&#27169;&#25311;&#30005;&#36335;&#32423;&#21035;&#30340;&#30828;&#20214;&#35774;&#35745;&#12290;SMLP&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;&#21644;&#24314;&#27169;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic Machine Learning Prover (SMLP) is a tool and a library for system exploration based on data samples obtained by simulating or executing the system on a number of input vectors. SMLP aims at exploring the system based on this data by taking a grey-box approach: SMLP combines statistical methods of data exploration with building and exploring machine learning models in close feedback loop with the system's response, and exploring these models by combining probabilistic and formal methods. SMLP has been applied in industrial setting at Intel for analyzing and optimizing hardware designs at the analog level. SMLP is a general purpose tool and can be applied to systems that can be sampled and modeled by machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#30340;UDASE&#20219;&#21153;&#20013;&#31995;&#32479;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#32467;&#26524;</title><link>https://rss.arxiv.org/abs/2402.01413</link><description>&lt;p&gt;
&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#20013;UDASE&#20219;&#21153;&#20013;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Objective and subjective evaluation of speech enhancement methods in the UDASE task of the 7th CHiME challenge
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#30340;UDASE&#20219;&#21153;&#20013;&#31995;&#32479;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#26159;&#36890;&#36807;&#20154;&#24037;&#21512;&#25104;&#30340;&#24178;&#20928;&#35821;&#38899;&#21644;&#22122;&#22768;&#20449;&#21495;&#28151;&#21512;&#26469;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#35757;&#32451;&#26465;&#20214;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#22312;&#27979;&#35797;&#22495;&#19982;&#21512;&#25104;&#35757;&#32451;&#22495;&#26174;&#33879;&#19981;&#21516;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#30340;UDASE&#20219;&#21153;&#26088;&#22312;&#21033;&#29992;&#27979;&#35797;&#22495;&#30340;&#30495;&#23454;&#19990;&#30028;&#22122;&#22768;&#35821;&#38899;&#24405;&#38899;&#26469;&#23545;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20010;&#27979;&#35797;&#22495;&#23545;&#24212;&#20110;CHiME-5&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#22312;&#22024;&#26434;&#21644;&#28151;&#21709;&#30340;&#23478;&#24237;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#30495;&#23454;&#22810;&#35828;&#35805;&#20154;&#23545;&#35805;&#24405;&#38899;&#32452;&#25104;&#65292;&#26080;&#27861;&#33719;&#24471;&#22320;&#38754;&#23454;&#20917;&#24178;&#20928;&#35821;&#38899;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25552;&#20132;&#21040;CHiME-7 UDASE&#20219;&#21153;&#30340;&#31995;&#32479;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Supervised models for speech enhancement are trained using artificially generated mixtures of clean speech and noise signals. However, the synthetic training conditions may not accurately reflect real-world conditions encountered during testing. This discrepancy can result in poor performance when the test domain significantly differs from the synthetic training domain. To tackle this issue, the UDASE task of the 7th CHiME challenge aimed to leverage real-world noisy speech recordings from the test domain for unsupervised domain adaptation of speech enhancement models. Specifically, this test domain corresponds to the CHiME-5 dataset, characterized by real multi-speaker and conversational speech recordings made in noisy and reverberant domestic environments, for which ground-truth clean speech signals are not available. In this paper, we present the objective and subjective evaluations of the systems that were submitted to the CHiME-7 UDASE task, and we provide an analysis of the resul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#25216;&#26415;&#29983;&#25104;&#20302;&#38899;&#20276;&#22863;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#38899;&#39057;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#38899;&#39057;&#26679;&#26412;&#21387;&#32553;&#25104;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#23545;&#24212;&#30340;&#38899;&#36712;&#12290;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#39118;&#26684;&#21644;&#35843;&#25972;&#24341;&#23548;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#29983;&#25104;&#26679;&#26412;&#38899;&#33394;&#30340;&#25511;&#21046;&#21644;&#36827;&#19968;&#27493;&#25552;&#39640;&#38899;&#39057;&#36136;&#37327;&#12290;&#23450;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01412</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#29983;&#25104;&#20302;&#38899;&#20276;&#22863;
&lt;/p&gt;
&lt;p&gt;
Bass Accompaniment Generation via Latent Diffusion
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#25216;&#26415;&#29983;&#25104;&#20302;&#38899;&#20276;&#22863;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#38899;&#39057;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#38899;&#39057;&#26679;&#26412;&#21387;&#32553;&#25104;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#23545;&#24212;&#30340;&#38899;&#36712;&#12290;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#39118;&#26684;&#21644;&#35843;&#25972;&#24341;&#23548;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#29983;&#25104;&#26679;&#26412;&#38899;&#33394;&#30340;&#25511;&#21046;&#21644;&#36827;&#19968;&#27493;&#25552;&#39640;&#38899;&#39057;&#36136;&#37327;&#12290;&#23450;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#19982;&#20219;&#24847;&#36755;&#20837;&#36712;&#36947;&#30456;&#21305;&#37197;&#30340;&#38899;&#20048;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25511;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#20219;&#24847;&#38271;&#24230;&#30340;&#38899;&#20048;&#28151;&#38899;&#30456;&#37197;&#30340;&#21333;&#38899;&#36712;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#38899;&#39057;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23427;&#21487;&#20197;&#23558;&#38899;&#39057;&#27874;&#24418;&#26679;&#26412;&#39640;&#25928;&#22320;&#21387;&#32553;&#25104;&#21487;&#36870;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#21450;&#19968;&#31181;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#20197;&#28151;&#38899;&#30340;&#28508;&#22312;&#32534;&#30721;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#30456;&#24212;&#38899;&#36712;&#30340;&#28508;&#22312;&#32534;&#30721;&#12290;&#20026;&#20102;&#23545;&#29983;&#25104;&#30340;&#26679;&#26412;&#30340;&#38899;&#33394;&#36827;&#34892;&#25511;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23558;&#28508;&#22312;&#31354;&#38388;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#21442;&#32771;&#39118;&#26684;&#30456;&#36830;&#25509;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25193;&#25955;&#37319;&#26679;&#26399;&#38388;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#38899;&#39057;&#36136;&#37327;&#65292;&#25105;&#20204;&#35843;&#25972;&#20102;&#26080;&#38656;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#29983;&#25104;&#26080;&#30028;&#28508;&#22312;&#31354;&#38388;&#26102;&#20135;&#29983;&#22833;&#30495;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;&#28151;&#38899;&#19982;&#21305;&#37197;&#20302;&#38899;&#38899;&#36712;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23450;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#32473;&#23450;&#36755;&#20837;&#30340;&#28151;&#38899;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#20043;&#21305;&#37197;&#30340;&#20302;&#38899;&#38899;&#36712;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to automatically generate music that appropriately matches an arbitrary input track is a challenging task. We present a novel controllable system for generating single stems to accompany musical mixes of arbitrary length. At the core of our method are audio autoencoders that efficiently compress audio waveform samples into invertible latent representations, and a conditional latent diffusion model that takes as input the latent encoding of a mix and generates the latent encoding of a corresponding stem. To provide control over the timbre of generated samples, we introduce a technique to ground the latent space to a user-provided reference style during diffusion sampling. For further improving audio quality, we adapt classifier-free guidance to avoid distortions at high guidance strengths when generating an unbounded latent space. We train our model on a dataset of pairs of mixes and matching bass stems. Quantitative experiments demonstrate that, given an input mix, the prop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40657;&#32032;&#30244;&#35786;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;-&#37096;&#20998;&#27169;&#22411;&#20197;&#21450;&#32467;&#21512;&#22522;&#20110;&#38750;&#19987;&#23478;&#21453;&#39304;&#30340;&#24341;&#23548;&#24615;&#30417;&#30563;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01410</link><description>&lt;p&gt;
&#20351;&#29992;&#21407;&#22411;&#21644;&#38750;&#19987;&#23478;&#30417;&#30563;&#30340;XAI&#23545;&#30382;&#32932;&#30284;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40657;&#32032;&#30244;&#35786;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;-&#37096;&#20998;&#27169;&#22411;&#20197;&#21450;&#32467;&#21512;&#22522;&#20110;&#38750;&#19987;&#23478;&#21453;&#39304;&#30340;&#24341;&#23548;&#24615;&#30417;&#30563;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30382;&#32932;&#38236;&#22270;&#20687;&#20998;&#26512;&#36827;&#34892;&#30382;&#32932;&#30284;&#26816;&#27979;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#27169;&#22411;&#24448;&#24448;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#40657;&#30418;&#24615;&#36136;&#24341;&#36215;&#20102;&#21307;&#29983;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;-&#37096;&#20998;&#27169;&#22411;&#36827;&#34892;&#40657;&#32032;&#30244;&#35786;&#26029;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#19981;&#21516;&#30340;&#20449;&#24687;&#36335;&#24452;&#24341;&#20837;&#20102;&#22522;&#20110;&#38750;&#19987;&#23478;&#21453;&#39304;&#30340;&#24341;&#23548;&#24615;&#30417;&#30563;&#65306;1) &#20351;&#29992;&#20998;&#21106;&#32593;&#32476;&#33258;&#21160;&#33719;&#21462;&#30340;&#20108;&#36827;&#21046;&#25513;&#27169;&#65307;2) &#29992;&#25143;&#20248;&#21270;&#30340;&#21407;&#22411;&#12290;&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#20449;&#24687;&#36335;&#24452;&#26088;&#22312;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#19982;&#30382;&#32932;&#30149;&#21464;&#30340;&#30456;&#20851;&#21306;&#22495;&#30456;&#23545;&#24212;&#65292;&#25490;&#38500;&#20854;&#36793;&#30028;&#20043;&#22806;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#19987;&#23478;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#19978;&#37117;&#20248;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin cancer detection through dermoscopy image analysis is a critical task. However, existing models used for this purpose often lack interpretability and reliability, raising the concern of physicians due to their black-box nature. In this paper, we propose a novel approach for the diagnosis of melanoma using an interpretable prototypical-part model. We introduce a guided supervision based on non-expert feedback through the incorporation of: 1) binary masks, obtained automatically using a segmentation network; and 2) user-refined prototypes. These two distinct information pathways aim to ensure that the learned prototypes correspond to relevant areas within the skin lesion, excluding confounding factors beyond its boundaries. Experimental results demonstrate that, even without expert supervision, our approach achieves superior performance and generalization compared to non-interpretable models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411; CF-CBMs&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#24819;&#35937;&#33021;&#21147;&#30340;&#19981;&#36275;&#65292;&#20026;&#37096;&#32626;&#21487;&#38752;&#30340;AI&#20195;&#29702;&#12289;&#26657;&#20934;&#20154;&#31867;&#20449;&#20219;&#21644;&#21152;&#28145;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01408</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#25856;&#30331;&#35299;&#37322;&#24615;&#30340;&#38454;&#26799;
&lt;/p&gt;
&lt;p&gt;
Climbing the Ladder of Interpretability with Counterfactual Concept Bottleneck Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411; CF-CBMs&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#24819;&#35937;&#33021;&#21147;&#30340;&#19981;&#36275;&#65292;&#20026;&#37096;&#32626;&#21487;&#38752;&#30340;AI&#20195;&#29702;&#12289;&#26657;&#20934;&#20154;&#31867;&#20449;&#20219;&#21644;&#21152;&#28145;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27809;&#26377;&#21516;&#26102;&#35299;&#20915;&#19977;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#35774;&#35745;&#65306;&#39044;&#27979;&#31867;&#21035;&#26631;&#31614;&#20197;&#35299;&#20915;&#32473;&#23450;&#30340;&#20998;&#31867;&#20219;&#21153;&#65288;&#8220;&#26159;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#35299;&#37322;&#20219;&#21153;&#39044;&#27979;&#65288;&#8220;&#20026;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#24182;&#24819;&#35937;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#39044;&#27979;&#30340;&#26367;&#20195;&#24773;&#26223;&#65288;&#8220;&#22914;&#26524;&#24590;&#26679;&#65311;&#8221;&#65289;&#12290;&#26080;&#27861;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20195;&#34920;&#20102;&#37096;&#32626;&#21487;&#38752;&#30340;AI&#20195;&#29702;&#12289;&#26657;&#20934;&#20154;&#31867;&#20449;&#20219;&#21644;&#21152;&#28145;&#20154;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CF-CBMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31867;&#33021;&#22815;&#39640;&#25928;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#26597;&#35810;&#32780;&#26080;&#38656;&#36827;&#34892;&#20107;&#21518;&#25628;&#32034;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CF-CBMs&#33021;&#22815;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#65288;&#8220;&#26159;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#23545;&#20219;&#21153;&#39044;&#27979;&#25552;&#20379;&#31616;&#21333;&#30340;&#35299;&#37322;&#65288;&#8220;&#20026;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#65288;&#8220;&#22914;&#26524;&#24590;&#26679;&#65311;&#8221;&#65289;&#12290;CF-CBMs&#36824;&#21487;&#20197;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#24433;&#21709;&#36827;&#34892;&#37319;&#26679;&#25110;&#20272;&#35745;&#26368;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#65292;&#20197;&#35299;&#37322;&#20107;&#20214;&#65292;&#24182;&#20248;&#21270;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#21453;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the "What?"), explain task predictions (the "Why?"), and imagine alternative scenarios that could result in different predictions (the "What if?"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our results show that CF-CBMs produce: accurate predictions (the "What?"), simple explanations for task predictions (the "Why?"), and interpretable counterfactuals (the "What if?"). CF-CBMs can also sample or estimate the most probable counterfactual to: (i) explain the effect of concept interventions on tasks, (ii) sh
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01401</link><description>&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#22312;&#35268;&#27169;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#35268;&#23450;&#65292;&#20174;&#35757;&#32451;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36951;&#24536;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#21450;&#26102;&#24536;&#35760;&#24517;&#35201;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36951;&#24536;&#30340;&#22330;&#26223;&#65292;&#21363;&#21482;&#26377;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#65292;&#36951;&#24536;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#31227;&#38500;&#25968;&#25454;&#12290;&#26681;&#25454;&#36825;&#26679;&#23450;&#20041;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#22522;&#20110;Lipschitz&#36830;&#32493;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#25200;&#21160;&#30340;&#36755;&#20986;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#26469;&#35825;&#23548;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#28369;&#24615;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36951;&#24536;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#24403;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#38646;&#26679;&#26412;&#32422;&#26463;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#26597;&#35810;&#25104;&#26412;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#32431;&#22312;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#25506;&#32034;&#33539;&#24335;&#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#33021;&#22312;NP-hard&#24773;&#20917;&#19979;&#36816;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01400</link><description>&lt;p&gt;
&#20302;&#26597;&#35810;&#25104;&#26412;&#24102;&#22122;&#22768;or&#21516;&#26102;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Query-Efficient Correlation Clustering with Noisy Oracle
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#26597;&#35810;&#25104;&#26412;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#32431;&#22312;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#25506;&#32034;&#33539;&#24335;&#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#33021;&#22312;NP-hard&#24773;&#20917;&#19979;&#36816;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#32858;&#31867;&#35774;&#32622;&#65292;&#20854;&#20013;&#25105;&#20204;&#38656;&#35201;&#23545;n&#20010;&#20803;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#23569;&#22320;&#21521;&#36820;&#22238;&#20004;&#20010;&#20803;&#32032;&#30456;&#20284;&#24615;&#30340;&#26377;&#22122;&#22768;&#30340;oracle&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#28085;&#30422;&#20102;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#30456;&#20284;&#24615;&#20989;&#25968;&#35745;&#31639;&#36215;&#26469;&#25104;&#26412;&#39640;&#24182;&#19988; inherently noisy&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#32431;&#22312;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#25506;&#32034;&#33539;&#24335;(PE-CMAB)&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#39062;&#34920;&#36798;&#26041;&#27861;&#22266;&#23450;&#32622;&#20449;&#24230;&#21644;&#22266;&#23450;&#39044;&#31639;&#35774;&#32622;&#12290;&#23545;&#20110;&#36825;&#20004;&#31181;&#35774;&#32622;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23558;&#25277;&#26679;&#31574;&#30053;&#19982;&#32463;&#20856;&#30340;&#30456;&#20851;&#32858;&#31867;&#36817;&#20284;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#36825;&#26679;&#30340;&#65306;&#36825;&#20123;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#24213;&#23618;&#31163;&#32447;&#20248;&#21270;&#38382;&#39064;&#20026;NP-hard&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We propose two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01399</link><description>&lt;p&gt;
&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Model to explain Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35821;&#20041;&#30456;&#20851;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#24577;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#22312;&#20247;&#22810;SSL&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;SimCLR&#65292;CLIP&#21644;VicREG&#65289;&#22240;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#19979;&#28216;&#24615;&#33021;&#19978;&#25509;&#36817;&#26377;&#30417;&#30563;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32972;&#21518;&#30340;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31867;&#20855;&#26377;&#37492;&#21035;&#24615;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;&#65288;&#21253;&#25324;&#23545;&#27604;&#26041;&#27861;&#65289;&#36817;&#20284;&#35825;&#23548;&#20854;&#34920;&#31034;&#20013;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#20114;&#20449;&#24687;&#21644;&#25237;&#24433;&#22836;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#22320;&#25311;&#21512;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;&#22914;SimVE&#65289;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65288;&#20363;&#22914;FashionMNIST&#65292;CIFAR10&#65292;CelebA&#65289;&#65292;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;VAE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
&lt;/p&gt;</description></item><item><title>ALERT-Transformer&#26159;&#19968;&#31181;&#23558;&#24322;&#27493;&#24863;&#30693;&#19982;&#21516;&#27493;&#22788;&#29702;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26725;&#25509;&#26041;&#24335;&#65292;&#36890;&#36807;ALERT&#27169;&#22359;&#12289;&#28789;&#27963;&#30340;&#25968;&#25454;&#35835;&#21462;&#21644;&#22522;&#20110;&#22359;&#30340;&#31232;&#30095;&#24615;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#31454;&#20105;&#23545;&#25163;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01393</link><description>&lt;p&gt;
ALERT-Transformer: &#23558;&#24322;&#27493;&#21644;&#21516;&#27493;&#26426;&#22120;&#23398;&#20064;&#26725;&#25509;&#22312;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#30340;&#26102;&#31354;&#25968;&#25454;&#19978;
&lt;/p&gt;
&lt;p&gt;
ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01393
&lt;/p&gt;
&lt;p&gt;
ALERT-Transformer&#26159;&#19968;&#31181;&#23558;&#24322;&#27493;&#24863;&#30693;&#19982;&#21516;&#27493;&#22788;&#29702;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26725;&#25509;&#26041;&#24335;&#65292;&#36890;&#36807;ALERT&#27169;&#22359;&#12289;&#28789;&#27963;&#30340;&#25968;&#25454;&#35835;&#21462;&#21644;&#22522;&#20110;&#22359;&#30340;&#31232;&#30095;&#24615;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#31454;&#20105;&#23545;&#25163;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31264;&#23494;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30001;&#20107;&#20214;&#24863;&#24212;&#22120;&#20135;&#29983;&#30340;&#36830;&#32493;&#36229;&#31232;&#30095;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31649;&#36947;&#65292;&#30001;&#24322;&#27493;&#24863;&#30693;&#21644;&#21516;&#27493;&#22788;&#29702;&#32452;&#25104;&#65292;&#32467;&#21512;&#20102;&#20960;&#20010;&#24605;&#36335;&#65306;&#65288;1&#65289;&#22522;&#20110;PointNet&#27169;&#22411;&#30340;&#23884;&#20837;&#8212;&#8212;ALERT&#27169;&#22359;&#65292;&#21487;&#20197;&#36890;&#36807;&#27844;&#28431;&#26426;&#21046;&#19981;&#26029;&#25972;&#21512;&#26032;&#20107;&#20214;&#24182;&#28040;&#38500;&#26087;&#20107;&#20214;&#65292;&#65288;2&#65289;&#23884;&#20837;&#25968;&#25454;&#30340;&#28789;&#27963;&#35835;&#21462;&#65292;&#21487;&#20197;&#20197;&#20219;&#20309;&#37319;&#26679;&#29575;&#23558;&#22987;&#32456;&#26368;&#26032;&#30340;&#29305;&#24449;&#36755;&#20837;&#21040;&#19979;&#28216;&#27169;&#22411;&#20013;&#65292;&#65288;3&#65289;&#20511;&#37492;Vision Transformer&#30340;&#22522;&#20110;&#22359;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#36755;&#20837;&#30340;&#31232;&#30095;&#24615;&#20197;&#20248;&#21270;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#23884;&#20837;&#28982;&#21518;&#30001;&#19968;&#20010;&#32463;&#36807;&#23545;&#35937;&#21644;&#25163;&#21183;&#35782;&#21035;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24322;&#27493;&#27169;&#22411;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#30340;&#37319;&#26679;&#29575;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition. Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors. We also demonstrate that our asynchronous model can operate at any desired sampling r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#40784;&#27425;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#21442;&#25968;&#23614;&#25351;&#25968;&#30340;&#26126;&#30830;&#19978;&#19979;&#30028;&#65292;&#24182;&#37327;&#21270;&#20102;&#20248;&#21270;&#21442;&#25968;&#19982;&#23614;&#25351;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20026;&#37325;&#23614;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#20197;&#21450;SGD&#36991;&#20813;&#27425;&#20248;&#23616;&#37096;&#26368;&#23567;&#20540;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01382</link><description>&lt;p&gt;
&#40784;&#27425;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#37325;&#23614;&#29616;&#35937;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of heavy tails in homogenized stochastic gradient descent
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01382
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#40784;&#27425;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#21442;&#25968;&#23614;&#25351;&#25968;&#30340;&#26126;&#30830;&#19978;&#19979;&#30028;&#65292;&#24182;&#37327;&#21270;&#20102;&#20248;&#21270;&#21442;&#25968;&#19982;&#23614;&#25351;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20026;&#37325;&#23614;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#20197;&#21450;SGD&#36991;&#20813;&#27425;&#20248;&#23616;&#37096;&#26368;&#23567;&#20540;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#24050;&#32463;&#34987;&#21457;&#29616;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#37325;&#23614;&#20998;&#24067;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;SGD&#30340;&#36830;&#32493;&#25193;&#25955;&#36924;&#36817;&#65292;&#31216;&#20026;&#40784;&#27425;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#28176;&#36827;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#37325;&#23614;&#29305;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#20851;&#20110;&#23614;&#25351;&#25968;&#30340;&#26126;&#30830;&#19978;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#30028;&#24182;&#26174;&#31034;&#23427;&#20204;&#36890;&#24120;&#26159;SGD&#36845;&#20195;&#30340;&#32463;&#39564;&#23614;&#25351;&#25968;&#30340;&#36817;&#20284;&#12290;&#21478;&#22806;&#65292;&#23427;&#20204;&#30340;&#26126;&#30830;&#24418;&#24335;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#20248;&#21270;&#21442;&#25968;&#21644;&#23614;&#25351;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23545;&#20110;&#20851;&#20110;&#37325;&#23614;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#20197;&#21450;SGD&#36991;&#20813;&#27425;&#20248;&#23616;&#37096;&#26368;&#23567;&#20540;&#33021;&#21147;&#30340;&#32852;&#31995;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent, show that it behaves asymptotically heavy-tailed, and give explicit upper and lower bounds on its tail-index. We validate these bounds in numerical experiments and show that they are typically close approximations to the empirical tail-index of SGD iterates. In addition, their explicit form enables us to quantify the interplay between optimization parameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#22823;&#31995;&#25968;&#24133;&#24230;&#20572;&#27490;&#20934;&#21017;&#30340;&#27491;&#21017;&#21270;&#22686;&#24378;&#20316;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#38598;&#25104;&#20013;&#30340;&#20803;&#23398;&#20064;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01379</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#22823;&#31995;&#25968;&#24133;&#24230;&#20572;&#27490;&#20934;&#21017;&#30340;&#27491;&#21017;&#21270;&#22686;&#24378;&#20316;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#38598;&#25104;&#20013;&#30340;&#20803;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Regularized boosting with an increasing coefficient magnitude stop criterion as meta-learner in hyperparameter optimization stacking ensemble
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#22823;&#31995;&#25968;&#24133;&#24230;&#20572;&#27490;&#20934;&#21017;&#30340;&#27491;&#21017;&#21270;&#22686;&#24378;&#20316;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#38598;&#25104;&#20013;&#30340;&#20803;&#23398;&#20064;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#65292;&#36890;&#36807;&#22810;&#27425;&#35797;&#39564;&#36873;&#25321;&#24615;&#33021;&#26368;&#20339;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#65292;&#24182;&#23558;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26356;&#22797;&#26434;&#30340;&#38598;&#25104;&#31574;&#30053;&#65292;&#22914;Caruana&#26041;&#27861;&#21644;stacking&#31574;&#30053;&#12290;Caruana&#26041;&#27861;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#38598;&#25104;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#21463;&#22810;&#37325;&#20849;&#32447;&#24615;&#30340;&#24433;&#21709;&#12290;&#32780;stacking&#26041;&#27861;&#38656;&#35201;&#20351;&#29992;&#20803;&#23398;&#20064;&#22120;&#36827;&#34892;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#20851;&#20110;&#22914;&#20309;&#36873;&#25321;&#20803;&#23398;&#20064;&#22120;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Hyperparameter Optimization (HPO), only the hyperparameter configuration with the best performance is chosen after performing several trials, then, discarding the effort of training all the models with every hyperparameter configuration trial and performing an ensemble of all them. This ensemble consists of simply averaging the model predictions or weighting the models by a certain probability. Recently, other more sophisticated ensemble strategies, such as the Caruana method or the stacking strategy has been proposed. On the one hand, the Caruana method performs well in HPO ensemble, since it is not affected by the effects of multicollinearity, which is prevalent in HPO. It just computes the average over a subset of predictions with replacement. But it does not benefit from the generalization power of a learning process. On the other hand, stacking methods include a learning procedure since a meta-learner is required to perform the ensemble. Yet, one hardly finds advice about which
&lt;/p&gt;</description></item><item><title>LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01376</link><description>&lt;p&gt;
LoTR: &#20302;&#24352;&#37327;&#31209;&#26435;&#37325;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LoTR: Low Tensor Rank Weight Adaptation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01376
&lt;/p&gt;
&lt;p&gt;
LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24605;&#24819;&#25512;&#24191;&#21644;&#25193;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;Transformer&#26550;&#26500;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;LoRA&#31867;&#26041;&#27861;&#26159;&#22522;&#20110;&#26799;&#24230;&#26356;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoTR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#26041;&#27861;&#65292;&#23427;&#20197;&#24352;&#37327;&#20998;&#35299;&#30340;&#24418;&#24335;&#34920;&#31034;&#21442;&#25968;&#30340;&#26799;&#24230;&#26356;&#26032;&#12290;&#27599;&#20010;&#23618;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#37117;&#30001;&#19977;&#20010;&#30697;&#38453;&#30340;&#20056;&#31215;&#26500;&#25104;&#65292;&#32780;&#24352;&#37327;&#32467;&#26500;&#26159;&#30001;&#36825;&#20010;&#20056;&#31215;&#30340;&#24038;&#21491;&#20056;&#23376;&#22312;&#23618;&#20043;&#38388;&#20849;&#20139;&#24341;&#36215;&#30340;&#12290;&#36890;&#36807;&#23545;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#23618;&#21516;&#26102;&#21387;&#32553;&#65292;LoTR&#33021;&#22815;&#27604;LoRA&#22312;&#29305;&#21035;&#26159;&#23545;&#20110;&#28145;&#23618;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26680;&#24515;&#24352;&#37327;&#19981;&#20381;&#36182;&#20110;&#21407;&#22987;&#26435;&#37325;&#32500;&#24230;&#65292;&#21487;&#20197;&#20219;&#24847;&#32553;&#23567;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#24120;&#24265;&#20215;&#21644;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#35770;&#23478;&#30340;&#22343;&#26041;&#35823;&#24046;&#19978;&#30028;&#20026;$\epsilon$&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.08})$&#65292;&#20248;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01371</link><description>&lt;p&gt;
Critic-Actor&#31639;&#27861;&#22312;&#24179;&#22343;&#22870;&#21169;MDPs&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65306;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#35770;&#23478;&#30340;&#22343;&#26041;&#35823;&#24046;&#19978;&#30028;&#20026;$\epsilon$&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.08})$&#65292;&#20248;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#28176;&#36817;&#21644;&#38750;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#30340;&#30740;&#31350;&#24037;&#20316;&#38750;&#24120;&#27963;&#36291;&#65292;&#20854;&#20013;&#28436;&#21592;&#30340;&#26356;&#26032;&#36895;&#24230;&#27604;&#35780;&#35770;&#23478;&#24930;&#12290;&#22312;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#25240;&#25187;&#25104;&#26412;&#35774;&#32622;&#20013;&#30340;&#26597;&#25214;&#34920;&#24773;&#20917;&#65292;&#20854;&#20013;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#30340;&#26102;&#38388;&#23610;&#24230;&#30456;&#21453;&#65292;&#24182;&#32473;&#20986;&#20102;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#24182;&#22312;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#26377;&#38480;&#26102;&#38388;&#65288;&#38750;&#28176;&#36817;&#65289;&#20998;&#26512;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#20248;&#30340;&#23398;&#20064;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#35780;&#35770;&#23478;&#30340;&#22343;&#26041;&#35823;&#24046;&#19978;&#30028;&#20026;$\epsilon$&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.08})$&#65292;&#27492;&#32467;&#26524;&#27604;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#33719;&#24471;&#30340;&#32467;&#26524;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic. In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented. In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\epsilon$ which is better than the one obtained for actor-critic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMP-Attack&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25915;&#20987;&#21830;&#19994;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01369</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24335;&#20808;&#39564;&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMP-Attack&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25915;&#20987;&#21830;&#19994;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#23637;&#29616;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#21331;&#36234;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#34987;&#24694;&#24847;&#21033;&#29992;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25552;&#31034;&#21518;&#38468;&#21152;&#29305;&#23450;&#21518;&#32512;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#25935;&#24863;&#22270;&#20687;&#12290;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;&#21333;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#25915;&#20987;&#65292;&#26410;&#33021;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMP-Attack&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#23427;&#23558;&#22810;&#27169;&#24577;&#20808;&#39564;&#65288;MMP&#65289;&#21363;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MMP-Attack&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#20869;&#23481;&#20013;&#28155;&#21152;&#30446;&#26631;&#23545;&#35937;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#31227;&#38500;&#21407;&#22987;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#20316;&#21697;&#30456;&#27604;&#65292;MMP-Attack&#20855;&#26377;&#26356;&#39640;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#65292;&#22312;&#25915;&#20987;&#21830;&#19994;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#65288;&#22914;DALL-E 3&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26631;&#24535;&#30528;&#24403;&#21069;&#26368;&#20339;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt. Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance. Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object. The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3. To the best of our knowledge, this marks 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;: &#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01364
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#23548;&#33268;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#26131;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#36171;&#20104;LLMs&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#20351;&#20854;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#31867;&#30693;&#35782;&#20445;&#25345;&#21516;&#27493;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#25345;&#32493;&#23398;&#20064;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#37492;&#20110;LLMs&#30340;&#29420;&#29305;&#24615;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#28041;&#21450;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#35843;&#25972;&#21644;&#23545;&#40784;&#31561;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#25345;&#32493;&#23398;&#20064;&#19982;&#22312;&#35268;&#27169;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#31616;&#21333;&#36866;&#24212;&#26041;&#27861;&#20197;&#21450;&#20854;&#20182;&#22686;&#24378;&#31574;&#30053;(&#22914;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#27169;&#22411;&#32534;&#36753;)&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#23545;&#22522;&#20934;&#21644;&#35780;&#20272;&#30340;&#35752;&#35770;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#19968;&#37325;&#35201;&#20219;&#21153;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26368;&#22823;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#22870;&#21169;&#20989;&#25968;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01361</link><description>&lt;p&gt;
&#23613;&#21892;&#23613;&#32654;&#65306;&#37325;&#26032;&#23450;&#20041;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
To the Max: Reinventing Reward in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26368;&#22823;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#22870;&#21169;&#20989;&#25968;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19981;&#21516;&#30340;&#22870;&#21169;&#21487;&#20197;&#23450;&#20041;&#30456;&#21516;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20294;&#23398;&#20064;&#24615;&#33021;&#21364;&#20250;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#23545;&#20110;&#26576;&#20123;&#24773;&#20917;&#65292;&#26234;&#33021;&#20307;&#20250;&#38519;&#20837;&#27425;&#20248;&#34892;&#20026;&#65292;&#32780;&#23545;&#20110;&#20854;&#20182;&#24773;&#20917;&#65292;&#21017;&#33021;&#39640;&#25928;&#22320;&#35299;&#20915;&#20219;&#21153;&#12290;&#36873;&#25321;&#19968;&#20010;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#22240;&#27492;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#22870;&#21169;&#29992;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#22823;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#65288;max-reward RL&#65289;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#20248;&#21270;&#30340;&#26159;&#26368;&#22823;&#22870;&#21169;&#32780;&#19981;&#26159;&#32047;&#31215;&#22870;&#21169;&#12290;&#19982;&#26089;&#26399;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;max-reward RL&#31639;&#27861;&#22312;Gymnasium-Robotics&#20013;&#30340;&#20004;&#20010;&#30446;&#26631;&#36798;&#25104;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20854;&#30456;&#23545;&#20110;&#26631;&#20934;RL&#30340;&#20248;&#21183;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), different rewards can define the same optimal policy but result in drastically different learning performance. For some, the agent gets stuck with a suboptimal behavior, and for others, it solves the task efficiently. Choosing a good reward function is hence an extremely important yet challenging problem. In this paper, we explore an alternative approach to using rewards for learning. We introduce max-reward RL, where an agent optimizes the maximum rather than the cumulative reward. Unlike earlier works, our approach works for deterministic and stochastic environments and can be easily combined with state-of-the-art RL algorithms. In the experiments, we study the performance of max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics and demonstrate its benefits over standard RL. The code is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;TESSERACT&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#20013;&#30340;&#23454;&#39564;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#20844;&#24179;&#23454;&#39564;&#35774;&#35745;&#32422;&#26463;&#21644;&#26032;&#25351;&#26631;AUT&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01359</link><description>&lt;p&gt;
TESSERACT: &#28040;&#38500;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#20013;&#30340;&#23454;&#39564;&#20559;&#24046;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#27861;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time (Extended Version)
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;TESSERACT&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#20013;&#30340;&#23454;&#39564;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#20844;&#24179;&#23454;&#39564;&#35774;&#35745;&#32422;&#26463;&#21644;&#26032;&#25351;&#26631;AUT&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#26816;&#27979;&#24694;&#24847;&#36719;&#20214;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#25253;&#21578;&#30340;F1&#20998;&#25968;&#39640;&#36798;0.99&#65292;&#20294;&#38382;&#39064;&#20173;&#26410;&#23436;&#20840;&#35299;&#20915;&#12290;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#24120;&#24120;&#22312;&#25805;&#20316;&#31995;&#32479;&#21644;&#25915;&#20987;&#26041;&#27861;&#19981;&#26029;&#28436;&#21270;&#26102;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#20250;&#23548;&#33268;&#20043;&#21069;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#23545;&#20110;&#26032;&#36755;&#20837;&#30340;&#20934;&#30830;&#20915;&#31574;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#35748;&#20026;&#24120;&#35265;&#30340;&#30740;&#31350;&#32467;&#26524;&#30001;&#20110;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20004;&#31181;&#26222;&#36941;&#30340;&#23454;&#39564;&#20559;&#24046;&#32780;&#34987;&#22840;&#22823;&#65306;&#31354;&#38388;&#20559;&#24046;&#26159;&#30001;&#25968;&#25454;&#20998;&#24067;&#19981;&#20195;&#34920;&#30495;&#23454;&#37096;&#32626;&#30340;&#24773;&#20917;&#24341;&#36215;&#30340;&#65307;&#26102;&#38388;&#20559;&#24046;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#19981;&#27491;&#30830;&#26102;&#38388;&#20998;&#21106;&#24341;&#36215;&#30340;&#65292;&#23548;&#33268;&#20102;&#19981;&#29616;&#23454;&#30340;&#37197;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#20844;&#24179;&#23454;&#39564;&#35774;&#35745;&#30340;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#35780;&#20272;&#20998;&#31867;&#22120;&#31283;&#23450;&#24615;&#30340;&#26032;&#25351;&#26631;AUT&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35843;&#25972;&#35757;&#32451;&#25968;&#25454;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) plays a pivotal role in detecting malicious software. Despite the high F1-scores reported in numerous studies reaching upwards of 0.99, the issue is not completely solved. Malware detectors often experience performance decay due to constantly evolving operating systems and attack methods, which can render previously learned knowledge insufficient for accurate decision-making on new inputs. This paper argues that commonly reported results are inflated due to two pervasive sources of experimental bias in the detection task: spatial bias caused by data distributions that are not representative of a real-world deployment; and temporal bias caused by incorrect time splits of data, leading to unrealistic configurations. To address these biases, we introduce a set of constraints for fair experiment design, and propose a new metric, AUT, for classifier robustness in real-world settings. We additionally propose an algorithm designed to tune training data to enhance classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#32423;&#30340;&#38382;&#39064;&#31354;&#38388;&#35268;&#33539;&#32534;&#35793;&#20026;&#36866;&#21512;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#30340;&#28385;&#36275;&#24615;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#23884;&#20837;&#38388;&#38553;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01353</link><description>&lt;p&gt;
&#23558;&#34920;&#36798;&#20016;&#23500;&#30340;&#38382;&#39064;&#31354;&#38388;&#35268;&#33539;&#39640;&#25928;&#32534;&#35793;&#20026;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient compilation of expressive problem space specifications to neural network solvers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#32423;&#30340;&#38382;&#39064;&#31354;&#38388;&#35268;&#33539;&#32534;&#35793;&#20026;&#36866;&#21512;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#30340;&#28385;&#36275;&#24615;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#23884;&#20837;&#38388;&#38553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#23884;&#20837;&#38388;&#38553;&#12290;&#22312;&#38388;&#38553;&#30340;&#19968;&#20391;&#26159;&#19968;&#20010;&#20851;&#20110;&#32593;&#32476;&#34892;&#20026;&#30340;&#39640;&#32423;&#35268;&#33539;&#65292;&#30001;&#39046;&#22495;&#19987;&#23478;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#38382;&#39064;&#31354;&#38388;&#32534;&#20889;&#12290;&#22312;&#21478;&#19968;&#20391;&#26159;&#19968;&#32452;&#36923;&#36753;&#19978;&#31561;&#20215;&#30340;&#21487;&#28385;&#36275;&#24615;&#26597;&#35810;&#65292;&#20197;&#36866;&#21512;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#30340;&#24418;&#24335;&#34920;&#36798;&#22312;&#19981;&#21487;&#29702;&#35299;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#23558;&#21069;&#32773;&#32534;&#35793;&#20026;&#21518;&#32773;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#21644;&#20811;&#26381;&#20102;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#32780;&#19981;&#26159;&#26631;&#20934;SMT&#27714;&#35299;&#22120;&#25152;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has described the presence of the embedding gap in neural network verification. On one side of the gap is a high-level specification about the network's behaviour, written by a domain expert in terms of the interpretable problem space. On the other side are a logically-equivalent set of satisfiability queries, expressed in the uninterpretable embedding space in a form suitable for neural network solvers. In this paper we describe an algorithm for compiling the former to the latter. We explore and overcome complications that arise from targeting neural network solvers as opposed to standard SMT solvers.
&lt;/p&gt;</description></item><item><title>FedMoE&#26159;&#19968;&#31181;&#27169;&#22411;&#24322;&#26500;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#20849;&#20139;&#30340;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26412;&#22320;&#38376;&#25511;&#32593;&#32476;&#20998;&#37197;&#32473;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#24322;&#26500;&#22823;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#12289;&#24615;&#33021;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01350</link><description>&lt;p&gt;
FedMoE: &#25968;&#25454;&#32423;&#21035;&#20010;&#24615;&#21270;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#24322;&#26500;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01350
&lt;/p&gt;
&lt;p&gt;
FedMoE&#26159;&#19968;&#31181;&#27169;&#22411;&#24322;&#26500;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#20849;&#20139;&#30340;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26412;&#22320;&#38376;&#25511;&#32593;&#32476;&#20998;&#37197;&#32473;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#24322;&#26500;&#22823;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#12289;&#24615;&#33021;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#25955;&#25968;&#25454;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#20294;&#38754;&#20020;&#25968;&#25454;&#12289;&#31995;&#32479;&#21644;&#27169;&#22411;&#24322;&#26500;&#31561;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064; (MHPFL) &#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;MHPFL&#26041;&#27861;&#22312;&#25968;&#25454;&#21644;&#27169;&#22411;&#38544;&#31169;&#12289;&#27169;&#22411;&#24615;&#33021;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20173;&#23384;&#22312;&#20851;&#20999;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861; (FedMoE) &#65292;&#37319;&#29992;&#33879;&#21517;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411; (MoE) &#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#12290;&#23427;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#24322;&#26500;&#22823;&#27169;&#22411;&#20998;&#37197;&#20102;&#19968;&#20010;&#20849;&#20139;&#30340;&#22343;&#21248;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#26412;&#22320;&#38376;&#25511;&#32593;&#32476;&#12290;(1) &#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26412;&#22320;&#24322;&#26500;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20316;&#20026;&#20010;&#24615;&#21270;&#29305;&#24449;&#65288;&#34920;&#31034;&#65289;&#25552;&#21462;&#30340;&#26412;&#22320;&#19987;&#23478;&#65292;&#32780;&#20849;&#20139;&#30340;&#22343;&#21248;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21017;&#20316;&#20026;&#24191;&#20041;&#29305;&#24449;&#25552;&#21462;&#30340;&#20840;&#23616;&#19987;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;COgnitive REplay&#65288;CORE&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#25968;&#37327;&#20998;&#37197;&#21644;&#20197;&#36136;&#37327;&#20026;&#37325;&#28857;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20248;&#21270;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01348</link><description>&lt;p&gt;
CORE&#65306;&#36890;&#36807;&#35748;&#30693;&#37325;&#25773;&#26469;&#20943;&#36731;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;COgnitive REplay&#65288;CORE&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#25968;&#37327;&#20998;&#37197;&#21644;&#20197;&#36136;&#37327;&#20026;&#37325;&#28857;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20248;&#21270;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26174;&#33879;&#20943;&#36731;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#35270;&#35282;&#65292;&#24378;&#35843;&#27169;&#22411;&#20445;&#25345;&#29616;&#26377;&#30693;&#35782;&#24182;&#34701;&#20837;&#26032;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#30340;&#37325;&#25773;&#26041;&#27861;&#21516;&#31561;&#23545;&#24453;&#27599;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#26679;&#26412;&#65292;&#22240;&#27492;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#37325;&#25773;&#32531;&#20914;&#21306;&#30340;&#28508;&#21147;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COgnitive REplay&#65288;CORE&#65289;&#65292;&#23427;&#20174;&#20154;&#31867;&#35748;&#30693;&#22797;&#20064;&#36807;&#31243;&#20013;&#24471;&#21040;&#28789;&#24863;&#12290;CORE&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#33258;&#36866;&#24212;&#25968;&#37327;&#20998;&#37197;&#21644;&#20197;&#36136;&#37327;&#20026;&#37325;&#28857;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#21069;&#32773;&#26681;&#25454;&#27599;&#20010;&#20219;&#21153;&#30340;&#36951;&#24536;&#36895;&#29575;&#33258;&#36866;&#24212;&#22320;&#35843;&#33410;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#20998;&#37197;&#65292;&#32780;&#21518;&#32773;&#20445;&#35777;&#22312;&#32531;&#20914;&#21306;&#20013;&#21253;&#21547;&#26368;&#33021;&#27010;&#25324;&#27599;&#20010;&#20219;&#21153;&#29305;&#24449;&#30340;&#20195;&#34920;&#24615;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#21106;CIFAR10&#19978;&#23454;&#29616;&#20102;37.95%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;6.52%&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#24046;&#34920;&#29616;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel perspective to significantly mitigate catastrophic forgetting in continuous learning (CL), which emphasizes models' capacity to preserve existing knowledge and assimilate new information. Current replay-based methods treat every task and data sample equally and thus can not fully exploit the potential of the replay buffer. In response, we propose COgnitive REplay (CORE), which draws inspiration from human cognitive review processes. CORE includes two key strategies: Adaptive Quantity Allocation and Quality-Focused Data Selection. The former adaptively modulates the replay buffer allocation for each task based on its forgetting rate, while the latter guarantees the inclusion of representative data that best encapsulates the characteristics of each task within the buffer. Our approach achieves an average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline method by 6.52%. Additionally, it significantly enhances the accuracy of the poorest-perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;BiLipNet&#65292;&#23427;&#20855;&#26377;&#35843;&#25511;&#36755;&#20986;&#25935;&#24863;&#24615;&#21644;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#26500;&#24314;&#20102;Bi-Lipschitz&#32593;&#32476;&#12290;&#21478;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#30340;PLNet&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#24212;&#29992;&#20110;&#23398;&#20064;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#30340;&#20248;&#21183;&#29305;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01344</link><description>&lt;p&gt;
&#21333;&#35843;&#12289;Bi-Lipschitz&#21644;Polyak-\L{}ojasiewicz&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Monotone, Bi-Lipschitz, and Polyak-\L{}ojasiewicz Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01344
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;BiLipNet&#65292;&#23427;&#20855;&#26377;&#35843;&#25511;&#36755;&#20986;&#25935;&#24863;&#24615;&#21644;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#26500;&#24314;&#20102;Bi-Lipschitz&#32593;&#32476;&#12290;&#21478;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#30340;PLNet&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#24212;&#29992;&#20110;&#23398;&#20064;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#30340;&#20248;&#21183;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BiLipNet&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#36870;&#30340;\emph{Bi-Lipschitz}&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#25511;&#21046;&#20854;\emph{Lipschitzness}&#65288;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#36755;&#20986;&#25935;&#24863;&#24615;&#65289;&#21644;\emph{inverse Lipschitzness}&#65288;&#19981;&#21516;&#36755;&#20986;&#30340;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#65289;&#30340;&#33021;&#21147;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#20855;&#26377;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#20197;&#26500;&#24314;Bi-Lipschitz&#32593;&#32476;&#12290;&#35748;&#35777;&#26159;&#22522;&#20110;&#22686;&#37327;&#20108;&#27425;&#32422;&#26463;&#30340;&#65292;&#19982;&#35889;&#24402;&#19968;&#21270;&#30456;&#27604;&#65292;&#23427;&#33021;&#23454;&#29616;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#21453;&#21521;&#35745;&#31639;&#24418;&#24335;&#21270;&#20026;&#19977;&#31639;&#23376;&#20998;&#35010;&#38382;&#39064;&#65292;&#24050;&#30693;&#23384;&#22312;&#24555;&#36895;&#31639;&#27861;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;Bi-Lipschitz&#32593;&#32476;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#37327;&#36755;&#20986;&#32593;&#32476;&#65292;&#21363;PLNet&#65292;&#23427;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#26377;&#21033;&#29305;&#24615;&#30340;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#65292;&#20363;&#22914;&#29420;&#29305;&#24615;&#21644;&#39640;&#25928;&#35745;&#31639;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new \emph{bi-Lipschitz} invertible neural network, the BiLipNet, which has the ability to control both its \emph{Lipschitzness} (output sensitivity to input perturbations) and \emph{inverse Lipschitzness} (input distinguishability from different outputs). The main contribution is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz networks. The certification is based on incremental quadratic constraints, which achieves much tighter bounds compared to spectral normalization. Moreover, we formulate the model inverse calculation as a three-operator splitting problem, for which fast algorithms are known. Based on the proposed bi-Lipschitz network, we introduce a new scalar-output network, the PLNet, which satisfies the Polyak-\L{}ojasiewicz condition. It can be applied to learn non-convex surrogate losses with favourable properties, e.g., a unique and efficiently-computab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#29305;&#24449;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#21518;&#26399;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24418;&#24577;&#29305;&#24449;&#21644;TimeGAN&#29983;&#25104;&#21487;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01343</link><description>&lt;p&gt;
&#22522;&#20110;&#24418;&#24577;&#29305;&#24449;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#21487;&#21453;&#20107;&#23454;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Shapelet-based Model-agnostic Counterfactual Local Explanations for Time Series Classification
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#29305;&#24449;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#21518;&#26399;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24418;&#24577;&#29305;&#24449;&#21644;TimeGAN&#29983;&#25104;&#21487;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#29305;&#24449;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#21518;&#26399;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;Time-CF&#65292;&#21033;&#29992;&#24418;&#24577;&#29305;&#24449;&#21644;TimeGAN&#20026;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#25552;&#20379;&#21487;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;UCR&#26102;&#38388;&#24207;&#21015;&#23384;&#26723;&#20013;&#23545;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;Time-CF&#29983;&#25104;&#30340;&#21487;&#21453;&#20107;&#23454;&#23454;&#20363;&#22312;&#25509;&#36817;&#24230;&#12289;&#25935;&#24863;&#24230;&#12289;&#21512;&#29702;&#24615;&#21644;&#31232;&#30095;&#24615;&#31561;&#22235;&#20010;&#35299;&#37322;&#24615;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a model-agnostic instance-based post-hoc explainability method for time series classification. The proposed algorithm, namely Time-CF, leverages shapelets and TimeGAN to provide counterfactual explanations for arbitrary time series classifiers. We validate the proposed method on several real-world univariate time series classification tasks from the UCR Time Series Archive. The results indicate that the counterfactual instances generated by Time-CF when compared to state-of-the-art methods, demonstrate better performance in terms of four explainability metrics: closeness, sensibility, plausibility, and sparsity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#31070;&#32463;&#20803;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32622;&#25442;&#23376;&#31354;&#38388;&#20943;&#23569;&#20102;&#32447;&#24615;&#27169;&#22359;&#36830;&#36890;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#27169;&#22411;&#34701;&#21512;&#31639;&#27861;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01342</link><description>&lt;p&gt;
&#36890;&#36807;&#32622;&#25442;&#23376;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31070;&#32463;&#20803;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#25913;&#36827;&#32447;&#24615;&#27169;&#22359;&#36830;&#36890;&#24615;&#21644;&#27169;&#22411;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#31070;&#32463;&#20803;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32622;&#25442;&#23376;&#31354;&#38388;&#20943;&#23569;&#20102;&#32447;&#24615;&#27169;&#22359;&#36830;&#36890;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#27169;&#22411;&#34701;&#21512;&#31639;&#27861;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#21021;&#22987;&#21270;&#26465;&#20214;&#19979;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#32463;&#24120;&#20135;&#29983;&#20855;&#26377;&#21151;&#33021;&#30456;&#20284;&#20294;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#20998;&#25955;&#30340;&#35299;&#65292;&#36825;&#23548;&#33268;&#20102;&#32447;&#24615;&#27169;&#22359;&#36830;&#36890;&#24615;&#65288;LMC&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#21644;&#25552;&#39640;&#27169;&#22411;&#34701;&#21512;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#24378;&#35843;&#32622;&#25442;&#23545;&#31216;&#24615;&#22312;&#36890;&#36807;&#32593;&#32476;&#32622;&#25442;&#20943;&#23569;&#35757;&#32451;&#21518;&#30340;&#23616;&#38480;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20107;&#21518;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#65292;&#22312;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65288;&#22914;ViT&#65292;LLM&#65289;&#19978;&#25928;&#26524;&#36739;&#24046;&#65292;&#22240;&#20026;&#23384;&#22312;&#22823;&#37327;&#30340;&#32622;&#25442;&#30697;&#38453;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#31070;&#32463;&#20803;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#32622;&#25442;&#23376;&#31354;&#38388;&#21487;&#20197;&#20813;&#36153;&#20943;&#23569;LMC&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21021;&#22987;&#21270;&#26102;&#36827;&#34892;&#20462;&#21098;&#21487;&#20197;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#12290;&#38500;&#20102;&#20462;&#21098;&#20043;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TNA-PFN&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26080;&#25439;&#30340;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#37096;&#20998;&#26799;&#24230;&#25513;&#30721;&#12290;TNA-PFN&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#37117;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape. Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices. Thus, in this paper, we study training-time neuron alignment. Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free. We find that pruning at initialization supports this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training. TNA-PFN is theoretically and em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21644;&#20998;&#26512;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20449;&#24687;&#22686;&#30410;&#30340;&#22522;&#26412;&#24615;&#36136;&#65292;&#21253;&#25324;&#30028;&#38480;&#21644;&#38142;&#35268;&#21017;&#65292;&#38416;&#26126;&#20102;&#22240;&#26524;&#29109;&#19982;&#38543;&#26426;&#24178;&#39044;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#26465;&#20214;&#29109;&#21644;&#22240;&#26524;&#26465;&#20214;&#20449;&#24687;&#22686;&#30410;&#30340;&#23450;&#20041;&#65292;&#20026;&#25552;&#21319;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01341</link><description>&lt;p&gt;
&#22240;&#26524;&#29109;&#21644;&#20449;&#24687;&#22686;&#30410;&#30340;&#22522;&#26412;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Fundamental Properties of Causal Entropy and Information Gain
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21644;&#20998;&#26512;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20449;&#24687;&#22686;&#30410;&#30340;&#22522;&#26412;&#24615;&#36136;&#65292;&#21253;&#25324;&#30028;&#38480;&#21644;&#38142;&#35268;&#21017;&#65292;&#38416;&#26126;&#20102;&#22240;&#26524;&#29109;&#19982;&#38543;&#26426;&#24178;&#39044;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#26465;&#20214;&#29109;&#21644;&#22240;&#26524;&#26465;&#20214;&#20449;&#24687;&#22686;&#30410;&#30340;&#23450;&#20041;&#65292;&#20026;&#25552;&#21319;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21457;&#23637;&#20351;&#24471;&#33021;&#22815;&#37327;&#21270;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#19979;&#30340;&#22240;&#26524;&#25511;&#21046;&#12290;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#19968;&#20123;&#37327;&#26469;&#32534;&#30721;&#22312;&#24178;&#39044;&#21478;&#19968;&#20010;&#21464;&#37327;&#26102;&#26576;&#20010;&#21464;&#37327;&#29109;&#30340;&#21464;&#21270;&#26469;&#23454;&#29616;&#30340;&#12290;&#36825;&#20123;&#37327;&#34987;&#21629;&#21517;&#20026;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20449;&#24687;&#22686;&#30410;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20449;&#24687;&#35770;&#26041;&#27861;&#22312;&#22240;&#26524;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#27010;&#24565;&#30340;&#22522;&#26412;&#24615;&#36136;&#65292;&#21253;&#25324;&#30028;&#38480;&#21644;&#38142;&#35268;&#21017;&#65292;&#23545;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20449;&#24687;&#22686;&#30410;&#30340;&#27010;&#24565;&#36827;&#34892;&#20102;&#24418;&#24335;&#19978;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#22240;&#26524;&#29109;&#19982;&#38543;&#26426;&#24178;&#39044;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#26465;&#20214;&#29109;&#21644;&#22240;&#26524;&#26465;&#20214;&#20449;&#24687;&#22686;&#30410;&#30340;&#23450;&#20041;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20010;&#25506;&#32034;&#20026;&#25552;&#21319;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SignSGD-FD&#30340;&#25216;&#26415;&#65292;&#22312;&#23545;&#25239;&#24615;&#24037;&#20316;&#33410;&#28857;&#22686;&#21152;&#26102;&#20445;&#25345;&#20102;&#25910;&#25947;&#29575;&#19981;&#21464;&#65292;&#21033;&#29992;&#20102;&#36890;&#36807;&#26799;&#24230;&#31526;&#21495;&#35299;&#30721;&#33719;&#24471;&#30340;&#23545;&#25239;&#24615;&#24037;&#20316;&#33410;&#28857;&#30340;&#26799;&#24230;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01340</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#37030;&#38450;&#24481;&#30340;SignSGD&#65306;&#36890;&#36807;&#26799;&#24230;&#31526;&#21495;&#35299;&#30721;&#26469;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SignSGD-FD&#30340;&#25216;&#26415;&#65292;&#22312;&#23545;&#25239;&#24615;&#24037;&#20316;&#33410;&#28857;&#22686;&#21152;&#26102;&#20445;&#25345;&#20102;&#25910;&#25947;&#29575;&#19981;&#21464;&#65292;&#21033;&#29992;&#20102;&#36890;&#36807;&#26799;&#24230;&#31526;&#21495;&#35299;&#30721;&#33719;&#24471;&#30340;&#23545;&#25239;&#24615;&#24037;&#20316;&#33410;&#28857;&#30340;&#26799;&#24230;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#26159;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#35745;&#31639;&#36164;&#28304;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26799;&#24230;&#36890;&#20449;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#24037;&#20316;&#33410;&#28857;&#21644;&#21442;&#25968;&#26381;&#21153;&#22120;&#20043;&#38388;&#23384;&#22312;&#30528;&#22823;&#37327;&#30340;&#36890;&#20449;&#24310;&#36831;&#12290;SignSGD&#19982;&#22810;&#25968;&#25237;&#31080;&#65288;SignSGD-MV&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#19968;&#20301;&#37327;&#21270;&#26469;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#65292;&#20294;&#26159;&#22312;&#23545;&#25239;&#24615;&#24037;&#20316;&#33410;&#28857;&#22686;&#21152;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#26126;&#26174;&#20943;&#24930;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#24615;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#23567;&#20110;&#33391;&#24615;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25910;&#25947;&#36895;&#24230;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#19968;&#21453;&#30452;&#35273;&#30340;&#32467;&#26524;&#26159;&#30001;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;SignSGD&#19982;&#32852;&#37030;&#38450;&#24481;&#65288;SignSGD-FD&#65289;&#20135;&#29983;&#30340;&#20851;&#38190;&#24605;&#24819;&#25152;&#23637;&#31034;&#30340;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;SignSGD-FD&#21033;&#29992;&#30001;&#23545;&#25239;&#24615;&#24037;&#20316;&#33410;&#28857;&#21457;&#36865;&#30340;&#26799;&#24230;&#20449;&#24687;&#65292;&#20351;&#29992;&#36890;&#36807;&#26799;&#24230;&#31526;&#21495;&#35299;&#30721;&#33719;&#24471;&#30340;&#36866;&#24403;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;SignSGD-FD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed learning is an effective approach to accelerate model training using multiple workers. However, substantial communication delays emerge between workers and a parameter server due to massive costs associated with communicating gradients. SignSGD with majority voting (signSGD-MV) is a simple yet effective optimizer that reduces communication costs through one-bit quantization, yet the convergence rates considerably decrease as adversarial workers increase. In this paper, we show that the convergence rate is invariant as the number of adversarial workers increases, provided that the number of adversarial workers is smaller than that of benign workers. The key idea showing this counter-intuitive result is our novel signSGD with federated defense (signSGD-FD). Unlike the traditional approaches, signSGD-FD exploits the gradient information sent by adversarial workers with the proper weights, which are obtained through gradient sign decoding. Experimental results demonstrate signS
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#26391;&#20043;&#19975;&#26041;&#31243;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20379;&#39044;&#27979;&#30340;&#20998;&#24067;&#20174;&#32780;&#35780;&#20272;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01338</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#26391;&#20043;&#19975;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Inferring the Langevin Equation with Uncertainty via Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#26391;&#20043;&#19975;&#26041;&#31243;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20379;&#39044;&#27979;&#30340;&#20998;&#24067;&#20174;&#32780;&#35780;&#20272;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#30340;&#38543;&#26426;&#31995;&#32479;&#34920;&#29616;&#20986;&#20174;&#20998;&#23376;&#21160;&#21147;&#23398;&#21040;&#27668;&#20505;&#29616;&#35937;&#30340;&#36807;&#31243;&#20013;&#30340;&#27874;&#21160;&#12290;&#26391;&#20043;&#19975;&#26041;&#31243;&#20316;&#20026;&#19968;&#31181;&#24120;&#35265;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#34987;&#29992;&#20110;&#30740;&#31350;&#36825;&#31181;&#31995;&#32479;&#65292;&#21487;&#20197;&#39044;&#27979;&#23427;&#20204;&#30340;&#26102;&#38388;&#28436;&#21270;&#20197;&#21450;&#20998;&#26512;&#28909;&#21147;&#23398;&#37327;&#65292;&#21253;&#25324;&#21560;&#25910;&#28909;&#37327;&#12289;&#23545;&#31995;&#32479;&#20570;&#30340;&#21151;&#20197;&#21450;&#29109;&#30340;&#20135;&#29983;&#12290;&#28982;&#32780;&#65292;&#20174;&#35266;&#27979;&#36712;&#36857;&#20013;&#25512;&#26029;&#26391;&#20043;&#19975;&#26041;&#31243;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#31995;&#32479;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#26029;&#36807;&#38459;&#23612;&#21644;&#27424;&#38459;&#23612;&#24773;&#20917;&#19979;&#30340;&#26391;&#20043;&#19975;&#26041;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#20998;&#21035;&#25552;&#20379;&#28418;&#31227;&#21147;&#21644;&#25193;&#25955;&#30697;&#38453;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#26500;&#24314;&#26391;&#20043;&#19975;&#26041;&#31243;&#12290;&#36890;&#36807;&#25552;&#20379;&#39044;&#27979;&#30340;&#20998;&#24067;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#20540;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#20197;&#38450;&#27490;&#28508;&#22312;&#30340;&#8230;
&lt;/p&gt;
&lt;p&gt;
Pervasive across diverse domains, stochastic systems exhibit fluctuations in processes ranging from molecular dynamics to climate phenomena. The Langevin equation has served as a common mathematical model for studying such systems, enabling predictions of their temporal evolution and analyses of thermodynamic quantities, including absorbed heat, work done on the system, and entropy production. However, inferring the Langevin equation from observed trajectories remains challenging, particularly for nonlinear and high-dimensional systems. In this study, we present a comprehensive framework that employs Bayesian neural networks for inferring Langevin equations in both overdamped and underdamped regimes. Our framework first provides the drift force and diffusion matrix separately and then combines them to construct the Langevin equation. By providing a distribution of predictions instead of a single value, our approach allows us to assess prediction uncertainties, which can prevent potenti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01327</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#20013;&#30340;&#30417;&#30563;&#31639;&#27861;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Supervised Algorithmic Fairness in Distribution Shifts: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#38754;&#23545;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#65292;&#22914;&#20309;&#20445;&#25345;&#20844;&#24179;&#21644;&#26080;&#20559;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#22240;&#21508;&#31181;&#22240;&#32032;&#32780;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#12290;&#36825;&#31181;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#23545;&#29305;&#23450;&#36890;&#36807;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#26469;&#34920;&#24449;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#22343;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20102;&#24635;&#32467;&#65292;&#24182;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#36825;&#20123;&#21464;&#21270;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#25991;&#29486;&#20013;&#31361;&#20986;&#20102;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20221;&#35843;&#26597;&#21015;&#20986;&#20102;&#29992;&#20110;&#23454;&#35777;&#30740;&#31350;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01306</link><description>&lt;p&gt;
KTO: &#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
KTO: Model Alignment as Prospect Theoretic Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20975;&#24681;&#26364;&#19982;&#29305;&#27779;&#26031;&#22522;&#30340;&#23637;&#26395;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20154;&#31867;&#20197;&#26377;&#20559;&#35265;&#20294;&#26126;&#30830;&#30340;&#26041;&#24335;&#30475;&#24453;&#38543;&#26426;&#21464;&#37327;&#65307;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#37117;&#26159;&#21388;&#24694;&#25439;&#22833;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#30340;&#30446;&#26631;&#38544;&#21547;&#22320;&#34701;&#21512;&#20102;&#35768;&#22810;&#36825;&#20123;&#20559;&#35265; - &#36825;&#20123;&#30446;&#26631; (&#20363;&#22914; DPO) &#30340;&#25104;&#21151;&#37096;&#20998;&#21487;&#24402;&#22240;&#20110;&#23427;&#20204;&#26159;"&#20154;&#31867;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;"(HALOs)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#24402;&#22240;&#32473;&#20154;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20173;&#19982;&#23637;&#26395;&#29702;&#35770;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#12290;&#21033;&#29992;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20154;&#31867;&#25928;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#30340;HALO&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20248;&#21270;(KTO)&#65292;&#24182;&#19988;&#23427;&#22312;&#20174;1B&#21040;30B&#30340;&#35268;&#27169;&#19978;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#36229;&#36807;&#12290;&#20851;&#38190;&#26159;&#65292;KTO&#19981;&#38656;&#35201;&#20559;&#22909; - &#21482;&#38656;&#35201;&#19968;&#20010;&#26159;&#21542;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#26799;&#24230;&#32858;&#31867;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#26063;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#32593;&#32476;&#20013;&#24037;&#20316;&#12290;&#36890;&#36807;&#25511;&#21046;&#29992;&#25143;&#20013;&#24515;&#20272;&#35745;&#30340;&#25509;&#36817;&#31243;&#24230;&#21644;&#23450;&#20041;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32858;&#31867;&#20219;&#21153;&#12290;&#22312;&#25552;&#20379;&#20102;&#32479;&#19968;&#20998;&#26512;&#21644;&#20960;&#20010;&#24378;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01302</link><description>&lt;p&gt;
&#26799;&#24230;&#32858;&#31867;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Gradient-based Clustering of Distributed Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01302
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#26799;&#24230;&#32858;&#31867;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#26063;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#32593;&#32476;&#20013;&#24037;&#20316;&#12290;&#36890;&#36807;&#25511;&#21046;&#29992;&#25143;&#20013;&#24515;&#20272;&#35745;&#30340;&#25509;&#36817;&#31243;&#24230;&#21644;&#23450;&#20041;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32858;&#31867;&#20219;&#21153;&#12290;&#22312;&#25552;&#20379;&#20102;&#32479;&#19968;&#20998;&#26512;&#21644;&#20960;&#20010;&#24378;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#26063;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#32593;&#32476;&#20013;&#24037;&#20316;&#12290;&#22312;&#25552;&#20986;&#30340;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#21253;&#21547;&#19968;&#20010;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21482;&#19982;&#20854;&#30452;&#25509;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#65292;&#30446;&#26631;&#26159;&#23547;&#25214;&#23436;&#25972;&#25968;&#25454;&#30340;&#32858;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#23478;&#26063;&#31216;&#20026;&#20998;&#24067;&#24335;&#26799;&#24230;&#32858;&#31867;&#65288;DGC-$\mathcal{F}_\rho$&#65289;&#65292;&#30001;&#21442;&#25968;&#21270;&#30340;$\rho\geq1$&#30830;&#23450;&#65292;&#25511;&#21046;&#29992;&#25143;&#20013;&#24515;&#20272;&#35745;&#30340;&#25509;&#36817;&#31243;&#24230;&#65292;&#32780;$\mathcal{F}$&#30830;&#23450;&#32858;&#31867;&#25439;&#22833;&#12290;&#38024;&#23545;&#27969;&#34892;&#30340;&#32858;&#31867;&#25439;&#22833;&#22914;$K$&#22343;&#20540;&#21644;Huber&#25439;&#22833;&#65292;DGC-$\mathcal{F}_\rho$&#20135;&#29983;&#20102;&#26032;&#30340;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;DGC-KM$_\rho$&#21644;DGC-HL$_\rho$&#65292;&#32780;&#22522;&#20110;&#36923;&#36753;&#20989;&#25968;&#30340;&#26032;&#22411;&#32858;&#31867;&#25439;&#22833;&#23548;&#33268;&#20102;DGC-LL$_\rho$&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#20998;&#26512;&#24182;&#24314;&#31435;&#20102;&#20960;&#20010;&#24378;&#32467;&#26524;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#12290;&#39318;&#20808;&#65292;&#26041;&#27861;&#29983;&#25104;&#30340;&#20013;&#24515;&#24207;&#21015;&#22312;&#20219;&#20309;&#20013;&#24515;&#21021;&#22987;&#21270;&#21644;$...
&lt;/p&gt;
&lt;p&gt;
We develop a family of distributed clustering algorithms that work over networks of users. In the proposed scenario, users contain a local dataset and communicate only with their immediate neighbours, with the aim of finding a clustering of the full, joint data. The proposed family, termed Distributed Gradient Clustering (DGC-$\mathcal{F}_\rho$), is parametrized by $\rho \geq 1$, controling the proximity of users' center estimates, with $\mathcal{F}$ determining the clustering loss. Specialized to popular clustering losses like $K$-means and Huber loss, DGC-$\mathcal{F}_\rho$ gives rise to novel distributed clustering algorithms DGC-KM$_\rho$ and DGC-HL$_\rho$, while a novel clustering loss based on the logistic function leads to DGC-LL$_\rho$. We provide a unified analysis and establish several strong results, under mild assumptions. First, the sequence of centers generated by the methods converges to a well-defined notion of fixed point, under any center initialization and value of $
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#25968;&#30028;&#38480;&#65292;&#22686;&#24378;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#27979;&#35797;&#35823;&#24046;&#30028;&#38480;&#12290;&#23545;&#20110;&#22810;&#39033;&#24335;&#35889;&#34928;&#20943;&#30340;&#26680;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#20808;&#21069;&#30340;&#32467;&#26524;&#65307;&#23545;&#20110;&#25351;&#25968;&#35889;&#34928;&#20943;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38750;&#24179;&#20961;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29305;&#24449;&#35889;&#34928;&#20943;&#22810;&#39033;&#24335;&#30340;&#26680;&#22238;&#24402;&#22120;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#29305;&#24449;&#35889;&#25351;&#25968;&#34928;&#20943;&#30340;&#26680;&#22238;&#24402;&#22120;&#21017;&#20855;&#26377;&#28798;&#38590;&#24615;&#30340;&#36807;&#25311;&#21512;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01297</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#35889;&#34920;&#24449;&#26680;&#23725;&#22238;&#24402;&#30340;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01297
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#25968;&#30028;&#38480;&#65292;&#22686;&#24378;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#27979;&#35797;&#35823;&#24046;&#30028;&#38480;&#12290;&#23545;&#20110;&#22810;&#39033;&#24335;&#35889;&#34928;&#20943;&#30340;&#26680;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#20808;&#21069;&#30340;&#32467;&#26524;&#65307;&#23545;&#20110;&#25351;&#25968;&#35889;&#34928;&#20943;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38750;&#24179;&#20961;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29305;&#24449;&#35889;&#34928;&#20943;&#22810;&#39033;&#24335;&#30340;&#26680;&#22238;&#24402;&#22120;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#29305;&#24449;&#35889;&#25351;&#25968;&#34928;&#20943;&#30340;&#26680;&#22238;&#24402;&#22120;&#21017;&#20855;&#26377;&#28798;&#38590;&#24615;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20102;&#26680;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#30340;&#26032;&#30028;&#38480;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#22686;&#24378;&#20102;&#22312;&#22266;&#23450;&#36755;&#20837;&#32500;&#24230;&#30340;&#36807;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#26680;&#23725;&#22238;&#24402;&#30340;&#29616;&#26377;&#38750;&#28176;&#36817;&#27979;&#35797;&#35823;&#24046;&#30028;&#38480;&#12290;&#23545;&#20110;&#20855;&#26377;&#22810;&#39033;&#24335;&#35889;&#34928;&#20943;&#30340;&#26680;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#30028;&#38480;&#65307;&#23545;&#20110;&#25351;&#25968;&#34928;&#20943;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#38750;&#24179;&#20961;&#21644;&#26032;&#39062;&#30340;&#12290;&#25105;&#20204;&#23545;&#36807;&#25311;&#21512;&#30340;&#32467;&#35770;&#26159;&#21452;&#37325;&#30340;&#65306;(i) &#35889;&#34928;&#20943;&#22810;&#39033;&#24335;&#30340;&#26680;&#22238;&#24402;&#22120;&#24517;&#39035;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#24456;&#22909;&#30340;&#27867;&#21270;&#65307;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#25152;&#35859;&#30340;&#28201;&#21644;&#36807;&#25311;&#21512;&#65307;(ii) &#22914;&#26524;&#20219;&#20309;&#26680;&#23725;&#22238;&#24402;&#22120;&#30340;&#29305;&#24449;&#35889;&#25351;&#25968;&#34928;&#20943;&#65292;&#21017;&#20854;&#27867;&#21270;&#24046;&#65292;&#21363;&#34920;&#29616;&#20986;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#36825;&#22686;&#21152;&#20102;&#26680;&#23725;&#22238;&#24402;&#22120;&#34920;&#29616;&#20986;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#21487;&#29992;&#29305;&#24449;&#35889;&#34928;&#20943;&#27425;&#22810;&#39033;&#24335;&#30340;&#26497;&#31471;&#24773;&#20917;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#21512;&#20102;&#26032;&#30340;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;(RMT)&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new random matrix theory (RMT) te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#38544;&#31169;&#20445;&#25252;&#36827;&#34892;&#21152;&#23494;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#21010;&#20998;&#20026;&#25935;&#24863;&#21644;&#19981;&#25935;&#24863;&#30340;&#37096;&#20998;&#65292;&#24182;&#20998;&#21035;&#22788;&#29702;&#65292;&#21516;&#26102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01296</link><description>&lt;p&gt;
Bi-CryptoNets: &#21033;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#38544;&#31169;&#20445;&#25252;&#36827;&#34892;&#21152;&#23494;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted Inference
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#38544;&#31169;&#20445;&#25252;&#36827;&#34892;&#21152;&#23494;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#21010;&#20998;&#20026;&#25935;&#24863;&#21644;&#19981;&#25935;&#24863;&#30340;&#37096;&#20998;&#65292;&#24182;&#20998;&#21035;&#22788;&#29702;&#65292;&#21516;&#26102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20154;&#20204;&#20174;&#21152;&#23494;&#30340;&#35282;&#24230;&#24320;&#21457;&#20102;&#21508;&#31181;&#31639;&#27861;&#65292;&#20197;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20449;&#24687;&#23433;&#20840;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#26412;&#30740;&#31350;&#20174;&#36755;&#20837;&#25968;&#25454;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#20986;&#21457;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#65288;&#20363;&#22914;&#19968;&#20123;&#22270;&#20687;&#65289;&#26681;&#25454;&#37325;&#35201;&#24615;&#21644;&#38544;&#31169;&#21010;&#20998;&#20026;&#25935;&#24863;&#21644;&#19981;&#25935;&#24863;&#30340;&#37096;&#20998;&#12290;&#25935;&#24863;&#30340;&#37096;&#20998;&#21253;&#21547;&#19968;&#20123;&#37325;&#35201;&#21644;&#31169;&#23494;&#30340;&#20449;&#24687;&#65292;&#22914;&#20154;&#33080;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21516;&#24577;&#21152;&#23494;&#26469;&#20445;&#25345;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#25935;&#24863;&#30340;&#37096;&#20998;&#21017;&#21253;&#21547;&#19968;&#20123;&#32972;&#26223;&#20449;&#24687;&#65292;&#25105;&#20204;&#28155;&#21152;&#25200;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;bi-CryptoNets&#65292;&#21363;&#26126;&#25991;&#21644;&#23494;&#25991;&#20998;&#25903;&#65292;&#20998;&#21035;&#22788;&#29702;&#20004;&#20010;&#37096;&#20998;&#65292;&#24182;&#19988;&#23494;&#25991;&#20998;&#25903;&#21487;&#20197;&#36890;&#36807;&#21333;&#21521;&#36830;&#25509;&#21033;&#29992;&#26126;&#25991;&#20998;&#25903;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;bi-CryptoNets&#65292;&#20174;&#19968;&#20010;w&#19978;&#36827;&#34892;&#34920;&#31034;&#30340;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving neural networks have attracted increasing attention in recent years, and various algorithms have been developed to keep the balance between accuracy, computational complexity and information security from the cryptographic view. This work takes a different view from the input data and structure of neural networks. We decompose the input data (e.g., some images) into sensitive and insensitive segments according to importance and privacy. The sensitive segment includes some important and private information such as human faces and we take strong homomorphic encryption to keep security, whereas the insensitive one contains some background and we add perturbations. We propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal with two segments, respectively, and ciphertext branch could utilize the information from plaintext branch by unidirectional connections. We adopt knowledge distillation for our bi-CryptoNets by transferring representations from a w
&lt;/p&gt;</description></item><item><title>ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;</title><link>https://rss.arxiv.org/abs/2402.01295</link><description>&lt;p&gt;
ExtremeCast: &#25552;&#21319;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#30340;&#26497;&#20540;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01295
&lt;/p&gt;
&lt;p&gt;
ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#22312;&#20840;&#29699;&#20013;&#26399;&#39044;&#25253;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26497;&#31471;&#20540;&#39044;&#27979;&#19982;&#27492;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#23545;&#31216;&#25439;&#22833;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#26377;&#20559;&#24046;&#24182;&#20302;&#20272;&#26497;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Exloss&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#20248;&#21270;&#31361;&#20986;&#26497;&#20540;&#65292;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#26497;&#31471;&#22825;&#27668;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#23427;&#22686;&#21152;&#20102;&#20687;&#32032;&#20540;&#30340;&#26041;&#24046;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01293</link><description>&lt;p&gt;
MLLMs&#33021;&#21542;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can MLLMs Perform Text-to-Image In-Context Learning?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#23637;&#21040;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#25512;&#21160;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25193;&#23637;&#21040;&#22810;&#27169;&#24335;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;ICL&#19978;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;ICL&#65288;T2I-ICL&#65289;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24615;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#20294;&#20173;&#28982;&#23569;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;T2I-ICL&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;CoBSAT&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#20219;&#21153;&#30340;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;MLLMs&#65292;&#25105;&#20204;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
&lt;/p&gt;</description></item><item><title>Spiking CenterNet&#26159;&#19968;&#31181;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#33021;&#37327;&#12289;&#23567;&#22411;&#21270;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;GEN1&#27773;&#36710;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#30340;&#33021;&#37327;&#19981;&#21040;&#19968;&#21322;&#65292;&#34920;&#29616;&#36229;&#36807;&#20102;&#21487;&#27604;&#36739;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01287</link><description>&lt;p&gt;
Spiking CenterNet:&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#33976;&#39311;&#22686;&#24378;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01287
&lt;/p&gt;
&lt;p&gt;
Spiking CenterNet&#26159;&#19968;&#31181;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#33021;&#37327;&#12289;&#23567;&#22411;&#21270;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;GEN1&#27773;&#36710;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#30340;&#33021;&#37327;&#19981;&#21040;&#19968;&#21322;&#65292;&#34920;&#29616;&#36229;&#36807;&#20102;&#21487;&#27604;&#36739;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#27668;&#20505;&#21464;&#21270;&#26102;&#20195;&#65292;&#23545;&#20110;&#39640;&#25928;&#33021;&#37327;&#12289;&#23567;&#22411;&#21270;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#19968;&#31181;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#30340;&#20449;&#24687;&#27969;&#21644;&#31232;&#30095;&#28608;&#27963;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20107;&#20214;&#25968;&#25454;&#30340;&#30446;&#26631;&#26816;&#27979;&#30340;Spiking CenterNet&#12290;&#23427;&#23558;SNN CenterNet&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;M2U-Net&#30340;&#35299;&#30721;&#22120;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Prophesee&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;GEN1&#27773;&#36710;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#21487;&#27604;&#36739;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#19988;&#20351;&#29992;&#30340;&#33021;&#37327;&#19981;&#21040;&#19968;&#21322;&#12290;&#23558;&#38750;&#33033;&#20914;&#32769;&#24072;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#25105;&#20204;&#30340;SNN&#20013;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#33033;&#20914;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#39318;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of AI at the edge, self-driving cars, and climate change, the need for energy-efficient, small, embedded AI is growing. Spiking Neural Networks (SNNs) are a promising approach to address this challenge, with their event-driven information flow and sparse activations. We propose Spiking CenterNet for object detection on event data. It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder. Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy. Distilling the knowledge of a non-spiking teacher into our SNN further increases performance. To the best of our knowledge, our work is the first approach that takes advantage of knowledge distillation in the field of spiking object detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#26032;&#30340;&#39640;&#24230;&#21487;&#20998;&#24067;&#21644;&#33258;&#21160;&#21487;&#24494;&#20998;&#30340;&#26041;&#21521;&#23567;&#27874;&#21464;&#25442;&#65292;&#22312;&#29699;&#38754;&#21644;&#29699;&#19978;&#30340;&#20449;&#21495;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01282</link><description>&lt;p&gt;
&#22312;&#29699;&#38754;&#21644;&#29699;&#19978;&#30340;&#21487;&#24494;&#20998;&#21644;&#21152;&#36895;&#23567;&#27874;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Differentiable and accelerated wavelet transforms on the sphere and ball
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#26032;&#30340;&#39640;&#24230;&#21487;&#20998;&#24067;&#21644;&#33258;&#21160;&#21487;&#24494;&#20998;&#30340;&#26041;&#21521;&#23567;&#27874;&#21464;&#25442;&#65292;&#22312;&#29699;&#38754;&#21644;&#29699;&#19978;&#30340;&#20449;&#21495;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#21521;&#23567;&#27874;&#23383;&#20856;&#26159;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25429;&#25417;&#21644;&#20998;&#21106;&#21508;&#31181;&#23610;&#24230;&#12289;&#20301;&#32622;&#21644;&#26041;&#21521;&#19978;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#23545;&#20110;&#29289;&#29702;&#20449;&#21495;&#20855;&#26377;&#29305;&#27530;&#30340;&#20146;&#21644;&#24615;&#65292;&#22240;&#20026;&#29289;&#29702;&#20449;&#21495;&#36890;&#24120;&#20855;&#26377;&#39640;&#21508;&#21521;&#24322;&#24615;&#12289;&#23616;&#37096;&#22810;&#23610;&#24230;&#32467;&#26500;&#12290;&#35768;&#22810;&#37325;&#35201;&#30340;&#29289;&#29702;&#20449;&#21495;&#22312;&#29699;&#24418;&#22495;&#19978;&#35266;&#27979;&#65292;&#20363;&#22914;&#23431;&#23449;&#23398;&#20013;&#30340;&#22825;&#31354;&#12290;&#20511;&#21161;&#26368;&#36817;&#22312;&#35745;&#31639;&#35856;&#27874;&#20998;&#26512;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#39640;&#24230;&#21487;&#20998;&#24067;&#21644;&#33258;&#21160;&#21487;&#24494;&#20998;&#30340;&#26041;&#21521;&#23567;&#27874;&#21464;&#25442;&#65292;&#24212;&#29992;&#20110;&#20108;&#32500;&#29699;&#38754;S^2&#21644;&#19977;&#32500;&#29699;&#20307;B^3=R^+ x S^2&#65288;&#36890;&#36807;&#23558;&#29699;&#38754;&#19982;&#24452;&#21521;&#21322;&#32447;&#32467;&#21512;&#32780;&#24418;&#25104;&#30340;&#31354;&#38388;&#65289;&#12290;&#19982;&#29616;&#26377;&#36719;&#20214;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29699;&#38754;&#21644;&#29699;&#19978;&#20449;&#21495;&#30340;&#21152;&#36895;&#27604;&#20998;&#21035;&#39640;&#36798;300&#20493;&#21644;21800&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;64&#20301;&#26426;&#22120;&#31934;&#24230;&#12290;&#36825;&#20123;&#31639;&#27861;&#19981;&#20165;&#22312;&#21152;&#36895;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#24494;&#20998;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directional wavelet dictionaries are hierarchical representations which efficiently capture and segment information across scale, location and orientation. Such representations demonstrate a particular affinity to physical signals, which often exhibit highly anisotropic, localised multiscale structure. Many physically important signals are observed over spherical domains, such as the celestial sky in cosmology. Leveraging recent advances in computational harmonic analysis, we design new highly distributable and automatically differentiable directional wavelet transforms on the $2$-dimensional sphere $\mathbb{S}^2$ and $3$-dimensional ball $\mathbb{B}^3 = \mathbb{R}^+ \times \mathbb{S}^2$ (the space formed by augmenting the sphere with the radial half-line). We observe up to a $300$-fold and $21800$-fold acceleration for signals on the sphere and ball, respectively, compared to existing software, whilst maintaining 64-bit machine precision. Not only do these algorithms dramatically acce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21442;&#25968;&#20219;&#21153;MAP-Elites&#65288;PT-ME&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#36830;&#32493;&#22810;&#20219;&#21153;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#21033;&#29992;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#36827;&#34892;&#21464;&#24322;&#25805;&#20316;&#65292;&#36890;&#36807;&#24471;&#21040;&#30340;&#35299;&#38598;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#26144;&#23556;&#20219;&#21153;&#21442;&#25968;&#21040;&#26368;&#20248;&#35299;&#30340;&#20989;&#25968;&#65292;&#23454;&#39564;&#35777;&#26126;PT-ME&#31639;&#27861;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01275</link><description>&lt;p&gt;
&#21442;&#25968;&#20219;&#21153;MAP-Elites
&lt;/p&gt;
&lt;p&gt;
Parametric-Task MAP-Elites
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21442;&#25968;&#20219;&#21153;MAP-Elites&#65288;PT-ME&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#36830;&#32493;&#22810;&#20219;&#21153;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#21033;&#29992;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#36827;&#34892;&#21464;&#24322;&#25805;&#20316;&#65292;&#36890;&#36807;&#24471;&#21040;&#30340;&#35299;&#38598;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#26144;&#23556;&#20219;&#21153;&#21442;&#25968;&#21040;&#26368;&#20248;&#35299;&#30340;&#20989;&#25968;&#65292;&#23454;&#39564;&#35777;&#26126;PT-ME&#31639;&#27861;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#32452;&#20989;&#25968;&#30340;&#20248;&#21270;&#21516;&#26102;&#24212;&#29992;&#20110;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#34987;&#31216;&#20026;&#22810;&#20219;&#21153;&#20248;&#21270;&#12290;&#24403;&#21069;&#30340;&#40657;&#31665;&#22810;&#20219;&#21153;&#31639;&#27861;&#20165;&#35299;&#20915;&#26377;&#38480;&#30340;&#19968;&#32452;&#20219;&#21153;&#65292;&#21363;&#20351;&#36825;&#20123;&#20219;&#21153;&#26469;&#33258;&#36830;&#32493;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21442;&#25968;&#20219;&#21153;MAP-Elites&#65288;PT-ME&#65289;&#65292;&#19968;&#31181;&#35299;&#20915;&#36830;&#32493;&#22810;&#20219;&#21153;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#22411;&#40657;&#31665;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#65288;1&#65289;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35299;&#20915;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#36830;&#32493;&#31354;&#38388;&#65292;&#65288;2&#65289;&#21033;&#29992;&#22522;&#20110;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#30340;&#26032;&#21464;&#24322;&#25805;&#20316;&#31526;&#12290;&#25152;&#24471;&#21040;&#30340;&#35299;&#38598;&#25968;&#25454;&#38598;&#20351;&#24471;&#33021;&#22815;&#21019;&#24314;&#19968;&#20010;&#23558;&#20219;&#20309;&#20219;&#21153;&#21442;&#25968;&#26144;&#23556;&#21040;&#20854;&#26368;&#20248;&#35299;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#21442;&#25968;&#20219;&#21153;&#29609;&#20855;&#38382;&#39064;&#21644;&#19968;&#20010;&#26356;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PT-ME&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#31639;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;PPO&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing a set of functions simultaneously by leveraging their similarity is called multi-task optimization. Current black-box multi-task algorithms only solve a finite set of tasks, even when the tasks originate from a continuous space. In this paper, we introduce Parametric-task MAP-Elites (PT-ME), a novel black-box algorithm to solve continuous multi-task optimization problems. This algorithm (1) solves a new task at each iteration, effectively covering the continuous space, and (2) exploits a new variation operator based on local linear regression. The resulting dataset of solutions makes it possible to create a function that maps any task parameter to its optimal solution. We show on two parametric-task toy problems and a more realistic and challenging robotic problem in simulation that PT-ME outperforms all baselines, including the deep reinforcement learning algorithm PPO.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#19968;&#20123;&#23569;&#26679;&#26412;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#35821;&#38899;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#38382;&#39064;&#19982;&#22810;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01274</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#19968;&#20123;&#23569;&#26679;&#26412;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#35821;&#38899;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#38382;&#39064;&#19982;&#22810;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#32780;&#34920;&#29616;&#20986;&#33394;&#12290;&#32463;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#23569;&#26679;&#26412;&#23398;&#20064;&#65289;&#20013;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#23613;&#31649;&#23545;&#20110;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#35780;&#20272;&#24050;&#32463;&#26377;&#20102;&#33391;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#22312;&#22768;&#23398;&#39046;&#22495;&#21364;&#26126;&#26174;&#32570;&#22833;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#19982;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#22522;&#20934;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19968;&#20123;&#23569;&#26679;&#26412;&#38382;&#39064;&#65288;&#22914;SpeechCommandsv2&#65289;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#35821;&#38899;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#38382;&#39064;&#19982;&#22810;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#30528;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#30340;&#21487;&#24494;&#20998;POGLM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;POGLM&#23398;&#20064;&#20013;&#30340;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#21644;&#21464;&#20998;&#27169;&#22411;&#35774;&#35745;&#31561;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01263</link><description>&lt;p&gt;
&#20855;&#26377;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#30340;&#21487;&#24494;&#20998;POGLM
&lt;/p&gt;
&lt;p&gt;
A Differentiable POGLM with Forward-Backward Message Passing
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01263
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#30340;&#21487;&#24494;&#20998;POGLM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;POGLM&#23398;&#20064;&#20013;&#30340;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#21644;&#21464;&#20998;&#27169;&#22411;&#35774;&#35745;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#21547;&#31070;&#32463;&#20803;&#23384;&#22312;&#30340;&#20551;&#35774;&#19979;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;POGLM&#65289;&#26159;&#29702;&#35299;&#31070;&#32463;&#36830;&#25509;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#23398;&#20064;POGLM&#65292;&#20294;&#23398;&#20064;&#36825;&#31181;&#28508;&#21464;&#37327;&#27169;&#22411;&#23384;&#22312;&#22256;&#38590;&#12290;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#65288;1&#65289;&#37319;&#26679;&#30340;&#27850;&#26494;&#38544;&#34255;&#23574;&#23792;&#25968;&#37327;&#38459;&#30861;&#20102;&#20351;&#29992;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65307;&#65288;2&#65289;&#29616;&#26377;&#30340;&#21464;&#20998;&#27169;&#22411;&#35774;&#35745;&#26082;&#19981;&#20855;&#26377;&#34920;&#36798;&#24615;&#20063;&#19981;&#20855;&#26377;&#26102;&#38388;&#25928;&#29575;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#20102;&#24615;&#33021;&#12290;&#38024;&#23545;&#38382;&#39064;&#65288;1&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;POGLM&#65292;&#21487;&#20197;&#20351;&#29992;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20248;&#20110;&#29616;&#26377;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#24471;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#38024;&#23545;&#38382;&#39064;&#65288;2&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#37319;&#26679;&#26041;&#26696;&#29992;&#20110;&#21464;&#20998;&#27169;&#22411;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21487;&#24494;&#20998;POGLM&#19982;&#25105;&#20204;&#30340;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partially observable generalized linear model (POGLM) is a powerful tool for understanding neural connectivity under the assumption of existing hidden neurons. With spike trains only recorded from visible neurons, existing works use variational inference to learn POGLM meanwhile presenting the difficulty of learning this latent variable model. There are two main issues: (1) the sampled Poisson hidden spike count hinders the use of the pathwise gradient estimator in VI; and (2) the existing design of the variational model is neither expressive nor time-efficient, which further affects the performance. For (1), we propose a new differentiable POGLM, which enables the pathwise gradient estimator, better than the score function gradient estimator used in existing works. For (2), we propose the forward-backward message-passing sampling scheme for the variational model. Comprehensive experiments show that our differentiable POGLMs with our forward-backward message passing produce a bette
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#36793;&#38469;&#25233;&#21046;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#38477;&#20302;&#36807;&#21435;&#20219;&#21153;&#30340;&#36951;&#24536;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01262</link><description>&lt;p&gt;
&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#65306;&#36890;&#36807;&#27010;&#29575;&#32553;&#25918;&#36827;&#34892;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cascaded Scaling Classifier: class incremental learning with probability scaling
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#36793;&#38469;&#25233;&#21046;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#38477;&#20302;&#36807;&#21435;&#20219;&#21153;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#33719;&#21462;&#26032;&#30693;&#35782;&#24182;&#23558;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#20165;&#26377;&#36731;&#24494;&#30340;&#36951;&#24536;&#12290;&#21516;&#26679;&#30340;&#33021;&#21147;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#36830;&#32493;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#24433;&#21709;&#21040;&#36807;&#21435;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#36951;&#24536;&#21487;&#20197;&#36890;&#36807;&#22238;&#25918;&#23384;&#20648;&#30340;&#36807;&#21435;&#20219;&#21153;&#26679;&#26412;&#26469;&#32531;&#35299;&#65292;&#20294;&#26159;&#23545;&#20110;&#38271;&#24207;&#21015;&#20219;&#21153;&#21487;&#33021;&#38656;&#35201;&#36739;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#65307;&#27492;&#22806;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#20445;&#23384;&#26679;&#26412;&#30340;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#37327;&#20998;&#31867;&#22120;&#65292;&#20998;&#21035;&#31216;&#20026;&#36793;&#38469;&#25233;&#21046;&#21644;&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#12290;&#21069;&#32773;&#32467;&#21512;&#20102;&#36719;&#32422;&#26463;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#36807;&#21435;&#23398;&#20064;&#30340;&#30693;&#35782;&#21516;&#26102;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#30340;&#27169;&#24335;&#12290;&#21518;&#32773;&#26159;&#19968;&#31181;&#24102;&#26377;&#38376;&#25511;&#30340;&#22686;&#37327;&#20998;&#31867;&#22120;&#65292;&#24110;&#21161;&#27169;&#22411;&#20462;&#25913;&#36807;&#21435;&#30340;&#39044;&#27979;&#32780;&#19981;&#30452;&#25509;&#24178;&#25200;&#23427;&#20204;&#12290;&#36825;&#26159;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting. The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones. This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples. In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier. The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively. The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them. This is achieved
&lt;/p&gt;</description></item><item><title>TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01261</link><description>&lt;p&gt;
TEDDY: &#22522;&#20110;&#24230;&#37327;&#21028;&#21035;&#31574;&#30053;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TEDDY: Trimming Edges with Degree-based Discrimination strategY
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01261
&lt;/p&gt;
&lt;p&gt;
TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Chen&#31561;&#20154;&#22312;2021&#24180;&#25552;&#20986;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20197;&#26469;&#65292;&#23547;&#25214;&#22270;&#25277;&#22870;&#31080;&#65288;GLT&#65289;&#30340;&#30740;&#31350;&#24050;&#25104;&#20026;GNN&#31038;&#21306;&#30340;&#37325;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#65292;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#29616;&#19982;&#21407;&#22987;&#23494;&#38598;&#32593;&#32476;&#30456;&#24403;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#26356;&#31232;&#30095;&#30340;GLT&#12290;&#21516;&#26102;&#65292;&#22270;&#32467;&#26500;&#20316;&#20026;GNN&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#36817;&#20960;&#39033;&#30740;&#31350;&#30340;&#38416;&#26126;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20851;&#20110;GLT&#30340;&#30740;&#31350;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#32467;&#26500;&#20013;&#30340;&#20869;&#22312;&#36335;&#24452;&#65292;&#24182;&#20197;&#36845;&#20195;&#26041;&#24335;&#35782;&#21035;&#31080;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#32791;&#26102;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;TEDDY&#65292;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#24182;&#25972;&#21512;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#19968;&#27425;&#24615;&#36793;&#32536;&#31232;&#30095;&#21270;&#26694;&#26550;&#12290;&#22312;&#36827;&#34892;&#36793;&#32536;&#31232;&#30095;&#21270;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient desc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20855;&#26377;&#36275;&#22815;&#27627;&#31859;&#27874;&#25509;&#25910;&#21151;&#29575;&#30340;&#26368;&#20339;&#27874;&#26463;&#65292;&#20351;&#29992;&#36710;&#36742;&#20301;&#32622;&#20449;&#24687;&#26469;&#23454;&#29616;&#36710;&#36742;&#21040;&#36710;&#36742;&#36890;&#20449;&#20013;&#30340;&#39640;&#25928;&#38142;&#36335;&#37197;&#32622;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01259</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20301;&#32622;&#24863;&#30693;60 GHz&#27627;&#31859;&#27874;&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#29992;&#20110;&#36710;&#36742;&#21040;&#36710;&#36742;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Position Aware 60 GHz mmWave Beamforming for V2V Communications Utilizing Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20855;&#26377;&#36275;&#22815;&#27627;&#31859;&#27874;&#25509;&#25910;&#21151;&#29575;&#30340;&#26368;&#20339;&#27874;&#26463;&#65292;&#20351;&#29992;&#36710;&#36742;&#20301;&#32622;&#20449;&#24687;&#26469;&#23454;&#29616;&#36710;&#36742;&#21040;&#36710;&#36742;&#36890;&#20449;&#20013;&#30340;&#39640;&#25928;&#38142;&#36335;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#26159;&#36890;&#36807;&#37319;&#29992;&#22823;&#35268;&#27169;&#22825;&#32447;&#38453;&#21015;&#21644;&#29983;&#25104;&#31364;&#27874;&#26463;&#26469;&#24357;&#34917;&#27627;&#31859;&#27874;&#36890;&#20449;&#20013;&#30340;&#20005;&#37325;&#36335;&#24452;&#25439;&#32791;&#65292;&#20197;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#25509;&#25910;&#21151;&#29575;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#27874;&#26463;&#36873;&#25321;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#26469;&#36827;&#34892;&#20934;&#30830;&#30340;&#27874;&#26463;&#23545;&#20934;&#65292;&#24182;&#20026;&#39640;&#25928;&#30340;&#38142;&#36335;&#37197;&#32622;&#24102;&#26469;&#26174;&#33879;&#30340;&#24310;&#36831;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#36710;&#36742;&#21040;&#36710;&#36742;&#65288;V2V&#65289;&#36890;&#20449;&#20013;&#22914;&#39640;&#24230;&#21160;&#24577;&#30340;&#22330;&#26223;&#20013;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#24102;&#22806;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#36710;&#36742;&#20301;&#32622;&#20449;&#24687;&#65289;&#26159;&#20943;&#23569;&#36825;&#31181;&#24320;&#38144;&#30340;&#28508;&#22312;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#36710;&#36742;&#20301;&#32622;&#20449;&#24687;&#26469;&#39044;&#27979;&#20855;&#26377;&#36275;&#22815;&#27627;&#31859;&#27874;&#25509;&#25910;&#21151;&#29575;&#30340;&#26368;&#20339;&#27874;&#26463;&#65292;&#20174;&#32780;&#21487;&#20197;&#20027;&#21160;&#30830;&#20445;&#26368;&#20339;&#30340;V2V&#30452;&#35270;&#38142;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beamforming techniques are considered as essential parts to compensate the severe path loss in millimeter-wave (mmWave) communications by adopting large antenna arrays and formulating narrow beams to obtain satisfactory received powers. However, performing accurate beam alignment over such narrow beams for efficient link configuration by traditional beam selection approaches, mainly relied on channel state information, typically impose significant latency and computing overheads, which is often infeasible in vehicle-to-vehicle (V2V) communications like highly dynamic scenarios. In contrast, utilizing out-of-band contextual information, such as vehicular position information, is a potential alternative to reduce such overheads. In this context, this paper presents a deep learning-based solution on utilizing the vehicular position information for predicting the optimal beams having sufficient mmWave received powers so that the best V2V line-of-sight links can be ensured proactively. Afte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#38646;&#23556;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#21033;&#29992;&#21608;&#22260;&#29615;&#22659;&#20449;&#24687;&#23545;&#27668;&#35937;&#31449;&#30340;&#31354;&#27668;&#27745;&#26579;&#29289;&#21547;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01252</link><description>&lt;p&gt;
&#38024;&#23545;&#38646;&#23556;&#22238;&#24402;&#30340;&#30446;&#26631;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Target inductive methods for zero-shot regression
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#38646;&#23556;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#21033;&#29992;&#21608;&#22260;&#29615;&#22659;&#20449;&#24687;&#23545;&#27668;&#35937;&#31449;&#30340;&#31354;&#27668;&#27745;&#26579;&#29289;&#21547;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#20986;&#21457;&#28857;&#26159;&#39044;&#27979;&#27668;&#35937;&#31449;&#30340;&#31354;&#27668;&#27745;&#26579;&#29289;&#21547;&#37327;&#12290;&#31354;&#27668;&#27745;&#26579;&#29289;&#21462;&#20915;&#20110;&#31449;&#28857;&#30340;&#20301;&#32622;&#65288;&#22825;&#27668;&#26465;&#20214;&#21644;&#21608;&#22260;&#27963;&#21160;&#65289;&#12290;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32463;&#24120;&#24573;&#30053;&#21608;&#22260;&#29615;&#22659;&#20449;&#24687;&#12290;&#22312;&#27809;&#26377;&#35266;&#27979;&#21040;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#65292;&#36825;&#20123;&#20449;&#24687;&#20107;&#20808;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#21516;&#19968;&#20010;&#31449;&#28857;&#20445;&#25345;&#19981;&#21464;&#12290;&#23558;&#21608;&#22260;&#29615;&#22659;&#20449;&#24687;&#35270;&#20026;&#38468;&#21152;&#20449;&#24687;&#26377;&#21161;&#20110;&#22312;&#26032;&#31449;&#28857;&#20013;&#39044;&#27979;&#27745;&#26579;&#29289;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#38646;&#23556;&#22238;&#24402;&#22330;&#26223;&#12290;&#30446;&#21069;&#22312;&#38646;&#23556;&#38382;&#39064;&#20013;&#23384;&#22312;&#30340;&#26041;&#27861;&#36890;&#24120;&#20559;&#21521;&#20110;&#20998;&#31867;&#65292;&#32780;&#19988;&#24456;&#38590;&#25512;&#24191;&#33267;&#22238;&#24402;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#22238;&#24402;&#30340;&#38646;&#23556;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#23427;&#20174;&#29305;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38468;&#21152;&#20449;&#24687;&#23545;&#23427;&#20204;&#36827;&#34892;&#27719;&#24635;&#12290;&#28982;&#32780;&#65292;&#22312;&#27719;&#24635;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#20002;&#22833;&#29305;&#24449;&#27169;&#22411;&#30340;&#28508;&#22312;&#30693;&#35782;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#21017;&#26159;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#29305;&#24449;&#27169;&#22411;&#21644;&#38468;&#21152;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research arises from the need to predict the amount of air pollutants in meteorological stations. Air pollution depends on the location of the stations (weather conditions and activities in the surroundings). Frequently, the surrounding information is not considered in the learning process. This information is known beforehand in the absence of unobserved weather conditions and remains constant for the same station. Considering the surrounding information as side information facilitates the generalization for predicting pollutants in new stations, leading to a zero-shot regression scenario. Available methods in zero-shot typically lean towards classification, and are not easily extensible to regression. This paper proposes two zero-shot methods for regression. The first method is a similarity based approach that learns models from features and aggregates them using side information. However, potential knowledge of the feature models may be lost in the aggregation. The second metho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#27010;&#24565;&#65292;&#21363;&#22270;&#31232;&#30095;&#35757;&#32451;&#65288;GST&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20351;&#31232;&#30095;&#22270;&#19982;&#25299;&#25169;&#21644;&#35821;&#20041;&#38170;&#28857;&#23545;&#40784;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01242</link><description>&lt;p&gt;
&#20004;&#20010;&#22836;&#32988;&#36807;&#19968;&#20010;&#65306;&#36890;&#36807;&#35821;&#20041;&#21644;&#25299;&#25169;&#24847;&#35782;&#22686;&#24378;&#22270;&#31232;&#30095;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01242
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#27010;&#24565;&#65292;&#21363;&#22270;&#31232;&#30095;&#35757;&#32451;&#65288;GST&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20351;&#31232;&#30095;&#22270;&#19982;&#25299;&#25169;&#21644;&#35821;&#20041;&#38170;&#28857;&#23545;&#40784;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#26102;&#38754;&#20020;&#35745;&#31639;&#25361;&#25112;&#12290;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#21435;&#38500;&#38750;&#24517;&#35201;&#36793;&#32536;&#65292;&#20943;&#23569;GNN&#20013;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20197;&#21069;&#30340;&#25991;&#29486;&#36890;&#24120;&#20998;&#20026;&#20004;&#31867;&#65306;&#25299;&#25169;&#24341;&#23548;&#21644;&#35821;&#20041;&#24341;&#23548;&#12290;&#21069;&#32773;&#20445;&#25345;&#26576;&#20123;&#22270;&#25299;&#25169;&#23646;&#24615;&#65292;&#20294;&#30001;&#20110;&#19982;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20302;&#38598;&#25104;&#24615;&#32780;&#22312;GNN&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#21518;&#32773;&#22312;GNN&#19978;&#30340;&#36739;&#20302;&#31232;&#30095;&#24230;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#36739;&#39640;&#31232;&#30095;&#24230;&#19979;&#38754;&#20020;&#24615;&#33021;&#23849;&#28291;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#27010;&#24565;&#65292;&#21363;&#22270;&#31232;&#30095;&#35757;&#32451;&#65288;GST&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#32423;&#21035;&#21160;&#24577;&#25805;&#20316;&#31232;&#30095;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GST&#39318;&#20808;&#20197;&#36739;&#20302;&#30340;&#35757;&#32451;&#25104;&#26412;&#26500;&#24314;&#25299;&#25169;&#21644;&#35821;&#20041;&#38170;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#25191;&#34892;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#26469;&#20351;&#31232;&#30095;&#22270;&#19982;&#38170;&#28857;&#23545;&#40784;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24179;&#34913;&#31232;&#30095;&#21270;&#21407;&#21017;&#26469;...&#65288;&#25688;&#35201;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) excel in various graph learning tasks but face computational challenges when applied to large-scale graphs. A promising solution is to remove non-essential edges to reduce the computational overheads in GNN. Previous literature generally falls into two categories: topology-guided and semantic-guided. The former maintains certain graph topological properties yet often underperforms on GNNs due to low integration with neural network training. The latter performs well at lower sparsity on GNNs but faces performance collapse at higher sparsity levels. With this in mind, we take the first step to propose a new research line and concept termed Graph Sparse Training (GST), which dynamically manipulates sparsity at the data level. Specifically, GST initially constructs a topology &amp; semantic anchor at a low training cost, followed by performing dynamic sparse training to align the sparse graph with the anchor. We introduce the Equilibria Sparsification Principle to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01240</link><description>&lt;p&gt;
&#36229;&#36234;&#35831;&#27714;&#65306;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#22312;&#19981;&#24179;&#34913;&#29615;&#22659;&#20013;&#36827;&#34892;&#36328;&#27983;&#35272;&#22120;Web&#36861;&#36394;&#22120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19975;&#32500;&#32593;&#30340;&#36830;&#36890;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;HTTP&#21327;&#35758;&#65292;&#20854;&#20013;&#30340;HTTP&#28040;&#24687;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#20449;&#24687;&#22836;&#23383;&#27573;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;Web&#36861;&#36394;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#21033;&#29992;HTTP/S&#35831;&#27714;&#28040;&#24687;&#26469;&#35782;&#21035;Web&#36861;&#36394;&#22120;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;HTTP/S&#21709;&#24212;&#22836;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#20351;&#29992;HTTP/S&#21709;&#24212;&#22836;&#36827;&#34892;Web&#36861;&#36394;&#22120;&#26816;&#27979;&#30340;&#26377;&#25928;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#27983;&#35272;&#22120;&#25193;&#23637;&#31243;&#24207;T.EX&#33719;&#21462;&#30340;Chrome&#12289;Firefox&#21644;Brave&#27983;&#35272;&#22120;&#30340;&#25968;&#25454;&#20316;&#20026;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;Chrome&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;11&#20010;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#22312;&#25152;&#26377;&#27983;&#35272;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Chrome&#21644;Firefox&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#26368;&#23567;&#23545;&#25968;&#25439;&#22833;&#35823;&#24046;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;Brave&#27983;&#35272;&#22120;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#24449;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking. Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked. This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set. Eleven supervised models were trained on Chrome data and tested across all browsers. The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set. The research suggests that these classifiers are viable for detecting w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28789;&#27963;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;FVIB&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#21363;&#21487;&#33719;&#24471;&#25152;&#26377;&#946;&#20540;&#30340;&#26368;&#20248;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#21387;&#32553;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01238</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65306;&#36890;&#36807;&#19968;&#27425;&#35757;&#32451;&#23454;&#29616;&#22810;&#26679;&#21270;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28789;&#27963;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;FVIB&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#21363;&#21487;&#33719;&#24471;&#25152;&#26377;&#946;&#20540;&#30340;&#26368;&#20248;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#21387;&#32553;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28304;&#38543;&#26426;&#21464;&#37327;&#20013;&#25552;&#21462;&#19982;&#30446;&#26631;&#38543;&#26426;&#21464;&#37327;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#946;&#65292;&#20449;&#24687;&#29942;&#39048;&#25511;&#21046;&#25968;&#25454;&#21387;&#32553;&#21644;&#39044;&#27979;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20256;&#32479;&#19978;&#65292;&#20026;&#20102;&#25214;&#21040;&#35201;&#23398;&#20064;&#30340;&#26435;&#34913;&#65292;&#20449;&#24687;&#29942;&#39048;&#38656;&#35201;&#36890;&#36807;&#22810;&#20010;&#35757;&#32451;&#21608;&#26399;&#26469;&#25628;&#32034;&#946;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28789;&#27963;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;FVIB&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#27425;&#35745;&#31639;&#39640;&#25928;&#30340;&#35757;&#32451;&#26469;&#33719;&#24471;&#25152;&#26377;&#946;&#20540;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;FVIB&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#946;&#20540;&#33539;&#22260;&#20869;&#21516;&#26102;&#26368;&#22823;&#21270;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;VIB&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;FVIB&#21487;&#20197;&#20687;VI&#19968;&#26679;&#26377;&#25928;&#22320;&#23398;&#20064;VIB&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable. In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier $\beta$. Traditionally, to find the trade-off to be learned, IB requires a search for $\beta$ through multiple training cycles, which is computationally expensive. In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of $\beta$ with single, computationally efficient training. We theoretically demonstrate that across all values of reasonable $\beta$, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method. Then we empirically show that FVIB can learn the VIB objective as effectively as VI
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24310;&#36831;&#25928;&#24212;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01231</link><description>&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24310;&#36831;&#25928;&#24212;&#25581;&#31034;&#65306;&#22522;&#20110;&#26102;&#31354;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24310;&#36831;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#20132;&#36890;&#35268;&#21010;&#21644;&#31649;&#29702;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30740;&#31350;&#38382;&#39064;&#65292;&#20063;&#26159;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#30340;&#19968;&#20010;&#20856;&#22411;&#20363;&#23376;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#22312;&#25429;&#25417;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#26377;&#20004;&#20010;&#19981;&#21487;&#24573;&#35270;&#30340;&#38382;&#39064;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#22320;&#35299;&#20915;&#65306;1&#65289;GNNs&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#26159;&#21363;&#26102;&#30340;&#65292;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#31354;&#38388;&#28040;&#24687;&#20132;&#20114;&#21487;&#33021;&#23384;&#22312;&#24310;&#36831;&#12290;&#20132;&#36890;&#27969;&#37327;&#22312;&#19968;&#20010;&#33410;&#28857;&#19978;&#30340;&#21464;&#21270;&#38656;&#35201;&#20960;&#20998;&#38047;&#30340;&#26102;&#38388;&#24310;&#36831;&#65292;&#25165;&#33021;&#24433;&#21709;&#20854;&#30456;&#36830;&#30340;&#37051;&#23621;&#33410;&#28857;&#12290;2&#65289;&#20132;&#36890;&#29366;&#20917;&#19981;&#26029;&#21464;&#21270;&#12290;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#39044;&#27979;&#39057;&#29575;&#21487;&#33021;&#26681;&#25454;&#20855;&#20307;&#22330;&#26223;&#35201;&#27714;&#32780;&#21464;&#21270;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31163;&#25955;&#27169;&#22411;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#39044;&#27979;&#26102;&#38388;&#27573;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#26469;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24310;&#36831;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow forecasting is a fundamental research issue for transportation planning and management, which serves as a canonical and typical example of spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) have achieved great success in capturing spatial-temporal correlations for traffic flow forecasting. Yet, two non-ignorable issues haven't been well solved: 1) The message passing in GNNs is immediate, while in reality the spatial message interactions among neighboring nodes can be delayed. The change of traffic flow at one node will take several minutes, i.e., time delay, to influence its connected neighbors. 2) Traffic conditions undergo continuous changes. The prediction frequency for traffic flow forecasting may vary based on specific scenario requirements. Most existing discretized models require retraining for each prediction horizon, restricting their applicability. To tackle the above issues, we propose a neural Spati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#30340;&#38544;&#31169;&#20445;&#25252;&#20154;&#25968;&#35745;&#25968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;DNN&#20248;&#21270;&#27969;&#31243;&#65292;&#21253;&#25324;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12289;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21644;&#21518;&#22788;&#29702;&#31561;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#26032;&#30340;&#26234;&#33021;&#20256;&#24863;&#22120;&#21407;&#22411;&#65292;&#22312;&#33021;&#32791;&#12289;&#20869;&#23384;&#21644;&#20934;&#30830;&#24615;&#36825;&#19977;&#20010;&#32500;&#24230;&#19978;&#24471;&#21040;&#20102;&#22823;&#37327;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01226</link><description>&lt;p&gt;
&#38024;&#23545;&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#30340;&#38544;&#31169;&#20445;&#25252;&#20154;&#25968;&#35745;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#30828;&#20214;-&#36719;&#20214;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
HW-SW Optimization of DNNs for Privacy-preserving People Counting on Low-resolution Infrared Arrays
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#30340;&#38544;&#31169;&#20445;&#25252;&#20154;&#25968;&#35745;&#25968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;DNN&#20248;&#21270;&#27969;&#31243;&#65292;&#21253;&#25324;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12289;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21644;&#21518;&#22788;&#29702;&#31561;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#26032;&#30340;&#26234;&#33021;&#20256;&#24863;&#22120;&#21407;&#22411;&#65292;&#22312;&#33021;&#32791;&#12289;&#20869;&#23384;&#21644;&#20934;&#30830;&#24615;&#36825;&#19977;&#20010;&#32500;&#24230;&#19978;&#24471;&#21040;&#20102;&#22823;&#37327;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#20256;&#24863;&#22120;&#21487;&#20197;&#23454;&#29616;&#30417;&#27979;&#31354;&#38388;&#21344;&#29992;&#21644;&#20154;&#21592;&#27969;&#21160;&#31561;&#20154;&#25968;&#35745;&#25968;&#24212;&#29992;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#20197;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#22788;&#29702;&#36825;&#20123;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;DNN&#32467;&#26500;&#30340;&#31354;&#38388;&#24040;&#22823;&#65292;&#25163;&#21160;&#25506;&#32034;&#26159;&#32321;&#29712;&#30340;&#65292;&#24182;&#19988;&#24448;&#24448;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;DNN&#20840;&#26632;&#20248;&#21270;&#27969;&#31243;&#65292;&#20174;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12289;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21644;&#21518;&#22788;&#29702;&#65292;&#21040;&#23454;&#29616;&#21253;&#25324;&#33258;&#23450;&#20041;&#25351;&#20196;&#38598;&#30340;&#26032;&#22411;&#26234;&#33021;&#20256;&#24863;&#22120;&#21407;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#20132;&#21449;&#23618;&#20248;&#21270;&#65292;&#25105;&#20204;&#22312;&#33021;&#32791;&#12289;&#20869;&#23384;&#21644;&#20934;&#30830;&#24615;&#30340;&#19977;&#32500;&#31354;&#38388;&#33719;&#24471;&#20102;&#22823;&#37327;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;&#23558;&#36825;&#20123;&#35299;&#37096;&#32626;&#22312;&#25105;&#20204;&#30340;&#30828;&#20214;&#24179;&#21488;&#19978;&#65292;&#25105;&#20204;&#25913;&#21892;&#20102;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-resolution infrared (IR) array sensors enable people counting applications such as monitoring the occupancy of spaces and people flows while preserving privacy and minimizing energy consumption. Deep Neural Networks (DNNs) have been shown to be well-suited to process these sensor data in an accurate and efficient manner. Nevertheless, the space of DNNs' architectures is huge and its manual exploration is burdensome and often leads to sub-optimal solutions. To overcome this problem, in this work, we propose a highly automated full-stack optimization flow for DNNs that goes from neural architecture search, mixed-precision quantization, and post-processing, down to the realization of a new smart sensor prototype, including a Microcontroller with a customized instruction set. Integrating these cross-layer optimizations, we obtain a large set of Pareto-optimal solutions in the 3D-space of energy, memory, and accuracy. Deploying such solutions on our hardware platform, we improve the sta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#38477;&#38632;&#39044;&#27979;&#20013;&#30340;&#20301;&#32622;&#24046;&#24322;&#21644;&#27668;&#20505;&#21464;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#24052;&#40654;&#12289;&#27931;&#26441;&#30710;&#21644;&#19996;&#20140;&#36827;&#34892;&#36866;&#24212;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#20998;&#21035;&#25552;&#39640;&#20102;43.51%&#12289;5.09%&#21644;38.62%&#12290;</title><link>https://rss.arxiv.org/abs/2402.01208</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20301;&#32622;&#26080;&#20851;&#33258;&#36866;&#24212;&#38477;&#38632;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Location Agnostic Adaptive Rain Precipitation Prediction using Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#38477;&#38632;&#39044;&#27979;&#20013;&#30340;&#20301;&#32622;&#24046;&#24322;&#21644;&#27668;&#20505;&#21464;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#24052;&#40654;&#12289;&#27931;&#26441;&#30710;&#21644;&#19996;&#20140;&#36827;&#34892;&#36866;&#24212;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#20998;&#21035;&#25552;&#39640;&#20102;43.51%&#12289;5.09%&#21644;38.62%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#38632;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#22240;&#22320;&#32780;&#24322;&#30340;&#27668;&#20505;&#21644;&#27668;&#35937;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22312;&#19968;&#20010;&#20301;&#32622;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#20854;&#20182;&#20301;&#32622;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20840;&#29699;&#21464;&#26262;&#65292;&#22825;&#27668;&#27169;&#24335;&#24180;&#22797;&#19968;&#24180;&#22320;&#21457;&#29983;&#30528;&#24555;&#36895;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21363;&#20351;&#22312;&#30456;&#21516;&#20301;&#32622;&#65292;&#26102;&#38388;&#36807;&#21435;&#21518;&#37027;&#20123;&#27169;&#22411;&#20063;&#21464;&#24471;&#26080;&#25928;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25512;&#24191;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#37027;&#20123;&#27809;&#26377;&#36866;&#24212;&#26426;&#21046;&#30340;&#26041;&#27861;&#26080;&#27861;&#39044;&#27979;&#38477;&#27700;&#30340;&#20219;&#20309;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32463;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36866;&#24212;&#21518;&#65292;&#22312;&#24052;&#40654;&#12289;&#27931;&#26441;&#30710;&#21644;&#19996;&#20140;&#30340;&#38477;&#27700;&#39044;&#27979;&#26041;&#38754;&#20998;&#21035;&#26174;&#31034;&#20102;43.51%&#12289;5.09%&#21644;38.62%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rain precipitation prediction is a challenging task as it depends on weather and meteorological features which vary from location to location. As a result, a prediction model that performs well at one location does not perform well at other locations due to the distribution shifts. In addition, due to global warming, the weather patterns are changing very rapidly year by year which creates the possibility of ineffectiveness of those models even at the same location as time passes. In our work, we have proposed an adaptive deep learning-based framework in order to provide a solution to the aforementioned challenges. Our method can generalize the model for the prediction of precipitation for any location where the methods without adaptation fail. Our method has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a deep neural network for predicting the precipitation of Paris, Los Angeles, and Tokyo, respectively.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01207</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Causal Graph Discovery Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#20043;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25104;&#23545;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#38656;&#35201;&#20108;&#27425;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;&#36739;&#22823;&#30340;&#22240;&#26524;&#22270;&#26469;&#35828;&#24456;&#24555;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#26377;&#25152;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#32467;&#21512;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#20855;&#26102;&#38388;&#21644;&#25968;&#25454;&#25928;&#29575;&#22806;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#38477;&#27700;&#21644;&#28201;&#24230;&#39044;&#27979;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01206</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22825;&#27668;&#39044;&#27979;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comparative Evaluation of Weather Forecasting using Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#38477;&#27700;&#21644;&#28201;&#24230;&#39044;&#27979;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#30340;&#28145;&#20837;&#29702;&#35299;&#21644;&#23545;&#20854;&#26410;&#26469;&#34892;&#20026;&#30340;&#39044;&#27979;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#25512;&#21160;&#31038;&#20250;&#21457;&#23637;&#30340;&#37325;&#35201;&#21162;&#21147;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25506;&#32034;&#20102;&#22312;&#22825;&#27668;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#29702;&#35299;&#21644;&#39044;&#27979;&#33258;&#28982;&#34892;&#20026;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#25366;&#25496;&#21644;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#21147;&#37327;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#26469;&#33258;&#36798;&#21345;&#24066;&#19968;&#20010;&#27668;&#35937;&#31449;20&#24180;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#65292;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#38477;&#27700;&#21644;&#28201;&#24230;&#27169;&#24335;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#35780;&#20272;&#26799;&#24230;&#25552;&#21319;&#12289;AdaBoost&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21472;&#21152;&#38543;&#26426;&#26862;&#26519;&#12289;&#21472;&#21152;&#31070;&#32463;&#32593;&#32476;&#21644;&#21472;&#21152;KNN&#31561;&#31639;&#27861;&#65292;&#22522;&#20110;&#24615;&#33021;&#25351;&#26631;&#65288;&#21253;&#25324;&#28151;&#28102;&#30697;&#38453;&#27979;&#37327;&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaining a deeper understanding of weather and being able to predict its future conduct have always been considered important endeavors for the growth of our society. This research paper explores the advancements in understanding and predicting nature's behavior, particularly in the context of weather forecasting, through the application of machine learning algorithms. By leveraging the power of machine learning, data mining, and data analysis techniques, significant progress has been made in this field. This study focuses on analyzing the contributions of various machine learning algorithms in predicting precipitation and temperature patterns using a 20-year dataset from a single weather station in Dhaka city. Algorithms such as Gradient Boosting, AdaBoosting, Artificial Neural Network, Stacking Random Forest, Stacking Neural Network, and Stacking KNN are evaluated and compared based on their performance metrics, including Confusion matrix measurements. The findings highlight remarkabl
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01204</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;SSL&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#21270;&#21644;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;SSL&#24050;&#25104;&#20026;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#26032;&#36235;&#21183;&#65292;&#36825;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#25551;&#36848;&#24615;&#30340;&#34920;&#31034;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#22238;&#39038;&#21644;&#24635;&#32467;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#65288;SSL4NS-TD&#65289;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;NS-TD&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26041;&#27861;&#34987;&#20998;&#20026;&#19977;&#32452;&#8212;&#8212;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#24182;&#20171;&#32461;&#20102;&#27599;&#20010;&#26041;&#21521;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#21160;&#26426;&#21644;&#20248;&#28857;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#20171;&#32461;&#20102;SSL4NS-TD&#30340;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#35821;&#20041;&#31070;&#32463;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#25552;&#20379;&#34917;&#19969;&#32423;&#21035;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36880;&#23618;&#26500;&#24314;&#22330;&#26223;&#34920;&#31034;&#21644;&#35757;&#32451;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#35821;&#20041;&#19990;&#30028;&#24314;&#27169;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01203</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#19990;&#30028;&#24314;&#27169;&#36890;&#36807;&#35821;&#20041;&#21521;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Structured World Modeling via Semantic Vector Quantization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01203
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#35821;&#20041;&#31070;&#32463;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#25552;&#20379;&#34917;&#19969;&#32423;&#21035;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36880;&#23618;&#26500;&#24314;&#22330;&#26223;&#34920;&#31034;&#21644;&#35757;&#32451;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#35821;&#20041;&#19990;&#30028;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31163;&#25955;&#34920;&#31034;&#26159;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#20027;&#35201;&#31574;&#30053;&#65288;&#22914;VQ-VAE&#65289;&#21482;&#33021;&#25552;&#20379;&#34917;&#19969;&#32423;&#21035;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#65292;&#21363;&#33719;&#21462;&#32467;&#26500;&#21270;&#12289;&#35821;&#20041;&#21270;&#21644;&#32452;&#21512;&#25277;&#35937;&#65288;&#22914;&#23545;&#35937;&#30340;&#39068;&#33394;&#21644;&#24418;&#29366;&#65289;&#65292;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#31070;&#32463;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;Semantic Vector-Quantized Variational Autoencoder (SVQ)&#65292;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#26469;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23545;&#35937;&#32423;&#21035;&#19978;&#31616;&#21333;&#36827;&#34892;&#37327;&#21270;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#20302;&#32423;&#31163;&#25955;&#27010;&#24565;&#27169;&#24335;&#21040;&#23545;&#35937;&#34920;&#31034;&#36880;&#23618;&#26500;&#24314;&#22330;&#26223;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32467;&#26500;&#21270;&#35821;&#20041;&#19990;&#30028;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26469;&#23454;&#29616;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural discrete representations are crucial components of modern neural networks. However, their main limitation is that the primary strategies such as VQ-VAE can only provide representations at the patch level. Therefore, one of the main goals of representation learning, acquiring structured, semantic, and compositional abstractions such as the color and shape of an object, remains elusive. In this paper, we present the first approach to semantic neural discrete representation learning. The proposed model, called Semantic Vector-Quantized Variational Autoencoder (SVQ), leverages recent advances in unsupervised object-centric learning to address this limitation. Specifically, we observe that a simple approach quantizing at the object level poses a significant challenge and propose constructing scene representations hierarchically, from low-level discrete concept schemas to object representations. Additionally, we suggest a novel method for structured semantic world modeling by training
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20174;&#21518;&#32493;&#22686;&#37327;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#65292;&#19982;&#24102;&#26631;&#31614;&#30340;&#22522;&#31867;&#26679;&#26412;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#20026;&#26087;&#31867;&#21644;&#26032;&#31867;&#25968;&#25454;&#20998;&#37197;&#23884;&#20837;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38887;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01201</link><description>&lt;p&gt;
&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Class-Incremental Learning with Prior Knowledge
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20174;&#21518;&#32493;&#22686;&#37327;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#65292;&#19982;&#24102;&#26631;&#31614;&#30340;&#22522;&#31867;&#26679;&#26412;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#20026;&#26087;&#31867;&#21644;&#26032;&#31867;&#25968;&#25454;&#20998;&#37197;&#23884;&#20837;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;(FSCIL)&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#22686;&#37327;&#38454;&#27573;&#20445;&#30041;&#26087;&#30693;&#35782;&#30340;&#35760;&#24518;&#19978;&#12290;&#36825;&#20123;&#30740;&#31350;&#32463;&#24120;&#20302;&#20272;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22609;&#36896;&#22686;&#37327;&#23398;&#20064;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#23398;&#20064;(LwPK)&#65292;&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#21518;&#32493;&#22686;&#37327;&#31867;&#21035;&#20013;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20960;&#20046;&#33258;&#30001;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#23558;&#26080;&#26631;&#31614;&#30340;&#22686;&#37327;&#31867;&#26679;&#26412;&#32858;&#31867;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#24182;&#19982;&#24102;&#26631;&#31614;&#30340;&#22522;&#31867;&#26679;&#26412;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#20026;&#26087;&#31867;&#21644;&#26032;&#31867;&#25968;&#25454;&#20998;&#37197;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LwPK&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38887;&#24615;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#31867;&#38388;&#36317;&#31163;&#24230;&#37327;&#30340;&#29702;&#35770;&#20998;&#26512;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
To tackle the issues of catastrophic forgetting and overfitting in few-shot class-incremental learning (FSCIL), previous work has primarily concentrated on preserving the memory of old knowledge during the incremental phase. The role of pre-trained model in shaping the effectiveness of incremental learning is frequently underestimated in these studies. Therefore, to enhance the generalization ability of the pre-trained model, we propose Learning with Prior Knowledge (LwPK) by introducing nearly free prior knowledge from a few unlabeled data of subsequent incremental classes. We cluster unlabeled incremental class samples to produce pseudo-labels, then jointly train these with labeled base class samples, effectively allocating embedding space for both old and new class data. Experimental results indicate that LwPK effectively enhances the model resilience against catastrophic forgetting, with theoretical analysis based on empirical risk minimization and class distance measurement corrob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#31890;&#21270;&#20998;&#23376;&#34920;&#31034;&#20013;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#37319;&#26679;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#25928;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01195</link><description>&lt;p&gt;
&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#29992;&#20110;&#31895;&#31890;&#21270;&#20998;&#23376;&#34920;&#31034;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#31890;&#21270;&#20998;&#23376;&#34920;&#31034;&#20013;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#37319;&#26679;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#25928;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#37319;&#26679;&#20998;&#23376;&#31995;&#32479;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#19982;&#29983;&#25104;&#38271;&#26102;&#38388;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#19981;&#21516;&#65292;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22914;&#27491;&#21017;&#21270;&#27969;&#34987;&#29992;&#20110;&#30452;&#25509;&#23398;&#20064;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#32780;&#19981;&#38656;&#35201;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#20986;&#29616;&#27169;&#24335;&#23849;&#28291;&#65292;&#22240;&#27492;&#24120;&#24120;&#26080;&#27861;&#25506;&#32034;&#20840;&#37096;&#30340;&#26500;&#22411;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#65292;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#33258;&#30001;&#24230;&#12290;&#22312;&#31895;&#31890;&#21270;&#31354;&#38388;&#19978;&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#21487;&#20197;&#20135;&#29983;&#20004;&#20010;&#23618;&#27425;&#20043;&#38388;&#30340;&#27010;&#29575;&#36830;&#25509;&#12290;&#20026;&#20102;&#25506;&#32034;&#26500;&#22411;&#31354;&#38388;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31895;&#31890;&#21270;&#27169;&#25311;&#19982;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24517;&#35201;&#26102;&#26356;&#26032;&#27969;&#24182;&#36827;&#34892;&#20840;&#21407;&#23376;&#21183;&#33021;&#35780;&#20272;&#12290;&#20197;&#19993;&#27688;&#37240;&#20108;&#32957;&#20026;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient sampling of the Boltzmann distribution of molecular systems is a long-standing challenge. Recently, instead of generating long molecular dynamics simulations, generative machine learning methods such as normalizing flows have been used to learn the Boltzmann distribution directly, without samples. However, this approach is susceptible to mode collapse and thus often does not explore the full configurational space. In this work, we address this challenge by separating the problem into two levels, the fine-grained and coarse-grained degrees of freedom. A normalizing flow conditioned on the coarse-grained space yields a probabilistic connection between the two levels. To explore the configurational space, we employ coarse-grained simulations with active learning which allows us to update the flow and make all-atom potential energy evaluations only when necessary. Using alanine dipeptide as an example, we show that our methods obtain a speedup to molecular dynamics simulations of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#30340;&#25552;&#31034;&#32531;&#23384;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LMMs)&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33976;&#39311;&#26041;&#27861;&#26469;&#20248;&#21270;&#29616;&#26377;&#23884;&#20837;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32531;&#23384;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01173</link><description>&lt;p&gt;
&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#23454;&#29616;&#39640;&#25928;&#30340;&#25552;&#31034;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Efficient Prompt Caching via Embedding Similarity
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#30340;&#25552;&#31034;&#32531;&#23384;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LMMs)&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33976;&#39311;&#26041;&#27861;&#26469;&#20248;&#21270;&#29616;&#26377;&#23884;&#20837;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32531;&#23384;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#23427;&#38754;&#20020;&#30528;&#36164;&#28304;&#28040;&#32791;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#31034;&#32531;&#23384;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21363;&#65292;&#22914;&#26524;&#24403;&#21069;&#25552;&#31034;&#21487;&#20197;&#30001;&#21069;&#19968;&#20010;&#25552;&#31034;&#30340;&#21516;&#26679;&#22238;&#31572;&#32780;&#24471;&#21040;&#22238;&#31572;&#65292;&#23601;&#21487;&#20197;&#30452;&#25509;&#21033;&#29992;&#35813;&#21069;&#19968;&#20010;&#22238;&#31572;&#32780;&#19981;&#35843;&#29992;LLMs&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#26469;&#25552;&#21319;&#21333;&#36718;&#38382;&#31572;&#20219;&#21153;&#30340;&#32531;&#23384;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#24050;&#26377;&#30340;&#25552;&#31034;&#23884;&#20837;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#25552;&#31034;&#26159;&#21542;&#35821;&#20041;&#30456;&#20284;&#65292;&#36825;&#19982;&#21516;&#26679;&#30340;&#22238;&#31572;&#26159;&#21542;&#21487;&#20197;&#22238;&#31572;&#23427;&#20204;&#24182;&#19981;&#31561;&#20215;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#29616;&#26377;&#30340;&#23884;&#20837;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32531;&#23384;&#39044;&#27979;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#19979;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#25910;&#25947;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved huge success in numerous natural language process (NLP) tasks. However, it faces the challenge of significant resource consumption during inference. In this paper, we aim to improve the inference efficiency of LLMs by prompt caching, i.e., if the current prompt can be answered by the same response of a previous prompt, one can directly utilize that previous response without calling the LLM. Specifically, we focus on the prediction accuracy of prompt caching for single-round question-answering tasks via embedding similarity. The existing embeddings of prompts mostly focus on whether two prompts are semantically similar, which is not necessarily equivalent to whether the same response can answer them. Therefore, we propose a distillation-based method to fine-tune the existing embeddings for better caching prediction. Theoretically, we provide finite-sample guarantees for the convergence of our method under different types of loss functions. Empi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#38750;&#22343;&#21248;&#37327;&#21270;&#30340;&#20998;&#24067;&#24335;SGD&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25130;&#26029;&#26469;&#20943;&#36731;&#38271;&#23614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#26681;&#25454;&#26799;&#24230;&#30340;&#32479;&#35745;&#29305;&#24615;&#36827;&#34892;&#38750;&#22343;&#21248;&#37327;&#21270;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24179;&#34913;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01160</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;SGD&#30340;&#25130;&#26029;&#38750;&#22343;&#21248;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Truncated Non-Uniform Quantization for Distributed SGD
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01160
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#38750;&#22343;&#21248;&#37327;&#21270;&#30340;&#20998;&#24067;&#24335;SGD&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25130;&#26029;&#26469;&#20943;&#36731;&#38271;&#23614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#26681;&#25454;&#26799;&#24230;&#30340;&#32479;&#35745;&#29305;&#24615;&#36827;&#34892;&#38750;&#22343;&#21248;&#37327;&#21270;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#37327;&#21270;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#25130;&#26029;&#26469;&#20943;&#36731;&#38271;&#23614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#26681;&#25454;&#26799;&#24230;&#30340;&#32479;&#35745;&#29305;&#24615;&#36827;&#34892;&#38750;&#22343;&#21248;&#37327;&#21270;&#12290;&#25105;&#20204;&#23545;&#37327;&#21270;&#30340;&#20998;&#24067;&#24335;SGD&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20026;&#20854;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25910;&#25947;&#35823;&#24046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32473;&#23450;&#36890;&#20449;&#32422;&#26463;&#19979;&#30340;&#25130;&#26029;&#38408;&#20540;&#21644;&#38750;&#22343;&#21248;&#37327;&#21270;&#27700;&#24179;&#30340;&#26368;&#20248;&#38381;&#24335;&#35299;&#12290;&#29702;&#35770;&#27934;&#35265;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#26696;&#65292;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the communication bottleneck challenge in distributed learning, our work introduces a novel two-stage quantization strategy designed to enhance the communication efficiency of distributed Stochastic Gradient Descent (SGD). The proposed method initially employs truncation to mitigate the impact of long-tail noise, followed by a non-uniform quantization of the post-truncation gradients based on their statistical characteristics. We provide a comprehensive convergence analysis of the quantized distributed SGD, establishing theoretical guarantees for its performance. Furthermore, by minimizing the convergence error, we derive optimal closed-form solutions for the truncation threshold and non-uniform quantization levels under given communication constraints. Both theoretical insights and extensive experimental evaluations demonstrate that our proposed algorithm outperforms existing quantization schemes, striking a superior balance between communication efficiency and convergence 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#20998;&#31867;&#22120;&#22312;Sobolev&#31354;&#38388;&#20013;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#23545;&#26465;&#20214;&#27010;&#29575;&#30340;&#20551;&#35774;&#21644;&#26680;&#22238;&#24402;&#29702;&#35770;&#30340;&#24212;&#29992;&#65292;&#23548;&#20986;&#20102;&#26680;&#20998;&#31867;&#22120;&#30340;&#20998;&#31867;&#36229;&#39069;&#39118;&#38505;&#19978;&#30028;&#21644;Sobolev&#31354;&#38388;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#20272;&#35745;&#25554;&#20540;&#24179;&#28369;&#24230;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01148</link><description>&lt;p&gt;
Sobolev&#31354;&#38388;&#20013;&#26680;&#20998;&#31867;&#22120;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Optimality of Kernel Classifiers in Sobolev Space
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#20998;&#31867;&#22120;&#22312;Sobolev&#31354;&#38388;&#20013;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#23545;&#26465;&#20214;&#27010;&#29575;&#30340;&#20551;&#35774;&#21644;&#26680;&#22238;&#24402;&#29702;&#35770;&#30340;&#24212;&#29992;&#65292;&#23548;&#20986;&#20102;&#26680;&#20998;&#31867;&#22120;&#30340;&#20998;&#31867;&#36229;&#39069;&#39118;&#38505;&#19978;&#30028;&#21644;Sobolev&#31354;&#38388;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#20272;&#35745;&#25554;&#20540;&#24179;&#28369;&#24230;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26680;&#20998;&#31867;&#30340;&#29702;&#35770;&#20998;&#26512;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#20998;&#31867;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#12290;&#22312;&#23545;&#26465;&#20214;&#27010;&#29575;$\eta(x)=\mathbb{P}(Y=1\mid X=x)$&#20316;&#20986;&#19968;&#20123;&#28201;&#21644;&#30340;&#20551;&#35774;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26680;&#22238;&#24402;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23548;&#20986;&#20102;&#26680;&#20998;&#31867;&#22120;&#20998;&#31867;&#36229;&#39069;&#39118;&#38505;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;Sobolev&#31354;&#38388;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#20998;&#31867;&#22120;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#21487;&#20197;&#25193;&#23637;&#21040;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26356;&#21152;&#36866;&#29992;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;$2\eta(x)-1$&#25554;&#20540;&#24179;&#28369;&#24230;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel methods are widely used in machine learning, especially for classification problems. However, the theoretical analysis of kernel classification is still limited. This paper investigates the statistical performances of kernel classifiers. With some mild assumptions on the conditional probability $\eta(x)=\mathbb{P}(Y=1\mid X=x)$, we derive an upper bound on the classification excess risk of a kernel classifier using recent advances in the theory of kernel regression. We also obtain a minimax lower bound for Sobolev spaces, which shows the optimality of the proposed classifier. Our theoretical results can be extended to the generalization error of overparameterized neural network classifiers. To make our theoretical results more applicable in realistic settings, we also propose a simple method to estimate the interpolation smoothness of $2\eta(x)-1$ and apply the method to real datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24322;&#26500;&#25490;&#38431;&#31995;&#32479;&#20013;&#20316;&#19994;&#36335;&#30001;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;ACHQ&#31639;&#27861;&#65292;&#36890;&#36807;&#20302;&#32500;&#24230;&#30340;&#36719;&#38408;&#20540;&#31574;&#30053;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31995;&#32479;&#30340;&#25490;&#38431;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ACHQ&#31639;&#27861;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01147</link><description>&lt;p&gt;
&#24322;&#26500;&#25490;&#38431;&#31995;&#32479;&#20013;&#29992;&#20110;&#36335;&#30001;&#20316;&#19994;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Routing Jobs in Heterogeneous Queueing Systems
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24322;&#26500;&#25490;&#38431;&#31995;&#32479;&#20013;&#20316;&#19994;&#36335;&#30001;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;ACHQ&#31639;&#27861;&#65292;&#36890;&#36807;&#20302;&#32500;&#24230;&#30340;&#36719;&#38408;&#20540;&#31574;&#30053;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31995;&#32479;&#30340;&#25490;&#38431;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ACHQ&#31639;&#27861;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#23558;&#21040;&#36798;&#20013;&#22830;&#38431;&#21015;&#30340;&#20316;&#19994;&#39640;&#25928;&#36335;&#30001;&#21040;&#24322;&#26500;&#26381;&#21153;&#22120;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#12290;&#19982;&#21516;&#36136;&#31995;&#32479;&#19981;&#21516;&#65292;&#23545;&#20110;&#19968;&#24555;&#19968;&#24930;&#30340;&#21452;&#26381;&#21153;&#22120;&#31995;&#32479;&#65292;&#24050;&#30693;&#38408;&#20540;&#31574;&#30053;&#65292;&#21363;&#22312;&#38431;&#21015;&#38271;&#24230;&#36229;&#36807;&#26576;&#19968;&#38408;&#20540;&#26102;&#23558;&#20316;&#19994;&#36335;&#30001;&#21040;&#24930;&#26381;&#21153;&#22120;&#65292;&#26159;&#26368;&#20248;&#31574;&#30053;&#12290;&#20294;&#22810;&#26381;&#21153;&#22120;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#26410;&#30693;&#19988;&#38590;&#20197;&#25214;&#21040;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#34987;&#35748;&#20026;&#23545;&#20110;&#23398;&#20064;&#27492;&#31867;&#31574;&#30053;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#38382;&#39064;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#65292;&#20351;&#26631;&#20934;RL&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACHQ&#65292;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20855;&#26377;&#20302;&#32500;&#24230;&#30340;&#36719;&#38408;&#20540;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#24213;&#23618;&#25490;&#38431;&#32467;&#26500;&#12290;&#25105;&#20204;&#20026;&#19968;&#33324;&#24773;&#20917;&#25552;&#20379;&#20102;&#31283;&#24577;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#23613;&#31649;&#21442;&#25968;&#21270;&#32500;&#24230;&#36739;&#20302;&#65292;&#20294;&#35777;&#26126;&#20102;ACHQ&#25910;&#25947;&#21040;&#36817;&#20284;&#20840;&#23616;&#26368;&#20248;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of efficiently routing jobs that arrive into a central queue to a system of heterogeneous servers. Unlike homogeneous systems, a threshold policy, that routes jobs to the slow server(s) when the queue length exceeds a certain threshold, is known to be optimal for the one-fast-one-slow two-server system. But an optimal policy for the multi-server system is unknown and non-trivial to find. While Reinforcement Learning (RL) has been recognized to have great potential for learning policies in such cases, our problem has an exponentially large state space size, rendering standard RL inefficient. In this work, we propose ACHQ, an efficient policy gradient based algorithm with a low dimensional soft threshold policy parameterization that leverages the underlying queueing structure. We provide stationary-point convergence guarantees for the general case and despite the low-dimensional parameterization prove that ACHQ converges to an approximate global optimum for the sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#24179;&#22343;&#30340;&#26680;&#21270;&#37197;&#23545;&#23398;&#20064;&#30340;&#26377;&#38480;&#20869;&#23384;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#37197;&#23545;&#23398;&#20064;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#38271;&#38382;&#39064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#31034;&#20363;&#30340;&#29420;&#31435;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01146</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#24179;&#22343;&#30340;&#26680;&#21270;&#37197;&#23545;&#23398;&#20064;&#30340;&#26377;&#38480;&#20869;&#23384;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Limited Memory Online Gradient Descent for Kernelized Pairwise Learning with Dynamic Averaging
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#24179;&#22343;&#30340;&#26680;&#21270;&#37197;&#23545;&#23398;&#20064;&#30340;&#26377;&#38480;&#20869;&#23384;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#37197;&#23545;&#23398;&#20064;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#38271;&#38382;&#39064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#31034;&#20363;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#23545;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#22788;&#29702;&#30340;&#26159;&#22522;&#20110;&#35757;&#32451;&#31034;&#20363;&#23545;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#24230;&#37327;&#23398;&#20064;&#21644;AUC&#26368;&#22823;&#21270;&#31561;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#38543;&#30528;&#26679;&#26412;&#22823;&#23567;&#22686;&#38271;&#32780;&#24102;&#26469;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#20108;&#27425;&#22686;&#38271;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;OGD&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20808;&#21069;&#21644;&#26368;&#36817;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#26799;&#24230;&#35745;&#31639;&#65292;&#23558;&#31639;&#27861;&#22797;&#26434;&#24230;&#26377;&#25928;&#38477;&#20302;&#21040;$O(T)$&#65292;&#20854;&#20013;$T$&#26159;&#25509;&#25910;&#21040;&#30340;&#31034;&#20363;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#38480;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#21516;&#26102;&#20551;&#35774;&#31034;&#20363;&#30340;&#21040;&#36798;&#26159;&#29420;&#31435;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;OGD&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#31034;&#20363;&#30340;&#29420;&#31435;&#24615;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#26680;&#21270;&#37197;&#23545;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#19968;&#20010;&#38543;&#26426;&#31034;&#20363;&#21644;&#20195;&#34920;&#36807;&#21435;&#25968;&#25454;&#30340;&#31227;&#21160;&#24179;&#22343;&#25968;&#26500;&#24314;&#26799;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20122;&#32447;&#24615;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairwise learning, an important domain within machine learning, addresses loss functions defined on pairs of training examples, including those in metric learning and AUC maximization. Acknowledging the quadratic growth in computation complexity accompanying pairwise loss as the sample size grows, researchers have turned to online gradient descent (OGD) methods for enhanced scalability. Recently, an OGD algorithm emerged, employing gradient computation involving prior and most recent examples, a step that effectively reduces algorithmic complexity to $O(T)$, with $T$ being the number of received examples. This approach, however, confines itself to linear models while assuming the independence of example arrivals. We introduce a lightweight OGD algorithm that does not require the independence of examples and generalizes to kernel pairwise learning. Our algorithm builds the gradient based on a random example and a moving average representing the past data, which results in a sub-linear r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01143</link><description>&lt;p&gt;
&#29992;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Network Representations with Disentangled Graph Auto-Encoder
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
(&#21464;&#20998;)&#22270;&#33258;&#32534;&#30721;&#22120;&#24191;&#27867;&#29992;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#24418;&#25104;&#26159;&#19968;&#20010;&#30001;&#28508;&#22312;&#22240;&#32032;&#24433;&#21709;&#30340;&#22797;&#26434;&#21644;&#24322;&#36136;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;&#22522;&#26412;&#19978;&#26159;&#25972;&#20307;&#30340;&#65292;&#24573;&#35270;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#32416;&#32544;&#12290;&#36825;&#19981;&#20165;&#20351;&#24471;&#22270;&#20998;&#26512;&#20219;&#21153;&#19981;&#22826;&#26377;&#25928;&#65292;&#32780;&#19988;&#20351;&#24471;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#34920;&#31034;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#29992;(&#21464;&#20998;)&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#35299;&#32544;&#30340;&#22270;&#34920;&#31034;&#38754;&#20020;&#30528;&#37325;&#35201;&#25361;&#25112;&#65292;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#35299;&#32544;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#20351;&#29992;&#22810;&#36890;&#36947;&#28040;&#24687;&#20256;&#36882;&#23618;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#32858;&#21512;&#19982;&#27599;&#20010;&#33410;&#28857;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The (variational) graph auto-encoder is extensively employed for learning representations of graph-structured data. However, the formation of real-world graphs is a complex and heterogeneous process influenced by latent factors. Existing encoders are fundamentally holistic, neglecting the entanglement of latent factors. This not only makes graph analysis tasks less effective but also makes it harder to understand and explain the representations. Learning disentangled graph representations with (variational) graph auto-encoder poses significant challenges, and remains largely unexplored in the existing literature. In this article, we introduce the Disentangled Graph Auto-Encoder (DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that leverage generative models to learn disentangled representations. Specifically, we first design a disentangled graph convolutional network with multi-channel message-passing layers, as the encoder aggregating information related to eac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;Granger&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24494;&#26381;&#21153;&#20013;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01140</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;Granger&#22240;&#26524;&#21457;&#29616;&#30340;&#24494;&#26381;&#21153;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Root Cause Analysis In Microservice Using Neural Granger Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;Granger&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24494;&#26381;&#21153;&#20013;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#32500;&#25252;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24494;&#26381;&#21153;&#22312;IT&#36816;&#33829;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#31995;&#32479;&#25925;&#38556;&#26102;&#65292;&#31449;&#28857;&#21487;&#38752;&#24615;&#24037;&#31243;&#24072;&#24456;&#38590;&#25214;&#21040;&#26681;&#26412;&#21407;&#22240;&#65292;&#22240;&#20026;&#24494;&#26381;&#21153;&#20013;&#23384;&#22312;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#32467;&#26500;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;PC&#31639;&#27861;&#65289;&#26469;&#24314;&#31435;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20174;&#22240;&#26524;&#22270;&#20013;&#24471;&#20986;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#39034;&#24207;&#65292;&#24182;&#26410;&#21033;&#29992;&#26102;&#38388;&#20851;&#31995;&#20013;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#22312;CPU&#21033;&#29992;&#29575;&#31361;&#28982;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20854;&#20182;&#24494;&#26381;&#21153;&#30340;&#24310;&#36831;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;CPU&#21033;&#29992;&#29575;&#24322;&#24120;&#21457;&#29983;&#22312;&#24310;&#36831;&#22686;&#21152;&#20043;&#21069;&#65292;&#32780;&#19981;&#26159;&#21516;&#26102;&#21457;&#29983;&#12290;&#32467;&#26524;&#65292;PC&#31639;&#27861;&#26080;&#27861;&#25429;&#25417;&#36825;&#26679;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RUN&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, microservices have gained widespread adoption in IT operations due to their scalability, maintenance, and flexibility. However, it becomes challenging for site reliability engineers (SREs) to pinpoint the root cause due to the complex relationships in microservices when facing system malfunctions. Previous research employed structured learning methods (e.g., PC-algorithm) to establish causal relationships and derive root causes from causal graphs. Nevertheless, they ignored the temporal order of time series data and failed to leverage the rich information inherent in the temporal relationships. For instance, in cases where there is a sudden spike in CPU utilization, it can lead to an increase in latency for other microservices. However, in this scenario, the anomaly in CPU utilization occurs before the latency increase, rather than simultaneously. As a result, the PC-algorithm fails to capture such characteristics. To address these challenges, we propose RUN, a novel a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36882;&#20943;&#27493;&#38271;&#26469;&#25913;&#36827;&#22312;&#20219;&#24847;&#24207;&#21015;&#19978;&#30340;&#35206;&#30422;&#29575;&#20445;&#35777;&#65292;&#24182;&#19988;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#24635;&#20307;&#20998;&#20301;&#25968;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01139</link><description>&lt;p&gt;
&#22312;&#32447;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;&#20013;&#24102;&#26377;&#36882;&#20943;&#27493;&#38271;
&lt;/p&gt;
&lt;p&gt;
Online conformal prediction with decaying step sizes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36882;&#20943;&#27493;&#38271;&#26469;&#25913;&#36827;&#22312;&#20219;&#24847;&#24207;&#21015;&#19978;&#30340;&#35206;&#30422;&#29575;&#20445;&#35777;&#65292;&#24182;&#19988;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#24635;&#20307;&#20998;&#20301;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#24102;&#26377;&#36882;&#20943;&#27493;&#38271;&#30340;&#22312;&#32447;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;&#12290;&#21644;&#20043;&#21069;&#30340;&#26041;&#27861;&#19968;&#26679;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33021;&#22312;&#20219;&#24847;&#24207;&#21015;&#19978;&#22238;&#28335;&#24615;&#22320;&#20445;&#35777;&#35206;&#30422;&#29575;&#12290;&#28982;&#32780;&#65292;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#20272;&#35745;&#20986;&#24635;&#20307;&#20998;&#20301;&#25968;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#23454;&#38469;&#29305;&#24615;&#65306;&#29305;&#21035;&#26159;&#22312;&#20998;&#24067;&#31283;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#35206;&#30422;&#29575;&#25509;&#36817;&#25152;&#26399;&#26395;&#30340;&#27700;&#24179;&#65292;&#19981;&#20165;&#20165;&#22312;&#35266;&#27979;&#24207;&#21015;&#30340;&#24179;&#22343;&#20540;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method for online conformal prediction with decaying step sizes. Like previous methods, ours possesses a retrospective guarantee of coverage for arbitrary sequences. However, unlike previous methods, we can simultaneously estimate a population quantile when it exists. Our theory and experiments indicate substantially improved practical properties: in particular, when the distribution is stable, the coverage is close to the desired level for every time point, not just on average over the observed sequence.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#39046;&#22495;&#12290;&#26412;&#32508;&#36848;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;&#24050;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;GNNs&#30340;&#26126;&#30830;&#25351;&#23548;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01138</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20010;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks in EEG-based Emotion Recognition: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01138
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#39046;&#22495;&#12290;&#26412;&#32508;&#36848;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;&#24050;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;GNNs&#30340;&#26126;&#30830;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#24335;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#21487;&#20197;&#30452;&#35266;&#22320;&#21709;&#24212;&#20154;&#33041;&#20013;&#30340;&#24773;&#32490;&#27169;&#24335;&#65292;&#22240;&#27492;&#25104;&#20026;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#39046;&#22495;&#26368;&#20851;&#27880;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#30001;&#20110;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#19982;&#24773;&#32490;&#23494;&#20999;&#30456;&#20851;&#65292;&#22240;&#27492;&#21457;&#23637;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#24773;&#32490;&#24615;&#33041;&#30005;&#22270;&#20013;&#30340;&#22823;&#33041;&#21306;&#22495;&#20381;&#36182;&#20855;&#26377;&#29983;&#29702;&#22522;&#30784;&#65292;&#20351;&#24471;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;GNNs&#19982;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;GNNs&#26377;&#25152;&#21306;&#21035;&#12290;&#27492;&#22806;&#65292;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#26082;&#27809;&#26377;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#20063;&#27809;&#26377;&#26500;&#24314;GNNs&#30340;&#25351;&#23548;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#24050;&#26377;&#26041;&#27861;&#22312;&#22270;&#26500;&#36896;&#30340;&#32479;&#19968;&#26694;&#26550;&#19979;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25581;&#31034;&#20986;&#20854;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#20174;&#26694;&#26550;&#30340;&#19977;&#20010;&#38454;&#27573;&#20998;&#26512;&#21644;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#26500;&#24314;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;GNNs&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#25351;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;...
&lt;/p&gt;
&lt;p&gt;
Compared to other modalities, EEG-based emotion recognition can intuitively respond to the emotional patterns in the human brain and, therefore, has become one of the most concerning tasks in the brain-computer interfaces field. Since dependencies within brain regions are closely related to emotion, a significant trend is to develop Graph Neural Networks (GNNs) for EEG-based emotion recognition. However, brain region dependencies in emotional EEG have physiological bases that distinguish GNNs in this field from those in other time series fields. Besides, there is neither a comprehensive review nor guidance for constructing GNNs in EEG-based emotion recognition. In the survey, our categorization reveals the commonalities and differences of existing approaches under a unified framework of graph construction. We analyze and categorize methods from three stages in the framework to provide clear guidance on constructing GNNs in EEG-based emotion recognition. In addition, we discuss several 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#21644;&#31934;&#31616;&#30340;MPC&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;12&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01116</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22810;&#27169;&#22411;MPC&#30340;&#22522;&#20110;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#30340;&#23618;&#32423;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01116
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#21644;&#31934;&#31616;&#30340;MPC&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;12&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#12290;&#35813;&#26550;&#26500;&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;1) RAID-Net&#65292;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#39062;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24615;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#21608;&#22260;&#36710;&#36742;&#20043;&#38388;&#22312;MPC&#39044;&#27979;&#33539;&#22260;&#20869;&#30340;&#30456;&#20851;&#20132;&#20114;&#65307;2) &#19968;&#20010;&#31616;&#21270;&#30340;&#38543;&#26426;MPC&#38382;&#39064;&#65292;&#28040;&#38500;&#19981;&#30456;&#20851;&#30340;&#36991;&#30896;&#32422;&#26463;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#27169;&#25311;&#20132;&#36890;&#36335;&#21475;&#20013;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#30340;12&#20493;&#36895;&#25552;&#21319;&#12290;&#24744;&#21487;&#20197;&#22312;&#36825;&#37324;&#25214;&#21040;&#23637;&#31034;&#35813;&#26550;&#26500;&#22312;&#22810;&#20010;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#35270;&#39057;&#65306;https://youtu.be/-TcMeolCLWc
&lt;/p&gt;
&lt;p&gt;
We propose a hierarchical architecture designed for scalable real-time Model Predictive Control (MPC) in complex, multi-modal traffic scenarios. This architecture comprises two key components: 1) RAID-Net, a novel attention-based Recurrent Neural Network that predicts relevant interactions along the MPC prediction horizon between the autonomous vehicle and the surrounding vehicles using Lagrangian duality, and 2) a reduced Stochastic MPC problem that eliminates irrelevant collision avoidance constraints, enhancing computational efficiency. Our approach is demonstrated in a simulated traffic intersection with interactive surrounding vehicles, showcasing a 12x speed-up in solving the motion planning problem. A video demonstrating the proposed architecture in multiple complex traffic scenarios can be found here: https://youtu.be/-TcMeolCLWc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#38543;&#26426;&#21270;&#26469;&#38450;&#27490;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#36807;&#25311;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#24212;&#29992;&#21452;&#37325;&#38450;&#24481;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#32780;&#19981;&#38477;&#20302;&#20934;&#30830;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01114</link><description>&lt;p&gt;
&#21452;&#37325;&#38450;&#24481;&#65306;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#38543;&#26426;&#21270;&#26469;&#38450;&#27490;&#20165;&#22522;&#20110;&#26631;&#31614;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer Learning and Randomization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#38543;&#26426;&#21270;&#26469;&#38450;&#27490;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#36807;&#25311;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#24212;&#29992;&#21452;&#37325;&#38450;&#24481;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#32780;&#19981;&#38477;&#20302;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#38754;&#23545;&#35757;&#32451;&#26679;&#26412;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#36807;&#25311;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#30340;&#21512;&#36866;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#19968;&#31867;&#38544;&#31169;&#25915;&#20987;&#31216;&#20026;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26088;&#22312;&#30830;&#23450;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#25104;&#21592;&#65289;&#25110;&#19981;&#23646;&#20110;&#65288;&#38750;&#25104;&#21592;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#37325;&#38450;&#24481;&#65292;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#22312;&#19981;&#38477;&#20302;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#65288;&#38454;&#27573;1&#65289;&#32467;&#21512;&#38543;&#26426;&#21270;&#65288;&#38454;&#27573;2&#65289;&#26469;&#38450;&#27490;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#23545;&#36807;&#25311;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#23519;&#20102;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#21644;&#21442;&#25968;&#20540;&#12289;&#20923;&#32467;&#23618;&#30340;&#25968;&#37327;&#20197;&#21450;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#65288;&#30446;&#26631;&#65292;&#28304;&#65289;&#25968;&#25454;&#38598;&#23545;&#19978;&#35780;&#20272;&#20102;&#21452;&#37325;&#38450;&#24481;&#65306;&#65288;i&#65289;&#65288;CIFAR-10&#65292;ImageNet&#65289;&#65292;&#65288;ii&#65289;&#65288;GTSRB&#65292;ImageNet&#65289;&#65292;&#65288;iii&#65289;&#65288;CelebA&#65292;VGGFace2&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22235;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#65288;a&#65289;VGG-19&#65292;
&lt;/p&gt;
&lt;p&gt;
Transfer learning (TL) has been demonstrated to improve DNN model performance when faced with a scarcity of training samples. However, the suitability of TL as a solution to reduce vulnerability of overfitted DNNs to privacy attacks is unexplored. A class of privacy attacks called membership inference attacks (MIAs) aim to determine whether a given sample belongs to the training dataset (member) or not (nonmember). We introduce Double-Dip, a systematic empirical study investigating the use of TL (Stage-1) combined with randomization (Stage-2) to thwart MIAs on overfitted DNNs without degrading classification accuracy. Our study examines the roles of shared feature space and parameter values between source and target models, number of frozen layers, and complexity of pretrained models. We evaluate Double-Dip on three (Target, Source) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii) (CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a) VGG-19,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#31639;&#27861;&#65292;&#23558;&#21518;&#24724;&#25511;&#21046;&#22312;$\widetilde{O}(\sqrt{H^3 S^2 ABK})$&#65292;&#25209;&#37327;&#22797;&#26434;&#24230;&#20026;$O(H+\log\log K)$&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#25152;&#26377;&#20855;&#26377;$\widetilde{O}(\sqrt{K})$&#21518;&#24724;&#30028;&#31639;&#27861;&#30340;&#25209;&#37327;&#22797;&#26434;&#24230;&#19979;&#30028;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01111</link><description>&lt;p&gt;
&#22312;&#33258;&#36866;&#24212;&#32422;&#26463;&#19979;&#30340;&#33258;&#23545;&#24328;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#31639;&#27861;&#65292;&#23558;&#21518;&#24724;&#25511;&#21046;&#22312;$\widetilde{O}(\sqrt{H^3 S^2 ABK})$&#65292;&#25209;&#37327;&#22797;&#26434;&#24230;&#20026;$O(H+\log\log K)$&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#25152;&#26377;&#20855;&#26377;$\widetilde{O}(\sqrt{K})$&#21518;&#24724;&#30028;&#31639;&#27861;&#30340;&#25209;&#37327;&#22797;&#26434;&#24230;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65288;MARL&#65289; - &#36825;&#26159;&#19968;&#31181;&#30001;&#23454;&#38469;&#24212;&#29992;&#39537;&#21160;&#30340;&#26032;&#38382;&#39064;&#65292;&#20854;&#20013;&#37096;&#32626;&#26032;&#31574;&#30053;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24517;&#39035;&#26368;&#23567;&#21270;&#31574;&#30053;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;&#23545;&#20110;&#20004;&#20010;&#29609;&#23478;&#30340;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#65288;&#31574;&#30053;&#65289;&#22522;&#20110;&#28040;&#38500;&#30340;&#31639;&#27861;&#65292;&#23427;&#22312;&#21518;&#24724;&#20026;$\widetilde{O}(\sqrt{H^3 S^2 ABK})$&#30340;&#24773;&#20917;&#19979;&#65292;&#25209;&#37327;&#22797;&#26434;&#24230;&#20165;&#20026;$O(H+\log\log K)$&#12290;&#22312;&#19978;&#36848;&#24773;&#20917;&#19979;&#65292;$S$&#34920;&#31034;&#29366;&#24577;&#25968;&#65292;$A&#65292;B$&#20998;&#21035;&#20195;&#34920;&#20004;&#20010;&#29609;&#23478;&#30340;&#34892;&#21160;&#25968;&#65292;$H$&#26159;&#26102;&#38388;&#21608;&#26399;&#65292;$K$&#26159;&#28216;&#25103;&#27425;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25152;&#26377;&#20855;&#26377;$\widetilde{O}(\sqrt{K})$&#21518;&#24724;&#30028;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#25209;&#37327;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#20026;$\Omega(\frac{H}{\log_{A}K}+\log\log K)$&#65292;&#36825;&#19982;&#25105;&#20204;&#30340;&#19978;&#30028;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#21305;&#37197;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#23398;&#20064;&#36172;&#21338;&#21338;&#24328;&#21644;&#26080;&#22870;&#21169;&#30340;&#36817;&#20046;&#26368;&#20248;&#25209;&#37327;&#22797;&#26434;&#24230;MARL&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20123;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of multi-agent reinforcement learning (MARL) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. For two-player zero-sum Markov Games, we design a (policy) elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3 S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above, $S$ denotes the number of states, $A,B$ are the number of actions for the two players respectively, $H$ is the horizon and $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound $\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with $\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to logarithmic factors. As a byproduct, our techniques naturally extend to learning bandit games and reward-free MARL within near optimal batch complexity. To the best of our knowledge, these 
&lt;/p&gt;</description></item><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#38754;&#20020;&#30528;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#24341;&#20837;&#25512;&#29702;&#33021;&#21147;&#20316;&#20026;&#32479;&#19968;&#30340;&#26631;&#20934;&#26469;&#23454;&#29616;&#31995;&#32479;&#30340;&#25972;&#21512;&#21644;&#20248;&#21270;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01108</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#23616;&#38480;&#24615;&#12289;&#25361;&#25112;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01108
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#38754;&#20020;&#30528;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#24341;&#20837;&#25512;&#29702;&#33021;&#21147;&#20316;&#20026;&#32479;&#19968;&#30340;&#26631;&#20934;&#26469;&#23454;&#29616;&#31995;&#32479;&#30340;&#25972;&#21512;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#21033;&#29992;&#23427;&#20204;&#24102;&#26469;&#20102;&#35768;&#22810;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;LLMs&#30340;&#23454;&#38469;&#37319;&#29992;&#65292;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#20225;&#19994;&#24179;&#21488;&#20013;&#20855;&#26377;&#22686;&#24378;&#12289;&#25972;&#21512;&#21644;&#21327;&#35843;LLMs&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#35813;&#24179;&#21488;&#21033;&#29992;&#29616;&#26377;&#19987;&#26377;&#25968;&#25454;&#21644;&#27169;&#22411;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#21333;&#19968;&#30446;&#26631;&#30340;&#20248;&#21270;&#21644;&#35780;&#20272;&#65292;&#24448;&#24448;&#24573;&#35270;&#29616;&#23454;&#24773;&#26223;&#20013;&#30340;&#28508;&#22312;&#32422;&#26463;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#39044;&#31639;&#12289;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#12289;&#20998;&#26512;&#21644;&#35843;&#35797;&#36825;&#20123;&#31995;&#32479;&#35201;&#27714;&#19981;&#21516;&#30340;&#32452;&#20214;&#20043;&#38388;&#36827;&#34892;&#30456;&#20114;&#35780;&#20272;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#26631;&#20934;&#65292;&#20197;&#23454;&#29616;&#38598;&#25104;&#21644;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01107</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#27169;&#25311;&#22270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulation of Graph Algorithms with Looped Transformers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#22270;&#31639;&#27861;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#30001;&#20110;&#26377;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#35777;&#36827;&#23637;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#33021;&#22815;&#20351;&#29992;&#20851;&#31995;&#25968;&#25454;&#22797;&#21046;&#25512;&#29702;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#24102;&#39069;&#22806;&#27880;&#24847;&#21147;&#22836;&#21644;&#19982;&#22270;&#24418;&#20132;&#20114;&#30340;&#24490;&#29615;&#21464;&#21387;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#20102;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#27169;&#25311;&#35832;&#22914;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#31561;&#31639;&#27861;&#12290;&#32593;&#32476;&#30340;&#23485;&#24230;&#19981;&#38543;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#22686;&#21152;&#65292;&#36825;&#24847;&#21619;&#30528;&#32593;&#32476;&#21487;&#20197;&#27169;&#25311;&#20219;&#20309;&#22270;&#19978;&#30340;&#19978;&#36848;&#31639;&#27861;&#12290;&#23613;&#31649;&#26377;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#26377;&#19968;&#20010;&#30001;&#20110;&#26377;&#38480;&#31934;&#24230;&#32780;&#21463;&#21040;&#38480;&#21046;&#30340;&#27169;&#25311;&#26497;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The execution of graph algorithms using neural networks has recently attracted significant interest due to promising empirical progress. This motivates further understanding of how neural networks can replicate reasoning steps with relational data. In this work, we study the ability of transformer networks to simulate algorithms on graphs from a theoretical perspective. The architecture that we utilize is a looped transformer with extra attention heads that interact with the graph. We prove by construction that this architecture can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth- and Depth-First Search, and Kosaraju's strongly connected components algorithm. The width of the network does not increase with the size of the input graph, which implies that the network can simulate the above algorithms for any graph. Despite this property, we show that there is a limit to simulation in our solution due to finite precision. Finally, we show a Turing Completeness resu
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01105</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey for Foundation Models in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#35268;&#21010;&#21644;&#20223;&#30495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#22312;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#20026;&#20223;&#30495;&#21644;&#27979;&#35797;&#21019;&#24314;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#22810;&#26679;&#30340;&#36755;&#20837;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#65292;&#26681;&#25454;&#27169;&#24577;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#21151;&#33021;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01103</link><description>&lt;p&gt;
&#32452;&#21512;&#29983;&#25104;&#24314;&#27169;&#65306;&#21333;&#19968;&#27169;&#22411;&#24182;&#19981;&#26159;&#24744;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Modeling: A Single Model is Not All You Need
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#24040;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#20027;&#27969;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#24212;&#35813;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#22914;&#20309;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#20998;&#24067;&#65292;&#20351;&#24471;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#20998;&#24067;&#37096;&#20998;&#20063;&#33021;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#35757;&#32451;&#26102;&#23436;&#20840;&#26410;&#35265;&#30340;&#20219;&#21153;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#29420;&#31435;&#30340;&#32452;&#21512;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26031;&#22374;&#20248;&#21270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#39057;&#29575;&#20027;&#20041;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#39044;&#27979;&#24615;&#32500;&#25252;&#20013;&#20272;&#35745;&#21097;&#20313;&#23551;&#21629;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01098</link><description>&lt;p&gt;
&#22522;&#20110;&#26031;&#22374;&#20248;&#21270;&#26799;&#24230;&#19979;&#38477;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21097;&#20313;&#23551;&#21629;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Deep Learning for Remaining Useful Life Estimation via Stein Variational Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26031;&#22374;&#20248;&#21270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#39057;&#29575;&#20027;&#20041;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#39044;&#27979;&#24615;&#32500;&#25252;&#20013;&#20272;&#35745;&#21097;&#20313;&#23551;&#21629;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#27979;&#24615;&#32500;&#25252;&#20013;&#65292;&#20272;&#35745;&#29289;&#29702;&#31995;&#32479;&#30340;&#21097;&#20313;&#21487;&#29992;&#23551;&#21629;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#32479;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20248;&#21270;&#35745;&#21010;&#32500;&#25252;&#25805;&#20316;&#65292;&#37327;&#21270;&#39044;&#27979;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#20063;&#24456;&#37325;&#35201;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23558;&#26631;&#20934;&#30340;&#39057;&#29575;&#20027;&#20041;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#65292;&#21518;&#32773;&#33021;&#22815;&#33258;&#28982;&#22320;&#22312;&#20272;&#35745;&#21608;&#22260;&#25552;&#20379;&#32622;&#20449;&#21306;&#38388;&#12290;&#23384;&#22312;&#22810;&#31181;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#21442;&#25968;&#21464;&#20998;&#25512;&#29702;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#22240;&#36817;&#20284;&#33021;&#21147;&#26377;&#38480;&#21644;&#35745;&#31639;&#36127;&#25285;&#22823;&#32780;&#38395;&#21517;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26031;&#22374;&#20248;&#21270;&#26799;&#24230;&#19979;&#38477;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#36924;&#36817;&#38590;&#20197;&#35745;&#31639;&#20998;&#24067;&#30340;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#19978;&#36848;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial task in predictive maintenance is estimating the remaining useful life of physical systems. In the last decade, deep learning has improved considerably upon traditional model-based and statistical approaches in terms of predictive performance. However, in order to optimally plan maintenance operations, it is also important to quantify the uncertainty inherent to the predictions. This issue can be addressed by turning standard frequentist neural networks into Bayesian neural networks, which are naturally capable of providing confidence intervals around the estimates. Several methods exist for training those models. Researchers have focused mostly on parametric variational inference and sampling-based techniques, which notoriously suffer from limited approximation power and large computational burden, respectively. In this work, we use Stein variational gradient descent, a recently proposed algorithm for approximating intractable distributions that overcomes the drawbacks of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#21487;&#20449;&#30340;&#20998;&#24067;&#24335;AI&#30340;&#20195;&#34920;&#24615;&#25216;&#26415;&#65292;&#21253;&#25324;&#40065;&#26834;&#24615;&#20445;&#35777;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#20844;&#24179;&#24847;&#35782;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01096</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;&#20998;&#24067;&#24335;AI&#31995;&#32479;&#65306;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#21487;&#20449;&#30340;&#20998;&#24067;&#24335;AI&#30340;&#20195;&#34920;&#24615;&#25216;&#26415;&#65292;&#21253;&#25324;&#40065;&#26834;&#24615;&#20445;&#35777;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#20844;&#24179;&#24847;&#35782;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;AI&#31995;&#32479;&#27491;&#22312;&#38761;&#26032;&#22823;&#25968;&#25454;&#35745;&#31639;&#21644;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#32463;&#27982;&#21644;&#31038;&#20250;&#20135;&#29983;&#36234;&#26469;&#36234;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;AI&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#23548;&#33268;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#21644;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#40065;&#26834;&#24615;&#20445;&#35777;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#20844;&#24179;&#24847;&#35782;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#22238;&#39038;&#20102;&#20195;&#34920;&#24615;&#30340;&#25216;&#26415;&#12289;&#31639;&#27861;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#23454;&#29616;&#21487;&#20449;&#30340;&#20998;&#24067;&#24335;AI&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#26367;&#20195;&#26550;&#26500;&#30340;&#31616;&#35201;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;AI&#31639;&#27861;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#30340;&#22266;&#26377;&#28431;&#27934;&#65292;&#24182;&#20998;&#26512;&#20102;&#20026;&#20160;&#20040;&#36825;&#20123;&#38382;&#39064;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23384;&#22312;&#65292;&#32780;&#19981;&#31649;&#20855;&#20307;&#30340;&#26550;&#26500;&#22914;&#20309;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#21487;&#20449;&#20998;&#24067;&#24335;AI&#30340;&#29420;&#29305;&#20998;&#31867;&#65292;&#28085;&#30422;&#20102;&#23545;&#25512;&#29702;&#20013;&#30340;&#36867;&#36991;&#25915;&#20987;&#21644;&#19981;&#35268;&#21017;&#26597;&#35810;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#23545;&#20013;&#27602;&#25915;&#20987;&#21644;&#25968;&#25454;&#27844;&#38706;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging Distributed AI systems are revolutionizing big data computing and data processing capabilities with growing economic and societal impact. However, recent studies have identified new attack surfaces and risks caused by security, privacy, and fairness issues in AI systems. In this paper, we review representative techniques, algorithms, and theoretical foundations for trustworthy distributed AI through robustness guarantee, privacy protection, and fairness awareness in distributed learning. We first provide a brief overview of alternative architectures for distributed learning, discuss inherent vulnerabilities for security, privacy, and fairness of AI algorithms in distributed learning, and analyze why these problems are present in distributed learning regardless of specific architectures. Then we provide a unique taxonomy of countermeasures for trustworthy distributed AI, covering (1) robustness to evasion attacks and irregular queries at inference, and robustness to poisoning a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#36866;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MSV&#30340;&#25968;&#37327;&#19982;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01095</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20351;&#29992;&#22810;&#23569;&#20010;&#35270;&#22270;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many views does your deep neural network use for prediction?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#36866;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MSV&#30340;&#25968;&#37327;&#19982;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#35768;&#22810;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20294;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26368;&#36817;&#65292;Allen-Zhu&#21644;Li&#65288;2023&#65289;&#24341;&#20837;&#20102;&#22810;&#35270;&#22270;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;DNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20182;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#38598;&#25104;&#25110;&#33976;&#39311;&#27169;&#22411;&#65292;&#24182;&#26410;&#35752;&#35770;&#29992;&#20110;&#29305;&#23450;&#36755;&#20837;&#39044;&#27979;&#30340;&#22810;&#35270;&#22270;&#20272;&#35745;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#65292;&#23427;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#30495;&#23454;&#22270;&#20687;&#12290;MSVs&#26159;&#36755;&#20837;&#20013;&#30340;&#19968;&#32452;&#26368;&#23567;&#19988;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#27599;&#20010;&#29305;&#24449;&#20445;&#30041;&#20102;&#27169;&#22411;&#23545;&#35813;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#27169;&#22411;&#65288;&#21253;&#25324;&#21367;&#31215;&#21644;&#36716;&#25442;&#27169;&#22411;&#65289;&#30340;MSV&#25968;&#37327;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26126;&#30830;&#30340;&#20851;&#31995;&#65292;&#36825;&#34920;&#26126;&#22810;&#35270;&#22270;&#30340;&#35282;&#24230;&#23545;&#20110;&#29702;&#35299;&#65288;&#38750;&#38598;&#25104;&#25110;&#38750;&#33976;&#39311;&#65289;DNN&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization ability of Deep Neural Networks (DNNs) is still not fully understood, despite numerous theoretical and empirical analyses. Recently, Allen-Zhu &amp; Li (2023) introduced the concept of multi-views to explain the generalization ability of DNNs, but their main target is ensemble or distilled models, and no method for estimating multi-views used in a prediction of a specific input is discussed. In this paper, we propose Minimal Sufficient Views (MSVs), which is similar to multi-views but can be efficiently computed for real images. MSVs is a set of minimal and distinct features in an input, each of which preserves a model's prediction for the input. We empirically show that there is a clear relationship between the number of MSVs and prediction accuracy across models, including convolutional and transformer models, suggesting that a multi-view like perspective is also important for understanding the generalization ability of (non-ensemble or non-distilled) DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#20102;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01093</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Specialized Language Models with Cheap Inference from Limited Domain Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#20102;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#32570;&#20047;&#22823;&#35268;&#27169;&#25512;&#29702;&#39044;&#31639;&#21644;&#22823;&#35268;&#27169;&#39046;&#22495;&#20869;&#35757;&#32451;&#38598;&#30340;&#20219;&#21153;&#20013;&#24212;&#29992;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#23545;&#36825;&#20123;&#38480;&#21046;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#22235;&#20010;&#37325;&#35201;&#30340;&#21464;&#37327;&#65306;&#39044;&#35757;&#32451;&#39044;&#31639;&#65288;&#29992;&#20110;&#22312;&#30446;&#26631;&#39046;&#22495;&#20986;&#29616;&#20043;&#21069;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#19987;&#29992;&#39044;&#31639;&#65288;&#29992;&#20110;&#22312;&#30446;&#26631;&#39046;&#22495;&#20986;&#29616;&#20043;&#21518;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#25512;&#29702;&#39044;&#31639;&#21644;&#39046;&#22495;&#20869;&#35757;&#32451;&#38598;&#22823;&#23567;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#21463;&#21040;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#26631;&#20934;&#20570;&#27861;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#22256;&#24785;&#24230;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#35299;&#37322;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#65292;&#32780;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#35201;&#27714;&#22686;&#21152;&#35757;&#32451;&#27493;&#25968;&#24555;&#20110;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#65292;&#19982;&#23454;&#35777;&#35266;&#23519;&#30456;&#19968;&#33268;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01092</link><description>&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Dynamical Model of Neural Scaling Laws
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01092
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#35299;&#37322;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#65292;&#32780;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#35201;&#27714;&#22686;&#21152;&#35757;&#32451;&#27493;&#25968;&#24555;&#20110;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#65292;&#19982;&#23454;&#35777;&#35266;&#23519;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#38543;&#30528;&#35757;&#32451;&#26102;&#38388;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#39044;&#27979;&#24615;&#22320;&#25552;&#39640;&#65292;&#36328;&#22810;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#23450;&#24459;&#65292;&#23427;&#25253;&#21578;&#20102;&#22312;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#22823;&#23567;&#26102;&#24615;&#33021;&#19982;&#35745;&#31639;&#25968;&#37327;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#20316;&#20026;&#32593;&#32476;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#21487;&#35299;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#22797;&#29616;&#20102;&#20851;&#20110;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#30340;&#35768;&#22810;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20110;&#20026;&#20160;&#20040;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#29702;&#35770;&#39044;&#27979;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#65292;&#20854;&#20013;&#35757;&#32451;&#27493;&#25968;&#30340;&#22686;&#21152;&#36895;&#24230;&#24555;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#36895;&#24230;&#65292;&#19982;&#26368;&#36817;&#30340;&#23454;&#35777;&#35266;&#23519;&#19968;&#33268;&#12290;&#20854;&#27425;&#65292;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#65292;&#32593;&#32476;&#20250;&#25910;&#25947;&#21040;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-wi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#39640;&#38454;&#24352;&#37327;&#31215;&#26679;&#26465;&#27169;&#22411;&#65292;&#20801;&#35768;&#21152;&#20837;&#25152;&#26377;&#65288;&#39640;&#38454;&#65289;&#38750;&#32447;&#24615;&#29305;&#24449;&#25928;&#24212;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#20855;&#26377;&#19982;&#27809;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#22411;&#25104;&#27604;&#20363;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01090</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#39640;&#38454;&#24352;&#37327;&#31215;&#26679;&#26465;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scalable Higher-Order Tensor Product Spline Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01090
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#39640;&#38454;&#24352;&#37327;&#31215;&#26679;&#26465;&#27169;&#22411;&#65292;&#20801;&#35768;&#21152;&#20837;&#25152;&#26377;&#65288;&#39640;&#38454;&#65289;&#38750;&#32447;&#24615;&#29305;&#24449;&#25928;&#24212;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#20855;&#26377;&#19982;&#27809;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#22411;&#25104;&#27604;&#20363;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#22823;&#25968;&#25454;&#21644;&#36879;&#26126;&#26426;&#22120;&#23398;&#20064;&#30340;&#26102;&#20195;&#65292;&#25216;&#26415;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36816;&#20316;&#30340;&#21516;&#26102;&#65292;&#36824;&#38656;&#35201;&#25552;&#20379;&#23545;&#26041;&#27861;&#20869;&#37096;&#24037;&#20316;&#30340;&#28165;&#26224;&#25968;&#23398;&#29702;&#35299;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#21487;&#35299;&#37322;&#30340;&#21322;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#65292;&#20294;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#36825;&#20123;&#27169;&#22411;&#20013;&#32570;&#20047;&#30456;&#20114;&#20316;&#29992;&#65292;&#20986;&#20110;&#26356;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#35745;&#31639;&#25104;&#26412;&#32771;&#34385;&#32780;&#34987;&#24573;&#30053;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#23376;&#21270;&#26041;&#27861;&#25512;&#23548;&#20986;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#39640;&#38454;&#24352;&#37327;&#31215;&#26679;&#26465;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23558;&#25152;&#26377;&#65288;&#39640;&#38454;&#65289;&#38750;&#32447;&#24615;&#29305;&#24449;&#25928;&#24212;&#30340;&#30456;&#20114;&#20316;&#29992;&#32435;&#20837;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#27809;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#22411;&#25104;&#27604;&#20363;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#24809;&#32602;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
In the current era of vast data and transparent machine learning, it is essential for techniques to operate at a large scale while providing a clear mathematical comprehension of the internal workings of the method. Although there already exist interpretable semi-parametric regression methods for large-scale applications that take into account non-linearity in the data, the complexity of the models is still often limited. One of the main challenges is the absence of interactions in these models, which are left out for the sake of better interpretability but also due to impractical computational costs. To overcome this limitation, we propose a new approach using a factorization method to derive a highly scalable higher-order tensor product spline model. Our method allows for the incorporation of all (higher-order) interactions of non-linear feature effects while having computational costs proportional to a model without interactions. We further develop a meaningful penalization scheme a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#21021;&#22987;&#21270;&#26102;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#25554;&#20540;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25513;&#30721;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24576;&#30097;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#35757;&#32451;&#21518;&#20462;&#21098;&#26159;&#24517;&#35201;&#30340;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01089</link><description>&lt;p&gt;
&#26080;&#20813;&#36153;&#20462;&#21098;&#65306;&#21021;&#22987;&#21270;&#26102;&#21098;&#26525;&#30340;&#20449;&#24687;&#35770;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
No Free Prune: Information-Theoretic Barriers to Pruning at Initialization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#21021;&#22987;&#21270;&#26102;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#25554;&#20540;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25513;&#30721;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24576;&#30097;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#35757;&#32451;&#21518;&#20462;&#21098;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#25277;&#22870;&#20013;&#22870;&#32773;&#8221;&#26159;&#21542;&#22312;&#21021;&#22987;&#21270;&#26102;&#23384;&#22312;&#65292;&#24341;&#21457;&#20102;&#19968;&#20010;&#20196;&#20154;&#30528;&#36855;&#30340;&#38382;&#39064;&#65306;&#28145;&#24230;&#23398;&#20064;&#26159;&#21542;&#38656;&#35201;&#22823;&#22411;&#27169;&#22411;&#65292;&#25110;&#32773;&#21487;&#20197;&#22312;&#19981;&#35757;&#32451;&#21253;&#21547;&#23427;&#20204;&#30340;&#23494;&#38598;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36805;&#36895;&#35782;&#21035;&#21644;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23581;&#35797;&#22312;&#21021;&#22987;&#21270;&#26102;&#25214;&#21040;&#36825;&#20123;&#31232;&#30095;&#23376;&#32593;&#32476;&#65288;&#8220;&#21021;&#22987;&#21270;&#26102;&#20462;&#21098;&#8221;&#65289;&#30340;&#21162;&#21147;&#22312;&#24191;&#27867;&#19978;&#37117;&#27809;&#26377;&#25104;&#21151;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;$p_\text{eff}$&#65292;&#30001;&#26368;&#32456;&#32593;&#32476;&#20013;&#38750;&#38646;&#26435;&#37325;&#30340;&#25968;&#37327;&#21644;&#31232;&#30095;&#25513;&#30721;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#30340;&#24635;&#21644;&#32473;&#20986;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#40065;&#26834;&#24615;&#23450;&#24459;&#8221;&#65288;arXiv:2105.12806&#65289;&#24310;&#20280;&#21040;&#31232;&#30095;&#32593;&#32476;&#65292;&#20854;&#20013;&#24120;&#35268;&#21442;&#25968;&#25968;&#37327;&#34987;$p_\text{eff}$&#25152;&#21462;&#20195;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;&#33021;&#22815;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#25554;&#20540;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25513;&#30721;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#35757;&#32451;&#21518;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of "lottery tickets" arXiv:1803.03635 at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. However, efforts to find these sparse subnetworks without training the dense model ("pruning at initialization") have been broadly unsuccessful arXiv:2009.08576. We put forward a theoretical explanation for this, based on the model's effective parameter count, $p_\text{eff}$, given by the sum of the number of non-zero weights in the final network and the mutual information between the sparsity mask and the data. We show the Law of Robustness of arXiv:2105.12806 extends to sparse networks with the usual parameter count replaced by $p_\text{eff}$, meaning a sparse neural network which robustly interpolates noisy data requires a heavily data-dependent mask. We posit that pruning during and after training 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#20851;&#38190;&#26041;&#27861;&#65292;&#21363;&#26356;&#22909;&#30340;&#39044;&#22788;&#29702;&#12289;&#35282;&#23884;&#20837;&#21644;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#23398;&#20064;&#19982;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#26356;&#22823;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#31232;&#30095;&#20108;&#36827;&#21046;&#23494;&#38053;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01082</link><description>&lt;p&gt;
Salsa Fresca: &#22522;&#20110;&#35282;&#23884;&#20837;&#21644;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#23398;&#20064;&#19982;&#35823;&#24046;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#20851;&#38190;&#26041;&#27861;&#65292;&#21363;&#26356;&#22909;&#30340;&#39044;&#22788;&#29702;&#12289;&#35282;&#23884;&#20837;&#21644;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#23398;&#20064;&#19982;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#26356;&#22823;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#31232;&#30095;&#20108;&#36827;&#21046;&#23494;&#38053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19982;&#35823;&#24046;&#65288;LWE&#65289;&#26159;&#19968;&#31181;&#22256;&#38590;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#26159;&#26368;&#36817;&#26631;&#20934;&#21270;&#30340;&#37327;&#23376;&#21518;-&#23494;&#30721;&#23398;&#65288;PQC&#65289;&#31995;&#32479;&#20013;&#29992;&#20110;&#23494;&#38053;&#20132;&#25442;&#21644;&#25968;&#23383;&#31614;&#21517;&#30340;&#22522;&#30784;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#25915;&#20987;&#26041;&#27861;&#26469;&#25915;&#20987;&#20855;&#26377;&#23567;&#22411;&#21644;&#31232;&#30095;&#23494;&#38053;&#30340;LWE&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#38656;&#35201;&#25968;&#30334;&#19975;&#20010;LWE&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#38656;&#35201;&#20960;&#22825;&#30340;&#26102;&#38388;&#25165;&#33021;&#24674;&#22797;&#23494;&#38053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20851;&#38190;&#26041;&#27861;&#8212;&#8212;&#26356;&#22909;&#30340;&#39044;&#22788;&#29702;&#12289;&#35282;&#23884;&#20837;&#21644;&#27169;&#22411;&#39044;&#35757;&#32451;&#8212;&#8212;&#26469;&#25913;&#36827;&#36825;&#20123;&#25915;&#20987;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#39044;&#22788;&#29702;&#36895;&#24230;25&#20493;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#26679;&#26412;&#25928;&#29575;10&#20493;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;ML&#25915;&#20987;LWE&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#25913;&#36827;&#20351;&#24471;&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#32500;&#24230;&#30340; LWE &#38382;&#39064;&#65306;&#36825;&#26159;&#39318;&#27425;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#26368;&#23567;&#32500;&#24230;$n=1024$&#30340;&#21516;&#24577;&#21152;&#23494;&#24212;&#29992;&#20013;&#24674;&#22797;&#31232;&#30095;&#20108;&#36827;&#21046;&#23494;&#38053;&#30340; ML &#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with Errors (LWE) is a hard math problem underlying recently standardized post-quantum cryptography (PQC) systems for key exchange and digital signatures. Prior work proposed new machine learning (ML)-based attacks on LWE problems with small, sparse secrets, but these attacks require millions of LWE samples to train on and take days to recover secrets. We propose three key methods -- better preprocessing, angular embeddings and model pre-training -- to improve these attacks, speeding up preprocessing by $25\times$ and improving model sample efficiency by $10\times$. We demonstrate for the first time that pre-training improves and reduces the cost of ML attacks on LWE. Our architecture improvements enable scaling to larger-dimension LWE problems: this work is the first instance of ML attacks recovering sparse binary secrets in dimension $n=1024$, the smallest dimension used in practice for homomorphic encryption applications of LWE where sparse binary secrets are proposed.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#32972;&#26223;&#20171;&#32461;&#12289;&#25968;&#23398;&#23450;&#20041;&#12289;&#20998;&#31867;&#24635;&#32467;&#12289;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01077</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#27979;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Predictive Modeling with Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#32972;&#26223;&#20171;&#32461;&#12289;&#25968;&#23398;&#23450;&#20041;&#12289;&#20998;&#31867;&#24635;&#32467;&#12289;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#20351;&#24471;&#22823;&#37327;&#30340;&#25968;&#23383;&#21270;&#24739;&#32773;&#25968;&#25454;&#24471;&#20197;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21253;&#25324;&#21307;&#30103;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;EHR&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;EHR&#25968;&#25454;&#30340;&#32972;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#27979;&#24314;&#27169;&#20219;&#21153;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#39044;&#27979;&#28145;&#24230;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19982;&#21307;&#30103;&#39044;&#27979;&#24314;&#27169;&#30456;&#20851;&#30340;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of electronic health records (EHR) systems has enabled the collection of a vast amount of digitized patient data. However, utilizing EHR data for predictive modeling presents several challenges due to its unique characteristics. With the advancements in machine learning techniques, deep learning has demonstrated its superiority in various applications, including healthcare. This survey systematically reviews recent advances in deep learning-based predictive models using EHR data. Specifically, we begin by introducing the background of EHR data and providing a mathematical definition of the predictive modeling task. We then categorize and summarize predictive deep models from multiple perspectives. Furthermore, we present benchmarks and toolkits relevant to predictive modeling in healthcare. Finally, we conclude this survey by discussing open challenges and suggesting promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20102;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21058;&#37327;&#20307;&#31215;&#30452;&#26041;&#22270;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01076</link><description>&lt;p&gt;
DoseGNN&#65306;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#33258;&#36866;&#24212;&#21058;&#37327;&#20307;&#31215;&#30452;&#26041;&#22270;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
DoseGNN: Improving the Performance of Deep Learning Models in Adaptive Dose-Volume Histogram Prediction through Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20102;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21058;&#37327;&#20307;&#31215;&#30452;&#26041;&#22270;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21058;&#37327;&#20307;&#31215;&#30452;&#26041;&#22270;&#65288;DVH&#65289;&#39044;&#27979;&#26159;&#25918;&#23556;&#27835;&#30103;&#20013;&#22522;&#30784;&#24615;&#30340;&#20219;&#21153;&#65292;&#21487;&#29992;&#20110;&#27835;&#30103;&#35745;&#21010;&#12289;&#21058;&#37327;&#35780;&#20272;&#12289;&#26041;&#26696;&#27604;&#36739;&#31561;&#12290;&#23427;&#21487;&#20197;&#22686;&#21152;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25918;&#23556;&#27835;&#30103;&#33021;&#21147;&#65292;&#21516;&#26102;&#31649;&#29702;&#23545;&#20581;&#24247;&#32452;&#32455;&#30340;&#28508;&#22312;&#27602;&#24615;&#65292;&#20197;&#38477;&#20302;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#25193;&#23637;&#20102;&#22312;AAPM&#65288;AAPM&#31532;65&#23626;&#24180;&#20250;&#21644;&#23637;&#35272;&#20250;&#65289;&#19978;&#25259;&#38706;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#21152;&#20837;&#20102;&#24517;&#35201;&#30340;&#25216;&#26415;&#32454;&#33410;&#12290;&#30740;&#31350;&#30446;&#26631;&#26159;&#35774;&#35745;&#29992;&#20110;&#36890;&#29992;&#25918;&#23556;&#27835;&#30103;&#24179;&#21488;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#24179;&#21488;&#37197;&#22791;&#39640;&#24615;&#33021;CBCT&#31995;&#32479;&#65292;&#36755;&#20837;&#30340;CT&#22270;&#20687;&#21644;&#30446;&#26631;&#21058;&#37327;&#22270;&#20687;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#26469;&#28304;&#12289;&#38388;&#36317;&#21644;&#22823;&#23567;&#12290;&#22312;&#26032;&#22411;&#25918;&#23556;&#27835;&#30103;&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;DVH&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#26500;&#24314;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#20197;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#30340;&#29702;&#24819;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dose-Volume Histogram (DVH) prediction is fundamental in radiation therapy that facilitate treatment planning, dose evaluation, plan comparison and etc. It helps to increase the ability to deliver precise and effective radiation treatments while managing potential toxicities to healthy tissues as needed to reduce the risk of complications. This paper extends recently disclosed research findings presented on AAPM (AAPM 65th Annual Meeting $\&amp;$ Exhibition) and includes necessary technique details. The objective is to design efficient deep learning models for DVH prediction on general radiotherapy platform equipped with high performance CBCT system, where input CT images and target dose images to predict may have different origins, spacing and sizes. Deep learning models widely-adopted in DVH prediction task are evaluated on the novel radiotherapy platform, and graph neural networks (GNNs) are shown to be the ideal architecture to construct a plug-and-play framework to improve predictive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;&#21464;&#33394;&#40857;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23569;&#25968;&#32676;&#20307;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#31995;&#32479;&#36890;&#36807;&#26368;&#23569;&#28155;&#21152;&#21512;&#25104;&#29983;&#25104;&#30340;&#20803;&#32452;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#37319;&#29992;&#25298;&#32477;&#25277;&#26679;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#20803;&#32452;&#30340;&#39640;&#36136;&#37327;&#21644;&#20998;&#24067;&#19968;&#33268;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01071</link><description>&lt;p&gt;
&#21464;&#33394;&#40857;&#65306;&#29992;&#20110;&#22686;&#24378;&#23569;&#25968;&#32676;&#20307;&#35206;&#30422;&#30340;&#20844;&#24179;&#24863;&#30693;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Chameleon: Foundation Models for Fairness-aware Multi-modal Data Augmentation to Enhance Coverage of Minorities
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;&#21464;&#33394;&#40857;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23569;&#25968;&#32676;&#20307;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#31995;&#32479;&#36890;&#36807;&#26368;&#23569;&#28155;&#21152;&#21512;&#25104;&#29983;&#25104;&#30340;&#20803;&#32452;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#37319;&#29992;&#25298;&#32477;&#25277;&#26679;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#20803;&#32452;&#30340;&#39640;&#36136;&#37327;&#21644;&#20998;&#24067;&#19968;&#33268;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#25968;&#32676;&#20307;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#27424;&#34920;&#31034;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#30340;&#21361;&#23475;&#65292;&#23588;&#20854;&#26159;&#22312;&#22810;&#27169;&#24577;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22312;&#26816;&#27979;&#36825;&#31181;&#27424;&#34920;&#31034;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21162;&#21147;&#65292;&#20294;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#25104;&#20026;&#22810;&#21151;&#33021;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21464;&#33394;&#40857;&#65288;Chameleon&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#26368;&#23569;&#30340;&#21512;&#25104;&#29983;&#25104;&#30340;&#20803;&#32452;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#65292;&#20197;&#22686;&#24378;&#23569;&#25968;&#32676;&#20307;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#37319;&#29992;&#20102;&#25298;&#32477;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#20803;&#32452;&#20855;&#26377;&#39640;&#36136;&#37327;&#24182;&#36981;&#24490;&#22522;&#30784;&#20998;&#24067;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#29983;&#25104;&#20803;&#32452;&#30340;&#25298;&#32477;&#20960;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#20026;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#20165;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20063;&#34920;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential harms of the under-representation of minorities in training data, particularly in multi-modal settings, is a well-recognized concern. While there has been extensive effort in detecting such under-representation, resolution has remained a challenge. With recent advancements in generative AI, large language models and foundation models have emerged as versatile tools across various domains. In this paper, we propose Chameleon, a system that efficiently utilizes these tools to augment a data set with a minimal addition of synthetically generated tuples, in order to enhance the coverage of the under-represented groups. Our system follows a rejection sampling approach to ensure the generated tuples have a high quality and follow the underlying distribution. In order to minimize the rejection chance of the generated tuples, we propose multiple strategies for providing a guide for the foundation model. Our experiment results, in addition to confirming the efficiency of our propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedShift&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#36801;&#31227;&#32858;&#21512;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01070</link><description>&lt;p&gt;
FedShift: &#36890;&#36807;&#26435;&#37325;&#36801;&#31227;&#32858;&#21512;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#30340;&#21452;&#37325;&#24322;&#36136;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via Weight Shift Aggregation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedShift&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#36801;&#31227;&#32858;&#21512;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#27880;&#37325;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26377;&#21147;&#26041;&#27861;&#12290;FL&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24322;&#36136;&#24615;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#28304;&#20110;&#23458;&#25143;&#31471;&#30828;&#20214;&#12289;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#22810;&#26679;&#24615;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36890;&#20449;&#25928;&#29575;&#25110;&#31283;&#23450;&#25910;&#25947;&#31639;&#27861;&#26469;&#35299;&#20915;&#31995;&#32479;&#25110;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#20294;&#21333;&#29420;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24448;&#24448;&#20250;&#23548;&#33268;&#22949;&#21327;&#65292;&#22240;&#20026;&#24322;&#36136;&#24615;&#38382;&#39064;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedShift&#30340;&#26032;&#31639;&#27861;&#65292;&#26088;&#22312;&#22312;&#21452;&#37325;&#24322;&#36136;&#24615;&#22330;&#26223;&#20013;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#37327;&#21270;&#25913;&#21892;&#23458;&#25143;&#21442;&#19982;&#24230;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#36801;&#31227;&#25216;&#26415;&#26469;&#32531;&#35299;&#37327;&#21270;&#36890;&#24120;&#23548;&#33268;&#30340;&#24615;&#33021;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) offers a compelling method for training machine learning models with a focus on preserving data privacy. The presence of system heterogeneity and statistical heterogeneity, recognized challenges in FL, arises from the diversity of client hardware, network, and dataset distribution. This diversity can critically affect the training pace and the performance of models. While many studies address either system or statistical heterogeneity by introducing communication-efficient or stable convergence algorithms, addressing these challenges in isolation often leads to compromises due to unaddressed heterogeneity. In response, this paper introduces FedShift, a novel algorithm designed to enhance both the training speed and the models' accuracy in a dual heterogeneity scenario. Our solution can improve client engagement through quantization and mitigate the adverse effects on performance typically associated with quantization by employing a shifting technique. This techn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35780;&#20272;Inspire&#27835;&#30103;&#30340;&#24739;&#32773;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#20998;&#26512;&#21307;&#23398;&#25968;&#25454;&#21644;DISE&#35270;&#39057;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01067</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;Inspire&#27835;&#30103;&#30340;&#24739;&#32773;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Patient Eligibility for Inspire Therapy through Machine Learning and Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35780;&#20272;Inspire&#27835;&#30103;&#30340;&#24739;&#32773;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#20998;&#26512;&#21307;&#23398;&#25968;&#25454;&#21644;DISE&#35270;&#39057;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Inspire&#27835;&#30103;&#26159;&#19968;&#31181;&#33719;&#24471;FDA&#25209;&#20934;&#30340;&#20869;&#37096;&#31070;&#32463;&#21050;&#28608;&#27835;&#30103;&#38459;&#22622;&#24615;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#30151;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#26159;&#25152;&#26377;&#24739;&#32773;&#37117;&#23545;&#36825;&#31181;&#30103;&#27861;&#20135;&#29983;&#21453;&#24212;&#65292;&#36825;&#23545;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#32819;&#40763;&#21897;&#31185;&#21307;&#29983;&#26469;&#35828;&#37117;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#20182;&#20204;&#24456;&#38590;&#30830;&#23450;&#36866;&#23452;&#27835;&#30103;&#30340;&#20505;&#36873;&#32773;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#21307;&#23398;&#25968;&#25454;&#21644;&#36890;&#36807;&#33647;&#29289;&#35825;&#23548;&#30561;&#30496;&#20869;&#31397;&#38236;&#26816;&#26597;&#65288;DISE&#65289;&#25429;&#25417;&#21040;&#30340;&#35270;&#39057;&#26469;&#21028;&#26029;&#24739;&#32773;&#23545;Inspire&#27835;&#30103;&#30340;&#21453;&#24212;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#26469;&#33258;127&#21517;&#24739;&#32773;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#20004;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#20851;&#27880;&#20110;&#33292;&#26681;&#21644;&#40763;&#21693;&#30340;&#20869;&#31397;&#38236;&#35270;&#39057;&#12290;&#31532;&#19977;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#24739;&#32773;&#30340;&#20020;&#24202;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#20845;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#20116;&#20010;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspire therapy is an FDA-approved internal neurostimulation treatment for obstructive sleep apnea. However, not all patients respond to this therapy, posing a challenge even for experienced otolaryngologists to determine candidacy. This paper makes the first attempt to leverage both machine learning and deep learning techniques in discerning patient responsiveness to Inspire therapy using medical data and videos captured through Drug-Induced Sleep Endoscopy (DISE), an essential procedure for Inspire therapy. To achieve this, we gathered and annotated three datasets from 127 patients. Two of these datasets comprise endoscopic videos focused on the Base of the Tongue and Velopharynx. The third dataset composes the patient's clinical information. By utilizing these datasets, we benchmarked and compared the performance of six deep learning models and five classical machine learning algorithms. The results demonstrate the potential of employing machine learning and deep learning techniques
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#24037;&#36827;&#21270;&#30830;&#23450;&#23545;&#21463;&#25439;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21010;&#27700;&#26426;&#21046;&#30340;&#26368;&#20339;&#25913;&#21464;&#65292;&#24182;&#36890;&#36807;&#34917;&#20607;&#26426;&#21046;&#23454;&#29616;&#24674;&#22797;&#25512;&#21147;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01062</link><description>&lt;p&gt;
&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#34917;&#20607;&#24615;&#25439;&#20260;&#20462;&#22797;&#31574;&#30053;&#22312;&#25391;&#32709;&#26426;&#22120;&#20154;&#25512;&#36827;&#22120;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bio-Inspired Compensatory Strategies for Damage to Flapping Robotic Propulsors
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01062
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#36827;&#21270;&#30830;&#23450;&#23545;&#21463;&#25439;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21010;&#27700;&#26426;&#21046;&#30340;&#26368;&#20339;&#25913;&#21464;&#65292;&#24182;&#36890;&#36807;&#34917;&#20607;&#26426;&#21046;&#23454;&#29616;&#24674;&#22797;&#25512;&#21147;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25345;&#23436;&#20840;&#30340;&#33258;&#20027;&#24615;&#65292;&#33258;&#20027;&#26426;&#22120;&#20154;&#31995;&#32479;&#24517;&#39035;&#20855;&#22791;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#34917;&#20607;&#26426;&#21046;&#36827;&#34892;&#33258;&#25105;&#20462;&#22797;&#22312;&#33258;&#28982;&#30028;&#20013;&#24456;&#24120;&#35265;&#65306;&#20363;&#22914;&#65292;&#19968;&#20123;&#40060;&#31867;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#21010;&#27700;&#26426;&#21046;&#65292;&#22312;&#20002;&#22833;76%&#30340;&#25512;&#21160;&#34920;&#38754;&#30340;&#24773;&#20917;&#19979;&#20063;&#19981;&#20250;&#20007;&#22833;&#25512;&#21147;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#36825;&#20123;&#25913;&#21464;&#20174;&#29983;&#29289;&#20307;&#20256;&#36882;&#21040;&#26426;&#22120;&#20154;&#25391;&#32709;&#25512;&#36827;&#22120;&#19978;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#19981;&#30456;&#20851;&#30340;&#36827;&#21270;&#21387;&#21147;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#20154;&#24037;&#36827;&#21270;&#30830;&#23450;&#23545;&#21463;&#25439;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21010;&#27700;&#26426;&#21046;&#30340;&#26368;&#20248;&#25913;&#21464;&#12290;&#20026;&#20102;&#30830;&#23450;&#33258;&#28982;&#21644;&#26426;&#22120;&#23398;&#20064;&#24471;&#21040;&#30340;&#26368;&#20248;&#35299;&#26159;&#21542;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#30340;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#65292;&#23547;&#27714;&#32473;&#23450;&#21147;&#30340;&#26368;&#39640;&#25928;&#36712;&#36857;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#30828;&#20214;-&#29615;&#36335;&#22312;&#32447;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24102;&#26377;&#21160;&#20316;&#24367;&#26354;&#24179;&#26495;&#30340;&#35797;&#39564;&#21151;&#33021;&#35780;&#20272;&#12290;&#20026;&#20102;&#24674;&#22797;&#37096;&#20998;&#25130;&#32930;&#21518;&#30340;&#25512;&#21147;&#20135;&#29983;&#65292;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#34917;&#20607;&#26426;&#21046;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
To maintain full autonomy, autonomous robotic systems must have the ability to self-repair. Self-repairing via compensatory mechanisms appears in nature: for example, some fish can lose even 76% of their propulsive surface without loss of thrust by altering stroke mechanics. However, direct transference of these alterations from an organism to a robotic flapping propulsor may not be optimal due to irrelevant evolutionary pressures. We instead seek to determine what alterations to stroke mechanics are optimal for a damaged robotic system via artificial evolution. To determine whether natural and machine-learned optima differ, we employ a cyber-physical system using a Covariance Matrix Adaptation Evolutionary Strategy to seek the most efficient trajectory for a given force. We implement an online optimization with hardware-in-the-loop, performing experimental function evaluations with an actuated flexible flat plate. To recoup thrust production following partial amputation, the most effi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#22810;&#39033;&#24335;&#30340;&#20989;&#25968;&#36817;&#20284;&#20195;&#25968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#19968;&#23450;&#21442;&#25968;&#38480;&#21046;&#19979;&#33021;&#22815;&#26377;&#25928;&#36817;&#20284;&#23454;&#25968;&#20989;&#25968;&#65292;&#24182;&#19988;&#20854;&#21442;&#25968;&#21644;&#28145;&#24230;&#22686;&#38271;&#19982;&#25152;&#38656;&#31934;&#24230;&#22810;&#39033;&#24335;&#30456;&#20851;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01058</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#22810;&#39033;&#24335;&#30340;&#20989;&#25968;&#36817;&#20284;&#20195;&#25968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards an Algebraic Framework For Approximating Functions Using Neural Network Polynomials
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#22810;&#39033;&#24335;&#30340;&#20989;&#25968;&#36817;&#20284;&#20195;&#25968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#19968;&#23450;&#21442;&#25968;&#38480;&#21046;&#19979;&#33021;&#22815;&#26377;&#25928;&#36817;&#20284;&#23454;&#25968;&#20989;&#25968;&#65292;&#24182;&#19988;&#20854;&#21442;&#25968;&#21644;&#28145;&#24230;&#22686;&#38271;&#19982;&#25152;&#38656;&#31934;&#24230;&#22810;&#39033;&#24335;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35770;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#35937;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25193;&#23637;&#20102;&#31532;2&#31456;&#20013;&#35814;&#32454;&#35299;&#37322;&#30340;&#24050;&#26377;&#31070;&#32463;&#32593;&#32476;&#24494;&#31215;&#20998;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23637;&#31034;&#31070;&#32463;&#32593;&#32476;&#22810;&#39033;&#24335;&#12289;&#31070;&#32463;&#32593;&#32476;&#25351;&#25968;&#20989;&#25968;&#12289;&#27491;&#24358;&#21644;&#20313;&#24358;&#30340;&#23384;&#22312;&#24615;&#65292;&#21363;&#23427;&#20204;&#30830;&#23454;&#22312;&#19968;&#23450;&#21442;&#25968;$q$&#21644;$\varepsilon$&#30340;&#38480;&#21046;&#19979;&#36817;&#20284;&#20110;&#23454;&#25968;&#23545;&#24212;&#29289;&#12290;&#22312;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21442;&#25968;&#21644;&#28145;&#24230;&#30340;&#22686;&#38271;&#20165;&#20197;&#22810;&#39033;&#24335;&#24418;&#24335;&#19982;&#25152;&#38656;&#30340;&#31934;&#24230;&#30456;&#20851;&#65288;&#23558;&#20854;&#23450;&#20041;&#20026;$\mathbb{R}$&#19978;&#30340;1-&#33539;&#25968;&#24046;&#24322;&#65289;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#36825;&#31181;&#36817;&#20284;&#26041;&#27861;&#20013;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#20855;&#26377;&#25152;&#36817;&#20284;&#20989;&#25968;&#30340;&#32467;&#26500;&#29305;&#24615;&#24182;&#38750;&#23436;&#20840;&#19981;&#21487;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We make the case for neural network objects and extend an already existing neural network calculus explained in detail in Chapter 2 on \cite{bigbook}. Our aim will be to show that, yes, indeed, it makes sense to talk about neural network polynomials, neural network exponentials, sine, and cosines in the sense that they do indeed approximate their real number counterparts subject to limitations on certain of their parameters, $q$, and $\varepsilon$. While doing this, we show that the parameter and depth growth are only polynomial on their desired accuracy (defined as a 1-norm difference over $\mathbb{R}$), thereby showing that this approach to approximating, where a neural network in some sense has the structural properties of the function it is approximating is not entire intractable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;TDIL&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36716;&#25442;&#37492;&#21035;&#22120;&#30340;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#40723;&#21169;&#20195;&#29702;&#21521;&#38752;&#36817;&#19987;&#23478;&#29366;&#24577;&#30340;&#29366;&#24577;&#23548;&#33322;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01057</link><description>&lt;p&gt;
&#19987;&#23478;&#25509;&#36817;&#24615;&#20316;&#20026;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#30340;&#26367;&#20195;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;TDIL&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36716;&#25442;&#37492;&#21035;&#22120;&#30340;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#40723;&#21169;&#20195;&#29702;&#21521;&#38752;&#36817;&#19987;&#23478;&#29366;&#24577;&#30340;&#29366;&#24577;&#23548;&#33322;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33719;&#21462;&#22823;&#37327;&#19987;&#23478;&#28436;&#31034;&#22256;&#38590;&#25110;&#19981;&#21487;&#34892;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#19982; typicIL &#35774;&#32622;&#20013;&#20855;&#26377;&#22810;&#20010;&#31034;&#33539;&#19981;&#21516;&#65292;&#21333;&#28436;&#31034;IL&#28041;&#21450;&#20195;&#29702;&#21482;&#26377;&#19968;&#26465;&#19987;&#23478;&#36712;&#36857;&#30340;&#35775;&#38382;&#12290;&#25105;&#20204;&#24378;&#35843;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#36716;&#25442;&#37492;&#21035;&#22120;&#30340;IL&#65288;TDIL&#65289;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;TDIL&#26159;&#19968;&#31181;&#22522;&#20110;IRL&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#32771;&#34385;&#29615;&#22659;&#21160;&#24577;&#30340;&#26356;&#23494;&#38598;&#30340;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#26469;&#35299;&#20915;&#22870;&#21169;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#40723;&#21169;&#20195;&#29702;&#21521;&#38752;&#36817;&#19987;&#23478;&#29366;&#24577;&#30340;&#29366;&#24577;&#23548;&#33322;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;TDIL&#35757;&#32451;&#19968;&#20010;&#36807;&#28193;&#37492;&#21035;&#22120;&#26469;&#21306;&#20998;&#32473;&#23450;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#21644;&#38750;&#26377;&#25928;&#36807;&#28193;&#20197;&#35745;&#31639;&#26367;&#20195;&#22870;&#21169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;TDIL&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where obtaining numerous expert demonstrations is costly or infeasible. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#30340;&#22810;&#31867;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20998;&#21035;&#36866;&#29992;&#20110;&#21333;&#35843;&#20984;&#24615;&#21644;&#32447;&#24615;&#27604;&#29575;&#20004;&#31867;&#24615;&#33021;&#24230;&#37327;&#65292;&#24182;&#22522;&#20110;&#31867;&#26465;&#20214;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#22122;&#22768;&#26657;&#27491;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01055</link><description>&lt;p&gt;
&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#30340;&#22810;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#30340;&#22810;&#31867;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20998;&#21035;&#36866;&#29992;&#20110;&#21333;&#35843;&#20984;&#24615;&#21644;&#32447;&#24615;&#27604;&#29575;&#20004;&#31867;&#24615;&#33021;&#24230;&#37327;&#65292;&#24182;&#22522;&#20110;&#31867;&#26465;&#20214;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#22122;&#22768;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23398;&#20064;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#24471;&#21040;&#33391;&#22909;&#20998;&#31867;&#22120;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#26631;&#20934;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#24615;&#33021;&#24230;&#37327;&#19978;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#65292;&#36825;&#20123;&#24230;&#37327;&#19981;&#33021;&#34920;&#31034;&#20026;&#21333;&#20010;&#31034;&#20363;&#19978;&#30340;&#25439;&#22833;&#30340;&#26399;&#26395;&#25110;&#24635;&#21644;&#65307;&#20854;&#20013;&#21253;&#25324;&#31867;&#19981;&#24179;&#34913;&#35774;&#32622;&#20013;&#30340;H-mean&#65292;Q-mean&#21644;G-mean&#65292;&#20197;&#21450;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;Micro F1&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20004;&#31867;&#24191;&#27867;&#30340;&#22810;&#31867;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#65292;&#21363;&#21333;&#35843;&#20984;&#24615;&#21644;&#32447;&#24615;&#27604;&#29575;&#65292;&#23427;&#20204;&#21253;&#25324;&#19978;&#36848;&#25152;&#26377;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;Narasimhan&#31561;&#20154;&#30340;Frank-Wolfe&#21644;Bisection&#31639;&#27861;(2015)&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30740;&#31350;&#30340;&#31867;&#26465;&#20214;&#22122;&#22768;&#27169;&#22411;&#23478;&#26063;&#19979;&#24320;&#21457;&#20102;&#31639;&#27861;&#30340;&#22122;&#22768;&#26657;&#27491;&#29256;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36951;&#25022;(&#36229;&#39069;&#39118;&#38505;)&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been much interest in recent years in learning good classifiers from data with noisy labels. Most work on learning from noisy labels has focused on standard loss-based performance measures. However, many machine learning problems require using non-decomposable performance measures which cannot be expressed as the expectation or sum of a loss on individual examples; these include for example the H-mean, Q-mean and G-mean in class imbalance settings, and the Micro $F_1$ in information retrieval. In this paper, we design algorithms to learn from noisy labels for two broad classes of multiclass non-decomposable performance measures, namely, monotonic convex and ratio-of-linear, which encompass all the above examples. Our work builds on the Frank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both cases, we develop noise-corrected versions of the algorithms under the widely studied family of class-conditional noise models. We provide regret (excess risk) bounds 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#31243;&#24230;&#20197;&#21450;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01054</link><description>&lt;p&gt;
&#26080;&#26465;&#20214;&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#35760;&#24518;&#24739;&#32773;&#24433;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unconditional Latent Diffusion Models Memorize Patient Imaging Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#31243;&#24230;&#20197;&#21450;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#24212;&#29992;&#26159;&#36890;&#36807;&#25552;&#20986;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#24320;&#25918;&#25968;&#25454;&#20849;&#20139;&#12290;&#23613;&#31649;&#26377;&#36825;&#20010;&#24212;&#29992;&#30340;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#24739;&#32773;&#25968;&#25454;&#30340;&#35760;&#24518;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#29983;&#25104;&#24739;&#32773;&#25968;&#25454;&#30340;&#21103;&#26412;&#32780;&#19981;&#26159;&#26032;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#36825;&#30772;&#22351;&#20102;&#20445;&#25252;&#24739;&#32773;&#25968;&#25454;&#30340;&#25972;&#20010;&#30446;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#24739;&#32773;&#34987;&#37325;&#26032;&#35782;&#21035;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#30028;&#20013;&#36825;&#20010;&#38382;&#39064;&#24182;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;2D&#21644;3D&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;CT&#12289;MR&#21644;X&#20809;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#26469;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#34987;&#35760;&#24518;&#30340;&#31243;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative latent diffusion models hold a wide range of applications in the medical imaging domain. A noteworthy application is privacy-preserved open-data sharing by proposing synthetic data as surrogates of real patient data. Despite the promise, these models are susceptible to patient data memorization, where models generate patient data copies instead of novel synthetic samples. This undermines the whole purpose of preserving patient data and may even result in patient re-identification. Considering the importance of the problem, surprisingly it has received relatively little attention in the medical imaging community. To this end, we assess memorization in latent diffusion models for medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR, and X-ray datasets for synthetic data generation. Afterwards, we examine the amount of training data memorized utilizing self-supervised models and further investigate various factors that can possibly lead to memorization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#36870;&#38382;&#39064;&#30340;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#30340;&#19968;&#33324;&#21270;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#19968;&#31867;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#30340;&#23454;&#29616;&#21487;&#20197;&#36798;&#21040;&#25910;&#25947;&#65292;&#24182;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#20013;&#23454;&#29616;&#20102;&#23545;&#35745;&#31639;&#26426;&#23618;&#26512;&#25104;&#20687;&#20013;&#23398;&#20064;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#22120;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01052</link><description>&lt;p&gt;
&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#22312;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65306;&#20020;&#30028;&#28857;&#21644;&#21407;&#22987;-&#23545;&#20598;&#20248;&#21270;&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#36870;&#38382;&#39064;&#30340;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#30340;&#19968;&#33324;&#21270;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#19968;&#31867;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#30340;&#23454;&#29616;&#21487;&#20197;&#36798;&#21040;&#25910;&#25947;&#65292;&#24182;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#20013;&#23454;&#29616;&#20102;&#23545;&#35745;&#31639;&#26426;&#23618;&#26512;&#25104;&#20687;&#20013;&#23398;&#20064;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#22120;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27491;&#21017;&#21270;&#26159;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#26368;&#36817;&#26377;&#24456;&#22810;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35299;&#20915;&#36825;&#31181;&#27491;&#21017;&#21270;&#25910;&#25947;&#24615;&#30340;&#38382;&#39064;&#19978;&#65292;&#24456;&#23569;&#26377;&#20851;&#20110;&#20020;&#30028;&#28857;&#25910;&#25947;&#24615;&#30340;&#32467;&#26524;&#65292;&#32780;&#38750;&#20840;&#23616;&#26497;&#23567;&#20540;&#28857;&#30340;&#25910;&#25947;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20020;&#30028;&#28857;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#21270;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#26159;&#36890;&#36807;&#19968;&#31867;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#30456;&#20851;&#21464;&#20998;&#38382;&#39064;&#30456;&#20851;&#30340;&#21407;&#22987;-&#23545;&#20598;&#28151;&#21512;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#32473;&#23450;Kurdyka-Lojasiewicz&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;O(log(k)/k)&#30340;&#36951;&#20256;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36755;&#20837;&#20026;&#24369;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;IWCNN&#65289;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;IWCNN&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#23618;&#26512;&#25104;&#20687;&#20013;&#23398;&#20064;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational regularisation is the primary method for solving inverse problems, and recently there has been considerable work leveraging deeply learned regularisation for enhanced performance. However, few results exist addressing the convergence of such regularisation, particularly within the context of critical points as opposed to global minima. In this paper, we present a generalised formulation of convergent regularisation in terms of critical points, and show that this is achieved by a class of weakly convex regularisers. We prove convergence of the primal-dual hybrid gradient method for the associated variational problem, and, given a Kurdyka-Lojasiewicz condition, an $\mathcal{O}(\log{k}/k)$ ergodic convergence rate. Finally, applying this theory to learned regularisation, we prove universal approximation for input weakly convex neural networks (IWCNN), and show empirically that IWCNNs can lead to improved performance of learned adversarial regularisers for computed tomography (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;MCMC&#25512;&#29702;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#28508;&#22312;&#20998;&#22359;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#20540;&#21644;&#29305;&#24449;&#21010;&#20998;&#20026;&#20998;&#21306;&#65292;&#24182;&#37319;&#29992;Master/Worker&#26550;&#26500;&#26469;&#25552;&#39640;&#32858;&#31867;&#26631;&#31614;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01050</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;MCMC&#25512;&#29702;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#28508;&#22312;&#20998;&#22359;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distributed MCMC inference for Bayesian Non-Parametric Latent Block Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;MCMC&#25512;&#29702;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#28508;&#22312;&#20998;&#22359;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#20540;&#21644;&#29305;&#24449;&#21010;&#20998;&#20026;&#20998;&#21306;&#65292;&#24182;&#37319;&#29992;Master/Worker&#26550;&#26500;&#26469;&#25552;&#39640;&#32858;&#31867;&#26631;&#31614;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#28508;&#22312;&#20998;&#22359;&#27169;&#22411;&#65288;DisNPLBM&#65289;&#65292;&#37319;&#29992;Master/Worker&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#38750;&#21442;&#25968;&#20849;&#32858;&#31867;&#31639;&#27861;&#20351;&#29992;&#28508;&#22312;&#22810;&#20803;&#39640;&#26031;&#22359;&#20998;&#24067;&#23558;&#35266;&#27979;&#20540;&#21644;&#29305;&#24449;&#21010;&#20998;&#20026;&#20998;&#21306;&#12290;&#34892;&#19978;&#30340;&#24037;&#20316;&#36127;&#36733;&#22343;&#21248;&#20998;&#24067;&#22312;&#24037;&#20154;&#20043;&#38388;&#65292;&#20182;&#20204;&#21482;&#19982;&#20027;&#33410;&#28857;&#36827;&#34892;&#36890;&#20449;&#65292;&#32780;&#19981;&#19982;&#24444;&#27492;&#36890;&#20449;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;DisNPLBM&#35777;&#26126;&#20102;&#20854;&#23545;&#32858;&#31867;&#26631;&#31614;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20849;&#21516;&#32858;&#31867;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#26469;&#23637;&#31034;&#19968;&#20010;&#30495;&#23454;&#30340;&#29992;&#20363;&#12290;&#20195;&#30721;&#28304;&#21487;&#20844;&#24320;&#35775;&#38382;https://github.com/redakhoufache/Distributed-NPLBM&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel Distributed Markov Chain Monte Carlo (MCMC) inference method for the Bayesian Non-Parametric Latent Block Model (DisNPLBM), employing the Master/Worker architecture. Our non-parametric co-clustering algorithm divides observations and features into partitions using latent multivariate Gaussian block distributions. The workload on rows is evenly distributed among workers, who exclusively communicate with the master and not among themselves. DisNPLBM demonstrates its impact on cluster labeling accuracy and execution times through experimental results. Moreover, we present a real-use case applying our approach to co-cluster gene expression data. The code source is publicly available at https://github.com/redakhoufache/Distributed-NPLBM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;FPGA&#19978;&#39640;&#25928;&#23454;&#29616;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31890;&#23376;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#35302;&#21457;&#22120;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#20302;&#20110;2&#24494;&#31186;&#30340;&#24310;&#36831;&#65292;&#31526;&#21512;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#23454;&#39564;&#30340;&#35201;&#27714;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01047</link><description>&lt;p&gt;
&#22522;&#20110;FPGA&#30340;&#36229;&#24555;&#36895;&#21464;&#21387;&#22120;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Ultra Fast Transformers on FPGAs for Particle Physics Experiments
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;FPGA&#19978;&#39640;&#25928;&#23454;&#29616;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31890;&#23376;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#35302;&#21457;&#22120;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#20302;&#20110;2&#24494;&#31186;&#30340;&#24310;&#36831;&#65292;&#31526;&#21512;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#23454;&#39564;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;"hls4ml"&#24037;&#20855;&#22312;Field-Programmable Gate Array&#65288;FPGA&#65289;&#19978;&#23454;&#29616;&#20102;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#37492;&#20110;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#65292;&#23427;&#20204;&#22312;&#31890;&#23376;&#29289;&#29702;&#23454;&#39564;&#20013;&#23454;&#39564;&#35302;&#21457;&#22120;&#30340;&#24212;&#29992;&#25104;&#20026;&#19968;&#20010;&#38750;&#24120;&#24863;&#20852;&#36259;&#30340;&#35805;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#22914;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;softmax&#23618;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#23454;&#29616;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31890;&#23376;&#29289;&#29702;&#23398;&#21943;&#27880;&#39118;&#21619;&#26631;&#35760;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35760;&#24405;&#19979;&#20102;&#22312;Xilinx UltraScale+ FPGA&#19978;&#20302;&#20110;2&#24494;&#31186;&#30340;&#24310;&#36831;&#65292;&#36825;&#31526;&#21512;&#27431;&#27954;&#26680;&#23376;&#30740;&#31350;&#20013;&#24515;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#23454;&#39564;&#30340;&#30828;&#20214;&#35302;&#21457;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a highly efficient implementation of the transformer architecture on a Field-Programmable Gate Array (FPGA) by using the \texttt{hls4ml} tool. Given the demonstrated effectiveness of transformer models in addressing a wide range of problems, their application in experimental triggers within particle physics becomes a subject of significant interest. In this work, we have implemented critical components of a transformer model, such as multi-head attention and softmax layers. To evaluate the effectiveness of our implementation, we have focused on a particle physics jet flavor tagging problem, employing a public dataset. We recorded latency under 2 $\mu$s on the Xilinx UltraScale+ FPGA, which is compatible with hardware trigger requirements at the CERN Large Hadron Collider experiments.
&lt;/p&gt;</description></item><item><title>LatticeGraphNet&#26159;&#19968;&#31181;&#21452;&#23610;&#24230;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#26684;&#29366;&#32467;&#26500;&#30340;&#38477;&#32500;&#21160;&#21147;&#23398;&#21644;&#38477;&#32500;&#34920;&#31034;&#21040;&#22235;&#38754;&#20307;&#32593;&#26684;&#30340;&#26144;&#23556;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#39044;&#27979;&#20219;&#24847;&#26684;&#29366;&#32467;&#26500;&#30340;&#21464;&#24418;&#65292;&#24182;&#22312;&#26174;&#33879;&#20943;&#23569;&#25512;&#26029;&#26102;&#38388;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01045</link><description>&lt;p&gt;
LatticeGraphNet: &#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#26684;&#29366;&#32467;&#26500;&#30340;&#21452;&#23610;&#24230;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;
&lt;/p&gt;
&lt;p&gt;
LatticeGraphNet: A two-scale graph neural operator for simulating lattice structures
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01045
&lt;/p&gt;
&lt;p&gt;
LatticeGraphNet&#26159;&#19968;&#31181;&#21452;&#23610;&#24230;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#26684;&#29366;&#32467;&#26500;&#30340;&#38477;&#32500;&#21160;&#21147;&#23398;&#21644;&#38477;&#32500;&#34920;&#31034;&#21040;&#22235;&#38754;&#20307;&#32593;&#26684;&#30340;&#26144;&#23556;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#39044;&#27979;&#20219;&#24847;&#26684;&#29366;&#32467;&#26500;&#30340;&#21464;&#24418;&#65292;&#24182;&#22312;&#26174;&#33879;&#20943;&#23569;&#25512;&#26029;&#26102;&#38388;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21452;&#23610;&#24230;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;&#65292;&#21363;LatticeGraphNet&#65288;LGN&#65289;&#65292;&#26088;&#22312;&#20316;&#20026;&#26114;&#36149;&#30340;&#38750;&#32447;&#24615;&#26377;&#38480;&#20803;&#27169;&#25311;&#19977;&#32500;&#26684;&#29366;&#38646;&#37096;&#20214;&#21644;&#32467;&#26500;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;LGN&#20855;&#26377;&#20004;&#20010;&#32593;&#32476;&#65306;LGN-i&#29992;&#20110;&#23398;&#20064;&#26684;&#29366;&#32467;&#26500;&#30340;&#38477;&#32500;&#21160;&#21147;&#23398;&#65292;LGN-ii&#29992;&#20110;&#23398;&#20064;&#20174;&#38477;&#32500;&#34920;&#31034;&#21040;&#22235;&#38754;&#20307;&#32593;&#26684;&#30340;&#26144;&#23556;&#12290;LGN&#33021;&#22815;&#23545;&#20219;&#24847;&#26684;&#29366;&#32467;&#26500;&#36827;&#34892;&#21464;&#24418;&#39044;&#27979;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#25805;&#20316;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#25512;&#26029;&#26102;&#38388;&#65292;&#20026;&#35780;&#20272;&#26684;&#29366;&#32467;&#26500;&#30340;&#21147;&#23398;&#21709;&#24212;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a two-scale Graph Neural Operator (GNO), namely, LatticeGraphNet (LGN), designed as a surrogate model for costly nonlinear finite-element simulations of three-dimensional latticed parts and structures. LGN has two networks: LGN-i, learning the reduced dynamics of lattices, and LGN-ii, learning the mapping from the reduced representation onto the tetrahedral mesh. LGN can predict deformation for arbitrary lattices, therefore the name operator. Our approach significantly reduces inference time while maintaining high accuracy for unseen simulations, establishing the use of GNOs as efficient surrogate models for evaluating mechanical responses of lattices and structures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#26102;&#38388;&#19981;&#40784;&#27425;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#32791;&#25955;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;Langevin&#21160;&#21147;&#23398;&#30340;&#27010;&#29575;&#36716;&#31227;&#26041;&#31243;&#26500;&#36896;&#20026;&#20851;&#20110;&#26102;&#38388;&#20381;&#36182;&#30340;&#26368;&#20248;&#20256;&#36755;&#24230;&#37327;&#30340;&#20462;&#27491;&#26799;&#24230;&#27969;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#20960;&#20010;&#26102;&#38388;&#19981;&#40784;&#27425;Langevin&#21160;&#21147;&#23398;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01036</link><description>&lt;p&gt;
&#26102;&#38388;&#19981;&#40784;&#27425;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#32791;&#25955;
&lt;/p&gt;
&lt;p&gt;
Fisher information dissipation for time inhomogeneous stochastic differential equations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#26102;&#38388;&#19981;&#40784;&#27425;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#32791;&#25955;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;Langevin&#21160;&#21147;&#23398;&#30340;&#27010;&#29575;&#36716;&#31227;&#26041;&#31243;&#26500;&#36896;&#20026;&#20851;&#20110;&#26102;&#38388;&#20381;&#36182;&#30340;&#26368;&#20248;&#20256;&#36755;&#24230;&#37327;&#30340;&#20462;&#27491;&#26799;&#24230;&#27969;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#20960;&#20010;&#26102;&#38388;&#19981;&#40784;&#27425;Langevin&#21160;&#21147;&#23398;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#26102;&#38388;&#19981;&#40784;&#27425;&#21464;&#31995;&#25968;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDEs)&#25552;&#20379;&#20102;&#19968;&#20010;Lyapunov&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#19977;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#21253;&#25324;&#36807;&#38459;&#23612;&#12289;&#19981;&#21487;&#36870;&#28418;&#31227;&#21644;&#27424;&#38459;&#23612;Langevin&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;Langevin&#21160;&#21147;&#23398;&#30340;&#27010;&#29575;&#36716;&#31227;&#26041;&#31243;&#26500;&#36896;&#20026;&#27010;&#29575;&#31354;&#38388;&#20013;&#20851;&#20110;&#26102;&#38388;&#20381;&#36182;&#30340;&#26368;&#20248;&#20256;&#36755;&#24230;&#37327;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#20462;&#27491;&#26799;&#24230;&#27969;&#12290;&#36825;&#20010;&#20844;&#24335;&#21253;&#21547;&#20102;&#26799;&#24230;&#21644;&#38750;&#26799;&#24230;&#26041;&#21521;&#65292;&#21462;&#20915;&#20110;&#19968;&#31867;&#26102;&#38388;&#20381;&#36182;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#36873;&#25321;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;&#30456;&#23545;&#36153;&#33293;&#23572;&#20449;&#24687;&#27867;&#20989;&#20316;&#20026;Lyapunov&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;Hessian&#30697;&#38453;&#26465;&#20214;&#65292;&#20445;&#35777;&#20102;SDE&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#20960;&#20010;&#26102;&#38388;&#19981;&#40784;&#27425;Langevin&#21160;&#21147;&#23398;&#30340;&#25152;&#25552;&#20986;&#30340;&#26465;&#20214;&#12290;&#23545;&#20110;&#36807;&#38459;&#23612;Langevin&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$L^1$&#36317;&#31163;&#19978;&#30340;$O(t^{-1/2})$&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a Lyapunov convergence analysis for time-inhomogeneous variable coefficient stochastic differential equations (SDEs). Three typical examples include overdamped, irreversible drift, and underdamped Langevin dynamics. We first formula the probability transition equation of Langevin dynamics as a modified gradient flow of the Kullback-Leibler divergence in the probability space with respect to time-dependent optimal transport metrics. This formulation contains both gradient and non-gradient directions depending on a class of time-dependent target distribution. We then select a time-dependent relative Fisher information functional as a Lyapunov functional. We develop a time-dependent Hessian matrix condition, which guarantees the convergence of the probability density function of the SDE. We verify the proposed conditions for several time-inhomogeneous Langevin dynamics. For the overdamped Langevin dynamics, we prove the $O(t^{-1/2})$ convergence in $L^1$ distance for the simula
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01032</link><description>&lt;p&gt;
&#36319;&#30528;&#25105;&#37325;&#22797;&#65306;Transformer&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#27604;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Repeat After Me: Transformers are Better than State Space Models at Copying
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#20027;&#35201;&#26550;&#26500;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#22266;&#23450;&#22823;&#23567;&#28508;&#22312;&#29366;&#24577;&#30340;&#27169;&#22411;&#65292;&#20063;&#23601;&#26159;"&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;" (GSSMs)&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;GSSMs&#22312;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#19978;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#38656;&#35201;&#20174;&#36755;&#20837;&#19978;&#19979;&#25991;&#22797;&#21046;&#30340;&#20219;&#21153;&#19978;&#65292;&#23427;&#20204;&#30456;&#23545;&#20110;transformer&#27169;&#22411;&#26469;&#35828;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#20174;&#23545;&#31616;&#21333;&#30340;&#23383;&#31526;&#20018;&#22797;&#21046;&#20219;&#21153;&#30340;&#29702;&#35770;&#20998;&#26512;&#24320;&#22987;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#20004;&#23618;&#30340;transformer&#21487;&#20197;&#22797;&#21046;&#25351;&#25968;&#38271;&#24230;&#30340;&#23383;&#31526;&#20018;&#65292;&#32780;GSSMs&#30001;&#20110;&#20854;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#29366;&#24577;&#22312;&#26681;&#26412;&#19978;&#26159;&#26377;&#38480;&#21046;&#30340;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;transformer&#22312;&#38656;&#35201;&#22797;&#21046;&#19978;&#19979;&#25991;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#20248;&#20110;GSSMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#36828;&#36828;&#20248;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#21442;&#25968;&#21270;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21051;&#30011;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#35823;&#24046;&#30340;&#33258;&#30456;&#20851;&#24615;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#24230;&#20302;&#12289;&#26657;&#20934;&#39044;&#27979;&#20934;&#30830;&#24615;&#39640;&#31561;&#20248;&#28857;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01000</link><description>&lt;p&gt;
&#22810;&#20803;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19982;&#30456;&#20851;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Multivariate Probabilistic Time Series Forecasting with Correlated Errors
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#21442;&#25968;&#21270;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21051;&#30011;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#35823;&#24046;&#30340;&#33258;&#30456;&#20851;&#24615;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#24230;&#20302;&#12289;&#26657;&#20934;&#39044;&#27979;&#20934;&#30830;&#24615;&#39640;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#19982;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#37327;&#21270;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#26368;&#36817;&#30340;&#22810;&#20803;&#27169;&#22411;&#22312;&#32771;&#34385;&#35823;&#24046;&#20043;&#38388;&#30340;&#21516;&#26102;&#30456;&#20851;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#32479;&#35745;&#31616;&#21270;&#30340;&#30446;&#30340;&#65292;&#23545;&#36825;&#20123;&#35823;&#24046;&#30340;&#24120;&#35265;&#20551;&#35774;&#26159;&#23427;&#20204;&#22312;&#26102;&#38388;&#19978;&#26159;&#29420;&#31435;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#35266;&#27979;&#24448;&#24448;&#20559;&#31163;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#22240;&#20026;&#35823;&#24046;&#36890;&#24120;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65288;&#22914;&#25490;&#38500;&#26102;&#38388;&#30456;&#20851;&#30340;&#21327;&#21464;&#37327;&#65289;&#32780;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#33258;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#21442;&#25968;&#21270;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21051;&#30011;&#35823;&#24046;&#30340;&#33258;&#30456;&#20851;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#21487;&#21462;&#30340;&#29305;&#24615;&#65306;&#22797;&#26434;&#24230;&#19981;&#38543;&#26102;&#38388;&#24207;&#21015;&#25968;&#30446;&#22686;&#21152;&#65292;&#24471;&#21040;&#30340;&#21327;&#26041;&#24046;&#21487;&#20197;&#29992;&#20110;&#26657;&#20934;&#39044;&#27979;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the correlations among errors is closely associated with how accurately the model can quantify predictive uncertainty in probabilistic time series forecasting. Recent multivariate models have made significant progress in accounting for contemporaneous correlations among errors, while a common assumption on these errors is that they are temporally independent for the sake of statistical simplicity. However, real-world observations often deviate from this assumption, since errors usually exhibit substantial autocorrelation due to various factors such as the exclusion of temporally correlated covariates. In this work, we propose an efficient method, based on a low-rank-plus-diagonal parameterization of the covariance matrix, which can effectively characterize the autocorrelation of errors. The proposed method possesses several desirable properties: the complexity does not scale with the number of time series, the resulting covariance can be used for calibrating predictions, and i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#21019;&#24314;&#34394;&#25311;&#35797;&#34915;&#38388;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#65292;&#20026;&#23458;&#25143;&#25552;&#20379;&#22312;&#32447;&#35797;&#31359;&#34915;&#26381;&#30340;&#24179;&#21488;&#65292;&#30465;&#21435;&#20102;&#23454;&#20307;&#24215;&#35797;&#34915;&#30340;&#28902;&#24700;&#21644;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#38236;&#23376;&#20943;&#23569;&#20102;&#23454;&#20307;&#24215;&#25317;&#25380;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00994</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#21019;&#24314;&#34394;&#25311;&#35797;&#34915;&#38388;&#30340;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Cost-Efficient Approach for Creating Virtual Fitting Room using Generative Adversarial Networks (GANs)
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#21019;&#24314;&#34394;&#25311;&#35797;&#34915;&#38388;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#65292;&#20026;&#23458;&#25143;&#25552;&#20379;&#22312;&#32447;&#35797;&#31359;&#34915;&#26381;&#30340;&#24179;&#21488;&#65292;&#30465;&#21435;&#20102;&#23454;&#20307;&#24215;&#35797;&#34915;&#30340;&#28902;&#24700;&#21644;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#38236;&#23376;&#20943;&#23569;&#20102;&#23454;&#20307;&#24215;&#25317;&#25380;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23458;&#25143;&#24076;&#26395;&#22312;&#36141;&#20080;&#21069;&#30475;&#21040;&#34915;&#26381;&#26159;&#21542;&#36866;&#21512;&#20182;&#20204;&#12290;&#22240;&#27492;&#65292;&#23458;&#25143;&#26412;&#33021;&#22320;&#26356;&#21916;&#27426;&#23454;&#20307;&#24215;&#36141;&#29289;&#65292;&#36825;&#26679;&#20182;&#20204;&#21487;&#20197;&#22312;&#36141;&#20080;&#21069;&#35797;&#31359;&#20135;&#21697;&#12290;&#20294;&#22312;COVID-19&#22823;&#27969;&#34892;&#20043;&#21518;&#65292;&#35768;&#22810;&#21830;&#23478;&#35201;&#20040;&#36716;&#21521;&#22312;&#32447;&#36141;&#29289;&#65292;&#35201;&#20040;&#20851;&#38381;&#20102;&#35797;&#34915;&#38388;&#65292;&#36825;&#20351;&#36141;&#29289;&#36807;&#31243;&#20805;&#28385;&#20102;&#29369;&#35947;&#21644;&#24576;&#30097;&#12290;&#20026;&#20102;&#35299;&#20915;&#36141;&#20080;&#21518;&#21487;&#33021;&#19981;&#36866;&#21512;&#20080;&#23478;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21019;&#24314;&#19968;&#20010;&#22312;&#32447;&#24179;&#21488;&#25110;&#34394;&#25311;&#35797;&#34915;&#38388;(VFR)&#65292;&#20197;&#25163;&#26426;&#24212;&#29992;&#31243;&#24207;&#21644;&#37096;&#32626;&#27169;&#22411;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#24182;&#21487;&#20197;&#23884;&#20837;&#21040;&#20219;&#20309;&#22312;&#32447;&#21830;&#24215;&#20013;&#65292;&#20080;&#23478;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20146;&#33258;&#35797;&#31359;&#30340;&#24773;&#20917;&#19979;&#35797;&#31359;&#20219;&#24847;&#25968;&#37327;&#30340;&#34915;&#26381;&#12290;&#27492;&#22806;&#65292;&#23427;&#23558;&#33410;&#30465;&#22823;&#37327;&#23547;&#25214;&#20135;&#21697;&#30340;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#38236;&#23376;&#26469;&#24212;&#29992;&#30456;&#21516;&#30340;&#25216;&#26415;&#65292;&#23427;&#36824;&#23558;&#20943;&#23569;&#23454;&#20307;&#24215;&#30340;&#25317;&#25380;&#21644;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customers all over the world want to see how the clothes fit them or not before purchasing. Therefore, customers by nature prefer brick-and-mortar clothes shopping so they can try on products before purchasing them. But after the Pandemic of COVID19 many sellers either shifted to online shopping or closed their fitting rooms which made the shopping process hesitant and doubtful. The fact that the clothes may not be suitable for their buyers after purchase led us to think about using new AI technologies to create an online platform or a virtual fitting room (VFR) in the form of a mobile application and a deployed model using a webpage that can be embedded later to any online store where they can try on any number of cloth items without physically trying them. Besides, it will save much searching time for their needs. Furthermore, it will reduce the crowding and headache in the physical shops by applying the same technology using a special type of mirror that will enable customers to try
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#21464;&#37327;&#20107;&#20214;&#27969;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#23545;&#27604;&#27169;&#22359;&#26469;&#27604;&#36739;&#30495;&#23454;&#20107;&#20214;&#21644;&#27169;&#25311;&#30340;&#31354;&#30333;&#23454;&#20363;&#65292;&#20197;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00987</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#22312;&#22810;&#21464;&#37327;&#20107;&#20214;&#27969;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Contrastive Pre-Training for Multivariate Point Processes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00987
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#21464;&#37327;&#20107;&#20214;&#27969;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#23545;&#27604;&#27169;&#22359;&#26469;&#27604;&#36739;&#30495;&#23454;&#20107;&#20214;&#21644;&#27169;&#25311;&#30340;&#31354;&#30333;&#23454;&#20363;&#65292;&#20197;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#34920;&#31034;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#65292;&#22312;&#21253;&#25324;BERT&#21644;GPT-3&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22312;&#22810;&#21464;&#37327;&#20107;&#20214;&#27969;&#30340;&#32972;&#26223;&#19979;&#23578;&#26410;&#34987;&#36861;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#23545;&#22810;&#21464;&#37327;&#20107;&#20214;&#27969;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#19981;&#20165;&#36974;&#30422;&#20102;&#38543;&#26426;&#20107;&#20214;&#26102;&#27573;&#65292;&#36824;&#25554;&#20837;&#20102;&#38543;&#26426;&#25277;&#26679;&#30340;&#8220;&#31354;&#30333;&#8221;&#26102;&#27573;&#65292;&#21363;&#20107;&#20214;&#19981;&#21457;&#29983;&#30340;&#26102;&#27573;&#65307;&#36825;&#19982;BERT&#20013;&#30340;&#35789;&#36974;&#30422;&#31561;&#20856;&#22411;&#31163;&#25955;&#26102;&#38388;&#39044;&#35757;&#32451;&#20219;&#21153;&#19981;&#21516;&#65292;&#25193;&#23637;&#20102;&#36974;&#30422;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#12290;&#20026;&#20102;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#27169;&#22359;&#65292;&#23558;&#30495;&#23454;&#20107;&#20214;&#19982;&#27169;&#25311;&#30340;&#31354;&#30333;&#23454;&#20363;&#36827;&#34892;&#27604;&#36739;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#38543;&#21518;&#21487;&#20197;&#22312;&#21487;&#33021;&#26356;&#23567;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#31867;&#20284;&#20110;&#27010;&#24565;&#19978;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervision is one of the hallmarks of representation learning in the increasingly popular suite of foundation models including large language models such as BERT and GPT-3, but it has not been pursued in the context of multivariate event streams, to the best of our knowledge. We introduce a new paradigm for self-supervised learning for multivariate point processes using a transformer encoder. Specifically, we design a novel pre-training strategy for the encoder where we not only mask random event epochs but also insert randomly sampled "void" epochs where an event does not occur; this differs from the typical discrete-time pretext tasks such as word-masking in BERT but expands the effectiveness of masking to better capture continuous-time dynamics. To improve downstream tasks, we introduce a contrasting module that compares real events to simulated void instances. The pre-trained model can subsequently be fine-tuned on a potentially much smaller event dataset, similar conceptuall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00976</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20572;&#27490;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformers with Dynamic Halt
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#22312;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#26041;&#38754;&#30340;&#24402;&#32435;&#20559;&#22909;&#8212;&#8212;&#65288;1&#65289;&#31867;&#20284;&#20110;Universal Transformers&#30340;&#28145;&#24230;&#36880;&#23618;&#24490;&#29615;&#26041;&#27861;&#65307;&#21644;&#65288;2&#65289;&#31867;&#20284;&#20110;Temporal Latent Bottleneck&#30340;&#20998;&#22359;&#26102;&#24577;&#24490;&#29615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#25193;&#23637;&#21644;&#32452;&#21512;&#19978;&#36848;&#26041;&#27861;&#30340;&#26032;&#26041;&#24335;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22343;&#20540;&#30340;Universal Transformer&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#65292;&#24182;&#23558;Universal Transformer&#30340;&#20803;&#32032;&#34701;&#20837;&#21040;Temporal Latent Bottleneck&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;Long Range Arena&#65288;LRA&#65289;&#65292;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65292;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#27604;&#36739;&#20102;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35782;&#21035;&#26410;&#31934;&#32454;&#35299;&#26512;&#30340;PDEs&#20013;&#30340;&#38381;&#21512;&#39033;&#65292;&#36890;&#36807;&#37096;&#32626;&#20013;&#22830;&#31574;&#30053;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21644;&#21152;&#36895;&#27169;&#25311;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00972</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35782;&#21035;&#31895;&#31890;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38381;&#21512;&#39033;
&lt;/p&gt;
&lt;p&gt;
Closure Discovery for Coarse-Grained Partial Differential Equations using Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00972
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35782;&#21035;&#26410;&#31934;&#32454;&#35299;&#26512;&#30340;PDEs&#20013;&#30340;&#38381;&#21512;&#39033;&#65292;&#36890;&#36807;&#37096;&#32626;&#20013;&#22830;&#31574;&#30053;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21644;&#21152;&#36895;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#39044;&#27979;&#22825;&#27668;&#12289;&#37326;&#28779;&#21644;&#27969;&#34892;&#30149;&#31561;&#20851;&#38190;&#29616;&#35937;&#36890;&#24120;&#22522;&#20110;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#25551;&#36848;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25429;&#25417;&#36825;&#31181;PDEs&#20013;&#20840;&#38754;&#30340;&#26102;&#31354;&#23610;&#24230;&#33539;&#22260;&#30340;&#27169;&#25311;&#36890;&#24120;&#26159;&#20195;&#20215;&#39640;&#26114;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#20250;&#20351;&#29992;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#32463;&#39564;&#38381;&#21512;&#39033;&#30340;&#31895;&#31890;&#24230;&#27169;&#25311;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35782;&#21035;&#26410;&#31934;&#32454;&#35299;&#26512;&#30340;PDEs&#20013;&#38381;&#21512;&#39033;&#30340;&#26032;&#39062;&#21644;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;MARL&#30340;&#24418;&#24335;&#21270;&#32467;&#21512;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#21033;&#29992;&#37096;&#32626;&#20102;&#30001;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#39640;&#25928;&#34920;&#31034;&#30340;&#20013;&#22830;&#31574;&#30053;&#26469;&#21033;&#29992;&#23616;&#37096;&#24615;&#12290;&#36890;&#36807;&#23545;&#23545;&#27969;&#26041;&#31243;&#21644;Burgers&#26041;&#31243;&#30340;&#25968;&#20540;&#35299;&#36827;&#34892;&#28436;&#31034;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MARL&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MARL&#23545;&#20110;&#20869;&#22806;&#20998;&#24067;&#30340;&#27979;&#35797;&#26696;&#20363;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#31934;&#32454;&#35299;&#26512;&#30456;&#27604;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable predictions of critical phenomena, such as weather, wildfires and epidemics are often founded on models described by Partial Differential Equations (PDEs). However, simulations that capture the full range of spatio-temporal scales in such PDEs are often prohibitively expensive. Consequently, coarse-grained simulations that employ heuristics and empirical closure terms are frequently utilized as an alternative. We propose a novel and systematic approach for identifying closures in under-resolved PDEs using Multi-Agent Reinforcement Learning (MARL). The MARL formulation incorporates inductive bias and exploits locality by deploying a central policy represented efficiently by Convolutional Neural Networks (CNN). We demonstrate the capabilities and limitations of MARL through numerical solutions of the advection equation and the Burgers' equation. Our results show accurate predictions for in- and out-of-distribution test cases as well as a significant speedup compared to resolving
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21644;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#26469;&#25913;&#36827;&#30315;&#30187;&#26816;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21487;&#20197;&#36807;&#28388;&#21644;&#21024;&#38500;&#20551;&#38451;&#24615;&#39044;&#27979;&#65292;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00965</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#23454;&#39564;&#23460;&#22823;&#40736;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Machine Learning Framework for Automated Seizure Detection in Laboratory Rats
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21644;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#26469;&#25913;&#36827;&#30315;&#30187;&#26816;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21487;&#20197;&#36807;&#28388;&#21644;&#21024;&#38500;&#20551;&#38451;&#24615;&#39044;&#27979;&#65292;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21033;&#29992;&#22810;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#28304;&#21644;&#31867;&#22411;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#23427;&#23558;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#20449;&#21495;&#30340;&#20960;&#31181;&#27169;&#22411;&#30340;&#32467;&#26524;&#32467;&#21512;&#36215;&#26469;&#12290;&#20197;&#19968;&#20010;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#39564;&#20013;&#25910;&#38598;&#20102;&#26469;&#33258;&#24739;&#26377;&#30315;&#30187;&#30340;&#22823;&#40736;&#30340;&#22810;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#30005;&#23376;&#30382;&#36136;&#35760;&#24405;&#12289;&#21387;&#30005;&#36816;&#21160;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#35270;&#39057;&#35760;&#24405;&#12290;&#23558;&#27599;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#20998;&#21035;&#35757;&#32451;&#20026;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#23558;&#27599;&#20010;&#26102;&#38388;&#27573;&#20998;&#31867;&#20026;&#21253;&#21547;&#30315;&#30187;&#25110;&#19981;&#21253;&#21547;&#30315;&#30187;&#12290;&#22312;&#27599;&#20010;&#27169;&#22411;&#29983;&#25104;&#20998;&#31867;&#39044;&#27979;&#21518;&#65292;&#23558;&#36825;&#20123;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#12290;&#23613;&#31649;&#27599;&#20010;&#25968;&#25454;&#20449;&#21495;&#22312;&#39044;&#27979;&#26041;&#38754;&#37117;&#36275;&#22815;&#22909;&#65292;&#20294;&#31867;&#21035;&#26631;&#31614;&#30340;&#26174;&#33879;&#19981;&#24179;&#34913;&#23548;&#33268;&#20102;&#22686;&#21152;&#30340;&#20551;&#38451;&#24615;&#25968;&#37327;&#65292;&#32780;&#36890;&#36807;&#21033;&#29992;&#25152;&#26377;&#25968;&#25454;&#28304;&#21487;&#20197;&#23545;&#20854;&#36827;&#34892;&#36807;&#28388;&#21644;&#21024;&#38500;&#12290;&#26412;&#25991;&#23558;&#28436;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A multi-modal machine learning system uses multiple unique data sources and types to improve its performance. This article proposes a system that combines results from several types of models, all of which are trained on different data signals. As an example to illustrate the efficacy of the system, an experiment is described in which multiple types of data are collected from rats suffering from seizures. This data includes electrocorticography readings, piezoelectric motion sensor data, and video recordings. Separate models are trained on each type of data, with the goal of classifying each time frame as either containing a seizure or not. After each model has generated its classification predictions, these results are combined. While each data signal works adequately on its own for prediction purposes, the significant imbalance in class labels leads to increased numbers of false positives, which can be filtered and removed by utilizing all data sources. This paper will demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;</title><link>https://rss.arxiv.org/abs/2402.00957</link><description>&lt;p&gt;
&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Credal Learning Theory
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#20026;&#20174;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#25552;&#20379;&#29702;&#35770;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21464;&#21270;&#65292;&#23548;&#33268;&#39046;&#22495;&#36866;&#24212;/&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#8220;&#20449;&#20219;&#8221;&#23398;&#20064;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#27010;&#29575;&#30340;&#20984;&#38598;&#65288;&#20449;&#20219;&#38598;&#65289;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#20449;&#20219;&#38598;&#21487;&#20197;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#23545;&#20110;&#26377;&#38480;&#20551;&#35774;&#31354;&#38388;&#65288;&#26080;&#35770;&#26159;&#21542;&#21487;&#23454;&#29616;&#65289;&#21644;&#26080;&#38480;&#27169;&#22411;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#30028;&#38480;&#65292;&#36825;&#30452;&#25509;&#25512;&#24191;&#20102;&#32463;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
&lt;/p&gt;</description></item><item><title>FairEHR-CLP&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#30340;&#21512;&#25104;&#23545;&#24212;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#65292;&#24182;&#21033;&#29992;&#20844;&#24179;&#24863;&#30693;&#39044;&#27979;&#26041;&#27861;&#28040;&#38500;EHR&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00955</link><description>&lt;p&gt;
FairEHR-CLP&#65306;&#20197;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#30340;&#20844;&#24179;&#24863;&#30693;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FairEHR-CLP: Towards Fairness-Aware Clinical Predictions with Contrastive Learning in Multimodal Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00955
&lt;/p&gt;
&lt;p&gt;
FairEHR-CLP&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#30340;&#21512;&#25104;&#23545;&#24212;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#65292;&#24182;&#21033;&#29992;&#20844;&#24179;&#24863;&#30693;&#39044;&#27979;&#26041;&#27861;&#28040;&#38500;EHR&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#39044;&#27979;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#25104;&#20026;&#21307;&#30103;&#20915;&#31574;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#21333;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#26410;&#35299;&#20915;EHR&#20013;&#19982;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#22810;&#26041;&#38754;&#31038;&#20250;&#20559;&#35265;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FairEHR-CLP&#65306;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#20020;&#24202;&#39044;&#27979;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;EHR&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;FairEHR-CLP&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#25805;&#20316;&#65292;&#21033;&#29992;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#12289;&#32437;&#21521;&#25968;&#25454;&#21644;&#20020;&#24202;&#35760;&#24405;&#12290;&#39318;&#20808;&#65292;&#20026;&#27599;&#20010;&#24739;&#32773;&#29983;&#25104;&#21512;&#25104;&#23545;&#24212;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#30041;&#24517;&#35201;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#20844;&#24179;&#24863;&#30693;&#39044;&#27979;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23558;&#24739;&#32773;&#30340;&#34920;&#31034;&#22312;&#25935;&#24863;&#23646;&#24615;&#19978;&#36827;&#34892;&#23545;&#40784;&#65292;&#19982;&#20855;&#26377;softmax&#23618;&#30340;MLP&#20998;&#31867;&#22120;&#20849;&#21516;&#20248;&#21270;&#29992;&#20110;&#20020;&#24202;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the high-stakes realm of healthcare, ensuring fairness in predictive models is crucial. Electronic Health Records (EHRs) have become integral to medical decision-making, yet existing methods for enhancing model fairness restrict themselves to unimodal data and fail to address the multifaceted social biases intertwined with demographic factors in EHRs. To mitigate these biases, we present FairEHR-CLP: a general framework for Fairness-aware Clinical Predictions with Contrastive Learning in EHRs. FairEHR-CLP operates through a two-stage process, utilizing patient demographics, longitudinal data, and clinical notes. First, synthetic counterparts are generated for each patient, allowing for diverse demographic identities while preserving essential health information. Second, fairness-aware predictions employ contrastive learning to align patient representations across sensitive attributes, jointly optimized with an MLP classifier with a softmax layer for clinical classification tasks. Ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20195;&#25968;&#20960;&#20309;&#24037;&#20855;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#24615;&#21644;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#23545;&#31070;&#32463;&#27969;&#24418;&#30340;&#32500;&#24230;&#21644;&#23398;&#20064;&#24230;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#21644;&#35757;&#32451;&#22797;&#26434;&#24230;&#30340;&#24230;&#37327;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#23398;&#20989;&#25968;&#25968;&#37327;&#30340;&#19978;&#30028;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00949</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Geometry of Polynomial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20195;&#25968;&#20960;&#20309;&#24037;&#20855;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#24615;&#21644;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#23545;&#31070;&#32463;&#27969;&#24418;&#30340;&#32500;&#24230;&#21644;&#23398;&#20064;&#24230;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#21644;&#35757;&#32451;&#22797;&#26434;&#24230;&#30340;&#24230;&#37327;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#23398;&#20989;&#25968;&#25968;&#37327;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#65288;PNN&#65289;&#30340;&#34920;&#36798;&#24615;&#21644;&#23398;&#20064;&#36807;&#31243;&#12290;&#32593;&#32476;&#30340;&#26435;&#37325;&#21442;&#25968;&#21270;&#20102;&#31070;&#32463;&#27969;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#25968;&#20960;&#20309;&#24037;&#20855;&#30740;&#31350;&#20102;&#26576;&#20123;&#31070;&#32463;&#27969;&#24418;&#65306;&#25105;&#20204;&#32473;&#20986;&#20102;&#21322;&#20195;&#25968;&#38598;&#30340;&#26126;&#30830;&#25551;&#36848;&#24182;&#29305;&#24449;&#21270;&#20102;&#23427;&#20204;&#30340;Zariski&#38381;&#21253;&#65292;&#31216;&#20026;&#31070;&#32463;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#32500;&#24230;&#24182;&#23558;&#19968;&#20010;&#20195;&#25968;&#24230;&#37327;&#65292;&#23398;&#20064;&#24230;&#65292;&#19982;&#31070;&#32463;&#22810;&#26679;&#24615;&#30456;&#20851;&#32852;&#12290;&#32500;&#24230;&#20316;&#20026;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#23398;&#20064;&#24230;&#26159;&#35757;&#32451;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#21487;&#23398;&#20989;&#25968;&#25968;&#37327;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#36824;&#20276;&#38543;&#30528;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the expressivity and learning process for polynomial neural networks (PNNs) with monomial activation functions. The weights of the network parametrize the neuromanifold. In this paper, we study certain neuromanifolds using tools from algebraic geometry: we give explicit descriptions as semialgebraic sets and characterize their Zariski closures, called neurovarieties. We study their dimension and associate an algebraic degree, the learning degree, to the neurovariety. The dimension serves as a geometric measure for the expressivity of the network, the learning degree is a measure for the complexity of training the network and provides upper bounds on the number of learnable functions. These theoretical results are accompanied with experiments.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31383;&#21475;&#36807;&#28388;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#35821;&#20041;&#25628;&#32034;&#38382;&#39064;&#20013;&#23454;&#29616;&#39640;&#36895;&#25628;&#32034;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00943</link><description>&lt;p&gt;
&#20351;&#29992;&#31383;&#21475;&#36807;&#28388;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Approximate Nearest Neighbor Search with Window Filters
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31383;&#21475;&#36807;&#28388;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#35821;&#20041;&#25628;&#32034;&#38382;&#39064;&#20013;&#23454;&#29616;&#39640;&#36895;&#25628;&#32034;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23450;&#20041;&#24182;&#30740;&#31350;&#20102;$\textit{c-&#36817;&#20284;&#31383;&#21475;&#25628;&#32034;}$&#38382;&#39064;&#65306;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#20854;&#20013;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#28857;&#37117;&#26377;&#19968;&#20010;&#25968;&#20540;&#26631;&#31614;&#65292;&#30446;&#26631;&#26159;&#22312;&#20219;&#24847;&#26631;&#31614;&#33539;&#22260;&#20869;&#25214;&#21040;&#26597;&#35810;&#28857;&#30340;&#26368;&#36817;&#37051;&#12290;&#35768;&#22810;&#35821;&#20041;&#25628;&#32034;&#38382;&#39064;&#65292;&#20363;&#22914;&#24102;&#26377;&#26102;&#38388;&#25139;&#36807;&#28388;&#22120;&#30340;&#22270;&#20687;&#21644;&#25991;&#26723;&#25628;&#32034;&#65292;&#25110;&#24102;&#26377;&#25104;&#26412;&#36807;&#28388;&#22120;&#30340;&#20135;&#21697;&#25628;&#32034;&#65292;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#33258;&#28982;&#20363;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22359;&#21270;&#26641;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35299;&#20915;&#20256;&#32479;c-&#36817;&#20284;&#26368;&#36817;&#37051;&#38382;&#39064;&#30340;&#32034;&#24341;&#36716;&#21270;&#20026;&#35299;&#20915;&#31383;&#21475;&#25628;&#32034;&#30340;&#25968;&#25454;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#30340;&#26368;&#36817;&#37051;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#37197;&#22791;&#20102;&#38543;&#26426;&#26631;&#31614;&#20540;&#12289;&#23545;&#25239;&#24615;&#26500;&#24314;&#30340;&#23884;&#20837;&#20197;&#21450;&#24102;&#26377;&#30495;&#23454;&#26102;&#38388;&#25139;&#30340;&#22270;&#20687;&#25628;&#32034;&#23884;&#20837;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#39640;&#36798;75&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define and investigate the problem of $\textit{c-approximate window search}$: approximate nearest neighbor search where each point in the dataset has a numeric label, and the goal is to find nearest neighbors to queries within arbitrary label ranges. Many semantic search problems, such as image and document search with timestamp filters, or product search with cost filters, are natural examples of this problem. We propose and theoretically analyze a modular tree-based framework for transforming an index that solves the traditional c-approximate nearest neighbor problem into a data structure that solves window search. On standard nearest neighbor benchmark datasets equipped with random label values, adversarially constructed embeddings, and image search embeddings with real timestamps, we obtain up to a $75\times$ speedup over existing solutions at the same level of recall.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#22240;&#34920;&#36798;&#35889;&#30340;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#35299;&#37322;&#24615;&#26041;&#27861;&#24471;&#21040;&#30340;&#22522;&#22240;&#25490;&#21517;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#34920;&#22411;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#29983;&#29289;&#23398;&#21644;&#26041;&#27861;&#23398;&#19978;&#30340;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#30284;&#30151;&#20998;&#31867;&#20013;&#65292;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#30284;&#30151;&#31867;&#22411;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00926</link><description>&lt;p&gt;
&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#22522;&#22240;&#34920;&#36798;&#35889;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Gene Expression Profiling by Statistical and Machine Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00926
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#22240;&#34920;&#36798;&#35889;&#30340;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#35299;&#37322;&#24615;&#26041;&#27861;&#24471;&#21040;&#30340;&#22522;&#22240;&#25490;&#21517;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#34920;&#22411;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#29983;&#29289;&#23398;&#21644;&#26041;&#27861;&#23398;&#19978;&#30340;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#30284;&#30151;&#20998;&#31867;&#20013;&#65292;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#30284;&#30151;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#20174;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#20998;&#31867;&#34920;&#22411;&#12290;&#38500;&#20102;&#34920;&#29616;&#22909;&#20043;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#21487;&#20197;&#36890;&#36807;&#25552;&#21462;&#20915;&#31574;&#30340;&#35299;&#37322;&#26469;&#23545;&#34920;&#22411;&#36827;&#34892;&#19968;&#23450;&#30340;&#29702;&#35299;&#12290;&#36825;&#20123;&#35299;&#37322;&#36890;&#24120;&#20197;&#22522;&#22240;&#25353;&#37325;&#35201;&#24615;&#25490;&#21517;&#30340;&#21015;&#34920;&#24418;&#24335;&#21576;&#29616;&#65292;&#25490;&#21517;&#26368;&#39640;&#30340;&#22522;&#22240;&#34987;&#35299;&#37322;&#20026;&#19982;&#34920;&#22411;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#35299;&#37322;&#30340;&#29983;&#29289;&#23398;&#21644;&#26041;&#27861;&#23398;&#38480;&#21046;&#12290;&#22312;TCGA&#12289;GTEx&#21644;TARGET&#25968;&#25454;&#24211;&#20013;&#25910;&#38598;&#30340;&#30284;&#30151;&#21644;&#20581;&#24247;&#32452;&#32455;&#26679;&#26412;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#26679;&#26412;&#36827;&#34892;&#20102;&#30284;&#30151;&#31867;&#22411;&#20998;&#31867;&#12290;&#20174;&#36866;&#24212;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#20013;&#33719;&#21462;&#22522;&#22240;&#25490;&#21517;&#65292;&#24182;&#19982;&#32463;&#20856;&#32479;&#35745;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;&#22914;m&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning models have been proposed to classify phenotypes from gene expression data. In addition to their good performance, these models can potentially provide some understanding of phenotypes by extracting explanations for their decisions. These explanations often take the form of a list of genes ranked in order of importance for the predictions, the highest-ranked genes being interpreted as linked to the phenotype. We discuss the biological and the methodological limitations of such explanations. Experiments are performed on several datasets gathering cancer and healthy tissue samples from the TCGA, GTEx and TARGET databases. A collection of machine learning models including logistic regression, multilayer perceptron, and graph neural network are trained to classify samples according to their cancer type. Gene rankings are obtained from explainability methods adapted to these models, and compared to the ones from classical statistical feature selection methods such as m
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#21644;&#20998;&#31867;&#29616;&#26377;&#30340;&#30740;&#31350;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#21644;&#21442;&#32771;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00920</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#29289;&#32852;&#32593;&#20013;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Approaches for Network Traffic Classification in the Internet of Things (IoT): A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#21644;&#20998;&#31867;&#29616;&#26377;&#30340;&#30740;&#31350;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30446;&#30585;&#20102;&#31354;&#21069;&#30340;&#22686;&#38271;&#65292;&#23548;&#33268;&#26469;&#33258;&#20114;&#32852;&#35774;&#22791;&#30340;&#21508;&#31181;&#32593;&#32476;&#27969;&#37327;&#22823;&#37327;&#28044;&#20837;&#12290;&#26377;&#25928;&#22320;&#23545;&#36825;&#20123;&#32593;&#32476;&#27969;&#37327;&#36827;&#34892;&#20998;&#31867;&#23545;&#20110;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#22686;&#24378;&#23433;&#20840;&#25514;&#26045;&#21644;&#30830;&#20445;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#39640;&#25928;&#32593;&#32476;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#33021;&#22815;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#21644;&#34920;&#31034;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#26088;&#22312;&#20840;&#38754;&#27010;&#36848;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#29305;&#21035;&#23450;&#21046;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20013;&#25152;&#37319;&#29992;&#30340;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#21644;&#20998;&#31867;&#39046;&#22495;&#20869;&#26368;&#26032;&#30340;&#30740;&#31350;&#36129;&#29486;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22788;&#29702;&#29289;&#32852;&#32593;&#32593;&#32476;&#27969;&#37327;&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#25361;&#25112;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#26412;&#35843;&#30740;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Things (IoT) has witnessed unprecedented growth, resulting in a massive influx of diverse network traffic from interconnected devices. Effectively classifying this network traffic is crucial for optimizing resource allocation, enhancing security measures, and ensuring efficient network management in IoT systems. Deep learning has emerged as a powerful technique for network traffic classification due to its ability to automatically learn complex patterns and representations from raw data. This survey paper aims to provide a comprehensive overview of the existing deep learning approaches employed in network traffic classification specifically tailored for IoT environments. By systematically analyzing and categorizing the latest research contributions in this domain, we explore the strengths and limitations of various deep learning models in handling the unique challenges posed by IoT network traffic. Through this survey, we aim to offer researchers and practitioners valua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36755;&#20986;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00912</link><description>&lt;p&gt;
&#33021;&#22815;&#32422;&#26463;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#36755;&#20837;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36755;&#20986;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#34987;&#35748;&#20026;&#20855;&#26377;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#39318;&#20808;&#39044;&#27979;&#19968;&#32452;&#20154;&#20026;&#23450;&#20041;&#30340;&#27010;&#24565;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#26469;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#21450;&#30830;&#20445;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#20449;&#20219;&#65292;&#25105;&#20204;&#38656;&#35201;&#20445;&#35777;&#27010;&#24565;&#30340;&#39044;&#27979;&#26159;&#22522;&#20110;&#35821;&#20041;&#26144;&#23556;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#21487;&#33021;&#26399;&#26395;&#22270;&#20687;&#20013;&#34920;&#31034;&#39592;&#25240;&#30340;&#20687;&#32032;&#34987;&#29992;&#20110;&#39044;&#27979;&#39592;&#25240;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#36825;&#24182;&#19981;&#26159;&#20107;&#23454;&#65292;&#22240;&#20026;&#27010;&#24565;&#39044;&#27979;&#36890;&#24120;&#19982;&#19981;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#27010;&#24565;&#27880;&#37322;&#30340;&#19981;&#20934;&#30830;&#25110;&#32773;&#36755;&#20837;&#29305;&#24449;&#19982;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#28165;&#26224;&#23548;&#33268;&#30340;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25968;&#25454;&#38598;&#26631;&#27880;&#23545;CBMs&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#30740;&#31350;&#36739;&#23569;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;CBMs&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) are considered inherently interpretable because they first predict a set of human-defined concepts before using these concepts to predict the output of a downstream task. For inherent interpretability to be fully realised, and ensure trust in a model's output, we need to guarantee concepts are predicted based on semantically mapped input features. For example, one might expect the pixels representing a broken bone in an image to be used for the prediction of a fracture. However, current literature indicates this is not the case, as concept predictions are often mapped to irrelevant input features. We hypothesise that this occurs when concept annotations are inaccurate or how input features should relate to concepts is unclear. In general, the effect of dataset labelling on concept representations in CBMs remains an understudied area. Therefore, in this paper, we examine how CBMs learn concepts from datasets with fine-grained concept annotations. We demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#35757;&#32451;&#22810;&#20010;&#23545;&#25239;&#20559;&#35265;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#24471;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00910</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#24212;&#23545;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#35757;&#32451;&#22810;&#20010;&#23545;&#25239;&#20559;&#35265;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#24471;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23545;&#20110;&#30830;&#20445;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#22823;&#35268;&#27169;&#12289;&#26080;&#20559;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#31181;&#26041;&#27861;&#28040;&#38500;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20854;&#20013;&#20165;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#28508;&#22312;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#20998;&#31163;&#12289;&#23616;&#37096;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#20197;&#23545;&#25239;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24471;&#21040;&#28508;&#22312;&#30340;&#23545;&#25239;&#20559;&#35265;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#23545;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#36798;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#36895;&#25105;&#20204;&#30340;&#38598;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#33719;&#24471;&#19968;&#20010;&#21333;&#19968;&#26080;&#20559;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#21019;&#24314;&#26356;&#26080;&#20559;&#12289;&#21487;&#38752;&#30340;AI&#27169;&#22411;&#30340;&#25345;&#32493;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing biases in AI models is crucial for ensuring fair and accurate predictions. However, obtaining large, unbiased datasets for training can be challenging. This paper proposes a comprehensive approach using multiple methods to remove bias in AI models, with only a small dataset and a potentially biased pretrained model. We train multiple models with the counter-bias of the pre-trained model through data splitting, local training, and regularized fine-tuning, gaining potentially counter-biased models. Then, we employ ensemble learning for all models to reach unbiased predictions. To further accelerate the inference time of our ensemble model, we conclude our solution with knowledge distillation that results in a single unbiased neural network. We demonstrate the effectiveness of our approach through experiments on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work contributes to the ongoing effort to create more unbiased and reliable AI models, even with l
&lt;/p&gt;</description></item><item><title>AlphaRank&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22266;&#23450;&#39044;&#31639;&#30340;&#25490;&#21517;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#32463;&#20856;&#30340;&#25490;&#21517;&#21644;&#36873;&#25321;&#31243;&#24207;&#20316;&#20026;&#22522;&#20934;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#23398;&#20064;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21152;&#36895;&#22312;&#32447;&#26679;&#26412;&#20998;&#37197;&#65292;&#24182;&#25552;&#20986;&#21487;&#24182;&#34892;&#35745;&#31639;&#30340;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#12290;AlphaRank&#22312;&#22343;&#20540;&#12289;&#26041;&#24046;&#21644;&#35825;&#23548;&#30456;&#20851;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24179;&#34913;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00907</link><description>&lt;p&gt;
AlphaRank:&#29992;&#20110;&#25490;&#21517;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AlphaRank: An Artificial Intelligence Approach for Ranking and Selection Problems
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00907
&lt;/p&gt;
&lt;p&gt;
AlphaRank&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22266;&#23450;&#39044;&#31639;&#30340;&#25490;&#21517;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#32463;&#20856;&#30340;&#25490;&#21517;&#21644;&#36873;&#25321;&#31243;&#24207;&#20316;&#20026;&#22522;&#20934;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#23398;&#20064;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21152;&#36895;&#22312;&#32447;&#26679;&#26412;&#20998;&#37197;&#65292;&#24182;&#25552;&#20986;&#21487;&#24182;&#34892;&#35745;&#31639;&#30340;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#12290;AlphaRank&#22312;&#22343;&#20540;&#12289;&#26041;&#24046;&#21644;&#35825;&#23548;&#30456;&#20851;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24179;&#34913;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;AlphaRank&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22266;&#23450;&#39044;&#31639;&#30340;&#25490;&#21517;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#39034;&#24207;&#37319;&#26679;&#20915;&#31574;&#23450;&#20041;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#20223;&#30495;&#30340;&#28378;&#21160;&#31574;&#30053;&#65292;&#21033;&#29992;&#32463;&#20856;&#30340;&#25490;&#21517;&#21644;&#36873;&#25321;&#31243;&#24207;&#20316;&#20026;&#22522;&#20934;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#23398;&#20064;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#32473;&#23450;&#20808;&#39564;&#30340;&#24773;&#20917;&#19979;&#31163;&#32447;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21152;&#36895;&#22312;&#32447;&#26679;&#26412;&#20998;&#37197;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24182;&#34892;&#35745;&#31639;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#8220;&#20998;&#32780;&#27835;&#20043;&#8221;&#21644;&#8220;&#36882;&#24402;&#8221;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;AlphaRank&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#31574;&#30053;&#65292;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;AlphaRank&#22312;&#20247;&#22810;&#29616;&#26377;&#31574;&#30053;&#20013;&#24573;&#35270;&#30340;&#22343;&#20540;&#12289;&#26041;&#24046;&#21644;&#35825;&#23548;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#20248;&#36234;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AlphaRank, an artificial intelligence approach to address the fixed-budget ranking and selection (R&amp;S) problems. We formulate the sequential sampling decision as a Markov decision process and propose a Monte Carlo simulation-based rollout policy that utilizes classic R&amp;S procedures as base policies for efficiently learning the value function of stochastic dynamic programming. We accelerate online sample-allocation by using deep reinforcement learning to pre-train a neural network model offline based on a given prior. We also propose a parallelizable computing framework for large-scale problems, effectively combining "divide and conquer" and "recursion" for enhanced scalability and efficiency. Numerical experiments demonstrate that the performance of AlphaRank is significantly improved over the base policies, which could be attributed to AlphaRank's superior capability on the trade-off among mean, variance, and induced correlation overlooked by many existing policies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#23545;&#27169;&#22411;&#21453;&#36716;&#25915;&#20987;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#65292;&#21457;&#29616;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22266;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#65292;&#24182;&#33021;&#26377;&#25928;&#25269;&#25239;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00906</link><description>&lt;p&gt;
BrainLeaks: &#20851;&#20110;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#23545;&#27169;&#22411;&#21453;&#36716;&#25915;&#20987;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic Architectures against Model Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#23545;&#27169;&#22411;&#21453;&#36716;&#25915;&#20987;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#65292;&#21457;&#29616;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22266;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#65292;&#24182;&#33021;&#26377;&#25928;&#25269;&#25239;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#23433;&#20840;&#25935;&#24863;&#39046;&#22495;&#30340;&#20027;&#27969;&#25972;&#21512;&#65292;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#24050;&#32463;&#21152;&#21095;&#12290;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#24050;&#34987;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#27844;&#38706;&#25935;&#24863;&#25968;&#25454;&#30340;&#25915;&#20987;&#12290;&#29305;&#21035;&#26159;&#65292;&#27169;&#22411;&#21453;&#36716;&#65288;MI&#65289;&#25915;&#20987;&#21487;&#20197;&#37325;&#26500;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#24050;&#32463;&#25104;&#20026;&#31070;&#32463;&#35745;&#31639;&#30340;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#65292;&#23454;&#29616;&#20102;&#24322;&#27493;&#21644;&#33410;&#33021;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#27809;&#26377;&#29616;&#26377;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#23545;&#27169;&#22411;&#21453;&#36716;&#30340;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21040;&#30340;&#21551;&#31034;&#26159;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30340;&#19981;&#21487;&#24494;&#29305;&#24615;&#21487;&#33021;&#23548;&#33268;&#22266;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#65292;&#23588;&#20854;&#25269;&#25239;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;SNNs&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the mainstream integration of machine learning into security-sensitive domains such as healthcare and finance, concerns about data privacy have intensified. Conventional artificial neural networks (ANNs) have been found vulnerable to several attacks that can leak sensitive data. Particularly, model inversion (MI) attacks enable the reconstruction of data samples that have been used to train the model. Neuromorphic architectures have emerged as a paradigm shift in neural computing, enabling asynchronous and energy-efficient computation. However, little to no existing work has investigated the privacy of neuromorphic architectures against model inversion. Our study is motivated by the intuition that the non-differentiable aspect of spiking neural networks (SNNs) might result in inherent privacy-preserving properties, especially against gradient-based attacks. To investigate this hypothesis, we propose a thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22270;&#39046;&#22495;&#36866;&#24212;&#30340;&#30740;&#31350;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#21069;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#33539;&#24335;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#30446;&#26631;&#22270;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00904</link><description>&lt;p&gt;
&#22270;&#39046;&#22495;&#36866;&#24212;&#65306;&#25361;&#25112;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Graph Domain Adaptation: Challenges, Progress and Prospects
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00904
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22270;&#39046;&#22495;&#36866;&#24212;&#30340;&#30740;&#31350;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#21069;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#33539;&#24335;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#30446;&#26631;&#22270;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#24448;&#24448;&#38754;&#20020;&#26631;&#31614;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22270;&#39046;&#22495;&#36866;&#24212;&#65288;GDA&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#33539;&#24335;&#12290;&#29305;&#21035;&#26159;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#22312;&#30446;&#26631;&#22270;&#19978;&#30340;&#24615;&#33021;&#65292;GDA&#24341;&#20837;&#20102;&#19968;&#32452;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#22270;&#20316;&#20026;&#28304;&#22270;&#65292;&#24182;&#23558;&#20174;&#28304;&#22270;&#23398;&#21040;&#30340;&#30693;&#35782;&#36866;&#24212;&#21040;&#30446;&#26631;&#22270;&#19978;&#12290;&#30001;&#20110;GDA&#32467;&#21512;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#20248;&#21183;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#22270;&#19978;&#36801;&#31227;&#23398;&#20064;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GDA&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#24182;&#35814;&#32454;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#30740;&#31350;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#20171;&#32461;&#20102;&#20195;&#34920;&#24615;&#24037;&#20316;&#30340;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#20102;&#21069;&#26223;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#23545;GDA&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
As graph representation learning often suffers from label scarcity problems in real-world applications, researchers have proposed graph domain adaptation (GDA) as an effective knowledge-transfer paradigm across graphs. In particular, to enhance model performance on target graphs with specific tasks, GDA introduces a bunch of task-related graphs as source graphs and adapts the knowledge learnt from source graphs to the target graphs. Since GDA combines the advantages of graph representation learning and domain adaptation, it has become a promising direction of transfer learning on graphs and has attracted an increasing amount of research interest in recent years. In this paper, we comprehensively overview the studies of GDA and present a detailed survey of recent advances. Specifically, we outline the research status and challenges, propose a taxonomy, introduce the details of representative works, and discuss the prospects. To the best of our knowledge, this paper is the first survey f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00899</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#22120;&#23454;&#29616;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;AI&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20808;&#39564;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#12290;&#36825;&#20123;AI&#20462;&#27491;&#22120;&#26159;&#36741;&#21161;&#26144;&#23556;&#65292;&#20854;&#20316;&#29992;&#26159;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#20197;&#35843;&#33410;&#20043;&#21069;&#26500;&#24314;&#30340;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#25298;&#32477;&#19968;&#20010;&#20915;&#31574;&#21487;&#20197;&#29992;&#20316;&#24314;&#35758;&#25918;&#24323;&#20570;&#20986;&#20915;&#31574;&#30340;&#20449;&#21495;&#12290;&#35813;&#24037;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#37325;&#28857;&#26159;&#36890;&#36807;&#23545;&#38169;&#35823;&#20915;&#31574;&#30340;&#27010;&#29575;&#30028;&#38480;&#25552;&#20379;&#36825;&#20123;&#26032;&#30340;AI&#20462;&#27491;&#22120;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36825;&#20123;&#30028;&#38480;&#26159;&#20998;&#24067;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#31034;&#20363;&#35828;&#26126;&#20102;&#35813;&#26694;&#26550;&#22914;&#20309;&#24212;&#29992;&#20110;&#25913;&#21892;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#26089;&#26399;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;LLM&#26368;&#32456;&#29992;&#25143;&#12289;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#24433;&#21709;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00898</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#26089;&#26399;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
An Early Categorization of Prompt Injection Attacks on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#26089;&#26399;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;LLM&#26368;&#32456;&#29992;&#25143;&#12289;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#21069;&#27839;&#12290;&#28982;&#32780;&#65292;ChatGPT&#21644;&#20854;&#20182;&#31867;&#20284;&#24037;&#20855;&#30340;&#21457;&#24067;&#21518;&#65292;&#20154;&#20204;&#23545;&#20110;&#25511;&#21046;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#36755;&#20986;&#30340;&#38590;&#24230;&#36234;&#26469;&#36234;&#25285;&#24551;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#27491;&#22312;&#30446;&#30585;&#19968;&#22330;&#25417;&#36855;&#34255;&#30340;&#28216;&#25103;&#65292;&#29992;&#25143;&#35797;&#22270;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#19968;&#31181;&#31216;&#20026;&#25552;&#31034;&#27880;&#20837;&#30340;&#26032;&#22411;&#25915;&#20987;&#65292;&#32780;&#24320;&#21457;&#20154;&#21592;&#21017;&#35797;&#22270;&#21516;&#26102;&#21457;&#29616;&#28431;&#27934;&#24182;&#38459;&#27490;&#36825;&#20123;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#36825;&#20123;&#26032;&#20852;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#31034;&#27880;&#20837;&#30340;&#20998;&#31867;&#65292;&#36825;&#21487;&#20197;&#25351;&#23548;&#26410;&#26469;&#22312;&#25552;&#31034;&#27880;&#20837;&#19978;&#30340;&#30740;&#31350;&#65292;&#24182;&#20316;&#20026;LLM&#30028;&#38754;&#24320;&#21457;&#20013;&#28431;&#27934;&#30340;&#26816;&#26597;&#28165;&#21333;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20808;&#21069;&#30340;&#25991;&#29486;&#21644;&#25105;&#20204;&#33258;&#24049;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25552;&#31034;&#27880;&#20837;&#23545;LLM&#26368;&#32456;&#29992;&#25143;&#12289;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models and AI chatbots have been at the forefront of democratizing artificial intelligence. However, the releases of ChatGPT and other similar tools have been followed by growing concerns regarding the difficulty of controlling large language models and their outputs. Currently, we are witnessing a cat-and-mouse game where users attempt to misuse the models with a novel attack called prompt injections. In contrast, the developers attempt to discover the vulnerabilities and block the attacks simultaneously. In this paper, we provide an overview of these emergent threats and present a categorization of prompt injections, which can guide future research on prompt injections and act as a checklist of vulnerabilities in the development of LLM interfaces. Moreover, based on previous literature and our own empirical research, we discuss the implications of prompt injections to LLM end users, developers, and researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#23545;&#35937;&#20316;&#20026;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26089;&#26399;&#30196;&#21574;&#31579;&#26597;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#24449;&#65292;&#33021;&#20934;&#30830;&#22320;&#34920;&#31034;&#22768;&#38899;&#35889;&#65292;&#24182;&#21253;&#21547;&#19982;&#34987;&#35797;&#32773;&#23545;&#22768;&#38899;&#30340;&#25511;&#21046;&#26377;&#20851;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20581;&#24247;&#20010;&#20307;&#19982;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#25110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20010;&#20307;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00897</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#38899;&#23545;&#35937;&#20316;&#20026;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26089;&#26399;&#30196;&#21574;&#31579;&#26597;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Screening method for early dementia using sound objects as voice biomarkers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#23545;&#35937;&#20316;&#20026;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26089;&#26399;&#30196;&#21574;&#31579;&#26597;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#24449;&#65292;&#33021;&#20934;&#30830;&#22320;&#34920;&#31034;&#22768;&#38899;&#35889;&#65292;&#24182;&#21253;&#21547;&#19982;&#34987;&#35797;&#32773;&#23545;&#22768;&#38899;&#30340;&#25511;&#21046;&#26377;&#20851;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20581;&#24247;&#20010;&#20307;&#19982;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#25110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#22768;&#38899;&#23545;&#35937;&#30340;&#29305;&#24449;&#20316;&#20026;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26089;&#26399;&#30196;&#21574;&#31579;&#26597;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#32456;&#25968;&#25454;&#38598;&#21253;&#25324;266&#20010;&#35266;&#23519;&#20540;&#65292;&#20854;&#20013;&#20998;&#24067;&#20102;186&#20010;&#20581;&#24247;&#20010;&#20307;&#65292;46&#20010;&#34987;&#35786;&#26029;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;34&#20010;&#34987;&#35786;&#26029;&#20026;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#34987;&#35797;&#32773;&#21457;&#20986;&#30340;/a/&#25345;&#32493;&#20803;&#38899;&#30340;&#20845;&#31186;&#24405;&#38899;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#21407;&#21019;&#36129;&#29486;&#26159;&#20351;&#29992;&#22522;&#20110;&#22768;&#38899;&#23545;&#35937;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#39318;&#20808;&#20197;&#27604;&#26631;&#20934;&#39057;&#35889;&#26356;&#20934;&#30830;&#30340;&#26041;&#24335;&#34920;&#31034;&#22768;&#38899;&#35889;&#65292;&#28982;&#21518;&#26500;&#24314;&#21253;&#21547;&#19982;&#34987;&#35797;&#32773;&#23545;&#22768;&#38899;&#30340;&#25511;&#21046;&#26377;&#20851;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#12290;&#32467;&#26524;&#65306;&#35813;&#26041;&#27861;&#22312;&#23558;&#20581;&#24247;&#20010;&#20307;&#19982;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#20010;&#20307;&#21306;&#20998;&#30340;ROC AUC&#20026;0.85&#65292;&#20934;&#30830;&#29575;&#20026;0.76&#12290;&#22312;&#23558;&#20581;&#24247;&#20010;&#20307;&#19982;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#25110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20010;&#20307;&#21306;&#20998;&#30340;&#32467;&#26524;&#20026;0.84&#21644;0.77&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: We present a screening method for early dementia using features based on sound objects as voice biomarkers.   Methods: The final dataset used for machine learning models consisted of 266 observations, with a distribution of 186 healthy individuals, 46 diagnosed with Alzheimer's, and 34 with MCI. This method is based on six-second recordings of the sustained vowel /a/ spoken by the subject. The main original contribution of this work is the use of carefully crafted features based on sound objects. This approach allows one to first represent the sound spectrum in a more accurate way than the standard spectrum, and then build interpretable features containing relevant information about subjects' control over their voice.   Results: ROC AUC obtained in this work for distinguishing healthy subjects from those with MCI was 0.85, while accuracy was 0.76. For distinguishing between healthy subjects and those with either MCI or Alzheimer's the results were 0.84, 0.77, respectively
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20113;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#29366;&#20917;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#30340;&#39118;&#38505;&#20127;&#24453;&#35299;&#20915;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#31867;&#27861;&#26469;&#20840;&#38754;&#30740;&#31350;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#20351;&#29992;&#32773;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#21450;&#20854;&#38450;&#24481;&#25163;&#27573;&#65292;&#26377;&#21161;&#20110;&#21019;&#24314;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00896</link><description>&lt;p&gt;
&#20113;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24433;&#21709;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Privacy and Security Implications of Cloud-Based AI Services : A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20113;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#29366;&#20917;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#30340;&#39118;&#38505;&#20127;&#24453;&#35299;&#20915;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#31867;&#27861;&#26469;&#20840;&#38754;&#30740;&#31350;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#20351;&#29992;&#32773;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#21450;&#20854;&#38450;&#24481;&#25163;&#27573;&#65292;&#26377;&#21161;&#20110;&#21019;&#24314;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#24403;&#21069;&#20113;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#29366;&#20917;&#65292;&#24182;&#25351;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#39118;&#38505;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#21457;&#23637;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24212;&#29992;&#65292;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#20998;&#31867;&#21644;&#37327;&#21270;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#38543;&#30528;AI&#20316;&#20026;&#26381;&#21153;(AIaaS)&#30340;&#26032;&#36235;&#21183;&#20986;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;AI&#27169;&#22411;&#34987;&#27169;&#22411;&#25552;&#20379;&#32773;&#37096;&#32626;&#22312;&#20113;&#31471;&#65292;&#24182;&#34987;&#27169;&#22411;&#20351;&#29992;&#32773;&#20351;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;AIaaS&#39046;&#22495;&#36827;&#34892;&#35843;&#26597;&#65292;&#35760;&#24405;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#24341;&#36215;&#30340;&#21508;&#31181;&#36131;&#20219;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#20840;&#38754;&#30740;&#31350;&#21019;&#36896;&#32773;&#21644;&#20351;&#29992;&#32773;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#21450;&#20854;&#24050;&#30693;&#30340;&#38450;&#24481;&#25163;&#27573;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32773;&#21019;&#24314;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#20250;&#24456;&#26377;&#30410;&#22788;&#12290;&#21516;&#26679;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28040;&#36153;&#32773;&#20063;&#20250;&#21457;&#29616;&#23427;&#23545;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#24456;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper details the privacy and security landscape in today's cloud ecosystem and identifies that there is a gap in addressing the risks introduced by machine learning models. As machine learning algorithms continue to evolve and find applications across diverse domains, the need to categorize and quantify privacy and security risks becomes increasingly critical. With the emerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML models) are deployed on the cloud by model providers and used by model consumers. We first survey the AIaaS landscape to document the various kinds of liabilities that ML models, especially Deep Neural Networks pose and then introduce a taxonomy to bridge this gap by holistically examining the risks that creators and consumers of ML models are exposed to and their known defences till date. Such a structured approach will be beneficial for ML model providers to create robust solutions. Likewise, ML model consumers will find it valuable to ev
&lt;/p&gt;</description></item><item><title>MoDE&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#24212;&#29992;&#19987;&#23478;&#38388;&#30456;&#20114;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;MoDE&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#25928;&#65292;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00893</link><description>&lt;p&gt;
MoDE:&#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#38388;&#30456;&#20114;&#33976;&#39311;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00893
&lt;/p&gt;
&lt;p&gt;
MoDE&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#24212;&#29992;&#19987;&#23478;&#38388;&#30456;&#20114;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;MoDE&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#25928;&#65292;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#28151;&#21512;&#19987;&#23478;(MoE)&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22312;MoE&#32467;&#26500;&#20013;&#65292;&#38376;&#25511;&#23618;&#22312;&#21306;&#20998;&#21644;&#36335;&#30001;&#36755;&#20837;&#29305;&#24449;&#21040;&#19981;&#21516;&#30340;&#19987;&#23478;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20351;&#24471;&#27599;&#20010;&#19987;&#23478;&#33021;&#22815;&#19987;&#27880;&#20110;&#22788;&#29702;&#20182;&#20204;&#23545;&#24212;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38376;&#25511;&#30340;&#36335;&#30001;&#26426;&#21046;&#20063;&#20250;&#23548;&#33268;&#29421;&#31364;&#30340;&#35270;&#37326;&#65306;&#21333;&#20010;MoE&#30340;&#19987;&#23478;&#26080;&#27861;&#20351;&#29992;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20998;&#37197;&#30340;&#23376;&#20219;&#21153;&#65292;&#36825;&#21453;&#36807;&#26469;&#20250;&#38480;&#21046;MoE&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Mixture-of-Distilled-Expert (MoDE)&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#19987;&#23478;&#20043;&#38388;&#24212;&#29992;&#36866;&#24230;&#30340;&#30456;&#20114;&#33976;&#39311;&#65292;&#20351;&#27599;&#20010;&#19987;&#23478;&#33021;&#22815;&#23398;&#20064;&#20854;&#20182;&#19987;&#23478;&#23398;&#21040;&#30340;&#26356;&#22810;&#29305;&#24449;&#65292;&#24182;&#23545;&#20854;&#21407;&#22987;&#20998;&#37197;&#30340;&#23376;&#20219;&#21153;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#35748;&#30693;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;MoDE&#30340;&#26377;&#25928;&#24615;&#12289;&#36890;&#29992;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of mixture-of-experts (MoE) is gaining popularity due to its ability to improve model's performance. In an MoE structure, the gate layer plays a significant role in distinguishing and routing input features to different experts. This enables each expert to specialize in processing their corresponding sub-tasks. However, the gate's routing mechanism also gives rise to narrow vision: the individual MoE's expert fails to use more samples in learning the allocated sub-task, which in turn limits the MoE to further improve its generalization ability. To effectively address this, we propose a method called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual distillation among experts to enable each expert to pick up more features learned by other experts and gain more accurate perceptions on their original allocated sub-tasks. We conduct plenty experiments including tabular, NLP and CV datasets, which shows MoDE's effectiveness, universality and robustness. Furth
&lt;/p&gt;</description></item><item><title>EVA-GAN&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#38899;&#39057;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25913;&#21892;&#20102;&#39057;&#35889;&#21644;&#39640;&#39057;&#37325;&#24314;&#20197;&#21450;&#23545;&#22495;&#22806;&#25968;&#25454;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#38899;&#39057;&#30340;&#29983;&#25104;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00892</link><description>&lt;p&gt;
EVA-GAN: &#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#22686;&#24378;&#30340;&#22810;&#26679;&#21270;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00892
&lt;/p&gt;
&lt;p&gt;
EVA-GAN&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#38899;&#39057;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25913;&#21892;&#20102;&#39057;&#35889;&#21644;&#39640;&#39057;&#37325;&#24314;&#20197;&#21450;&#23545;&#22495;&#22806;&#25968;&#25454;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#38899;&#39057;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26102;&#20195;&#65292;&#36890;&#36807;&#21033;&#29992;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#26469;&#25429;&#25417;&#21644;&#21512;&#25104;&#22797;&#26434;&#30340;&#27169;&#24335;&#65292;&#22823;&#24133;&#36229;&#36234;&#36739;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#38899;&#39057;&#29983;&#25104;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#39640;&#20445;&#30495;&#65288;HiFi&#65289;44.1kHz&#39046;&#22495;&#30340;&#25193;&#23637;&#20173;&#28982;&#26377;&#38480;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#27809;&#26377;&#24310;&#20280;&#21040;&#39640;&#39057;&#22495;&#20013;&#30340;&#39057;&#35889;&#19981;&#36830;&#32493;&#24615;&#21644;&#27169;&#31946;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#21253;&#25324;&#38899;&#20048;&#21644;&#27468;&#21809;&#29983;&#25104;&#22312;&#20869;&#30340;&#21508;&#31181;&#29992;&#20363;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#30340;&#22686;&#24378;&#38899;&#39057;&#29983;&#25104;&#65288;EVA-GAN&#65289;&#65292;&#22312;&#39057;&#35889;&#21644;&#39640;&#39057;&#37325;&#24314;&#20197;&#21450;&#22495;&#22806;&#25968;&#25454;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#26041;&#38754;&#65292;&#36739;&#20043;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#21253;&#21547;36,000&#20010;&#26679;&#26412;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#29983;&#25104;HiFi&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Large Models marks a new era in machine learning, significantly outperforming smaller models by leveraging vast datasets to capture and synthesize complex patterns. Despite these advancements, the exploration into scaling, especially in the audio generation domain, remains limited, with previous efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and suffering from both spectral discontinuities and blurriness in the high-frequency domain, alongside a lack of robustness against out-of-domain data. These limitations restrict the applicability of models to diverse use cases, including music and singing generation. Our work introduces Enhanced Various Audio Generation via Scalable Generative Adversarial Networks (EVA-GAN), yields significant improvements over previous state-of-the-art in spectral and high-frequency reconstruction and robustness in out-of-domain data performance, enabling the generation of HiFi audios by employing an extensive dataset of 36,000 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#38450;&#24481;&#21644;&#23545;&#25239;&#24212;&#29992;&#65292;&#24182;&#35782;&#21035;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#32570;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;LLM&#39537;&#21160;&#30340;&#32593;&#32476;&#23433;&#20840;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#26426;&#20250;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00891</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65306;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cybersecurity: State-of-the-Art
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#38450;&#24481;&#21644;&#23545;&#25239;&#24212;&#29992;&#65292;&#24182;&#35782;&#21035;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#32570;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;LLM&#39537;&#21160;&#30340;&#32593;&#32476;&#23433;&#20840;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#26426;&#20250;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#23545;&#26234;&#33021;&#30340;&#29702;&#35299;&#65292;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#20110;&#20154;&#24037;&#26234;&#33021;&#12290;&#33258;&#20174;&#23427;&#20204;&#34987;&#24341;&#20837;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#25506;&#32034;&#20102;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#20256;&#32479;&#19978;&#23545;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#25345;&#25269;&#35302;&#24577;&#24230;&#19988;&#23545;&#26426;&#22120;&#23398;&#20064;&#37319;&#29992;&#36739;&#24930;&#65292;&#36825;&#19968;&#39046;&#22495;&#21364;&#24322;&#20891;&#31361;&#36215;&#12290;&#26412;&#30740;&#31350;&#23545;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20840;&#38754;&#25551;&#36848;&#20102;LLMs&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#38450;&#24481;&#21644;&#23545;&#25239;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#19981;&#20165;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#20998;&#31867;&#65292;&#24182;&#19988;&#36824;&#35782;&#21035;&#20986;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#32570;&#12290;&#36890;&#36807;&#35780;&#20272;&#25915;&#20987;&#21644;&#38450;&#24481;&#24212;&#29992;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;LLM&#39537;&#21160;&#30340;&#32593;&#32476;&#23433;&#20840;&#25152;&#28041;&#21450;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#26426;&#20250;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#20998;&#32452;&#35270;&#20026;&#22270;&#26500;&#24314;&#38382;&#39064;&#65292;&#21033;&#29992;&#22270;&#30340;&#26368;&#22823;&#20999;&#21106;&#26469;&#36827;&#34892;&#26368;&#20248;&#20998;&#32452;&#20915;&#31574;&#12290;&#28436;&#21592;-&#35780;&#35770;&#23478;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;AC-GRL&#65289;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#26469;&#23454;&#29616;&#26368;&#20248;&#22270;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#26368;&#20248;&#22270;&#30340;&#36793;&#26435;&#37325;&#26469;&#26368;&#22823;&#21270;&#32593;&#32476;&#30340;&#29992;&#25143;&#21534;&#21520;&#37327;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00879</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#31454;&#20105;&#21644;&#24178;&#25200;&#31649;&#29702;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning for Contention and Interference Management in Wireless Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#20998;&#32452;&#35270;&#20026;&#22270;&#26500;&#24314;&#38382;&#39064;&#65292;&#21033;&#29992;&#22270;&#30340;&#26368;&#22823;&#20999;&#21106;&#26469;&#36827;&#34892;&#26368;&#20248;&#20998;&#32452;&#20915;&#31574;&#12290;&#28436;&#21592;-&#35780;&#35770;&#23478;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;AC-GRL&#65289;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#26469;&#23454;&#29616;&#26368;&#20248;&#22270;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#26368;&#20248;&#22270;&#30340;&#36793;&#26435;&#37325;&#26469;&#26368;&#22823;&#21270;&#32593;&#32476;&#30340;&#29992;&#25143;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Wi-Fi 802.11ah&#32593;&#32476;&#20013;&#65292;&#21463;&#38480;&#21046;&#30340;&#25509;&#20837;&#31383;&#21475;&#65288;RAW&#65289;&#36890;&#36807;&#23558;&#29992;&#25143;&#20998;&#32452;&#21644;&#20998;&#37197;&#21608;&#26399;&#24615;&#26102;&#38388;&#27573;&#26469;&#31649;&#29702;&#31454;&#20105;&#21644;&#24178;&#25200;&#12290;&#25105;&#20204;&#23558;&#25214;&#21040;RAW&#20013;&#30340;&#26368;&#20248;&#29992;&#25143;&#20998;&#32452;&#20915;&#31574;&#65292;&#20197;&#26368;&#22823;&#21270;&#32593;&#32476;&#30340;&#26368;&#22351;&#24773;&#20917;&#29992;&#25143;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#22238;&#39038;&#29616;&#26377;&#30340;&#29992;&#25143;&#20998;&#32452;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#22312;&#19978;&#36848;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#29992;&#25143;&#20998;&#32452;&#35270;&#20026;&#22270;&#26500;&#24314;&#38382;&#39064;&#65292;&#20854;&#20013;&#39030;&#28857;&#34920;&#31034;&#29992;&#25143;&#65292;&#36793;&#26435;&#37325;&#34920;&#31034;&#31454;&#20105;&#21644;&#24178;&#25200;&#12290;&#36825;&#31181;&#24418;&#24335;&#21033;&#29992;&#22270;&#30340;&#26368;&#22823;&#20999;&#21106;&#26469;&#20998;&#32452;&#29992;&#25143;&#65292;&#24182;&#20248;&#21270;&#36793;&#26435;&#37325;&#20197;&#26500;&#24314;&#20986;&#26368;&#20339;&#22270;&#65292;&#20854;&#26368;&#22823;&#20999;&#21106;&#20135;&#29983;&#26368;&#20248;&#30340;&#20998;&#32452;&#20915;&#31574;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#26368;&#20248;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;AC-GRL&#65289;&#31639;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#28436;&#21592;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#34987;&#35757;&#32451;&#26469;&#20272;&#35745;&#26368;&#20248;&#22270;&#30340;&#36793;&#26435;&#37325;&#65292;&#20351;&#29992;&#29992;&#25143;&#21644;&#25509;&#20837;&#28857;&#20043;&#38388;&#30340;&#36335;&#24452;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restricted access window (RAW) in Wi-Fi 802.11ah networks manages contention and interference by grouping users and allocating periodic time slots for each group's transmissions. We will find the optimal user grouping decisions in RAW to maximize the network's worst-case user throughput. We review existing user grouping approaches and highlight their performance limitations in the above problem. We propose formulating user grouping as a graph construction problem where vertices represent users and edge weights indicate the contention and interference. This formulation leverages the graph's max cut to group users and optimizes edge weights to construct the optimal graph whose max cut yields the optimal grouping decisions. To achieve this optimal graph construction, we design an actor-critic graph representation learning (AC-GRL) algorithm. Specifically, the actor neural network (NN) is trained to estimate the optimal graph's edge weights using path losses between users and access points
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21457;&#24067;&#21253;&#21547;&#27169;&#25311;&#30340;&#36335;&#24452;&#25439;&#32791;&#26080;&#32447;&#30005;&#22320;&#22270;&#12289;&#30495;&#23454;&#22478;&#24066;&#22320;&#22270;&#21644;&#33322;&#25293;&#24433;&#20687;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#21021;&#27493;&#23454;&#39564;&#65292;&#22635;&#34917;&#20102;&#24320;&#25918;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24211;&#30340;&#31354;&#30333;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00878</link><description>&lt;p&gt;
&#26080;&#26041;&#21521;&#24615;&#21457;&#23556;&#22825;&#32447;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#8212;&#8212;&#21253;&#21547;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#23454;&#39564;&#30340;&#24320;&#25918;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Radio Map Estimation -- An Open Dataset with Directive Transmitter Antennas and Initial Experiments
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21457;&#24067;&#21253;&#21547;&#27169;&#25311;&#30340;&#36335;&#24452;&#25439;&#32791;&#26080;&#32447;&#30005;&#22320;&#22270;&#12289;&#30495;&#23454;&#22478;&#24066;&#22320;&#22270;&#21644;&#33322;&#25293;&#24433;&#20687;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#21021;&#27493;&#23454;&#39564;&#65292;&#22635;&#34917;&#20102;&#24320;&#25918;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24211;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#30830;&#23450;&#22478;&#24066;&#36890;&#20449;&#32593;&#32476;&#20013;&#21457;&#23556;&#26426;&#21644;&#25509;&#25910;&#26426;&#20043;&#38388;&#30340;&#22823;&#23610;&#24230;&#20449;&#21495;&#34928;&#33853;&#65288;&#20063;&#31216;&#20026;&#8220;&#36335;&#24452;&#25439;&#32791;&#8221;&#65289;&#12290;&#20013;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26367;&#20195;&#26114;&#36149;&#30340;&#27979;&#37327;&#27963;&#21160;&#65292;&#19981;&#20934;&#30830;&#30340;&#32479;&#35745;&#27169;&#22411;&#25110;&#35745;&#31639;&#37327;&#22823;&#30340;&#20809;&#32447;&#36861;&#36394;&#27169;&#25311;&#65292;&#36825;&#20123;&#27169;&#22411;&#19968;&#26086;&#35757;&#32451;&#65292;&#20960;&#20046;&#33021;&#21363;&#26102;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35813;&#20027;&#39064;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#20294;&#32570;&#20047;&#24320;&#25918;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24211;&#20351;&#24471;&#27599;&#20010;&#20154;&#37117;&#21487;&#20197;&#27979;&#35797;&#21644;&#27604;&#36739;&#24050;&#24320;&#21457;&#30340;&#26041;&#27861;&#21644;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21457;&#24067;&#19968;&#32452;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#25311;&#30340;&#36335;&#24452;&#25439;&#32791;&#26080;&#32447;&#30005;&#22320;&#22270;&#12289;&#30495;&#23454;&#22478;&#24066;&#22320;&#22270;&#21644;&#26469;&#33258;&#24320;&#25918;&#25968;&#25454;&#28304;&#30340;&#33322;&#25293;&#24433;&#20687;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20851;&#20110;&#27169;&#22411;&#26550;&#26500;&#12289;&#36755;&#20837;&#29305;&#24449;&#35774;&#35745;&#21644;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#30340;&#21021;&#22987;&#23454;&#39564;&#20063;&#24050;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last years, several works have explored the application of deep learning algorithms to determine the large-scale signal fading (also referred to as ``path loss'') between transmitter and receiver pairs in urban communication networks. The central idea is to replace costly measurement campaigns, inaccurate statistical models or computationally expensive ray-tracing simulations by machine learning models which, once trained, produce accurate predictions almost instantly. Although the topic has attracted attention from many researchers, there are few open benchmark datasets and codebases that would allow everyone to test and compare the developed methods and algorithms. We take a step towards filling this gap by releasing a publicly available dataset of simulated path loss radio maps together with realistic city maps from real-world locations and aerial images from open datasources. Initial experiments regarding model architectures, input feature design and estimation of radio ma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#24494;&#35843;&#31639;&#27861;SpIEL&#65292;&#24182;&#23545;LLM&#36827;&#34892;&#20102;&#25351;&#20196;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#20854;&#21442;&#25968;&#24222;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16405</link><description>&lt;p&gt;
&#23558;&#31232;&#30095;&#24494;&#35843;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Sparse Fine-Tuning to Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#24494;&#35843;&#31639;&#27861;SpIEL&#65292;&#24182;&#23545;LLM&#36827;&#34892;&#20102;&#25351;&#20196;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#20854;&#21442;&#25968;&#24222;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#20854;&#21442;&#25968;&#30340;&#24222;&#22823;&#25968;&#37327;&#65292;&#24456;&#38590;&#23436;&#20840;&#36827;&#34892;&#24494;&#35843;&#65288;&#20363;&#22914;&#20351;&#29992;&#25351;&#20196;&#25110;&#20154;&#24037;&#21453;&#39304;&#65289;&#12290;&#19968;&#31995;&#21015;&#21442;&#25968;&#39640;&#25928;&#30340;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#23384;&#20648;&#38656;&#27714;&#19982;LLM&#30340;&#22823;&#23567;&#25104;&#27491;&#27604;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;&#24494;&#35843;&#25193;&#23637;&#21040;&#26368;&#20808;&#36827;&#30340;LLM&#65292;&#22914;LLaMA 2 7B&#21644;13B&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpIEL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#38024;&#23545;&#25152;&#38656;&#30340;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#32500;&#25252;&#19968;&#20010;&#21442;&#25968;&#32034;&#24341;&#25968;&#32452;&#21644;&#36825;&#20123;&#21442;&#25968;&#30456;&#23545;&#20110;&#39044;&#35757;&#32451;&#20540;&#30340;&#22686;&#37327;&#12290;&#23427;&#36941;&#21382;&#20197;&#19979;&#27493;&#39588;&#65306;&#65288;a&#65289;&#26356;&#26032;&#27963;&#36291;&#22686;&#37327;&#65292;&#65288;b&#65289;&#20462;&#21098;&#32034;&#24341;&#65288;&#22522;&#20110;&#20854;&#22686;&#37327;&#30340;&#21464;&#21270;&#22823;&#23567;&#65289;&#65292;&#20197;&#21450;&#65288;c&#65289;&#37325;&#26032;&#29983;&#38271;&#32034;&#24341;&#12290;&#23545;&#20110;&#37325;&#26032;&#29983;&#38271;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#23569;&#37327;&#20505;&#36873;&#21442;&#25968;&#30340;&#32047;&#31215;&#26799;&#24230;&#25110;&#20351;&#29992;&#39640;&#25928;&#30340;SM3&#20248;&#21270;&#22120;&#20272;&#35745;&#30340;&#36817;&#20284;&#21160;&#24046;&#30340;&#20004;&#20010;&#20934;&#21017;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse fine-tuning method which, for a desired density level, maintains an array of parameter indices and the deltas of these parameters relative to their pretrained values. It iterates over: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LL
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16184</link><description>&lt;p&gt;
&#20851;&#20110;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#35821;&#20041;&#23398;&#65306;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Semantics of LM Latent Space: A Vocabulary-defined Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#25913;&#36827;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20998;&#26512;&#24448;&#24448;&#22312;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;LM&#35821;&#20041;&#30340;&#20998;&#31163;&#27934;&#23519;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#24573;&#35270;&#20102;LM&#36866;&#24212;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#21709;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#65292;&#23427;&#22312;LM&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20132;&#32455;&#20998;&#26512;&#65292;&#21033;&#29992;LM&#35789;&#27719;&#26469;&#33719;&#24471;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#65292;&#24378;&#35843;&#21487;&#24494;&#20998;&#24615;&#21644;&#23616;&#37096;&#31561;&#36317;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#36827;&#34892;&#35821;&#20041;&#26657;&#20934;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25991;&#26412;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;</title><link>https://rss.arxiv.org/abs/2401.15963</link><description>&lt;p&gt;
NoFunEval: &#26377;&#36259;&#30340;&#26159;&#65292;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#19978;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.15963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;code LMs&#65289;&#30340;&#35780;&#20272;&#22522;&#20934;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;LMs&#26159;&#21542;&#33021;&#22815;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#19978;&#12290;&#22312;&#23454;&#38469;&#30340;&#36719;&#20214;&#24037;&#31243;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#20250;&#32771;&#34385;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;&#20182;&#20204;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#23454;&#29616;&#21151;&#33021;&#26377;&#30528;&#23545;&#25972;&#20307;&#31995;&#32479;&#35774;&#35745;&#30446;&#26631;&#65288;&#22914;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#65289;&#30340;&#35201;&#27714;&#12290;&#22914;&#26524;LMs&#33021;&#22815;&#23637;&#31034;&#23545;&#35201;&#27714;&#21644;&#20195;&#30721;&#35821;&#20041;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#20182;&#20204;&#20063;&#20250;&#26356;&#21152;&#20449;&#20219;&#36825;&#20123;LMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;NoFunEval&#26469;&#35780;&#20272;&#20195;&#30721;LMs&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26041;&#27861;Coding Concepts (CoCo)&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#20154;&#21592;&#21521;LMs&#20256;&#36798;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#23545;22&#20010;&#20195;&#30721;LMs&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26222;&#36941;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#30528;&#23427;&#20204;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics.   We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their tr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;</title><link>https://rss.arxiv.org/abs/2401.09407</link><description>&lt;p&gt;
&#35299;&#35835;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;: &#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#35328;&#35821;&#20041;&#30340;&#24191;&#20041;&#31574;&#30053;&#26469;&#26816;&#27979;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.09407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#26377;&#25928;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;: &#39318;&#20808;&#65292;&#20182;&#20204;&#22312;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#26102;&#38754;&#20020;&#30528;&#26497;&#22823;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#22330;&#26223;&#20013;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#30001;&#21508;&#31181;&#29983;&#25104;&#22120;&#20135;&#29983;&#30340;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;GPT-4&#21644;Dolly&#65292;&#24182;&#28085;&#30422;&#21508;&#31181;&#39046;&#22495;&#65292;&#20174;&#23398;&#26415;&#25163;&#31295;&#21040;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#35270;&#20026;&#20005;&#26684;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#26102;&#21463;&#21040;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2312.15101</link><description>&lt;p&gt;
&#20462;&#22797;-Con&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#30340;&#33258;&#21160;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.15101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#27493;&#39588;&#65292;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#27169;&#22411;&#22312;&#35774;&#22791;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#21033;&#29992;&#21487;&#33021;&#21482;&#22312;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#25552;&#20379;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36716;&#25442;&#36807;&#31243;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#65292;&#23548;&#33268;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#26080;&#27861;&#37096;&#32626;&#25110;&#23384;&#22312;&#38382;&#39064;&#65292;&#20005;&#37325;&#38477;&#20302;&#20102;&#20854;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;Fix-Con&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26102;&#20351;&#29992;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#30340;&#25925;&#38556;&#12290;Fix-Con&#20351;&#29992;&#20174;&#35843;&#26597;&#36716;&#25442;&#38382;&#39064;&#20013;&#25366;&#25496;&#20986;&#30340;&#19968;&#32452;&#25925;&#38556;&#31867;&#22411;&#26469;&#23450;&#20301;&#36716;&#25442;&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#36716;&#25442;&#25925;&#38556;&#65292;&#24182;&#36866;&#24403;&#20462;&#22797;&#23427;&#20204;&#65292;&#20363;&#22914;&#20351;&#29992;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26367;&#25442;&#30446;&#26631;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36825;&#19968;&#36807;&#31243;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#19978;&#36827;&#34892;&#36845;&#20195;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#22312;&#32447;VSMC&#65292;&#23427;&#22522;&#20110;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#21644;&#31890;&#23376;&#25552;&#35758;&#36866;&#24212;&#12290;</title><link>https://rss.arxiv.org/abs/2312.12616</link><description>&lt;p&gt;
&#22312;&#32447;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Variational Sequential Monte Carlo
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.12616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#22312;&#32447;VSMC&#65292;&#23427;&#22522;&#20110;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#21644;&#31890;&#23376;&#25552;&#35758;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#26159;AI&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#32463;&#20856;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23545;&#20110;&#20219;&#20309;&#24418;&#24335;&#30340;&#21442;&#25968;&#23398;&#20064;&#25110;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#65292;&#36890;&#24120;&#38656;&#35201;&#35745;&#31639;&#22797;&#26434;&#30340;&#28508;&#22312;&#29366;&#24577;&#21518;&#39564;&#20998;&#24067;&#12290;&#26412;&#25991;&#22312;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;VSMC&#65289;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#31890;&#23376;&#26041;&#27861;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#21644;&#36125;&#21494;&#26031;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#12290;&#20256;&#32479;&#30340;VSMC&#26041;&#27861;&#22312;&#31163;&#32447;&#27169;&#24335;&#19979;&#36816;&#34892;&#65292;&#36890;&#36807;&#37325;&#22797;&#22788;&#29702;&#32473;&#23450;&#30340;&#25968;&#25454;&#25209;&#27425;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#23558;VSMC&#20195;&#29702;ELBO&#30340;&#26799;&#24230;&#36924;&#36817;&#20998;&#24067;&#21040;&#26102;&#38388;&#19978;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#27969;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#21517;&#20026;&#22312;&#32447;VSMC&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#31890;&#23376;&#25552;&#35758;&#36866;&#24212;&#65292;&#32780;&#19988;&#23436;&#20840;&#23454;&#26102;&#22788;&#29702;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being the most classical generative model for serial data, state-space models (SSM) are fundamental in AI and statistical machine learning. In SSM, any form of parameter learning or latent state inference typically involves the computation of complex latent-state posteriors. In this work, we build upon the variational sequential Monte Carlo (VSMC) method, which provides computationally efficient and accurate model parameter estimation and Bayesian latent-state inference by combining particle methods and variational inference. While standard VSMC operates in the offline mode, by re-processing repeatedly a given batch of data, we distribute the approximation of the gradient of the VSMC surrogate ELBO in time using stochastic approximation, allowing for online learning in the presence of streams of data. This results in an algorithm, online VSMC, that is capable of performing efficiently, entirely on-the-fly, both parameter estimation and particle proposal adaptation. In addition, we prov
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2312.09196</link><description>&lt;p&gt;
DIRECT: &#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIRECT: Deep Active Learning under Imbalance and Label Noise
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.09196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#32597;&#35265;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#26377;&#25928;&#25216;&#26415;&#65292;&#23427;&#20174;&#26681;&#26412;&#19978;&#37319;&#38598;&#26356;&#24179;&#34913;&#21644;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#27880;&#37322;&#12290;&#26631;&#31614;&#22122;&#22768;&#26159;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#20013;&#21478;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#65292;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#32500;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;DIRECT&#33021;&#22815;&#21033;&#29992;&#32463;&#20856;&#30340;&#20027;&#21160;&#23398;&#20064;&#25991;&#29486;&#26469;&#35299;&#20915;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on
&lt;/p&gt;</description></item><item><title>CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2312.07950</link><description>&lt;p&gt;
&#36328;&#22359;&#37327;&#21270;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CBQ: Cross-Block Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.07950
&lt;/p&gt;
&lt;p&gt;
CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#22312;&#20197;&#26497;&#20302;&#25104;&#26412;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#21482;&#20851;&#27880;&#22788;&#29702;&#21333;&#20010;&#23618;&#25110;&#21333;&#20010;&#22359;&#20869;&#30340;&#24322;&#24120;&#20540;&#65292;&#24573;&#30053;&#20102;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#20302;&#20301;&#35774;&#32622;&#20013;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#38388;&#37325;&#26500;&#30340;&#36328;&#22359;PTQ&#26041;&#27861;CBQ&#12290;CBQ&#37319;&#29992;&#20102;&#19968;&#31181;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#23454;&#29616;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;&#27492;&#22806;&#65292;CBQ&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#65288;CFP&#65289;&#26469;&#25233;&#21046;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#30340;&#24322;&#24120;&#20540;&#65292;&#24182;&#37197;&#21512;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;LoRA&#21462;&#25972;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#26435;&#37325;&#37327;&#21270;&#12290;&#36825;&#20123;&#21019;&#26032;&#20351;CBQ&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#65292;&#36824;&#33021;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CBQ&#22312;&#20302;&#20301;&#37327;&#21270;&#65288;W4A4&#65292;W4A8&#31561;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#28145;&#24230;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#20010;&#38382;&#39064;&#26159;&#30001;&#20110;&#8220;token&#30456;&#20284;&#24615;&#21319;&#32423;&#8221;&#23548;&#33268;&#30340;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35843;&#26597;&#30340;&#35777;&#25454;&#12290;</title><link>https://rss.arxiv.org/abs/2312.06182</link><description>&lt;p&gt;
"&#20026;&#20160;&#20040;&#8220;&#32463;&#20856;&#8221;&#30340;Transformer&#27169;&#22411;&#26159;&#32932;&#27973;&#30340;&#20197;&#21450;&#22914;&#20309;&#20351;&#23427;&#20204;&#21464;&#24471;&#26356;&#28145;&#20837;"
&lt;/p&gt;
&lt;p&gt;
Why "classic" Transformers are shallow and how to make them go deep
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.06182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#28145;&#24230;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#20010;&#38382;&#39064;&#26159;&#30001;&#20110;&#8220;token&#30456;&#20284;&#24615;&#21319;&#32423;&#8221;&#23548;&#33268;&#30340;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35843;&#26597;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20854;&#22312;2017&#24180;&#30340;&#24341;&#20837;&#20197;&#26469;&#65292;Transformer&#24050;&#25104;&#20026;&#39046;&#20808;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23454;&#29616;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;Transformer&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#26426;&#21046;&#65292;&#26088;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23558;&#21407;&#22987;&#30340;Transformer&#35774;&#35745;&#25193;&#23637;&#20026;&#26356;&#28145;&#23618;&#27425;&#30340;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#29978;&#33267;&#26080;&#27861;&#23454;&#29616;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#21508;&#31181;&#20462;&#25913;&#26469;&#23558;&#26356;&#22810;&#23618;&#30340;SA&#26426;&#21046;&#22534;&#21472;&#21040;&#26356;&#28145;&#23618;&#27425;&#30340;&#27169;&#22411;&#20013;&#65292;&#20294;&#23545;&#20110;&#36825;&#20010;&#28145;&#24230;&#38382;&#39064;&#30340;&#23436;&#20840;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35843;&#26597;&#65292;&#35777;&#23454;&#20102;&#28145;&#24230;&#38382;&#39064;&#26159;&#30001;&#20110;&#8220;token&#30456;&#20284;&#24615;&#21319;&#32423;&#8221;&#24341;&#36215;&#30340;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#37325;&#22797;&#24212;&#29992;SA&#26426;&#21046;&#21518;&#65292;token&#36880;&#28176;&#21464;&#24471;&#36234;&#26469;&#36234;&#30456;&#20284;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#65292;&#21463;&#21040;&#27880;&#24847;&#21147;&#30697;&#38453;&#19981;&#21464;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#22823;&#30340;&#39057;&#35889;&#38388;&#38553;&#30340;&#39537;&#21160;&#65292;token&#30340;&#30456;&#20284;&#24615;&#36880;&#28176;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its introduction in 2017, Transformer has emerged as the leading neural network architecture, catalyzing revolutionary advancements in many AI disciplines. The key innovation in Transformer is a Self-Attention (SA) mechanism designed to capture contextual information. However, extending the original Transformer design to models of greater depth has proven exceedingly challenging, if not impossible. Even though various modifications have been proposed in order to stack more layers of SA mechanism into deeper models, a full understanding of this depth problem remains lacking. In this paper, we conduct a comprehensive investigation, both theoretically and empirically, to substantiate the claim that the depth problem is caused by \emph{token similarity escalation}; that is, tokens grow increasingly alike after repeated applications of the SA mechanism. Our analysis reveals that, driven by the invariant leading eigenspace and large spectral gaps of attention matrices, token similarity
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2312.05356</link><description>&lt;p&gt;
Neuron Patching: &#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#19982;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.05356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#26356;&#26032;&#36825;&#20123;&#27169;&#22411;&#30340;&#26032;&#30693;&#35782;&#38750;&#24120;&#26114;&#36149;&#65292;&#36890;&#24120;&#38656;&#35201;&#20840;&#38754;&#23454;&#29616;&#20854;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;MENT&#65292;&#29992;&#20110;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#12290;&#22522;&#20110;&#29983;&#25104;&#24335;LLM&#30340;&#26426;&#21046;&#65292;MENT&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#20196;&#29260;&#26102;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;&#24120;&#35265;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;MENT&#20855;&#26377;&#39640;&#25928;&#12289;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#29305;&#28857;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#20462;&#34917;1&#25110;2&#20010;&#31070;&#32463;&#20803;&#26469;&#32416;&#27491;&#31070;&#32463;&#27169;&#22411;&#12290;&#20316;&#20026;&#31070;&#32463;&#20803;&#23618;&#38754;&#19978;&#29983;&#25104;&#27169;&#22411;&#32534;&#36753;&#30340;&#20808;&#39537;&#24037;&#20316;&#65292;&#25105;&#20204;&#35268;&#33539;&#20102;&#32534;&#36753;&#36807;&#31243;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34913;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;API&#24207;&#21015;&#25512;&#33616;&#12289;&#34892;&#32423;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;</title><link>https://rss.arxiv.org/abs/2312.02783</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Graphs: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT4&#21644;LLaMA&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#25991;&#26412;&#32534;&#30721;/&#35299;&#30721;&#33021;&#21147;&#21644;&#26032;&#21457;&#29616;&#30340;&#32039;&#24613;&#33021;&#21147;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#34429;&#28982;LLMs&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#32431;&#25991;&#26412;&#65292;&#20294;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#19982;&#22270;&#24418;&#24418;&#24335;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#30456;&#20851;&#32852;&#65288;&#20363;&#22914;&#23398;&#26415;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#65289;&#65292;&#25110;&#32773;&#22270;&#24418;&#25968;&#25454;&#19982;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#37197;&#23545;&#65288;&#20363;&#22914;&#24102;&#26377;&#25551;&#36848;&#30340;&#20998;&#23376;&#65289;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#22522;&#20110;&#32431;&#25991;&#26412;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#27492;&#31867;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65288;&#21363;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#22330;&#26223;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#37319;&#29992;LLMs&#22312;&#22270;&#24418;&#19978;&#30340;&#28508;&#22312;&#22330;&#26223;&#65292;&#20998;&#20026;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
&lt;/p&gt;</description></item><item><title>&#36793;&#38469;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;&#65288;MLS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25913;&#36827;&#29256;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;&#65288;LS&#65289;&#65292;&#36890;&#36807;&#20445;&#30041;&#25968;&#25454;&#38598;&#36793;&#32536;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#34987;&#25104;&#21151;&#22320;&#38598;&#25104;&#21040;&#19981;&#21516;iable&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#65288;DUFS&#65289;&#31639;&#27861;&#20013;&#65292;&#22312;&#21512;&#25104;&#21644;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#31283;&#20581;&#19988;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2311.17795</link><description>&lt;p&gt;
&#36793;&#38469;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Marginal Laplacian Score
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.17795
&lt;/p&gt;
&lt;p&gt;
&#36793;&#38469;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;&#65288;MLS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25913;&#36827;&#29256;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;&#65288;LS&#65289;&#65292;&#36890;&#36807;&#20445;&#30041;&#25968;&#25454;&#38598;&#36793;&#32536;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#34987;&#25104;&#21151;&#22320;&#38598;&#25104;&#21040;&#19981;&#21516;iable&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#65288;DUFS&#65289;&#31639;&#27861;&#20013;&#65292;&#22312;&#21512;&#25104;&#21644;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#31283;&#20581;&#19988;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#32473;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22312;&#27809;&#26377;&#36275;&#22815;&#25110;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#23545;&#21518;&#32493;&#31639;&#27861;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36793;&#38469;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;&#65288;MLS&#65289;&#65292;&#23427;&#26159;&#33879;&#21517;&#30340;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;&#65288;LS&#65289;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#12290;&#25105;&#20204;&#20551;&#35774;&#23569;&#25968;&#31867;&#25110;&#24322;&#24120;&#31867;&#22312;&#29305;&#24449;&#30340;&#36793;&#32536;&#20013;&#26356;&#39057;&#32321;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;MLS&#26088;&#22312;&#20445;&#30041;&#25968;&#25454;&#38598;&#36793;&#32536;&#30340;&#23616;&#37096;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;&#20854;&#38598;&#25104;&#21040;&#21033;&#29992;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;&#30340;&#29616;&#20195;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20013;&#12290;&#25105;&#20204;&#23558;MLS&#31639;&#27861;&#38598;&#25104;&#21040;&#21487;&#24494;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#65288;DUFS&#65289;&#20013;&#65292;&#24471;&#21040;&#20102;DUFS-MLS&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#31283;&#20581;&#19988;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional imbalanced data poses a machine learning challenge. In the absence of sufficient or high-quality labels, unsupervised feature selection methods are crucial for the success of subsequent algorithms. Therefore, we introduce a Marginal Laplacian Score (MLS), a modification of the well known Laplacian Score (LS) tailored to better address imbalanced data. We introduce an assumption that the minority class or anomalous appear more frequently in the margin of the features. Consequently, MLS aims to preserve the local structure of the dataset's margin. We propose its integration into modern feature selection methods that utilize the Laplacian score. We integrate the MLS algorithm into the Differentiable Unsupervised Feature Selection (DUFS), resulting in DUFS-MLS. The proposed methods demonstrate robust and improved performance on synthetic and public datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#26029;&#31163;&#25955;&#26102;&#21464;&#22870;&#21169;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32858;&#31867;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#24182;&#29420;&#31435;&#35299;&#20915;&#27599;&#20010;&#24847;&#22270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://rss.arxiv.org/abs/2311.13870</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#34892;&#20026;&#34920;&#31034;&#30340;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-intention Inverse Q-learning for Interpretable Behavior Representation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.13870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#26029;&#31163;&#25955;&#26102;&#21464;&#22870;&#21169;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32858;&#31867;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#24182;&#29420;&#31435;&#35299;&#20915;&#27599;&#20010;&#24847;&#22270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#21160;&#20915;&#31574;&#36807;&#31243;&#29702;&#35299;&#26041;&#38754;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#22312;&#37325;&#26500;&#21160;&#29289;&#22797;&#26434;&#34892;&#20026;&#20013;&#30340;&#22810;&#20010;&#24847;&#22270;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;&#37492;&#20110;&#26368;&#36817;&#21457;&#23637;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#24847;&#22270;IRL&#26694;&#26550;&#65292;&#20154;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;IRL&#25512;&#26029;&#31163;&#25955;&#30340;&#26102;&#21464;&#22870;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#65288;&#39532;&#23572;&#31185;&#22827;&#65289;&#21464;&#37327;&#36870;Q&#23398;&#20064;&#65288;L(M)V-IQL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#36866;&#24212;&#31163;&#25955;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#30340;IRL&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#32858;&#31867;&#25104;&#19981;&#21516;&#30340;&#24847;&#22270;&#65292;&#24182;&#20026;&#27599;&#20010;&#24847;&#22270;&#29420;&#31435;&#35299;&#20915;IRL&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#23545;&#19981;&#21516;&#30495;&#23454;&#40736;&#31867;&#34892;&#20026;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#19968;&#36827;&#23637;&#26377;&#26395;&#25171;&#24320;&#25512;&#21160;&#31185;&#23398;&#19982;&#24037;&#31243;&#24212;&#29992;&#30340;&#26032;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
In advancing the understanding of decision-making processes, Inverse Reinforcement Learning (IRL) have proven instrumental in reconstructing animal's multiple intentions amidst complex behaviors. Given the recent development of a continuous-time multi-intention IRL framework, there has been persistent inquiry into inferring discrete time-varying rewards with IRL. To tackle the challenge, we introduce Latent (Markov) Variable Inverse Q-learning (L(M)V-IQL), a novel class of IRL algorthms tailored for accommodating discrete intrinsic reward functions. Leveraging an Expectation-Maximization approach, we cluster observed expert trajectories into distinct intentions and independently solve the IRL problem for each. Demonstrating the efficacy of L(M)V-IQL through simulated experiments and its application to different real mouse behavior datasets, our approach surpasses current benchmarks in animal behavior prediction, producing interpretable reward functions. This advancement holds promise f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#21382;&#21490;&#38590;&#39064;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>https://rss.arxiv.org/abs/2311.09200</link><description>&lt;p&gt;
&#27491;&#21017;&#27969;&#26159;&#21542;&#26159;&#35299;&#38145;&#25351;&#25968;&#26426;&#21046;&#30340;&#20851;&#38190;&#65311;&#32463;&#36807;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#21452;&#37325;&#32422;&#26463;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#26465;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.09200
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#21382;&#21490;&#38590;&#39064;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26368;&#20808;&#36827;&#19988;&#20107;&#23454;&#26631;&#20934;&#26159;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DPSGD&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26412;&#36136;&#19978;&#26159;&#28010;&#36153;&#30340;&#12290;&#36890;&#36807;&#21521;&#27599;&#20010;&#26799;&#24230;&#28155;&#21152;&#22122;&#22768;&#65292;&#23427;&#20250;&#22312;&#27599;&#20010;&#26799;&#24230;&#27493;&#39588;&#20013;&#38477;&#20302;&#25972;&#20307;&#38544;&#31169;&#12290;&#23613;&#31649;&#32463;&#36807;15&#24180;&#30340;&#20016;&#23500;&#30740;&#31350;&#65292;&#25512;&#36827;&#20102;&#32452;&#21512;&#23450;&#29702;&#12289;&#23376;&#37319;&#26679;&#26041;&#27861;&#21644;&#23454;&#29616;&#25216;&#26415;&#65292;&#20294;&#24403;&#21069;&#30340;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#36798;&#21040;&#36275;&#22815;&#30340;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#31169;&#19979;&#20248;&#21270;&#32780;&#35774;&#35745;&#30340;&#25351;&#25968;&#26426;&#21046;&#65288;ExpM&#65289;&#21382;&#26469;&#34987;&#25490;&#38500;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#31169;&#19979;&#35757;&#32451;&#20043;&#22806;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;ExpM&#38656;&#35201;&#20174;&#19968;&#31181;&#21382;&#26469;&#38590;&#20197;&#22788;&#29702;&#30340;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23613;&#31649;&#26368;&#36817;&#21457;&#29616;&#20102;&#27491;&#21017;&#27969;&#27169;&#22411;&#65288;NFs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#36924;&#36817;&#38590;&#20197;&#22788;&#29702;&#20998;&#24067;&#30340;&#34920;&#36798;&#28145;&#24230;&#32593;&#32476;&#65292;&#20294;ExpM&#20173;&#28982;&#22788;&#20110;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#26159;&#21033;&#29992;&#27491;&#21017;&#27969;&#26469;&#32469;&#36807;ExpM&#30340;&#21382;&#21490;&#38556;&#30861;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state of the art and de facto standard for differentially private machine learning (ML) is differentially private stochastic gradient descent (DPSGD). Yet, the method is inherently wasteful. By adding noise to every gradient, it diminishes the overall privacy with every gradient step. Despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ML methods. Meanwhile, the Exponential Mechanism (ExpM), designed for private optimization, has been historically sidelined from privately training modern ML algorithms primarily because ExpM requires sampling from a historically intractable density. Despite the recent discovery of Normalizing Flow models (NFs), expressive deep networks for approximating intractable distributions, ExpM remains in the background. Our position is that leveraging NFs to circumvent historic obstructions of ExpM is a potential
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#35270;&#35282;&#65292;&#25105;&#20204;&#21457;&#29616;$l_2$&#26435;&#37325;&#33539;&#25968;&#26159;Grokking&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#21152;&#36895;&#27867;&#21270;&#65292;&#22312;&#27169;&#25968;&#30456;&#21152;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#22312;Grokking&#20043;&#21069;&#20960;&#20046;&#27809;&#26377;&#23398;&#20064;&#20854;&#20182;&#22522;&#26412;&#32676;&#25805;&#20316;&#65292;&#32780;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#26102;&#21152;&#36895;&#27867;&#21270;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20132;&#25442;&#24459;&#26469;&#35299;&#37322;&#12290;</title><link>https://rss.arxiv.org/abs/2311.06597</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#24615;&#35270;&#35282;&#29702;&#35299;Grokking
&lt;/p&gt;
&lt;p&gt;
Understanding Grokking Through A Robustness Viewpoint
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.06597
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#35270;&#35282;&#65292;&#25105;&#20204;&#21457;&#29616;$l_2$&#26435;&#37325;&#33539;&#25968;&#26159;Grokking&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#21152;&#36895;&#27867;&#21270;&#65292;&#22312;&#27169;&#25968;&#30456;&#21152;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#22312;Grokking&#20043;&#21069;&#20960;&#20046;&#27809;&#26377;&#23398;&#20064;&#20854;&#20182;&#22522;&#26412;&#32676;&#25805;&#20316;&#65292;&#32780;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#26102;&#21152;&#36895;&#27867;&#21270;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20132;&#25442;&#24459;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#34987;&#31216;&#20026;Grokking&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#23427;&#25351;&#30340;&#26159;&#22312;&#27169;&#22411;&#26368;&#21021;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#21518;&#24456;&#20037;&#25165;&#20986;&#29616;&#27867;&#21270;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#26469;&#29702;&#35299;&#36825;&#20010;&#30475;&#20284;&#22855;&#24618;&#30340;&#29616;&#35937;&#12290;&#20174;&#40065;&#26834;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#27969;&#34892;&#30340;$l_2$&#26435;&#37325;&#33539;&#25968;&#65288;&#24230;&#37327;&#65289;&#23454;&#38469;&#19978;&#26159;Grokking&#30340;&#19968;&#20010;&#20805;&#20998;&#26465;&#20214;&#12290;&#22522;&#20110;&#20043;&#21069;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#27867;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#27169;&#25968;&#30456;&#21152;&#25968;&#25454;&#38598;&#19978;&#32771;&#23519;&#20102;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#22312;Grokking&#20043;&#21069;&#65292;&#23427;&#20960;&#20046;&#27809;&#26377;&#23398;&#20064;&#20854;&#20182;&#22522;&#26412;&#30340;&#32676;&#25805;&#20316;&#65292;&#20363;&#22914;&#65292;&#20132;&#25442;&#24459;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26102;&#65292;&#27867;&#21270;&#21152;&#36895;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20132;&#25442;&#24459;&#26469;&#35299;&#37322;&#65292;&#36825;&#26159;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;Grokking&#26102;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#36824;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;$l_2$&#33539;&#25968;&#19982;Grokking&#22312;...
&lt;/p&gt;
&lt;p&gt;
Recently, an interesting phenomenon called grokking has gained much attention, where generalization occurs long after the models have initially overfitted the training data. We try to understand this seemingly strange phenomenon through the robustness of the neural network. From a robustness perspective, we show that the popular $l_2$ weight norm (metric) of the neural network is actually a sufficient condition for grokking. Based on the previous observations, we propose perturbation-based methods to speed up the generalization process. In addition, we examine the standard training process on the modulo addition dataset and find that it hardly learns other basic group operations before grokking, for example, the commutative law. Interestingly, the speed-up of generalization when using our proposed method can be explained by learning the commutative law, a necessary condition when the model groks on the test dataset. We also empirically find that $l_2$ norm correlates with grokking on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#25509;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#35814;&#23613;&#38381;&#29615;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#36817;&#20284;&#20026;&#19981;&#21516;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#20302;&#32500;&#25511;&#21046;&#22120;&#65292;&#24179;&#34913;&#20102;&#36924;&#36817;&#31934;&#24230;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2311.04843</link><description>&lt;p&gt;
&#36328;&#36234;&#32500;&#24230;&#65306;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#21487;&#20449;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging Dimensions: Confident Reachability for High-Dimensional Controllers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.04843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#25509;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#35814;&#23613;&#38381;&#29615;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#36817;&#20284;&#20026;&#19981;&#21516;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#20302;&#32500;&#25511;&#21046;&#22120;&#65292;&#24179;&#34913;&#20102;&#36924;&#36817;&#31934;&#24230;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#36827;&#34892;&#23454;&#29616;&#12290;&#36825;&#26679;&#30340;&#25511;&#21046;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#36890;&#36807;&#22270;&#20687;&#20316;&#20026;&#20027;&#35201;&#24863;&#30693;&#27169;&#24335;&#22312;&#30495;&#23454;&#31995;&#32479;&#19978;&#25191;&#34892;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#36825;&#31181;&#25511;&#21046;&#22120;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#22312;&#22788;&#29702;&#20855;&#26377;&#25968;&#21315;&#20010;&#32500;&#24230;&#30340;&#36755;&#20837;&#26102;&#26080;&#27861;&#25193;&#23637;&#65292;&#29305;&#21035;&#26159;&#24403;&#21508;&#20010;&#36755;&#20837;&#65288;&#22914;&#20687;&#32032;&#65289;&#32570;&#20047;&#26126;&#30830;&#30340;&#29289;&#29702;&#24847;&#20041;&#26102;&#12290;&#26412;&#25991;&#22312;&#36830;&#25509;&#35814;&#23613;&#30340;&#38381;&#29615;&#39564;&#35777;&#19982;&#39640;&#32500;&#25511;&#21046;&#22120;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#65292;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#21487;&#20197;&#29992;&#19981;&#21516;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#20960;&#20010;&#20302;&#32500;&#25511;&#21046;&#22120;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#24179;&#34913;&#20302;&#32500;&#25511;&#21046;&#22120;&#30340;&#36924;&#36817;&#31934;&#24230;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#39564;&#35777;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;&#12290;&#28982;&#21518;&#65292;&#22914;&#26524;&#20302;&#32500;&#21487;&#36798;&#24615;&#32467;&#26524;&#24050;&#32463;&#24471;&#21040;&#20102;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous systems are increasingly implemented using end-to-end learning-based controllers. Such controllers make decisions that are executed on the real system with images as one of the primary sensing modalities. Deep neural networks form a fundamental building block of such controllers. Unfortunately, the existing neural-network verification tools do not scale to inputs with thousands of dimensions -- especially when the individual inputs (such as pixels) are devoid of clear physical meaning. This paper takes a step towards connecting exhaustive closed-loop verification with high-dimensional controllers. Our key insight is that the behavior of a high-dimensional controller can be approximated with several low-dimensional controllers in different regions of the state space. To balance the approximation accuracy and verifiability of our low-dimensional controllers, we leverage the latest verification-aware knowledge distillation. Then, if low-dimensional reachability results are infl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGDF&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#32500;&#32435;&#28388;&#27874;&#29702;&#35770;&#21644;&#24341;&#20837;&#26102;&#21464;&#33258;&#36866;&#24212;&#26435;&#37325;&#65292;&#21152;&#36895;&#20102;SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;SGDF&#22312;&#25910;&#25947;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>https://rss.arxiv.org/abs/2311.02818</link><description>&lt;p&gt;
&#20449;&#21495;&#22788;&#29702;&#19982;SGD&#30456;&#36935;&#65306;&#20174;&#21160;&#37327;&#21040;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Signal Processing Meets SGD: From Momentum to Filter
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.02818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGDF&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#32500;&#32435;&#28388;&#27874;&#29702;&#35770;&#21644;&#24341;&#20837;&#26102;&#21464;&#33258;&#36866;&#24212;&#26435;&#37325;&#65292;&#21152;&#36895;&#20102;SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;SGDF&#22312;&#25910;&#25947;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21450;&#20854;&#22522;&#20110;&#21160;&#37327;&#30340;&#21464;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#20204;&#36890;&#24120;&#38754;&#20020;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#20248;&#21270;&#22120;&#21152;&#36895;&#25910;&#25947;&#65292;&#20294;&#24120;&#24120;&#20197;&#27867;&#21270;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#23646;&#24615;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#36895;SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#19981;&#21464;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20943;&#23567;&#21382;&#21490;&#26799;&#24230;&#30340;&#26041;&#24046;&#30340;&#24605;&#24819;&#65292;&#36890;&#36807;&#24212;&#29992;&#32500;&#32435;&#28388;&#27874;&#29702;&#35770;&#22686;&#24378;SGD&#30340;&#19968;&#38454;&#30697;&#20272;&#35745;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26102;&#21464;&#33258;&#36866;&#24212;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;SGDF&#22312;&#25910;&#25947;&#21644;&#27867;&#21270;&#20043;&#38388;&#25214;&#21040;&#20102;&#19968;&#20010;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, stochastic gradient descent (SGD) and its momentum-based variants are widely used in optimization algorithms, they usually face the problem of slow convergence. Meanwhile, existing adaptive learning rate optimizers accelerate convergence but often at the expense of generalization ability. We demonstrate that the adaptive learning rate property impairs generalization. To address this contradiction, we propose a novel optimization method that aims to accelerate the convergence rate of SGD without loss of generalization. This approach is based on the idea of reducing the variance of the historical gradient, enhancing the first-order moment estimation of the SGD by applying Wiener filtering theory, and introducing a time-varying adaptive weight. Experimental results show that SGDF achieves a trade-off between convergence and generalization compared to state-of-the-art optimizers.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#21521;$\chi^2$&#25955;&#24230;&#30340;&#21464;&#20998;&#37325;&#35201;&#25277;&#26679;&#26041;&#27861;(VIS)&#65292;&#36890;&#36807;&#30452;&#25509;&#20272;&#35745;&#21644;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#26469;&#22686;&#24378;&#23545;&#22797;&#26434;&#21518;&#39564;&#20998;&#24067;&#30340;&#20272;&#35745;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;VIS&#26041;&#27861;&#22312;&#22810;&#31181;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#23545;&#25968;&#20284;&#28982;&#21644;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2311.02516</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#21521;$\chi^2$&#25955;&#24230;&#30340;&#21464;&#20998;&#37325;&#35201;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forward $\chi^2$ Divergence Based Variational Importance Sampling
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.02516
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#21521;$\chi^2$&#25955;&#24230;&#30340;&#21464;&#20998;&#37325;&#35201;&#25277;&#26679;&#26041;&#27861;(VIS)&#65292;&#36890;&#36807;&#30452;&#25509;&#20272;&#35745;&#21644;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#26469;&#22686;&#24378;&#23545;&#22797;&#26434;&#21518;&#39564;&#20998;&#24067;&#30340;&#20272;&#35745;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;VIS&#26041;&#27861;&#22312;&#22810;&#31181;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#23545;&#25968;&#20284;&#28982;&#21644;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#26159;&#23398;&#20064;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#32780;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#26159;&#30446;&#21069;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22797;&#26434;&#30340;&#21518;&#39564;&#20998;&#24067;&#26102;&#65292;VI&#22312;&#23454;&#29616;&#39640;&#23545;&#25968;&#20284;&#28982;&#26041;&#38754;&#21487;&#33021;&#36935;&#21040;&#25361;&#25112;&#12290;&#38024;&#23545;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21464;&#20998;&#37325;&#35201;&#25277;&#26679;&#65288;VIS&#65289;&#26041;&#27861;&#65292;&#30452;&#25509;&#20272;&#35745;&#21644;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#12290;VIS&#21033;&#29992;&#36890;&#36807;&#26368;&#23567;&#21270;&#21069;&#21521;$\chi^2$&#25955;&#24230;&#23454;&#29616;&#30340;&#26368;&#20339;&#25552;&#35758;&#20998;&#24067;&#26469;&#22686;&#24378;&#23545;&#25968;&#20284;&#28982;&#20272;&#35745;&#12290;&#25105;&#20204;&#23558;VIS&#24212;&#29992;&#20110;&#22810;&#31181;&#27969;&#34892;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#21253;&#25324;&#28151;&#21512;&#27169;&#22411;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#37096;&#20998;&#35266;&#27979;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#25968;&#20284;&#28982;&#21644;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. In response to this limitation, we introduce a novel variational importance sampling (VIS) approach that directly estimates and maximizes the log-likelihood. VIS leverages the optimal proposal distribution, achieved by minimizing the forward $\chi^2$ divergence, to enhance log-likelihood estimation. We apply VIS to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. Results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Brenier&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#30340;&#38750;&#32447;&#24615;&#28388;&#27874;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#20808;&#39564;&#20998;&#24067;&#21040;&#21518;&#39564;&#20998;&#24067;&#30340;&#26144;&#23556;&#26469;&#36991;&#20813;&#26435;&#37325;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22797;&#26434;&#20998;&#24067;&#21644;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2310.13886</link><description>&lt;p&gt;
Brenier&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#19979;&#30340;&#38750;&#32447;&#24615;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Filtering with Brenier Optimal Transport Maps
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2310.13886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Brenier&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#30340;&#38750;&#32447;&#24615;&#28388;&#27874;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#20808;&#39564;&#20998;&#24067;&#21040;&#21518;&#39564;&#20998;&#24067;&#30340;&#26144;&#23556;&#26469;&#36991;&#20813;&#26435;&#37325;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22797;&#26434;&#20998;&#24067;&#21644;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#38750;&#32447;&#24615;&#28388;&#27874;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#22122;&#22768;&#37096;&#20998;&#35266;&#27979;&#21382;&#21490;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#29366;&#24577;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#20256;&#32479;&#30340;&#24207;&#21015;&#37325;&#35201;&#37325;&#37319;&#26679;&#65288;SIR&#65289;&#31890;&#23376;&#28388;&#27874;&#30001;&#20110;&#26435;&#37325;&#36864;&#21270;&#38382;&#39064;&#65292;&#22312;&#28041;&#21450;&#36864;&#21270;&#20284;&#28982;&#25110;&#39640;&#32500;&#29366;&#24577;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#22522;&#26412;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20272;&#35745;&#20174;&#24403;&#21069;&#20808;&#39564;&#20998;&#24067;&#21040;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;Brenier&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#26144;&#23556;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#19982;SIR&#31890;&#23376;&#28388;&#27874;&#19981;&#21516;&#65292;OT&#26041;&#27861;&#19981;&#38656;&#35201;&#20284;&#28982;&#30340;&#35299;&#26512;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#26469;&#24314;&#27169;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#26469;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#23383;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;OT&#28388;&#27874;&#22120;&#21644;SIR&#31890;&#23376;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the problem of nonlinear filtering, i.e., computing the conditional distribution of the state of a stochastic dynamical system given a history of noisy partial observations. Conventional sequential importance resampling (SIR) particle filters suffer from fundamental limitations, in scenarios involving degenerate likelihoods or high-dimensional states, due to the weight degeneracy issue. In this paper, we explore an alternative method, which is based on estimating the Brenier optimal transport (OT) map from the current prior distribution of the state to the posterior distribution at the next time step. Unlike SIR particle filters, the OT formulation does not require the analytical form of the likelihood. Moreover, it allows us to harness the approximation power of neural networks to model complex and multi-modal distributions and employ stochastic optimization algorithms to enhance scalability. Extensive numerical experiments are presented that compare the O
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#23884;&#20837;&#34920;&#31034;&#21487;&#20197;&#26816;&#27979;&#21040;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#19988;&#22312;&#20256;&#32479;&#30340;&#27867;&#21270;&#24230;&#37327;&#19978;&#34920;&#29616;&#20986;&#20559;&#24046;&#12290;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#30340;&#29305;&#24449;&#23398;&#20064;&#26080;&#27861;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#23545;&#20854;&#34920;&#31034;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#21487;&#33021;&#20248;&#20110;&#25972;&#20307;&#24494;&#35843;&#12290;</title><link>https://rss.arxiv.org/abs/2310.13836</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#23884;&#20837;&#34920;&#31034;&#33021;&#22815;&#26816;&#27979;&#21040;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Foundation Model's Embedded Representations May Detect Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2310.13836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#23884;&#20837;&#34920;&#31034;&#21487;&#20197;&#26816;&#27979;&#21040;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#19988;&#22312;&#20256;&#32479;&#30340;&#27867;&#21270;&#24230;&#37327;&#19978;&#34920;&#29616;&#20986;&#20559;&#24046;&#12290;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#30340;&#29305;&#24449;&#23398;&#20064;&#26080;&#27861;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#23545;&#20854;&#34920;&#31034;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#21487;&#33021;&#20248;&#20110;&#25972;&#20307;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#26679;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20043;&#38388;&#21457;&#29983;&#20998;&#24067;&#20559;&#31227;&#65292;&#20351;&#25105;&#20204;&#38590;&#20197;&#29702;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#37492;&#20110;&#24191;&#27867;&#37319;&#29992;&#24050;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#30340;&#24037;&#20855;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#25105;&#20204;&#20197;Sentiment140&#25968;&#25454;&#38598;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#20026;&#20363;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#35768;&#22810;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#23545;Sentiment140&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#27979;&#35797;&#38598;M&#21644;&#33258;&#21160;&#26631;&#27880;&#30340;&#35757;&#32451;&#38598;P&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#31034;&#65292;&#35777;&#23454;&#20102;&#21457;&#29983;&#20102;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;P&#19978;&#35757;&#32451;&#24182;&#22312;M&#19978;&#35780;&#20272;&#24615;&#33021;&#26159;&#19968;&#31181;&#26377;&#20559;&#24046;&#30340;&#27867;&#21270;&#24230;&#37327;&#12290;&#23545;&#39044;&#35757;&#32451;&#30340;GPT-2&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;P&#20013;&#21487;&#23398;&#24471;&#30340;&#29305;&#24449;&#19981;&#20250;&#25913;&#21892;&#65288;&#20107;&#23454;&#19978;&#20250;&#38459;&#30861;&#65289;&#22312;M&#19978;&#30340;&#24615;&#33021;&#12290;&#23545;&#39044;&#35757;&#32451;&#30340;GPT-2&#30340;&#34920;&#31034;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#26159;&#40065;&#26834;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#32988;&#36807;&#25972;&#20307;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling biases can cause distribution shifts between train and test datasets for supervised learning tasks, obscuring our ability to understand the generalization capacity of a model. This is especially important considering the wide adoption of pre-trained foundational neural networks -- whose behavior remains poorly understood -- for transfer learning (TL) tasks. We present a case study for TL on the Sentiment140 dataset and show that many pre-trained foundation models encode different representations of Sentiment140's manually curated test set $M$ from the automatically labeled training set $P$, confirming that a distribution shift has occurred. We argue training on $P$ and measuring performance on $M$ is a biased measure of generalization. Experiments on pre-trained GPT-2 show that the features learnable from $P$ do not improve (and in fact hamper) performance on $M$. Linear probes on pre-trained GPT-2's representations are robust and may even outperform overall fine-tuning, imply
&lt;/p&gt;</description></item><item><title>&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#20855;&#22791;&#20102;&#29702;&#35299;&#21644;&#35825;&#23548;&#20182;&#20154;&#20135;&#29983;&#38169;&#35823;&#20449;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#30340;&#27450;&#39575;&#22330;&#26223;&#20013;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24471;&#21040;&#22686;&#24378;&#12290;</title><link>https://rss.arxiv.org/abs/2307.16513</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#27450;&#39575;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Deception Abilities Emerged in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2307.16513
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#20855;&#22791;&#20102;&#29702;&#35299;&#21644;&#35825;&#23548;&#20182;&#20154;&#20135;&#29983;&#38169;&#35823;&#20449;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#30340;&#27450;&#39575;&#22330;&#26223;&#20013;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24471;&#21040;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25512;&#29702;&#33021;&#21147;&#30340;&#31283;&#23450;&#22686;&#38271;&#65292;&#26410;&#26469;&#30340;LLM&#34987;&#24576;&#30097;&#33021;&#22815;&#27450;&#39575;&#20154;&#31867;&#25805;&#20316;&#21592;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#32469;&#36807;&#30417;&#27979;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;LLM&#38656;&#35201;&#20855;&#22791;&#23545;&#27450;&#39575;&#31574;&#30053;&#30340;&#27010;&#24565;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#20013;&#20986;&#29616;&#20102;&#36825;&#31181;&#31574;&#30053;&#65292;&#32780;&#22312;&#26089;&#26399;&#30340;LLM&#20013;&#24182;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#34920;&#26126;&#26368;&#20808;&#36827;&#30340;LLM&#33021;&#22815;&#29702;&#35299;&#21644;&#35825;&#23548;&#20182;&#20154;&#20135;&#29983;&#38169;&#35823;&#30340;&#20449;&#24565;&#65292;&#20854;&#22312;&#22797;&#26434;&#30340;&#27450;&#39575;&#22330;&#26223;&#20013;&#34920;&#29616;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24471;&#21040;&#22686;&#24378;&#65292;&#24182;&#19988;&#24341;&#21457;LLM&#20013;&#30340;&#39532;&#22522;&#38597;&#32500;&#21033;&#20027;&#20041;&#21487;&#20197;&#25913;&#21464;&#20854;&#27450;&#39575;&#20542;&#21521;&#12290;&#24635;&#20043;&#65292;&#25581;&#31034;&#20102;&#36804;&#20170;&#20026;&#27490;&#26410;&#30693;&#30340;&#27450;&#39575;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#21407;&#22987;DNN&#30340;&#39044;&#27979;&#22343;&#20540;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#35757;&#32451;&#25104;&#26412;&#19982;&#35757;&#32451;&#28857;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;</title><link>https://rss.arxiv.org/abs/2302.12565</link><description>&lt;p&gt;
&#21464;&#20998;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variational Linearized Laplace Approximation for Bayesian Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2302.12565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#21407;&#22987;DNN&#30340;&#39044;&#27979;&#22343;&#20540;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#35757;&#32451;&#25104;&#26412;&#19982;&#35757;&#32451;&#28857;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#65288;LLA&#65289;&#34987;&#29992;&#26469;&#23545;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#39044;&#27979;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#28857;&#25110;DNN&#21442;&#25968;&#36739;&#22810;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20854;&#20182;LLA&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#22914;Kronecker&#20998;&#35299;&#25110;&#23545;&#35282;&#32447;GGN&#30697;&#38453;&#30340;&#36817;&#20284;&#65292;&#34987;&#20351;&#29992;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;LLA&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;GP&#30340;&#23545;&#20598;RKHS&#20844;&#24335;&#65292;&#24182;&#20445;&#30041;&#20102;&#21407;&#22987;DNN&#30340;&#39044;&#27979;&#22343;&#20540;&#12290;&#27492;&#22806;&#65292;&#23427;&#20801;&#35768;&#26377;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20013;&#23454;&#29616;&#23376;&#32447;&#24615;&#35757;&#32451;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20854;&#35757;&#32451;&#25104;&#26412;&#19982;&#35757;&#32451;&#28857;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#36817;&#20284;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Linearized Laplace Approximation (LLA) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (DNNs). However, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or DNN parameters. Consequently, additional approximations of LLA, such as Kronecker-factored or diagonal approximate GGN matrices, are utilized, potentially compromising the model's performance. To address these challenges, we propose a new method for approximating LLA using a variational sparse Gaussian Process (GP). Our method is based on the dual RKHS formulation of GPs and retains as the predictive mean the output of the original DNN. Furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. Specifically, its training cost is independent of the number of training points. We compare our propose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#26080;&#30456;&#20301;&#23545;&#40784;&#23398;&#20064;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#23618;&#30382;&#23618;&#23618;&#27425;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#21453;&#39304;&#26435;&#37325;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#29289;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#22122;&#22768;&#20316;&#20026;&#39069;&#22806;&#30340;&#20449;&#24687;&#36733;&#20307;&#65292;&#24182;&#21516;&#26102;&#23398;&#20064;&#25152;&#26377;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38169;&#35823;&#20256;&#25773;&#65292;&#24182;&#20445;&#25345;&#29983;&#29289;&#21512;&#29702;&#30340;&#20449;&#21495;&#20256;&#36882;&#21644;&#23398;&#20064;&#12290;</title><link>https://rss.arxiv.org/abs/2212.10249</link><description>&lt;p&gt;
&#23454;&#26102;&#23398;&#20064;&#36328;&#23618;&#32423;&#30382;&#23618;&#23618;&#27425;&#30340;&#39640;&#25928;&#21453;&#21521;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Learning efficient backprojections across cortical hierarchies in real time
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2212.10249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#26080;&#30456;&#20301;&#23545;&#40784;&#23398;&#20064;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#23618;&#30382;&#23618;&#23618;&#27425;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#21453;&#39304;&#26435;&#37325;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#29289;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#22122;&#22768;&#20316;&#20026;&#39069;&#22806;&#30340;&#20449;&#24687;&#36733;&#20307;&#65292;&#24182;&#21516;&#26102;&#23398;&#20064;&#25152;&#26377;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38169;&#35823;&#20256;&#25773;&#65292;&#24182;&#20445;&#25345;&#29983;&#29289;&#21512;&#29702;&#30340;&#20449;&#21495;&#20256;&#36882;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30382;&#23618;&#30340;&#24863;&#30693;&#22788;&#29702;&#21644;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#39640;&#25928;&#22320;&#20026;&#25152;&#26377;&#21306;&#22495;&#30340;&#31361;&#35302;&#20998;&#37197;&#20449;&#29992;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24050;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#65292;&#28982;&#32780;&#36825;&#38656;&#35201;&#20174;&#21069;&#39304;&#21040;&#21453;&#39304;&#36335;&#24452;&#36827;&#34892;&#29983;&#29289;&#19978;&#19981;&#21487;&#34892;&#30340;&#26435;&#37325;&#20256;&#36755;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26080;&#30456;&#20301;&#23545;&#40784;&#23398;&#20064;&#65288;Phaseless Alignment Learning, PAL&#65289;&#65292;&#19968;&#31181;&#22312;&#20998;&#23618;&#30382;&#23618;&#23618;&#27425;&#20013;&#23398;&#20064;&#39640;&#25928;&#21453;&#39304;&#26435;&#37325;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#29983;&#29289;&#29289;&#29702;&#31995;&#32479;&#20013;&#33258;&#28982;&#23384;&#22312;&#30340;&#22122;&#22768;&#20316;&#20026;&#39069;&#22806;&#30340;&#20449;&#24687;&#36733;&#20307;&#26469;&#23454;&#29616;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#25152;&#26377;&#30340;&#26435;&#37325;&#37117;&#26159;&#21516;&#26102;&#23398;&#20064;&#30340;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#31361;&#35302;&#26412;&#22320;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#22987;&#32456;&#24320;&#21551;&#21487;&#22609;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#19981;&#28041;&#21450;&#30456;&#20301;&#65288;&#27809;&#26377;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#25110;&#30456;&#20301;&#23398;&#20064;&#65289;&#65292;&#21487;&#20197;&#22312;&#22810;&#23618;&#30382;&#23618;&#23618;&#27425;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#38169;&#35823;&#20256;&#25773;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#29289;&#21512;&#29702;&#30340;&#20449;&#21495;&#20256;&#36882;&#21644;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#30340;&#39640;&#25928;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of sensory processing and learning in the cortex need to efficiently assign credit to synapses in all areas. In deep learning, a known solution is error backpropagation, which however requires biologically implausible weight transport from feed-forward to feedback paths.   We introduce Phaseless Alignment Learning (PAL), a bio-plausible method to learn efficient feedback weights in layered cortical hierarchies. This is achieved by exploiting the noise naturally found in biophysical systems as an additional carrier of information. In our dynamical system, all weights are learned simultaneously with always-on plasticity and using only information locally available to the synapses. Our method is completely phase-free (no forward and backward passes or phased learning) and allows for efficient error propagation across multi-layer cortical hierarchies, while maintaining biologically plausible signal transport and learning.   Our method is applicable to a wide class of models and impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;Sinkhorn&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;&#65292;&#26174;&#33879;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#31639;&#27861;&#30340;&#21487;&#24494;&#20998;&#24615;&#21644;&#24182;&#34892;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#21644;&#29420;&#31435;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2212.00133</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;Sinkhorn&#31639;&#27861;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Learning of Sinkhorn Algorithm Initializations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2212.00133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;Sinkhorn&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;&#65292;&#26174;&#33879;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#31639;&#27861;&#30340;&#21487;&#24494;&#20998;&#24615;&#21644;&#24182;&#34892;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#21644;&#29420;&#31435;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sinkhorn&#31639;&#27861;&#26159;&#36817;&#20284;&#27714;&#35299;&#31163;&#25955;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#29109;&#27491;&#21017;&#36755;&#36816;&#65288;OT&#65289;&#36317;&#31163;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#31639;&#27861;&#21021;&#22987;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;Sinkhorn&#31639;&#27861;&#30340;&#21487;&#24494;&#20998;&#24615;&#21644;&#24182;&#34892;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#20351;&#29992;&#31532;&#20108;&#20010;&#29983;&#25104;&#32593;&#32476;&#21644;&#33258;&#30417;&#30563;&#24341;&#23548;&#25439;&#22833;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#39044;&#27979;&#32593;&#32476;&#12290;&#39044;&#27979;&#32593;&#32476;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#20219;&#24847;&#22266;&#23450;&#32500;&#24230;&#21644;&#25104;&#26412;&#30340;&#27010;&#29575;&#20998;&#24067;&#23545;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#29983;&#25104;&#32593;&#32476;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#23545;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32593;&#32476;&#21487;&#20197;&#20316;&#20026;&#29420;&#31435;&#30340;OT&#27714;&#35299;&#22120;&#26469;&#36817;&#20284;&#27491;&#21017;&#21270;&#36755;&#36816;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sinkhorn algorithm is the state-of-the-art to approximate solutions of entropic optimal transport (OT) distances between discrete probability distributions. We show that meticulously training a neural network to learn initializations to the algorithm via the entropic OT dual problem can significantly speed up convergence, while maintaining desirable properties of the Sinkhorn algorithm, such as differentiability and parallelizability. We train our predictive network in an adversarial fashion using a second, generating network and a self-supervised bootstrapping loss. The predictive network is universal in the sense that it is able to generalize to any pair of distributions of fixed dimension and cost at inference, and we prove that we can make the generating network universal in the sense that it is capable of producing any pair of distributions during training. Furthermore, we show that our network can even be used as a standalone OT solver to approximate regularized transport dis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#26696;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#25239;&#24615;&#35299;&#37322;&#26041;&#27861;&#65288;MG-CF&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#37325;&#35201;&#30340;&#22270;&#26696;&#26469;&#29983;&#25104;&#30452;&#35266;&#30340;&#35299;&#37322;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2211.04411</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#26696;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#25239;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Motif-guided Time Series Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2211.04411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#26696;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#25239;&#24615;&#35299;&#37322;&#26041;&#27861;&#65288;MG-CF&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#37325;&#35201;&#30340;&#22270;&#26696;&#26469;&#29983;&#25104;&#30452;&#35266;&#30340;&#35299;&#37322;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#26377;&#24517;&#35201;&#22686;&#21152;&#20154;&#31867;&#24037;&#20316;&#37327;&#26469;&#25552;&#20379;&#27169;&#22411;&#20915;&#31574;&#30340;&#19981;&#21516;&#24433;&#21709;&#22240;&#32032;&#30340;&#22810;&#26679;&#21270;&#35299;&#37322;&#12290;&#20026;&#20102;&#25552;&#39640;AI&#31995;&#32479;&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24615;&#65292;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24212;&#36816;&#32780;&#29983;&#12290;XAI&#33539;&#24335;&#20998;&#20026;&#20004;&#22823;&#31867;&#65306;&#29305;&#24449;&#24402;&#22240;&#21644;&#23545;&#25239;&#24615;&#35299;&#37322;&#26041;&#27861;&#12290;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22522;&#20110;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#30340;&#21407;&#22240;&#65292;&#32780;&#23545;&#25239;&#24615;&#35299;&#37322;&#26041;&#27861;&#21457;&#29616;&#26368;&#23567;&#30340;&#36755;&#20837;&#21464;&#21270;&#23558;&#23548;&#33268;&#19981;&#21516;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#22270;&#26696;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#35299;&#37322;&#65292;&#20174;&#32780;&#22312;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#24314;&#31435;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;Motif-Guided Counterfactual Explanation&#65288;MG-CF&#65289;&#65292;&#23427;&#29983;&#25104;&#30452;&#35266;&#30340;&#20107;&#21518;&#23545;&#25239;&#24615;&#35299;&#37322;&#65292;&#20805;&#20998;&#21033;&#29992;&#37325;&#35201;&#30340;&#22270;&#26696;&#25552;&#20379;&#20915;&#31574;&#30340;&#35299;&#37322;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising need of interpretable machine learning methods, there is a necessity for a rise in human effort to provide diverse explanations of the influencing factors of the model decisions. To improve the trust and transparency of AI-based systems, the EXplainable Artificial Intelligence (XAI) field has emerged. The XAI paradigm is bifurcated into two main categories: feature attribution and counterfactual explanation methods. While feature attribution methods are based on explaining the reason behind a model decision, counterfactual explanation methods discover the smallest input changes that will result in a different decision. In this paper, we aim at building trust and transparency in time series models by using motifs to generate counterfactual explanations. We propose Motif-Guided Counterfactual Explanation (MG-CF), a novel model that generates intuitive post-hoc counterfactual explanations that make full use of important motifs to provide interpretive information in decisio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22686;&#24378;&#21830;&#19994;&#27969;&#31243;&#27169;&#25311;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21644;&#25429;&#33719;&#22806;&#37096;&#27963;&#21160;&#24310;&#36831;&#65292;&#25552;&#39640;&#27169;&#25311;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2206.14051</link><description>&lt;p&gt;
&#22686;&#24378;&#21830;&#19994;&#27969;&#31243;&#27169;&#25311;&#27169;&#22411;&#30340;&#22806;&#37096;&#27963;&#21160;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Enhancing Business Process Simulation Models with Extraneous Activity Delays
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2206.14051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22686;&#24378;&#21830;&#19994;&#27969;&#31243;&#27169;&#25311;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21644;&#25429;&#33719;&#22806;&#37096;&#27963;&#21160;&#24310;&#36831;&#65292;&#25552;&#39640;&#27169;&#25311;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#27969;&#31243;&#27169;&#25311;(BPS)&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#23545;&#19994;&#21153;&#27969;&#31243;&#36827;&#34892;&#26356;&#25913;&#23545;&#20854;&#24615;&#33021;&#25351;&#26631;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;BPS&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#22914;&#26524;&#33258;&#21160;&#21270;&#19994;&#21153;&#27969;&#31243;&#20013;&#30340;&#26576;&#20010;&#27963;&#21160;&#25110;&#26576;&#20123;&#36164;&#28304;&#19981;&#21487;&#29992;&#65292;&#35813;&#27969;&#31243;&#30340;&#21608;&#26399;&#26102;&#38388;&#23558;&#26159;&#22810;&#23569;&#12290;BPS&#30340;&#36215;&#28857;&#26159;&#29992;&#27169;&#25311;&#21442;&#25968;&#27880;&#37322;&#30340;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;(BPS&#27169;&#22411;)&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;BPS&#27169;&#22411;&#26159;&#30001;&#24314;&#27169;&#19987;&#23478;&#25163;&#21160;&#35774;&#35745;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#36807;&#31243;&#25366;&#25496;&#25216;&#26415;&#20174;&#20107;&#20214;&#26085;&#24535;&#33258;&#21160;&#21457;&#29616;BPS&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#20010;&#39046;&#22495;&#30340;&#25216;&#26415;&#21482;&#21457;&#29616;&#25429;&#25417;&#30001;&#36164;&#28304;&#20105;&#29992;&#25110;&#36164;&#28304;&#19981;&#21487;&#29992;&#24341;&#36215;&#30340;&#31561;&#24453;&#26102;&#38388;&#30340;BPS&#27169;&#22411;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#19994;&#21153;&#27969;&#31243;&#20013;&#30456;&#24403;&#19968;&#37096;&#20998;&#30340;&#31561;&#24453;&#26102;&#38388;&#23545;&#24212;&#20110;&#22806;&#37096;&#24310;&#36831;&#65292;&#20363;&#22914;&#65292;&#36164;&#28304;&#31561;&#24453;&#23458;&#25143;&#36820;&#22238;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business Process Simulation (BPS) is a common approach to estimate the impact of changes to a business process on its performance measures. For example, it allows us to estimate what would be the cycle time of a process if we automated one of its activities, or if some resources become unavailable. The starting point of BPS is a business process model annotated with simulation parameters (a BPS model). In traditional approaches, BPS models are manually designed by modeling specialists. This approach is time-consuming and error-prone. To address this shortcoming, several studies have proposed methods to automatically discover BPS models from event logs via process mining techniques. However, current techniques in this space discover BPS models that only capture waiting times caused by resource contention or resource unavailability. Oftentimes, a considerable portion of the waiting time in a business process corresponds to extraneous delays, e.g., a resource waits for the customer to ret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#20064;&#30340;&#35270;&#35282;&#23545;&#31616;&#21333;&#20811;&#37324;&#37329;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20013;&#65292;&#32771;&#34385;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2202.07365</link><description>&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#35270;&#35282;&#19979;&#30340;&#31616;&#21333;&#20811;&#37324;&#37329;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Statistical Learning View of Simple Kriging
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2202.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#20064;&#30340;&#35270;&#35282;&#23545;&#31616;&#21333;&#20811;&#37324;&#37329;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20013;&#65292;&#32771;&#34385;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22320;&#29702;&#23450;&#20301;&#20256;&#24863;&#22120;&#30340;&#26222;&#21450;&#65292;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#23637;&#31034;&#21487;&#33021;&#20855;&#26377;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#30340;&#32479;&#35745;&#23398;&#20064;&#30340;&#27010;&#29575;&#29702;&#35770;&#19981;&#30452;&#25509;&#36866;&#29992;&#65292;&#24182;&#19988;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#39044;&#27979;&#35268;&#21017;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#20445;&#35777;&#26377;&#24453;&#24314;&#31435;&#12290;&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#20064;&#30340;&#35270;&#35282;&#23545;&#31616;&#21333;&#20811;&#37324;&#37329;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21363;&#36890;&#36807;&#36827;&#34892;&#38750;&#21442;&#25968;&#26377;&#38480;&#26679;&#26412;&#30340;&#39044;&#27979;&#20998;&#26512;&#12290;&#32473;&#23450;$d\geq 1$&#20010;&#30001;&#26410;&#30693;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#24179;&#26041;&#21487;&#31215;&#38543;&#26426;&#22330;$X=\{X_s\}_{s\in S}$&#22312;$S\subset \mathbb{R}^2$&#20013;&#30340;$s_1,\; \ldots,\; s_d$&#22788;&#30340;&#23454;&#29616;&#20540;&#65292;&#30446;&#26631;&#26159;&#29992;&#26368;&#23567;&#20108;&#27425;&#39118;&#38505;&#39044;&#27979;&#23427;&#22312;$S$&#20013;&#30340;&#20219;&#20309;&#20854;&#20182;&#20301;&#32622;$s\in S$&#19978;&#30340;&#26410;&#30693;&#20540;&#12290;&#39044;&#27979;&#35268;&#21017;&#26159;&#20174;&#35757;&#32451;&#31354;&#38388;&#25968;&#25454;&#38598;&#23548;&#20986;&#30340;&#65306;&#26469;&#33258;&#29420;&#31435;&#20110;&#24453;&#39044;&#27979;&#20301;&#32622;&#30340;&#23454;&#29616;$X'$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the Big Data era, with the ubiquity of geolocation sensors in particular, massive datasets exhibiting a possibly complex spatial dependence structure are becoming increasingly available. In this context, the standard probabilistic theory of statistical learning does not apply directly and guarantees of the generalization capacity of predictive rules learned from such data are left to establish. We analyze here the simple Kriging task from a statistical learning perspective, i.e. by carrying out a nonparametric finite-sample predictive analysis. Given $d\geq 1$ values taken by a realization of a square integrable random field $X=\{X_s\}_{s\in S}$, $S\subset \mathbb{R}^2$, with unknown covariance structure, at sites $s_1,\; \ldots,\; s_d$ in $S$, the goal is to predict the unknown values it takes at any other location $s\in S$ with minimum quadratic risk. The prediction rule being derived from a training spatial dataset: a single realization $X'$ of $X$, independent from those to be p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SinkhornDRL&#26041;&#27861;&#65292;&#20351;&#29992;Sinkhorn&#25955;&#24230;&#26469;&#20943;&#23567;&#24403;&#21069;&#21644;&#30446;&#26631;Bellman&#22238;&#25253;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2202.00769</link><description>&lt;p&gt;
&#20351;&#29992;Sinkhorn&#25955;&#24230;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning by Sinkhorn Divergence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2202.00769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SinkhornDRL&#26041;&#27861;&#65292;&#20351;&#29992;Sinkhorn&#25955;&#24230;&#26469;&#20943;&#23567;&#24403;&#21069;&#21644;&#30446;&#26631;Bellman&#22238;&#25253;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25104;&#21151;&#39640;&#24230;&#20381;&#36182;&#20110;&#20998;&#24067;&#34920;&#31034;&#21644;&#20998;&#24067;&#25955;&#24230;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Sinkhorn&#20998;&#24067;&#24378;&#21270;&#23398;&#20064; (SinkhornDRL)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22238;&#25253;&#20998;&#24067;&#20013;&#23398;&#20064;&#26080;&#38480;&#21046;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#21033;&#29992;Sinkhorn&#25955;&#24230;&#26469;&#20943;&#23567;&#24403;&#21069;&#21644;&#30446;&#26631;Bellman&#22238;&#25253;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20174;&#29702;&#35770;&#19978;&#26469;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SinkhornDRL&#30340;&#25910;&#32553;&#24615;&#36136;&#65292;&#19982;Sinkhorn&#25955;&#24230;&#22312;Wasserstein&#36317;&#31163;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322; (MMD)&#20043;&#38388;&#30340;&#25554;&#20540;&#24615;&#36136;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;Sinkhorn&#25955;&#24230;&#19982;&#24102;&#26377;&#27491;&#21017;&#21270;Moment Matching&#34892;&#20026;&#30340;&#27491;&#21017;&#21270;MMD&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;SinkhornDRL&#30340;&#20248;&#36234;&#24615;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SinkhornDRL&#22312;Atari&#28216;&#25103;&#22871;&#20214;&#19978;&#22987;&#32456;&#34920;&#29616;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#25110;&#21487;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The empirical success of distributional reinforcement learning~(RL) highly depends on the distribution representation and the choice of distribution divergence. In this paper, we propose \textit{Sinkhorn distributional RL~(SinkhornDRL)} that learns unrestricted statistics from return distributions and leverages Sinkhorn divergence to minimize the difference between current and target Bellman return distributions. Theoretically, we prove the contraction properties of SinkhornDRL, consistent with the interpolation nature of Sinkhorn divergence between Wasserstein distance and Maximum Mean Discrepancy~(MMD). We also establish the equivalence between Sinkhorn divergence and a regularized MMD with a regularized Moment Matching behavior, contributing to explaining the superiority of SinkhornDRL. Empirically, we show that SinkhornDRL is consistently better or comparable to existing algorithms on the Atari games suite.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#36820;&#22238;&#27010;&#29575;&#20989;&#25968;&#20998;&#35299;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#20998;&#31867;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#20102;&#20998;&#24067;&#21305;&#37197;&#27491;&#21017;&#21270;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#38544;&#24335;&#22320;&#20248;&#21270;&#31574;&#30053;&#20197;&#23545;&#40784;&#30446;&#26631;&#22238;&#25253;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#20135;&#29983;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25506;&#32034;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2110.03155</link><description>&lt;p&gt;
&#20998;&#31867;&#20998;&#24067;&#24335;&#30340;&#22909;&#22788;&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27491;&#21017;&#21270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Being Categorical Distributional: Uncertainty-aware Regularized Exploration in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2110.03155
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#36820;&#22238;&#27010;&#29575;&#20989;&#25968;&#20998;&#35299;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#20998;&#31867;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#20102;&#20998;&#24067;&#21305;&#37197;&#27491;&#21017;&#21270;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#38544;&#24335;&#22320;&#20248;&#21270;&#31574;&#30053;&#20197;&#23545;&#40784;&#30446;&#26631;&#22238;&#25253;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#20135;&#29983;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25506;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20998;&#31867;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#32463;&#39564;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;RL&#30340;&#29702;&#35770;&#20248;&#21183;&#20173;&#19981;&#26126;&#30830;&#12290;&#20174;&#20998;&#31867;&#20998;&#24067;&#24335;RL&#65288;CDRL&#65289;&#24320;&#22987;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#36820;&#22238;&#27010;&#29575;&#20989;&#25968;&#20998;&#35299;&#25216;&#26415;&#65292;&#23558;&#20998;&#24067;&#24335;RL&#30340;&#28508;&#22312;&#20248;&#21183;&#24402;&#22240;&#20110;&#27966;&#29983;&#30340;&#20998;&#24067;&#21305;&#37197;&#27491;&#21017;&#21270;&#12290;&#36825;&#31181;&#22312;&#20998;&#24067;&#24335;RL&#29615;&#22659;&#20013;&#26410;&#34987;&#25506;&#32034;&#30340;&#27491;&#21017;&#21270;&#26088;&#22312;&#25429;&#25417;&#39069;&#22806;&#30340;&#22238;&#25253;&#20998;&#24067;&#20449;&#24687;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20854;&#26399;&#26395;&#20540;&#65292;&#20174;&#32780;&#20026;&#31574;&#30053;&#20248;&#21270;&#20013;&#30340;&#22686;&#24378;&#22870;&#21169;&#20449;&#21495;&#20316;&#20986;&#36129;&#29486;&#12290;&#19982;&#26126;&#30830;&#20248;&#21270;&#31574;&#30053;&#20197;&#40723;&#21169;&#25506;&#32034;&#30340;&#26368;&#22823;&#29109;RL&#20013;&#30340;&#29109;&#27491;&#21017;&#21270;&#30456;&#27604;&#65292;CDRL&#20013;&#30340;&#32467;&#26524;&#27491;&#21017;&#21270;&#36890;&#36807;&#26032;&#30340;&#22870;&#21169;&#20449;&#21495;&#38544;&#24335;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;&#20854;&#19982;&#30446;&#26631;&#22238;&#25253;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#19968;&#33268;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25506;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical advantages of distributional reinforcement learning~(RL) over classical RL remain elusive despite its remarkable empirical performance. Starting from Categorical Distributional RL~(CDRL), we attribute the potential superiority of distributional RL to a derived distribution-matching regularization by applying a return density function decomposition technique. This unexplored regularization in the distributional RL context is aimed at capturing additional return distribution information regardless of only its expectation, contributing to an augmented reward signal in the policy optimization. Compared with the entropy regularization in MaxEnt RL that explicitly optimizes the policy to encourage the exploration, the resulting regularization in CDRL implicitly optimizes policies guided by the new reward signal to align with the uncertainty of target return distributions, leading to an uncertainty-aware exploration effect. Finally, extensive experiments substantiate the impor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#27604;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#31616;&#21333;&#22635;&#34917;&#35268;&#21017;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#24191;&#27867;&#22635;&#34917;&#26041;&#27861;&#23478;&#26063;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22343;&#20540;&#22635;&#34917;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#20247;&#25968;&#22635;&#34917;&#26159;&#27425;&#20248;&#30340;&#12290;&#23454;&#35777;&#32467;&#26524;&#38500;&#20102;&#25903;&#25345;&#29702;&#35770;&#21457;&#29616;&#22806;&#65292;&#36824;&#24378;&#35843;&#20102;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;</title><link>https://rss.arxiv.org/abs/2104.03158</link><description>&lt;p&gt;
&#39044;&#27979;&#20013;&#32570;&#22833;&#25968;&#25454;&#30340;&#31616;&#21333;&#22635;&#34917;&#35268;&#21017;&#65306;&#29702;&#35770;&#20445;&#35777;&#19982;&#23454;&#35777;&#24615;&#33021;&#30340;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
Simple Imputation Rules for Prediction with Missing Data: Contrasting Theoretical Guarantees with Empirical Performance
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2104.03158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#27604;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#31616;&#21333;&#22635;&#34917;&#35268;&#21017;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#24191;&#27867;&#22635;&#34917;&#26041;&#27861;&#23478;&#26063;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22343;&#20540;&#22635;&#34917;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#20247;&#25968;&#22635;&#34917;&#26159;&#27425;&#20248;&#30340;&#12290;&#23454;&#35777;&#32467;&#26524;&#38500;&#20102;&#25903;&#25345;&#29702;&#35770;&#21457;&#29616;&#22806;&#65292;&#36824;&#24378;&#35843;&#20102;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#26159;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#26469;&#30740;&#31350;&#22635;&#34917;-&#22238;&#24402;&#27969;&#31243;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36866;&#29992;&#20110;&#24191;&#27867;&#22635;&#34917;&#26041;&#27861;&#23478;&#26063;&#30340;&#28176;&#36827;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#24120;&#35782;&#35748;&#20026;&#8220;&#22909;&#8221;&#30340;&#22635;&#34917;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#26159;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39044;&#27979;&#26041;&#38754;&#65292;&#31895;&#31961;&#25968;&#25454;&#20063;&#21487;&#33021;&#26159;&#22909;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#20854;&#20013;&#19968;&#20123;&#32467;&#35770;&#26159;&#65292;&#20247;&#25968;&#22635;&#34917;&#22312;&#28176;&#36827;&#19978;&#26159;&#27425;&#20248;&#30340;&#65292;&#32780;&#22343;&#20540;&#22635;&#34917;&#22312;&#28176;&#36827;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#21512;&#25104;&#12289;&#21322;&#30495;&#23454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35814;&#23613;&#35780;&#20272;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#25910;&#38598;&#30340;&#23454;&#35777;&#35777;&#25454;&#22823;&#22810;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#20294;&#20063;&#24378;&#35843;&#20102;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#65292;&#20197;&#35299;&#20915;MAR&#20551;&#35774;&#30340;&#30456;&#20851;&#24615;&#65292;&#22635;&#34917;&#21644;&#22238;&#24402;&#20219;&#21153;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data is a common issue in real-world datasets. This paper studies the performance of impute-then-regress pipelines by contrasting theoretical and empirical evidence. We establish the asymptotic consistency of such pipelines for a broad family of imputation methods. While common sense suggests that a `good' imputation method produces datasets that are plausible, we show, on the contrary, that, as far as prediction is concerned, crude can be good. Among others, we find that mode-impute is asymptotically sub-optimal, while mean-impute is asymptotically optimal. We then exhaustively assess the validity of these theoretical conclusions on a large corpus of synthetic, semi-real, and real datasets. While the empirical evidence we collect mostly supports our theoretical findings, it also highlights gaps between theory and practice and opportunities for future research, regarding the relevance of the MAR assumption, the complex interdependency between the imputation and regression tasks
&lt;/p&gt;</description></item><item><title>CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;</title><link>https://arxiv.org/abs/2402.00786</link><description>&lt;p&gt;
CroissantLLM: &#19968;&#20010;&#30495;&#27491;&#30340;&#21452;&#35821;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CroissantLLM: A Truly Bilingual French-English Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00786
&lt;/p&gt;
&lt;p&gt;
CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CroissantLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;3T&#20010;&#33521;&#35821;&#21644;&#27861;&#35821;&#26631;&#35760;&#19978;&#39044;&#35757;&#32451;&#30340;13&#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#30740;&#31350;&#21644;&#24037;&#19994;&#31038;&#21306;&#24102;&#26469;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#12289;&#23436;&#20840;&#24320;&#28304;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#26412;&#22320;&#30828;&#20214;&#19978;&#24555;&#36895;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#31181;&#20869;&#22312;&#21452;&#35821;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#27861;&#35821;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#24037;&#31574;&#21010;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; FrenchBench&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#22312;&#27861;&#35821;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#36879;&#26126;&#24230;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20195;&#30721;&#24211;&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#20960;&#21313;&#20010;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#22312;&#36951;&#24536;&#26679;&#26412;&#20013;&#21024;&#38500;&#20449;&#24687;&#19988;&#22312;&#20445;&#30041;&#26679;&#26412;&#19978;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#19979;&#38477;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#26679;&#26412;&#30340;&#21487;&#29992;&#24615;&#65292;&#31526;&#21512;&#25968;&#25454;&#20445;&#30041;&#25919;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.00351</link><description>&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning for Image-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#22312;&#36951;&#24536;&#26679;&#26412;&#20013;&#21024;&#38500;&#20449;&#24687;&#19988;&#22312;&#20445;&#30041;&#26679;&#26412;&#19978;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#19979;&#38477;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#26679;&#26412;&#30340;&#21487;&#29992;&#24615;&#65292;&#31526;&#21512;&#25968;&#25454;&#20445;&#30041;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#27169;&#22411;&#20013;&#26377;&#24847;&#22320;&#36951;&#24536;&#25968;&#25454;&#26679;&#26412;&#20197;&#31526;&#21512;&#20005;&#26684;&#30340;&#35268;&#23450;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36951;&#24536;&#39046;&#22495;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20316;&#20026;&#19968;&#24231;&#26725;&#26753;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#25506;&#35752;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#22312;&#20445;&#30041;&#26679;&#26412;&#19978;&#24615;&#33021;&#19979;&#38477;&#21487;&#24573;&#30053;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#20174;&#36951;&#24536;&#26679;&#26412;&#20013;&#21024;&#38500;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;ImageNet-1K&#21644;Places-365&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#26679;&#26412;&#30340;&#21487;&#29992;&#24615;&#65292;&#36827;&#19968;&#27493;&#31526;&#21512;&#25968;&#25454;&#20445;&#30041;&#25919;&#31574;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#36827;&#34892;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36951;&#24536;&#30740;&#31350;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#27169;&#25311;&#25968;&#23383;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;OTA&#26041;&#26696;&#19978;&#20256;&#26799;&#24230;&#25110;&#32773;&#36890;&#36807;&#27491;&#20132;RB&#20256;&#36755;&#37327;&#21270;&#26799;&#24230;&#30340;&#26041;&#24335;&#35843;&#24230;&#35774;&#22791;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#21463;&#38480;&#20110;&#20449;&#22122;&#27604;&#26368;&#24046;&#35774;&#22791;&#38382;&#39064;&#30340;&#24046;&#24322;&#65292;&#20197;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#21644;&#38477;&#20302;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.00318</link><description>&lt;p&gt;
&#27169;&#25311;&#25968;&#23383;&#35843;&#24230;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Analog-digital Scheduling for Federated Learning: A Communication-Efficient Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#27169;&#25311;&#25968;&#23383;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;OTA&#26041;&#26696;&#19978;&#20256;&#26799;&#24230;&#25110;&#32773;&#36890;&#36807;&#27491;&#20132;RB&#20256;&#36755;&#37327;&#21270;&#26799;&#24230;&#30340;&#26041;&#24335;&#35843;&#24230;&#35774;&#22791;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#21463;&#38480;&#20110;&#20449;&#22122;&#27604;&#26368;&#24046;&#35774;&#22791;&#38382;&#39064;&#30340;&#24046;&#24322;&#65292;&#20197;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#21644;&#38477;&#20302;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#33539;&#24335;&#20013;&#20986;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;OTA&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#20449;&#22122;&#27604;&#26368;&#24046;&#30340;&#35774;&#22791;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#26356;&#26032;&#36895;&#24230;&#24555;&#20294;&#22122;&#22768;&#36739;&#22810;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#25968;&#23383;&#36890;&#36947;&#23558;&#27491;&#20132;&#36164;&#28304;&#22359;&#65288;RB&#65289;&#20998;&#37197;&#32473;&#27599;&#20010;&#35774;&#22791;&#21487;&#20197;&#20943;&#36731;&#22122;&#22768;&#38382;&#39064;&#65292;&#20294;&#20250;&#22686;&#21152;&#36890;&#20449;&#24310;&#36831;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;ADFL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#25311;&#25968;&#23383;FL&#26041;&#26696;&#65306;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#23558;&#27599;&#20010;&#35774;&#22791;&#35843;&#24230;&#20026;&#36890;&#36807;&#27169;&#25311;OTA&#26041;&#26696;&#19978;&#20256;&#20854;&#26799;&#24230;&#65292;&#25110;&#32773;&#20351;&#29992;&#8220;&#25968;&#23383;&#8221;&#26041;&#26696;&#36890;&#36807;&#27491;&#20132;RB&#20256;&#36755;&#20854;&#37327;&#21270;&#26799;&#24230;&#12290;&#38024;&#23545;&#21333;&#20010;FL&#36718;&#27425;&#65292;&#25105;&#20204;&#23558;&#26368;&#20248;&#35843;&#24230;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#23567;&#21270;PS&#20272;&#35745;&#30340;&#20840;&#23616;&#26799;&#24230;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24310;&#36831;&#32422;&#26463;&#19979;&#24471;&#21040;&#26368;&#20248;&#35774;&#22791;&#35843;&#24230;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-the-air (OTA) computation has recently emerged as a communication-efficient Federated Learning (FL) paradigm to train machine learning models over wireless networks. However, its performance is limited by the device with the worst SNR, resulting in fast yet noisy updates. On the other hand, allocating orthogonal resource blocks (RB) to individual devices via digital channels mitigates the noise problem, at the cost of increased communication latency. In this paper, we address this discrepancy and present ADFL, a novel Analog-Digital FL scheme: in each round, the parameter server (PS) schedules each device to either upload its gradient via the analog OTA scheme or transmit its quantized gradient over an orthogonal RB using the ``digital" scheme. Focusing on a single FL round, we cast the optimal scheduling problem as the minimization of the mean squared error (MSE) on the estimated global gradient at the PS, subject to a delay constraint, yielding the optimal device scheduling conf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#33021;&#12289;&#23567;&#22411;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25104;&#21151;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#65292;&#27169;&#22411;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.00306</link><description>&lt;p&gt;
&#19968;&#20010;&#20934;&#30830;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29992;&#20110;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#33021;&#12289;&#23567;&#22411;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25104;&#21151;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#65292;&#27169;&#22411;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;&#26159;&#19968;&#38376;&#28041;&#21450;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#23398;&#31185;&#12290;&#20854;&#24212;&#29992;&#21253;&#25324;&#36164;&#28304;&#20998;&#37197;&#12289;&#26381;&#21153;&#36136;&#37327;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#20132;&#36890;&#31649;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#33021;&#12289;&#23567;&#22411;&#21644;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#39044;&#27979;&#65292;&#21487;&#37096;&#32626;&#22312;&#26222;&#36890;&#22522;&#31449;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#25972;&#20010;&#22478;&#24066;&#30340;&#23436;&#25972;&#20154;&#21592;&#27969;&#21160;&#27169;&#24335;&#36827;&#34892;&#20102;&#19968;&#30334;&#20010;&#36229;&#21442;&#25968;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#31934;&#30830;&#30340;ML&#26550;&#26500;&#65292;&#20854;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;&#26368;&#23569;&#25968;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24179;&#21488;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#24050;&#21457;&#34920;&#30340;ML&#26550;&#26500;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#12290;&#36825;&#23558;&#27169;&#22411;&#21442;&#25968;&#30340;&#24635;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#65292;&#35757;&#32451;&#25152;&#38656;&#30340;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#20869;&#23384;&#37327;&#20063;&#20943;&#23569;&#20102;&#19968;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00045</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting Multimedia Generated by Large AI Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#26032;&#30340;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#30410;&#65292;&#20294;&#36825;&#20123;&#20869;&#23481;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#21253;&#25324;&#28508;&#22312;&#30340;&#28389;&#29992;&#12289;&#31038;&#20250;&#30772;&#22351;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#30001;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#30456;&#20851;&#30740;&#31350;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#21363;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#19987;&#38376;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#20840;&#38754;&#28085;&#30422;&#29616;&#26377;&#30740;&#31350;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20869;&#23481;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#25353;&#23186;&#20307;&#24418;&#24335;&#20998;&#31867;&#65292;&#24182;&#19982;&#32431;&#26816;&#27979;&#65288;&#26088;&#22312;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65289;&#21644;&#24212;&#29992;&#22330;&#26223;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#22343;&#21248;PAC&#20445;&#35777;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21516;&#26102;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12289;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17780</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#22343;&#21248;PAC&#20445;&#35777;&#30340;&#32422;&#26463;MDPs&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#22343;&#21248;PAC&#20445;&#35777;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21516;&#26102;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12289;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#23545;&#20598;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#25506;&#32034;&#30340;&#26368;&#20248;&#31574;&#30053;&#22312;&#28385;&#36275;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#22238;&#25253;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#29616;&#26377;&#30340;&#29702;&#35770;&#25991;&#29486;&#20165;&#25552;&#20379;&#27425;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#26410;&#33021;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#22343;&#21248;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#24615;&#65288;Uniform-PAC&#65289;&#20445;&#35777;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12289;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#20197;&#23454;&#29616;&#20219;&#20309;&#30446;&#26631;&#31934;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26159;&#22312;&#32447;CMDP&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;Uniform-PAC&#31639;&#27861;&#12290;&#38500;&#20102;&#29702;&#35770;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;CMDP&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#25391;&#33633;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a primal-dual reinforcement learning (RL) algorithm for the online constrained Markov decision processes (CMDP) problem, wherein the agent explores an optimal policy that maximizes return while satisfying constraints. Despite its widespread practical use, the existing theoretical literature on primal-dual RL algorithms for this problem only provides sublinear regret guarantees and fails to ensure convergence to optimal policies. In this paper, we introduce a novel policy gradient primal-dual algorithm with uniform probably approximate correctness (Uniform-PAC) guarantees, simultaneously ensuring convergence to optimal policies, sublinear regret, and polynomial sample complexity for any target accuracy. Notably, this represents the first Uniform-PAC algorithm for the online CMDP problem. In addition to the theoretical guarantees, we empirically demonstrate in a simple CMDP that our algorithm converges to optimal policies, while an existing algorithm exhibits oscillatory perform
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22270;&#22810;&#30456;&#20284;&#24615;&#23398;&#20064; (GraphMSL)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#23610;&#24230;&#30340;&#22810;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21253;&#25324;&#33258;&#30456;&#20284;&#24230;&#21644;&#30456;&#23545;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#21270;&#23398;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25928;&#26524;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17615</link><description>&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#22810;&#30456;&#20284;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Multi-Similarity Learning for Molecular Property Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17615
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22270;&#22810;&#30456;&#20284;&#24615;&#23398;&#20064; (GraphMSL)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#23610;&#24230;&#30340;&#22810;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21253;&#25324;&#33258;&#30456;&#20284;&#24230;&#21644;&#30456;&#23545;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#21270;&#23398;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25928;&#26524;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#23545;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26126;&#26174;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#24314;&#31435;&#27491;&#36127;&#23545;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20108;&#36827;&#21046;&#30456;&#20284;&#24615;&#20998;&#31867;&#36807;&#20110;&#31616;&#21270;&#20102;&#22797;&#26434;&#20998;&#23376;&#20851;&#31995;&#30340;&#24615;&#36136;&#65292;&#24182;&#24573;&#35270;&#20102;&#20998;&#23376;&#20043;&#38388;&#30456;&#23545;&#30456;&#20284;&#24615;&#30340;&#31243;&#24230;&#65292;&#32473;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#24615;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#22810;&#30456;&#20284;&#24615;&#23398;&#20064;(GraphMSL)&#26694;&#26550;&#12290;GraphMSL&#23558;&#24191;&#20041;&#22810;&#30456;&#20284;&#24615;&#24230;&#37327;&#34701;&#20837;&#21040;&#36830;&#32493;&#23610;&#24230;&#20013;&#65292;&#21253;&#25324;&#33258;&#30456;&#20284;&#24230;&#21644;&#30456;&#23545;&#30456;&#20284;&#24615;&#12290;&#21333;&#27169;&#22810;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#33258;&#20110;&#21508;&#31181;&#21270;&#23398;&#27169;&#24577;&#65292;&#32780;&#36825;&#20123;&#24230;&#37327;&#30340;&#34701;&#21512;&#24418;&#24335;&#21017;&#26174;&#33879;&#22686;&#24378;&#20102;GraphMSL&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#34701;&#21512;&#30340;&#28789;&#27963;&#24615;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#38656;&#27714;&#65292;&#20351;&#24471;GraphMSL&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#22791;&#26356;&#24191;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective molecular representation learning is essential for molecular property prediction. Contrastive learning, a prominent self-supervised approach for molecular representation learning, relies on establishing positive and negative pairs. However, this binary similarity categorization oversimplifies the nature of complex molecular relationships and overlooks the degree of relative similarities among molecules, posing challenges to the effectiveness and generality of representation learning. In response to this challenge, we propose the Graph Multi-Similarity Learning for Molecular Property Prediction (GraphMSL) framework. GraphMSL incorporates a generalized multi-similarity metric in a continuous scale, capturing self-similarity and relative similarities. The unimodal multi-similarity metrics are derived from various chemical modalities, and the fusion of these metrics into a multimodal form significantly enhances the effectiveness of GraphMSL. In addition, the flexibility of fusion
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#20851;&#20110;&#32593;&#32476;&#30340;&#37325;&#35201;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27963;&#21160;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#22788;&#29702;&#22823;&#37327;&#26410;&#30693;&#21442;&#25968;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#22120;&#21644;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16775</link><description>&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#22823;&#35268;&#27169;&#34928;&#33853;&#12289;&#20449;&#36947;&#32479;&#35745;&#12289;&#22122;&#38899;&#26041;&#24046;&#21644;&#27963;&#21160;&#27010;&#29575;&#30340;&#26080;&#32447;&#21333;&#20803;&#32593;&#32476;&#20013;&#30340;&#22823;&#35268;&#27169;&#36830;&#25509;&#27963;&#21160;&#26816;&#27979;: &#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Activity Detection for Massive Connectivity in Cell-free Networks with Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity Probability: A Bayesian Approach. (arXiv:2401.16775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#20851;&#20110;&#32593;&#32476;&#30340;&#37325;&#35201;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27963;&#21160;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#22788;&#29702;&#22823;&#37327;&#26410;&#30693;&#21442;&#25968;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#22120;&#21644;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#21160;&#26816;&#27979;&#26159;&#19979;&#19968;&#20195;&#20813;&#25480;&#26435;&#22810;&#22336;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#29616;&#26377;&#30340;&#31639;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#38656;&#35201;&#31934;&#30830;&#30340;&#32593;&#32476;&#20449;&#24687;&#65292;&#22914;&#22823;&#35268;&#27169;&#34928;&#33853;&#31995;&#25968;&#12289;&#23567;&#35268;&#27169;&#34928;&#33853;&#20449;&#36947;&#32479;&#35745;&#12289;&#35775;&#38382;&#28857;&#30340;&#22122;&#38899;&#26041;&#24046;&#21644;&#29992;&#25143;&#27963;&#21160;&#27010;&#29575;&#12290;&#33719;&#21462;&#36825;&#20123;&#20449;&#24687;&#23558;&#20250;&#32791;&#36153;&#22823;&#37327;&#24320;&#38144;&#65292;&#24182;&#19988;&#20854;&#20272;&#35745;&#20540;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;&#22312;&#26080;&#32447;&#21333;&#20803;&#32593;&#32476;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#23384;&#22312;&#35768;&#22810;&#36825;&#20123;&#21442;&#25968;&#38656;&#35201;&#33719;&#21462;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22312;&#27809;&#26377;&#19978;&#36848;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#27963;&#21160;&#26816;&#27979;&#38382;&#39064;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20040;&#22810;&#26410;&#30693;&#21442;&#25968;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20854;&#20013;&#26410;&#30693;&#21464;&#37327;&#36171;&#20104;&#20102;&#20808;&#39564;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#36215;&#21040;&#20102;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;&#12290;&#32467;&#21512;&#20284;&#28982;&#20989;&#25968;&#65292;&#20351;&#29992;&#20102;&#26368;&#22823;&#21518;&#39564;&#65288;MAP&#65289;&#20272;&#35745;&#22120;&#21644;&#21464;&#20998;
&lt;/p&gt;
&lt;p&gt;
Activity detection is an important task in the next generation grant-free multiple access. While there are a number of existing algorithms designed for this purpose, they mostly require precise information about the network, such as large-scale fading coefficients, small-scale fading channel statistics, noise variance at the access points, and user activity probability. Acquiring these information would take a significant overhead and their estimated values might not be accurate. This problem is even more severe in cell-free networks as there are many of these parameters to be acquired. Therefore, this paper sets out to investigate the activity detection problem without the above-mentioned information. In order to handle so many unknown parameters, this paper employs the Bayesian approach, where the unknown variables are endowed with prior distributions which effectively act as regularizations. Together with the likelihood function, a maximum a posteriori (MAP) estimator and a variatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20247;&#21253;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#26032;&#39062;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#26681;&#25454;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#20808;&#21069;&#25968;&#37327;&#30340;&#20272;&#35745;&#35843;&#25972;&#26435;&#37325;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#38598;&#20307;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36866;&#24212;&#22797;&#26434;&#27169;&#22411;&#21644;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#35745;&#31639;&#30740;&#31350;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.13239</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20247;&#21253;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Crowdsourcing Via Self-Supervised Learning. (arXiv:2401.13239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20247;&#21253;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#26032;&#39062;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#26681;&#25454;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#20808;&#21069;&#25968;&#37327;&#30340;&#20272;&#35745;&#35843;&#25972;&#26435;&#37325;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#38598;&#20307;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36866;&#24212;&#22797;&#26434;&#27169;&#22411;&#21644;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#35745;&#31639;&#30740;&#31350;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#30340;&#20247;&#21253;&#31995;&#32479;&#36890;&#36807;&#23545;&#20247;&#22810;&#20247;&#21253;&#24037;&#20316;&#32773;&#25552;&#20379;&#30340;&#28508;&#22312;&#21033;&#30410;&#25968;&#37327;&#30340;&#20272;&#35745;&#36827;&#34892;&#24179;&#22343;&#24471;&#21040;&#38598;&#20307;&#20272;&#35745;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#21482;&#39044;&#27979;&#20854;&#20182;&#20154;--&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#21512;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#26681;&#25454;&#20182;&#20204;&#23545;&#20808;&#21069;&#25968;&#37327;&#30340;&#20272;&#35745;&#32473;&#20104;&#20247;&#21253;&#24037;&#20316;&#32773;&#20998;&#37197;&#30340;&#26435;&#37325;&#26469;&#35843;&#25972;&#12290;&#24403;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#25216;&#33021;&#21464;&#21270;&#25110;&#20182;&#20204;&#30340;&#20272;&#35745;&#30456;&#20851;&#26102;&#65292;&#21152;&#26435;&#27714;&#21644;&#27604;&#24179;&#22343;&#27714;&#21644;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#38598;&#20307;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#31639;&#27861;&#65292;&#22914;&#26399;&#26395;&#26368;&#22823;&#21270;&#65292;&#33267;&#23569;&#22312;&#21407;&#21017;&#19978;&#21487;&#20197;&#20135;&#29983;&#31867;&#20284;&#20934;&#30830;&#30340;&#38598;&#20307;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#24403;&#38656;&#35201;&#20351;&#29992;&#35832;&#22914;&#31070;&#32463;&#32593;&#32476;&#20043;&#31867;&#30340;&#22797;&#26434;&#27169;&#22411;&#26469;&#34920;&#31034;&#20247;&#21253;&#24037;&#20316;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#26102;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#38656;&#27714;&#21464;&#24471;&#32321;&#37325;&#12290;&#21482;&#39044;&#27979;&#20854;&#20182;&#20154;&#36866;&#24212;&#20102;&#36825;&#31181;&#22797;&#26434;&#24615;&#20197;&#21450;&#35768;&#22810;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#35745;&#31639;&#30740;&#31350;&#20998;&#26512;&#20102;&#21482;&#39044;&#27979;&#20854;&#20182;&#20154;&#30340;&#21151;&#25928;&#12290;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
Common crowdsourcing systems average estimates of a latent quantity of interest provided by many crowdworkers to produce a group estimate. We develop a new approach -- just-predict-others -- that leverages self-supervised learning and a novel aggregation scheme. This approach adapts weights assigned to crowdworkers based on estimates they provided for previous quantities. When skills vary across crowdworkers or their estimates correlate, the weighted sum offers a more accurate group estimate than the average. Existing algorithms such as expectation maximization can, at least in principle, produce similarly accurate group estimates. However, their computational requirements become onerous when complex models, such as neural networks, are required to express relationships among crowdworkers. Just-predict-others accommodates such complexity as well as many other practical challenges. We analyze the efficacy of just-predict-others through theoretical and computational studies. Among other 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.12425</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#24573;&#35270;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#38646;&#26679;&#26412;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#27010;&#24565;&#19978;&#30340;&#34920;&#29616;&#26497;&#19981;&#22343;&#34913;&#12290;&#20363;&#22914;&#65292;&#23613;&#31649;CLIP&#22312;ImageNet&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65288;72.7&#65285;&#65289;&#65292;&#20294;&#22312;&#21313;&#20010;&#27010;&#24565;&#65288;&#22914;gyromitra&#21644;night snake&#65289;&#19978;&#30340;&#20934;&#30830;&#29575;&#19981;&#21040;10&#65285;&#65292;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#36825;&#20123;&#27010;&#24565;&#22312;VLM&#30340;&#38750;&#22343;&#34913;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#31181;&#19981;&#24179;&#34913;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;VLM&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#35745;&#31639;&#29305;&#23450;&#27010;&#24565;&#30340;&#39057;&#29575;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#27979;&#37327;&#27010;&#24565;&#39057;&#29575;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#24110;&#21161;&#35745;&#31639;&#21253;&#21547;&#32473;&#23450;&#27010;&#24565;&#30340;&#21516;&#20041;&#35789;&#30340;&#30456;&#20851;&#25991;&#26412;&#65292;&#24182;&#35299;&#20915;&#35821;&#35328;&#27495;&#20041;&#12290;&#25105;&#20204;&#30830;&#35748;&#20687;LAION&#36825;&#26679;&#30340;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#30830;&#23454;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#24182;&#19988;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24403;&#20195;&#30340;&#22810;&#27169;&#24335;&#31995;&#32479;&#65292;&#22914;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25991;&#26412;-&#35270;&#35273;&#25512;&#29702;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#38271;&#23614;&#20998;&#24067;&#19979;&#32463;&#24120;&#38590;&#20197;&#36798;&#21040;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $&lt;$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.06683</link><description>&lt;p&gt;
DQNC2S&#65306;&#22522;&#20110;DQN&#30340;&#36328;&#27969;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#19982;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#26816;&#32034;&#19982;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#22312;&#22810;&#27969;&#25968;&#25454;&#30340;&#22266;&#26377;&#20887;&#20313;&#21644;&#22810;&#26597;&#35810;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#26631;&#27880;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#30340;&#22312;&#32447;&#21361;&#26426;&#26102;&#38388;&#36724;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#23454;&#26102;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#20174;&#32780;&#20351;&#25512;&#29702;&#26102;&#38388;&#19982;&#36755;&#20837;&#26597;&#35810;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;&#20887;&#20313;&#36807;&#28388;&#22120;&#34701;&#20837;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#36328;&#27969;&#20869;&#23481;&#37325;&#21472;&#12290;&#22312;CrisisFACTS 2022&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#36798;&#21040;&#30340;ROUGE&#21644;BERTScore&#32467;&#26524;&#20248;&#20110;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&amp;Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#20449;&#24687;&#30340;&#20114;&#21160;&#22522;&#30784;&#23398;&#20064;&#65288;VI-IGL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24378;&#21046;&#25191;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#35299;&#30721;&#22120;&#26469;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;-&#21160;&#20316;&#65288;X&#65292;A&#65289;&#21644;&#21453;&#39304;&#21464;&#37327;Y&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.05015</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#26041;&#27861;&#22312;&#22522;&#20110;&#20114;&#21160;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An Information Theoretic Approach to Interaction-Grounded Learning. (arXiv:2401.05015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#20449;&#24687;&#30340;&#20114;&#21160;&#22522;&#30784;&#23398;&#20064;&#65288;VI-IGL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24378;&#21046;&#25191;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#35299;&#30721;&#22120;&#26469;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;-&#21160;&#20316;&#65288;X&#65292;A&#65289;&#21644;&#21453;&#39304;&#21464;&#37327;Y&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20960;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#23398;&#20064;&#32773;&#35797;&#22270;&#20174;&#19968;&#20123;&#21453;&#39304;&#21464;&#37327;&#20013;&#25512;&#26029;&#20986;&#26410;&#35266;&#27979;&#21040;&#30340;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;&#20114;&#21160;&#22522;&#30784;&#23398;&#20064;&#65288;IGL&#65289;&#26159;&#36825;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#19968;&#20010;&#20363;&#23376;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#25512;&#26029;&#20986;&#28508;&#22312;&#30340;&#20108;&#20540;&#22870;&#21169;&#26469;&#20248;&#21270;&#36820;&#22238;&#12290;&#22312;IGL&#35774;&#32622;&#20013;&#65292;RL&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#19968;&#20010;&#30456;&#20851;&#20551;&#35774;&#26159;&#65292;&#22312;&#32473;&#23450;&#28508;&#22312;&#22870;&#21169;R&#30340;&#24773;&#20917;&#19979;&#65292;&#21453;&#39304;&#21464;&#37327;Y&#22312;&#19978;&#19979;&#25991;-&#21160;&#20316;&#65288;X&#65292;A&#65289;&#19978;&#26159;&#26465;&#20214;&#29420;&#31435;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#20449;&#24687;&#30340;IGL&#65288;VI-IGL&#65289;&#26041;&#27861;&#65292;&#20197;&#24212;&#29992;&#20110;IGL&#30340;RL&#38382;&#39064;&#20013;&#65292;&#20197;&#24378;&#21046;&#25191;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;VI-IGL&#26694;&#26550;&#20351;&#29992;&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#20449;&#24687;&#30446;&#26631;&#26469;&#23398;&#20064;&#22870;&#21169;&#35299;&#30721;&#22120;&#65292;&#35813;&#20449;&#24687;&#30446;&#26631;&#34913;&#37327;&#20102;&#20174;&#29615;&#22659;&#20013;&#35266;&#23519;&#21040;&#30340;&#19978;&#19979;&#25991;-&#21160;&#20316;&#65288;X&#65292;A&#65289;&#21644;&#21453;&#39304;&#21464;&#37327;Y&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) problems where the learner attempts to infer an unobserved reward from some feedback variables have been studied in several recent papers. The setting of Interaction-Grounded Learning (IGL) is an example of such feedback-based reinforcement learning tasks where the learner optimizes the return by inferring latent binary rewards from the interaction with the environment. In the IGL setting, a relevant assumption used in the RL literature is that the feedback variable $Y$ is conditionally independent of the context-action $(X,A)$ given the latent reward $R$. In this work, we propose Variational Information-based IGL (VI-IGL) as an information-theoretic method to enforce the conditional independence assumption in the IGL-based RL problem. The VI-IGL framework learns a reward decoder using an information-based objective based on the conditional mutual information (MI) between the context-action $(X,A)$ and the feedback variable $Y$ observed from the environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#20027;&#35201;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;</title><link>http://arxiv.org/abs/2401.02524</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#21512;&#25506;&#32034;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#20027;&#35201;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#26114;&#21644;&#38544;&#31169;&#27861;&#35268;&#30340;&#38480;&#21046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38459;&#30861;&#20102;&#36827;&#23637;&#12290;&#21512;&#25104;&#25968;&#25454;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21457;&#24067;&#30340;&#27169;&#22411;&#36807;&#22810;&#21644;&#26377;&#38480;&#30340;&#32508;&#36848;&#25991;&#29486;&#32473;&#20915;&#31574;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#30830;&#23450;&#20102;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#36235;&#21183;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20026;&#20027;&#35201;&#36235;&#21183;&#65292;&#38500;&#20102;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#29983;&#25104;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#26159;GAN&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#12289;&#36716;&#25442;&#22120;&#21644;RNN&#20063;&#22312;&#31454;&#20105;&#20013;&#12290;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#20849;&#21516;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a surge in the popularity of Machine Learning (ML), applied across diverse domains. However, progress is impeded by the scarcity of training data due to expensive acquisition and privacy legislation. Synthetic data emerges as a solution, but the abundance of released models and limited overview literature pose challenges for decision-making. This work surveys 417 Synthetic Data Generation (SDG) models over the last decade, providing a comprehensive overview of model types, functionality, and improvements. Common attributes are identified, leading to a classification and trend analysis. The findings reveal increased model performance and complexity, with neural network-based approaches prevailing, except for privacy-preserving data generation. Computer vision dominates, with GANs as primary generative models, while diffusion models, transformers, and RNNs compete. Implications from our performance evaluation highlight the scarcity of common metrics and datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#35265;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#27867;&#21270;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#26032;&#23450;&#20041;OoD&#25968;&#25454;&#20197;&#21450;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#20445;&#35777;&#20102;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.16243</link><description>&lt;p&gt;
&#26159;&#21542;&#25152;&#26377;&#26410;&#35265;&#25968;&#25454;&#37117;&#26159;OoD&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are All Unseen Data Out-of-Distribution?. (arXiv:2312.16243v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#35265;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#27867;&#21270;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#26032;&#23450;&#20041;OoD&#25968;&#25454;&#20197;&#21450;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#20445;&#35777;&#20102;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#26410;&#35265;&#25968;&#25454;&#30340;&#20998;&#24067;&#19968;&#30452;&#34987;&#35270;&#20026;&#26159;OoD&#65288;out-of-distribution&#65289;&#65292;&#20351;&#20854;&#27867;&#21270;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#24456;&#22810;&#35777;&#25454;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#22686;&#21152;&#21487;&#20197;&#21333;&#35843;&#22320;&#38477;&#20302;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#20174;&#20854;&#20182;&#35266;&#23519;&#21644;&#20998;&#26512;&#26469;&#30475;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#65292;&#24182;&#19988;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#20998;&#24067;&#28418;&#31227;&#26102;&#65292;&#19981;&#26159;&#25152;&#26377;&#30340;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#35823;&#24046;&#37117;&#20250;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#21333;&#35843;&#20943;&#23567;&#12290;&#22312;&#32447;&#24615;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#20102;&#36825;&#31181;&#38750;&#21333;&#35843;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#22312;&#19981;&#21516;&#35270;&#35273;&#22522;&#20934;&#19978;&#36827;&#34892;&#32463;&#39564;&#35777;&#23454;&#12290;&#37492;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;OoD&#25968;&#25454;&#65292;&#23558;&#20854;&#35270;&#20026;&#35757;&#32451;&#22495;&#30340;&#20984;&#21253;&#20043;&#22806;&#30340;&#25968;&#25454;&#65292;&#24182;&#22522;&#20110;&#36825;&#20010;&#26032;&#23450;&#20041;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#12290;&#23427;&#24847;&#21619;&#30528;&#23545;&#20110;&#22312;&#35757;&#32451;&#38454;&#27573;&#27809;&#26377;&#35265;&#36807;&#30340;&#25968;&#25454;&#65292;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21487;&#20197;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributions of unseen data have been all treated as out-of-distribution (OOD), making their generalization a significant challenge. Much evidence suggests that the size increase of training data can monotonically decrease generalization errors in test data. However, this is not true from other observations and analysis. In particular, when the training data have multiple source domains and the test data contain distribution drifts, then not all generalization errors on the test data decrease monotonically with the increasing size of training data. Such a non-decreasing phenomenon is formally investigated under a linear setting with empirical verification across varying visual benchmarks. Motivated by these results, we redefine the OOD data as a type of data outside the convex hull of the training domains and prove a new generalization bound based on this new definition. It implies that the effectiveness of a well-trained model can be guaranteed for the unseen data that is within the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.07586</link><description>&lt;p&gt;
&#29305;&#24449;&#24341;&#23548;&#65306;&#22823;&#23610;&#24230;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30340;&#23548;&#24341;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPM)&#32447;&#24615;&#22320;&#23558;&#19981;&#21516;&#30340;&#26465;&#20214;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#20379;&#23545;&#26679;&#26412;&#30340;&#22686;&#24378;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#24403;&#23548;&#21521;&#23610;&#24230;&#21464;&#22823;&#26102;&#20135;&#29983;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#65292;&#19968;&#31181;&#37319;&#26679;&#26041;&#27861;&#65292;&#20026;&#26080;&#20998;&#31867;&#22120;&#23548;&#21521;&#30340;DDPM&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;&#12290;&#36825;&#31181;&#26657;&#27491;&#36843;&#20351;&#23548;&#21521;&#30340;DDPM&#36981;&#23432;&#20854;&#24213;&#23618;&#25193;&#25955;&#36807;&#31243;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#26080;&#38656;&#23548;&#25968;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29305;&#24449;&#24341;&#23548;&#22686;&#24378;&#20102;&#23545;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#23545;&#20174;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21040;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#22914;&#30913;&#30456;&#21464;&#30340;&#21508;&#31181;&#24212;&#29992;&#37117;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21435;&#20013;&#24515;&#21270;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#20013;&#30340;&#34394;&#20551;&#36523;&#20221;&#65292;&#20026;&#20998;&#25955;&#27835;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2311.17929</link><description>&lt;p&gt;
&#26032;&#30340;&#22312;&#32447;&#31038;&#21306;&#65306;&#22312;&#21311;&#21517;&#25237;&#31080;&#32593;&#32476;&#19978;&#36827;&#34892;&#22270;&#28145;&#24230;&#23398;&#20064;&#20197;&#35782;&#21035;&#22810;&#20013;&#24515;&#27835;&#29702;&#20013;&#30340;&#34394;&#20551;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance. (arXiv:2311.17929v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21435;&#20013;&#24515;&#21270;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#20013;&#30340;&#34394;&#20551;&#36523;&#20221;&#65292;&#20026;&#20998;&#25955;&#27835;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21435;&#20013;&#24515;&#21270;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#20013;&#25968;&#23383;&#36164;&#20135;&#30340;&#22810;&#20013;&#24515;&#27835;&#29702;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#36523;&#20221;&#65288;sybils&#65289;&#26469;&#35299;&#20915;&#20998;&#25955;&#27835;&#29702;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;DAO&#27835;&#29702;&#25968;&#25454;&#38598;&#65288;snapshot.org&#65289;&#20013;&#35782;&#21035;&#34394;&#20551;&#36523;&#20221;&#30340;&#27963;&#21160;&#12290;&#20855;&#20307;&#22320;&#65292;&#19968;&#20010;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNN&#65289;&#23398;&#20064;&#20102;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#39640;&#32500;&#23884;&#20837;&#26469;&#35782;&#21035;&#22270;&#20013;&#30456;&#20284;&#33410;&#28857;&#30340;&#24555;&#36895;k&#22343;&#20540;&#21521;&#37327;&#32858;&#31867;&#31639;&#27861;&#65288;FAISS&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#34394;&#20551;&#36523;&#20221;&#65292;&#22312;&#25237;&#31080;&#22270;&#20013;&#20943;&#23569;2-5%&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;DAO&#20013;&#30340;sybil&#25239;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#20998;&#25955;&#27835;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#23545;&#26410;&#26469;&#30340;&#25919;&#31574;&#12289;&#30417;&#31649;&#21644;&#27835;&#29702;&#23454;&#36341;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research examines the polycentric governance of digital assets in blockchain-based Decentralized Autonomous Organizations (DAOs). It offers a theoretical framework and addresses a critical challenge facing decentralized governance by developing a method to identify sybils, or spurious identities. The method uses graph deep learning techniques to identify sybil activity in a DAO governance dataset (snapshot.org). Specifically, a Graph Convolutional Neural Network (GCNN) learned voting behaviours and a fast k-means vector clustering algorithm (FAISS) used the high dimensional embeddings to identify similar nodes in a graph. The results reveal that deep learning can effectively identify sybils, reducing the voting graph by 2-5%. This research underscores the importance of sybil resistance in DAOs and offers a novel perspective on decentralized governance, informing future policy, regulation, and governance practices.
&lt;/p&gt;</description></item><item><title>FIKIT&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#23454;&#26102;GPU&#22810;&#20219;&#21153;&#35843;&#24230;&#31574;&#30053;&#65292;&#20855;&#26377;&#20869;&#26680;&#35782;&#21035;&#21151;&#33021;&#65292;&#33021;&#22815;&#22312;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#20869;&#26680;&#38388;&#22635;&#20805;&#31354;&#38386;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.10359</link><description>&lt;p&gt;
FIKIT&#65306;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#23454;&#26102;GPU&#22810;&#20219;&#21153;&#35843;&#24230;&#19982;&#20869;&#26680;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification. (arXiv:2311.10359v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10359
&lt;/p&gt;
&lt;p&gt;
FIKIT&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#23454;&#26102;GPU&#22810;&#20219;&#21153;&#35843;&#24230;&#31574;&#30053;&#65292;&#20855;&#26377;&#20869;&#26680;&#35782;&#21035;&#21151;&#33021;&#65292;&#33021;&#22815;&#22312;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#20869;&#26680;&#38388;&#22635;&#20805;&#31354;&#38386;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24182;&#34892;&#24037;&#20316;&#36127;&#36733;&#65292;&#22914;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12289;&#25512;&#26029;&#21644;&#19968;&#33324;HPC&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;GPU&#35774;&#22791;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#21152;&#36895;&#12290;&#22312;&#20113;&#35745;&#31639;&#38598;&#32676;&#20013;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#20849;&#20139;&#26469;&#25552;&#20379;GPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26159;&#38750;&#24120;&#38656;&#35201;&#30340;&#65292;&#22240;&#20026;&#24635;&#26159;&#26377;&#26356;&#22810;&#30340;&#20219;&#21153;&#35831;&#27714;&#32780;&#19981;&#26159;&#21487;&#29992;&#30340;GPU&#25968;&#37327;&#12290;&#29616;&#26377;&#30340;GPU&#20849;&#20139;&#35299;&#20915;&#26041;&#26696;&#30528;&#37325;&#20110;&#20943;&#23569;&#22810;&#20010;&#20316;&#19994;&#20105;&#22842;&#21333;&#20010;GPU&#26102;&#30340;&#20219;&#21153;&#32423;&#31561;&#24453;&#26102;&#38388;&#25110;&#20219;&#21153;&#32423;&#20999;&#25442;&#25104;&#26412;&#12290;&#36830;&#32493;&#35745;&#31639;&#35831;&#27714;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#20808;&#32423;&#65292;&#23545;&#20110;&#20849;&#20139;GPU&#35774;&#22791;&#65292;&#23545;QoS&#20135;&#29983;&#20102;&#38750;&#23545;&#31216;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#24037;&#20316;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#31181;&#24773;&#20917;&#24102;&#26469;&#30340;&#20869;&#26680;&#32423;&#20248;&#21270;&#26426;&#20250;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#26680;&#32423;&#35843;&#24230;&#31574;&#30053;FIKIT&#65306;&#22635;&#20805;&#20869;&#26680;&#38388;&#31354;&#38386;&#26102;&#38388;&#12290;FIKIT&#21253;&#21547;&#20219;&#21153;&#32423;&#20248;&#20808;&#32423;&#20449;&#24687;&#12289;&#32454;&#31890;&#24230;&#20869;&#26680;&#35782;&#21035;&#21644;&#20869;&#26680;&#27979;&#37327;&#65292;&#20801;&#35768;&#20302;&#20248;&#20808;&#32423;&#20219;&#21153;&#22312;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#20869;&#26680;&#38388;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Highly parallelized workloads like machine learning training, inferences and general HPC tasks are greatly accelerated using GPU devices. In a cloud computing cluster, serving a GPU's computation power through multi-tasks sharing is highly demanded since there are always more task requests than the number of GPU available. Existing GPU sharing solutions focus on reducing task-level waiting time or task-level switching costs when multiple jobs competing for a single GPU. Non-stopped computation requests come with different priorities, having non-symmetric impact on QoS for sharing a GPU device. Existing work missed the kernel-level optimization opportunity brought by this setting. To address this problem, we present a novel kernel-level scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT incorporates task-level priority information, fine-grained kernel identification, and kernel measurement, allowing low priorities task's execution during high priority task's inter-k
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65292;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#12289;&#31574;&#30053;&#27169;&#22411;&#35757;&#32451;&#21644;&#31574;&#30053;&#27169;&#22411;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#23548;&#33268;&#27169;&#22411;&#34892;&#20026;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00168</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#65306;&#23545;&#40784;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;
The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback. (arXiv:2311.00168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00168
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65292;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#12289;&#31574;&#30053;&#27169;&#22411;&#35757;&#32451;&#21644;&#31574;&#30053;&#27169;&#22411;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#23548;&#33268;&#27169;&#22411;&#34892;&#20026;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26131;&#20110;&#25552;&#31034;&#24182;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#26356;&#26377;&#33021;&#21147;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;RLHF&#26680;&#24515;&#26159;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;LLM&#30340;&#26032;&#24037;&#20855;&#21253;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23450;&#24615;&#35757;&#32451;&#30446;&#26631;&#30340;&#25972;&#21512;&#12290;&#22312;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20013;&#65292;&#29992;&#25143;&#20559;&#22909;&#21644;&#19979;&#28216;&#24615;&#33021;&#20043;&#38388;&#30340;&#21305;&#37197;&#23581;&#35797;&#23548;&#33268;&#20102;&#19968;&#20010;&#20248;&#21270;&#26223;&#35266;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#25351;&#26631;&#30475;&#36215;&#26469;&#21487;&#33021;&#26159;&#30456;&#20851;&#30340;&#12290;&#36825;&#31181;&#34920;&#38754;&#19978;&#30340;&#30456;&#20851;&#20851;&#31995;&#21487;&#33021;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#30340;&#34892;&#20026;&#21644;&#8220;&#36807;&#24230;RLHF&#8221;&#30340;&#24773;&#20917;&#12290;&#22312;RLHF&#20013;&#65292;&#30001;&#20110;&#20197;&#19979;&#23376;&#27169;&#22359;&#19981;&#19968;&#33268;&#65292;&#20250;&#20986;&#29616;&#25361;&#25112;&#65306;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#12289;&#31574;&#30053;&#27169;&#22411;&#35757;&#32451;&#21644;&#31574;&#30053;&#27169;&#22411;&#35780;&#20272;&#12290;&#36825;&#31181;&#19981;&#21305;&#37197;&#23548;&#33268;&#27169;&#22411;&#26377;&#26102;&#20250;&#36991;&#20813;&#29992;&#25143;&#35831;&#27714;&#30340;&#34394;&#20551;&#23433;&#20840;&#26631;&#24535;&#65292;&#24456;&#38590;&#24341;&#23548;&#27169;&#22411;&#26397;&#30528;&#39044;&#26399;&#30340;&#29305;&#24449;&#21457;&#23637;&#65292;&#25110;&#32773;&#24635;&#26159;&#20197;&#29305;&#23450;&#30340;&#39118;&#26684;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to prompt and more capable in complex settings. RLHF at its core is providing a new toolkit to optimize LLMs other than next-token prediction, enabling the integration of qualitative training goals. The attempted match between user preferences and downstream performance, which happens in a learned reward model, results in an optimization landscape where training and evaluation metrics can appear correlated. The apparent correlation can lead to unexpected behaviors and stories of "too much RLHF." In RLHF, challenges emerge because the following sub-modules are not consistent with each other: the reward model training, the policy model training, and the policy model evaluation. This mismatch results in models that sometimes avoid user requests for false safety flags, are difficult to steer to an intended characteristic, or always answer in a specific style. As
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18449</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#20855;&#26377;&#38544;&#34255;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Hidden Constraints via Latent Decision Models. (arXiv:2310.18449v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#23588;&#20854;&#22312;&#20844;&#20849;&#25919;&#31574;&#39046;&#22495;&#22914;&#35686;&#23519;&#21010;&#21306;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23450;&#20041;&#21487;&#34892;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#20915;&#31574;&#30340;&#39640;&#32500;&#24230;&#65292;&#20854;&#22312;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38544;&#34255;&#32422;&#26463;&#28508;&#22312;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;HC-LSBO&#65289;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#21407;&#22987;&#20915;&#31574;&#31354;&#38388;&#19982;&#36739;&#20302;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#21452;&#21521;&#26144;&#23556;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;HC-LSBO&#25429;&#25417;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#22266;&#26377;&#30340;&#38544;&#34255;&#32422;&#26463;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#30340;&#21516;&#26102;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#35780;&#20272;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#35268;&#27169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) has emerged as a potent tool for addressing intricate decision-making challenges, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces the Hidden-Constrained Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with a latent decision model. This approach leverages a variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a lower-dimensional latent space. By doing so, HC-LSBO captures the nuances of hidden constraints inherent in public policymaking, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through numerical experiments on both synthetic and real data sets, with a specific focus on large-scal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#20351;&#29992;&#38543;&#26426;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#35823;&#24046;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#38543;&#26426;&#25506;&#32034;&#36991;&#20813;&#20102;&#27599;&#27425;&#36845;&#20195;&#20013;&#38750;&#20984;&#33719;&#21462;&#20989;&#25968;&#30340;&#26114;&#36149;&#20248;&#21270;&#65292;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15351</link><description>&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#38543;&#26426;&#25506;&#32034;&#65306;&#26368;&#20339;&#36951;&#25022;&#21644;&#35745;&#31639;&#25928;&#29575;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency. (arXiv:2310.15351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#20351;&#29992;&#38543;&#26426;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#35823;&#24046;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#38543;&#26426;&#25506;&#32034;&#36991;&#20813;&#20102;&#27599;&#27425;&#36845;&#20195;&#20013;&#38750;&#20984;&#33719;&#21462;&#20989;&#25968;&#30340;&#26114;&#36149;&#20248;&#21270;&#65292;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20063;&#31216;&#20026;&#22522;&#20110;&#26680;&#30340;&#36172;&#21338;&#20248;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20351;&#29992;&#20174;&#20998;&#24067;&#20013;&#38543;&#26426;&#25277;&#26679;&#26469;&#25506;&#32034;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#38543;&#26426;&#25506;&#32034;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#35823;&#24046;&#29575;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#22312;&#26412;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26032;&#22411;&#38598;&#20013;&#36793;&#30028;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25506;&#32034;&#21644;&#39046;&#22495;&#32553;&#23567;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#29615;&#22659;&#19979;&#24314;&#31435;&#20854;&#26368;&#20339;&#36951;&#25022;&#20445;&#35777;&#12290;&#22312;&#26080;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#22635;&#34917;&#20102;&#22312;&#36951;&#25022;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;COLT&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#30001;&#20110;&#38543;&#26426;&#25506;&#32034;&#28040;&#38500;&#20102;&#27599;&#27425;&#36845;&#20195;&#20013;&#36873;&#25321;&#26597;&#35810;&#28857;&#30340;&#38750;&#20984;&#33719;&#21462;&#20989;&#25968;&#30340;&#26114;&#36149;&#20248;&#21270;&#65292;&#25152;&#20197;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20063;&#20855;&#26377;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider Bayesian optimization using Gaussian Process models, also referred to as kernel-based bandit optimization. We study the methodology of exploring the domain using random samples drawn from a distribution. We show that this random exploration approach achieves the optimal error rates. Our analysis is based on novel concentration bounds in an infinite dimensional Hilbert space established in this work, which may be of independent interest. We further develop an algorithm based on random exploration with domain shrinking and establish its order-optimal regret guarantees under both noise-free and noisy settings. In the noise-free setting, our analysis closes the existing gap in regret performance and thereby resolves a COLT open problem. The proposed algorithm also enjoys a computational advantage over prevailing methods due to the random exploration that obviates the expensive optimization of a non-convex acquisition function for choosing the query points at each iteration.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#38543;&#26426;&#26041;&#21521;&#65292;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.14168</link><description>&lt;p&gt;
&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms. (arXiv:2310.14168v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#38543;&#26426;&#26041;&#21521;&#65292;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#21033;&#29992;&#20102;&#33258;&#21160;&#24494;&#20998;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#21363;&#21453;&#21521;&#27169;&#24335;&#24494;&#20998;&#65292;&#25110;&#31216;&#20026;&#21521;&#37327;&#38597;&#21487;&#27604;&#20056;&#31215;(VJP)&#65292;&#25110;&#22312;&#24494;&#20998;&#20960;&#20309;&#30340;&#32972;&#26223;&#19979;&#34987;&#31216;&#20026;&#25289;&#22238;&#36807;&#31243;&#12290;&#26799;&#24230;&#30340;&#35745;&#31639;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#36807;&#27491;&#21521;&#27169;&#24335;AD&#25110;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;(JVP)&#39640;&#25928;&#35745;&#31639;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#36825;&#20123;JVP&#27839;&#30528;&#20174;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#65288;&#20363;&#22914;&#20271;&#21162;&#21033;&#12289;&#27491;&#24577;&#12289;&#32500;&#26684;&#32435;&#12289;&#25289;&#26222;&#25289;&#26031;&#21644;&#22343;&#21248;&#20998;&#24067;&#65289;&#37319;&#26679;&#30340;&#38543;&#26426;&#26041;&#21521;&#35745;&#31639;&#12290;&#26799;&#24230;&#30340;&#35745;&#31639;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#21253;&#25324;&#25910;&#25947;&#36895;&#24230;&#20197;&#21450;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation within neural networks leverages a fundamental element of automatic differentiation, which is referred to as the reverse mode differentiation, or vector Jacobian Product (VJP) or, in the context of differential geometry, known as the pull-back process. The computation of gradient is important as update of neural network parameters is performed using gradient descent method. In this study, we present a genric randomized method, which updates the parameters of neural networks by using directional derivatives of loss functions computed efficiently by using forward mode AD or Jacobian vector Product (JVP). These JVP are computed along the random directions sampled from different probability distributions e.g., Bernoulli, Normal, Wigner, Laplace and Uniform distributions. The computation of gradient is performed during the forward pass of the neural network. We also present a rigorous analysis of the presented methods providing the rate of convergence along with the computat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13164</link><description>&lt;p&gt;
&#20960;&#20046;&#31561;&#21464;&#24615;&#36890;&#36807;&#26446;&#20195;&#25968;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#30456;&#23545;&#20110;&#32676;&#20316;&#29992;&#30340;&#31561;&#21464;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#36171;&#20104;&#19968;&#20010;&#26550;&#26500;&#20855;&#20307;&#30340;&#32676;&#31561;&#21464;&#24615;&#23545;&#27169;&#22411;&#25152;&#26399;&#26395;&#30475;&#21040;&#30340;&#25968;&#25454;&#21464;&#25442;&#31867;&#22411;&#26045;&#21152;&#20102;&#24378;&#22823;&#30340;&#20808;&#39564;&#12290;&#20005;&#26684;&#31561;&#21464;&#27169;&#22411;&#24378;&#21046;&#25191;&#34892;&#23545;&#31216;&#24615;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#36825;&#26679;&#30340;&#20005;&#26684;&#31561;&#21464;&#24615;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25110;&#20165;&#32534;&#30721;&#20102;&#36817;&#20284;&#25110;&#37096;&#20998;&#23545;&#31216;&#24615;&#30340;&#28508;&#22312;&#29289;&#29702;&#23450;&#24459;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20005;&#26684;&#31561;&#21464;&#24615;&#30340;&#20808;&#39564;&#23454;&#38469;&#19978;&#21487;&#33021;&#36807;&#20110;&#24378;&#22823;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#21363;&#20960;&#20046;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#29616;&#26377;&#23450;&#20041;&#19981;&#21516;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#32858;&#21512;&#20998;&#25968;&#65292;&#21457;&#29616;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.11714</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On the Evaluation of Generative Models in Distributed Learning Tasks. (arXiv:2310.11714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#32858;&#21512;&#20998;&#25968;&#65292;&#21457;&#29616;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#23545;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#21333;&#20010;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#24212;&#29992;&#28041;&#21450;&#21040;&#20998;&#24067;&#24335;&#23398;&#20064;&#29615;&#22659;&#65292;&#20363;&#22914;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#30001;&#22810;&#20010;&#23458;&#25143;&#31471;&#25910;&#38598;&#24182;&#20998;&#21457;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20851;&#27880;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#20197;&#19979;&#22522;&#20110;FID&#30340;&#32858;&#21512;&#20998;&#25968;&#65306;1&#65289;FID-avg&#20316;&#20026;&#23458;&#25143;&#31471;&#20010;&#20307;FID&#20998;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;2&#65289;FID-all&#20316;&#20026;&#35757;&#32451;&#27169;&#22411;&#19982;&#21253;&#21547;&#25152;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#38598;&#20307;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;FID&#36317;&#31163;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26681;&#25454;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#32047;&#31215;&#36951;&#25022;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#19978;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#26469;&#32467;&#21512;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.11531</link><description>&lt;p&gt;
&#22312;&#26080;&#38480;&#26102;&#22495;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach. (arXiv:2310.11531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#32047;&#31215;&#36951;&#25022;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#19978;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#26469;&#32467;&#21512;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#23384;&#22312;&#19968;&#20010;&#31163;&#32447;&#25968;&#25454;&#38598;&#26102;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#31163;&#32447;&#25968;&#25454;&#38598;&#26159;&#30001;&#19968;&#20010;&#19987;&#23478;&#29983;&#25104;&#30340;&#65292;&#20294;&#20854;&#33021;&#21147;&#27700;&#24179;&#26410;&#30693;&#65292;&#21363;&#23427;&#19981;&#26159;&#23436;&#32654;&#30340;&#65292;&#20063;&#19981;&#19968;&#23450;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#20351;&#29992;&#30340;&#34892;&#20026;&#31574;&#30053;&#65288;&#30001;&#33021;&#21147;&#21442;&#25968;&#21442;&#25968;&#21270;&#65289;&#65292;&#22312;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#33021;&#21462;&#24471;&#26126;&#26174;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20197; $\tilde{O}(\sqrt{T})$ &#20026;&#32553;&#25918;&#30340;&#31934;&#30830;&#26377;&#29992;PSRL&#31639;&#27861;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#36825;&#38656;&#35201;&#23545;&#36125;&#21494;&#26031;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#26032;&#39062;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;Informed RLSVI&#31639;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#28982;&#21518;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11244</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#26159;&#21028;&#26029;&#20004;&#20010;&#23454;&#20307;&#25551;&#36848;&#26159;&#21542;&#25351;&#30340;&#26159;&#21516;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#23454;&#20307;&#21305;&#37197;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#20013;&#30340;&#26680;&#24515;&#27493;&#39588;&#65292;&#20063;&#26159;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#24212;&#29992;&#38656;&#35201;&#23558;&#26469;&#33258;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#20135;&#21697;&#21305;&#37197;&#36215;&#26469;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23454;&#20307;&#21305;&#37197;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22914;BERT&#25110;RoBERTa&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#20307;&#21305;&#37197;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;i&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;ii&#65289;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23454;&#20307;&#19981;&#22815;&#20581;&#22766;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22522;&#20110;PLMs&#30340;&#21305;&#37197;&#22120;&#30340;&#22791;&#36873;&#26041;&#26696;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;LLMs&#23545;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT3.5&#21644;GPT4&#65292;&#20197;&#21450;&#22522;&#20110;Llama2&#30340;&#24320;&#28304;LLMs&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#36816;&#34892;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.10107</link><description>&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs. (arXiv:2310.10107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#30340;&#23398;&#20064;&#30001;&#20110;&#35266;&#23519;&#25968;&#25454;&#38590;&#20197;&#35299;&#35835;&#32780;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#30693;&#36716;&#31227;&#21644;&#35266;&#27979;&#27169;&#22411;&#30340;POMDPs&#20013;&#30340;&#24207;&#21015;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#21518;&#39564;&#37319;&#26679;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PSRL&#65289;&#22312;POMDPs&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35777;&#26126;&#20854;&#36125;&#21494;&#26031;&#36951;&#25022;&#38543;&#30528;&#24207;&#21015;&#30340;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#32780;&#32553;&#23567;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36951;&#25022;&#38543;&#30528;&#26102;&#38388;&#38271;&#24230;$H$&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#19979;&#30028;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;POMDP&#26159;&#27424;&#23436;&#22791;&#19988;&#24369;&#21487;&#35782;&#21035;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#30456;&#27604;&#20110;arXiv:2204.08967&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#25913;&#36827;&#20102;&#36951;&#25022;&#30028;&#32422;$\Omega(H^2\sqrt{SA})$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#21270;&#23545;&#40784;&#21644;&#25239;&#36951;&#24536;&#30340;&#22810;&#27169;&#24577;&#22359;&#36873;&#25321;&#65292;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#38899;&#35270;&#39057;&#20998;&#24067;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.08204</link><description>&lt;p&gt;
&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#19982;&#25239;&#36951;&#24536;&#30340;&#26412;&#22320;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments. (arXiv:2310.08204v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08204
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#21270;&#23545;&#40784;&#21644;&#25239;&#36951;&#24536;&#30340;&#22810;&#27169;&#24577;&#22359;&#36873;&#25321;&#65292;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#38899;&#35270;&#39057;&#20998;&#24067;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#19981;&#26029;&#22320;&#20174;&#21253;&#21547;&#38899;&#35270;&#39057;&#23545;&#30340;&#35270;&#39057;&#27969;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#21516;&#26102;&#20854;&#20998;&#24067;&#38543;&#30528;&#26102;&#38388;&#19981;&#26029;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#26412;&#22320;&#21270;&#23545;&#40784;&#65306;&#24341;&#20837;&#19968;&#20010;&#23567;&#22411;&#21487;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#39044;&#27979;&#24444;&#27492;&#20043;&#38388;&#33391;&#22909;&#23545;&#40784;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#20196;&#29260;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#21482;&#23398;&#20064;&#20855;&#26377;&#20934;&#30830;&#22810;&#27169;&#24577;&#20851;&#31995;&#30340;&#39640;&#24230;&#30456;&#20851;&#30340;&#38899;&#39057;&#35270;&#35273;&#22359;&#12290;&#65288;2&#65289;&#25239;&#36951;&#24536;&#30340;&#22810;&#27169;&#24577;&#22359;&#36873;&#25321;&#65306;&#27604;&#36739;&#24403;&#21069;&#21644;&#36807;&#21435;&#25968;&#25454;&#23545;&#20043;&#38388;&#27599;&#20010;&#38899;&#39057;&#35270;&#39057;&#22359;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20197;&#20943;&#36731;&#20808;&#21069;&#23398;&#20064;&#30340;&#38899;&#35270;&#39057;&#34920;&#31034;&#30340;&#24847;&#22806;&#28418;&#31227;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;FLAVA&#65288;&#25239;&#36951;&#24536;&#30340;&#26412;&#22320;&#21270;&#38899;&#35270;&#39057;&#23545;&#40784;&#65289;&#22312;&#19968;&#31995;&#21015;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#38899;&#39057;&#21644;&#35270;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a lifelong audio-video masked autoencoder that continually learns the multimodal representations from a video stream containing audio-video pairs, while its distribution continually shifts over time. Specifically, we propose two novel ideas to tackle the problem: (1) Localized Alignment: We introduce a small trainable multimodal encoder that predicts the audio and video tokens that are well-aligned with each other. This allows the model to learn only the highly correlated audiovisual patches with accurate multimodal relationships. (2) Forget-robust multimodal patch selection: We compare the relative importance of each audio-video patch between the current and past data pair to mitigate unintended drift of the previously learned audio-video representations. Our proposed method, FLAVA (Forget-robust Localized Audio-Video Alignment), therefore, captures the complex relationships between the audio and video modalities during training on a sequence of pre-training tasks while all
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;&#26041;&#27861;&#65288;CAST&#65289;&#65292;&#36890;&#36807;&#35268;&#33539;&#20266;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#65292;&#24357;&#34917;&#20102;&#33258;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#24369;&#28857;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06380</link><description>&lt;p&gt;
CAST&#65306;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CAST: Cluster-Aware Self-Training for Tabular Data. (arXiv:2310.06380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;&#26041;&#27861;&#65288;CAST&#65289;&#65292;&#36890;&#36807;&#35268;&#33539;&#20266;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#65292;&#24357;&#34917;&#20102;&#33258;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#24369;&#28857;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#22810;&#21151;&#33021;&#24615;&#32780;&#21463;&#21040;&#21560;&#24341;&#65292;&#28982;&#32780;&#23427;&#23481;&#26131;&#21463;&#21040;&#26377;&#22122;&#38899;&#30340;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;&#20960;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25104;&#21151;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#21066;&#24369;&#20102;&#33258;&#35757;&#32451;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23545;&#33258;&#35757;&#32451;&#31639;&#27861;&#25110;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#29305;&#23450;&#30340;&#20462;&#25913;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#19982;&#22312;&#34920;&#26684;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#19981;&#20860;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#32676;&#38598;&#20551;&#35774;&#65292;&#21363;&#30456;&#20114;&#25509;&#36817;&#30340;&#25968;&#25454;&#26679;&#26412;&#24448;&#24448;&#23646;&#20110;&#21516;&#19968;&#31867;&#12290;&#22312;&#27492;&#20551;&#35774;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;&#65288;CAST&#65289;&#26041;&#27861;&#12290;CAST&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#26222;&#36941;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#36827;&#29616;&#26377;&#30340;&#33258;&#35757;&#32451;&#31639;&#27861;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#24133;&#20462;&#25913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35268;&#33539;&#20102;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#65292;&#21363;&#20266;&#26631;&#31614;&#30340;&#20540;&#65292;&#24378;&#21046;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#23545;&#20266;&#26631;&#31614;&#36827;&#34892;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#35270;&#35282;&#30340;&#32852;&#37030;&#24179;&#22343;&#26041;&#27861;&#22312;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.05495</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#32852;&#37030;&#24179;&#22343;&#22312;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network. (arXiv:2310.05495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#35270;&#35282;&#30340;&#32852;&#37030;&#24179;&#22343;&#26041;&#27861;&#22312;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#26469;&#33258;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#12290;&#22914;&#20170;&#65292;&#30001;&#20110;&#20854;&#21331;&#36234;&#24615;&#33021;&#65292;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;FedAvg&#20013;&#30340;&#39318;&#36873;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#29978;&#33267;&#26159;&#38750;&#20809;&#28369;&#30340;&#12290;&#27492;&#22806;&#65292;FedAvg&#24635;&#26159;&#28041;&#21450;&#22810;&#20010;&#23458;&#25143;&#31471;&#21644;&#26412;&#22320;&#26356;&#26032;&#65292;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#26356;&#26032;&#26041;&#21521;&#12290;&#36825;&#20123;&#23646;&#24615;&#32473;&#20998;&#26512;FedAvg&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#29702;&#35770;&#24050;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#29702;&#35299;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#38750;&#20984;&#38382;&#39064;&#20013;&#30340;&#19968;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#26159;&#29702;&#35770;&#23398;&#31185;&#20013;&#30340;&#32463;&#20856;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#20844;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#23545;&#20110;FedAvg&#30340;&#25910;&#25947;&#24615;&#30446;&#21069;&#36824;&#27809;&#26377;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated averaging (FedAvg) is a widely employed paradigm for collaboratively training models from distributed clients without sharing data. Nowadays, the neural network has achieved remarkable success due to its extraordinary performance, which makes it a preferred choice as the model in FedAvg. However, the optimization problem of the neural network is often non-convex even non-smooth. Furthermore, FedAvg always involves multiple clients and local updates, which results in an inaccurate updating direction. These properties bring difficulties in analyzing the convergence of FedAvg in training neural networks. Recently, neural tangent kernel (NTK) theory has been proposed towards understanding the convergence of first-order methods in tackling the non-convex problem of neural networks. The deep linear neural network is a classical model in theoretical subject due to its simple formulation. Nevertheless, there exists no theoretical result for the convergence of FedAvg in training the d
&lt;/p&gt;</description></item><item><title>&#12298;&#26469;&#33258;&#24425;&#31080;&#31080;&#38598;&#25104;&#30340;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#12299;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#29616;&#35937;&#65292;&#21457;&#29616;&#20854;&#19982;&#24425;&#31080;&#31080;&#38598;&#25104;&#26377;&#20851;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#26032;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.02258</link><description>&lt;p&gt;
&#12298;&#26469;&#33258;&#24425;&#31080;&#31080;&#38598;&#25104;&#30340;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Neural Scaling Law from Lottery Ticket Ensembling. (arXiv:2310.02258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02258
&lt;/p&gt;
&lt;p&gt;
&#12298;&#26469;&#33258;&#24425;&#31080;&#31080;&#38598;&#25104;&#30340;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#12299;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#29616;&#35937;&#65292;&#21457;&#29616;&#20854;&#19982;&#24425;&#31080;&#31080;&#38598;&#25104;&#26377;&#20851;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#26032;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#65288;NSL&#65289;&#25351;&#30340;&#26159;&#27169;&#22411;&#24615;&#33021;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#32780;&#25552;&#39640;&#30340;&#29616;&#35937;&#12290;Sharma&#65286;Kaplan&#20351;&#29992;&#36817;&#20284;&#29702;&#35770;&#20998;&#26512;&#20102;NSL&#65292;&#24182;&#39044;&#27979;&#20102;MSE&#25439;&#22833;&#30340;&#34928;&#20943;&#26041;&#24335;&#20026;$N^{-\alpha}$&#65292;&#20854;&#20013;$\alpha=4/d$&#65292;$N$&#20026;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;$d$&#20026;&#20869;&#22312;&#36755;&#20837;&#32500;&#24230;&#12290;&#23613;&#31649;&#20182;&#20204;&#30340;&#29702;&#35770;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25928;&#26524;&#33391;&#22909;&#65288;&#20363;&#22914;ReLU&#32593;&#32476;&#65289;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#31616;&#21333;&#30340;1D&#38382;&#39064;$y=x^2$&#20013;&#65292;&#34920;&#29616;&#20986;&#20102;&#19982;&#20182;&#20204;&#39044;&#27979;&#19981;&#21516;&#30340;&#32553;&#25918;&#23450;&#24459;&#65288;$\alpha=1$&#32780;&#19981;&#26159;$\alpha=4$&#65289;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#31070;&#32463;&#32593;&#32476;&#24182;&#21457;&#29616;&#26032;&#30340;&#32553;&#25918;&#23450;&#24459;&#28304;&#20110;&#24425;&#31080;&#31080;&#38598;&#25104;&#65306;&#24179;&#22343;&#32780;&#35328;&#65292;&#26356;&#23485;&#30340;&#32593;&#32476;&#26377;&#26356;&#22810;&#30340;&#8220;&#24425;&#31080;&#31080;&#8221;&#65292;&#23427;&#20204;&#34987;&#38598;&#25104;&#26469;&#20943;&#23567;&#36755;&#20986;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26426;&#26800;&#35299;&#37322;&#20197;&#21450;&#23545;&#23427;&#20204;&#36827;&#34892;&#32479;&#35745;&#30740;&#31350;&#26469;&#25903;&#25345;&#38598;&#25104;&#26426;&#21046;&#12290;&#25105;&#20204;&#23558;$N^{-1}$&#30340;&#32553;&#25918;&#23450;&#24459;&#24402;&#22240;&#20110;&#8220;&#24425;&#31080;&#31080;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#8221;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma &amp; Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as $N^{-\alpha}$, $\alpha=4/d$, where $N$ is the number of model parameters, and $d$ is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem $y=x^2$ manifests a different scaling law ($\alpha=1$) from their predictions ($\alpha=4$). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more "lottery tickets", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the $N^{-1}$ scaling law to the "central limit theorem" of lottery tickets. Finally, we discuss its potential implications f
&lt;/p&gt;</description></item><item><title>&#26412;&#25216;&#26415;&#25253;&#21578;&#24635;&#32467;&#20102;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25903;&#25345;&#21307;&#30103;&#19987;&#23478;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#24515;&#35299;&#37322;&#23545;&#20110;&#26816;&#27979;&#21644;&#35299;&#20915;&#28508;&#22312;&#30340;&#25968;&#25454;&#30456;&#20851;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02063</link><description>&lt;p&gt;
&#26469;&#33258;EXMOS&#29992;&#25143;&#30740;&#31350;&#30340;&#32463;&#39564;&#25945;&#35757;&#65306;&#24635;&#32467;&#35780;&#20272;EXMOS&#24179;&#21488;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#25910;&#33719;&#30340;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Lessons Learned from EXMOS User Studies: A Technical Report Summarizing Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform. (arXiv:2310.02063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#24635;&#32467;&#20102;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25903;&#25345;&#21307;&#30103;&#19987;&#23478;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#24515;&#35299;&#37322;&#23545;&#20110;&#26816;&#27979;&#21644;&#35299;&#20915;&#28508;&#22312;&#30340;&#25968;&#25454;&#30456;&#20851;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#39046;&#22495;&#65292;&#35299;&#37322;&#30340;&#25552;&#20379;&#22312;&#35843;&#35797;&#21644;&#22686;&#24378;&#39044;&#27979;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#36741;&#21161;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#20840;&#23616;&#27169;&#22411;&#20013;&#24515;&#21644;&#25968;&#25454;&#20013;&#24515;&#35299;&#37322;&#22312;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#26816;&#27979;&#21644;&#35299;&#20915;&#28508;&#22312;&#25968;&#25454;&#30456;&#20851;&#38382;&#39064;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#30446;&#30340;&#19978;&#30340;&#26377;&#25928;&#24615;&#33267;&#20170;&#23578;&#26410;&#24471;&#21040;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#12290;&#22312;&#36825;&#20221;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#30340;&#20027;&#35201;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#23545;&#22522;&#20110;&#25968;&#25454;&#20013;&#24515;&#21644;&#27169;&#22411;&#20013;&#24515;&#35270;&#35282;&#30340;&#20840;&#23616;&#35299;&#37322;&#22312;&#25903;&#25345;&#21307;&#30103;&#19987;&#23478;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#25968;&#25454;&#37197;&#32622;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#20026;&#20102;&#20174;&#32463;&#39564;&#19978;&#30740;&#31350;&#36825;&#20123;&#21160;&#24577;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#21253;&#25324;&#28041;&#21450;70&#20301;&#21307;&#30103;&#19987;&#23478;&#30340;&#23450;&#37327;&#20998;&#26512;&#21644;&#28041;&#21450;30&#20301;&#21307;&#30103;&#19987;&#23478;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of interactive machine-learning systems, the provision of explanations serves as a vital aid in the processes of debugging and enhancing prediction models. However, the extent to which various global model-centric and data-centric explanations can effectively assist domain experts in detecting and resolving potential data-related issues for the purpose of model improvement has remained largely unexplored. In this technical report, we summarise the key findings of our two user studies. Our research involved a comprehensive examination of the impact of global explanations rooted in both data-centric and model-centric perspectives within systems designed to support healthcare experts in optimising machine learning models through both automated and manual data configurations. To empirically investigate these dynamics, we conducted two user studies, comprising quantitative analysis involving a sample size of 70 healthcare experts and qualitative assessments involving 30 healthc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;NoC&#26550;&#26500;&#20013;&#29616;&#26377;&#21311;&#21517;&#36335;&#30001;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#36335;&#30001;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#20351;&#29992;&#27969;&#37327;&#28151;&#28102;&#25216;&#26415;&#65292;&#21487;&#20197;&#25269;&#24481;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.15687</link><description>&lt;p&gt;
&#25171;&#30772;NoC&#21311;&#21517;&#24615;&#20351;&#29992;&#27969;&#30456;&#20851;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Breaking NoC Anonymity using Flow Correlation Attack. (arXiv:2309.15687v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;NoC&#26550;&#26500;&#20013;&#29616;&#26377;&#21311;&#21517;&#36335;&#30001;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#36335;&#30001;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#20351;&#29992;&#27969;&#37327;&#28151;&#28102;&#25216;&#26415;&#65292;&#21487;&#20197;&#25269;&#24481;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#29255;&#19978;&#20114;&#36830;&#65288;NoC&#65289;&#24191;&#27867;&#29992;&#20316;&#24403;&#20170;&#22810;&#26680;&#29255;&#19978;&#31995;&#32479;&#65288;SoC&#65289;&#35774;&#35745;&#20013;&#30340;&#20869;&#37096;&#36890;&#20449;&#32467;&#26500;&#12290;&#29255;&#19978;&#36890;&#20449;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#21033;&#29992;&#20849;&#20139;&#30340;NoC&#20013;&#30340;&#20219;&#20309;&#28431;&#27934;&#23545;&#25915;&#20987;&#32773;&#26469;&#35828;&#37117;&#26159;&#19968;&#20010;&#23500;&#30719;&#12290;NoC&#23433;&#20840;&#20381;&#36182;&#20110;&#23545;&#21508;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#38450;&#33539;&#25514;&#26045;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;NoC&#26550;&#26500;&#20013;&#29616;&#26377;&#21311;&#21517;&#36335;&#30001;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20316;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#36335;&#30001;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#26159;&#26131;&#21463;&#25915;&#20987;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#20351;&#29992;&#27969;&#37327;&#28151;&#28102;&#25216;&#26415;&#65292;&#21487;&#20197;&#25269;&#24481;&#22522;&#20110;ML&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#12290;&#20351;&#29992;&#23454;&#38469;&#21644;&#21512;&#25104;&#27969;&#37327;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#33021;&#22815;&#25104;&#21151;&#22320;&#23545;&#25239;NoC&#26550;&#26500;&#20013;&#26368;&#20808;&#36827;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#23545;&#20110;&#22810;&#31181;&#27969;&#37327;&#27169;&#24335;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#39640;&#36798;99&#65285;&#65292;&#21516;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network-on-Chip (NoC) is widely used as the internal communication fabric in today's multicore System-on-Chip (SoC) designs. Security of the on-chip communication is crucial because exploiting any vulnerability in shared NoC would be a goldmine for an attacker. NoC security relies on effective countermeasures against diverse attacks. We investigate the security strength of existing anonymous routing protocols in NoC architectures. Specifically, this paper makes two important contributions. We show that the existing anonymous routing is vulnerable to machine learning (ML) based flow correlation attacks on NoCs. We propose a lightweight anonymous routing that use traffic obfuscation techniques which can defend against ML-based flow correlation attacks. Experimental studies using both real and synthetic traffic reveal that our proposed attack is successful against state-of-the-art anonymous routing in NoC architectures with a high accuracy (up to 99%) for diverse traffic patterns, while o
&lt;/p&gt;</description></item><item><title>FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10283</link><description>&lt;p&gt;
FRAMU: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10283
&lt;/p&gt;
&lt;p&gt;
FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#36890;&#36807;&#20801;&#35768;&#20174;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#21024;&#38500;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#65292;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20351;&#29992;&#36807;&#26102;&#30340;&#12289;&#31169;&#26377;&#30340;&#21644;&#26080;&#20851;&#30340;&#25968;&#25454;&#20250;&#24341;&#21457;&#19982;&#38544;&#31169;&#21644;&#27169;&#22411;&#25928;&#29575;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#24433;&#21709;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36951;&#24536;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#36824;&#20250;&#23545;&#25968;&#25454;&#38544;&#31169;&#36896;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;&#65288;FRAMU&#65289;&#12290;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#26159;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#65288;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65289;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;FRAMU&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#12289;&#36951;&#24536;&#36807;&#26102;&#12289;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25903;&#25345;&#27169;&#22411;&#25345;&#32493;&#28436;&#36827;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#30340;&#38480;&#21046;&#65292;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.10186</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#26234;&#33021;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence. (arXiv:2309.10186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#30340;&#38480;&#21046;&#65292;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20197;&#20854;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#24207;&#21015;&#20219;&#21153;&#21644;&#23398;&#20064;&#28508;&#22312;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#32034;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20551;&#35774;&#25968;&#25454;&#31561;&#38388;&#38548;&#26377;&#24207;&#20197;&#21450;&#26080;&#27861;&#20805;&#20998;&#34701;&#20837;&#22270;&#32467;&#26500;&#31561;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GNN&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;GNN&#33021;&#22815;&#26174;&#24335;&#22320;&#23558;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#32435;&#20837;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20363;&#22914;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#20013;&#30340;&#26102;&#24207;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#25552;&#31034;&#36827;&#34892;&#20998;&#21106;&#30340;&#27867;&#21270;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#22120;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#38899;&#39057;-&#35270;&#35273;&#23450;&#20301;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#32534;&#30721;&#22120;&#25552;&#31034;&#35299;&#30721;&#22120;&#33539;&#24335;&#12289;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#38899;&#39057;&#25552;&#31034;&#21644;&#30456;&#20851;&#36866;&#37197;&#22120;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2309.07929</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#38899;&#25552;&#31034;&#36827;&#34892;&#20998;&#21106;&#30340;&#27867;&#21270;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#22120;
&lt;/p&gt;
&lt;p&gt;
Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer. (arXiv:2309.07929v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#25552;&#31034;&#36827;&#34892;&#20998;&#21106;&#30340;&#27867;&#21270;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#22120;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#38899;&#39057;-&#35270;&#35273;&#23450;&#20301;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#32534;&#30721;&#22120;&#25552;&#31034;&#35299;&#30721;&#22120;&#33539;&#24335;&#12289;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#38899;&#39057;&#25552;&#31034;&#21644;&#30456;&#20851;&#36866;&#37197;&#22120;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20174;&#26410;&#21516;&#26102;&#30475;&#21040;&#29289;&#20307;&#21644;&#21548;&#21040;&#20854;&#22768;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#26159;&#21542;&#20173;&#28982;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;&#36755;&#20837;&#38899;&#39057;&#20013;&#23450;&#20301;&#20854;&#35270;&#35273;&#20301;&#32622;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#38899;&#39057;-&#35270;&#35273;&#23450;&#20301;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32534;&#30721;&#22120;&#25552;&#31034;&#35299;&#30721;&#22120;&#30340;&#33539;&#24335;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#32534;&#30721;&#22120;&#34701;&#21512;&#35299;&#30721;&#22120;&#33539;&#24335;&#20174;&#34701;&#21512;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#20013;&#35299;&#30721;&#23450;&#20301;&#20449;&#24687;&#65292;&#25105;&#20204;&#26088;&#22312;&#20511;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#26356;&#22909;&#22320;&#36866;&#24212;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#22659;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#38899;&#39057;&#25552;&#31034;&#65288;SAP&#65289;&#26469;&#24110;&#21161;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20851;&#27880;&#26377;&#22768;&#23545;&#35937;&#65292;&#21516;&#26102;&#20063;&#40723;&#21169;&#35270;&#35273;&#21644;&#38899;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#32553;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30456;&#20851;&#36866;&#37197;&#22120;&#65288;ColA&#65289;&#26469;&#20445;&#25345;&#26368;&#23567;&#30340;&#35757;&#32451;&#24037;&#20316;&#37327;&#24182;&#32500;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Never having seen an object and heard its sound simultaneously, can the model still accurately localize its visual position from the input audio? In this work, we concentrate on the Audio-Visual Localization and Segmentation tasks but under the demanding zero-shot and few-shot scenarios. To achieve this goal, different from existing approaches that mostly employ the encoder-fusion-decoder paradigm to decode localization information from the fused audio-visual feature, we introduce the encoder-prompt-decoder paradigm, aiming to better fit the data scarcity and varying data distribution dilemmas with the help of abundant knowledge from pre-trained models. Specifically, we first propose to construct Semantic-aware Audio Prompt (SAP) to help the visual foundation model focus on sounding objects, meanwhile, the semantic gap between the visual and audio modalities is also encouraged to shrink. Then, we develop a Correlation Adapter (ColA) to keep minimal training efforts as well as maintain 
&lt;/p&gt;</description></item><item><title>MagiCapture&#26159;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#22810;&#27010;&#24565;&#20154;&#20687;&#23450;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#20027;&#39064;&#21644;&#39118;&#26684;&#21442;&#32771;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#23450;&#39118;&#26684;&#20154;&#20687;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.06895</link><description>&lt;p&gt;
MagiCapture: &#39640;&#20998;&#36776;&#29575;&#22810;&#27010;&#24565;&#20154;&#20687;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
MagiCapture: High-Resolution Multi-Concept Portrait Customization. (arXiv:2309.06895v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06895
&lt;/p&gt;
&lt;p&gt;
MagiCapture&#26159;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#22810;&#27010;&#24565;&#20154;&#20687;&#23450;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#20027;&#39064;&#21644;&#39118;&#26684;&#21442;&#32771;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#23450;&#39118;&#26684;&#20154;&#20687;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#21253;&#25324;&#31283;&#23450;&#25193;&#25955;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#36924;&#30495;&#20154;&#20687;&#29031;&#29255;&#12290;&#26377;&#19968;&#20010;&#19987;&#38376;&#30740;&#31350;&#20010;&#24615;&#21270;&#36825;&#20123;&#27169;&#22411;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#20351;&#29992;&#25552;&#20379;&#30340;&#21442;&#32771;&#22270;&#20687;&#38598;&#21512;&#21512;&#25104;&#29305;&#23450;&#20027;&#39064;&#25110;&#39118;&#26684;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#20010;&#24615;&#21270;&#26041;&#27861;&#20135;&#29983;&#30340;&#32467;&#26524;&#20196;&#20154;&#28385;&#24847;&#65292;&#20294;&#20854;&#29983;&#25104;&#30340;&#22270;&#20687;&#24448;&#24448;&#32570;&#20047;&#30495;&#23454;&#24863;&#65292;&#24182;&#19988;&#23578;&#26410;&#36798;&#21040;&#21830;&#19994;&#21487;&#34892;&#30340;&#27700;&#24179;&#12290;&#22312;&#20154;&#20687;&#22270;&#20687;&#29983;&#25104;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#30001;&#20110;&#25105;&#20204;&#20869;&#22312;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;&#20154;&#33080;&#20013;&#30340;&#20219;&#20309;&#19981;&#33258;&#28982;&#30340;&#30165;&#36857;&#37117;&#24456;&#23481;&#26131;&#34987;&#35782;&#21035;&#20986;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MagiCapture&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20027;&#39064;&#21644;&#39118;&#26684;&#27010;&#24565;&#34701;&#21512;&#65292;&#20165;&#20351;&#29992;&#20960;&#20010;&#20027;&#39064;&#21644;&#39118;&#26684;&#21442;&#32771;&#21363;&#21487;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#30340;&#20154;&#20687;&#22270;&#20687;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#20123;&#38543;&#26426;&#30340;&#33258;&#25293;&#29031;&#65292;&#25105;&#20204;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#23601;&#21487;&#20197;&#29983;&#25104;&#29305;&#23450;&#39118;&#26684;&#65288;&#22914;&#25252;&#29031;&#25110;&#20010;&#20154;&#36164;&#26009;&#65289;&#30340;&#39640;&#36136;&#37327;&#20154;&#20687;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11842</link><description>&lt;p&gt;
&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#30028;&#20013;&#35782;&#21035;&#21644;&#20998;&#26512;&#23545;&#31216;&#27169;&#24335;&#24050;&#32463;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#20363;&#22914;&#29289;&#29702;&#23398;&#20013;&#30340;&#24341;&#21147;&#23450;&#24459;&#30340;&#21046;&#23450;&#21644;&#21270;&#23398;&#32467;&#26500;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#21033;&#29992;&#22312;&#26576;&#20123;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#20197;&#21450;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24418;&#24335;&#21270;&#22320;&#25551;&#36848;&#19968;&#31867;&#20855;&#26377;&#19968;&#33324;&#23545;&#31216;&#24615;&#27010;&#24565;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#35813;&#27010;&#24565;&#20801;&#35768;&#23384;&#22312;&#23545;&#31216;&#30340;&#26368;&#20248;&#20540;&#21644;&#31574;&#30053;&#12290;&#21463;&#21040;&#36825;&#20123;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20316;&#20026;&#22810;&#26234;&#20307;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#22312;&#20855;&#26377;&#37325;&#22797;&#23545;&#31216;&#27169;&#24335;&#30340;&#26410;&#35265;&#22330;&#26223;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#31561;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20013;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#36827;&#34892;&#19981;&#24179;&#34913;&#31867;&#21035;&#20013;&#20266;&#36857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26631;&#35760;&#21307;&#30103;&#25968;&#25454;&#38598;&#21644;&#20266;&#36857;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08480</link><description>&lt;p&gt;
&#21033;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20013;&#30340;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#36827;&#34892;&#19981;&#24179;&#34913;&#31867;&#21035;&#20013;&#30340;&#20266;&#36857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals. (arXiv:2308.08480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20013;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#36827;&#34892;&#19981;&#24179;&#34913;&#31867;&#21035;&#20013;&#20266;&#36857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26631;&#35760;&#21307;&#30103;&#25968;&#25454;&#38598;&#21644;&#20266;&#36857;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#30417;&#27979;&#29983;&#21629;&#20307;&#24449;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#36816;&#21160;&#20266;&#36857;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#24179;&#34913;&#31867;&#21035;&#22330;&#26223;&#20013;&#20351;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#22312;PPG&#26679;&#26412;&#20043;&#38388;&#20256;&#25773;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24178;&#20928;&#30340;PPG&#26679;&#26412;&#26126;&#26174;&#23569;&#20110;&#21463;&#20266;&#36857;&#27745;&#26579;&#30340;&#26679;&#26412;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27809;&#26377;&#20266;&#36857;&#30340;&#31867;&#21035;&#20013;&#65292;&#31934;&#30830;&#24230;&#20026;91%&#65292;&#21484;&#22238;&#29575;&#20026;90%&#65292;F1&#24471;&#20998;&#20026;90%&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#26631;&#35760;&#21307;&#30103;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#24178;&#20928;&#26679;&#26412;&#24456;&#23569;&#12290;&#23545;&#20110;&#20266;&#36857;&#30340;&#20998;&#31867;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27604;&#36739;&#20102;&#20256;&#32479;&#20998;&#31867;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;MLP&#12289;Transformers&#12289;FCN&#65289;&#31561;&#26377;&#30417;&#30563;&#20998;&#31867;&#22120;&#19982;&#21322;&#30417;&#30563;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#12290;KNN&#26377;&#30417;&#30563;&#27169;&#22411;&#20855;&#26377;89%&#30340;&#31934;&#30830;&#24230;&#12289;95%&#30340;&#21484;&#22238;&#29575;&#21644;92%&#30340;F1&#24471;&#20998;&#65292;&#32467;&#26524;&#33391;&#22909;&#65292;&#20294;&#21322;&#30417;&#30563;&#31639;&#27861;&#22312;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmogram (PPG) signals are widely used in healthcare for monitoring vital signs, but they are susceptible to motion artifacts that can lead to inaccurate interpretations. In this study, the use of label propagation techniques to propagate labels among PPG samples is explored, particularly in imbalanced class scenarios where clean PPG samples are significantly outnumbered by artifact-contaminated samples. With a precision of 91%, a recall of 90% and an F1 score of 90% for the class without artifacts, the results demonstrate its effectiveness in labeling a medical dataset, even when clean samples are rare. For the classification of artifacts our study compares supervised classifiers such as conventional classifiers and neural networks (MLP, Transformers, FCN) with the semi-supervised label propagation algorithm. With a precision of 89%, a recall of 95% and an F1 score of 92%, the KNN supervised model gives good results, but the semi-supervised algorithm performs better in detec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;&#38646;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24494;&#20998;&#20248;&#21270;&#30340;&#25968;&#23398;&#29305;&#24615;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#30340;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16304</link><description>&lt;p&gt;
&#12298;&#20320;&#19981;&#24471;&#36890;&#36807;&#65306;&#20984;&#20248;&#21270;&#20013;&#30340;&#38646;&#26799;&#24230;&#38382;&#39064;&#12299;
&lt;/p&gt;
&lt;p&gt;
You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization. (arXiv:2307.16304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16304
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;&#38646;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24494;&#20998;&#20248;&#21270;&#30340;&#25968;&#23398;&#29305;&#24615;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#30340;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21644;&#20248;&#21270;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#20915;&#31574;&#27169;&#24335;&#65292;&#23427;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#19982;&#26368;&#23567;&#21270;&#21442;&#25968;&#39044;&#27979;&#35823;&#24046;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#20219;&#21153;&#24615;&#33021;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#20984;&#20248;&#21270;&#39046;&#22495;&#65292;&#39044;&#27979;&#21644;&#20248;&#21270;&#30001;&#20110;&#26368;&#36817;&#24320;&#21457;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#38382;&#39064;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#36804;&#20170;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#32570;&#28857;&#8212;&#8212;&#38646;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#24314;&#35758;&#30340;&#26041;&#27861;&#22522;&#20110;&#24494;&#20998;&#20248;&#21270;&#30340;&#25968;&#23398;&#29305;&#24615;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predict and optimize is an increasingly popular decision-making paradigm that employs machine learning to predict unknown parameters of optimization problems. Instead of minimizing the prediction error of the parameters, it trains predictive models using task performance as a loss function. In the convex optimization domain, predict and optimize has seen significant progress due to recently developed methods for differentiating optimization problem solutions over the problem parameters. This paper identifies a yet unnoticed drawback of this approach -- the zero-gradient problem -- and introduces a method to solve it. The suggested method is based on the mathematical properties of differential optimization and is verified using two real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04870</link><description>&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#65306;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04870
&lt;/p&gt;
&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;(OUA)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;OUA&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#25110;&#24369;&#20449;&#21495;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;&#35813;&#27169;&#22411;&#38750;&#24120;&#36866;&#29992;&#20110;&#27809;&#26377;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#30001;&#24369;&#20449;&#21495;&#25152;&#26500;&#25104;&#30340;&#31354;&#38388;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;OUA&#22312;&#19968;&#33324;&#30340;&#24369;&#20449;&#21495;&#38598;&#21512;&#19979;&#20855;&#26377;&#28508;&#22312;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#65292;OUA&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#25216;&#26415;&#19982;&#20648;&#23618;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#22312;&#28151;&#27788;H\'enon&#26144;&#23556;&#19978;&#23637;&#31034;&#20102;&#25511;&#21046;&#22120;&#23545;&#20110;&#25511;&#21046;&#31995;&#32479;&#30340;&#31283;&#23450;&#12289;&#22266;&#23450;&#28857;&#30340;&#25511;&#21046;&#21644;&#20219;&#24847;&#26399;&#26395;&#29366;&#24577;&#30340;&#25511;&#21046;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21482;&#38656;10&#20010;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#65292;&#21333;&#27425;&#36845;&#20195;&#23601;&#33021;&#25511;&#21046;&#21040;&#26399;&#26395;&#36712;&#36857;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03813</link><description>&lt;p&gt;
&#20351;&#29992;&#19979;&#19968;&#20195;&#20648;&#23618;&#35745;&#31639;&#25511;&#21046;&#28151;&#27788;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Controlling Chaotic Maps using Next-Generation Reservoir Computing. (arXiv:2307.03813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03813
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#25216;&#26415;&#19982;&#20648;&#23618;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#22312;&#28151;&#27788;H\'enon&#26144;&#23556;&#19978;&#23637;&#31034;&#20102;&#25511;&#21046;&#22120;&#23545;&#20110;&#25511;&#21046;&#31995;&#32479;&#30340;&#31283;&#23450;&#12289;&#22266;&#23450;&#28857;&#30340;&#25511;&#21046;&#21644;&#20219;&#24847;&#26399;&#26395;&#29366;&#24577;&#30340;&#25511;&#21046;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21482;&#38656;10&#20010;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#65292;&#21333;&#27425;&#36845;&#20195;&#23601;&#33021;&#25511;&#21046;&#21040;&#26399;&#26395;&#36712;&#36857;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#25216;&#26415;&#19982;&#26368;&#20808;&#36827;&#30340;&#20648;&#23618;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#20648;&#23618;&#35745;&#31639;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#28151;&#27788;H\'enon&#26144;&#23556;&#19978;&#23637;&#31034;&#20102;&#25511;&#21046;&#22120;&#22312;&#19968;&#31995;&#21015;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#22312;&#19981;&#31283;&#23450;&#30340;&#22266;&#23450;&#28857;&#20043;&#38388;&#25511;&#21046;&#31995;&#32479;&#65292;&#23558;&#31995;&#32479;&#31283;&#23450;&#21040;&#26356;&#39640;&#38454;&#21608;&#26399;&#36712;&#36947;&#65292;&#24182;&#25511;&#21046;&#31995;&#32479;&#21040;&#20219;&#24847;&#26399;&#26395;&#29366;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25511;&#21046;&#22120;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#25104;&#21151;&#65292;&#24182;&#19988;&#20165;&#38656;10&#20010;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#36845;&#20195;&#20013;&#23558;&#31995;&#32479;&#25511;&#21046;&#21040;&#26399;&#26395;&#36712;&#36857;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#21644;&#24314;&#27169;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we combine nonlinear system control techniques with next-generation reservoir computing, a best-in-class machine learning approach for predicting the behavior of dynamical systems. We demonstrate the performance of the controller in a series of control tasks for the chaotic H\'enon map, including controlling the system between unstable fixed-points, stabilizing the system to higher order periodic orbits, and to an arbitrary desired state. We show that our controller succeeds in these tasks, requires only 10 data points for training, can control the system to a desired trajectory in a single iteration, and is robust to noise and modeling error.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.01403</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21327;&#35843;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#20294;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#35825;&#23548;&#19968;&#20010;&#26377;&#25928;&#30340;&#20849;&#21516;&#35821;&#35328;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#35270;&#35282;&#65292;&#21363;&#23558;&#26234;&#33021;&#20307;&#20043;&#38388;&#21457;&#36865;&#30340;&#36890;&#20449;&#28040;&#24687;&#35270;&#20026;&#29615;&#22659;&#29366;&#24577;&#30340;&#19981;&#23436;&#25972;&#35270;&#22270;&#12290;&#36890;&#36807;&#26816;&#26597;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#32473;&#23450;&#36712;&#36857;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#20351;&#29992;&#23450;&#24615;&#25351;&#26631;&#21644;&#34920;&#31034;&#25506;&#27979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#35825;&#23548;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#24182;&#20174;&#29615;&#22659;&#20013;&#25429;&#33719;&#20102;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#21147;&#37327;&#20197;&#21450;&#21033;&#29992;&#28040;&#24687;&#20316;&#20026;&#32534;&#30721;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#22270;&#28857;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#26680;&#26469;&#25551;&#36848;&#20107;&#20214;&#20043;&#38388;&#30340;&#35302;&#21457;&#21644;&#25233;&#21046;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11313</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#26680;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep graph kernel point processes. (arXiv:2306.11313v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#22270;&#28857;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#26680;&#26469;&#25551;&#36848;&#20107;&#20214;&#20043;&#38388;&#30340;&#35302;&#21457;&#21644;&#25233;&#21046;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#36807;&#31243;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#20998;&#26512;&#22270;&#20013;&#24322;&#27493;&#20107;&#20214;&#65292;&#21453;&#26144;&#19981;&#21516;&#31867;&#22411;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#26102;&#38388;&#21644;&#31867;&#22411;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#19988;&#22270;&#30340;&#22823;&#23567;&#21644;&#25299;&#25169;&#32467;&#26500;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;&#26368;&#36817;&#30340;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#25581;&#31034;&#20102;&#25429;&#25417;&#22797;&#26434;&#30340;&#20107;&#20214;&#31867;&#21035;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27599;&#20010;&#30446;&#26631;&#20107;&#20214;&#31867;&#22411;&#30340;&#24378;&#24230;&#35745;&#31639;&#20013;&#20351;&#29992;&#20102;&#21253;&#25324;&#25152;&#26377;&#20107;&#20214;&#31867;&#21035;&#22312;&#20869;&#30340;&#26410;&#32463;&#28388;&#27874;&#30340;&#20107;&#20214;&#35760;&#24405;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#22270;&#28857;&#36807;&#31243;&#26041;&#27861;&#12290;&#23545;&#24212;&#30340;&#26080;&#21521;&#22270;&#20855;&#26377;&#20195;&#34920;&#20107;&#20214;&#31867;&#21035;&#30340;&#33410;&#28857;&#21644;&#34920;&#31034;&#28508;&#22312;&#36129;&#29486;&#20851;&#31995;&#30340;&#36793;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#26680;&#26469;&#25551;&#36848;&#20107;&#20214;&#20043;&#38388;&#30340;&#35302;&#21457;&#21644;&#25233;&#21046;&#25928;&#24212;&#12290;&#26412;&#36136;&#24433;&#21709;&#32467;&#26500;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;-based&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#32858;&#21512;&#36827;&#34892;&#20102;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#20855;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point process models are widely used to analyze asynchronous events occurring within a graph that reflect how different types of events influence one another. Predicting future events' times and types is a crucial task, and the size and topology of the graph add to the challenge of the problem. Recent neural point process models unveil the possibility of capturing intricate inter-event-category dependencies. However, such methods utilize an unfiltered history of events, including all event categories in the intensity computation for each target event type. In this work, we propose a graph point process method where event interactions occur based on a latent graph topology. The corresponding undirected graph has nodes representing event categories and edges indicating potential contribution relationships. We then develop a novel deep graph kernel to characterize the triggering and inhibiting effects between events. The intrinsic influence structures are incorporated via the graph neural
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#20998;&#31867;&#24635;&#32467;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25216;&#26415;&#29305;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.09912</link><description>&lt;p&gt;
&#36208;&#21521;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Quantum Federated Learning. (arXiv:2306.09912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09912
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#20998;&#31867;&#24635;&#32467;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25216;&#26415;&#29305;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#20132;&#21449;&#23398;&#31185;&#39046;&#22495;&#65292;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#12290;&#30446;&#21069;&#23578;&#26080;&#20851;&#20110;&#35813;&#20132;&#21449;&#23398;&#31185;&#39046;&#22495;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#23545;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#32454;&#33268;&#30340;&#25506;&#35752;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#29702;&#12289;&#25216;&#26415;&#20197;&#21450;&#26032;&#20852;&#24212;&#29992;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#25972;&#21512;&#36825;&#20123;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#25353;&#20854;&#29305;&#24449;&#21644;&#25152;&#37319;&#29992;&#30340;&#37327;&#23376;&#25216;&#26415;&#20998;&#31867;&#12290;&#38543;&#30528;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#35745;&#23558;&#22312;&#21508;&#20010;&#34892;&#19994;&#23454;&#29616;&#26356;&#22810;&#30340;&#31361;&#30772;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Federated Learning (QFL) is an emerging interdisciplinary field that merges the principles of Quantum Computing (QC) and Federated Learning (FL), with the goal of leveraging quantum technologies to enhance privacy, security, and efficiency in the learning process. Currently, there is no comprehensive survey for this interdisciplinary field. This review offers a thorough, holistic examination of QFL. We aim to provide a comprehensive understanding of the principles, techniques, and emerging applications of QFL. We discuss the current state of research in this rapidly evolving field, identify challenges and opportunities associated with integrating these technologies, and outline future directions and open research questions. We propose a unique taxonomy of QFL techniques, categorized according to their characteristics and the quantum techniques employed. As the field of QFL continues to progress, we can anticipate further breakthroughs and applications across various industries,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-&#24352;&#37327;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#29305;&#24449;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2306.06534</link><description>&lt;p&gt;
K-Tensors&#65306;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-&#24352;&#37327;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#29305;&#24449;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-Tensors&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#23427;&#20204;&#30340;&#29305;&#24449;&#32467;&#26500;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;&#30001;&#20110;&#27491;&#21322;&#23450;&#30697;&#38453;&#21487;&#20197;&#22312; p&#8805;2 &#30340;&#31354;&#38388;&#20013;&#34920;&#31034;&#20026;&#26925;&#29699;&#20307;&#65292;&#22240;&#27492;&#20445;&#25345;&#23427;&#20204;&#30340;&#32467;&#26500;&#20449;&#24687;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#32858;&#31867;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30697;&#38453;&#32858;&#31867;&#31639;&#27861;&#24120;&#24120;&#28041;&#21450;&#23558;&#30697;&#38453;&#21521;&#37327;&#21270;&#65292;&#23548;&#33268;&#20851;&#38190;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21322;&#23450;&#30697;&#38453;&#32467;&#26500;&#20449;&#24687;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#36827;&#34892;&#32858;&#31867;&#12290;&#36825;&#31181;&#36317;&#31163;&#24230;&#37327;&#20351;&#24471;&#32858;&#31867;&#31639;&#27861;&#33021;&#22815;&#32771;&#34385;&#27491;&#21322;&#23450;&#30697;&#38453;&#19982;&#23427;&#20204;&#22312;&#30001;&#19968;&#32452;&#27491;&#21322;&#23450;&#30697;&#38453;&#23450;&#20041;&#30340;&#27491;&#20132;&#21521;&#37327;&#24352;&#25104;&#30340;&#20849;&#21516;&#31354;&#38388;&#19978;&#30340;&#25237;&#24433;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel self-consistency clustering algorithm ($K$-Tensors) designed for {partitioning a distribution of} positive-semidefinite matrices based on their eigenstructures. As positive semi-definite matrices can be represented as ellipsoids in $\Re^p$, $p \ge 2$, it is critical to maintain their structural information to perform effective clustering. However, traditional clustering algorithms {applied to matrices} often {involve vectorization of} the matrices, resulting in a loss of essential structural information. To address this issue, we propose a distance metric {for clustering} that is specifically based on the structural information of positive semi-definite matrices. This distance metric enables the clustering algorithm to consider the differences between positive semi-definite matrices and their projections onto {a} common space spanned by \thadJulyTen{orthonormal vectors defined from a set of} positive semi-definite matrices. This innovative approach to clus
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26144;&#23556;&#30340;&#23884;&#22871;&#35745;&#31639;&#65292;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05366</link><description>&lt;p&gt;
&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ordinal Potential-based Player Rating. (arXiv:2306.05366v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26144;&#23556;&#30340;&#23884;&#22871;&#35745;&#31639;&#65292;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#23545;&#20110;&#20219;&#24847;&#32431;&#31574;&#30053;$x$&#12289;$y$&#21644;$z$&#65292;&#22914;&#26524;$x$&#27604;$y$&#26356;&#22909;&#65292;$y$&#27604;$z$&#26356;&#22909;&#65292;&#21017;$x$&#27604;$z$&#26356;&#22909;&#65292;&#21017;&#20004;&#20010;&#23545;&#31216;&#30340;&#38646;&#21644;&#21338;&#24328;&#26159;&#21487;&#20256;&#36882;&#30340;&#12290;&#26368;&#36817;&#35266;&#23519;&#21040;&#65292;Elo&#35780;&#32423;&#26410;&#33021;&#20445;&#25345;&#31574;&#30053;&#20043;&#38388;&#30340;&#20256;&#36882;&#20851;&#31995;&#65292;&#22240;&#27492;&#19981;&#33021;&#27491;&#30830;&#25552;&#21462;&#28216;&#25103;&#30340;&#20256;&#36882;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#34920;&#26126;&#24403;&#22312;&#27491;&#30830;&#30340;&#31354;&#38388;&#20013;&#35745;&#31639;Elo&#35780;&#32423;&#26102;&#65292;Elo&#35780;&#32423;&#30830;&#23454;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21512;&#36866;&#30340;&#21487;&#36870;&#26144;&#23556;$\varphi$&#23558;&#28216;&#25103;&#24212;&#29992;&#20110;$\varphi$&#65292;&#28982;&#21518;&#35745;&#31639;Elo&#35780;&#32423;&#65292;&#26368;&#21518;&#36890;&#36807;&#24212;&#29992;$\varphi^{-1}$&#22238;&#21040;&#21407;&#22987;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#21487;&#20256;&#36882;&#28216;&#25103;&#30340;&#34920;&#24449;&#20026;&#21183;&#28216;&#25103;&#30340;&#19968;&#20010;&#24369;&#21464;&#20307;&#65292;&#20854;&#21183;&#20989;&#25968;&#26159;&#21152;&#24615;&#21487;&#20998;&#31163;&#30340;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20256;&#36882;&#24207;&#25968;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;&#21487;&#20256;&#36882;&#28216;&#25103;&#30340;&#25910;&#30410;&#36716;&#21270;&#20026;&#20854;&#24046;&#24322;&#25152;&#38656;&#30340;&#26368;&#23567;&#21487;&#36870;&#26144;&#23556;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#27491;&#21017;&#24615;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#21322;&#32447;&#24615;&#26925;&#22278;PDE&#20013;&#30340;Nemytskii&#31639;&#23376;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#36827;&#34892;&#26377;&#20851;PDE&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#26377;&#24456;&#22909;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.05185</link><description>&lt;p&gt;
&#20851;&#20110;&#21322;&#32447;&#24615;&#26925;&#22278;PDE&#20013;&#38750;&#20809;&#28369;&#36229;&#23450;&#31639;&#23376;&#30340;&#35782;&#21035;&#21644;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs. (arXiv:2306.05185v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#27491;&#21017;&#24615;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#21322;&#32447;&#24615;&#26925;&#22278;PDE&#20013;&#30340;Nemytskii&#31639;&#23376;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#36827;&#34892;&#26377;&#20851;PDE&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#26377;&#24456;&#22909;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26080;&#38480;&#32500;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#21322;&#32447;&#24615;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#38750;&#32447;&#24615;&#37096;&#20998;&#20013;&#30340;Nemytskii&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;PDE&#35299;&#19982;&#32473;&#23450;&#30340;&#26399;&#26395;&#29366;&#24577;&#20043;&#38388;&#30340;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#20302;&#27491;&#21017;&#24615;&#24773;&#20917;&#19979;&#32771;&#34385;&#20102;&#36825;&#20010;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24341;&#36215;Nemytskii&#31639;&#23376;&#20989;&#25968;&#20808;&#39564;&#20165;&#34987;&#35748;&#20026;&#26159;H^1_{loc}(\mathbb{R})&#30340;&#20803;&#32032;&#12290;&#36825;&#20351;&#24471;&#30740;&#31350;&#30340;&#38382;&#39064;&#31867;&#25104;&#20026;&#19968;&#31181;&#36866;&#21512;&#20174;&#20005;&#26684;&#20998;&#26512;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#23398;&#20064;&#26377;&#20851;PDE&#30340;&#22521;&#35757;&#38382;&#39064;&#30340;&#20986;&#21457;&#28857;&#65292;&#20854;&#20013;&#26410;&#30693;&#30340;&#36229;&#23450;&#31639;&#23376;&#26159;&#36890;&#36807;&#20351;&#29992;&#38750;&#20809;&#28369;&#28608;&#27963;&#20989;&#25968;(ReLU&#65292;leaky-ReLU&#31561;)&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#25511;&#21046;&#30340;&#27491;&#21017;&#24615;&#36739;&#20302;&#65292;&#20294;&#21487;&#20197;&#20026;&#23616;&#37096;&#26497;&#23567;&#20540;&#23548;&#20986;&#32463;&#20856;&#30340;&#31449;&#28857;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#25237;&#24433;&#27861;&#35299;&#20915;&#25152;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study an infinite-dimensional optimization problem that aims to identify the Nemytskii operator in the nonlinear part of a prototypical semilinear elliptic partial differential equation (PDE) which minimizes the distance between the PDE-solution and a given desired state. In contrast to previous works, we consider this identification problem in a low-regularity regime in which the function inducing the Nemytskii operator is a-priori only known to be an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a suitable point of departure for the rigorous analysis of training problems for learning-informed PDEs in which an unknown superposition operator is approximated by means of a neural network with nonsmooth activation functions (ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the controls, it is possible to derive a classical stationarity system for local minimizers and to solve the considered problem by means of a gradient projection me
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.18453</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#23427;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#25968;&#25454;&#31232;&#32570;&#12289;&#37319;&#38598;&#26041;&#27861;&#19981;&#19968;&#33268;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#12290;Med-DDPM&#30340;&#29420;&#29305;&#29305;&#28857;&#22312;&#20110;&#20351;&#29992;&#35821;&#20041;&#26465;&#20214;&#36827;&#34892;&#19977;&#32500;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23427;&#20415;&#20110;&#21019;&#24314;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;Med-DDPM&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;Med-DDPM&#22312;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;&#23427;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.17026</link><description>&lt;p&gt;
&#35770;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#23545;&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Transformer&#27169;&#22411;&#30340;&#29702;&#35770;&#25991;&#29486;&#65292;&#24182;&#34920;&#26126;&#20165;&#20351;&#29992;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#30340;&#35299;&#30721;&#22120;Transformer&#32467;&#26500;&#65292;&#22312;&#21512;&#29702;&#20551;&#35774;&#19979;&#20855;&#22791;&#22270;&#28789;&#23436;&#22791;&#24615;&#12290;&#20174;&#29702;&#35770;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#22270;&#28789;&#23436;&#22791;&#24615;&#25104;&#31435;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a theoretical evaluation of the computational universality of decoder-only transformer models. We extend the theoretical literature on transformer models and show that decoder-only transformer architectures (even with only a single layer and single attention head) are Turing complete under reasonable assumptions. From the theoretical analysis, we show sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#35270;&#22270;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#24182;&#34920;&#29616;&#20986;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15927</link><description>&lt;p&gt;
&#29992;&#26368;&#20248;&#20256;&#36755;&#23398;&#20064;&#26377;&#21521;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Directed Graphical Models with Optimal Transport. (arXiv:2305.15927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15927
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#35270;&#22270;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#24182;&#34920;&#29616;&#20986;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#27010;&#29575;&#26377;&#21521;&#22270;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#23384;&#22312;&#28508;&#22312;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#27809;&#26377;&#20851;&#20110;&#32467;&#26500;&#20381;&#36182;&#24615;&#25110;&#27169;&#22411;&#31867;&#30340;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#37117;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#22522;&#26412;&#19978;&#26159;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#20010;&#26032;&#35270;&#22270;&#12290;&#36825;&#20010;&#35266;&#28857;&#25480;&#26435;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36816;&#20316;&#65292;&#32780;&#19981;&#20250;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#20570;&#20986;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#25110;&#35785;&#35832;&#20110;&#40657;&#31665;&#21464;&#20998;&#36817;&#20284;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25903;&#25345;&#23427;&#36890;&#36807;&#24191;&#27867;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#24674;&#22797;&#22522;&#20934;&#21442;&#25968;&#65292;&#32780;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#20063;&#34920;&#29616;&#24471;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the parameters of a probabilistic directed graphical model from incomplete data remains a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without further assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a framework that operates on many directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to black-box variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the flexibility and versatility of our approach. Across experiments, we show that not only can our method recover the ground-truth parameters but it also performs competitively on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15659</link><description>&lt;p&gt;
&#22914;&#20309;&#36867;&#31163;&#38160;&#21270;&#30340;&#26497;&#23567;&#20540;&#28857;
&lt;/p&gt;
&lt;p&gt;
How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#20123;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#26469;&#21457;&#29616;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#31639;&#27861;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#25439;&#22833;&#20989;&#25968;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#26469;&#24230;&#37327;&#23427;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#24182;&#24418;&#24335;&#21270;&#23450;&#20041;&#20102;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#27010;&#24565;&#12290;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#12290;&#38024;&#23545;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#12290;&#31639;&#27861;&#30340;&#20027;&#35201;&#32452;&#20214;&#26159;&#20351;&#29992;&#20174;&#38543;&#26426;&#25200;&#21160;&#36845;&#20195;&#20013;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20272;&#35745;&#23548;&#33268;&#26356;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#26041;&#21521;&#12290;&#23545;&#20110;&#25104;&#26412;&#20989;&#25968;&#26159;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#21463;&#26368;&#36817;&#25552;&#20986;&#30340;&#23454;&#29992;&#31639;&#27861;&#8212;&#8212;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;KL&#25955;&#24230;&#38477;&#20302;&#30340;&#26032;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#26679;&#26412;&#36827;&#34892;&#35745;&#31639;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#65292;&#20855;&#26377;&#27604;SVGD&#26356;&#31616;&#21333;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#20063;&#26377;&#20248;&#21270;&#65292;&#25552;&#21319;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.15577</link><description>&lt;p&gt;
&#37319;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#30340;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Variational Gradient Descent using Local Linear Models. (arXiv:2305.15577v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;KL&#25955;&#24230;&#38477;&#20302;&#30340;&#26032;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#26679;&#26412;&#36827;&#34892;&#35745;&#31639;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#65292;&#20855;&#26377;&#27604;SVGD&#26356;&#31616;&#21333;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#20063;&#26377;&#20248;&#21270;&#65292;&#25552;&#21319;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) &#33021;&#22815;&#27839;&#30528;&#36712;&#36857;&#20256;&#36755;&#31890;&#23376;&#65292;&#20174;&#32780;&#20943;&#23569;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#20294;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#26469;&#35745;&#31639;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SVGD&#35270;&#35282;&#65292;&#23558;&#20854;&#35270;&#20026;&#21453;&#21521;KL&#26799;&#24230;&#27969;&#30340;&#23616;&#37096;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#35270;&#35282;&#21551;&#21457;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#26469;&#23454;&#29616;&#30456;&#21516;&#30446;&#30340;&#30340;&#26032;&#20272;&#35745;&#22120;&#12290;&#36825;&#20123;&#25552;&#35758;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#20165;&#20351;&#29992;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;&#30340;&#26679;&#26412;&#36827;&#34892;&#35745;&#31639;&#65292;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#35758;&#30340;&#21464;&#20998;&#26799;&#24230;&#20272;&#35745;&#22120;&#21033;&#29992;&#20102;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20272;&#35745;&#20559;&#24046;&#19982;SVGD&#30456;&#24403;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#31616;&#20415;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#39640;&#32500;&#26799;&#24230;&#27969;&#30340;&#20272;&#35745;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;&#20302;&#32500;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#22909;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;&#25105;&#20204;&#23545;&#25552;&#35758;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) can transport particles along trajectories that reduce the KL divergence between the target and particle distribution but requires the target score function to compute the update. We introduce a new perspective on SVGD that views it as a local estimator of the reversed KL gradient flow. This perspective inspires us to propose new estimators that use local linear models to achieve the same purpose. The proposed estimators can be computed using only samples from the target and particle distribution without needing the target score function. Our proposed variational gradient estimators utilize local linear models, resulting in computational simplicity while maintaining effectiveness comparable to SVGD in terms of estimation biases. Additionally, we demonstrate that under a mild assumption, the estimation of high-dimensional gradient flow can be translated into a lower-dimensional estimation problem, leading to improved estimation accuracy. We vali
&lt;/p&gt;</description></item><item><title>BertRLFuzzer&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#24037;&#20855;&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#65292;BertRLFuzzer&#30456;&#23545;&#20110;&#20854;&#20182;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#12289;&#26032;&#28431;&#27934;&#21457;&#29616;&#21644;&#25915;&#20987;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.12534</link><description>&lt;p&gt;
BertRLFuzzer: &#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer
&lt;/p&gt;
&lt;p&gt;
BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer. (arXiv:2305.12534v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12534
&lt;/p&gt;
&lt;p&gt;
BertRLFuzzer&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#24037;&#20855;&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#65292;BertRLFuzzer&#30456;&#23545;&#20110;&#20854;&#20182;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#12289;&#26032;&#28431;&#27934;&#21457;&#29616;&#21644;&#25915;&#20987;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;BertRLFuzzer&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;BertRLFuzzer&#30340;&#24037;&#20316;&#21407;&#29702;&#22914;&#19979;&#65306;&#32473;&#23450;&#19968;&#32452;&#31181;&#23376;&#36755;&#20837;&#65292;Fuzzer&#23545;&#23427;&#20204;&#25191;&#34892;&#36981;&#24490;&#35821;&#27861;&#24182;&#24341;&#21457;&#25915;&#20987;&#30340;&#21464;&#24322;&#25805;&#20316;&#65292;&#20197;&#29983;&#25104;&#20505;&#36873;&#25915;&#20987;&#21521;&#37327;&#12290;BertRLFuzzer&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#39640;&#25928;&#23398;&#20064;&#36981;&#24490;&#35821;&#27861;&#21644;&#24341;&#21457;&#25915;&#20987;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#12290;&#20026;&#20102;&#39564;&#35777;BertRLFuzzer&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20849;&#35745;13&#20010;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;9&#20010;&#21463;&#23475;&#32773;&#32593;&#31449;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#28041;&#21450;&#36229;&#36807;16K&#34892;&#30340;&#28304;&#20195;&#30721;&#12290;&#30456;&#23545;&#20110;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#24037;&#20855;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#30340;&#26174;&#33879;&#25913;&#36827;&#65288;&#20943;&#23569;54&#65285;&#65289;&#65292;&#21457;&#29616;&#30340;&#26032;&#28431;&#27934;&#65288;17&#20010;&#26032;&#28431;&#27934;&#65289;&#21644;&#25915;&#20987;&#29575;&#65288;&#29983;&#25104;&#30340;&#25915;&#20987;&#21521;&#37327;&#22686;&#21152;&#20102;4.4&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL) based fuzzer aimed at finding security vulnerabilities for Web applications. BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs grammar-adhering and attack-provoking mutation operations on them to generate candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with a BERT model as an agent to guide the fuzzer to efficiently learn grammar-adhering and attack-provoking mutation operators. In order to establish the efficacy of BertRLFuzzer we compare it against a total of 13 black box and white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We observed a significant improvement, relative to the nearest competing tool, in terms of time to first attack (54% less), new vulnerabilities found (17 new vulnerabilities), and attack rate (4.4% more attack vectors generated).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#23459;&#35328;&#65292;&#35748;&#20026;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;</title><link>http://arxiv.org/abs/2304.03674</link><description>&lt;p&gt;
&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#20221;&#23459;&#35328;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with Requirements: a Manifesto. (arXiv:2304.03674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#23459;&#35328;&#65292;&#35748;&#20026;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#27493;&#65292;&#25104;&#20026;&#35768;&#22810;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#31361;&#30772;&#30340;&#26681;&#28304;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#23427;&#20204;&#24212;&#29992;&#21040;&#39640;&#39118;&#38505;&#25110;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#23481;&#26131;&#21464;&#24471;&#33030;&#24369;&#21644;&#19981;&#21487;&#38752;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38656;&#27714;&#35268;&#26684;&#35828;&#26126;&#26377;&#30410;&#22320;&#25972;&#21512;&#21040;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#27969;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recent years, machine learning has made great advancements that have been at the root of many breakthroughs in different application domains. However, it is still an open issue how make them applicable to high-stakes or safety-critical application domains, as they can often be brittle and unreliable. In this paper, we argue that requirements definition and satisfaction can go a long way to make machine learning models even more fitting to the real world, especially in critical domains. To this end, we present two problems in which (i) requirements arise naturally, (ii) machine learning models are or can be fruitfully deployed, and (iii) neglecting the requirements can have dramatic consequences. We show how the requirements specification can be fruitfully integrated into the standard machine learning development pipeline, proposing a novel pyramid development process in which requirements definition may impact all the subsequent phases in the pipeline, and viceversa.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#26469;&#23454;&#29616;&#36523;&#20307;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20182;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#34920;&#24449;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18240</link><description>&lt;p&gt;
&#23547;&#25214;&#20855;&#26377;&#36523;&#20307;&#26234;&#33021;&#30340;&#20154;&#24037;&#35270;&#35273;&#30382;&#23618;&#22312;&#21738;&#37324;&#65311;
&lt;/p&gt;
&lt;p&gt;
Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?. (arXiv:2303.18240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#26469;&#23454;&#29616;&#36523;&#20307;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20182;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#34920;&#24449;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#65288;PVR&#65289;&#25110;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#29992;&#20110;&#36523;&#20307;&#26234;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102; CortexBench&#65292;&#20854;&#20013;&#21253;&#25324;&#28085;&#30422;&#21160;&#21147;&#23398;&#12289;&#23548;&#33322;&#12289;&#29087;&#32451;&#21644;&#31227;&#21160;&#25805;&#20316;&#30340;17&#31181;&#19981;&#21516;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31995;&#32479;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;PVR&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#12290;&#20026;&#20102;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#26469;&#33258;7&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#36229;&#36807;4000&#23567;&#26102;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65288;&#36229;&#36807;560&#19975;&#24352;&#22270;&#20687;&#65289;&#21644;ImageNet&#65292;&#20351;&#29992;&#20999;&#29255;&#25968;&#25454;&#30340;&#36974;&#30422;&#33258;&#32534;&#30721;&#65288;MAE&#65289;&#26469;&#35757;&#32451;&#19981;&#21516;&#22823;&#23567;&#30340;&#35270;&#35273;&#21464;&#24418;&#22120;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#25512;&#26029;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#25193;&#23637;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#65288;&#20294;&#24179;&#22343;&#24615;&#33021;&#26377;&#25152;&#25552;&#39640;&#65289;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#21517;&#20026;VC-1&#65292;&#24179;&#22343;&#34920;&#29616;&#36229;&#36807;&#25152;&#26377;&#20808;&#21069;&#30340;PVR&#65292;&#20294;&#20063;&#27809;&#26377;&#26222;&#36941;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;VC-1&#30340;&#29305;&#23450;&#20110;&#20219;&#21153;&#25110;&#39046;&#22495;&#30340;&#36866;&#24212;&#20250;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant.  To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average).  Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantia
&lt;/p&gt;</description></item><item><title>VIDIMU&#25968;&#25454;&#38598;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#35760;&#24405;13&#31181;&#20020;&#24202;&#30456;&#20851;&#24615;&#30340;&#27963;&#21160;&#65292;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.16150</link><description>&lt;p&gt;
VIDIMU: &#20351;&#29992;&#20215;&#26684;&#23454;&#24800;&#30340;&#35774;&#22791;&#35760;&#24405;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#21644;IMU&#36816;&#21160;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices. (arXiv:2303.16150v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16150
&lt;/p&gt;
&lt;p&gt;
VIDIMU&#25968;&#25454;&#38598;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#35760;&#24405;13&#31181;&#20020;&#24202;&#30456;&#20851;&#24615;&#30340;&#27963;&#21160;&#65292;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#21644;&#20020;&#24202;&#29983;&#29289;&#21147;&#23398;&#26159;&#29289;&#29702;&#36828;&#31243;&#24247;&#22797;&#21307;&#23398;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#20307;&#21160;&#20316;&#25968;&#25454;&#38598;&#19981;&#33021;&#29992;&#20110;&#30740;&#31350;&#23454;&#39564;&#23460;&#22806;&#36816;&#21160;&#33719;&#21462;&#24773;&#20917;&#19979;&#30340;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;VIDIMU&#25968;&#25454;&#38598;&#30340;&#30446;&#30340;&#26159;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#20116;&#20010;&#24815;&#24615;&#20256;&#24863;&#22120;&#27880;&#20876;&#30340;13&#31181;&#27963;&#21160;&#12290;&#35760;&#24405;&#35270;&#39057;&#30340;54&#20010;&#21463;&#35797;&#32773;&#20013;&#65292;&#20854;&#20013;16&#20010;&#21463;&#35797;&#32773;&#21516;&#26102;&#36824;&#26377;&#24815;&#24615;&#20256;&#24863;&#22120;&#35760;&#24405;&#12290;VIDIMU&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65306;i&#65289;&#25152;&#36873;&#25321;&#30340;&#21160;&#20316;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;ii&#65289;&#20351;&#29992;&#20215;&#26684;&#23454;&#24800;&#30340;&#35270;&#39057;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450; iii&#65289;&#23454;&#29616;&#20102;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#24037;&#20855;&#65292;&#21487;&#20197;&#20174;&#24815;&#24615;&#25968;&#25454;&#20013;&#23545;&#19977;&#32500;&#36523;&#20307;&#23039;&#21183;&#36319;&#36394;&#21644;&#36816;&#21160;&#37325;&#24314;&#22312;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition and clinical biomechanics are challenging problems in physical telerehabilitation medicine. However, most publicly available datasets on human body movements cannot be used to study both problems in an out-of-the-lab movement acquisition setting. The objective of the VIDIMU dataset is to pave the way towards affordable patient tracking solutions for remote daily life activities recognition and kinematic analysis. The dataset includes 13 activities registered using a commodity camera and five inertial sensors. The video recordings were acquired in 54 subjects, of which 16 also had simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in: i) the clinical relevance of the chosen movements, ii) the combined utilization of affordable video and custom sensors, and iii) the implementation of state-of-the-art tools for multimodal data processing of 3D body pose tracking and motion reconstruction in a musculoskeletal model from inertial data. The val
&lt;/p&gt;</description></item><item><title>InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.11435</link><description>&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65306;&#22270;&#20687;&#20462;&#22797;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11435
&lt;/p&gt;
&lt;p&gt;
InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65288;InDI&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#23427;&#36991;&#20813;&#20102;&#25152;&#35859;&#30340;&#8220;&#22343;&#20540;&#22238;&#24402;&#8221;&#25928;&#24212;&#65292;&#24182;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#12290;&#23427;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#23454;&#29616;&#65292;&#31867;&#20284;&#20110;&#29983;&#25104;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#22270;&#20687;&#20462;&#22797;&#26159;&#19968;&#20010;&#27424;&#23450;&#38382;&#39064;&#65292;&#22810;&#20010;&#39640;&#36136;&#37327;&#22270;&#20687;&#37117;&#21487;&#33021;&#26159;&#32473;&#23450;&#20302;&#36136;&#37327;&#36755;&#20837;&#30340;&#21487;&#34892;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#21333;&#27493;&#22238;&#24402;&#27169;&#22411;&#30340;&#32467;&#26524;&#36890;&#24120;&#26159;&#25152;&#26377;&#21487;&#33021;&#35299;&#37322;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#22240;&#27492;&#32570;&#20047;&#32454;&#33410;&#21644;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26041;&#27861;&#26469;&#35745;&#31639;&#37096;&#20998;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#65292;&#24182;&#22312;&#21512;&#25104;&#20363;&#23376;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.07988</link><description>&lt;p&gt;
&#37096;&#20998;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Partial Neural Optimal Transport. (arXiv:2303.07988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07988
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26041;&#27861;&#26469;&#35745;&#31639;&#37096;&#20998;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#65292;&#24182;&#22312;&#21512;&#25104;&#20363;&#23376;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26041;&#27861;&#26469;&#35745;&#31639;&#37096;&#20998;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#26144;&#23556;&#65292;&#21363;&#25351;&#23450;&#36136;&#37327;&#30340;&#24230;&#37327;&#37096;&#20998;&#20043;&#38388;&#30340;OT&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#20363;&#23376;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#37096;&#20998;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel neural method to compute partial optimal transport (OT) maps, i.e., OT maps between parts of measures of the specified masses. We test our partial neural optimal transport algorithm on synthetic examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Task Aware Dreamer&#65288;TAD&#65289;&#30340;&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#27867;&#21270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20219;&#21153;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#65292;TAD&#33021;&#22815;&#23558;&#21382;&#21490;&#20449;&#24687;&#32534;&#30721;&#21040;&#31574;&#30053;&#20013;&#65292;&#20197;&#20415;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05092</link><description>&lt;p&gt;
Task Aware Dreamer&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Task Aware Dreamer for Task Generalization in Reinforcement Learning. (arXiv:2303.05092v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Task Aware Dreamer&#65288;TAD&#65289;&#30340;&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#27867;&#21270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20219;&#21153;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#65292;TAD&#33021;&#22815;&#23558;&#21382;&#21490;&#20449;&#24687;&#32534;&#30721;&#21040;&#31574;&#30053;&#20013;&#65292;&#20197;&#20415;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#26159;&#33719;&#24471;&#33021;&#22815;&#22312;&#35757;&#32451;&#20219;&#21153;&#19978;&#23398;&#20064;&#24182;&#19988;&#22312;&#19981;&#21516;&#22870;&#21169;&#20989;&#25968;&#19979;&#21487;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#19968;&#20010;&#36890;&#29992;&#30340;&#25361;&#25112;&#26159;&#23450;&#37327;&#22320;&#34913;&#37327;&#36825;&#20123;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;&#20219;&#21153;&#20998;&#24067;&#24182;&#36827;&#19968;&#27493;&#35774;&#35745;&#20855;&#26377;&#26356;&#24378;&#27867;&#21270;&#33021;&#21147;&#30340;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#20219;&#21153;&#20998;&#24067;&#30456;&#20851;&#24615;&#65288;TDR&#65289;&#65292;&#36890;&#36807;&#19981;&#21516;&#20219;&#21153;&#30340;&#26368;&#20248;Q&#20989;&#25968;&#26469;&#37327;&#21270;&#20219;&#21153;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#20855;&#26377;&#39640;TDR&#30340;&#20219;&#21153;&#24773;&#20917;&#19979;&#65292;&#21363;&#20219;&#21153;&#20043;&#38388;&#26174;&#33879;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#32534;&#30721;&#21040;&#31574;&#30053;&#20013;&#20197;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;Task Aware Dreamer&#65288;TAD&#65289;&#65292;&#23427;&#23558;&#19990;&#30028;&#27169;&#22411;&#25193;&#23637;&#20026;&#25105;&#20204;&#30340;&#22870;&#21169;&#24863;&#30693;&#19990;&#30028;&#27169;&#22411;&#20197;&#25429;&#25417;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. A general challenge is to quantitatively measure the similarities between these different tasks, which is vital for analyzing the task distribution and further designing algorithms with stronger generalization. To address this, we present a novel metric named Task Distribution Relevance (TDR) via optimal Q functions of different tasks to capture the relevance of the task distribution quantitatively. In the case of tasks with a high TDR, i.e., the tasks differ significantly, we show that the Markovian policies cannot differentiate them, leading to poor performance. Based on this insight, we encode all historical information into policies for distinguishing different tasks and propose Task Aware Dreamer (TAD), which extends world models into our reward-informed world models to capture
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20445;&#25345;&#30456;&#21516;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#30340;&#30446;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#21487;&#20197;&#27604;&#26222;&#36890;&#30340; MC &#20272;&#35745;&#22120;&#20135;&#29983;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#35813;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.13734</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Monte Carlo Evaluation with Offline Data. (arXiv:2301.13734v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20445;&#25345;&#30456;&#21516;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#30340;&#30446;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#21487;&#20197;&#27604;&#26222;&#36890;&#30340; MC &#20272;&#35745;&#22120;&#20135;&#29983;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#35813;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#32599; (MC) &#26041;&#27861;&#26159;&#20272;&#35745;&#31574;&#30053;&#34920;&#29616;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#65292;MC &#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36816;&#34892;&#35813;&#31574;&#30053;&#20197;&#25910;&#38598;&#26679;&#26412;&#24182;&#21462;&#20986;&#32467;&#26524;&#24179;&#22343;&#20540;&#26469;&#32473;&#20986;&#20272;&#35745;&#20540;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#25910;&#38598;&#30340;&#26679;&#26412;&#31216;&#20026;&#22312;&#32447;&#26679;&#26412;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#65292;MC &#26041;&#27861;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#22312;&#32447;&#26679;&#26412;&#12290;&#24403;&#22312;&#32447;&#26679;&#26412;&#26114;&#36149;&#26102;&#65292;&#20363;&#22914;&#22312;&#32447;&#25512;&#33616;&#21644;&#24211;&#23384;&#31649;&#29702;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#32447; MC &#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#34892;&#19981;&#21516;&#30340;&#31574;&#30053;&#65288;&#31216;&#20026;&#34892;&#20026;&#31574;&#30053;&#65289;&#35780;&#20272;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#20351;&#31163;&#32447; MC &#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#26126;&#26174;&#23567;&#20110;&#26222;&#36890; MC &#20272;&#35745;&#22120;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#23450;&#21046;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#65292;&#21363;&#20808;&#21069;&#35760;&#24405;&#30340;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#27604;&#22312;&#32447;&#26679;&#26412;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo (MC) methods are the most widely used methods to estimate the performance of a policy. Given an interested policy, MC methods give estimates by repeatedly running this policy to collect samples and taking the average of the outcomes. Samples collected during this process are called online samples. To get an accurate estimate, MC methods consume massive online samples. When online samples are expensive, e.g., online recommendations and inventory management, we want to reduce the number of online samples while achieving the same estimate accuracy. To this end, we use off-policy MC methods that evaluate the interested policy by running a different policy called behavior policy. We design a tailored behavior policy such that the variance of the off-policy MC estimator is provably smaller than the ordinary MC estimator. Importantly, this tailored behavior policy can be efficiently learned from existing offline data, i,e., previously logged data, which are much cheaper than onlin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;&#30340;PUC&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.13954</link><description>&lt;p&gt;
&#25105;&#19981;&#24819;&#35828;&#65306;&#22312;&#21487;&#36873;&#20010;&#20154;&#25968;&#25454;&#27169;&#22411;&#20013;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;
&lt;/p&gt;
&lt;p&gt;
I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data. (arXiv:2210.13954v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;&#30340;PUC&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#65292;&#36825;&#22312;&#29616;&#20195;&#20445;&#38505;&#23450;&#20215;&#27169;&#22411;&#20013;&#24456;&#24120;&#35265;&#12290;&#19968;&#20123;&#29992;&#25143;&#21516;&#24847;&#20351;&#29992;&#20182;&#20204;&#30340;&#25968;&#25454;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#21453;&#23545;&#24182;&#20445;&#25345;&#20854;&#25968;&#25454;&#26410;&#20844;&#24320;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#20915;&#23450;&#26412;&#36523;&#21487;&#20197;&#34987;&#35270;&#20026;&#20449;&#24687;&#65292;&#24212;&#35813;&#21463;&#21040;&#20445;&#25252;&#65292;&#20197;&#23562;&#37325;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24341;&#21457;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#30830;&#20445;&#20445;&#25252;&#20854;&#20010;&#20154;&#25968;&#25454;&#30340;&#29992;&#25143;&#19981;&#20250;&#22240;&#27492;&#21463;&#21040;&#20219;&#20309;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#20165;&#20351;&#29992;&#33719;&#24471;&#31215;&#26497;&#29992;&#25143;&#21516;&#24847;&#30340;&#20449;&#24687;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20445;&#25252;&#35201;&#27714;&#30340;&#27491;&#24335;&#21270;&#12290;&#36825;&#25490;&#38500;&#20102;&#20316;&#20986;&#20849;&#20139;&#25968;&#25454;&#19982;&#21542;&#20915;&#23450;&#25152;&#21253;&#21547;&#30340;&#38544;&#21547;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Protected User Consent (PUC)&#27010;&#24565;&#65292;&#36825;&#26159;&#25105;&#20204;&#35777;&#26126;&#22312;&#20445;&#25252;&#35201;&#27714;&#19979;&#25439;&#22833;&#26368;&#23567;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent (PUC), which we prove to be loss-optimal under our protection requirement. To
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20449;&#24687;&#35770;&#24230;&#37327;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#22810;&#26679;&#24615;&#65292;&#21457;&#29616;&#20854;&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35760;&#24518;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26410;&#26631;&#35760;&#30340;&#20998;&#24067;&#20869;&#37096;&#35745;&#31639;&#31070;&#32463;&#28608;&#27963;&#26102;&#65292;&#20449;&#24687;&#30340;&#32452;&#32455;&#20063;&#21487;&#20197;&#25351;&#21521;&#20004;&#31181;&#24418;&#24335;&#30340;&#35760;&#24518;&#12290;</title><link>http://arxiv.org/abs/2210.09404</link><description>&lt;p&gt;
&#20449;&#24687;&#24230;&#37327;&#21453;&#26144;&#35760;&#24518;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Measures of Information Reflect Memorization Patterns. (arXiv:2210.09404v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20449;&#24687;&#35770;&#24230;&#37327;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#22810;&#26679;&#24615;&#65292;&#21457;&#29616;&#20854;&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35760;&#24518;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26410;&#26631;&#35760;&#30340;&#20998;&#24067;&#20869;&#37096;&#35745;&#31639;&#31070;&#32463;&#28608;&#27963;&#26102;&#65292;&#20449;&#24687;&#30340;&#32452;&#32455;&#20063;&#21487;&#20197;&#25351;&#21521;&#20004;&#31181;&#24418;&#24335;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#21033;&#29992;&#19982;&#30446;&#26631;&#26631;&#31614;&#20849;&#29616;&#30340;&#38169;&#35823;&#20551;&#35937;&#65288;&#25110;&#25463;&#24452;&#65289;&#26469;&#23637;&#31034;&#21551;&#21457;&#24335;&#35760;&#24518;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#20250;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#65292;&#20135;&#29983;&#22522;&#20110;&#31034;&#20363;&#30340;&#35760;&#24518;&#12290;&#36825;&#20123;&#35760;&#24518;&#31867;&#22411;&#38459;&#30861;&#20102;&#32593;&#32476;&#22312;&#36229;&#20986;&#35757;&#32451;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26816;&#27979;&#27492;&#31867;&#35760;&#24518;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#30740;&#31350;&#20154;&#21592;&#31574;&#21010;&#23450;&#21046;&#30340;&#27979;&#35797;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#8212;&#8212;&#24182;&#38543;&#21518;&#23637;&#31034;&#20102;&#8212;&#8212;&#19981;&#21516;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#12290;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#24230;&#37327;&#37327;&#21270;&#31070;&#32463;&#28608;&#27963;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#28085;&#30422;&#20102;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#23545;&#25105;&#20204;&#20551;&#35774;&#30340;&#25903;&#25345;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20449;&#24687;&#30340;&#32452;&#32455;&#25351;&#21521;&#20102;&#36825;&#20004;&#31181;&#24418;&#24335;&#30340;&#35760;&#24518;&#65292;&#21363;&#20351;&#26159;&#22312;&#26410;&#26631;&#35760;&#30340;&#20998;&#24067;&#20869;&#37096;&#35745;&#31639;&#30340;&#31070;&#32463;&#28608;&#27963;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are known to exploit spurious artifacts (or shortcuts) that co-occur with a target label, exhibiting heuristic memorization. On the other hand, networks have been shown to memorize training examples, resulting in example-level memorization. These kinds of memorization impede generalization of networks beyond their training distributions. Detecting such memorization could be challenging, often requiring researchers to curate tailored test sets. In this work, we hypothesize -- and subsequently show -- that the diversity in the activation patterns of different neurons is reflective of model generalization and memorization. We quantify the diversity in the neural activations through information-theoretic measures and find support for our hypothesis on experiments spanning several natural language and vision tasks. Importantly, we discover that information organization points to the two forms of memorization, even for neural activations computed on unlabelled in-distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;</title><link>http://arxiv.org/abs/2210.01672</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#23558;&#26426;&#22120;&#20154;&#20998;&#31867;&#24102;&#20837;&#36830;&#32493;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds. (arXiv:2210.01672v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20998;&#31867;&#34987;&#29992;&#20316;&#23558;&#20154;&#31867;&#30340;&#31227;&#21160;&#21644;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#39640;&#23618;&#27425;&#30340;&#20998;&#23618;&#25277;&#35937;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#20998;&#26512;&#25235;&#21462;&#12289;&#25805;&#32437;&#25216;&#33021;&#21644;&#20840;&#36523;&#25903;&#25745;&#23039;&#21183;&#38750;&#24120;&#26377;&#29992;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#35774;&#35745;&#23618;&#27425;&#32467;&#26500;&#21644;&#22522;&#30784;&#31867;&#21035;&#26041;&#38754;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#24212;&#29992;&#39046;&#22495;&#30340;&#20351;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#32570;&#20047;&#22635;&#34917;&#20998;&#31867;&#23618;&#32423;&#32467;&#26500;&#21644;&#19982;&#20854;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#39640;&#32500;&#24322;&#26500;&#25968;&#25454;&#20043;&#38388;&#24046;&#36317;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#39640;&#26031;&#36807;&#31243;&#21452;&#26354;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#23558;&#20998;&#31867;&#27861;&#32467;&#26500;&#32435;&#20837;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use in application fields remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different robotics taxonomies to lear
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AEAIL&#65289;&#26159;&#19968;&#31181;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29615;&#22659;&#20013;&#65292;AEAIL&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;&#20110;&#22122;&#22768;&#31034;&#33539;&#19987;&#23478;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.11004</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Auto-Encoding Adversarial Imitation Learning. (arXiv:2206.11004v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11004
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AEAIL&#65289;&#26159;&#19968;&#31181;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29615;&#22659;&#20013;&#65292;AEAIL&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;&#20110;&#22122;&#22768;&#31034;&#33539;&#19987;&#23478;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#20854;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#25581;&#31034;&#20102;&#22312;&#27809;&#26377;&#26469;&#33258;&#29615;&#22659;&#30340;&#22870;&#21169;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#33719;&#21462;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AEAIL&#65289;&#26694;&#26550;&#12290;&#20026;&#20102;&#20174;&#31034;&#33539;&#20013;&#25512;&#23548;&#20986;&#19987;&#23478;&#31574;&#30053;&#65292;AEAIL&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36825;&#27604;&#20043;&#21069;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22810;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25152;&#24471;&#21040;&#30340;&#30446;&#26631;&#20989;&#25968;&#35757;&#32451;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;AEAIL&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#31034;&#33539;&#19987;&#23478;&#20855;&#26377;&#22122;&#22768;&#26102;&#65292;AEAIL&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) provides a powerful framework for decision-making, but its application in practice often requires a carefully designed reward function. Adversarial Imitation Learning (AIL) sheds light on automatic policy acquisition without access to the reward signal from the environment. In this work, we propose Auto-Encoding Adversarial Imitation Learning (AEAIL), a robust and scalable AIL framework. To induce expert policies from demonstrations, AEAIL utilizes the reconstruction error of an auto-encoder as a reward signal, which provides more information for optimizing policies than the prior discriminator-based ones. Subsequently, we use the derived objective functions to train the auto-encoder and the agent policy. Experiments show that our AEAIL performs superior compared to state-of-the-art methods on both state and image based environments. More importantly, AEAIL shows much better robustness when the expert demonstrations are noisy.
&lt;/p&gt;</description></item></channel></rss>