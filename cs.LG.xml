<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#33021;&#28304;&#28040;&#32791;&#20043;&#38388;&#26435;&#34913;&#30340;&#21487;&#25345;&#32493;&#24615;&#25351;&#26631;&#65292;&#38024;&#23545;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#38754;&#21521;&#33021;&#28304;&#24863;&#30693;&#30340;&#32852;&#21512;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.10645</link><description>&lt;p&gt;
&#38754;&#21521;&#33021;&#28304;&#24863;&#30693;&#30340;&#32852;&#21512;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#22312;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Energy-Aware Federated Traffic Prediction for Cellular Networks. (arXiv:2309.10645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#33021;&#28304;&#28040;&#32791;&#20043;&#38388;&#26435;&#34913;&#30340;&#21487;&#25345;&#32493;&#24615;&#25351;&#26631;&#65292;&#38024;&#23545;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#38754;&#21521;&#33021;&#28304;&#24863;&#30693;&#30340;&#32852;&#21512;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#23545;&#20110;&#20248;&#21270;&#31532;&#20116;&#20195;(5G)&#21450;&#26356;&#39640;&#29256;&#26412;&#30340;&#32593;&#32476;&#33267;&#20851;&#37325;&#35201;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#23545;&#20110;&#26234;&#33021;&#32593;&#32476;&#35774;&#35745;&#12289;&#36164;&#28304;&#20998;&#37197;&#21644;&#24322;&#24120;&#32531;&#35299;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;(ML)&#26159;&#19968;&#31181;&#26377;&#25928;&#39044;&#27979;&#32593;&#32476;&#27969;&#37327;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#38598;&#20013;&#23384;&#20648;&#22312;&#21333;&#20010;&#25968;&#25454;&#20013;&#24515;&#20013;&#65292;&#28041;&#21450;&#21040;&#26426;&#23494;&#24615;&#12289;&#38544;&#31169;&#21644;&#25968;&#25454;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#32852;&#21512;&#23398;&#20064;(FL)&#20316;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;ML&#35757;&#32451;&#26694;&#26550;&#24212;&#36816;&#32780;&#29983;&#65292;&#36890;&#36807;&#24182;&#34892;&#20998;&#24067;&#24335;&#35745;&#31639;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#29615;&#22659;&#24433;&#21709;&#24120;&#24120;&#34987;&#24573;&#35270;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20110;&#20854;&#21487;&#25345;&#32493;&#24615;&#30340;&#36136;&#30097;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#25345;&#32493;&#24615;&#25351;&#26631;&#65292;&#35299;&#20915;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#20934;&#30830;&#24615;&#21644;&#33021;&#28304;&#28040;&#32791;&#20043;&#38388;&#30340;&#25240;&#34935;&#38382;&#39064;&#65292;&#24182;&#23545;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular traffic prediction is a crucial activity for optimizing networks in fifth-generation (5G) networks and beyond, as accurate forecasting is essential for intelligent network design, resource allocation and anomaly mitigation. Although machine learning (ML) is a promising approach to effectively predict network traffic, the centralization of massive data in a single data center raises issues regarding confidentiality, privacy and data transfer demands. To address these challenges, federated learning (FL) emerges as an appealing ML training framework which offers high accurate predictions through parallel distributed computations. However, the environmental impact of these methods is often overlooked, which calls into question their sustainability. In this paper, we address the trade-off between accuracy and energy consumption in FL by proposing a novel sustainability indicator that allows assessing the feasibility of ML models. Then, we comprehensively evaluate state-of-the-art d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10639</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20840;&#23616;${\mathcal L}^2$&#26368;&#23567;&#21270;&#22120;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#65292;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#65292;${\mathcal L}^2$ Schatten&#31867;&#65288;&#25110;Hilbert-Schmidt&#65289;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#30456;&#31561;&#32500;&#24230;$Q\geq1$&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#12290;&#38544;&#34255;&#23618;&#20063;&#23450;&#20041;&#22312;${\mathbb R}^{Q}$&#30340;&#31354;&#38388;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#26368;&#26032;&#30340;&#20851;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#65292;&#22312;$L\geq Q$&#30340;&#24773;&#20917;&#19979;&#26500;&#36896;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#26063;&#26159;&#36864;&#21270;&#30340;&#12290;&#22312;&#36825;&#37324;&#25552;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;DL&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#36890;&#36807;&#23545;&#35757;&#32451;&#36755;&#20837;&#30340;&#36882;&#24402;&#25130;&#26029;&#26144;&#23556;&#30340;&#24212;&#29992;&#26469;&#8220;&#25972;&#29702;&#8221;&#35757;&#32451;&#36755;&#20837;&#65292;&#20197;&#26368;&#23567;&#21270;&#22122;&#22768;&#19982;&#20449;&#21495;&#30340;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$2^Q-1$&#20010;&#19981;&#21516;&#30340;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#24191;&#35282;&#30524;&#24213;&#22270;&#20687;&#30340;&#26080;&#28304;&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;SFADA&#65289;&#29992;&#20110;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20998;&#32423;&#65292;&#36890;&#36807;&#29983;&#25104;&#36830;&#32493;&#28436;&#21464;&#20851;&#31995;&#30340;&#29305;&#24449;&#12289;&#27963;&#21160;&#36873;&#25321;&#26377;&#20215;&#20540;&#30340;UWF&#30524;&#24213;&#22270;&#20687;&#36827;&#34892;&#26631;&#27880;&#20197;&#21450;&#21033;&#29992;DR&#30149;&#21464;&#21407;&#22411;&#36866;&#24212;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;DR&#20998;&#32423;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10619</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#24191;&#35282;&#30524;&#24213;&#22270;&#20687;&#30340;&#26080;&#28304;&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20998;&#32423;
&lt;/p&gt;
&lt;p&gt;
Source-free Active Domain Adaptation for Diabetic Retinopathy Grading Based on Ultra-wide-field Fundus Image. (arXiv:2309.10619v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#24191;&#35282;&#30524;&#24213;&#22270;&#20687;&#30340;&#26080;&#28304;&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;SFADA&#65289;&#29992;&#20110;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20998;&#32423;&#65292;&#36890;&#36807;&#29983;&#25104;&#36830;&#32493;&#28436;&#21464;&#20851;&#31995;&#30340;&#29305;&#24449;&#12289;&#27963;&#21160;&#36873;&#25321;&#26377;&#20215;&#20540;&#30340;UWF&#30524;&#24213;&#22270;&#20687;&#36827;&#34892;&#26631;&#27880;&#20197;&#21450;&#21033;&#29992;DR&#30149;&#21464;&#21407;&#22411;&#36866;&#24212;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;DR&#20998;&#32423;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26410;&#27880;&#37322;&#30340;&#36229;&#24191;&#35282;&#65288;UWF&#65289;&#30524;&#24213;&#22270;&#20687;&#30340;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#20998;&#32423;&#20013;&#65292;&#21487;&#20197;&#20174;&#24050;&#26631;&#27880;&#30340;&#24425;&#33394;&#30524;&#24213;&#22270;&#20687;&#20013;&#36716;&#21270;&#24050;&#27880;&#37322;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#22495;&#24046;&#24322;&#21644;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;DA&#31639;&#27861;&#22312;DR&#20998;&#32423;&#24615;&#33021;&#19978;&#36828;&#36828;&#36798;&#19981;&#21040;&#20020;&#24202;&#35786;&#26029;&#30340;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#28304;&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;&#65288;SFADA&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;DR&#20998;&#32423;&#38382;&#39064;&#26412;&#36523;&#65292;&#25552;&#20986;&#20197;DR&#36830;&#32493;&#28436;&#21464;&#20851;&#31995;&#30340;&#26041;&#24335;&#29983;&#25104;&#24425;&#33394;&#30524;&#24213;&#22270;&#20687;&#29305;&#24449;&#65292;&#36890;&#36807;&#23616;&#37096;&#34920;&#31034;&#21305;&#37197;&#20027;&#21160;&#36873;&#25321;&#19968;&#20123;&#26377;&#20215;&#20540;&#30340;UWF&#30524;&#24213;&#22270;&#20687;&#36827;&#34892;&#26631;&#27880;&#65292;&#24182;&#21033;&#29992;DR&#30149;&#21464;&#21407;&#22411;&#22312;UWF&#30524;&#24213;&#22270;&#20687;&#19978;&#36866;&#24212;&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SFADA&#36824;&#32771;&#34385;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;SFADA&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;DR&#20998;&#32423;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation (DA) has been widely applied in the diabetic retinopathy (DR) grading of unannotated ultra-wide-field (UWF) fundus images, which can transfer annotated knowledge from labeled color fundus images. However, suffering from huge domain gaps and complex real-world scenarios, the DR grading performance of most mainstream DA is far from that of clinical diagnosis. To tackle this, we propose a novel source-free active domain adaptation (SFADA) in this paper. Specifically, we focus on DR grading problem itself and propose to generate features of color fundus images with continuously evolving relationships of DRs, actively select a few valuable UWF fundus images for labeling with local representation matching, and adapt model on UWF fundus images with DR lesion prototypes. Notably, the SFADA also takes data privacy and computational efficiency into consideration. Extensive experimental results demonstrate that our proposed SFADA achieves state-of-the-art DR grading performance,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#65288;NLFA&#65289;&#27169;&#22411;&#23545;&#39640;&#32500;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10618</link><description>&lt;p&gt;
&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#30340;&#21160;&#24577;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Dynamic Linear Bias Incorporation Scheme for Nonnegative Latent Factor Analysis. (arXiv:2309.10618v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#65288;NLFA&#65289;&#27169;&#22411;&#23545;&#39640;&#32500;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#24230;&#21644;&#19981;&#23436;&#25972;&#65288;HDI&#65289;&#25968;&#25454;&#22312;&#22823;&#25968;&#25454;&#30456;&#20851;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#27604;&#22914;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#28041;&#21450;&#19982;&#20247;&#22810;&#33410;&#28857;&#20043;&#38388;&#30340;&#26377;&#38480;&#20132;&#20114;&#12290;&#20174;HDI&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#26159;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22240;&#20026;HDI&#25968;&#25454;&#20013;&#23884;&#20837;&#20102;&#20016;&#23500;&#30340;&#27169;&#24335;&#65292;&#22914;&#33410;&#28857;&#34892;&#20026;&#12290;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#65288;NLFA&#65289;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#65288;LBI&#65289;&#26041;&#26696;&#23545;&#20110;&#22788;&#29702;&#35757;&#32451;&#36807;&#24230;&#25670;&#21160;&#12289;&#27874;&#21160;&#20197;&#21450;&#38450;&#27490;&#27169;&#22411;&#36807;&#26089;&#25910;&#25947;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LBI&#26041;&#26696;&#37117;&#26159;&#32479;&#35745;&#22411;&#30340;&#65292;&#32447;&#24615;&#20559;&#32622;&#26159;&#22266;&#23450;&#30340;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20135;&#29983;&#30340;NLFA&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#23548;&#33268;&#20102;&#23545;HDI&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#25439;&#22833;&#12290;&#22312;&#20197;&#19978;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional and Incomplete (HDI) data is commonly encountered in big data-related applications like social network services systems, which are concerning the limited interactions among numerous nodes. Knowledge acquisition from HDI data is a vital issue in the domain of data science due to their embedded rich patterns like node behaviors, where the fundamental task is to perform HDI data representation learning. Nonnegative Latent Factor Analysis (NLFA) models have proven to possess the superiority to address this issue, where a linear bias incorporation (LBI) scheme is important in present the training overshooting and fluctuation, as well as preventing the model from premature convergence. However, existing LBI schemes are all statistic ones where the linear biases are fixed, which significantly restricts the scalability of the resultant NLFA model and results in loss of representation learning ability to HDI data. Motivated by the above discoveries, this paper innovatively pres
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Python&#21253;ELFI&#20013;&#30340;Robust Optimisation Monte Carlo&#65288;ROMC&#65289;&#26041;&#27861;&#30340;&#23454;&#29616;&#12290;ROMC&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#26080;&#20284;&#28982;&#20989;&#25968;&#25512;&#29702;&#65288;LFI&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#21152;&#26435;&#21518;&#39564;&#26679;&#26412;&#12290;&#23454;&#29616;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;LFI&#31639;&#27861;&#20351;&#29992;&#65292;&#20063;&#21487;&#20197;&#25903;&#25345;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.10612</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;Robust Optimisation Monte Carlo&#30340;Python&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
An Extendable Python Implementation of Robust Optimisation Monte Carlo. (arXiv:2309.10612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10612
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Python&#21253;ELFI&#20013;&#30340;Robust Optimisation Monte Carlo&#65288;ROMC&#65289;&#26041;&#27861;&#30340;&#23454;&#29616;&#12290;ROMC&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#26080;&#20284;&#28982;&#20989;&#25968;&#25512;&#29702;&#65288;LFI&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#21152;&#26435;&#21518;&#39564;&#26679;&#26412;&#12290;&#23454;&#29616;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;LFI&#31639;&#27861;&#20351;&#29992;&#65292;&#20063;&#21487;&#20197;&#25903;&#25345;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26080;&#27861;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#32479;&#35745;&#27169;&#22411;&#20013;&#36827;&#34892;&#25512;&#29702;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#26080;&#20284;&#28982;&#20989;&#25968;&#25512;&#29702;&#65288;LFI&#65289;&#26041;&#27861;&#20250;&#36935;&#21040;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Python&#21253;ELFI&#20013;LFI&#26041;&#27861;Robust Optimisation Monte Carlo&#65288;ROMC&#65289;&#30340;&#23454;&#29616;&#12290;ROMC&#26159;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#65288;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#65289;&#30340;LFI&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#21152;&#26435;&#21518;&#39564;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#20351;&#29992;&#12290;&#39318;&#20808;&#65292;&#31185;&#23398;&#23478;&#21487;&#20197;&#23558;&#20854;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;LFI&#31639;&#27861;&#20351;&#29992;&#65307;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;API&#65292;&#19982;ELFI&#30340;&#21407;&#21017;&#30456;&#21327;&#35843;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#21253;&#20013;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;ROMC&#32454;&#20998;&#20026;&#23396;&#31435;&#30340;&#32452;&#20214;&#65292;&#20197;&#25903;&#25345;&#21487;&#25193;&#23637;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#23454;&#39564;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;ROMC&#30340;&#19968;&#37096;&#20998;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#23454;&#29616;&#25972;&#20010;&#36807;&#31243;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#26223;&#19979;&#65292;ROMC&#37096;&#20998;&#21487;&#20197;&#23436;&#20840;&#24182;&#34892;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing inference in statistical models with an intractable likelihood is challenging, therefore, most likelihood-free inference (LFI) methods encounter accuracy and efficiency limitations. In this paper, we present the implementation of the LFI method Robust Optimisation Monte Carlo (ROMC) in the Python package ELFI. ROMC is a novel and efficient (highly-parallelizable) LFI framework that provides accurate weighted samples from the posterior. Our implementation can be used in two ways. First, a scientist may use it as an out-of-the-box LFI algorithm; we provide an easy-to-use API harmonized with the principles of ELFI, enabling effortless comparisons with the rest of the methods included in the package. Additionally, we have carefully split ROMC into isolated components for supporting extensibility. A researcher may experiment with novel method(s) for solving part(s) of ROMC without reimplementing everything from scratch. In both scenarios, the ROMC parts can run in a fully-paralle
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#23545;&#20855;&#26377;&#32473;&#23450;&#34892;&#26143;&#30340;&#20849;&#36712;&#36947;&#36816;&#21160;&#30340;&#23567;&#34892;&#26143;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#30740;&#31350;&#19982;&#20849;&#25391;&#30456;&#20851;&#30340;&#35282;&#24230;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#20102;&#23545;&#23567;&#34892;&#26143;&#20849;&#36712;&#36947;&#36816;&#21160;&#30340;&#26377;&#25928;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.10603</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20849;&#36712;&#36947;&#36816;&#21160;&#30340;&#23567;&#34892;&#26143;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Asteroids co-orbital motion classification based on Machine Learning. (arXiv:2309.10603v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#23545;&#20855;&#26377;&#32473;&#23450;&#34892;&#26143;&#30340;&#20849;&#36712;&#36947;&#36816;&#21160;&#30340;&#23567;&#34892;&#26143;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#30740;&#31350;&#19982;&#20849;&#25391;&#30456;&#20851;&#30340;&#35282;&#24230;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#20102;&#23545;&#23567;&#34892;&#26143;&#20849;&#36712;&#36947;&#36816;&#21160;&#30340;&#26377;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#20855;&#26377;&#32473;&#23450;&#34892;&#26143;&#30340;&#20849;&#36712;&#36947;&#36816;&#21160;&#30340;&#23567;&#34892;&#26143;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19982;&#34892;&#26143;&#30340;&#24179;&#22343;&#36816;&#21160;&#20849;&#25391;&#26377;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#36816;&#21160;&#65292;&#20998;&#21035;&#26159;Tadpole&#65288;&#34636;&#34474;&#22411;&#65289;&#65292;Horseshoe&#65288;&#39532;&#36420;&#22411;&#65289;&#21644;Quasi-satellite&#65288;&#20934;&#21355;&#26143;&#22411;&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#20026;&#30495;&#23454;&#25968;&#25454;&#38598;&#65288;&#37319;&#29992;JPL Horizons&#31995;&#32479;&#20013;&#30340;&#30495;&#23454;&#23567;&#34892;&#26143;&#26143;&#21382;&#65289;&#12289;&#29702;&#24819;&#25968;&#25454;&#38598;&#21644;&#25200;&#21160;&#25968;&#25454;&#38598;&#65288;&#22343;&#20026;&#36890;&#36807;&#20256;&#25773;&#32771;&#34385;&#19981;&#21516;&#21160;&#21147;&#31995;&#32479;&#30340;&#21021;&#22987;&#26465;&#20214;&#24471;&#21040;&#30340;&#27169;&#25311;&#25968;&#25454;&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35757;&#32451;&#21644;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#23545;&#19982;&#20849;&#25391;&#30456;&#20851;&#30340;&#35282;&#24230;&#21464;&#37327;theta&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#30740;&#31350;&#65292;&#35813;&#27969;&#31243;&#30001;&#25968;&#25454;&#21019;&#24314;&#21644;&#27880;&#37322;&#12289;&#22522;&#20110;tsfresh&#21253;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#25552;&#21462;&#65288;&#21487;&#33021;&#21253;&#25324;&#36873;&#25321;&#21644;&#26631;&#20934;&#21270;&#65289;&#20197;&#21450;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#38477;&#32500;&#21644;&#20998;&#31867;&#32452;&#25104;&#12290;&#36825;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20998;&#31867;&#23567;&#34892;&#26143;&#30340;&#20849;&#36712;&#36947;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore how to classify asteroids in co-orbital motion with a given planet using Machine Learning. We consider four different kinds of motion in mean motion resonance with the planet, nominally Tadpole, Horseshoe and Quasi-satellite, building 3 datasets defined as Real (taking the ephemerides of real asteroids from the JPL Horizons system), Ideal and Perturbed (both simulated, obtained by propagating initial conditions considering two different dynamical systems) for training and testing the Machine Learning algorithms in different conditions.  The time series of the variable theta (angle related to the resonance) are studied with a data analysis pipeline defined ad hoc for the problem and composed by: data creation and annotation, time series features extraction thanks to the tsfresh package (potentially followed by selection and standardization) and the application of Machine Learning algorithms for Dimensionality Reduction and Classification. Such approach, based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Neural Metamaterial Networks (NMN)&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#28369;&#31070;&#32463;&#34920;&#31034;&#32534;&#30721;&#25972;&#20010;&#36229;&#26448;&#26009;&#23478;&#26063;&#30340;&#38750;&#32447;&#24615;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#20174;&#32467;&#26500;&#21442;&#25968;&#21040;&#24615;&#33021;&#31354;&#38388;&#30340;&#24179;&#28369;&#26144;&#23556;&#65292;&#36866;&#21512;&#36827;&#34892;&#26799;&#24230;&#20248;&#21270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#38750;&#32447;&#24615;&#26448;&#26009;&#35774;&#35745;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.10600</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#36229;&#26448;&#26009;&#32593;&#32476;&#29992;&#20110;&#38750;&#32447;&#24615;&#26448;&#26009;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Metamaterial Networks for Nonlinear Material Design. (arXiv:2309.10600v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Neural Metamaterial Networks (NMN)&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#28369;&#31070;&#32463;&#34920;&#31034;&#32534;&#30721;&#25972;&#20010;&#36229;&#26448;&#26009;&#23478;&#26063;&#30340;&#38750;&#32447;&#24615;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#20174;&#32467;&#26500;&#21442;&#25968;&#21040;&#24615;&#33021;&#31354;&#38388;&#30340;&#24179;&#28369;&#26144;&#23556;&#65292;&#36866;&#21512;&#36827;&#34892;&#26799;&#24230;&#20248;&#21270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#38750;&#32447;&#24615;&#26448;&#26009;&#35774;&#35745;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#12289;&#21307;&#23398;&#12289;&#26426;&#22120;&#20154;&#23398;&#31561;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#23450;&#21046;&#21270;&#26426;&#26800;&#29305;&#24615;&#30340;&#38750;&#32447;&#24615;&#36229;&#26448;&#26009;&#20855;&#26377;&#24456;&#22810;&#24212;&#29992;&#12290;&#34429;&#28982;&#24314;&#27169;&#23427;&#20204;&#30340;&#23439;&#35266;&#21147;&#23398;&#34892;&#20026;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#26159;&#25214;&#21040;&#33021;&#22815;&#23454;&#29616;&#39640;&#27700;&#24179;&#24615;&#33021;&#30446;&#26631;&#30340;&#32467;&#26500;&#21442;&#25968;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#20803;&#36229;&#26448;&#26009;&#32593;&#32476;&#65288;NMN&#65289;- &#19968;&#31181;&#33021;&#22815;&#32534;&#30721;&#25972;&#20010;&#36229;&#26448;&#26009;&#23478;&#26063;&#30340;&#38750;&#32447;&#24615;&#21147;&#23398;&#30340;&#24179;&#28369;&#31070;&#32463;&#34920;&#31034;&#12290;&#32473;&#23450;&#32467;&#26500;&#21442;&#25968;&#20316;&#20026;&#36755;&#20837;&#65292;NMN&#36820;&#22238;&#36830;&#32493;&#21487;&#24494;&#30340;&#24212;&#21464;&#33021;&#23494;&#24230;&#20989;&#25968;&#65292;&#20174;&#32780;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#20445;&#23432;&#21147;&#12290;&#34429;&#28982;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;NMN&#19981;&#20250;&#32487;&#25215;&#26377;&#38480;&#20803;&#32593;&#26684;&#20013;&#25299;&#25169;&#21464;&#21270;&#23548;&#33268;&#30340;&#19981;&#36830;&#32493;&#24615;&#12290;&#23427;&#20204;&#25552;&#20379;&#30340;&#26159;&#19968;&#20010;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#24615;&#33021;&#31354;&#38388;&#30340;&#24179;&#28369;&#26144;&#23556;&#65292;&#23436;&#20840;&#21487;&#24494;&#65292;&#38750;&#24120;&#36866;&#21512;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#23558;&#36870;&#21521;&#26448;&#26009;&#35774;&#35745;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#38750;&#32447;&#24615;&#31243;ryn
&lt;/p&gt;
&lt;p&gt;
Nonlinear metamaterials with tailored mechanical properties have applications in engineering, medicine, robotics, and beyond. While modeling their macromechanical behavior is challenging in itself, finding structure parameters that lead to ideal approximation of high-level performance goals is a challenging task. In this work, we propose Neural Metamaterial Networks (NMN) -- smooth neural representations that encode the nonlinear mechanics of entire metamaterial families. Given structure parameters as input, NMN return continuously differentiable strain energy density functions, thus guaranteeing conservative forces by construction. Though trained on simulation data, NMN do not inherit the discontinuities resulting from topological changes in finite element meshes. They instead provide a smooth map from parameter to performance space that is fully differentiable and thus well-suited for gradient-based optimization. On this basis, we formulate inverse material design as a nonlinear prog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#21644;&#26426;&#22120;&#32763;&#35793;&#22120;&#23545;&#30693;&#35782;&#22270;&#35889;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#40784;&#31574;&#30053;&#65292;&#29983;&#25104;&#20102;&#26377;&#25490;&#21517;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21270;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.10598</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#28145;&#24230;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Cross-Language Entity Alignment. (arXiv:2309.10598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#21644;&#26426;&#22120;&#32763;&#35793;&#22120;&#23545;&#30693;&#35782;&#22270;&#35889;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#40784;&#31574;&#30053;&#65292;&#29983;&#25104;&#20102;&#26377;&#25490;&#21517;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21270;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#30456;&#21516;&#35821;&#20041;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#32467;&#21512;&#26426;&#22120;&#32763;&#35793;&#22120;&#26469;&#32534;&#30721;&#30693;&#35782;&#22270;&#35889;&#25991;&#26412;&#65292;&#20943;&#23569;&#23545;&#26631;&#31614;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#19982;&#20165;&#24378;&#35843;&#20840;&#23616;&#25110;&#23616;&#37096;&#23545;&#40784;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#20004;&#31181;&#23545;&#40784;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#23545;&#40784;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#20108;&#20998;&#21305;&#37197;&#38382;&#39064;&#65292;&#28982;&#21518;&#37319;&#29992;&#37325;&#26032;&#20132;&#25442;&#30340;&#24605;&#24819;&#26469;&#23436;&#25104;&#23545;&#40784;&#12290;&#19982;&#20165;&#32473;&#20986;&#19968;&#20010;&#26368;&#20248;&#35299;&#30340;&#20256;&#32479;&#20108;&#20998;&#21305;&#37197;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#29983;&#25104;&#20102;&#26377;&#25490;&#21517;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#35768;&#22810;&#28508;&#22312;&#30340;&#19979;&#28216;&#20219;&#21153;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20108;&#20998;&#21305;&#37197;&#36807;&#31243;&#20013;&#36866;&#24212;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20248;&#21270;&#65288;&#26368;&#23567;&#21644;&#26368;&#22823;&#65289;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual entity alignment is the task of finding the same semantic entities from different language knowledge graphs. In this paper, we propose a simple and novel unsupervised method for cross-language entity alignment. We utilize the deep learning multi-language encoder combined with a machine translator to encode knowledge graph text, which reduces the reliance on label data. Unlike traditional methods that only emphasize global or local alignment, our method simultaneously considers both alignment strategies. We first view the alignment task as a bipartite matching problem and then adopt the re-exchanging idea to accomplish alignment. Compared with the traditional bipartite matching algorithm that only gives one optimal solution, our algorithm generates ranked matching results which enabled many potentials downstream tasks. Additionally, our method can adapt two different types of optimization (minimal and maximal) in the bipartite matching process, which provides more flexibil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31526;&#21495;&#38899;&#20048;&#30340;&#20027;&#39064;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#27969;&#31243;&#23398;&#20064;&#20027;&#39064;&#21644;&#20854;&#21464;&#21270;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20114;&#34917;&#20805;&#65292;&#20351;&#31934;&#30830;&#29575;-&#21484;&#22238;&#29575;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#25552;&#21319;&#20102;12.6%&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#33719;&#24471;&#30340;&#20027;&#39064;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10597</link><description>&lt;p&gt;
&#38754;&#21521;&#31526;&#21495;&#38899;&#20048;&#30340;&#20027;&#39064;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Motif-Centric Representation Learning for Symbolic Music. (arXiv:2309.10597v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10597
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31526;&#21495;&#38899;&#20048;&#30340;&#20027;&#39064;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#27969;&#31243;&#23398;&#20064;&#20027;&#39064;&#21644;&#20854;&#21464;&#21270;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20114;&#34917;&#20805;&#65292;&#20351;&#31934;&#30830;&#29575;-&#21484;&#22238;&#29575;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#25552;&#21319;&#20102;12.6%&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#33719;&#24471;&#30340;&#20027;&#39064;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#20027;&#39064;&#20316;&#20026;&#38899;&#20048;&#32452;&#21512;&#30340;&#27010;&#24565;&#26500;&#24314;&#22359;&#65292;&#22312;&#38899;&#20048;&#32467;&#26500;&#20998;&#26512;&#21644;&#33258;&#21160;&#20316;&#26354;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20154;&#31867;&#21548;&#20247;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#20027;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#35745;&#31639;&#27169;&#22411;&#22312;&#34920;&#31034;&#20027;&#39064;&#21450;&#20854;&#21457;&#23637;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#36825;&#26159;&#22240;&#20026;&#20027;&#39064;&#30340;&#29305;&#24615;&#26159;&#38544;&#21547;&#30340;&#65292;&#32780;&#20027;&#39064;&#21464;&#21270;&#30340;&#22810;&#26679;&#24615;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#37325;&#22797;&#21644;&#35843;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#23398;&#20064;&#20027;&#39064;&#19982;&#20854;&#21464;&#21270;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#65292;&#20351;&#29992;Siamese&#32593;&#32476;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#27969;&#31243;&#12290;&#37319;&#29992;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;VICReg&#26041;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#24494;&#35843;&#12290;&#23545;&#22522;&#20110;&#26816;&#32034;&#30340;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20114;&#34917;&#65292;&#20351;&#22312;&#31934;&#30830;&#29575;-&#21484;&#22238;&#29575;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#25552;&#21319;&#20102;12.6%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21487;&#35270;&#21270;&#33719;&#21462;&#30340;&#20027;&#39064;&#34920;&#31034;&#65292;&#25552;&#20379;&#30452;&#35266;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music motif, as a conceptual building block of composition, is crucial for music structure analysis and automatic composition. While human listeners can identify motifs easily, existing computational models fall short in representing motifs and their developments. The reason is that the nature of motifs is implicit, and the diversity of motif variations extends beyond simple repetitions and modulations. In this study, we aim to learn the implicit relationship between motifs and their variations via representation learning, using the Siamese network architecture and a pretraining and fine-tuning pipeline. A regularization-based method, VICReg, is adopted for pretraining, while contrastive learning is used for fine-tuning. Experimental results on a retrieval-based task show that these two methods complement each other, yielding an improvement of 12.6% in the area under the precision-recall curve. Lastly, we visualize the acquired motif representations, offering an intuitive comprehension
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#31995;&#32479;&#20013;&#30340;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#21512;&#21305;&#37197;&#29702;&#35770;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#31227;&#21160;&#21333;&#20803;&#30340;&#20010;&#20307;&#30446;&#26631;&#65292;&#21516;&#26102;&#22312;&#32447;&#23398;&#20064;&#31227;&#21160;&#21333;&#20803;&#30340;&#21162;&#21147;&#65292;&#35299;&#20915;&#20102;&#24863;&#30693;&#24179;&#21488;&#21644;&#31227;&#21160;&#21333;&#20803;&#20043;&#38388;&#30340;&#30446;&#26631;&#20914;&#31361;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21019;&#26032;&#30340;"&#26080;&#24863;&#30693;"&#26426;&#21046;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#24182;&#20943;&#23569;&#20102;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2309.10594</link><description>&lt;p&gt;
&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#20013;&#30340;&#20219;&#21153;&#20998;&#37197;&#28216;&#25103;&#20013;&#30340;&#21435;&#20013;&#24515;&#21270;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Online Learning in Task Assignment Games for Mobile Crowdsensing. (arXiv:2309.10594v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#31995;&#32479;&#20013;&#30340;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#21512;&#21305;&#37197;&#29702;&#35770;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#31227;&#21160;&#21333;&#20803;&#30340;&#20010;&#20307;&#30446;&#26631;&#65292;&#21516;&#26102;&#22312;&#32447;&#23398;&#20064;&#31227;&#21160;&#21333;&#20803;&#30340;&#21162;&#21147;&#65292;&#35299;&#20915;&#20102;&#24863;&#30693;&#24179;&#21488;&#21644;&#31227;&#21160;&#21333;&#20803;&#20043;&#38388;&#30340;&#30446;&#26631;&#20914;&#31361;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21019;&#26032;&#30340;"&#26080;&#24863;&#30693;"&#26426;&#21046;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#24182;&#20943;&#23569;&#20102;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#31995;&#32479;&#20013;&#21327;&#35843;&#25968;&#25454;&#25910;&#38598;&#30340;&#38382;&#39064;&#12290;&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#24179;&#21488;&#36880;&#27493;&#21457;&#24067;&#24863;&#30693;&#20219;&#21153;&#32473;&#21487;&#29992;&#31227;&#21160;&#21333;&#20803;&#65292;&#24182;&#36890;&#36807;&#22238;&#20256;&#24863;&#30693;&#25253;&#20215;&#26469;&#34920;&#31034;&#23427;&#20204;&#21442;&#19982;&#20219;&#21153;&#30340;&#24847;&#24895;&#12290;&#26681;&#25454;&#25152;&#25910;&#21040;&#30340;&#25253;&#20215;&#65292;&#24863;&#30693;&#24179;&#21488;&#20915;&#23450;&#20219;&#21153;&#20998;&#37197;&#12290;&#31283;&#23450;&#30340;&#20219;&#21153;&#20998;&#37197;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#25361;&#25112;&#65306;&#24863;&#30693;&#24179;&#21488;&#21644;&#31227;&#21160;&#21333;&#20803;&#20043;&#38388;&#30340;&#30446;&#26631;&#20914;&#31361;&#65292;&#20197;&#21450;&#23545;&#31227;&#21160;&#21333;&#20803;&#25152;&#38656;&#21162;&#21147;&#21644;&#20559;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21305;&#37197;&#29702;&#35770;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#26032;&#39062;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;"&#30896;&#25758;&#36991;&#20813;&#22810;&#33218;&#32769;&#34382;&#26426;-&#31574;&#30053;&#26080;&#24863;&#30693;"&#65288;CA-MAB-SFS&#65289;&#12290;&#23558;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#24314;&#27169;&#20026;&#32771;&#34385;&#24863;&#30693;&#24179;&#21488;&#21644;&#31227;&#21160;&#21333;&#20803;&#30340;&#20010;&#20307;&#30446;&#26631;&#30340;&#21305;&#37197;&#28216;&#25103;&#65292;&#32780;&#31227;&#21160;&#21333;&#20803;&#22312;&#32447;&#23398;&#20064;&#20854;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#30340;"&#26080;&#24863;&#30693;"&#26426;&#21046;&#26174;&#33879;&#25552;&#39640;&#20102;&#31227;&#21160;&#21333;&#20803;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of coordinated data collection is studied for a mobile crowdsensing (MCS) system. A mobile crowdsensing platform (MCSP) sequentially publishes sensing tasks to the available mobile units (MUs) that signal their willingness to participate in a task by sending sensing offers back to the MCSP. From the received offers, the MCSP decides the task assignment. A stable task assignment must address two challenges: the MCSP's and MUs' conflicting goals, and the uncertainty about the MUs' required efforts and preferences. To overcome these challenges a novel decentralized approach combining matching theory and online learning, called collision-avoidance multi-armed bandit with strategic free sensing (CA-MAB-SFS), is proposed. The task assignment problem is modeled as a matching game considering the MCSP's and MUs' individual goals while the MUs learn their efforts online. Our innovative "free-sensing" mechanism significantly improves the MU's learning process while reducing collision
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#21363;&#25915;&#20987;&#32773;&#36890;&#36807;&#25805;&#32437;&#19981;&#30830;&#23450;&#24230;&#20272;&#35745;&#26469;&#21066;&#24369;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#25928;&#26524;&#65292;&#26080;&#35770;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#22914;&#20309;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#23041;&#32961;&#27169;&#22411;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#25915;&#20987;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.10586</link><description>&lt;p&gt;
&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks Against Uncertainty Quantification. (arXiv:2309.10586v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#21363;&#25915;&#20987;&#32773;&#36890;&#36807;&#25805;&#32437;&#19981;&#30830;&#23450;&#24230;&#20272;&#35745;&#26469;&#21066;&#24369;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#25928;&#26524;&#65292;&#26080;&#35770;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#22914;&#20309;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#23041;&#32961;&#27169;&#22411;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#25915;&#20987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#34987;&#23545;&#25239;&#24615;&#31034;&#20363;&#25152;&#27450;&#39575;&#65292;&#21363;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#36755;&#20837;&#25200;&#21160;&#26469;&#36843;&#20351;&#27169;&#22411;&#36755;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#20551;&#35774;&#36825;&#20123;&#25915;&#20987;&#26174;&#31034;&#27604;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#34920;&#26126;&#38024;&#23545;&#20943;&#23569;&#19981;&#30830;&#23450;&#24230;&#20272;&#35745;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#21487;&#20197;&#36731;&#26131;&#32469;&#36807;&#36825;&#31181;&#38450;&#24481;&#26426;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#21478;&#19968;&#31181;&#23545;&#25239;&#24615;&#22330;&#26223;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#20173;&#28982;&#26377;&#20852;&#36259;&#25805;&#32437;&#19981;&#30830;&#23450;&#24230;&#20272;&#35745;&#65292;&#20294;&#19981;&#32771;&#34385;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#65307;&#20855;&#20307;&#32780;&#35328;&#65292;&#30446;&#26631;&#26159;&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#34987;&#19979;&#28216;&#27169;&#22359;&#25110;&#20154;&#25805;&#20316;&#21592;&#20351;&#29992;&#26102;&#21066;&#24369;&#20854;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#25105;&#20204;&#65306;\textit{(i)}&#20026;&#38024;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25915;&#20987;&#35774;&#35745;&#20102;&#19968;&#20010;&#23041;&#32961;&#27169;&#22411;&#65307;\textit{(ii)}&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#25915;&#20987;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Machine-learning models can be fooled by adversarial examples, i.e., carefully-crafted input perturbations that force models to output wrong predictions. While uncertainty quantification has been recently proposed to detect adversarial inputs, under the assumption that such attacks exhibit a higher prediction uncertainty than pristine data, it has been shown that adaptive attacks specifically aimed at reducing also the uncertainty estimate can easily bypass this defense mechanism. In this work, we focus on a different adversarial scenario in which the attacker is still interested in manipulating the uncertainty estimate, but regardless of the correctness of the prediction; in particular, the goal is to undermine the use of machine-learning models when their outputs are consumed by a downstream module or by a human operator. Following such direction, we: \textit{(i)} design a threat model for attacks targeting uncertainty quantification; \textit{(ii)} devise different attack strategies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;&#31995;&#32479;&#65288;PDRL&#65289;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30417;&#27979;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#31574;&#30053;&#26469;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2309.10576</link><description>&lt;p&gt;
PDRL&#65306;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring. (arXiv:2309.10576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;&#31995;&#32479;&#65288;PDRL&#65289;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30417;&#27979;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#31574;&#30053;&#26469;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#20174;&#20197;&#24448;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#20570;&#20986;&#33258;&#36866;&#24212;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;&#22312;&#30417;&#25511;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#34987;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#30417;&#25511;&#24212;&#29992;&#22823;&#22810;&#26159;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#35757;&#32451;&#26631;&#31614;&#25968;&#25454;&#65292;&#26080;&#27861;&#22312;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#20570;&#20986;&#33258;&#36866;&#24212;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#21363;&#20855;&#26377;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#39044;&#27979;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;PDRL&#65289;&#65292;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29615;&#22659;&#12290;&#35813;&#25552;&#20986;&#30340;&#36890;&#29992;&#26694;&#26550;&#21487;&#20197;&#23481;&#32435;&#34394;&#25311;&#28145;&#24230; Q &#32593;&#32476;&#65288;DQN&#65289;&#26234;&#33021;&#20307;&#65292;&#20197;&#30417;&#27979;&#22797;&#26434;&#29615;&#22659;&#30340;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#26681;&#25454;&#26126;&#30830;&#23450;&#20041;&#30340;&#22870;&#21169;&#31574;&#30053;&#20351;&#26234;&#33021;&#20307;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#21516;&#26102;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#12290;&#22312;&#35780;&#20272;&#35813;&#26694;&#26550;&#30340;&#36807;&#31243;&#20013;&#65292;&#37096;&#32626;&#20102;&#19977;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20197;&#30417;&#27979;&#36890;&#36807; BiLSTM &#27169;&#22411;&#39044;&#27979;&#30340;&#21463;&#35797;&#32773;&#26410;&#26469;&#30340;&#24515;&#29575;&#12289;&#21628;&#21560;&#29575;&#21644;&#20307;&#28201;&#12290;&#38543;&#30528;&#27599;&#27425;&#36845;&#20195;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#23454;&#38469;&#21453;&#39304;&#35843;&#25972;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been increasingly applied in monitoring applications because of its ability to learn from previous experiences and can make adaptive decisions. However, existing machine learning-based health monitoring applications are mostly supervised learning algorithms, trained on labels and they cannot make adaptive decisions in an uncertain complex environment. This study proposes a novel and generic system, predictive deep reinforcement learning (PDRL) with multiple RL agents in a time series forecasting environment. The proposed generic framework accommodates virtual Deep Q Network (DQN) agents to monitor predicted future states of a complex environment with a well-defined reward policy so that the agent learns existing knowledge while maximizing their rewards. In the evaluation process of the proposed framework, three DRL agents were deployed to monitor a subject's future heart rate, respiration, and temperature predicted using a BiLSTM model. With each iteration, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20219;&#21153;&#22270;&#31163;&#36733;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.10569</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20219;&#21153;&#22270;&#31163;&#36733;
&lt;/p&gt;
&lt;p&gt;
Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing. (arXiv:2309.10569v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20219;&#21153;&#22270;&#31163;&#36733;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20854;&#20013;&#21253;&#21547;&#30340;&#20381;&#36182;&#20219;&#21153;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#36890;&#24120;&#20855;&#26377;&#20302;&#24310;&#36831;&#35201;&#27714;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#24613;&#21095;&#22686;&#21152;&#12290;&#38543;&#30528;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#30340;&#20986;&#29616;&#65292;&#23558;&#24212;&#29992;&#31243;&#24207;&#20219;&#21153;&#21368;&#36733;&#21040;&#37096;&#32626;&#22312;&#31227;&#21160;&#32593;&#32476;&#36793;&#32536;&#30340;&#23567;&#22411;&#35774;&#22791;&#19978;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#29992;&#25143;&#20307;&#39564;&#25104;&#20026;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;MEC&#29615;&#22659;&#26159;&#21160;&#24577;&#30340;&#65292;&#22823;&#22810;&#25968;&#20381;&#36182;&#19987;&#23478;&#30693;&#35782;&#25110;&#20934;&#30830;&#30340;&#20998;&#26512;&#27169;&#22411;&#30340;&#29616;&#26377;&#24037;&#20316;&#22312;&#20219;&#21153;&#22270;&#31163;&#36733;&#26041;&#38754;&#26080;&#27861;&#23436;&#20840;&#36866;&#24212;&#36825;&#31181;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;MEC&#20013;&#30340;&#20219;&#21153;&#22270;&#31163;&#36733;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#31163;&#36733;&#30340;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#19968;&#20010;Markov&#20915;&#31574;&#36807;&#31243;&#65288;Markov Decision Process&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Mark
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21475;&#35821;&#35782;&#21035;&#26041;&#27861;MuSeLI&#65292;&#21033;&#29992;&#35270;&#39057;&#26631;&#39064;&#12289;&#25551;&#36848;&#21644;&#22320;&#29702;&#20301;&#32622;&#31561;&#20803;&#25968;&#25454;&#26469;&#22686;&#24378;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#22312;&#20004;&#20010;YouTube&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10567</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24314;&#27169;&#29992;&#20110;&#21475;&#35821;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multimodal Modeling For Spoken Language Identification. (arXiv:2309.10567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21475;&#35821;&#35782;&#21035;&#26041;&#27861;MuSeLI&#65292;&#21033;&#29992;&#35270;&#39057;&#26631;&#39064;&#12289;&#25551;&#36848;&#21644;&#22320;&#29702;&#20301;&#32622;&#31561;&#20803;&#25968;&#25454;&#26469;&#22686;&#24378;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#22312;&#20004;&#20010;YouTube&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#35782;&#21035;&#26159;&#25351;&#22312;&#32473;&#23450;&#30340;&#35805;&#35821;&#20013;&#33258;&#21160;&#39044;&#27979;&#21475;&#35821;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#34987;&#24314;&#27169;&#20026;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#12290;&#20197;&#24448;&#30340;&#25216;&#26415;&#37117;&#23616;&#38480;&#20110;&#21333;&#19968;&#27169;&#24577;&#65307;&#28982;&#32780;&#65292;&#22312;&#35270;&#39057;&#25968;&#25454;&#20013;&#65292;&#23384;&#22312;&#35768;&#22810;&#20854;&#20182;&#20803;&#25968;&#25454;&#65292;&#36825;&#20123;&#20803;&#25968;&#25454;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#20250;&#26377;&#30410;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MuSeLI&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#21475;&#35821;&#35782;&#21035;&#26041;&#27861;&#65292;&#23427;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;&#21508;&#31181;&#20803;&#25968;&#25454;&#28304;&#26469;&#22686;&#24378;&#35821;&#35328;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35832;&#22914;&#35270;&#39057;&#26631;&#39064;&#12289;&#25551;&#36848;&#21644;&#22320;&#29702;&#20301;&#32622;&#31561;&#20803;&#25968;&#25454;&#25552;&#20379;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#33021;&#22815;&#35782;&#21035;&#22810;&#23186;&#20307;&#24405;&#21046;&#30340;&#21475;&#35821;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;YouTube&#35270;&#39057;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#22312;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#28040;&#34701;&#30740;&#31350;&#65292;&#25551;&#36848;&#20102;&#27599;&#31181;&#27169;&#24577;&#23545;&#35821;&#35328;&#35782;&#21035;&#30340;&#29420;&#29305;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.10563</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents. (arXiv:2309.10563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21450;&#20854;&#35299;&#37322;&#24120;&#24120;&#38754;&#20020;&#38271;&#36798;&#25968;&#19975;&#23383;&#30340;&#26696;&#20363;&#25991;&#20214;&#21644;&#38750;&#32479;&#19968;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#22312;&#27809;&#26377;&#32467;&#26500;&#26631;&#27880;&#30340;&#25991;&#20214;&#19978;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#23558;&#36825;&#19968;&#38382;&#39064;&#23450;&#20041;&#20026;&#8220;&#31232;&#32570;&#26631;&#27880;&#27861;&#24459;&#25991;&#20214;&#8221;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;MESc&#65288;&#22522;&#20110;&#22810;&#38454;&#27573;&#32534;&#30721;&#22120;&#30340;&#24102;&#32858;&#31867;&#30340;&#30417;&#30563;&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#26694;&#26550;&#26469;&#25506;&#32034;&#32570;&#20047;&#32467;&#26500;&#20449;&#24687;&#21644;&#38271;&#25991;&#26723;&#30340;&#29305;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#65292;&#20174;&#33258;&#23450;&#20041;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#21518;&#22235;&#20010;&#23618;&#20013;&#25552;&#21462;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#24182;&#35797;&#22270;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#26469;&#36817;&#20284;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21478;&#19968;&#32452;Transformer&#32534;&#30721;&#22120;&#23618;&#23398;&#20064;&#37096;&#20998;&#20043;&#38388;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#21644;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10553</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#23398;&#20064;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#30340;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Hybrid State Space-based Learning for Sequential Data Prediction with Joint Optimization. (arXiv:2309.10553v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#21644;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;/&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#24418;&#24335;&#30340;&#32852;&#21512;&#26426;&#21046;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#20256;&#32479;&#38750;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#20013;&#29305;&#23450;&#39046;&#22495;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#32452;&#20998;&#30340;&#26377;&#25928;&#28151;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#36882;&#24402;&#32467;&#26500;&#20174;&#21407;&#22987;&#39034;&#24207;&#24207;&#21015;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20256;&#32479;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#23395;&#33410;&#24615;&#12289;&#36235;&#21183;&#31561;&#12290;&#19982;&#29616;&#26377;&#30340;&#38598;&#25104;&#25110;&#28151;&#21512;&#27169;&#22411;&#36890;&#24120;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#65292;&#32780;&#19988;&#30001;&#20110;&#24314;&#27169;&#30340;&#20998;&#38548;&#25110;&#29420;&#31435;&#35757;&#32451;&#32780;&#38750;&#26368;&#20248;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#25991;&#29486;&#20013;&#37319;&#29992;&#32852;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#32852;&#21512;&#20248;&#21270;&#22686;&#24378;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LSTM&#65289;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#21644;ARMA&#31995;&#21015;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;SARIMAX&#65289;&#36827;&#34892;&#26377;&#25928;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate nonlinear prediction/regression in an online setting and introduce a hybrid model that effectively mitigates, via a joint mechanism through a state space formulation, the need for domain-specific feature engineering issues of conventional nonlinear prediction models and achieves an efficient mix of nonlinear and linear components. In particular, we use recursive structures to extract features from raw sequential sequences and a traditional linear time series model to deal with the intricacies of the sequential data, e.g., seasonality, trends. The state-of-the-art ensemble or hybrid models typically train the base models in a disjoint manner, which is not only time consuming but also sub-optimal due to the separation of modeling or independent training. In contrast, as the first time in the literature, we jointly optimize an enhanced recurrent neural network (LSTM) for automatic feature extraction from raw data and an ARMA-family time series model (SARIMAX) for effectivel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#36890;&#36807;&#32771;&#34385;&#39044;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#21333;&#35789;&#30340;&#37051;&#22495;&#26469;&#30830;&#23450;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26426;&#21046;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#65292;&#21516;&#26102;&#20445;&#35777;&#26356;&#39640;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.10551</link><description>&lt;p&gt;
&#29992;&#20110;&#38745;&#24577;&#35789;&#23884;&#20837;&#30340;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings. (arXiv:2309.10551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#36890;&#36807;&#32771;&#34385;&#39044;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#21333;&#35789;&#30340;&#37051;&#22495;&#26469;&#30830;&#23450;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26426;&#21046;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#65292;&#21516;&#26102;&#20445;&#35777;&#26356;&#39640;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39044;&#35757;&#32451;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#21333;&#35789;&#37051;&#22495;&#30340;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#65288;NADP&#65289;&#26426;&#21046;&#65292;&#20197;&#30830;&#23450;&#20445;&#35777;&#25351;&#23450;&#38544;&#31169;&#32423;&#21035;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#23427;&#20204;&#30340;&#23884;&#20837;&#26500;&#24314;&#21333;&#35789;&#30340;&#26368;&#36817;&#37051;&#22270;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#36830;&#36890;&#20998;&#37327;&#65288;&#21363;&#37051;&#22495;&#65289;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#26681;&#25454;&#35813;&#37051;&#22495;&#20013;&#30340;&#21333;&#35789;&#38598;&#21512;&#20998;&#21035;&#23545;&#21333;&#35789;&#24212;&#29992;&#19981;&#21516;&#27700;&#24179;&#30340;&#39640;&#26031;&#22122;&#22768;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;NADP&#26426;&#21046;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#22810;&#20010;&#20808;&#21069;&#25552;&#20986;&#30340;DP&#26426;&#21046;&#65292;&#22914;&#25289;&#26222;&#25289;&#26031;&#12289;&#39640;&#26031;&#21644;&#39532;&#27663;&#36317;&#31163;&#65292;&#21516;&#26102;&#20445;&#35777;&#26356;&#39640;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism considering the neighbourhood of a word in a pretrained static word embedding space to determine the minimal amount of noise required to guarantee a specified privacy level. We first construct a nearest neighbour graph over the words using their embeddings, and factorise it into a set of connected components (i.e. neighbourhoods). We then separately apply different levels of Gaussian noise to the words in each neighbourhood, determined by the set of words in that neighbourhood. Experiments show that our proposed NADP mechanism consistently outperforms multiple previously proposed DP mechanisms such as Laplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while guaranteeing higher levels of privacy.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;(Mean Absolute Directional Loss&#65292;MADL)&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#24212;&#29992;&#20110;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#20013;&#30340;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20004;&#31181;&#19981;&#21516;&#36164;&#20135;&#31867;&#21035;&#30340;&#25968;&#25454;&#19978;&#39564;&#35777;&#65292;MADL&#20989;&#25968;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#24182;&#33719;&#24471;&#26356;&#26377;&#25928;&#30340;&#25237;&#36164;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.10546</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#29992;&#20110;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#24179;&#22343;&#32477;&#23545;&#26041;&#21521;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Mean Absolute Directional Loss as a New Loss Function for Machine Learning Problems in Algorithmic Investment Strategies. (arXiv:2309.10546v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10546
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;(Mean Absolute Directional Loss&#65292;MADL)&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#24212;&#29992;&#20110;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#20013;&#30340;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20004;&#31181;&#19981;&#21516;&#36164;&#20135;&#31867;&#21035;&#30340;&#25968;&#25454;&#19978;&#39564;&#35777;&#65292;MADL&#20989;&#25968;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#24182;&#33719;&#24471;&#26356;&#26377;&#25928;&#30340;&#25237;&#36164;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#65288;AIS&#65289;&#26500;&#24314;&#20013;&#65292;&#29992;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36866;&#24403;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#22343;&#32477;&#23545;&#26041;&#21521;&#25439;&#22833;&#65288;MADL&#65289;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#39044;&#27979;&#35823;&#24046;&#20989;&#25968;&#22312;&#20174;&#39044;&#27979;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#21019;&#24314;&#26377;&#25928;&#30340;&#20080;&#21334;&#20449;&#21495;&#26041;&#38754;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#20004;&#31181;&#19981;&#21516;&#36164;&#20135;&#31867;&#21035;&#65288;&#21152;&#23494;&#36135;&#24065;&#65306;&#27604;&#29305;&#24065;&#21644;&#22823;&#23447;&#21830;&#21697;&#65306;&#21407;&#27833;&#65289;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#20351;&#25105;&#20204;&#33021;&#22815;&#36873;&#25321;&#26356;&#22909;&#30340;LSTM&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;&#24182;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#19978;&#33719;&#24471;&#26356;&#26377;&#25928;&#30340;&#25237;&#36164;&#31574;&#30053;&#65292;&#30456;&#23545;&#20110;&#39118;&#38505;&#35843;&#25972;&#22238;&#25253;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the issue of an adequate loss function in the optimization of machine learning models used in the forecasting of financial time series for the purpose of algorithmic investment strategies (AIS) construction. We propose the Mean Absolute Directional Loss (MADL) function, solving important problems of classical forecast error functions in extracting information from forecasts to create efficient buy/sell signals in algorithmic investment strategies. Finally, based on the data from two different asset classes (cryptocurrencies: Bitcoin and commodities: Crude Oil), we show that the new loss function enables us to select better hyperparameters for the LSTM model and obtain more efficient investment strategies, with regard to risk-adjusted return metrics on the out-of-sample data.
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#21560;&#21462;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#25915;&#20987;&#65292;&#33021;&#22815;&#23558;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25552;&#21462;&#21040;&#19968;&#20010;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10544</link><description>&lt;p&gt;
&#27169;&#22411;&#21560;&#21462;: &#38024;&#23545;LLMs&#30340;&#19968;&#31181;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Model Leeching: An Extraction Attack Targeting LLMs. (arXiv:2309.10544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10544
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21560;&#21462;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#25915;&#20987;&#65292;&#33021;&#22815;&#23558;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25552;&#21462;&#21040;&#19968;&#20010;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21560;&#21462;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26032;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#33021;&#22815;&#23558;&#30446;&#26631;LLM&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25552;&#28860;&#21040;&#19968;&#20010;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;ChatGPT-3.5-Turbo&#20013;&#25552;&#21462;&#20219;&#21153;&#33021;&#21147;&#26469;&#28436;&#31034;&#25105;&#20204;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;73%&#30340;&#20934;&#30830;&#21305;&#37197;(EM)&#30456;&#20284;&#24615;&#20197;&#21450;75%&#30340;SQuAD EM&#20934;&#30830;&#29575;&#21644;87%&#30340;F1&#24471;&#20998;&#65292;&#20165;&#38656;50&#32654;&#20803;&#30340;API&#36153;&#29992;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36890;&#36807;&#27169;&#22411;&#21560;&#21462;&#25552;&#21462;&#30340;&#27169;&#22411;&#22312;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#26102;&#30340;&#21487;&#34892;&#24615;&#65292;&#24403;&#24212;&#29992;&#20110;ChatGPT-3.5-Turbo&#26102;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;11%&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Leeching is a novel extraction attack targeting Large Language Models (LLMs), capable of distilling task-specific knowledge from a target LLM into a reduced parameter model. We demonstrate the effectiveness of our attack by extracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match (EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%, respectively for only $50 in API cost. We further demonstrate the feasibility of adversarial attack transferability from an extracted model extracted via Model Leeching to perform ML attack staging against a target LLM, resulting in an 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#21644;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#28608;&#27963;&#22270;&#19978;&#24212;&#29992;&#21516;&#24577;&#21152;&#23494;&#65292;&#26377;&#25928;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#24182;&#25913;&#36827;&#20102;&#20197;&#24448;&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.10517</link><description>&lt;p&gt;
&#29233;&#36824;&#26159;&#24680;&#65311;&#20849;&#20139;&#36824;&#26159;&#20998;&#21106;&#65311;&#20351;&#29992;&#20998;&#21106;&#23398;&#20064;&#21644;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Love or Hate? Share or Split? Privacy-Preserving Training Using Split Learning and Homomorphic Encryption. (arXiv:2309.10517v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#21644;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#28608;&#27963;&#22270;&#19978;&#24212;&#29992;&#21516;&#24577;&#21152;&#23494;&#65292;&#26377;&#25928;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#24182;&#25913;&#36827;&#20102;&#20197;&#24448;&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#23398;&#20064;&#25216;&#26415;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#65292;&#20363;&#22914;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#65292;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23458;&#25143;&#31471;&#39318;&#20808;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#24212;&#29992;&#20854;&#25152;&#23646;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#20998;&#65292;&#29983;&#25104;&#28608;&#27963;&#22270;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#20197;&#32487;&#32493;&#35757;&#32451;&#36807;&#31243;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37325;&#24314;&#28608;&#27963;&#22270;&#21487;&#33021;&#20250;&#23548;&#33268;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20943;&#36731;SL&#38544;&#31169;&#27844;&#28431;&#30340;&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#35777;&#26126;&#26174;&#33879;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#20197;&#24448;&#30340;&#24037;&#20316;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;U&#22411;SL&#30340;&#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#21516;&#24577;&#21152;&#23494;&#25968;&#25454;&#19978;&#25805;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#23458;&#25143;&#31471;&#22312;&#23558;&#28608;&#27963;&#22270;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#20043;&#21069;&#23545;&#20854;&#36827;&#34892;&#21516;&#24577;&#21152;&#23494;&#65292;&#20174;&#32780;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25913;&#36827;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning (SL) is a new collaborative learning technique that allows participants, e.g. a client and a server, to train machine learning models without the client sharing raw data. In this setting, the client initially applies its part of the machine learning model on the raw data to generate activation maps and then sends them to the server to continue the training process. Previous works in the field demonstrated that reconstructing activation maps could result in privacy leakage of client data. In addition to that, existing mitigation techniques that overcome the privacy leakage of SL prove to be significantly worse in terms of accuracy. In this paper, we improve upon previous works by constructing a protocol based on U-shaped SL that can operate on homomorphically encrypted data. More precisely, in our approach, the client applies homomorphic encryption on the activation maps before sending them to the server, thus protecting user privacy. This is an important improvement that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#20998;&#21106;&#21644;&#21435;&#22122;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#65292;&#19988;&#33021;&#22815;&#22788;&#29702;&#39640;&#22122;&#22768;&#21644;&#36890;&#29992;&#32441;&#29702;&#65292;&#24182;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.10511</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#32852;&#21512;&#20998;&#21106;&#21644;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Single-Image based unsupervised joint segmentation and denoising. (arXiv:2309.10511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#20998;&#21106;&#21644;&#21435;&#22122;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#65292;&#19988;&#33021;&#22815;&#22788;&#29702;&#39640;&#22122;&#22768;&#21644;&#36890;&#29992;&#32441;&#29702;&#65292;&#24182;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#20998;&#21106;&#21644;&#21435;&#22122;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#20998;&#21106;&#26041;&#27861;&#30340;&#20248;&#21183;&#19982;&#33258;&#30417;&#30563;&#12289;&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#65292;&#19982;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#24211;&#30340;&#24773;&#20917;&#19979;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#22810;&#20010;&#26377;&#24847;&#20041;&#30340;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#20854;&#20013;&#21435;&#22122;&#21644;&#20998;&#21106;&#20197;&#19968;&#31181;&#30456;&#20114;&#21463;&#30410;&#30340;&#26041;&#24335;&#32806;&#21512;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#19982;&#33258;&#30417;&#30563;&#22270;&#20687;&#21435;&#22122;&#30340;&#29305;&#23450;&#32452;&#21512;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#30340;&#21333;&#22270;&#20687;&#22522;&#20110;&#21464;&#20998;&#20998;&#21106;&#26041;&#27861;&#23545;&#39640;&#22122;&#22768;&#25110;&#36890;&#29992;&#32441;&#29702;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26174;&#24494;&#38236;&#20013;&#21487;&#29992;&#30340;&#38750;&#24120;&#22024;&#26434;&#30340;&#22270;&#20687;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop an unsupervised method for the joint segmentation and denoising of a single image. To this end, we combine the advantages of a variational segmentation method with the power of a self-supervised, single-image based deep learning approach. One major strength of our method lies in the fact, that in contrast to data-driven methods, where huge amounts of labeled samples are necessary, our model can segment an image into multiple meaningful regions without any training database. Further, we introduce a novel energy functional in which denoising and segmentation are coupled in a way that both tasks benefit from each other. The limitations of existing single-image based variational segmentation methods, which are not capable of dealing with high noise or generic texture, are tackled by this specific combination with self-supervised image denoising. We propose a unified optimisation strategy and show that, especially for very noisy images available in microscopy, our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#20174;&#32780;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10505</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning End-to-End Channel Coding with Diffusion Models. (arXiv:2309.10505v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#20174;&#32780;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36817;&#20284;&#20449;&#36947;&#20998;&#24067;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;&#21508;&#31181;&#20449;&#36947;&#27169;&#22411;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25193;&#25955;&#27169;&#22411;&#31934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#65288;SER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of neural encoders via deep learning necessitates a differentiable channel model due to the backpropagation algorithm. This requirement can be sidestepped by approximating either the channel distribution or its gradient through pilot signals in real-world scenarios. The initial approach draws upon the latest advancements in image generation, utilizing generative adversarial networks (GANs) or their enhanced variants to generate channel distributions. In this paper, we address this channel approximation challenge with diffusion models, which have demonstrated high sample quality in image generation. We offer an end-to-end channel coding framework underpinned by diffusion models and propose an efficient training algorithm. Our simulations with various channel models establish that our diffusion models learn the channel distribution accurately, thereby achieving near-optimal end-to-end symbol error rates (SERs). We also note a significant advantage of diffusion models: A robu
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#22788;&#29702;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35813;&#24211;&#29983;&#25104;&#19981;&#21516;&#20998;&#24067;&#30340;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29983;&#25104;&#21442;&#25968;&#21644;&#29983;&#25104;&#35268;&#21017;&#36827;&#34892;&#33258;&#23450;&#20041;&#25511;&#21046;&#12290;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10498</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#29992;&#20110;&#29983;&#25104;&#21644;&#25805;&#20316;&#36855;&#23467;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Configurable Library for Generating and Manipulating Maze Datasets. (arXiv:2309.10498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#22788;&#29702;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35813;&#24211;&#29983;&#25104;&#19981;&#21516;&#20998;&#24067;&#30340;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29983;&#25104;&#21442;&#25968;&#21644;&#29983;&#25104;&#35268;&#21017;&#36827;&#34892;&#33258;&#23450;&#20041;&#25511;&#21046;&#12290;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20998;&#24067;&#20559;&#31227;&#30340;&#21709;&#24212;&#26041;&#24335;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#29983;&#25104;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#24179;&#21488;&#26469;&#27169;&#25311;&#24494;&#22937;&#21644;&#26174;&#33879;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36855;&#23467;&#20316;&#20026;&#19968;&#20010;&#20248;&#31168;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#20026;&#20102;&#25903;&#25345;&#23545;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31163;&#25968;&#25454;&#19978;&#34892;&#20026;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;maze-dataset&#8221;&#65292;&#19968;&#20010;&#21253;&#21547;&#36855;&#23467;&#27714;&#35299;&#20219;&#21153;&#30340;&#29983;&#25104;&#12289;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#24211;&#12290;&#20511;&#21161;&#36825;&#20010;&#24211;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#20351;&#29992;&#30340;&#29983;&#25104;&#31639;&#27861;&#12289;&#20256;&#36882;&#32473;&#36873;&#25321;&#31639;&#27861;&#30340;&#21442;&#25968;&#21644;&#29983;&#25104;&#30340;&#36855;&#23467;&#24517;&#39035;&#28385;&#36275;&#30340;&#31579;&#36873;&#22120;&#36827;&#34892;&#24191;&#27867;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#23427;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#21253;&#25324;&#26629;&#26684;&#21270;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#27169;&#22411;&#12290;&#36825;&#20123;&#26684;&#24335;&#20197;&#21450;&#29992;&#20110;&#21487;&#35270;&#21270;&#21644;&#36716;&#25442;&#30340;&#24037;&#20855;&#30830;&#20445;&#20102;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;Grid&#21477;&#23376;&#21644;&#33258;&#28982;&#21477;&#23376;&#22312;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;Grid&#21477;&#23376;&#30340;alpha&#27604;&#20363;&#22686;&#21152;&#26356;&#22823;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#22522;&#20110;EMALG&#35757;&#32451;&#30340;StarGAN&#27169;&#22411;&#22312;&#20027;&#35266;&#21487;&#25026;&#24230;&#35780;&#20272;&#20013;&#19968;&#33268;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10485</link><description>&lt;p&gt;
Grid&#21644;&#33258;&#28982;&#35821;&#21477;&#23545;Normal-to-Lombard&#36716;&#25442;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion. (arXiv:2309.10485v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;Grid&#21477;&#23376;&#21644;&#33258;&#28982;&#21477;&#23376;&#22312;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;Grid&#21477;&#23376;&#30340;alpha&#27604;&#20363;&#22686;&#21152;&#26356;&#22823;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#22522;&#20110;EMALG&#35757;&#32451;&#30340;StarGAN&#27169;&#22411;&#22312;&#20027;&#35266;&#21487;&#25026;&#24230;&#35780;&#20272;&#20013;&#19968;&#33268;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grid&#21477;&#23376;&#24120;&#29992;&#20110;&#30740;&#31350;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#22522;&#20110;Grid&#21477;&#23376;&#35757;&#32451;&#30340;Normal-to-Lombard&#27169;&#22411;&#26159;&#21542;&#36275;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#38899;&#21487;&#25026;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24179;&#34892;&#30340;Lombard&#35821;&#26009;&#24211;&#65288;&#31216;&#20026;Lombard Chinese TIMIT&#65292;LCT&#65289;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20102;&#20013;&#25991;TIMIT&#30340;&#33258;&#28982;&#21477;&#23376;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LCT&#21644;Enhanced Mandarin Lombard Grid&#35821;&#26009;&#24211;&#65288;EMALG&#65289;&#27604;&#36739;&#20102;&#33258;&#28982;&#21477;&#23376;&#21644;Grid&#21477;&#23376;&#22312;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#26041;&#38754;&#12290;&#36890;&#36807;&#23545;Lombard&#25928;&#24212;&#30340;&#21442;&#25968;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;&#33258;&#28982;&#21477;&#23376;&#21644;Grid&#21477;&#23376;&#30340;&#21442;&#25968;&#21464;&#21270;&#30456;&#20284;&#65292;&#20294;&#22312;alpha&#27604;&#20363;&#22686;&#21152;&#26041;&#38754;&#65292;Grid&#21477;&#23376;&#30340;&#22686;&#21152;&#26356;&#22823;&#12290;&#22312;&#36328;&#24615;&#21035;&#21644;&#20449;&#22122;&#27604;&#30340;&#20027;&#35266;&#21487;&#25026;&#24230;&#35780;&#20272;&#20013;&#65292;&#22522;&#20110;EMALG&#35757;&#32451;&#30340;StarGAN&#27169;&#22411;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grid sentence is commonly used for studying the Lombard effect and Normal-to-Lombard conversion. However, it's unclear if Normal-to-Lombard models trained on grid sentences are sufficient for improving natural speech intelligibility in real-world applications. This paper presents the recording of a parallel Lombard corpus (called Lombard Chinese TIMIT, LCT) extracting natural sentences from Chinese TIMIT. Then We compare natural and grid sentences in terms of Lombard effect and Normal-to-Lombard conversion using LCT and Enhanced MAndarin Lombard Grid corpus (EMALG). Through a parametric analysis of the Lombard effect, We find that as the noise level increases, both natural sentences and grid sentences exhibit similar changes in parameters, but in terms of the increase of the alpha ratio, grid sentences show a greater increase. Following a subjective intelligibility assessment across genders and Signal-to-Noise Ratios, the StarGAN model trained on EMALG consistently outperforms the mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#21462;&#19968;&#20010;&#21512;&#29702;&#30340;&#23376;&#38598;&#65292;&#21152;&#36895;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#30340;&#35757;&#32451;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#26680;&#24515;&#38598;&#19978;&#35757;&#32451;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#19982;&#22312;&#23436;&#25972;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10441</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35777;&#26126;&#30340;&#27867;&#21270;&#21152;&#36895;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26680;&#24515;&#38598;&#36873;&#21462;
&lt;/p&gt;
&lt;p&gt;
Coreset selection can accelerate quantum machine learning models with provable generalization. (arXiv:2309.10441v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#21462;&#19968;&#20010;&#21512;&#29702;&#30340;&#23376;&#38598;&#65292;&#21152;&#36895;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#30340;&#35757;&#32451;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#26680;&#24515;&#38598;&#19978;&#35757;&#32451;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#19982;&#22312;&#23436;&#25972;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20855;&#26377;&#31361;&#20986;&#22320;&#20301;&#65292;&#21033;&#29992;&#21363;&#23558;&#21040;&#26469;&#30340;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#33021;&#21147;&#26469;&#20811;&#26381;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#25928;&#29575;&#30340;&#25361;&#25112;&#38480;&#21046;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#22312;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65306;&#26680;&#24515;&#38598;&#36873;&#25321;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20986;&#19968;&#20010;&#21512;&#29702;&#30340;&#23376;&#38598;&#26469;&#21152;&#36895;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#24403;&#22312;&#36825;&#20123;&#26680;&#24515;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#22312;&#23436;&#25972;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#24515;&#38598;&#36873;&#25321;&#22312;&#21152;&#36895;&#28085;&#30422;&#21512;&#25104;&#25968;&#25454;&#20998;&#31867;&#12289;&#37327;&#23376;&#21327;&#35758;&#37492;&#23450;&#31561;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks (QNNs) and quantum kernels stand as prominent figures in the realm of quantum machine learning, poised to leverage the nascent capabilities of near-term quantum computers to surmount classical machine learning challenges. Nonetheless, the training efficiency challenge poses a limitation on both QNNs and quantum kernels, curbing their efficacy when applied to extensive datasets. To confront this concern, we present a unified approach: coreset selection, aimed at expediting the training of QNNs and quantum kernels by distilling a judicious subset from the original training dataset. Furthermore, we analyze the generalization error bounds of QNNs and quantum kernels when trained on such coresets, unveiling the comparable performance with those training on the complete original dataset. Through systematic numerical simulations, we illuminate the potential of coreset selection in expediting tasks encompassing synthetic data classification, identification of quantum co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#28378;&#21160;&#36724;&#25215;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36724;&#25215;&#30340;&#32452;&#20214;&#34920;&#31034;&#20026;&#22270;&#20013;&#30340;&#33410;&#28857;&#65292;GNN&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#32452;&#20214;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#30417;&#27979;&#26059;&#36716;&#26426;&#22120;&#30340;&#20581;&#24247;&#29366;&#24577;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10418</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#28378;&#21160;&#36724;&#25215;&#21160;&#21147;&#23398;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Dynamic Modeling of Roller Bearing. (arXiv:2309.10418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#28378;&#21160;&#36724;&#25215;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36724;&#25215;&#30340;&#32452;&#20214;&#34920;&#31034;&#20026;&#22270;&#20013;&#30340;&#33410;&#28857;&#65292;GNN&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#32452;&#20214;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#30417;&#27979;&#26059;&#36716;&#26426;&#22120;&#30340;&#20581;&#24247;&#29366;&#24577;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26694;&#26550;&#24212;&#29992;&#20110;&#39044;&#27979;&#28378;&#21160;&#36724;&#25215;&#30340;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#29992;&#20110;&#23454;&#26102;&#36816;&#34892;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#65292;&#29992;&#20110;&#30417;&#27979;&#26059;&#36716;&#26426;&#22120;&#30340;&#20581;&#24247;&#29366;&#24577;&#12290;&#36890;&#36807;&#23558;&#36724;&#25215;&#30340;&#32452;&#20214;&#34920;&#31034;&#20026;&#22270;&#20013;&#30340;&#33410;&#28857;&#65292;GNN&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#23427;&#20204;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#36724;&#25215;&#30340;&#21160;&#24577;&#24377;&#31783;&#38459;&#23612;&#27169;&#22411;&#29983;&#25104;GNN&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#31163;&#25955;&#30340;&#36136;&#37327;&#20195;&#34920;&#36724;&#25215;&#30340;&#32452;&#20214;&#65292;&#22914;&#28378;&#21160;&#20307;&#12289;&#20869;&#22280;&#21644;&#22806;&#22280;&#65292;&#32780;&#36203;&#20857;&#25509;&#35302;&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#36825;&#20123;&#32452;&#20214;&#20043;&#38388;&#30340;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#19981;&#21516;&#20110;&#35757;&#32451;&#37197;&#32622;&#30340;&#36724;&#25215;&#37197;&#32622;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;GNN&#26694;&#26550;&#30340;&#23398;&#20064;&#21644;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the presented work, we propose to apply the framework of graph neural networks (GNNs) to predict the dynamics of a rolling element bearing. This approach offers generalizability and interpretability, having the potential for scalable use in real-time operational digital twin systems for monitoring the health state of rotating machines. By representing the bearing's components as nodes in a graph, the GNN can effectively model the complex relationships and interactions among them. We utilize a dynamic spring-mass-damper model of a bearing to generate the training data for the GNN. In this model, discrete masses represent bearing components such as rolling elements, inner raceways, and outer raceways, while a Hertzian contact model is employed to calculate the forces between these components.  We evaluate the learning and generalization capabilities of the proposed GNN framework by testing different bearing configurations that deviate from the training configurations. Through this app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#25968;&#20540;&#33410;&#28857;&#23646;&#24615;&#20043;&#38388;&#30340;&#32593;&#32476;&#36317;&#31163;&#26469;&#21019;&#24314;&#32593;&#32476;&#24863;&#30693;&#23884;&#20837;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10408</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#24863;&#30693;&#23884;&#20837;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning via Network-Aware Embeddings. (arXiv:2309.10408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#25968;&#20540;&#33410;&#28857;&#23646;&#24615;&#20043;&#38388;&#30340;&#32593;&#32476;&#36317;&#31163;&#26469;&#21019;&#24314;&#32593;&#32476;&#24863;&#30693;&#23884;&#20837;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32858;&#31867;&#26159;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#20219;&#21153;&#26159;&#26681;&#25454;&#30456;&#20284;&#24615;&#23545;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#20998;&#32452;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#30340;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#22797;&#26434;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#20363;&#22914;&#20154;&#20204;&#22312;&#22797;&#26434;&#30340;&#31038;&#20132;&#32593;&#32476;&#20013;&#21487;&#33021;&#20855;&#26377;&#30340;&#21508;&#31181;&#29305;&#24449;&#21644;&#35266;&#28857;&#12290;&#24403;&#21069;&#30340;&#32858;&#31867;&#26041;&#27861;&#24456;&#38590;&#22788;&#29702;&#36825;&#31181;&#22797;&#26434;&#24615;&#65306;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#36817;&#20284;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#19981;&#33021;&#23558;&#20854;&#26174;&#24335;&#22320;&#20316;&#20026;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#36825;&#20010;&#30450;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#25968;&#20540;&#33410;&#28857;&#23646;&#24615;&#20043;&#38388;&#30340;&#32593;&#32476;&#36317;&#31163;&#26469;&#21019;&#24314;&#32593;&#32476;&#24863;&#30693;&#23884;&#20837;&#65292;&#36890;&#36807;&#24191;&#20041;&#27431;&#27663;&#36317;&#31163;&#35745;&#31639;&#12290;&#19982;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#25152;&#26377;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#26159;&#23545;&#32593;&#32476;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#32780;&#26159;&#23545;&#20854;&#33410;&#28857;&#23646;&#24615;&#36827;&#34892;&#32858;&#31867;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data clustering, the task of grouping observations according to their similarity, is a key component of unsupervised learning -- with real world applications in diverse fields such as biology, medicine, and social science. Often in these fields the data comes with complex interdependencies between the dimensions of analysis, for instance the various characteristics and opinions people can have live on a complex social network. Current clustering methods are ill-suited to tackle this complexity: deep learning can approximate these dependencies, but not take their explicit map as the input of the analysis. In this paper, we aim at fixing this blind spot in the unsupervised learning literature. We can create network-aware embeddings by estimating the network distance between numeric node attributes via the generalized Euclidean distance. Differently from all methods in the literature that we know of, we do not cluster the nodes of the network, but rather its node attributes. In our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;ReLU-Like&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#22312;&#32039;&#33268;&#22495;&#19978;&#23558;$L^p$&#20989;&#25968;&#20174;$[0,1]^{d_x}$&#36924;&#36817;&#21040;$\mathbb R^{d_y}$&#25152;&#38656;&#30340;&#26368;&#23567;&#23485;&#24230;&#20026;$\max\{d_x,d_y,2\}$&#65292;&#20174;&#32780;&#34920;&#26126;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36924;&#36817;&#27604;&#22312;${\mathbb R^{d_x}}$&#19978;&#30340;&#36924;&#36817;&#26356;&#23481;&#26131;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#33268;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;&#19979;&#30028;&#20026;$w_{\min}\ge d_y+1$&#65288;&#24403;$d_x&lt;d_y\le2d_x$&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.10402</link><description>&lt;p&gt;
&#20351;&#29992;ReLU&#32593;&#32476;&#22312;&#32039;&#33268;&#22495;&#19978;&#36827;&#34892;&#36890;&#29992;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;
&lt;/p&gt;
&lt;p&gt;
Minimum width for universal approximation using ReLU networks on compact domain. (arXiv:2309.10402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;ReLU-Like&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#22312;&#32039;&#33268;&#22495;&#19978;&#23558;$L^p$&#20989;&#25968;&#20174;$[0,1]^{d_x}$&#36924;&#36817;&#21040;$\mathbb R^{d_y}$&#25152;&#38656;&#30340;&#26368;&#23567;&#23485;&#24230;&#20026;$\max\{d_x,d_y,2\}$&#65292;&#20174;&#32780;&#34920;&#26126;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36924;&#36817;&#27604;&#22312;${\mathbb R^{d_x}}$&#19978;&#30340;&#36924;&#36817;&#26356;&#23481;&#26131;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#33268;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;&#19979;&#30028;&#20026;$w_{\min}\ge d_y+1$&#65288;&#24403;$d_x&lt;d_y\le2d_x$&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#30740;&#31350;&#65292;&#38480;&#21046;&#23485;&#24230;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#24050;&#32463;&#20316;&#20026;&#28145;&#24230;&#38480;&#21046;&#32593;&#32476;&#30340;&#32463;&#20856;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#30340;&#23545;&#20598;&#36827;&#34892;&#30740;&#31350;&#12290;&#24050;&#32463;&#26377;&#20960;&#27425;&#23581;&#35797;&#26469;&#34920;&#24449;&#20351;&#24471;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#25104;&#31435;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}$&#65292;&#20294;&#21482;&#26377;&#24456;&#23569;&#20960;&#20010;&#25214;&#21040;&#20102;&#30830;&#20999;&#30340;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20174;$[0,1]^{d_x}$&#21040;$\mathbb R^{d_y}$&#30340;$L^p$&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;&#65292;&#22914;&#26524;&#28608;&#27963;&#20989;&#25968;&#26159;ReLU-Like&#65288;&#20363;&#22914;ReLU&#65292;GELU&#65292;Softplus&#65289;&#65292;&#37027;&#20040;&#23427;&#30340;&#30830;&#20999;&#20540;&#26159;$\max\{d_x,d_y,2\}$&#12290;&#19982;&#24050;&#30693;&#30340;&#32467;&#26524;$w_{\min}=\max\{d_x+1,d_y\}$&#30456;&#27604;&#65292;&#24403;&#22495;&#20026;${\mathbb R^{d_x}}$&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#34920;&#26126;&#65292;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36924;&#36817;&#35201;&#27714;&#27604;&#22312;${\mathbb R^{d_x}}$&#19978;&#30340;&#35201;&#27714;&#26356;&#23567;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#21033;&#29992;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#19968;&#33268;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}$&#35777;&#26126;&#20102;&#19968;&#20010;&#19979;&#30028;&#65306;&#22914;&#26524;$d_x&lt;d_y\le2d_x$&#65292;&#21017;$w_{\min}\ge d_y+1$&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#20010;&#20108;&#20998;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The universal approximation property of width-bounded networks has been studied as a dual of the classical universal approximation theorem for depth-bounded ones. There were several attempts to characterize the minimum width $w_{\min}$ enabling the universal approximation property; however, only a few of them found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$ is exactly $\max\{d_x,d_y,2\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\min}=\max\{d_x+1,d_y\}$ when the domain is ${\mathbb R^{d_x}}$, our result first shows that approximation on a compact domain requires smaller width than on ${\mathbb R^{d_x}}$. We next prove a lower bound on $w_{\min}$ for uniform approximation using general activation functions including ReLU: $w_{\min}\ge d_y+1$ if $d_x&lt;d_y\le2d_x$. Together with our first result, this shows a dichotomy be
&lt;/p&gt;</description></item><item><title>DQAS&#26159;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;NISQ&#26102;&#20195;&#33258;&#21160;&#35774;&#35745;&#37327;&#23376;&#30005;&#36335;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;DQAS&#22312;&#35299;&#20915;&#37327;&#23376;&#28145;&#24230;Q-learning&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10392</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentiable Quantum Architecture Search for Quantum Reinforcement Learning. (arXiv:2309.10392v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10392
&lt;/p&gt;
&lt;p&gt;
DQAS&#26159;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;NISQ&#26102;&#20195;&#33258;&#21160;&#35774;&#35745;&#37327;&#23376;&#30005;&#36335;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;DQAS&#22312;&#35299;&#20915;&#37327;&#23376;&#28145;&#24230;Q-learning&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;DQAS&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;NISQ&#26102;&#20195;&#33258;&#21160;&#35774;&#35745;&#37327;&#23376;&#30005;&#36335;&#12290;&#23427;&#30340;&#21160;&#26426;&#26159;&#37327;&#23376;&#30828;&#20214;&#30340;&#20302;&#20445;&#30495;&#24230;&#65292;&#30005;&#36335;&#26550;&#26500;&#30340;&#20302;&#28789;&#27963;&#24615;&#65292;&#30005;&#36335;&#35774;&#35745;&#25104;&#26412;&#39640;&#65292;&#24179;&#22374;&#30340;&#33618;&#21407;&#38382;&#39064;&#21644;&#26435;&#37325;&#30340;&#21608;&#26399;&#24615;&#12290;&#20154;&#20204;&#20351;&#29992;&#23427;&#26469;&#35299;&#20915;&#22522;&#20110;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#35823;&#24046;&#32531;&#35299;&#12289;&#37193;&#20998;&#35299;&#21644;&#37327;&#23376;&#36924;&#36817;&#20248;&#21270;&#38382;&#39064;&#12290;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#24120;&#26377;&#21508;&#31181;&#25968;&#25454;&#12290;QRL&#36890;&#24120;&#20351;&#29992;&#25163;&#21160;&#35774;&#35745;&#30340;&#30005;&#36335;&#12290;&#28982;&#32780;&#65292;&#39044;&#23450;&#20041;&#30340;&#30005;&#36335;&#38656;&#35201;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;&#30005;&#36335;&#35774;&#35745;&#21487;&#33021;&#21464;&#24471;&#26840;&#25163;&#65292;&#29305;&#21035;&#26159;&#22312;&#30005;&#36335;&#35268;&#27169;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;DQAS&#33021;&#21542;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#37327;&#23376;&#28145;&#24230;Q-learning&#38382;&#39064;&#20173;&#26410;&#35299;&#20915;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21457;&#29616;DQAS&#22312;&#35299;&#20915;&#37327;&#23376;&#28145;&#24230;Q-learning&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable quantum architecture search (DQAS) is a gradient-based framework to design quantum circuits automatically in the NISQ era. It was motivated by such as low fidelity of quantum hardware, low flexibility of circuit architecture, high circuit design cost, barren plateau (BP) problem, and periodicity of weights. People used it to address error mitigation, unitary decomposition, and quantum approximation optimization problems based on fixed datasets. Quantum reinforcement learning (QRL) is a part of quantum machine learning and often has various data. QRL usually uses a manually designed circuit. However, the pre-defined circuit needs more flexibility for different tasks, and the circuit design based on various datasets could become intractable in the case of a large circuit. The problem of whether DQAS can be applied to quantum deep Q-learning with various datasets is still open. The main target of this work is to discover the capability of DQAS to solve quantum deep Q-learni
&lt;/p&gt;</description></item><item><title>COLA&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#22270;&#25193;&#22686;&#26469;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#20351;&#24471;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#24555;&#36895;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.10376</link><description>&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#19982;&#22270;&#20803;&#23398;&#20064;&#30456;&#36935;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20219;&#21153;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks. (arXiv:2309.10376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10376
&lt;/p&gt;
&lt;p&gt;
COLA&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#22270;&#25193;&#22686;&#26469;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#20351;&#24471;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#24555;&#36895;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#24212;&#29992;&#26159;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#36981;&#24490;&#20803;&#23398;&#20064;&#33539;&#24335;&#65292;&#23637;&#31034;&#20102;&#23545;&#23569;&#26679;&#26412;&#20219;&#21153;&#24555;&#36895;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#24494;&#35843;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#32972;&#21518;&#21407;&#22240;&#30340;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#30456;&#23545;&#20110;&#20803;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65292;&#21253;&#25324;&#65288;1&#65289;&#20840;&#38754;&#21033;&#29992;&#22270;&#33410;&#28857;&#21644;&#65288;2&#65289;&#22270;&#25193;&#22686;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23558;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#20248;&#21183;&#32467;&#21512;&#21040;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65306;&#23545;&#27604;&#24335;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#65288;COLA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;COLA&#21033;&#29992;&#22270;&#25193;&#22686;&#26469;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#20351;&#24471;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become popular in Graph Representation Learning (GRL). One fundamental application is few-shot node classification. Most existing methods follow the meta learning paradigm, showing the ability of fast generalization to few-shot tasks. However, recent works indicate that graph contrastive learning combined with fine-tuning can significantly outperform meta learning methods. Despite the empirical success, there is limited understanding of the reasons behind it. In our study, we first identify two crucial advantages of contrastive learning compared to meta learning, including (1) the comprehensive utilization of graph nodes and (2) the power of graph augmentations. To integrate the strength of both contrastive learning and meta learning on the few-shot node classification tasks, we introduce a new paradigm: Contrastive Few-Shot Node Classification (COLA). Specifically, COLA employs graph augmentations to identify semantically similar nodes, which enables 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.10370</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#19968;&#20010;&#38544;&#34255;&#23618;&#12289;&#19968;&#20010;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#12289;&#19968;&#20010;${\mathcal L}^2$&#35889;&#33539;&#31867;&#65288;&#25110;&#32773;Hilbert-Schmidt&#65289;&#30340;&#20195;&#20215;&#20989;&#25968;&#12289;&#36755;&#20837;&#31354;&#38388;${\mathbb R}^M$&#12289;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#65288;&#20854;&#20013;$Q\leq M$&#65289;&#65292;&#20197;&#21450;&#35757;&#32451;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;$N&gt;QM$&#25152;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;$O(\delta_P)$&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$\delta_P$&#34913;&#37327;&#20102;&#35757;&#32451;&#36755;&#20837;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24212;&#20110;&#23646;&#20110;&#21516;&#19968;&#36755;&#20986;&#21521;&#37327;$y_j$&#30340;&#35757;&#32451;&#36755;&#20837;&#21521;&#37327;$\overline{x_{0,j}}$&#30340;&#25237;&#24433;&#26469;&#33719;&#24471;&#36817;&#20284;&#30340;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;$j=1,\dots,Q$&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;$M=Q$&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#30830;&#23450;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#19968;&#20010;&#30830;&#20999;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#65307;&#36825;&#20010;&#23574;&#38160;&#30340;&#20540;&#19982;&#23545;&#20110;$Q\leq M$&#25152;&#33719;&#24471;&#30340;&#19978;&#30028;&#20043;&#38388;&#26377;&#19968;&#20010;&#30456;&#23545;&#35823;&#24046;$O(\delta_P^2)$&#12290;&#19978;&#30028;&#35777;&#26126;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26500;&#36896;&#24615;&#35757;&#32451;&#30340;&#32593;&#32476;&#65307;&#25105;&#20204;&#35777;&#26126;&#23427;&#27979;&#24230;&#20102;$Q$&#32500;&#31354;&#38388;&#20013;&#30340;&#32473;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N&gt;QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#12289;&#20197;&#21450;&#20943;&#23569;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#65292;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#36793;&#32536;&#33410;&#28857;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10367</link><description>&lt;p&gt;
&#36793;&#32536;&#33410;&#28857;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Toward efficient resource utilization at edge nodes in federated learning. (arXiv:2309.10367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#12289;&#20197;&#21450;&#20943;&#23569;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#65292;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#36793;&#32536;&#33410;&#28857;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#36793;&#32536;&#33410;&#28857;&#33021;&#22815;&#20849;&#21516;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20182;&#20204;&#30340;&#25968;&#25454;&#12290;&#36825;&#26159;&#36890;&#36807;&#35774;&#22791;&#35745;&#31639;&#26412;&#22320;&#31169;&#26377;&#27169;&#22411;&#26356;&#26032;&#65292;&#28982;&#21518;&#30001;&#26381;&#21153;&#22120;&#36827;&#34892;&#32858;&#21512;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#21644;&#32593;&#32476;&#36890;&#20449;&#23545;&#20110;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#36739;&#22823;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#25104;&#20026;&#20005;&#37325;&#29942;&#39048;&#12290;&#36793;&#32536;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#26377;&#38480;&#30340;&#30828;&#20214;&#36164;&#28304;&#65288;RAM&#12289;CPU&#65289;&#65292;&#32780;&#36793;&#32536;&#30340;&#32593;&#32476;&#24102;&#23485;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#25193;&#23637;&#32852;&#37030;&#36710;&#38431;&#24212;&#29992;&#26469;&#35828;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;FL&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#65292;&#20197;&#21450;&#27599;&#20010;&#20840;&#23616;&#35757;&#32451;&#36718;&#27425;&#20013;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#12290;&#23545;&#20110;&#27599;&#20010;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#35201;&#35757;&#32451;&#30340;&#23618;&#65292;&#20923;&#32467;&#27169;&#22411;&#30340;&#20854;&#20313;&#37096;&#20998;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25490;&#38500;&#25152;&#26377;&#26410;&#35757;&#32451;&#30340;&#37096;&#20998;&#26469;&#20943;&#23569;&#27599;&#36718;&#30340;&#26381;&#21153;&#22120;&#36127;&#36733;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tend to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning in order to reduce resource utilization on devices, as well as the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. In doing so, we can reduce both server load and communication costs per round by excluding all untrained
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#26500;&#24314;&#20284;&#28982;&#20989;&#25968;&#65292;&#23454;&#29616;&#20174;&#20302;&#33021;&#37327;&#27979;&#37327;&#21040;&#39640;&#33021;&#37327;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#30340;&#20934;&#30830;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#26679;&#26412;&#29983;&#25104;&#21644;&#21345;&#26041;&#26816;&#39564;&#32479;&#35745;&#37327;&#65292;&#20197;&#30740;&#31350;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.10365</link><description>&lt;p&gt;
&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#25311;&#21512;&#30340;&#21487;&#27979;&#20284;&#28982;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Testable Likelihoods for Beyond-the-Standard Model Fits. (arXiv:2309.10365v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10365
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#26500;&#24314;&#20284;&#28982;&#20989;&#25968;&#65292;&#23454;&#29616;&#20174;&#20302;&#33021;&#37327;&#27979;&#37327;&#21040;&#39640;&#33021;&#37327;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#30340;&#20934;&#30830;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#26679;&#26412;&#29983;&#25104;&#21644;&#21345;&#26041;&#26816;&#39564;&#32479;&#35745;&#37327;&#65292;&#20197;&#30740;&#31350;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#30830;&#27979;&#37327;&#36793;&#30028;&#19978;&#30740;&#31350;&#28508;&#22312;&#30340;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#25928;&#24212;&#65292;&#38656;&#35201;&#23558;&#20302;&#33021;&#37327;&#27979;&#37327;&#30340;&#20449;&#24687;&#20934;&#30830;&#20256;&#36882;&#32473;&#39640;&#33021;&#37327;&#30340;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#26500;&#24314;&#23454;&#29616;&#36825;&#31181;&#20256;&#36882;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#20197;&#36825;&#31181;&#26041;&#24335;&#26500;&#24314;&#30340;&#20284;&#28982;&#20989;&#25968;&#33021;&#22815;&#29983;&#25104;&#39069;&#22806;&#30340;&#26679;&#26412;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;"&#24179;&#20961;"&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#65292;&#21363;&#21345;&#26041;&#26816;&#39564;&#32479;&#35745;&#37327;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#24402;&#19968;&#21270;&#27969;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#21644;&#38750;&#39640;&#26031;&#30340;&#20363;&#23376;&#65292;&#24182;&#37327;&#21270;&#20102;&#20284;&#28982;&#20989;&#25968;&#21450;&#20854;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying potential BSM effects at the precision frontier requires accurate transfer of information from low-energy measurements to high-energy BSM models. We propose to use normalising flows to construct likelihood functions that achieve this transfer. Likelihood functions constructed in this way provide the means to generate additional samples and admit a ``trivial'' goodness-of-fit test in form of a $\chi^2$ test statistic. Here, we study a particular form of normalising flow, apply it to a multi-modal and non-Gaussian example, and quantify the accuracy of the likelihood function and its test statistic.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LP-CLIP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#33258;&#35757;&#32451;&#26469;&#25552;&#39640;CLIP&#22810;&#27169;&#24577;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#12290; LP-CLIP&#36890;&#36807;&#22312;CLIP&#32534;&#30721;&#32467;&#26500;&#39030;&#37096;&#28155;&#21152;&#32447;&#24615;&#25506;&#27979;&#23618;&#26469;&#33976;&#39311;CLIP&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#30001;CLIP&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#22686;&#24378;&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#23545;&#21508;&#31181;&#25361;&#25112;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10361</link><description>&lt;p&gt;
&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#33258;&#35757;&#32451;&#25552;&#39640;CLIP&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving CLIP Robustness with Knowledge Distillation and Self-Training. (arXiv:2309.10361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LP-CLIP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#33258;&#35757;&#32451;&#26469;&#25552;&#39640;CLIP&#22810;&#27169;&#24577;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#12290; LP-CLIP&#36890;&#36807;&#22312;CLIP&#32534;&#30721;&#32467;&#26500;&#39030;&#37096;&#28155;&#21152;&#32447;&#24615;&#25506;&#27979;&#23618;&#26469;&#33976;&#39311;CLIP&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#30001;CLIP&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#22686;&#24378;&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#23545;&#21508;&#31181;&#25361;&#25112;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;CLIP&#65288;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65289;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;CLIP&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#32034;&#22686;&#24378;&#20854;&#40065;&#26834;&#24615;&#30340;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;LP-CLIP&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#22312;&#20854;&#32534;&#30721;&#32467;&#26500;&#39030;&#37096;&#21152;&#20837;&#19968;&#20010;&#32447;&#24615;&#25506;&#27979;&#23618;&#65292;&#23558;CLIP&#29305;&#24449;&#33976;&#39311;&#20986;&#26469;&#12290;&#36825;&#20010;&#26032;&#22686;&#30340;&#23618;&#20351;&#29992;CLIP&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#33258;&#35757;&#32451;&#31574;&#30053;&#12290;LP-CLIP&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;CLIP&#40065;&#26834;&#24615;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#25506;&#27979;&#23618;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;&#35813;&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#38754;&#20020;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the robustness of a multi-modal computer vision model, CLIP (Contrastive Language-Image Pretraining), in the context of unsupervised learning. The main objective is twofold: first, to evaluate the robustness of CLIP, and second, to explore strategies for augmenting its robustness. To achieve this, we introduce a novel approach named LP-CLIP. This technique involves the distillation of CLIP features through the incorporation of a linear probing layer positioned atop its encoding structure. This newly added layer is trained utilizing pseudo-labels produced by CLIP, coupled with a self-training strategy. The LP-CLIP technique offers a promising approach to enhance the robustness of CLIP without the need for annotations. By leveraging a simple linear probing layer, we aim to improve the model's ability to withstand various uncertainties and challenges commonly encountered in real-world scenarios. Importantly, our approach does not rely on annotated data, which makes it 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#20928;&#21270;&#65288;LGAP&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#26631;&#39064;&#29983;&#25104;&#22120;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#30340;&#26631;&#39064;&#24182;&#36890;&#36807;&#25193;&#25955;&#32593;&#32476;&#36827;&#34892;&#24341;&#23548;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10348</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
Language Guided Adversarial Purification. (arXiv:2309.10348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#20928;&#21270;&#65288;LGAP&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#26631;&#39064;&#29983;&#25104;&#22120;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#30340;&#26631;&#39064;&#24182;&#36890;&#36807;&#25193;&#25955;&#32593;&#32476;&#36827;&#34892;&#24341;&#23548;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#23545;&#25239;&#38450;&#24481;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20381;&#36182;&#20998;&#31867;&#22120;&#21644;&#25915;&#20987;&#25163;&#27861;&#65292;&#20351;&#20854;&#20855;&#26377;&#22810;&#21151;&#33021;&#24615;&#20294;&#36890;&#24120;&#35745;&#31639;&#23494;&#38598;&#12290;&#26368;&#36817;&#22312;&#25193;&#25955;&#21644;&#35780;&#20998;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#21644;&#23545;&#25239;&#20928;&#21270;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#31867;&#39640;&#25928;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#31216;&#20026;&#23545;&#25239;&#35757;&#32451;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#25915;&#20987;&#21521;&#37327;&#30693;&#35782;&#65292;&#36843;&#20351;&#23427;&#20204;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#36827;&#34892;&#22823;&#37327;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#20928;&#21270;&#65288;LGAP&#65289;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#26631;&#39064;&#29983;&#25104;&#22120;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#26631;&#39064;&#65292;&#28982;&#21518;&#36890;&#36807;&#25193;&#25955;&#32593;&#32476;&#26469;&#25351;&#23548;&#23545;&#25239;&#20928;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24050;&#32463;&#38024;&#23545;&#24378;&#22823;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial purification using generative models demonstrates strong adversarial defense performance. These methods are classifier and attack-agnostic, making them versatile but often computationally intensive. Recent strides in diffusion and score networks have improved image generation and, by extension, adversarial purification. Another highly efficient class of adversarial defense methods known as adversarial training requires specific knowledge of attack vectors, forcing them to be trained extensively on adversarial examples. To overcome these limitations, we introduce a new framework, namely Language Guided Adversarial Purification (LGAP), utilizing pre-trained diffusion models and caption generators to defend against adversarial attacks. Given an input image, our method first generates a caption, which is then used to guide the adversarial purification process through a diffusion network. Our approach has been evaluated against strong adversarial attacks, proving its effectivene
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20855;&#22791;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#20284;&#30340;&#24110;&#21161;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10346</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#26234;&#33021;&#20307;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Explaining Agent Behavior with Large Language Models. (arXiv:2309.10346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10346
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20855;&#22791;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#20284;&#30340;&#24110;&#21161;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#22914;&#26426;&#22120;&#20154;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#33021;&#22815;&#21521;&#20154;&#31867;&#23545;&#31561;&#20307;&#35299;&#37322;&#20182;&#20204;&#20915;&#31574;&#32972;&#21518;&#30340;&#25512;&#29702;&#65292;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#34892;&#20026;&#36890;&#24120;&#26159;&#30001;&#19981;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#35266;&#23519;&#65292;&#19981;&#32771;&#34385;&#24213;&#23618;&#27169;&#22411;&#34920;&#31034;&#65292;&#29983;&#25104;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#31616;&#27905;&#34920;&#31034;&#65292;&#24182;&#29992;&#20854;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#19982;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#20132;&#20114;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21644;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#29983;&#25104;&#30340;&#35299;&#37322;&#19968;&#26679;&#26377;&#29992;&#65292;&#21516;&#26102;&#20855;&#22791;&#26377;&#30410;&#30340;&#20132;&#20114;&#65292;&#22914;&#28548;&#28165;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts, however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation. We show how a compact representation of the agent's behavior can be learned and used to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those generated by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20174;&#38544;&#31169;&#25935;&#24863;&#21334;&#26041;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#25191;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#21270;&#27979;&#35797;&#25439;&#22833;&#12289;&#21334;&#26041;&#38544;&#31169;&#21644;&#25903;&#20184;&#30340;&#21152;&#26435;&#32452;&#21512;&#30340;&#26368;&#20339;&#26426;&#21046;&#65292;&#36890;&#36807;&#32467;&#21512;&#21338;&#24328;&#35770;&#12289;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24605;&#24819;&#65292;&#35299;&#20915;&#20102;&#20080;&#26041;&#30340;&#30446;&#26631;&#20989;&#25968;&#38750;&#20984;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24403;&#21334;&#26041;&#25968;&#37327;&#21464;&#22823;&#26102;&#30340;&#28176;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10340</link><description>&lt;p&gt;
&#23547;&#25214;&#24179;&#34913;&#65306;&#29992;&#20110;&#24322;&#26500;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#37319;&#38598;&#30340;&#36923;&#36753;&#22238;&#24402;&#30340;&#26368;&#20339;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Striking a Balance: An Optimal Mechanism Design for Heterogenous Differentially Private Data Acquisition for Logistic Regression. (arXiv:2309.10340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20174;&#38544;&#31169;&#25935;&#24863;&#21334;&#26041;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#25191;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#21270;&#27979;&#35797;&#25439;&#22833;&#12289;&#21334;&#26041;&#38544;&#31169;&#21644;&#25903;&#20184;&#30340;&#21152;&#26435;&#32452;&#21512;&#30340;&#26368;&#20339;&#26426;&#21046;&#65292;&#36890;&#36807;&#32467;&#21512;&#21338;&#24328;&#35770;&#12289;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24605;&#24819;&#65292;&#35299;&#20915;&#20102;&#20080;&#26041;&#30340;&#30446;&#26631;&#20989;&#25968;&#38750;&#20984;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24403;&#21334;&#26041;&#25968;&#37327;&#21464;&#22823;&#26102;&#30340;&#28176;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20174;&#38544;&#31169;&#25935;&#24863;&#21334;&#26041;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#25191;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#25968;&#25454;&#26159;&#31169;&#26377;&#30340;&#65292;&#21334;&#26041;&#24517;&#39035;&#36890;&#36807;&#25903;&#20184;&#26469;&#28608;&#21169;&#20182;&#20204;&#25552;&#20379;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#26426;&#21046;&#65292;&#20248;&#21270;&#27979;&#35797;&#25439;&#22833;&#12289;&#21334;&#26041;&#38544;&#31169;&#21644;&#25903;&#20184;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#21363;&#22312;&#22810;&#20010;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#20043;&#38388;&#23547;&#25214;&#24179;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#21338;&#24328;&#35770;&#12289;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20080;&#26041;&#30340;&#30446;&#26631;&#20989;&#25968;&#21487;&#33021;&#38750;&#24120;&#38750;&#20984;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#38382;&#39064;&#21442;&#25968;&#30340;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21464;&#37327;&#30340;&#21464;&#25442;&#23558;&#38382;&#39064;&#20984;&#21270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24403;&#21334;&#26041;&#25968;&#37327;&#21464;&#22823;&#26102;&#65292;&#20080;&#26041;&#30340;&#27979;&#35797;&#35823;&#24046;&#21644;&#25903;&#20184;&#30340;&#28176;&#36817;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#19968;&#20010;&#30495;&#23454;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of performing logistic regression on data collected from privacy-sensitive sellers. Since the data is private, sellers must be incentivized through payments to provide their data. Thus, the goal is to design a mechanism that optimizes a weighted combination of test loss, seller privacy, and payment, i.e., strikes a balance between multiple objectives of interest. We solve the problem by combining ideas from game theory, statistical learning theory, and differential privacy. The buyer's objective function can be highly non-convex. However, we show that, under certain conditions on the problem parameters, the problem can be convexified by using a change of variables. We also provide asymptotic results characterizing the buyer's test error and payments when the number of sellers becomes large. Finally, we demonstrate our ideas by applying them to a real healthcare data set.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedWOA&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#20174;&#23478;&#24237;&#33021;&#28304;&#25968;&#25454;&#35757;&#32451;&#30340;&#23616;&#37096;LTSM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#32858;&#21512;&#20986;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#23548;&#33268;&#30340;&#39044;&#27979;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10337</link><description>&lt;p&gt;
FedWOA:&#19968;&#31181;&#21033;&#29992;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#21487;&#20877;&#29983;&#33021;&#28304;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FedWOA: A Federated Learning Model that uses the Whale Optimization Algorithm for Renewable Energy Prediction. (arXiv:2309.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedWOA&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#20174;&#23478;&#24237;&#33021;&#28304;&#25968;&#25454;&#35757;&#32451;&#30340;&#23616;&#37096;LTSM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#32858;&#21512;&#20986;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#23548;&#33268;&#30340;&#39044;&#27979;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#22312;&#22788;&#29702;&#25935;&#24863;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#33021;&#28304;&#39046;&#22495;&#65292;&#35775;&#38382;&#23478;&#24237;&#29983;&#20135;&#32773;&#21644;&#28040;&#36153;&#32773;&#30340;&#33021;&#28304;&#25968;&#25454;&#23545;&#20110;&#33021;&#28304;&#39044;&#27979;&#20197;&#25903;&#25345;&#33021;&#28304;&#32593;&#26684;&#31649;&#29702;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#22823;&#35268;&#27169;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#20844;&#27665;&#36890;&#24120;&#19981;&#24895;&#24847;&#25480;&#20104;&#22522;&#20110;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#12290;&#32852;&#37030;&#23398;&#20064;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#38544;&#31169;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#21457;&#30005;&#27169;&#24335;&#30340;&#21464;&#21270;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#23548;&#33268;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#30340;&#29983;&#25104;&#23384;&#22312;&#38382;&#39064;&#65292;&#36827;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;FedWOA&#36825;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#20174;&#22522;&#20110;&#23478;&#24237;&#33021;&#28304;&#25968;&#25454;&#35757;&#32451;&#30340;&#23616;&#37096;LTSM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#32858;&#21512;&#20986;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#35782;&#21035;&#20986;&#26368;&#20248;&#30340;&#26435;&#37325;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy is important when dealing with sensitive personal information in machine learning models, which require large data sets for training. In the energy field, access to household prosumer energy data is crucial for energy predictions to support energy grid management and large-scale adoption of renewables however citizens are often hesitant to grant access to cloud-based machine learning models. Federated learning has been proposed as a solution to privacy challenges however report issues in generating the global prediction model due to data heterogeneity, variations in generation patterns, and the high number of parameters leading to even lower prediction accuracy. This paper addresses these challenges by introducing FedWOA a novel federated learning model that employs the Whale Optimization Algorithm to aggregate global prediction models from the weights of local LTSM neural network models trained on prosumer energy data. The proposed solution identifies the optimal vector of wei
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#22270;&#29255;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#38646;-shot&#25552;&#21462;UI&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#26041;&#27861;&#23454;&#29616;&#24212;&#29992;&#21040;&#24212;&#29992;&#30340;&#26816;&#32034;&#21644;&#35774;&#35745;&#19968;&#33268;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10328</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#24212;&#29992;&#26816;&#32034;&#21644;&#35774;&#35745;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Computational Approaches for App-to-App Retrieval and Design Consistency Check. (arXiv:2309.10328v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#22270;&#29255;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#38646;-shot&#25552;&#21462;UI&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#26041;&#27861;&#23454;&#29616;&#24212;&#29992;&#21040;&#24212;&#29992;&#30340;&#26816;&#32034;&#21644;&#35774;&#35745;&#19968;&#33268;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31227;&#21160;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#20013;&#25552;&#21462;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#23558;&#36825;&#20123;&#34920;&#31034;&#29992;&#20110;&#35774;&#35745;&#24072;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#20316;&#20026;&#26377;&#25928;&#30340;&#35745;&#31639;&#35774;&#35745;&#25903;&#25345;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22312;&#23567;&#35268;&#27169;&#31227;&#21160;UI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#21462;&#35821;&#20041;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#23631;&#24149;&#25130;&#22270;&#36827;&#34892;&#23545;&#27604;&#26469;&#26816;&#32034;&#32473;&#23450;&#26597;&#35810;&#25130;&#22270;&#30340;&#30456;&#20284;UI&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#19981;&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#19988;&#23545;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#65292;&#35757;&#32451;&#27969;&#31243;&#22797;&#26434;&#65292;&#24182;&#19988;&#26080;&#27861;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#21040;&#24212;&#29992;&#31243;&#24207;&#30340;&#26816;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#22270;&#29255;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#24182;&#27979;&#35797;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#20197;&#38646;-shot&#26041;&#24335;&#25552;&#21462;UI&#34920;&#31034;&#65292;&#24182;&#36229;&#36234;&#29616;&#26377;&#30340;&#19987;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#25968;&#23398;&#26041;&#27861;&#23454;&#29616;&#24212;&#29992;&#21040;&#24212;&#29992;&#30340;&#26816;&#32034;&#21644;&#35774;&#35745;&#19968;&#33268;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Extracting semantic representations from mobile user interfaces (UI) and using the representations for designers' decision-making processes have shown the potential to be effective computational design support tools. Current approaches rely on machine learning models trained on small-sized mobile UI datasets to extract semantic vectors and use screenshot-to-screenshot comparison to retrieve similar-looking UIs given query screenshots. However, the usability of these methods is limited because they are often not open-sourced and have complex training pipelines for practitioners to follow, and are unable to perform screenshot set-to-set (i.e., app-to-app) retrieval. To this end, we (1) employ visual models trained with large web-scale images and test whether they could extract a UI representation in a zero-shot way and outperform existing specialized models, and (2) use mathematically founded methods to enable app-to-app retrieval and design consistency analysis. Our experiments show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>TensorCodec&#26159;&#19968;&#31181;&#32039;&#20945;&#30340;&#26377;&#25439;&#24352;&#37327;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#24378;&#25968;&#25454;&#20551;&#35774;&#30340;&#19968;&#33324;&#24352;&#37327;&#12290;&#23427;&#37319;&#29992;&#31070;&#32463;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#12289;&#25240;&#21472;&#36755;&#20837;&#24352;&#37327;&#21644;&#37325;&#26032;&#25490;&#24207;&#27169;&#24335;&#32034;&#24341;&#31561;&#20851;&#38190;&#28857;&#26469;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10310</link><description>&lt;p&gt;
TensorCodec: &#26080;&#24378;&#25968;&#25454;&#20551;&#35774;&#30340;&#32039;&#20945;&#26377;&#25439;&#24352;&#37327;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions. (arXiv:2309.10310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10310
&lt;/p&gt;
&lt;p&gt;
TensorCodec&#26159;&#19968;&#31181;&#32039;&#20945;&#30340;&#26377;&#25439;&#24352;&#37327;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#24378;&#25968;&#25454;&#20551;&#35774;&#30340;&#19968;&#33324;&#24352;&#37327;&#12290;&#23427;&#37319;&#29992;&#31070;&#32463;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#12289;&#25240;&#21472;&#36755;&#20837;&#24352;&#37327;&#21644;&#37325;&#26032;&#25490;&#24207;&#27169;&#24335;&#32034;&#24341;&#31561;&#20851;&#38190;&#28857;&#26469;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#37117;&#26159;&#20197;&#24352;&#37327;&#30340;&#24418;&#24335;&#34920;&#31034;&#30340;&#65292;&#21363;&#22810;&#32500;&#25968;&#20540;&#25968;&#32452;&#12290;&#22914;&#26524;&#19981;&#36827;&#34892;&#21387;&#32553;&#65292;&#23384;&#20648;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#31354;&#38388;&#65292;&#32780;&#19988;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#24352;&#37327;&#21387;&#32553;&#31639;&#27861;&#21487;&#29992;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#20381;&#36182;&#20110;&#20851;&#20110;&#25968;&#25454;&#32500;&#24230;&#12289;&#31232;&#30095;&#24615;&#12289;&#31209;&#21644;&#24179;&#28369;&#24615;&#30340;&#24378;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TENSORCODEC&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#19968;&#33324;&#24352;&#37327;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#31526;&#21512;&#24378;&#20551;&#35774;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;TENSORCODEC&#21253;&#21547;&#20102;&#19977;&#20010;&#20851;&#38190;&#28857;&#12290;&#31532;&#19968;&#20010;&#20851;&#38190;&#28857;&#26159;&#31070;&#32463;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#65288;NTTD&#65289;&#65292;&#25105;&#20204;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#20013;&#65292;&#20197;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#24182;&#20943;&#36731;&#30001;&#20302;&#31209;&#20551;&#35774;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#28857;&#26159;&#23558;&#36755;&#20837;&#24352;&#37327;&#25240;&#21472;&#25104;&#26356;&#39640;&#38454;&#30340;&#24352;&#37327;&#65292;&#20197;&#20943;&#23569;NTTD&#25152;&#38656;&#30340;&#31354;&#38388;&#12290;&#26368;&#21518;&#65292;&#23545;&#36755;&#20837;&#24352;&#37327;&#36827;&#34892;&#27169;&#24335;&#32034;&#24341;&#30340;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#25581;&#31034;&#21487;&#20197;&#34987;&#21033;&#29992;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TENSORCODEC, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions. TENSORCODEC incorporates three key ideas. The first idea is Neural Tensor-Train Decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be explo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10302</link><description>&lt;p&gt;
&#35299;&#32806;&#35757;&#32451;&#65306;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#21333;&#22810;&#39046;&#22495;&#23398;&#20064;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning. (arXiv:2309.10302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10302
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#37325;&#21472;&#20294;&#38750;&#30456;&#21516;&#30340;&#39046;&#22495;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#24179;&#22343;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20559;&#24046;&#21644;&#39046;&#22495;&#20248;&#21183;&#30340;&#25361;&#25112;&#65292;&#20174;&#23545;&#40784;&#20998;&#24067;&#20943;&#23569;&#39046;&#22495;&#24046;&#36317;&#30340;&#35282;&#24230;&#25110;&#36890;&#36807;&#23454;&#26045;&#39046;&#22495;&#29305;&#23450;&#30340;&#22612;&#12289;&#38376;&#29978;&#33267;&#19987;&#23478;&#26469;&#20445;&#30041;&#24046;&#24322;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;MDL&#26041;&#27861;&#12290;MDL&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20855;&#26377;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#25110;&#25439;&#22833;&#20989;&#25968;&#65292;&#24341;&#20837;&#39069;&#22806;&#30340;&#21442;&#25968;&#24182;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#12290;D-Train&#26159;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#20174;&#19968;&#33324;&#21040;&#29305;&#27530;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#22312;&#25152;&#26377;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#28909;&#36523;&#19968;&#20010;&#26681;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#20854;&#25286;&#20998;&#20026;&#22810;&#20010;&#22836;&#37096;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36890;&#36807;&#22266;&#23450;&#39592;&#24178;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10301</link><description>&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#31361;&#20986;&#20316;&#29992;&#65306;&#29702;&#35770;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms. (arXiv:2309.10301v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#19968;&#20010;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#24403;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#28304;&#25968;&#25454;&#20998;&#24067;&#19982;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#20986;&#29616;&#12290;&#34429;&#28982;&#35768;&#22810;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#30456;&#24403;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#26159;&#30450;&#30446;&#24212;&#29992;&#36825;&#20123;&#31639;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#35201;&#30340;&#26159;&#28548;&#28165;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#22312;&#20855;&#22791;&#33391;&#22909;&#30446;&#26631;&#24615;&#33021;&#30340;&#20551;&#35774;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#39044;&#27979;&#20013;&#20855;&#22791;&#26465;&#20214;&#19981;&#21464;&#30340;&#32452;&#20214;&#65288;CICs&#65289;&#30340;&#23384;&#22312;&#20551;&#35774;&#65292;&#36825;&#20123;&#32452;&#20214;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#20445;&#25345;&#26465;&#20214;&#19981;&#21464;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CICs&#65292;&#36890;&#36807;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#65288;CIP&#65289;&#21487;&#20197;&#20272;&#35745;&#65292;&#20855;&#22791;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25552;&#20379;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#30340;&#19977;&#20010;&#31361;&#20986;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CICs&#30340;&#26032;&#31639;&#27861;&#65292;&#21363;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#65288;IW-CIP&#65289;&#65292;&#23427;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation (DA) is a statistical learning problem that arises when the distribution of the source data used to train a model differs from that of the target data used to evaluate the model. While many DA algorithms have demonstrated considerable empirical success, blindly applying these algorithms can often lead to worse performance on new datasets. To address this, it is crucial to clarify the assumptions under which a DA algorithm has good target performance. In this work, we focus on the assumption of the presence of conditionally invariant components (CICs), which are relevant for prediction and remain conditionally invariant across the source and target data. We demonstrate that CICs, which can be estimated through conditional invariant penalty (CIP), play three prominent roles in providing target risk guarantees in DA. First, we propose a new algorithm based on CICs, importance-weighted conditional invariant penalty (IW-CIP), which has target risk guarantees beyond simple 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24494;&#35843;&#21644;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#31639;&#27861;&#26469;&#25913;&#36827;Whisper&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#20248;&#20110;&#26631;&#20934;&#26463;&#25628;&#32034;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10299</link><description>&lt;p&gt;
&#20351;&#29992;&#24494;&#35843;&#21644;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#26469;&#25552;&#39640;Whisper
&lt;/p&gt;
&lt;p&gt;
Using fine-tuning and min lookahead beam search to improve Whisper. (arXiv:2309.10299v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10299
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24494;&#35843;&#21644;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#31639;&#27861;&#26469;&#25913;&#36827;Whisper&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#26368;&#23567;&#20808;&#34892;&#25628;&#32034;&#20248;&#20110;&#26631;&#20934;&#26463;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#36828;&#31163;&#23436;&#32654;&#12290;&#38500;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;Whisper&#20013;&#20351;&#29992;&#30340;&#26463;&#25628;&#32034;&#31639;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#39069;&#22806;&#30340;&#25968;&#25454;&#19978;&#23545;Whisper&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#22312;&#36234;&#21335;&#35821;&#19978;&#65292;&#20351;&#29992;LoRA&#23545;Whisper-Tiny&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#23558;WER&#30340;&#25913;&#36827;&#25552;&#39640;38.49&#65292;&#30456;&#27604;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;1.45&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;Filter-Ends&#21644;Min Lookahead&#35299;&#30721;&#31639;&#27861;&#65292;&#19982;&#26631;&#20934;&#26463;&#25628;&#32034;&#30456;&#27604;&#65292;WER&#22312;&#19968;&#31995;&#21015;&#35821;&#35328;&#20013;&#24179;&#22343;&#38477;&#20302;&#20102;2.26&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;Whisper&#27169;&#22411;&#23610;&#23544;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;Min Lookahead&#20248;&#20110;Whisper&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#26463;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Whisper in low-resource languages is still far from perfect. In addition to a lack of training data on low-resource languages, we identify some limitations in the beam search algorithm used in Whisper. To address these issues, we fine-tune Whisper on additional data and propose an improved decoding algorithm. On the Vietnamese language, fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the zero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to full-parameter fine-tuning. Additionally, by using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search. These results generalise to larger Whisper model sizes. We also prove a theorem that Min Lookahead outperforms the standard beam search algorithm used in Whisper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36712;&#36947;&#31283;&#23450;&#31995;&#32479;&#29992;&#20110;&#22270;&#31034;&#25945;&#23398;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#36712;&#36947;&#28176;&#36817;&#31283;&#23450;&#31995;&#32479;&#36827;&#34892;&#24418;&#21464;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#36319;&#38543;&#29992;&#25143;&#25351;&#23450;&#33609;&#22270;&#36827;&#34892;&#21608;&#26399;&#36816;&#21160;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.10298</link><description>&lt;p&gt;
&#23398;&#20064;&#36712;&#36947;&#31283;&#23450;&#31995;&#32479;&#20197;&#22270;&#31034;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Orbitally Stable Systems for Diagrammatically Teaching. (arXiv:2309.10298v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36712;&#36947;&#31283;&#23450;&#31995;&#32479;&#29992;&#20110;&#22270;&#31034;&#25945;&#23398;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#36712;&#36947;&#28176;&#36817;&#31283;&#23450;&#31995;&#32479;&#36827;&#34892;&#24418;&#21464;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#36319;&#38543;&#29992;&#25143;&#25351;&#23450;&#33609;&#22270;&#36827;&#34892;&#21608;&#26399;&#36816;&#21160;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31034;&#25945;&#23398;&#26159;&#19968;&#31181;&#26426;&#22120;&#20154;&#33719;&#21462;&#26032;&#25216;&#33021;&#30340;&#33539;&#24335;&#65292;&#29992;&#25143;&#22312;&#22330;&#26223;&#22270;&#20687;&#19978;&#25552;&#20379;2D&#33609;&#22270;&#26469;&#25351;&#23548;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#25945;&#23548;&#26426;&#22120;&#20154;&#25509;&#36817;&#34920;&#38754;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#21608;&#26399;&#36816;&#21160;&#30340;&#38382;&#39064;&#65292;&#36816;&#21160;&#30340;&#21608;&#26399;&#21487;&#20197;&#30001;&#29992;&#25143;&#25552;&#20379;&#30340;&#21333;&#20010;&#33609;&#22270;&#22312;&#26426;&#22120;&#20154;&#25668;&#20687;&#22836;&#30340;&#22270;&#20687;&#19978;&#20219;&#24847;&#25351;&#23450;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#31283;&#23450;&#36842;&#27498;&#24418;&#22270;&#31034;&#25945;&#23398;&#8221;&#65288;SDDT&#65289;&#26694;&#26550;&#65292;&#23558;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#24314;&#27169;&#20026;&#8220;&#36712;&#36947;&#28176;&#36817;&#31283;&#23450;&#8221;&#65288;O.A.S.&#65289;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#23398;&#20064;&#36319;&#38543;&#29992;&#25143;&#25351;&#23450;&#30340;&#33609;&#22270;&#12290;&#36890;&#36807;&#24212;&#29992;&#21487;&#24494;&#20998;&#19988;&#21487;&#36870;&#30340;&#20989;&#25968;&#26469;&#23545;&#24050;&#30693;&#30340;O.A.S.&#31995;&#32479;&#36827;&#34892;&#24418;&#21464;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#21442;&#25968;&#21270;&#30340;&#36842;&#27491;&#24418;&#21464;&#22312;&#25105;&#20204;&#24314;&#27169;&#31995;&#32479;&#30340;&#26497;&#38480;&#21608;&#26399;&#21644;&#33609;&#22270;&#20043;&#38388;&#36827;&#34892;Hausdorff&#36317;&#31163;&#20248;&#21270;&#65292;&#20135;&#29983;&#25152;&#38656;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagrammatic Teaching is a paradigm for robots to acquire novel skills, whereby the user provides 2D sketches over images of the scene to shape the robot's motion. In this work, we tackle the problem of teaching a robot to approach a surface and then follow cyclic motion on it, where the cycle of the motion can be arbitrarily specified by a single user-provided sketch over an image from the robot's camera. Accordingly, we introduce the \emph{Stable Diffeomorphic Diagrammatic Teaching} (SDDT) framework. SDDT models the robot's motion as an \emph{Orbitally Asymptotically Stable} (O.A.S.) dynamical system that learns to follow the user-specified sketch. This is achieved by applying a \emph{diffeomorphism}, i.e. a differentiable and invertible function, to morph a known O.A.S. system. The parameterised diffeomorphism is then optimised with respect to the Hausdorff distance between the limit cycle of our modelled system and the sketch, to produce the desired robot motion. We provide theoret
&lt;/p&gt;</description></item><item><title>Koopman&#21487;&#36870;&#33258;&#32534;&#30721;&#22120;&#65288;KIA&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;Koopman&#31639;&#23376;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#26469;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#20445;&#35777;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#25805;&#20316;&#30340;&#21487;&#36870;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10291</link><description>&lt;p&gt;
Koopman&#21487;&#36870;&#33258;&#32534;&#30721;&#22120;&#65306;&#21033;&#29992;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#36827;&#34892;&#26102;&#38388;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Koopman Invertible Autoencoder: Leveraging Forward and Backward Dynamics for Temporal Modeling. (arXiv:2309.10291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10291
&lt;/p&gt;
&lt;p&gt;
Koopman&#21487;&#36870;&#33258;&#32534;&#30721;&#22120;&#65288;KIA&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;Koopman&#31639;&#23376;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#26469;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#20445;&#35777;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#25805;&#20316;&#30340;&#21487;&#36870;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#21482;&#33021;&#25429;&#25417;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#32479;&#35745;&#20851;&#31995;&#65292;&#38590;&#20197;&#23398;&#20064;&#30446;&#26631;&#31995;&#32479;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#65292;&#22240;&#27492;&#24314;&#31435;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman&#31639;&#23376;&#29702;&#35770;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;Koopman&#21487;&#36870;&#33258;&#32534;&#30721;&#22120;&#65288;KIA&#65289;&#65292;&#23427;&#22312;&#26080;&#31351;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#38271;&#26399;&#31995;&#32479;&#34892;&#20026;&#30340;&#26356;&#20934;&#30830;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#36870;&#35774;&#35745;&#20445;&#35777;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#25805;&#20316;&#30340;&#21487;&#36870;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;KIA&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate long-term predictions are the foundations for many machine learning applications and decision-making processes. However, building accurate long-term prediction models remains challenging due to the limitations of existing temporal models like recurrent neural networks (RNNs), as they capture only the statistical connections in the training data and may fail to learn the underlying dynamics of the target system. To tackle this challenge, we propose a novel machine learning model based on Koopman operator theory, which we call Koopman Invertible Autoencoders (KIA), that captures the inherent characteristic of the system by modeling both forward and backward dynamics in the infinite-dimensional Hilbert space. This enables us to efficiently learn low-dimensional representations, resulting in more accurate predictions of long-term system behavior. Moreover, our method's invertibility design guarantees reversibility and consistency in both forward and inverse operations. We illustra
&lt;/p&gt;</description></item><item><title>Flash-LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20302;&#25104;&#26412;&#39640;&#25928;&#22320;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25903;&#25345;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#22312;&#39640;&#24615;&#33021;&#20294;&#20855;&#26377;&#39640;&#38480;&#21046;&#24615;&#30340;Tensor Cores&#19978;&#24037;&#20316;&#12290;&#23427;&#33021;&#22815;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#37327;&#65292;&#24182;&#19988;&#20445;&#25345;&#33391;&#22909;&#30340;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.10285</link><description>&lt;p&gt;
Flash-LLM: &#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#25903;&#25345;&#65292;&#23454;&#29616;&#32463;&#27982;&#39640;&#25928;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity. (arXiv:2309.10285v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10285
&lt;/p&gt;
&lt;p&gt;
Flash-LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20302;&#25104;&#26412;&#39640;&#25928;&#22320;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25903;&#25345;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#22312;&#39640;&#24615;&#33021;&#20294;&#20855;&#26377;&#39640;&#38480;&#21046;&#24615;&#30340;Tensor Cores&#19978;&#24037;&#20316;&#12290;&#23427;&#33021;&#22815;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#37327;&#65292;&#24182;&#19988;&#20445;&#25345;&#33391;&#22909;&#30340;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21442;&#25968;&#35268;&#27169;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#37096;&#32626;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#28040;&#32791;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#38750;&#32467;&#26500;&#21270;&#27169;&#22411;&#20462;&#21098;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;GPU&#20869;&#23384;&#21344;&#29992;&#21644;&#25972;&#20307;&#35745;&#31639;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#27169;&#22411;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22788;&#29702;&#29616;&#20195;GPU&#19978;&#30340;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#26041;&#38754;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#39640;&#25928;&#30340;&#25903;&#25345;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#24352;&#37327;&#26680;&#24515;&#30828;&#20214;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Flash-LLM&#65292;&#20197;&#23454;&#29616;&#23545;&#39640;&#24615;&#33021;&#20294;&#20855;&#26377;&#39640;&#38480;&#21046;&#24615;&#30340;&#24352;&#37327;&#26680;&#24515;&#19978;&#30340;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#36827;&#34892;&#20302;&#25104;&#26412;&#21644;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#65292;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#33509;&#24178;&#20010;skinny&#30697;&#38453;&#20056;&#27861;&#65292;&#20854;&#20013;&#30001;&#20110;&#35745;&#31639;&#24378;&#24230;&#20302;&#65292;Tensor Cores&#30340;&#21033;&#29992;&#29575;&#26126;&#26174;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36127;&#36733;...
&lt;/p&gt;
&lt;p&gt;
With the fast growth of parameter size, it becomes increasingly challenging to deploy large generative models as they typically require large GPU memory consumption and massive computation. Unstructured model pruning has been a common approach to reduce both GPU memory footprint and the overall computation while retaining good model accuracy. However, the existing solutions do not provide a highly-efficient support for handling unstructured sparsity on modern GPUs, especially on the highly-structured Tensor Core hardware. Therefore, we propose Flash-LLM for enabling low-cost and highly-efficient large generative model inference with the sophisticated support of unstructured sparsity on high-performance but highly restrictive Tensor Cores. Based on our key observation that the main bottleneck of generative model inference is the several skinny matrix multiplications for which Tensor Cores would be significantly under-utilized due to low computational intensity, we propose a general Load
&lt;/p&gt;</description></item><item><title>FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10283</link><description>&lt;p&gt;
FRAMU: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10283
&lt;/p&gt;
&lt;p&gt;
FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#36890;&#36807;&#20801;&#35768;&#20174;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#21024;&#38500;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#65292;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20351;&#29992;&#36807;&#26102;&#30340;&#12289;&#31169;&#26377;&#30340;&#21644;&#26080;&#20851;&#30340;&#25968;&#25454;&#20250;&#24341;&#21457;&#19982;&#38544;&#31169;&#21644;&#27169;&#22411;&#25928;&#29575;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#24433;&#21709;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36951;&#24536;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#36824;&#20250;&#23545;&#25968;&#25454;&#38544;&#31169;&#36896;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;&#65288;FRAMU&#65289;&#12290;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#26159;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#65288;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65289;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;FRAMU&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#12289;&#36951;&#24536;&#36807;&#26102;&#12289;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25903;&#25345;&#27169;&#22411;&#25345;&#32493;&#28436;&#36827;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#35821;&#38899;&#38899;&#39057;&#30340;&#20154;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#27169;&#22411;&#23454;&#29616;&#21307;&#38498;&#20505;&#35786;&#23460;&#30340;&#21344;&#29992;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#26159;&#39318;&#27425;&#25552;&#20986;&#20351;&#29992;&#38750;&#35821;&#38899;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#21344;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10280</link><description>&lt;p&gt;
Crowdotic&#65306;&#22522;&#20110;Transformer&#30340;&#21307;&#38498;&#20505;&#35786;&#23460;&#38750;&#35821;&#38899;&#38899;&#39057;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#21344;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Crowdotic: Transformer-based Occupancy Estimation for Hospital Waiting Rooms with Non-speech Audio and Differential Privacy. (arXiv:2309.10280v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#35821;&#38899;&#38899;&#39057;&#30340;&#20154;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#27169;&#22411;&#23454;&#29616;&#21307;&#38498;&#20505;&#35786;&#23460;&#30340;&#21344;&#29992;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#26159;&#39318;&#27425;&#25552;&#20986;&#20351;&#29992;&#38750;&#35821;&#38899;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#21344;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#20154;&#32676;&#23494;&#24230;&#20998;&#26512;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#22312;&#25552;&#39640;&#26234;&#33021;&#24314;&#31569;&#36816;&#33829;&#31649;&#29702;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#19981;&#21516;&#31354;&#38388;&#38544;&#31169;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#35821;&#38899;&#38899;&#39057;&#30340;&#20154;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#38750;&#35821;&#38899;&#38899;&#39057;&#23601;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#20998;&#26512;&#65292;&#32780;&#19988;&#20934;&#30830;&#24615;&#21313;&#20998;&#20986;&#33394;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#25552;&#20986;&#20351;&#29992;&#38750;&#35821;&#38899;&#38899;&#39057;&#20449;&#21495;&#26469;&#39044;&#27979;&#21344;&#29992;&#24773;&#20917;&#12290;&#25130;&#33267;&#30446;&#21069;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#26377;&#20854;&#20182;&#31867;&#20284;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#19968;&#23478;&#22823;&#22411;&#21307;&#38498;&#30340;&#20505;&#35786;&#23460;&#20013;&#37096;&#32626;&#20102;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#24179;&#21488;&#65292;&#24182;&#33719;&#24471;&#20102;IRB&#25209;&#20934;&#65292;&#22312;&#25968;&#20010;&#26376;&#30340;&#26102;&#38388;&#37324;&#25429;&#33719;&#20102;&#38750;&#35821;&#38899;&#38899;&#39057;&#21644;&#28909;&#20687;&#22270;&#20197;&#29992;&#20110;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#38750;&#35821;&#38899;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#22522;&#20110;&#28909;&#20687;&#25668;&#20687;&#22836;&#30340;&#27169;&#22411;&#21644;&#20854;&#20182;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving crowd density analysis finds application across a wide range of scenarios, substantially enhancing smart building operation and management while upholding privacy expectations in various spaces. We propose a non-speech audio-based approach for crowd analytics, leveraging a transformer-based model. Our results demonstrate that non-speech audio alone can be used to conduct such analysis with remarkable accuracy. To the best of our knowledge, this is the first time when non-speech audio signals are proposed for predicting occupancy. As far as we know, there has been no other similar approach of its kind prior to this. To accomplish this, we deployed our sensor-based platform in the waiting room of a large hospital with IRB approval over a period of several months to capture non-speech audio and thermal images for the training and evaluation of our models. The proposed non-speech-based approach outperformed the thermal camera-based model and all other baselines. In addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#31995;&#32479;&#36716;&#21464;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#36890;&#36807;&#20559;&#32622;&#21407;&#22987;&#21160;&#21147;&#23398;&#26469;&#20419;&#36827;&#36716;&#21464;&#65292;&#21478;&#19968;&#31181;&#26159;&#23558;&#21407;&#22987;&#36716;&#21464;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#36716;&#21464;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25968;&#25454;&#20016;&#23500;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#37117;&#23637;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10276</link><description>&lt;p&gt;
&#29983;&#25104;&#36807;&#28193;&#36335;&#24452;&#30340;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diffusion Methods for Generating Transition Paths. (arXiv:2309.10276v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#31995;&#32479;&#36716;&#21464;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#36890;&#36807;&#20559;&#32622;&#21407;&#22987;&#21160;&#21147;&#23398;&#26469;&#20419;&#36827;&#36716;&#21464;&#65292;&#21478;&#19968;&#31181;&#26159;&#23558;&#21407;&#22987;&#36716;&#21464;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#36716;&#21464;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25968;&#25454;&#20016;&#23500;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#37117;&#23637;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#20351;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#27169;&#25311;&#31232;&#26377;&#30340;&#20122;&#31283;&#24577;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36716;&#21464;&#36335;&#24452;&#30340;&#39640;&#25928;&#26041;&#27861;&#23545;&#20110;&#30740;&#31350;&#20998;&#23376;&#31995;&#32479;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#36335;&#24452;&#29983;&#25104;&#26041;&#27861;&#65306;&#22522;&#20110;&#38142;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#20013;&#28857;&#30340;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#20559;&#32622;&#21407;&#22987;&#21160;&#21147;&#23398;&#26469;&#20419;&#36827;&#36716;&#21464;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#21017;&#37319;&#29992;&#20998;&#35010;&#25216;&#26415;&#24182;&#23558;&#21407;&#22987;&#36716;&#21464;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#36716;&#21464;&#12290;&#22312;M\"uller&#21183;&#21644;&#20108;&#32957;&#22522;&#19993;&#27688;&#37240;&#30340;&#29983;&#25104;&#36716;&#21464;&#36335;&#24452;&#30340;&#25968;&#20540;&#32467;&#26524;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25968;&#25454;&#20016;&#23500;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#37117;&#23637;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we seek to simulate rare transitions between metastable states using score-based generative models. An efficient method for generating high-quality transition paths is valuable for the study of molecular systems since data is often difficult to obtain. We develop two novel methods for path generation in this paper: a chain-based approach and a midpoint-based approach. The first biases the original dynamics to facilitate transitions, while the second mirrors splitting techniques and breaks down the original transition into smaller transitions. Numerical results of generated transition paths for the M\"uller potential and for Alanine dipeptide demonstrate the effectiveness of these approaches in both the data-rich and data-scarce regimes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.10275</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#20026;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#20013;&#32676;&#20307;&#12289;&#33258;&#21160;&#21270;&#20179;&#20648;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#21035;&#65306;&#38598;&#20013;&#24335;&#35268;&#21010;&#21644;&#20998;&#25955;&#24335;&#35268;&#21010;&#12290;&#38598;&#20013;&#24335;&#35268;&#21010;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#22240;&#27492;&#22312;&#22823;&#22411;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20998;&#25955;&#24335;&#35268;&#21010;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#26102;&#36335;&#24452;&#35268;&#21010;&#65292;&#23637;&#31034;&#20102;&#38544;&#24335;&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#19988;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRAMP&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#24335;&#35838;&#31243;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.10254</link><description>&lt;p&gt;
LLM&#24179;&#21488;&#23433;&#20840;&#65306;&#23558;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;ChatGPT&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22914;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24179;&#21488;&#24320;&#22987;&#25552;&#20379;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#65292;&#20197;&#19982;&#20114;&#32852;&#32593;&#19978;&#30340;&#31532;&#19977;&#26041;&#26381;&#21153;&#36827;&#34892;&#20132;&#20114;&#12290;&#34429;&#28982;&#36825;&#20123;&#25554;&#20214;&#25193;&#23637;&#20102;LLM&#24179;&#21488;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#26159;&#30001;&#20219;&#24847;&#30340;&#31532;&#19977;&#26041;&#24320;&#21457;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#38544;&#24335;&#20449;&#20219;&#12290;&#25554;&#20214;&#36824;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;LLM&#24179;&#21488;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#31946;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;LLM&#24179;&#21488;&#35774;&#35745;&#32773;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#20010;&#25915;&#20987;&#20998;&#31867;&#27861;&#30340;&#34920;&#36848;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;LLM&#24179;&#21488;&#30456;&#20851;&#26041;&#22914;&#20309;&#21033;&#29992;&#20182;&#20204;&#30340;&#33021;&#21147;&#21644;&#36131;&#20219;&#23545;&#24444;&#27492;&#36827;&#34892;&#25915;&#20987;&#26469;&#24320;&#21457;&#30340;&#12290;&#20316;&#20026;&#25105;&#20204;&#36845;&#20195;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the poten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#21160;&#20316;&#29983;&#25104;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#35780;&#20272;&#26368;&#21563;&#21512;&#30340;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26356;&#20248;&#24230;&#37327;&#12290;&#32467;&#26524;&#21457;&#29616;&#24403;&#21069;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#20302;&#65292;&#20294;&#29992;&#20110;&#35780;&#20272;&#24179;&#22343;&#27169;&#22411;&#24615;&#33021;&#30340;&#24120;&#29992;&#24230;&#37327;&#26174;&#31034;&#20986;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10248</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#21160;&#20316;&#29983;&#25104;&#30340;&#26368;&#20339;&#33258;&#21160;&#24230;&#37327;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is the Best Automated Metric for Text to Motion Generation?. (arXiv:2309.10248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#21160;&#20316;&#29983;&#25104;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#35780;&#20272;&#26368;&#21563;&#21512;&#30340;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26356;&#20248;&#24230;&#37327;&#12290;&#32467;&#26524;&#21457;&#29616;&#24403;&#21069;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#20302;&#65292;&#20294;&#29992;&#20110;&#35780;&#20272;&#24179;&#22343;&#27169;&#22411;&#24615;&#33021;&#30340;&#24120;&#29992;&#24230;&#37327;&#26174;&#31034;&#20986;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#24182;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20026;&#27492;&#20219;&#21153;&#24320;&#21457;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#65292;&#20294;&#22312;&#30830;&#23450;&#36866;&#24403;&#30340;&#35780;&#20272;&#24230;&#37327;&#26041;&#38754;&#27809;&#26377;&#36827;&#34892;&#37325;&#35201;&#24037;&#20316;&#12290;&#20154;&#31867;&#35780;&#20272;&#26159;&#35813;&#20219;&#21153;&#30340;&#26368;&#32456;&#20934;&#30830;&#24615;&#24230;&#37327;&#26631;&#20934;&#65292;&#33258;&#21160;&#24230;&#37327;&#24212;&#19982;&#20154;&#31867;&#36136;&#37327;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#30001;&#20110;&#25551;&#36848;&#19982;&#35768;&#22810;&#21160;&#20316;&#30456;&#20860;&#23481;&#65292;&#30830;&#23450;&#27491;&#30830;&#30340;&#24230;&#37327;&#23545;&#20110;&#35780;&#20272;&#21644;&#35774;&#35745;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21738;&#20123;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20272;&#26368;&#21563;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20043;&#26356;&#21152;&#21563;&#21512;&#30340;&#26032;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#24230;&#37327;&#20013;&#27809;&#26377;&#19968;&#20010;&#22312;&#26679;&#26412;&#32423;&#21035;&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26377;&#20013;&#31561;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35780;&#20272;&#24179;&#22343;&#27169;&#22411;&#24615;&#33021;&#65292;&#24120;&#29992;&#30340;&#24230;&#37327;&#22914;R-Precision&#21644;&#36739;&#23569;&#20351;&#29992;&#30340;&#22352;&#26631;&#35823;&#24046;&#26174;&#31034;&#20986;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in generating skeleton-based human motions from natural language descriptions. While most efforts have focused on developing better neural architectures for this task, there has been no significant work on determining the proper evaluation metric. Human evaluation is the ultimate accuracy measure for this task, and automated metrics should correlate well with human quality judgments. Since descriptions are compatible with many motions, determining the right metric is critical for evaluating and designing effective generative models. This paper systematically studies which metrics best align with human evaluations and proposes new metrics that align even better. Our findings indicate that none of the metrics currently used for this task show even a moderate correlation with human judgments on a sample level. However, for assessing average model performance, commonly used metrics such as R-Precision and less-used coordinate errors show strong correlations. Addit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22312;&#20219;&#24847;&#25968;&#25454;&#27969;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#25512;&#23548;&#20986;&#20102;&#26126;&#30830;&#30340;&#22352;&#26631;&#19981;&#21464;&#20844;&#24335;&#26469;&#35745;&#31639;&#20869;&#22312;&#21644;&#22806;&#22312;&#26354;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20854;&#20013;&#20869;&#22312;&#26354;&#29575;&#24230;&#37327;&#30053;&#20248;&#20110;&#22806;&#22312;&#26354;&#29575;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10237</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#26174;&#24335;&#26354;&#29575;&#27491;&#21017;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Explicit Curvature Regularization in Deep Generative Models. (arXiv:2309.10237v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22312;&#20219;&#24847;&#25968;&#25454;&#27969;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#25512;&#23548;&#20986;&#20102;&#26126;&#30830;&#30340;&#22352;&#26631;&#19981;&#21464;&#20844;&#24335;&#26469;&#35745;&#31639;&#20869;&#22312;&#21644;&#22806;&#22312;&#26354;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20854;&#20013;&#20869;&#22312;&#26354;&#29575;&#24230;&#37327;&#30053;&#20248;&#20110;&#22806;&#22312;&#26354;&#29575;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26063;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;&#23884;&#20837;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#25968;&#25454;&#27969;&#24418;&#65292;&#25512;&#23548;&#20986;&#20102;&#22312;&#20869;&#22312;&#21644;&#22806;&#22312;&#26354;&#29575;&#24230;&#37327;&#19979;&#30340;&#26126;&#30830;&#30340;&#22352;&#26631;&#19981;&#21464;&#20844;&#24335;&#12290;&#30001;&#20110;&#35745;&#31639;&#26354;&#29575;&#26159;&#19968;&#20010;&#28041;&#21450;&#21040;&#20108;&#38454;&#23548;&#25968;&#27714;&#20540;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#36807;&#31243;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#29992;&#20110;&#36817;&#20284;&#35780;&#20272;&#20869;&#22312;&#21644;&#22806;&#22312;&#26354;&#29575;&#30340;&#26377;&#25928;&#20844;&#24335;&#12290;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20102;&#20869;&#22312;&#26354;&#29575;&#21644;&#22806;&#22312;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#24230;&#37327;&#30340;&#30456;&#23545;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#19982;&#29616;&#26377;&#33258;&#21160;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#26354;&#29575;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#19988;&#20869;&#22312;&#26354;&#29575;&#24230;&#37327;&#30053;&#20248;&#20110;&#22806;&#22312;&#26354;&#29575;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a family of curvature-based regularization terms for deep generative model learning. Explicit coordinate-invariant formulas for both intrinsic and extrinsic curvature measures are derived for the case of arbitrary data manifolds embedded in higher-dimensional Euclidean space. Because computing the curvature is a highly computation-intensive process involving the evaluation of second-order derivatives, efficient formulas are derived for approximately evaluating intrinsic and extrinsic curvatures. Comparative studies are conducted that compare the relative efficacy of intrinsic versus extrinsic curvature-based regularization measures, as well as performance comparisons against existing autoencoder training methods. Experiments involving noisy motion capture data confirm that curvature-based methods outperform existing autoencoder regularization methods, with intrinsic curvature measures slightly more effective than extrinsic curvature measures.
&lt;/p&gt;</description></item><item><title>&#22810;&#23618;&#27425;&#27668;&#20505;&#27169;&#24335;&#21442;&#25968;&#21270;&#26041;&#27861;&#25972;&#21512;&#20102;&#19981;&#21516;&#20934;&#30830;&#24615;&#21644;&#20016;&#23500;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#26041;&#27861;&#27867;&#21270;&#21644;&#22806;&#25512;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#39044;&#27979;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10231</link><description>&lt;p&gt;
&#22810;&#23618;&#27425;&#27668;&#20505;&#27169;&#24335;&#21442;&#25968;&#21270;&#25552;&#39640;&#27867;&#21270;&#21644;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity climate model parameterization for better generalization and extrapolation. (arXiv:2309.10231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10231
&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#27425;&#27668;&#20505;&#27169;&#24335;&#21442;&#25968;&#21270;&#26041;&#27861;&#25972;&#21512;&#20102;&#19981;&#21516;&#20934;&#30830;&#24615;&#21644;&#20016;&#23500;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#26041;&#27861;&#27867;&#21270;&#21644;&#22806;&#25512;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#39044;&#27979;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27668;&#20505;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#20840;&#29699;&#27668;&#20505;&#27169;&#22411;&#25110;&#28237;&#27969;&#27169;&#25311;&#20013;&#30340;&#23376;&#32593;&#26684;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#26041;&#26696;&#65307;&#30456;&#27604;&#20110;&#22522;&#20110;&#32463;&#39564;&#20844;&#24335;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#27867;&#21270;&#21644;&#22806;&#25512;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#32780;&#36825;&#23545;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#39044;&#27979;&#20197;&#21450;&#28237;&#27969;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#24577;&#21183;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22810;&#23618;&#27425;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25972;&#21512;&#20102;&#19981;&#21516;&#20934;&#30830;&#24615;&#21644;&#20016;&#23500;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#25552;&#20379;&#26368;&#20339;&#30340;&#32467;&#26524;&#65306;&#22522;&#20110;&#29289;&#29702;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20855;&#22791;&#22806;&#25512;&#33021;&#21147;&#65292;&#24182;&#19988;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#22312;&#24212;&#29992;&#20110;&#27668;&#20505;&#24314;&#27169;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#22810;&#23618;&#27425;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#22823;&#24133;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning-based parameterizations (i.e. representation of sub-grid processes) of global climate models or turbulent simulations have recently been proposed as a powerful alternative to physical, but empirical, representations, offering a lower computational cost and higher accuracy. Yet, those approaches still suffer from a lack of generalization and extrapolation beyond the training data, which is however critical to projecting climate change or unobserved regimes of turbulence. Here we show that a multi-fidelity approach, which integrates datasets of different accuracy and abundance, can provide the best of both worlds: the capacity to extrapolate leveraging the physically-based parameterization and a higher accuracy using the machine-learning-based parameterizations. In an application to climate modeling, the multi-fidelity framework yields more accurate climate projections without requiring major increase in computational resources. Our multi-fidelity randomized prior networ
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20154;&#29983;&#25104;&#30340;&#22240;&#26524;&#30693;&#35782;&#26469;&#25913;&#36827;&#25968;&#25454;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25351;&#31034;&#20102;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10211</link><description>&lt;p&gt;
&#22240;&#26524;&#29702;&#35770;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#34920;&#31034;&#30340;&#25913;&#36827;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification. (arXiv:2309.10211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10211
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#29983;&#25104;&#30340;&#22240;&#26524;&#30693;&#35782;&#26469;&#25913;&#36827;&#25968;&#25454;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25351;&#31034;&#20102;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22914;&#20309;&#20351;&#29992;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22240;&#26524;&#29702;&#35770;&#21644;&#26469;&#33258;&#21160;&#21147;&#31995;&#32479;&#25991;&#29486;&#30340;&#24037;&#20855;&#65292;&#26469;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#20998;&#31867;&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#26469;&#23637;&#31034;&#65292;&#20351;&#29992;&#23558;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#19981;&#21464;&#32467;&#26500;&#22240;&#26524;&#29305;&#24449;&#26126;&#30830;&#26174;&#31034;&#22312;&#25968;&#25454;&#34920;&#31034;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26356;&#20026;&#22825;&#30495;&#30340;&#25968;&#25454;&#34920;&#31034;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#20154;&#29983;&#25104;&#30340;&#22240;&#26524;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#32773;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#21152;&#26126;&#30830;&#35268;&#33539;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#12290;&#36825;&#36827;&#32780;&#25351;&#31034;&#20102;&#36890;&#36807;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#23454;&#36341;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#26356;&#24191;&#27867;&#21162;&#21147;&#20013;&#30340;&#21160;&#21147;&#31995;&#32479;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider how human-centered causal theories and tools from the dynamical systems literature can be deployed to guide the representation of data when training neural networks for complex classification tasks. Specifically, we use simulated data to show that training a neural network with a data representation that makes explicit the invariant structural causal features of the data generating process of an epidemic system improves out-of-distribution (OOD) generalization performance on a classification task as compared to a more naive approach to data representation. We take these results to demonstrate that using human-generated causal knowledge to reduce the epistemic uncertainty of ML developers can lead to more well-specified ML pipelines. This, in turn, points to the utility of a dynamical systems approach to the broader effort aimed at improving the robustness and safety of machine learning systems via improved ML system development practices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10194</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
The Kernel Density Integral Transformation. (arXiv:2309.10194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#20110;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#29305;&#24449;&#39044;&#22788;&#29702;&#32487;&#32493;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32508;&#21512;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#26041;&#27861;&#20316;&#20026;&#26497;&#38480;&#24773;&#20917;&#65306;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#19981;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#21487;&#20197;&#20316;&#20026;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#24369;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#35843;&#25972;&#19968;&#20010;&#36830;&#32493;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#32463;&#24120;&#20248;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#26680;&#23494;&#24230;&#36716;&#25442;&#21487;&#20197;&#26377;&#30410;&#22320;&#24212;&#29992;&#20110;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#20851;&#24615;&#20998;&#26512;&#21644;&#21333;&#21464;&#37327;&#32858;&#31867;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#38543;&#26426;&#28145;&#24230;&#24211;&#26222;&#26364;&#65288;SDK&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#22810;&#38454;&#27573;&#21046;&#36896;&#31995;&#32479;&#65288;MMS&#65289;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#20256;&#25773;&#20851;&#38190;&#36136;&#37327;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10193</link><description>&lt;p&gt;
&#38543;&#26426;&#28145;&#24230;&#24211;&#26222;&#26364;&#27169;&#22411;&#29992;&#20110;&#22810;&#38454;&#27573;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#36136;&#37327;&#20256;&#25773;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Deep Koopman Model for Quality Propagation Analysis in Multistage Manufacturing Systems. (arXiv:2309.10193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#38543;&#26426;&#28145;&#24230;&#24211;&#26222;&#26364;&#65288;SDK&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#22810;&#38454;&#27573;&#21046;&#36896;&#31995;&#32479;&#65288;MMS&#65289;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#20256;&#25773;&#20851;&#38190;&#36136;&#37327;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38454;&#27573;&#21046;&#36896;&#31995;&#32479;&#65288;MMS&#65289;&#30340;&#24314;&#27169;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#26356;&#22810;&#20851;&#27880;&#12290;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#20026;&#20197;&#36739;&#20302;&#25104;&#26412;&#21644;&#19987;&#19994;&#30693;&#35782;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#38543;&#26426;&#28145;&#24230;&#24211;&#26222;&#26364;&#65288;SDK&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;MMS&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24211;&#26222;&#26364;&#31639;&#23376;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#20851;&#38190;&#36136;&#37327;&#20449;&#24687;&#30340;&#20256;&#25773;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#36716;&#25442;&#30340;&#32447;&#24615;&#34920;&#31034;&#26377;&#25928;&#22320;&#25429;&#25417;&#20135;&#21697;&#36136;&#37327;&#30340;&#19968;&#33324;&#38750;&#32447;&#24615;&#28436;&#21464;&#65292;&#20174;&#32780;&#22686;&#24378;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;SDK&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#22914;&#19979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#27979;&#20998;&#38454;&#27573;&#20135;&#21697;&#36136;&#37327;&#26041;&#38754;&#65292;SDK&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#20854;&#20182;&#27969;&#34892;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modeling of multistage manufacturing systems (MMSs) has attracted increased attention from both academia and industry. Recent advancements in deep learning methods provide an opportunity to accomplish this task with reduced cost and expertise. This study introduces a stochastic deep Koopman (SDK) framework to model the complex behavior of MMSs. Specifically, we present a novel application of Koopman operators to propagate critical quality information extracted by variational autoencoders. Through this framework, we can effectively capture the general nonlinear evolution of product quality using a transferred linear representation, thus enhancing the interpretability of the data-driven model. To evaluate the performance of the SDK framework, we carried out a comparative study on an open-source dataset. The main findings of this paper are as follows. Our results indicate that SDK surpasses other popular data-driven models in accuracy when predicting stagewise product quality within t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#30340;&#38480;&#21046;&#65292;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.10186</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#26234;&#33021;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence. (arXiv:2309.10186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#30340;&#38480;&#21046;&#65292;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20197;&#20854;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#24207;&#21015;&#20219;&#21153;&#21644;&#23398;&#20064;&#28508;&#22312;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#32034;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20551;&#35774;&#25968;&#25454;&#31561;&#38388;&#38548;&#26377;&#24207;&#20197;&#21450;&#26080;&#27861;&#20805;&#20998;&#34701;&#20837;&#22270;&#32467;&#26500;&#31561;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GNN&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;GNN&#33021;&#22815;&#26174;&#24335;&#22320;&#23558;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#32435;&#20837;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20363;&#22914;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#20013;&#30340;&#26102;&#24207;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#32593;&#32476;-&#20113;&#38598;&#25104;&#29615;&#22659;&#20013;&#30340;&#26381;&#21153;&#37096;&#32626;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#23481;&#37327;&#38480;&#21046;&#12289;&#21160;&#24577;&#29992;&#25143;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#24182;&#35299;&#20915;&#20102;&#36229;&#36234;5G&#20013;&#30340;&#26381;&#21153;&#36830;&#32493;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10185</link><description>&lt;p&gt;
&#22522;&#20110;&#20113;-&#32593;&#32476;&#38598;&#25104;&#30340;&#36229;&#36234;5G&#20013;&#30340;QoS&#24863;&#30693;&#26381;&#21153;&#39044;&#27979;&#19982;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
QoS-Aware Service Prediction and Orchestration in Cloud-Network Integrated Beyond 5G. (arXiv:2309.10185v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#32593;&#32476;-&#20113;&#38598;&#25104;&#29615;&#22659;&#20013;&#30340;&#26381;&#21153;&#37096;&#32626;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#23481;&#37327;&#38480;&#21046;&#12289;&#21160;&#24577;&#29992;&#25143;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#24182;&#35299;&#20915;&#20102;&#36229;&#36234;5G&#20013;&#30340;&#26381;&#21153;&#36830;&#32493;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;&#20803;&#23431;&#23449;&#31561;&#26032;&#22411;&#24212;&#29992;&#31361;&#26174;&#20102;&#36229;&#36234;5G&#32593;&#32476;&#30340;&#28508;&#21147;&#65292;&#36825;&#38656;&#35201;&#36229;&#20302;&#24310;&#36831;&#36890;&#20449;&#21644;&#22823;&#35268;&#27169;&#30340;&#23485;&#24102;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#20276;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#19981;&#26029;&#27874;&#21160;&#30340;&#36825;&#31867;&#26381;&#21153;&#30340;&#34028;&#21187;&#38656;&#27714;&#65292;&#20351;&#24471;&#22312;B5G&#20013;&#38656;&#35201;&#26356;&#21152;&#20851;&#27880;&#26381;&#21153;&#36830;&#32493;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#26381;&#21153;&#65292;&#36793;&#32536;&#20113;&#27169;&#24335;&#26159;&#21033;&#29992;&#20113;&#23481;&#37327;&#24182;&#23454;&#26102;&#26377;&#25928;&#22320;&#31649;&#29702;&#29992;&#25143;&#22312;&#32593;&#32476;&#20013;&#31227;&#21160;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#20113;&#32593;&#32476;&#38754;&#20020;&#30528;&#35832;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#24517;&#39035;&#38598;&#20307;&#31649;&#29702;&#32593;&#32476;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#37322;&#25918;&#20854;&#20840;&#37096;&#28508;&#21147;&#12290;&#26412;&#25991;&#22312;&#32771;&#34385;&#23481;&#37327;&#38480;&#21046;&#12289;&#21160;&#24577;&#29992;&#25143;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;-&#20113;&#38598;&#25104;&#29615;&#22659;&#20013;&#26381;&#21153;&#37096;&#32626;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21046;&#23450;&#20102;&#20248;&#21270;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel applications such as the Metaverse have highlighted the potential of beyond 5G networks, which necessitate ultra-low latency communications and massive broadband connections. Moreover, the burgeoning demand for such services with ever-fluctuating users has engendered a need for heightened service continuity consideration in B5G. To enable these services, the edge-cloud paradigm is a potential solution to harness cloud capacity and effectively manage users in real time as they move across the network. However, edge-cloud networks confront a multitude of limitations, including networking and computing resources that must be collectively managed to unlock their full potential. This paper addresses the joint problem of service placement and resource allocation in a network-cloud integrated environment while considering capacity constraints, dynamic users, and end-to-end delays. We present a non-linear programming model that formulates the optimization problem with the aiming objectiv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;B&amp;B-CCRA&#21644;WF-CCRA&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.10180</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#20302;&#24310;&#36831;5G&#24212;&#29992;&#36335;&#24452;&#36873;&#25321;&#21644;&#26381;&#21153;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Double Deep Q-Learning-based Path Selection and Service Placement for Latency-Sensitive Beyond 5G Applications. (arXiv:2309.10180v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;B&amp;B-CCRA&#21644;WF-CCRA&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#38543;&#30528;&#23545;&#23481;&#37327;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#20840;&#26032;&#30340;&#26381;&#21153;&#27491;&#22312;&#28044;&#29616;&#12290;&#20026;&#20102;&#20197;&#23454;&#26102;&#21709;&#24212;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#25552;&#20379;&#36825;&#20123;&#26381;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;&#22362;&#23454;&#30340;&#20113;&#32593;&#32476;&#38598;&#25104;&#22522;&#30784;&#35774;&#26045;&#12290;&#30001;&#20110;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#38480;&#23481;&#37327;&#65292;&#24517;&#39035;&#21327;&#21516;&#31649;&#29702;&#36825;&#20123;&#36164;&#28304;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#26041;&#27861;&#26469;&#32534;&#25490;&#36825;&#20123;&#36164;&#28304;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#24573;&#30053;&#20102;&#32593;&#32476;&#36164;&#28304;&#25110;&#23558;&#32593;&#32476;&#31616;&#21270;&#20026;&#31616;&#21333;&#30340;&#22270;&#65292;&#21482;&#20851;&#27880;&#20113;&#36164;&#28304;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#38382;&#39064;&#65292;&#31216;&#20026;CCRA&#65292;&#21253;&#25324;&#21151;&#33021;&#37096;&#32626;&#21644;&#20998;&#37197;&#12289;&#27969;&#37327;&#20248;&#20808;&#32423;&#21644;&#36335;&#24452;&#36873;&#25321;&#65292;&#32771;&#34385;&#23481;&#37327;&#38480;&#21046;&#21644;&#36136;&#37327;&#35201;&#27714;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#38750;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;B&amp;B-CCRA&#21644;WF-CCRA&#65292;&#22522;&#20110;Br...
&lt;/p&gt;
&lt;p&gt;
Nowadays, as the need for capacity continues to grow, entirely novel services are emerging. A solid cloud-network integrated infrastructure is necessary to supply these services in a real-time responsive, and scalable way. Due to their diverse characteristics and limited capacity, communication and computing resources must be collaboratively managed to unleash their full potential. Although several innovative methods have been proposed to orchestrate the resources, most ignored network resources or relaxed the network as a simple graph, focusing only on cloud resources. This paper fills the gap by studying the joint problem of communication and computing resource allocation, dubbed CCRA, including function placement and assignment, traffic prioritization, and path selection considering capacity constraints and quality requirements, to minimize total cost. We formulate the problem as a non-linear programming model and propose two approaches, dubbed B\&amp;B-CCRA and WF-CCRA, based on the Br
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#20803;&#23431;&#23449;&#24212;&#29992;&#20013;&#30340;&#22810;&#37325;&#25509;&#20837;&#38382;&#39064;&#65292;&#22312;&#37319;&#29992;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#36830;&#32493;&#35757;&#32451;&#26469;&#23454;&#29616;&#33258;&#25105;&#32500;&#25345;&#30340;&#31574;&#30053;&#12290;&#35813;&#30740;&#31350;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#36866;&#24212;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#20195;&#29702;&#26426;&#21046;&#38382;&#39064;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.10177</link><description>&lt;p&gt;
&#33258;&#25105;&#32500;&#25345;&#30340;&#36830;&#32493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21160;&#24577;&#20803;&#23431;&#23449;&#24212;&#29992;&#20013;&#30340;&#22810;&#37325;&#25509;&#20837;
&lt;/p&gt;
&lt;p&gt;
Self-Sustaining Multiple Access with Continual Deep Reinforcement Learning for Dynamic Metaverse Applications. (arXiv:2309.10177v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#20803;&#23431;&#23449;&#24212;&#29992;&#20013;&#30340;&#22810;&#37325;&#25509;&#20837;&#38382;&#39064;&#65292;&#22312;&#37319;&#29992;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#36830;&#32493;&#35757;&#32451;&#26469;&#23454;&#29616;&#33258;&#25105;&#32500;&#25345;&#30340;&#31574;&#30053;&#12290;&#35813;&#30740;&#31350;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#36866;&#24212;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#20195;&#29702;&#26426;&#21046;&#38382;&#39064;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#26159;&#19968;&#20010;&#26088;&#22312;&#21019;&#24314;&#30001;&#20247;&#22810;&#19990;&#30028;&#32452;&#25104;&#30340;&#34394;&#25311;&#29615;&#22659;&#30340;&#26032;&#33539; Paradigm&#65292;&#20854;&#20013;&#27599;&#20010;&#19990;&#30028;&#23558;&#25552;&#20379;&#19981;&#21516;&#30340;&#26381;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#22914;&#27492;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#22330;&#26223;&#65292;&#24182;&#32771;&#34385;&#21040;&#38024;&#23545;&#31532;&#20845;&#20195;&#36890;&#20449;&#31995;&#32479;&#65288;6G&#65289;&#30340;&#20005;&#26684;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#65292;&#19968;&#31181;&#28508;&#22312;&#30340;&#26041;&#27861;&#26159;&#37319;&#29992;&#33258;&#25105;&#32500;&#25345;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#65288;&#33258;&#36866;&#24212;AI&#65289;&#26469;&#23454;&#29616;&#65292;&#20854;&#20013;&#27169;&#22411;&#23558;&#19981;&#26029;&#22320;&#26681;&#25454;&#26032;&#30340;&#25968;&#25454;&#21644;&#26465;&#20214;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#33258;&#25105;&#32500;&#25345;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#23545;&#39057;&#35889;&#30340;&#22810;&#37325;&#25509;&#20837;&#36827;&#34892;&#31649;&#29702;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#65292;&#20294;&#26159;&#36866;&#24212;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#20195;&#29702;&#26426;&#21046;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20934;&#30830;&#35299;&#20915;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22810;&#36890;&#36947;&#29615;&#22659;&#20013;&#30340;&#22810;&#37325;&#25509;&#20837;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Metaverse is a new paradigm that aims to create a virtual environment consisting of numerous worlds, each of which will offer a different set of services. To deal with such a dynamic and complex scenario, considering the stringent quality of service requirements aimed at the 6th generation of communication systems (6G), one potential approach is to adopt self-sustaining strategies, which can be realized by employing Adaptive Artificial Intelligence (Adaptive AI) where models are continually re-trained with new data and conditions. One aspect of self-sustainability is the management of multiple access to the frequency spectrum. Although several innovative methods have been proposed to address this challenge, mostly using Deep Reinforcement Learning (DRL), the problem of adapting agents to a non-stationary environment has not yet been precisely addressed. This paper fills in the gap in the current literature by investigating the problem of multiple access in multi-channel environment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#20154;&#31867;&#28436;&#31034;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36712;&#36857;&#65292;&#25104;&#21151;&#23436;&#25104;&#20102;&#19977;&#20010;&#22359;&#25805;&#20316;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26242;&#23384;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#20316;&#22359;&#20195;&#29702;&#30340;&#34892;&#21160;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.10175</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#28436;&#31034;&#34892;&#20026;&#20811;&#38534;&#19982;&#21160;&#20316;&#20998;&#22359;&#36716;&#25442;&#30340;&#19968;&#20154;&#21095;&#26412;
&lt;/p&gt;
&lt;p&gt;
One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers. (arXiv:2309.10175v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#20154;&#31867;&#28436;&#31034;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36712;&#36857;&#65292;&#25104;&#21151;&#23436;&#25104;&#20102;&#19977;&#20010;&#22359;&#25805;&#20316;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26242;&#23384;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#20316;&#22359;&#20195;&#29702;&#30340;&#34892;&#21160;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#31034;&#33539;&#20013;&#23398;&#20064;&#65288;&#34892;&#20026;&#20811;&#38534;&#65289;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#34892;&#20026;&#20811;&#38534;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#31034;&#33539;&#26469;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#22810;&#26679;&#21270;&#21021;&#22987;&#26465;&#20214;&#30340;&#19968;&#33324;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21482;&#38656;&#30475;&#21040;&#19968;&#20004;&#20010;&#31034;&#33539;&#23601;&#33021;&#23398;&#20250;&#23436;&#25104;&#20219;&#21153;&#65292;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#27169;&#25311;&#36825;&#31181;&#33021;&#21147;&#65292;&#20165;&#20973;&#19968;&#27425;&#20154;&#31867;&#28436;&#31034;&#23601;&#20351;&#29992;&#34892;&#20026;&#20811;&#38534;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#26469;&#22686;&#24378;&#21333;&#20010;&#28436;&#31034;&#65292;&#29983;&#25104;&#19968;&#31995;&#21015;&#36866;&#29992;&#20110;&#21508;&#31181;&#21021;&#22987;&#26465;&#20214;&#30340;&#36712;&#36857;&#65292;&#23454;&#29616;&#20102;&#36825;&#20010;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#20123;&#31034;&#33539;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#25104;&#21151;&#23436;&#25104;&#19977;&#20010;&#22359;&#25805;&#20316;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26242;&#23384;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#20316;&#22359;&#20195;&#29702;&#30340;&#34892;&#21160;&#39044;&#27979;&#20013;&#32508;&#21512;&#26631;&#20934;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human demonstrations (behavior cloning) is a cornerstone of robot learning. However, most behavior cloning algorithms require a large number of demonstrations to learn a task, especially for general tasks that have a large variety of initial conditions. Humans, however, can learn to complete tasks, even complex ones, after only seeing one or two demonstrations. Our work seeks to emulate this ability, using behavior cloning to learn a task given only a single human demonstration. We achieve this goal by using linear transforms to augment the single demonstration, generating a set of trajectories for a wide range of initial conditions. With these demonstrations, we are able to train a behavior cloning agent to successfully complete three block manipulation tasks. Additionally, we developed a novel addition to the temporal ensembling method used by action chunking agents during inference. By incorporating the standard deviation of the action predictions into the ensembling m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#65292;&#21033;&#29992;&#24322;&#24120;&#30340;&#26102;&#22495;&#28436;&#21270;&#21644;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#26368;&#22823;&#21270;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10157</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#22312;&#32447;&#25968;&#25454;&#36136;&#37327;&#30417;&#27979;&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based Anomaly Detection System for Online Data Quality Monitoring of the CMS Electromagnetic Calorimeter. (arXiv:2309.10157v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#65292;&#21033;&#29992;&#24322;&#24120;&#30340;&#26102;&#22495;&#28436;&#21270;&#21644;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#26368;&#22823;&#21270;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CMS&#25506;&#27979;&#22120;&#26159;&#22312;LHC&#19978;&#25506;&#27979;&#39640;&#33021;&#30896;&#25758;&#20135;&#29983;&#30340;&#36890;&#29992;&#35013;&#32622;&#12290;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#22312;&#32447;&#25968;&#25454;&#36136;&#37327;&#30417;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25805;&#20316;&#24037;&#20855;&#65292;&#33021;&#22815;&#35753;&#25506;&#27979;&#22120;&#19987;&#23478;&#24555;&#36895;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#35786;&#26029;&#21487;&#33021;&#24433;&#21709;&#29289;&#29702;&#25968;&#25454;&#36136;&#37327;&#30340;&#21508;&#31181;&#25506;&#27979;&#22120;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#33258;&#32534;&#30721;&#22120;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#26816;&#27979;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#12290;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#30340;&#26102;&#22495;&#28436;&#21270;&#21644;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#26368;&#22823;&#21270;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#36825;&#20010;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#31995;&#32479;&#33021;&#22815;&#39640;&#25928;&#22320;&#26816;&#27979;&#24322;&#24120;&#65292;&#21516;&#26102;&#20445;&#25345;&#38750;&#24120;&#20302;&#30340;&#35823;&#25253;&#29575;&#12290;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#36890;&#36807;&#22312;2018&#24180;&#21644;2022&#24180;LHC&#30896;&#25758;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#24322;&#24120;&#36827;&#34892;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#39318;&#27425;&#25253;&#36947;&#20102;&#37096;&#32626;&#35813;&#31995;&#32479;&#21518;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CMS detector is a general-purpose apparatus that detects high-energy collisions produced at the LHC. Online Data Quality Monitoring of the CMS electromagnetic calorimeter is a vital operational tool that allows detector experts to quickly identify, localize, and diagnose a broad range of detector issues that could affect the quality of physics data. A real-time autoencoder-based anomaly detection system using semi-supervised machine learning is presented enabling the detection of anomalies in the CMS electromagnetic calorimeter data. A novel method is introduced which maximizes the anomaly detection performance by exploiting the time-dependent evolution of anomalies as well as spatial variations in the detector response. The autoencoder-based system is able to efficiently detect anomalies, while maintaining a very low false discovery rate. The performance of the system is validated with anomalies found in 2018 and 2022 LHC collision data. Additionally, the first results from deploy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#32959;&#30244;&#20307;&#31215;&#30340;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#26469;&#35299;&#20915;&#32959;&#30244;&#21306;&#22495;&#20307;&#31215;&#21464;&#21270;&#22833;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10153</link><description>&lt;p&gt;
&#20445;&#25345;&#32959;&#30244;&#20307;&#31215;&#30340;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Preserving Tumor Volumes for Unsupervised Medical Image Registration. (arXiv:2309.10153v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#32959;&#30244;&#20307;&#31215;&#30340;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#26469;&#35299;&#20915;&#32959;&#30244;&#21306;&#22495;&#20307;&#31215;&#21464;&#21270;&#22833;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#26159;&#20272;&#35745;&#22270;&#20687;&#38388;&#31354;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#37117;&#20381;&#36182;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#29983;&#25104;&#21464;&#24418;&#22330;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#19981;&#30456;&#20284;&#21306;&#22495;&#30340;&#20307;&#31215;&#21464;&#21270;&#22833;&#34913;&#65292;&#29305;&#21035;&#26159;&#22312;&#32959;&#30244;&#21306;&#22495;&#12290;&#36825;&#20123;&#25913;&#21464;&#20250;&#26174;&#33879;&#25913;&#21464;&#32959;&#30244;&#22823;&#23567;&#21644;&#24213;&#23618;&#35299;&#21078;&#32467;&#26500;&#65292;&#38480;&#21046;&#20102;&#22270;&#20687;&#37197;&#20934;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20197;&#32959;&#30244;&#20026;&#32422;&#26463;&#26465;&#20214;&#30340;&#22270;&#20687;&#37197;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#37197;&#20934;&#26469;&#36890;&#36807;&#20307;&#31215;&#21464;&#21270;&#35782;&#21035;&#28508;&#22312;&#30340;&#32959;&#30244;&#21306;&#22495;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#36719;&#32959;&#30244;&#25513;&#33180;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20445;&#25345;&#20307;&#31215;&#30340;&#37197;&#20934;&#26041;&#27861;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#20307;&#31215;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image registration is a critical task that estimates the spatial correspondence between pairs of images. However, current traditional and deep-learning-based methods rely on similarity measures to generate a deforming field, which often results in disproportionate volume changes in dissimilar regions, especially in tumor regions. These changes can significantly alter the tumor size and underlying anatomy, which limits the practical use of image registration in clinical diagnosis. To address this issue, we have formulated image registration with tumors as a constraint problem that preserves tumor volumes while maximizing image similarity in other normal regions. Our proposed strategy involves a two-stage process. In the first stage, we use similarity-based registration to identify potential tumor regions by their volume change, generating a soft tumor mask accordingly. In the second stage, we propose a volume-preserving registration with a novel adaptive volume-preserving loss t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#20351;&#29992;&#20102;$\ell_0$-&#33539;&#25968;&#32422;&#26463;&#65292;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.10152</link><description>&lt;p&gt;
Primal-Dual $\ell_0$-&#32422;&#26463;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual $\ell_0$-Constrained Sparse Index Tracking. (arXiv:2309.10152v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#20351;&#29992;&#20102;$\ell_0$-&#33539;&#25968;&#32422;&#26463;&#65292;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#34987;&#21160;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#26469;&#36319;&#36394;&#37329;&#34701;&#25351;&#25968;&#12290;&#30456;&#27604;&#20110;&#20840;&#20179;&#25237;&#36164;&#32452;&#21512;&#65292;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#22312;&#38477;&#20302;&#20132;&#26131;&#25104;&#26412;&#21644;&#36991;&#20813;&#19981;&#27969;&#21160;&#36164;&#20135;&#26041;&#38754;&#26356;&#20855;&#20248;&#21183;&#12290;&#20026;&#20102;&#24378;&#21046;&#25237;&#36164;&#32452;&#21512;&#30340;&#31232;&#30095;&#24615;&#65292;&#20256;&#32479;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;$\ell_p$-&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#20844;&#24335;&#65292;&#20316;&#20026;$\ell_0$-&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#26367;&#20195;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#20844;&#24335;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#65292;&#20294;&#22312;&#23454;&#38469;&#25237;&#36164;&#20013;&#21364;&#19981;&#26131;&#20351;&#29992;&#65292;&#22240;&#20026;&#32454;&#33268;&#30340;&#21442;&#25968;&#35843;&#25972;&#26469;&#25351;&#23450;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#26159;&#33392;&#38590;&#19988;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#20351;&#29992;&#20102;$\ell_0$-&#33539;&#25968;&#32422;&#26463;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#20801;&#35768;&#22312;&#25237;&#36164;&#32452;&#21512;&#31232;&#30095;&#24615;&#21644;&#25442;&#25163;&#29575;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse index tracking is one of the prominent passive portfolio management strategies that construct a sparse portfolio to track a financial index. A sparse portfolio is desirable over a full portfolio in terms of transaction cost reduction and avoiding illiquid assets. To enforce the sparsity of the portfolio, conventional studies have proposed formulations based on $\ell_p$-norm regularizations as a continuous surrogate of the $\ell_0$-norm regularization. Although such formulations can be used to construct sparse portfolios, they are not easy to use in actual investments because parameter tuning to specify the exact upper bound on the number of assets in the portfolio is delicate and time-consuming. In this paper, we propose a new problem formulation of sparse index tracking using an $\ell_0$-norm constraint that enables easy control of the upper bound on the number of assets in the portfolio. In addition, our formulation allows the choice between portfolio sparsity and turnover spa
&lt;/p&gt;</description></item><item><title>Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2309.10150</link><description>&lt;p&gt;
Q-Transformer&#65306;&#36890;&#36807;&#33258;&#22238;&#24402;Q-&#20989;&#25968;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. (arXiv:2309.10150v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10150
&lt;/p&gt;
&lt;p&gt;
Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#21644;&#33258;&#20027;&#37319;&#38598;&#25968;&#25454;&#30340;&#22823;&#22411;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;Transformer&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;Q&#20989;&#25968;&#34920;&#31034;&#65292;&#36890;&#36807;&#31163;&#32447;&#26102;&#24046;&#22791;&#20221;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#31216;&#20026;Q-Transformer&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#30340;Q&#20540;&#34920;&#31034;&#20026;&#21333;&#29420;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#21487;&#20197;&#24212;&#29992;&#39640;&#23481;&#37327;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#36827;&#34892;Q&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#35774;&#35745;&#20915;&#31574;&#65292;&#20351;&#20854;&#22312;&#31163;&#32447;RL&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;Q-Transformer&#22312;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#22871;&#20214;&#19978;&#20248;&#20110;&#20197;&#24448;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#21644;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#12290;&#35813;&#39033;&#30446;&#30340;&#32593;&#31449;&#21644;&#35270;&#39057;&#21487;&#20197;&#22312;https://q-transformer.github.io&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#26469;&#33719;&#24471;&#23545;&#26410;&#30693;&#21464;&#21270;&#40065;&#26834;&#30340;&#39044;&#27979;&#22120;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10149</link><description>&lt;p&gt;
AI&#20195;&#29702;&#30340;&#35760;&#24518;&#21644;&#27867;&#21270;&#33021;&#21147;&#20998;&#26512;&#65306;&#36830;&#32493;&#23398;&#20064;&#32773;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?. (arXiv:2309.10149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#26469;&#33719;&#24471;&#23545;&#26410;&#30693;&#21464;&#21270;&#40065;&#26834;&#30340;&#39044;&#27979;&#22120;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;AI&#20195;&#29702;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25110;&#26426;&#22120;&#20154;&#65289;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#20174;&#38750;&#31283;&#24577;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#12290;&#23545;&#20110;&#36825;&#31867;&#24212;&#29992;&#30340;&#23454;&#38469;&#37096;&#32626;&#65292;&#20445;&#35777;&#23545;&#26410;&#30693;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#20445;&#30041;&#36807;&#21435;&#30340;&#32463;&#39564;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#32771;&#34385;&#21040;&#36830;&#32493;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#65292;&#20197;&#20272;&#35745;&#29615;&#22659;&#21464;&#21270;&#30340;&#39118;&#38505;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#23545;&#26410;&#30693;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#22120;&#12290;&#23545;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#24615;&#33021;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#38543;&#20869;&#23384;&#22823;&#23567;&#30340;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning (CL), an AI agent (e.g., autonomous vehicles or robotics) learns from non-stationary data streams under dynamic environments. For the practical deployment of such applications, it is important to guarantee robustness to unseen environments while maintaining past experiences. In this paper, a novel CL framework is proposed to achieve robust generalization to dynamic environments while retaining past knowledge. The considered CL agent uses a capacity-limited memory to save previously observed environmental information to mitigate forgetting issues. Then, data points are sampled from the memory to estimate the distribution of risks over environmental change so as to obtain predictors that are robust with unseen changes. The generalization and memorization performance of the proposed framework are theoretically analyzed. This analysis showcases the tradeoff between memorization and generalization with the memory size. Experiments show that the proposed algorithm outpe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#32593;&#32476;&#36319;&#36394;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Tor&#36319;&#36394;&#30340;&#22686;&#24378;&#25216;&#26415;NetAugment&#65292;&#21487;&#20197;&#22686;&#24378;&#32593;&#31449;&#25351;&#32441;&#35782;&#21035;&#22120;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#32593;&#32476;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10147</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#32593;&#32476;&#36319;&#36394;&#23454;&#29616;&#36924;&#30495;&#30340;&#32593;&#31449;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Realistic Website Fingerprinting By Augmenting Network Trace. (arXiv:2309.10147v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10147
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#32593;&#32476;&#36319;&#36394;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Tor&#36319;&#36394;&#30340;&#22686;&#24378;&#25216;&#26415;NetAugment&#65292;&#21487;&#20197;&#22686;&#24378;&#32593;&#31449;&#25351;&#32441;&#35782;&#21035;&#22120;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#32593;&#32476;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#31449;&#25351;&#32441;&#35782;&#21035;&#65288;WF&#65289;&#34987;&#35748;&#20026;&#26159;&#23545;Tor&#29992;&#25143;&#65288;&#20197;&#21450;&#20854;&#20182;&#21311;&#21517;&#31995;&#32479;&#65289;&#21311;&#21517;&#24615;&#30340;&#20027;&#35201;&#23041;&#32961;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;WF&#25216;&#26415;&#22768;&#31216;&#20855;&#26377;&#36739;&#39640;&#30340;&#25915;&#20987;&#20934;&#30830;&#29575;&#65292;&#20363;&#22914;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#20294;&#26368;&#36817;&#30340;&#20960;&#39033;&#30740;&#31350;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#20013;&#25152;&#20570;&#20551;&#35774;&#30340;&#23454;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#19981;&#20999;&#23454;&#38469;&#30340;&#38382;&#39064;&#20027;&#35201;&#26159;&#30001;&#20110;&#25915;&#20987;&#32773;&#26080;&#27861;&#22312;&#20840;&#38754;&#30340;&#32593;&#32476;&#26465;&#20214;&#19979;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#24341;&#36215;&#30340;&#65292;&#20363;&#22914;&#65292;WF&#20998;&#31867;&#22120;&#21487;&#33021;&#21482;&#22312;&#29305;&#23450;&#39640;&#24102;&#23485;&#32593;&#32476;&#38142;&#36335;&#19978;&#25910;&#38598;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#20855;&#26377;&#19981;&#21516;&#32593;&#32476;&#26465;&#20214;&#30340;&#36830;&#25509;&#19978;&#37096;&#32626;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22686;&#21152;&#32593;&#32476;&#36319;&#36394;&#21487;&#20197;&#25552;&#39640;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#32593;&#32476;&#26465;&#20214;&#19979;WF&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NetAugment&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;Tor&#36319;&#36394;&#35268;&#33539;&#30340;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
Website Fingerprinting (WF) is considered a major threat to the anonymity of Tor users (and other anonymity systems). While state-of-the-art WF techniques have claimed high attack accuracies, e.g., by leveraging Deep Neural Networks (DNN), several recent works have questioned the practicality of such WF attacks in the real world due to the assumptions made in the design and evaluation of these attacks. In this work, we argue that such impracticality issues are mainly due to the attacker's inability in collecting training data in comprehensive network conditions, e.g., a WF classifier may be trained only on samples collected on specific high-bandwidth network links but deployed on connections with different network conditions. We show that augmenting network traces can enhance the performance of WF classifiers in unobserved network conditions. Specifically, we introduce NetAugment, an augmentation technique tailored to the specifications of Tor traces. We instantiate NetAugment through 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20960;&#20309;&#65292;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23884;&#22871;&#25216;&#26415;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.10140</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20960;&#20309;&#65292;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23884;&#22871;&#25216;&#26415;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#23398;&#20064;&#31995;&#32479;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#20960;&#20309;&#65292;&#23427;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#32467;&#26500;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#29305;&#24449;&#20960;&#20309;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#35299;&#20915;&#30001;&#23398;&#20064;&#35774;&#32622;&#25351;&#23450;&#30340;&#20381;&#36182;&#32452;&#20214;&#30340;&#26368;&#20339;&#29305;&#24449;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#25216;&#26415;&#26469;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#26368;&#20339;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#22120;&#12290;&#20026;&#20102;&#23637;&#31034;&#23884;&#22871;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#65292;&#21253;&#25324;&#26465;&#20214;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#20339;&#29305;&#24449;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#32463;&#20856;&#26041;&#27861;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20302;&#31209;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ELR-GNN&#65289;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20302;&#31209;&#21644;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26356;&#39640;&#30340;&#25928;&#29575;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2309.10136</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#20302;&#31209;&#22270;&#31070;&#32463;&#32593;&#32476;&#25269;&#24481;&#32467;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Efficient Low-Rank GNN Defense Against Structural Attacks. (arXiv:2309.10136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20302;&#31209;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ELR-GNN&#65289;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20302;&#31209;&#21644;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26356;&#39640;&#30340;&#25928;&#29575;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#25968;&#25454;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;GNNs&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#20063;&#20250;&#20005;&#37325;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23545;&#22797;&#26434;&#25915;&#20987;&#26080;&#25928;&#65292;&#35201;&#20040;&#38656;&#35201;&#20248;&#21270;&#31264;&#23494;&#37051;&#25509;&#30697;&#38453;&#65292;&#36825;&#38750;&#24120;&#32791;&#26102;&#19988;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20302;&#31209;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ELR-GNN&#65289;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#20302;&#31209;&#21644;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#20445;&#35777;&#26377;&#25928;&#30340;&#38450;&#24481;&#19982;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ELR-GNN&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#31895;&#31961;&#20302;&#31209;&#20272;&#35745;&#27169;&#22359;&#21644;&#32454;&#31890;&#24230;&#20272;&#35745;&#27169;&#22359;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#37319;&#29992;&#25130;&#26029;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#26469;&#21021;&#22987;&#21270;&#20302;&#31209;&#37051;&#25509;&#30697;&#38453;&#30340;&#20272;&#35745;&#65292;&#36825;&#20316;&#20026;&#20248;&#21270;&#20302;&#31209;&#37051;&#25509;&#30697;&#38453;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been shown to possess strong representation abilities over graph data. However, GNNs are vulnerable to adversarial attacks, and even minor perturbations to the graph structure can significantly degrade their performance. Existing methods either are ineffective against sophisticated attacks or require the optimization of dense adjacency matrices, which is time-consuming and prone to local minima. To remedy this problem, we propose an Efficient Low-Rank Graph Neural Network (ELR-GNN) defense method, which aims to learn low-rank and sparse graph structures for defending against adversarial attacks, ensuring effective defense with greater efficiency. Specifically, ELR-GNN consists of two modules: a Coarse Low-Rank Estimation Module and a Fine-Grained Estimation Module. The first module adopts the truncated Singular Value Decomposition (SVD) to initialize the low-rank adjacency matrix estimation, which serves as a starting point for optimizing the low-rank 
&lt;/p&gt;</description></item><item><title>GDM&#26159;&#19968;&#31181;&#21452;&#37325;&#30340;mixup&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#23454;&#20363;&#30340;&#21151;&#33021;&#21644;&#32467;&#26500;&#20449;&#24687;&#29983;&#25104;&#26032;&#30340;&#26631;&#35760;&#22270;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.10134</link><description>&lt;p&gt;
GDM: &#21452;&#37325;Mixup&#29992;&#20110;&#26377;&#38480;&#26631;&#27880;&#30340;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
GDM: Dual Mixup for Graph Classification with Limited Supervision. (arXiv:2309.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10134
&lt;/p&gt;
&lt;p&gt;
GDM&#26159;&#19968;&#31181;&#21452;&#37325;&#30340;mixup&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#23454;&#20363;&#30340;&#21151;&#33021;&#21644;&#32467;&#26500;&#20449;&#24687;&#29983;&#25104;&#26032;&#30340;&#26631;&#35760;&#22270;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#23545;&#20110;&#22270;&#20998;&#31867;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#22270;&#26679;&#26412;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#26631;&#35760;&#22270;&#26679;&#26412;&#25968;&#37327;&#30340;&#20943;&#23569;&#65292;GNNs&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#65292;&#24320;&#21457;&#33021;&#22815;&#29983;&#25104;&#26032;&#30340;&#22270;&#23454;&#20363;&#20197;&#22686;&#21152;&#21487;&#29992;&#26631;&#35760;&#22270;&#26679;&#26412;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;mixup&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;Graph Dual Mixup (GDM)&#65292;&#23427;&#21033;&#29992;&#22270;&#23454;&#20363;&#30340;&#21151;&#33021;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#29983;&#25104;&#26032;&#30340;&#26631;&#35760;&#22270;&#26679;&#26412;&#12290;GDM&#20351;&#29992;&#22270;&#32467;&#26500;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#22270;&#26679;&#26412;&#30340;&#32467;&#26500;&#23884;&#20837;&#65292;&#28982;&#21518;&#22312;&#23398;&#21040;&#30340;&#32467;&#26500;&#23884;&#20837;&#31354;&#38388;&#20013;&#24212;&#29992;mixup&#21040;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20174;mixup&#32467;&#26500;&#23884;&#20837;&#20013;&#29983;&#25104;&#26032;&#30340;&#22270;&#32467;&#26500;&#12290;&#33267;&#20110;&#21151;&#33021;&#20449;&#24687;&#65292;GDM&#24212;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) require a large number of labeled graph samples to obtain good performance on the graph classification task. The performance of GNNs degrades significantly as the number of labeled graph samples decreases. To reduce the annotation cost, it is therefore important to develop graph augmentation methods that can generate new graph instances to increase the size and diversity of the limited set of available labeled graph samples. In this work, we propose a novel mixup-based graph augmentation method, Graph Dual Mixup (GDM), that leverages both functional and structural information of the graph instances to generate new labeled graph samples. GDM employs a graph structural auto-encoder to learn structural embeddings of the graph samples, and then applies mixup to the structural information of the graphs in the learned structural embedding space and generates new graph structures from the mixup structural embeddings. As for the functional information, GDM applies 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22270;&#24418;&#25351;&#23548;&#35843;&#20248;&#30340;&#26367;&#20195;fine-tuning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#29305;&#24449;&#33410;&#28857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20196;&#29260;&#65292;&#26469;&#22686;&#24378;&#22270;&#24418;&#21464;&#25442;&#22120;&#27169;&#22411;&#22312;&#19979;&#28216;&#22270;&#24418;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#33258;&#30001;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#27169;&#22411;&#21103;&#26412;&#30340;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.10131</link><description>&lt;p&gt;
&#22270;&#24418;&#21464;&#25442;&#22120;&#30340;&#28145;&#24230;&#25351;&#23548;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Deep Prompt Tuning for Graph Transformers. (arXiv:2309.10131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22270;&#24418;&#25351;&#23548;&#35843;&#20248;&#30340;&#26367;&#20195;fine-tuning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#29305;&#24449;&#33410;&#28857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20196;&#29260;&#65292;&#26469;&#22686;&#24378;&#22270;&#24418;&#21464;&#25442;&#22120;&#27169;&#22411;&#22312;&#19979;&#28216;&#22270;&#24418;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#33258;&#30001;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#27169;&#22411;&#21103;&#26412;&#30340;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#21464;&#25442;&#22120;&#36890;&#36807;&#35299;&#20915;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22312;&#21508;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#26550;&#26500;&#20013;&#30340;&#22823;&#35268;&#27169;&#23618;&#21472;&#32473;&#23558;&#20854;&#24212;&#29992;&#20110;&#22522;&#20110;&#22270;&#24418;&#30340;&#39044;&#27979;&#20219;&#21153;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#24120;&#35265;&#26041;&#27861;fine-tuning&#32791;&#36153;&#36164;&#28304;&#19988;&#38656;&#35201;&#23384;&#20648;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#30340;&#21103;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#28145;&#24230;&#22270;&#24418;&#25351;&#23548;&#35843;&#20248;&#65292;&#20316;&#20026;&#22312;&#19979;&#28216;&#22270;&#24418;&#39044;&#27979;&#20219;&#21153;&#20013;&#21033;&#29992;&#22823;&#22411;&#22270;&#24418;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#21487;&#35757;&#32451;&#30340;&#29305;&#24449;&#33410;&#28857;&#21040;&#22270;&#24418;&#20013;&#65292;&#24182;&#22312;&#22270;&#24418;&#21464;&#25442;&#22120;&#19978;&#39044;&#20808;&#28155;&#21152;&#20219;&#21153;&#29305;&#23450;&#30340;&#20196;&#29260;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#21442;&#25968;&#24182;&#20165;&#26356;&#26032;&#28155;&#21152;&#30340;&#20196;&#29260;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#33258;&#30001;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#28040;&#38500;&#20102;&#22810;&#20010;&#27169;&#22411;&#21103;&#26412;&#30340;&#38656;&#27714;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers have gained popularity in various graph-based tasks by addressing challenges faced by traditional Graph Neural Networks. However, the quadratic complexity of self-attention operations and the extensive layering in graph transformer architectures present challenges when applying them to graph based prediction tasks. Fine-tuning, a common approach, is resource-intensive and requires storing multiple copies of large models. We propose a novel approach called deep graph prompt tuning as an alternative to fine-tuning for leveraging large graph transformer models in downstream graph based prediction tasks. Our method introduces trainable feature nodes to the graph and pre-pends task-specific tokens to the graph transformer, enhancing the model's expressive power. By freezing the pre-trained parameters and only updating the added tokens, our approach reduces the number of free parameters and eliminates the need for multiple model copies, making it suitable for small dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#32500;&#21452;&#26354;&#22411;&#23432;&#24658;&#23450;&#24459;&#38382;&#39064;&#30340;&#28145;&#24230;&#24179;&#28369;WENO&#26684;&#24335;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35843;&#25972;&#24179;&#28369;&#24615;&#25351;&#26631;&#65292;&#22312;&#25968;&#20540;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#21183;&#65292;&#23588;&#20854;&#26159;&#22312;&#38497;&#23789;&#28608;&#27874;&#38468;&#36817;&#12290;</title><link>http://arxiv.org/abs/2309.10117</link><description>&lt;p&gt;
&#20108;&#32500;&#21452;&#26354;&#22411;&#23432;&#24658;&#23450;&#24459;&#38382;&#39064;&#30340;&#28145;&#24230;&#24179;&#28369;WENO&#26684;&#24335;&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24179;&#28369;&#24615;&#25351;&#26631;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep smoothness WENO scheme for two-dimensional hyperbolic conservation laws: A deep learning approach for learning smoothness indicators. (arXiv:2309.10117v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10117
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#32500;&#21452;&#26354;&#22411;&#23432;&#24658;&#23450;&#24459;&#38382;&#39064;&#30340;&#28145;&#24230;&#24179;&#28369;WENO&#26684;&#24335;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35843;&#25972;&#24179;&#28369;&#24615;&#25351;&#26631;&#65292;&#22312;&#25968;&#20540;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#21183;&#65292;&#23588;&#20854;&#26159;&#22312;&#38497;&#23789;&#28608;&#27874;&#38468;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20116;&#38454;&#21152;&#26435;&#23454;&#36136;&#38750;&#25391;&#33633;&#65288;WENO&#65289;&#28608;&#27874;&#25429;&#33719;&#26684;&#24335;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#25913;&#36827;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35843;&#25972;WENO&#26684;&#24335;&#20013;&#30340;&#24179;&#28369;&#24615;&#25351;&#26631;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;WENO&#31639;&#27861;&#12290;&#36825;&#31181;&#25913;&#36827;&#25552;&#39640;&#20102;&#25968;&#20540;&#32467;&#26524;&#30340;&#31934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38497;&#23789;&#28608;&#27874;&#38468;&#36817;&#12290;&#19982;&#20197;&#21069;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#26469;&#20445;&#25345;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20108;&#32500;&#27431;&#25289;&#27668;&#20307;&#21160;&#21147;&#23398;&#26041;&#31243;&#30340;&#20960;&#20010;&#25991;&#29486;&#20363;&#23376;&#23637;&#31034;&#20102;&#25105;&#20204;&#26032;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#36890;&#36807;&#23545;&#28041;&#21450;&#21508;&#31181;&#28608;&#27874;&#21644;&#23637;&#24320;&#27874;&#30340;&#36825;&#20123;&#27979;&#35797;&#38382;&#39064;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#26032;&#25216;&#26415;&#34920;&#29616;&#20986;&#22312;&#25968;&#20540;&#35299;&#22312;&#28608;&#27874;&#38468;&#36817;&#21576;&#29616;&#20986;&#36807;&#24230;&#25193;&#25955;&#25110;&#36229;&#35843;&#26102;&#30340;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#20116;&#38454;WENO&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce an improved version of the fifth-order weighted essentially non-oscillatory (WENO) shock-capturing scheme by incorporating deep learning techniques. The established WENO algorithm is improved by training a compact neural network to adjust the smoothness indicators within the WENO scheme. This modification enhances the accuracy of the numerical results, particularly near abrupt shocks. Unlike previous deep learning-based methods, no additional post-processing steps are necessary for maintaining consistency. We demonstrate the superiority of our new approach using several examples from the literature for the two-dimensional Euler equations of gas dynamics. Through intensive study of these test problems, which involve various shocks and rarefaction waves, the new technique is shown to outperform traditional fifth-order WENO schemes, especially in cases where the numerical solutions exhibit excessive diffusion or overshoot around shocks.
&lt;/p&gt;</description></item><item><title>AR-TTA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#23558;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10109</link><description>&lt;p&gt;
AR-TTA: &#19968;&#31181;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation. (arXiv:2309.10109v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10109
&lt;/p&gt;
&lt;p&gt;
AR-TTA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#23558;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#23427;&#20801;&#35768;&#28304;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#21482;&#26159;&#23454;&#38469;&#22330;&#26223;&#31616;&#21270;&#29256;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26368;&#36817;&#25512;&#20986;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;CLAD-C&#21644;SHIFT&#26469;&#39564;&#35777;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#21069;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#26377;&#25928;&#22788;&#29702;&#19981;&#21516;&#31243;&#24230;&#30340;&#22495;&#20559;&#31227;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#20302;&#20110;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#38382;&#39064;&#30340;&#26681;&#28304;&#22312;&#20110;&#26080;&#27861;&#20445;&#30041;&#28304;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#12289;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#19968;&#20010;&#23567;&#30340;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#21040;&#25104;&#29087;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#20013;&#65292;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#21516;&#26102;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#27169;&#22411;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#27491;&#38754;&#24433;&#21709;&#65292;&#20294;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#19982;&#24494;&#35843;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20559;&#21521;&#20110;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#23581;&#35797;&#24674;&#22797;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10105</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#25512;&#29702;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Understanding Catastrophic Forgetting in Language Models via Implicit Inference. (arXiv:2309.10105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#27169;&#22411;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#27491;&#38754;&#24433;&#21709;&#65292;&#20294;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#19982;&#24494;&#35843;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20559;&#21521;&#20110;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#23581;&#35797;&#24674;&#22797;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#65288;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#25110;&#20174;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#31561;&#26041;&#27861;&#65289;&#26159;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#40065;&#26834;&#22320;&#25191;&#34892;&#25152;&#38656;&#20219;&#21153;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#24494;&#35843;&#30340;&#24433;&#21709;&#30340;&#31995;&#32479;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#29421;&#31364;&#30340;&#24494;&#35843;&#20998;&#24067;&#20043;&#22806;&#30340;&#20219;&#21153;&#19978;&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20869;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#30340;&#21516;&#26102;&#65292;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36864;&#21270;&#22312;&#19982;&#24494;&#35843;&#20998;&#24067;&#8220;&#26368;&#25509;&#36817;&#8221;&#30340;&#20219;&#21153;&#20013;&#23588;&#20026;&#26174;&#33879;&#12290;&#25105;&#20204;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20986;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20027;&#35201;&#20559;&#21521;&#20110;&#24494;&#35843;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#65292;&#20197;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#20197;&#26597;&#30475;&#26159;&#21542;&#21487;&#20197;&#24674;&#22797;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#12290;&#20849;&#36717;&#25552;&#31034;&#20250;&#20154;&#20026;&#22320;&#20351;&#20219;&#21153;&#30475;&#36215;&#26469;&#19982;&#24494;&#35843;&#20998;&#24067;&#36739;&#36828;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest. However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks. This degradation is especially pronounced for tasks "closest" to the fine-tuning distribution. We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution. To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities. Conjugate prompting artificially makes the task look farther from the fine-tun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#26080;&#26631;&#31614;&#30340;&#20107;&#20214;&#26679;&#26412;&#26469;&#35780;&#20272;&#25552;&#21319;&#29616;&#26377;&#20107;&#20214;&#35782;&#21035;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10095</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#20107;&#20214;&#35782;&#21035;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Semi-Supervised Approach for Power System Event Identification. (arXiv:2309.10095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10095
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#26080;&#26631;&#31614;&#30340;&#20107;&#20214;&#26679;&#26412;&#26469;&#35780;&#20272;&#25552;&#21319;&#29616;&#26377;&#20107;&#20214;&#35782;&#21035;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#35782;&#21035;&#34987;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#23545;&#20110;&#25552;&#39640;&#30005;&#21147;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#30456;&#37327;&#27979;&#37327;&#35013;&#32622;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#25968;&#25454;&#31185;&#23398;&#30340;&#36827;&#27493;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#25216;&#26415;&#25506;&#32034;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20107;&#20214;&#35782;&#21035;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24037;&#20316;&#37327;&#22823;&#21644;&#23454;&#26102;&#20107;&#20214;&#31867;&#22411;&#65288;&#31867;&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#33719;&#21462;&#31934;&#30830;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#26679;&#26412;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65288;&#21516;&#26102;&#20351;&#29992;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#26679;&#26412;&#65289;&#26159;&#24456;&#33258;&#28982;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event identification is increasingly recognized as crucial for enhancing the reliability, security, and stability of the electric power system. With the growing deployment of Phasor Measurement Units (PMUs) and advancements in data science, there are promising opportunities to explore data-driven event identification via machine learning classification techniques. However, obtaining accurately-labeled eventful PMU data samples remains challenging due to its labor-intensive nature and uncertainty about the event type (class) in real-time. Thus, it is natural to use semi-supervised learning techniques, which make use of both labeled and unlabeled samples. %We propose a novel semi-supervised framework to assess the effectiveness of incorporating unlabeled eventful samples to enhance existing event identification methodologies. We evaluate three categories of classical semi-supervised approaches: (i) self-training, (ii) transductive support vector machines (TSVM), and (iii) graph-based lab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#27169;&#22411;UCoFiA&#65292;&#29992;&#20110;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#30456;&#20284;&#24615;&#32858;&#21512;&#27169;&#22359;&#26377;&#25928;&#32771;&#34385;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#26368;&#32456;&#35299;&#20915;&#20102;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#31934;&#30830;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10091</link><description>&lt;p&gt;
&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#30340;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Coarse-to-Fine Alignment for Video-Text Retrieval. (arXiv:2309.10091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10091
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#27169;&#22411;UCoFiA&#65292;&#29992;&#20110;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#30456;&#20284;&#24615;&#32858;&#21512;&#27169;&#22359;&#26377;&#25928;&#32771;&#34385;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#26368;&#32456;&#35299;&#20915;&#20102;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#31934;&#30830;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#36890;&#24120;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#20043;&#38388;&#30340;&#31895;&#31890;&#24230;&#25110;&#32454;&#31890;&#24230;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25991;&#26412;&#26597;&#35810;&#26816;&#32034;&#27491;&#30830;&#30340;&#35270;&#39057;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33021;&#22815;&#25512;&#29702;&#20986;&#39640;&#32423;&#65288;&#22330;&#26223;&#65289;&#21644;&#20302;&#32423;&#65288;&#23545;&#35937;&#65289;&#35270;&#35273;&#32447;&#32034;&#21450;&#20854;&#19982;&#25991;&#26412;&#26597;&#35810;&#30340;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UCoFiA&#30340;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20449;&#24687;&#12290;&#20026;&#20943;&#36731;&#26080;&#20851;&#35270;&#35273;&#32447;&#32034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#20132;&#20114;&#24335;&#30456;&#20284;&#24615;&#32858;&#21512;&#27169;&#22359;&#65288;ISA&#65289;&#26469;&#32771;&#34385;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#32858;&#21512;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20197;&#33719;&#24471;&#27599;&#20010;&#31890;&#24230;&#30340;&#30456;&#20284;&#24230;&#24471;&#20998;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;Sinkhorn-Knopp&#31639;&#27861;&#23545;&#27599;&#20010;&#32423;&#21035;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#20197;&#20943;&#36731;&#19981;&#21516;&#32423;&#21035;&#19978;&#30340;&#36807;&#24230;&#25110;&#19981;&#36275;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The canonical approach to video-text retrieval leverages a coarse-grained or fine-grained alignment between visual and textual information. However, retrieving the correct video according to the text query is often challenging as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they relate to the text query. To this end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA. Specifically, our model captures the cross-modal similarity information at different granularity levels. To alleviate the effect of irrelevant visual clues, we also apply an Interactive Similarity Aggregation module (ISA) to consider the importance of different visual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different l
&lt;/p&gt;</description></item><item><title>HTEC&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#22635;&#20805;&#20004;&#20010;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#20462;&#27491;&#25805;&#20316;&#21015;&#34920;&#65292;&#24182;&#38024;&#23545;&#21024;&#38500;&#38169;&#35823;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.10089</link><description>&lt;p&gt;
HTEC: &#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
HTEC: Human Transcription Error Correction. (arXiv:2309.10089v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10089
&lt;/p&gt;
&lt;p&gt;
HTEC&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#22635;&#20805;&#20004;&#20010;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#20462;&#27491;&#25805;&#20316;&#21015;&#34920;&#65292;&#24182;&#38024;&#23545;&#21024;&#38500;&#38169;&#35823;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#36716;&#24405;&#23545;&#20110;&#35757;&#32451;&#21644;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27599;&#22686;&#21152;1%&#30340;&#36716;&#24405;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#20351;&#29992;&#36825;&#20123;&#36716;&#24405;&#26469;&#35757;&#32451;ASR&#27169;&#22411;&#23558;&#22686;&#21152;&#32422;2%&#30340;ASR WER&#12290;&#21363;&#20351;&#26159;&#32463;&#36807;&#39640;&#24230;&#22521;&#35757;&#30340;&#27880;&#37322;&#21592;&#20063;&#38590;&#20813;&#20986;&#29616;&#36716;&#24405;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#30340;&#20462;&#27491;&#26041;&#27861;&#12290;&#20854;&#20182;&#38382;&#39064;&#30340;&#38169;&#35823;&#20462;&#27491;&#26041;&#27861;&#65292;&#22914;ASR&#38169;&#35823;&#20462;&#27491;&#21644;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#34920;&#29616;&#19981;&#22815;&#22909;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HTEC&#29992;&#20110;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#12290;HTEC&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;Trans-Checker&#65292;&#19968;&#31181;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#23631;&#34109;&#38169;&#35823;&#21333;&#35789;&#65292;&#21644;Trans-Filler&#65292;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22635;&#20805;&#23631;&#34109;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#20462;&#27491;&#25805;&#20316;&#21015;&#34920;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#22788;&#29702;&#21024;&#38500;&#38169;&#35823;&#30340;&#26032;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality human transcription is essential for training and improving Automatic Speech Recognition (ASR) models. Recent study~\cite{libricrowd} has found that every 1% worse transcription Word Error Rate (WER) increases approximately 2% ASR WER by using the transcriptions to train ASR models. Transcription errors are inevitable for even highly-trained annotators. However, few studies have explored human transcription correction. Error correction methods for other problems, such as ASR error correction and grammatical error correction, do not perform sufficiently for this problem. Therefore, we propose HTEC for Human Transcription Error Correction. HTEC consists of two stages: Trans-Checker, an error detection model that predicts and masks erroneous words, and Trans-Filler, a sequence-to-sequence generative model that fills masked positions. We propose a holistic list of correction operations, including four novel operations handling deletion errors. We further propose a variant of e
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#19979;&#65292;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#19968;&#33324;&#19981;&#20855;&#26377;&#19981;&#21464;&#21644;&#31283;&#20581;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#38480;&#21046;&#20998;&#24067;&#20559;&#31227;&#31867;&#21035;&#21644;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#27169;&#22411;&#20013;&#23454;&#29616;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10083</link><description>&lt;p&gt;
&#19981;&#21464;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Invariant Probabilistic Prediction. (arXiv:2309.10083v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10083
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#19979;&#65292;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#19968;&#33324;&#19981;&#20855;&#26377;&#19981;&#21464;&#21644;&#31283;&#20581;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#38480;&#21046;&#20998;&#24067;&#20559;&#31227;&#31867;&#21035;&#21644;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#27169;&#22411;&#20013;&#23454;&#29616;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#21464;&#21270;&#19979;&#34920;&#29616;&#31283;&#20581;&#30340;&#32479;&#35745;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#34429;&#28982;&#22823;&#37096;&#20998;&#30456;&#20851;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#30340;&#28857;&#39044;&#27979;&#19978;&#65292;&#20294;&#26412;&#25991;&#23558;&#28966;&#28857;&#36716;&#21521;&#20102;&#27010;&#29575;&#39044;&#27979;&#65292;&#26088;&#22312;&#20840;&#38754;&#37327;&#21270;&#32473;&#23450;&#21327;&#21464;&#37327;&#30340;&#32467;&#26524;&#21464;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27010;&#29575;&#39044;&#27979;&#22312;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#19979;&#30340;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#19968;&#33324;&#19981;&#20855;&#26377;&#19981;&#21464;&#21644;&#31283;&#20581;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#19982;&#28857;&#39044;&#27979;&#30340;&#24773;&#20917;&#30456;&#21453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#24182;&#38480;&#21046;&#20998;&#24067;&#20559;&#31227;&#31867;&#21035;&#65292;&#20197;&#23454;&#29616;&#21407;&#22411;&#39640;&#26031;&#24322;&#26041;&#24046;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#19981;&#21464;&#24615;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20135;&#29983;&#19981;&#21464;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#24191;&#20041;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;GAME&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#24182;&#37319;&#29992;&#26032;&#39062;&#30340;&#20851;&#27880;&#26426;&#21046;&#65292;&#33021;&#22815;&#23545;&#38738;&#23569;&#24180;&#30340;&#24515;&#29702;&#29366;&#20917;&#36827;&#34892;&#39640;&#20934;&#30830;&#24230;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#27599;&#31181;&#27169;&#24577;&#37117;&#23545;&#24515;&#29702;&#38556;&#30861;&#30340;&#31579;&#26597;&#21644;&#20849;&#30149;&#29366;&#20917;&#26377;&#21160;&#24577;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.10077</link><description>&lt;p&gt;
GAME: &#19968;&#31181;&#29992;&#20110;&#26089;&#26399;&#31579;&#26597;&#38738;&#23569;&#24180;&#24515;&#29702;&#38556;&#30861;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#30340;&#24191;&#20041;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GAME: Generalized deep learning model towards multimodal data integration for early screening of adolescent mental disorders. (arXiv:2309.10077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#24191;&#20041;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;GAME&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#24182;&#37319;&#29992;&#26032;&#39062;&#30340;&#20851;&#27880;&#26426;&#21046;&#65292;&#33021;&#22815;&#23545;&#38738;&#23569;&#24180;&#30340;&#24515;&#29702;&#29366;&#20917;&#36827;&#34892;&#39640;&#20934;&#30830;&#24230;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#27599;&#31181;&#27169;&#24577;&#37117;&#23545;&#24515;&#29702;&#38556;&#30861;&#30340;&#31579;&#26597;&#21644;&#20849;&#30149;&#29366;&#20917;&#26377;&#21160;&#24577;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#21457;&#29616;&#38738;&#23569;&#24180;&#24515;&#29702;&#38556;&#30861;&#26159;&#19968;&#20010;&#20840;&#29699;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#30001;&#20110;&#20854;&#22797;&#26434;&#32780;&#24494;&#22937;&#30340;&#24615;&#36136;&#65292;&#21333;&#19968;&#22240;&#32032;&#24456;&#38590;&#26816;&#27979;&#21040;&#24322;&#24120;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#24191;&#20041;&#30340;&#22810;&#27169;&#24577;&#22270;&#20687;&#19982;&#26426;&#22120;&#20154;&#30456;&#20114;&#20316;&#29992;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#31579;&#26597;&#31995;&#32479;&#29992;&#20110;&#26816;&#27979;&#38738;&#23569;&#24180;&#24515;&#29702;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#20415;&#25658;&#26426;&#22120;&#20154;&#19978;&#37096;&#32626;&#30340;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#65292;&#37319;&#29992;&#36855;&#20320;&#28216;&#25103;&#21644;&#32842;&#22825;&#35760;&#24405;&#65292;&#23545;3,783&#21517;&#20013;&#23398;&#29983;&#36827;&#34892;&#31579;&#26597;&#65292;&#24182;&#26500;&#24314;&#20102;&#21253;&#25324;&#38754;&#37096;&#22270;&#20687;&#12289;&#29983;&#29702;&#25351;&#26631;&#12289;&#35821;&#38899;&#35760;&#24405;&#21644;&#25991;&#26412;&#36716;&#24405;&#30340;&#22810;&#27169;&#24577;&#31579;&#26597;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;GAME&#65288;&#20855;&#26377;&#20851;&#27880;&#26426;&#21046;&#21644;&#22810;&#27169;&#24577;EmbraceNet&#30340;&#24191;&#20041;&#27169;&#22411;&#65289;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36328;&#27169;&#24577;&#29305;&#24449;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#65292;&#23545;&#38738;&#23569;&#24180;&#30340;&#24515;&#29702;&#29366;&#20917;&#36827;&#34892;&#39640;&#20934;&#30830;&#24230;&#65288;73.34%-92.77%&#65289;&#21644;F1-Score&#65288;71.32%-91.06%&#65289;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#27599;&#31181;&#27169;&#24577;&#21160;&#24577;&#22320;&#23545;&#24515;&#29702;&#38556;&#30861;&#31579;&#26597;&#21644;&#20849;&#30149;&#29366;&#20917;&#20316;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The timely identification of mental disorders in adolescents is a global public health challenge.Single factor is difficult to detect the abnormality due to its complex and subtle nature. Additionally, the generalized multimodal Computer-Aided Screening (CAS) systems with interactive robots for adolescent mental disorders are not available. Here, we design an android application with mini-games and chat recording deployed in a portable robot to screen 3,783 middle school students and construct the multimodal screening dataset, including facial images, physiological signs, voice recordings, and textual transcripts.We develop a model called GAME (Generalized Model with Attention and Multimodal EmbraceNet) with novel attention mechanism that integrates cross-modal features into the model. GAME evaluates adolescent mental conditions with high accuracy (73.34%-92.77%) and F1-Score (71.32%-91.06%).We find each modality contributes dynamically to the mental disorders screening and comorbiditi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25506;&#35752;&#38750;&#24179;&#31283;&#26680;&#22312;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.10068</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#26680;&#23545;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes. (arXiv:2309.10068v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25506;&#35752;&#38750;&#24179;&#31283;&#26680;&#22312;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#29992;&#20110;&#25968;&#25454;&#30340;&#38543;&#26426;&#20989;&#25968;&#36817;&#20284;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32479;&#35745;&#25216;&#26415;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#30001;&#20110;&#20854;&#20248;&#36234;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#24773;&#20917;&#19979;&#65292;&#20197;&#21450;&#20854;&#22266;&#26377;&#30340;&#25552;&#20379;&#24378;&#20581;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;GP&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#26680;&#24515;&#26041;&#27861;&#30340;&#22797;&#26434;&#23450;&#21046;&#65292;&#36825;&#24448;&#24448;&#22312;&#20351;&#29992;&#26631;&#20934;&#35774;&#32622;&#21644;&#29616;&#25104;&#36719;&#20214;&#24037;&#20855;&#26102;&#20351;&#20174;&#19994;&#32773;&#19981;&#28385;&#24847;&#12290;&#21487;&#20197;&#35828;&#65292;GP&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#26680;&#20989;&#25968;&#65292;&#23427;&#25198;&#28436;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#35282;&#33394;&#12290;Mat\'ern&#31867;&#30340;&#24179;&#31283;&#26680;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#30740;&#31350;&#20013;&#34987;&#20351;&#29992;&#65307;&#20302;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#29616;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24448;&#24448;&#26159;&#20854;&#32467;&#26524;&#12290;&#38750;&#24179;&#31283;&#26680;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#26356;&#21152;&#22797;&#26434;&#30340;&#23646;&#24615;&#65292;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat\'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32437;&#21521;&#24352;&#37327;&#21709;&#24212;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#27719;&#38598;&#31354;&#38388;&#20998;&#24067;&#30340;&#20307;&#32032;&#20449;&#24687;&#26469;&#25512;&#26029;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#24182;&#23545;&#21327;&#21464;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65292;&#21033;&#29992;&#20302;&#31209;&#20998;&#35299;&#20943;&#23569;&#32500;&#24230;&#24182;&#20445;&#25345;&#32500;&#24230;&#30340;&#31354;&#38388;&#37197;&#32622;&#65292;&#36890;&#36807;&#28385;&#36275;&#21518;&#39564;&#20998;&#24067;&#24418;&#29366;&#30340;&#32852;&#21512;&#21487;&#20449;&#21306;&#22495;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.10065</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#32437;&#21521;&#24352;&#37327;&#21709;&#24212;&#22238;&#24402;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Bayesian longitudinal tensor response regression for modeling neuroplasticity. (arXiv:2309.10065v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10065
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32437;&#21521;&#24352;&#37327;&#21709;&#24212;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#27719;&#38598;&#31354;&#38388;&#20998;&#24067;&#30340;&#20307;&#32032;&#20449;&#24687;&#26469;&#25512;&#26029;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#24182;&#23545;&#21327;&#21464;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65292;&#21033;&#29992;&#20302;&#31209;&#20998;&#35299;&#20943;&#23569;&#32500;&#24230;&#24182;&#20445;&#25345;&#32500;&#24230;&#30340;&#31354;&#38388;&#37197;&#32622;&#65292;&#36890;&#36807;&#28385;&#36275;&#21518;&#39564;&#20998;&#24067;&#24418;&#29366;&#30340;&#32852;&#21512;&#21487;&#20449;&#21306;&#22495;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#31070;&#32463;&#24433;&#20687;&#30740;&#31350;&#20013;&#23545;&#20110;&#30001;&#27835;&#30103;&#21644;&#20854;&#20182;&#22240;&#32032;&#24341;&#36215;&#30340;&#20307;&#32032;&#32423;&#31070;&#32463;&#21487;&#22609;&#24615;&#30340;&#35843;&#26597;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20307;&#32032;&#32423;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#24352;&#37327;&#21709;&#24212;&#22238;&#24402;&#30340;&#32437;&#21521;&#24433;&#20687;&#25968;&#25454;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27719;&#38598;&#31354;&#38388;&#20998;&#24067;&#30340;&#20307;&#32032;&#20449;&#24687;&#26469;&#25512;&#26029;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#24182;&#23545;&#21327;&#21464;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#37319;&#26679;&#23454;&#29616;&#65292;&#21033;&#29992;&#20302;&#31209;&#20998;&#35299;&#26469;&#20943;&#23569;&#32500;&#24230;&#24182;&#20445;&#25345;&#20272;&#35745;&#31995;&#25968;&#20013;&#20307;&#32032;&#30340;&#31354;&#38388;&#37197;&#32622;&#12290;&#23427;&#36824;&#36890;&#36807;&#28385;&#36275;&#21518;&#39564;&#20998;&#24067;&#24418;&#29366;&#30340;&#32852;&#21512;&#21487;&#20449;&#21306;&#22495;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25512;&#26029;&#12290;&#38500;&#20102;&#32676;&#20307;&#27700;&#24179;&#30340;&#25512;&#26029;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#25512;&#26029;&#20010;&#20307;&#27700;&#24179;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#20010;&#20307;&#27700;&#24179;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major interest in longitudinal neuroimaging studies involves investigating voxel-level neuroplasticity due to treatment and other factors across visits. However, traditional voxel-wise methods are beset with several pitfalls, which can compromise the accuracy of these approaches. We propose a novel Bayesian tensor response regression approach for longitudinal imaging data, which pools information across spatially-distributed voxels to infer significant changes while adjusting for covariates. The proposed method, which is implemented using Markov chain Monte Carlo (MCMC) sampling, utilizes low-rank decomposition to reduce dimensionality and preserve spatial configurations of voxels when estimating coefficients. It also enables feature selection via joint credible regions which respect the shape of the posterior distributions for more accurate inference. In addition to group level inferences, the method is able to infer individual-level neuroplasticity, allowing for examination of pers
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27169;&#22411;&#31363;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23545;&#31216;&#23398;&#29983;&#26469;&#24341;&#23548;&#29983;&#25104;&#22120;&#29983;&#25104;&#26679;&#26412;&#65292;&#20351;&#24471;&#36825;&#20004;&#20010;&#23398;&#29983;&#23545;&#26679;&#26412;&#30340;&#20998;&#31867;&#24847;&#35265;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#22312;&#29983;&#25104;&#22120;&#20013;&#28608;&#21169;&#25506;&#32034;&#26356;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#23398;&#29983;&#27169;&#22411;&#30340;&#26799;&#24230;&#38388;&#25509;&#20272;&#35745;&#30446;&#26631;&#27169;&#22411;&#30340;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.10058</link><description>&lt;p&gt;
&#26080;&#25968;&#25454;&#27169;&#22411;&#31363;&#21462;&#30340;&#21452;&#23398;&#29983;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dual Student Networks for Data-Free Model Stealing. (arXiv:2309.10058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10058
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27169;&#22411;&#31363;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23545;&#31216;&#23398;&#29983;&#26469;&#24341;&#23548;&#29983;&#25104;&#22120;&#29983;&#25104;&#26679;&#26412;&#65292;&#20351;&#24471;&#36825;&#20004;&#20010;&#23398;&#29983;&#23545;&#26679;&#26412;&#30340;&#20998;&#31867;&#24847;&#35265;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#22312;&#29983;&#25104;&#22120;&#20013;&#28608;&#21169;&#25506;&#32034;&#26356;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#23398;&#29983;&#27169;&#22411;&#30340;&#26799;&#24230;&#38388;&#25509;&#20272;&#35745;&#30446;&#26631;&#27169;&#22411;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26080;&#25968;&#25454;&#27169;&#22411;&#31363;&#21462;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#22120;&#20135;&#29983;&#26679;&#26412;&#26469;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20197;&#21305;&#37197;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#27809;&#26377;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#30446;&#26631;&#27169;&#22411;&#30340;&#26799;&#24230;&#65292;&#24182;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20805;&#20998;&#25506;&#32034;&#36755;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23398;&#29983;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#35757;&#32451;&#20004;&#20010;&#23398;&#29983;&#65292;&#20026;&#29983;&#25104;&#22120;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#65292;&#29983;&#25104;&#20004;&#20010;&#23398;&#29983;&#22312;&#20854;&#19978;&#24847;&#35265;&#19981;&#19968;&#33268;&#30340;&#26679;&#26412;&#12290;&#19968;&#26041;&#38754;&#65292;&#26679;&#26412;&#19978;&#30340;&#24847;&#35265;&#19981;&#19968;&#33268;&#24847;&#21619;&#30528;&#33267;&#23569;&#26377;&#19968;&#20010;&#23398;&#29983;&#23558;&#26679;&#26412;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#19982;&#30446;&#26631;&#27169;&#22411;&#30456;&#27604;&#12290;&#36825;&#31181;&#23545;&#19981;&#19968;&#33268;&#30340;&#28608;&#21169;&#38544;&#21547;&#22320;&#20419;&#20351;&#29983;&#25104;&#22120;&#25506;&#32034;&#36755;&#20837;&#31354;&#38388;&#20013;&#26356;&#22810;&#26679;&#21270;&#30340;&#21306;&#22495;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23398;&#29983;&#27169;&#22411;&#30340;&#26799;&#24230;&#38388;&#25509;&#20272;&#35745;&#30446;&#26631;&#27169;&#22411;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29992;&#20110;&#29983;&#25104;&#22120;&#32593;&#32476;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#19982;&#29616;&#26377;&#26041;&#27861;&#31561;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;"Bacteria-Farm"&#65292;&#22522;&#20110;&#23454;&#39564;&#23460;&#23553;&#38381;&#39282;&#20859;&#22330;&#20013;&#32454;&#33740;&#29983;&#38271;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#24179;&#34913;&#24615;&#33021;&#21644;&#23547;&#25214;&#26368;&#20248;&#21442;&#25968;&#30340;&#38590;&#24230;&#26469;&#35299;&#20915;&#32858;&#31867;&#31639;&#27861;&#20013;&#21442;&#25968;&#20272;&#35745;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#21019;&#24314;&#19981;&#21516;&#30340;&#29256;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23450;&#22122;&#22768;&#35268;&#33539;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10047</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#22122;&#22768;&#35268;&#33539;&#30340;&#27169;&#22359;&#21270;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Modular Spatial Clustering Algorithm with Noise Specification. (arXiv:2309.10047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;"Bacteria-Farm"&#65292;&#22522;&#20110;&#23454;&#39564;&#23460;&#23553;&#38381;&#39282;&#20859;&#22330;&#20013;&#32454;&#33740;&#29983;&#38271;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#24179;&#34913;&#24615;&#33021;&#21644;&#23547;&#25214;&#26368;&#20248;&#21442;&#25968;&#30340;&#38590;&#24230;&#26469;&#35299;&#20915;&#32858;&#31867;&#31639;&#27861;&#20013;&#21442;&#25968;&#20272;&#35745;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#21019;&#24314;&#19981;&#21516;&#30340;&#29256;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23450;&#22122;&#22768;&#35268;&#33539;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#25216;&#26415;&#26159;&#25968;&#25454;&#25366;&#25496;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#27169;&#24335;&#35782;&#21035;&#30340;&#20851;&#38190;&#39537;&#21160;&#21147;&#12290;&#20854;&#20013;&#26368;&#27969;&#34892;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#26159;DBSCAN&#65292;&#30001;&#20110;&#20854;&#39640;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#23481;&#24525;&#24230;&#12290;&#35768;&#22810;&#20248;&#31168;&#30340;&#31639;&#27861;&#65292;&#22914;DBSCAN&#65292;&#20854;&#36755;&#20837;&#21442;&#25968;&#38590;&#20197;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#25214;&#21040;&#36825;&#20123;&#21442;&#25968;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#31639;&#27861;"Bacteria-Farm"&#65292;&#23427;&#22312;&#24615;&#33021;&#21644;&#23547;&#25214;&#32858;&#31867;&#30340;&#26368;&#20248;&#21442;&#25968;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;Bacteria-Farm&#31639;&#27861;&#21463;&#21040;&#23454;&#39564;&#23460;&#23553;&#38381;&#39282;&#20859;&#22330;&#20013;&#32454;&#33740;&#29983;&#38271;&#30340;&#21551;&#21457; - &#23427;&#20204;&#30340;&#33021;&#21147;&#28040;&#32791;&#39135;&#29289;&#24182;&#29983;&#38271; - &#36825;&#19982;&#32858;&#31867;&#31639;&#27861;&#20013;&#26399;&#26395;&#30340;&#29702;&#24819;&#32858;&#31867;&#29983;&#38271;&#26041;&#24335;&#38750;&#24120;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#20801;&#35768;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;/&#25968;&#25454;&#20998;&#24067;&#30340;&#29256;&#26412;&#21019;&#24314;&#31639;&#27861;&#12290;&#19982;&#20854;&#20182;&#32858;&#31867;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#25552;&#20379;&#20102;&#25351;&#23450;&#22122;&#22768;&#35268;&#33539;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering techniques have been the key drivers of data mining, machine learning and pattern recognition for decades. One of the most popular clustering algorithms is DBSCAN due to its high accuracy and noise tolerance. Many superior algorithms such as DBSCAN have input parameters that are hard to estimate. Therefore, finding those parameters is a time consuming process. In this paper, we propose a novel clustering algorithm Bacteria-Farm, which balances the performance and ease of finding the optimal parameters for clustering. Bacteria- Farm algorithm is inspired by the growth of bacteria in closed experimental farms - their ability to consume food and grow - which closely represents the ideal cluster growth desired in clustering algorithms. In addition, the algorithm features a modular design to allow the creation of versions of the algorithm for specific tasks / distributions of data. In contrast with other clustering algorithms, our algorithm also has a provision to specify the amo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEL&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;20&#20010;&#35757;&#32451;&#36718;&#25968;&#20869;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#24120;&#29992;&#24494;&#35843;&#26041;&#27861;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10019</link><description>&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#38271;&#23614;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Long-Tailed Recognition. (arXiv:2309.10019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEL&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;20&#20010;&#35757;&#32451;&#36718;&#25968;&#20869;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#24120;&#29992;&#24494;&#35843;&#26041;&#27861;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20986;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;CLIP&#65289;&#65292;"&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;"&#33539;&#20363;&#22312;&#35299;&#20915;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#20808;&#21069;&#30740;&#31350;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#36718;&#25968;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEL&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;20&#20010;&#35757;&#32451;&#36718;&#25968;&#20869;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;&#20363;&#22914;&#20840;&#38754;&#24494;&#35843;&#21644;&#20998;&#31867;&#22120;&#24494;&#35843;&#65289;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;PEL&#37319;&#29992;&#20102;&#29616;&#26377;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#65292;&#24341;&#20837;&#20102;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The "pre-training and fine-tuning" paradigm in addressing long-tailed recognition tasks has sparked significant interest since the emergence of large vision-language models like the contrastive language-image pre-training (CLIP). While previous studies have shown promise in adapting pre-trained models for these tasks, they often undesirably require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.10016</link><description>&lt;p&gt;
GPT-3&#29992;&#20110;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction. (arXiv:2309.10016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#33647;&#29289;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#22312;&#20116;&#31181;&#32452;&#32455;&#31867;&#22411;&#20013;&#25506;&#31350;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20998;&#21035;&#37319;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#24494;&#35843;&#33539;&#24335;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#26377;&#26395;&#20026;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the potential of GPT-3 for the anti-cancer drug sensitivity prediction task using structured pharmacogenomics data across five tissue types and evaluated its performance with zero-shot prompting and fine-tuning paradigms. The drug's smile representation and cell line's genomic mutation features were predictive of the drug response. The results from this study have the potential to pave the way for designing more efficient treatment protocols in precision oncology.
&lt;/p&gt;</description></item><item><title>SYNDICOM&#26159;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#24120;&#35782;&#30340;&#26041;&#27861;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#24120;&#35782;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10015</link><description>&lt;p&gt;
SYNDICOM: &#38169;&#35823;&#27880;&#20837;&#21644;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25913;&#36827;&#23545;&#35805;&#24120;&#35782;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback. (arXiv:2309.10015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10015
&lt;/p&gt;
&lt;p&gt;
SYNDICOM&#26159;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#24120;&#35782;&#30340;&#26041;&#27861;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#24120;&#35782;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26159;&#20154;&#31867;&#20132;&#27969;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24120;&#35782;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SYNDICOM - &#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#20013;&#24120;&#35782;&#30340;&#26041;&#27861;&#12290;SYNDICOM&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#26159;&#19968;&#20010;&#30001;&#30693;&#35782;&#22270;&#21019;&#24314;&#30340;&#24120;&#35782;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#21512;&#25104;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#21644;&#26080;&#25928;&#22238;&#31572;&#65292;&#20197;&#21450;&#23545;&#26080;&#25928;&#22238;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65288;NLF&#65289;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#20004;&#27493;&#30340;&#36807;&#31243;&#65306;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#26080;&#25928;&#22238;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65288;NLF&#65289;&#65292;&#28982;&#21518;&#26681;&#25454;&#39044;&#27979;&#30340;NLF&#12289;&#26080;&#25928;&#22238;&#31572;&#21644;&#23545;&#35805;&#26465;&#20214;&#35757;&#32451;&#19968;&#20010;&#24212;&#31572;&#29983;&#25104;&#27169;&#22411;&#12290;SYNDICOM&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#65292;&#19981;&#38656;&#35201;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#20219;&#21153;&#30340;&#32463;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is a critical aspect of human communication. Despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce SYNDICOM - a method for improving commonsense in dialogue response generation. SYNDICOM consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. SYNDICOM is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;Transformer&#27169;&#22411;&#36808;&#20986;&#20102;&#39044;&#27979;&#22810;&#21464;&#37327;&#30005;&#27744;&#24615;&#33021;&#21644;&#20581;&#24247;&#29366;&#24577;&#30340;&#31532;&#19968;&#27493;&#65292;&#20026;&#35774;&#35745;&#26356;&#22909;&#30340;&#30005;&#27744;&#21644;&#20943;&#23569;&#23454;&#39564;&#24037;&#20316;&#37327;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10014</link><description>&lt;p&gt;
&#36890;&#36807;Transformer&#27169;&#22411;&#39044;&#27979;&#22810;&#21464;&#37327;&#30005;&#27744;&#24615;&#33021;&#21644;&#20581;&#24247;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Prognosis of Multivariate Battery State of Performance and Health via Transformers. (arXiv:2309.10014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;Transformer&#27169;&#22411;&#36808;&#20986;&#20102;&#39044;&#27979;&#22810;&#21464;&#37327;&#30005;&#27744;&#24615;&#33021;&#21644;&#20581;&#24247;&#29366;&#24577;&#30340;&#31532;&#19968;&#27493;&#65292;&#20026;&#35774;&#35745;&#26356;&#22909;&#30340;&#30005;&#27744;&#21644;&#20943;&#23569;&#23454;&#39564;&#24037;&#20316;&#37327;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#22312;&#28145;&#24230;&#21435;&#30899;&#21270;&#30340;&#26410;&#26469;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#29702;&#35299;&#30005;&#27744;&#24615;&#33021;&#21644;&#8220;&#23551;&#21629;&#8221;&#19982;&#35774;&#35745;&#21644;&#20351;&#29992;&#30340;&#20851;&#31995;&#23545;&#20110;&#21152;&#24555;&#37319;&#29992;&#30005;&#27744;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#65288;SOH&#65289;&#34987;&#24635;&#32467;&#20026;&#19968;&#20010;&#21442;&#25968;&#65292;&#21363;&#30005;&#27744;&#23481;&#37327;&#30456;&#23545;&#20110;&#21021;&#22987;&#29366;&#24577;&#30340;&#27604;&#20363;&#12290;&#28982;&#32780;&#65292;&#26356;&#23454;&#29992;&#30340;&#26041;&#27861;&#26159;&#32508;&#21512;&#25551;&#36848;&#20854;&#29366;&#24577;&#21644;&#22797;&#26434;&#24615;&#65292;&#20351;&#29992;&#19968;&#32452;&#30456;&#20851;&#30340;&#25551;&#36848;&#31526;&#65292;&#21253;&#25324;&#23481;&#37327;&#12289;&#33021;&#37327;&#12289;&#31163;&#23376;&#21644;&#30005;&#23376;&#38459;&#25239;&#12289;&#24320;&#36335;&#30005;&#21387;&#21644;&#24494;&#35266;&#32467;&#26500;&#24230;&#37327;&#12290;&#20107;&#23454;&#19978;&#65292;&#20934;&#30830;&#39044;&#27979;&#30005;&#27744;&#20351;&#29992;&#36807;&#31243;&#20013;&#19968;&#31995;&#21015;&#24615;&#33021;&#26159;&#30005;&#27744;&#31185;&#23398;&#30340;&#8220;&#22307;&#26479;&#8221;&#65307;&#23427;&#21487;&#20197;&#20026;&#26356;&#22909;&#22320;&#35774;&#35745;&#30005;&#27744;&#25552;&#20379;&#21069;&#25152;&#26410;&#26377;&#30340;&#35265;&#35299;&#65292;&#24182;&#38477;&#20302;&#28385;&#36275;CO2&#20943;&#25490;&#30446;&#26631;&#25152;&#24517;&#38656;&#30340;&#33021;&#28304;&#20648;&#23384;&#25237;&#36164;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;Transformer&#27169;&#22411;&#36808;&#20986;&#20102;&#36825;&#20010;&#26041;&#21521;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batteries are an essential component in a deeply decarbonized future. Understanding battery performance and "useful life" as a function of design and use is of paramount importance to accelerating adoption. Historically, battery state of health (SOH) was summarized by a single parameter, the fraction of a battery's capacity relative to its initial state. A more useful approach, however, is a comprehensive characterization of its state and complexities, using an interrelated set of descriptors including capacity, energy, ionic and electronic impedances, open circuit voltages, and microstructure metrics. Indeed, predicting across an extensive suite of properties as a function of battery use is a "holy grail" of battery science; it can provide unprecedented insights toward the design of better batteries with reduced experimental effort, and de-risking energy storage investments that are necessary to meet CO2 reduction targets. In this work, we present a first step in that direction via de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#36229;&#20960;&#20309;&#21644;&#27431;&#20960;&#37324;&#24471;&#23884;&#20837;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#27604;&#36739;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36229;&#20960;&#20309;&#23884;&#20837;&#22312;&#39640;&#32500;&#24230;&#20013;&#21576;&#29616;&#20986;&#36793;&#30028;&#25910;&#25947;&#30340;&#36235;&#21183;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#19981;&#21516;&#65292;&#30740;&#31350;&#32773;&#36824;&#21457;&#29616;&#65292;&#37197;&#22791;&#22266;&#23450;&#21322;&#24452;&#30340;&#27431;&#20960;&#37324;&#24471;&#32534;&#30721;&#22120;&#20063;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10013</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#19982;&#27431;&#20960;&#37324;&#24471;&#23884;&#20837;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#27604;&#36739;&#65306;&#21516;&#19968;&#20010;&#30828;&#24065;&#30340;&#20004;&#38754;&#12290;(arXiv:2309.10013v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin. (arXiv:2309.10013v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#36229;&#20960;&#20309;&#21644;&#27431;&#20960;&#37324;&#24471;&#23884;&#20837;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#27604;&#36739;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36229;&#20960;&#20309;&#23884;&#20837;&#22312;&#39640;&#32500;&#24230;&#20013;&#21576;&#29616;&#20986;&#36793;&#30028;&#25910;&#25947;&#30340;&#36235;&#21183;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#19981;&#21516;&#65292;&#30740;&#31350;&#32773;&#36824;&#21457;&#29616;&#65292;&#37197;&#22791;&#22266;&#23450;&#21322;&#24452;&#30340;&#27431;&#20960;&#37324;&#24471;&#32534;&#30721;&#22120;&#20063;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#34920;&#26126;&#65292;&#23618;&#27425;&#21270;&#25968;&#25454;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#20855;&#26377;&#20302;&#32500;&#19988;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36229;&#20960;&#20309;&#23884;&#20837;&#22312;&#22270;&#20687;&#35782;&#21035;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#23427;&#20204;&#30340;&#20248;&#21270;&#23481;&#26131;&#36935;&#21040;&#25968;&#20540;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#27431;&#20960;&#37324;&#24471;&#29305;&#24449;&#30456;&#27604;&#65292;&#36229;&#20960;&#20309;&#24615;&#23545;&#21738;&#20123;&#24212;&#29992;&#31243;&#24207;&#26368;&#26377;&#30410;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#21407;&#22411;&#36229;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#12290;&#29305;&#21035;&#26159;&#36229;&#20960;&#20309;&#23884;&#20837;&#22312;&#39640;&#32500;&#24230;&#20013;&#25910;&#25947;&#20110;Poincar\'e&#29699;&#36793;&#30028;&#30340;&#36235;&#21183;&#20197;&#21450;&#36825;&#23545;&#23569;&#26679;&#26412;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#22909;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#26159;&#22312;&#20849;&#21516;&#30340;&#36229;&#20960;&#20309;&#21322;&#24452;&#19979;&#33719;&#24471;&#30340;&#36229;&#20960;&#20309;&#23884;&#20837;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#35770;Euclidean&#24230;&#37327;&#65292;&#37197;&#22791;&#22266;&#23450;&#21322;&#24452;&#30340;&#32534;&#30721;&#22120;&#37117;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#19981;&#26029;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#20197;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#33976;&#39311;&#28508;&#22312;&#31354;&#38388;&#12289;&#25913;&#21892;&#29983;&#25104;&#29305;&#24449;&#23545;&#40784;&#21644;&#21608;&#26399;&#24615;&#29983;&#25104;&#20197;&#22686;&#24378;&#30693;&#35782;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2309.10012</link><description>&lt;p&gt;
&#31397;&#25506;&#36807;&#21435;&#65306;&#25913;&#36827;&#19981;&#26029;&#23398;&#20064;&#20013;&#29983;&#25104;&#22238;&#25918;&#30340;&#30693;&#35782;&#20445;&#30041;
&lt;/p&gt;
&lt;p&gt;
Looking through the past: better knowledge retention for generative replay in continual learning. (arXiv:2309.10012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#19981;&#26029;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#20197;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#33976;&#39311;&#28508;&#22312;&#31354;&#38388;&#12289;&#25913;&#21892;&#29983;&#25104;&#29305;&#24449;&#23545;&#40784;&#21644;&#21608;&#26399;&#24615;&#29983;&#25104;&#20197;&#22686;&#24378;&#30693;&#35782;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#19981;&#26029;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#29983;&#25104;&#22238;&#25918;&#65292;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#24403;&#21069;&#30340;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#36890;&#24120;&#22312;&#23567;&#22411;&#21644;&#31616;&#21333;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#36275;&#20197;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#21644;&#26356;&#22810;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#22312;&#22522;&#20110;VAE&#30340;&#29983;&#25104;&#22238;&#25918;&#20013;&#65292;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;&#24403;&#29983;&#25104;&#30340;&#29305;&#24449;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26102;&#19982;&#21407;&#22987;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24403;&#21069;&#27169;&#22411;&#19982;&#20808;&#21069;&#27169;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#33976;&#39311;&#65292;&#20197;&#20943;&#23569;&#29305;&#24449;&#28418;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#29983;&#25104;&#29305;&#24449;&#23545;&#40784;&#30340;&#37325;&#24314;&#21644;&#21407;&#22987;&#25968;&#25454;&#30340;&#28508;&#22312;&#21305;&#37197;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#22522;&#20110;&#23545;&#20445;&#23384;&#30693;&#35782;&#30340;&#37325;&#24314;&#25928;&#26524;&#26356;&#22909;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#36890;&#36807;&#20808;&#21069;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#21608;&#26399;&#24615;&#29983;&#25104;&#30340;&#26041;&#24335;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#30693;&#35782;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we improve the generative replay in a continual learning setting to perform well on challenging scenarios. Current generative rehearsal methods are usually benchmarked on small and simple datasets as they are not powerful enough to generate more complex data with a greater number of classes. We notice that in VAE-based generative replay, this could be attributed to the fact that the generated features are far from the original ones when mapped to the latent space. Therefore, we propose three modifications that allow the model to learn and generate complex data. More specifically, we incorporate the distillation in latent space between the current and previous models to reduce feature drift. Additionally, a latent matching for the reconstruction and original data is proposed to improve generated features alignment. Further, based on the observation that the reconstructions are better for preserving knowledge, we add the cycling of generations through the previously trained
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22902;&#29275;&#20013;&#26089;&#26399;&#39044;&#27979;&#21644;&#26816;&#27979;&#25968;&#23383;&#30382;&#28814;&#12290;&#20854;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;79%&#21644;64%&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#23454;&#26102;&#33258;&#21160;&#21270;&#24037;&#20855;&#36827;&#34892;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.10010</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#21644;&#26816;&#27979;&#22902;&#29275;&#26089;&#21457;&#24615;&#25968;&#23383;&#30382;&#28814;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Approaches to Predict and Detect Early-Onset of Digital Dermatitis in Dairy Cows using Sensor Data. (arXiv:2309.10010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22902;&#29275;&#20013;&#26089;&#26399;&#39044;&#27979;&#21644;&#26816;&#27979;&#25968;&#23383;&#30382;&#28814;&#12290;&#20854;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;79%&#21644;64%&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#23454;&#26102;&#33258;&#21160;&#21270;&#24037;&#20855;&#36827;&#34892;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22522;&#20110;&#20256;&#24863;&#22120;&#34892;&#20026;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#22902;&#29275;&#25968;&#23383;&#30382;&#28814;&#65288;DD&#65289;&#21644;&#39044;&#27979;DD&#12290;&#36890;&#36807;&#24314;&#31435;&#26089;&#26399;&#39044;&#35686;&#24037;&#20855;&#65292;&#26356;&#22909;&#22320;&#30417;&#27979;&#21644;&#31649;&#29702;&#21830;&#19994;&#29615;&#22659;&#19979;&#30340;DD&#65292;&#38477;&#20302;DD&#30340;&#21457;&#30149;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#65292;&#21516;&#26102;&#25913;&#21892;&#21160;&#29289;&#31119;&#21033;&#12290;&#22312;&#26412;&#25506;&#32034;&#24615;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#33021;&#22815;&#22522;&#20110;&#34892;&#20026;&#20256;&#24863;&#22120;&#25968;&#25454;&#39044;&#27979;&#21644;&#26816;&#27979;&#25955;&#20859;&#26465;&#20214;&#19979;&#22902;&#29275;&#25968;&#23383;&#30382;&#28814;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#39318;&#27425;&#20986;&#29616;&#20020;&#24202;&#30151;&#29366;&#30340;&#31532;0&#22825;&#65292;DD&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;79%&#65292;&#22312;&#39318;&#27425;&#20020;&#24202;&#30151;&#29366;&#20986;&#29616;&#21069;2&#22825;&#65292;DD&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;64%&#12290;&#25152;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#23454;&#26102;&#33258;&#21160;&#21270;&#24037;&#20855;&#36827;&#34892;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study was to employ machine learning algorithms based on sensor behavior data for (1) early-onset detection of digital dermatitis (DD); and (2) DD prediction in dairy cows. With the ultimate goal to set-up early warning tools for DD prediction, which would than allow a better monitoring and management of DD under commercial settings, resulting in a decrease of DD prevalence and severity, while improving animal welfare. A machine learning model that is capable of predicting and detecting digital dermatitis in cows housed under free-stall conditions based on behavior sensor data has been purposed and tested in this exploratory study. The model for DD detection on day 0 of the appearance of the clinical signs has reached an accuracy of 79%, while the model for prediction of DD 2 days prior to the appearance of the first clinical signs has reached an accuracy of 64%. The proposed machine learning models could help to develop a real-time automated tool for monitoring and dia
&lt;/p&gt;</description></item><item><title>DeepHEN&#26159;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;&#38271;&#38142;&#38750;&#32534;&#30721;RNA&#22522;&#22240;&#24517;&#38656;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#33021;&#30830;&#23450;&#24207;&#21015;&#29305;&#24449;&#21644;&#32593;&#32476;&#31354;&#38388;&#29305;&#24449;&#23545;&#24517;&#38656;&#24615;&#30340;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10008</link><description>&lt;p&gt;
DeepHEN: &#39044;&#27979;&#22522;&#22240;&#30340;&#24517;&#38656;&#24615;&#38271;&#38142;&#38750;&#32534;&#30721;RNA&#22522;&#22240;&#24182;&#37325;&#26032;&#24605;&#32771;&#38271;&#38142;&#38750;&#32534;&#30721;RNA&#22522;&#22240;&#30340;&#24517;&#38656;&#24615;
&lt;/p&gt;
&lt;p&gt;
DeepHEN: quantitative prediction essential lncRNA genes and rethinking essentialities of lncRNA genes. (arXiv:2309.10008v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10008
&lt;/p&gt;
&lt;p&gt;
DeepHEN&#26159;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;&#38271;&#38142;&#38750;&#32534;&#30721;RNA&#22522;&#22240;&#24517;&#38656;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#33021;&#30830;&#23450;&#24207;&#21015;&#29305;&#24449;&#21644;&#32593;&#32476;&#31354;&#38388;&#29305;&#24449;&#23545;&#24517;&#38656;&#24615;&#30340;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#30340;&#24517;&#38656;&#24615;&#26159;&#25351;&#22522;&#22240;&#23545;&#29983;&#29289;&#20307;&#30340;&#29983;&#23384;&#21644;&#32321;&#27542;&#25928;&#26524;&#30340;&#24517;&#35201;&#31243;&#24230;&#12290;&#23613;&#31649;&#38750;&#32534;&#30721;&#22522;&#22240;&#30340;&#24517;&#38656;&#24615;&#24050;&#32463;&#26377;&#25152;&#35760;&#24405;&#65292;&#20294;&#25105;&#20204;&#23545;&#38750;&#32534;&#30721;&#22522;&#22240;&#30340;&#24517;&#38656;&#24615;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#26410;&#30693;&#30340;&#26041;&#38754;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#24207;&#21015;&#29305;&#24449;&#21644;&#32593;&#32476;&#31354;&#38388;&#29305;&#24449;&#23545;&#24517;&#38656;&#24615;&#30340;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepHEN&#65292;&#23427;&#21487;&#20197;&#22238;&#31572;&#19978;&#36848;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#26032;&#30340;&#38271;&#38142;&#38750;&#32534;&#30721;RNA-&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;DeepHEN&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#38271;&#38142;&#38750;&#32534;&#30721;RNA&#22522;&#22240;&#30340;&#24517;&#38656;&#24615;&#12290;&#19982;&#20854;&#20182;&#39044;&#27979;&#38271;&#38142;&#38750;&#32534;&#30721;RNA&#22522;&#22240;&#24517;&#38656;&#24615;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;DeepHEN&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#30830;&#23450;&#24207;&#21015;&#29305;&#24449;&#25110;&#32593;&#32476;&#31354;&#38388;&#29305;&#24449;&#23545;&#24517;&#38656;&#24615;&#30340;&#24433;&#21709;&#26356;&#22823;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#24517;&#38656;&#24615;&#36739;&#23569;&#32780;&#23548;&#33268;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gene essentiality refers to the degree to which a gene is necessary for the survival and reproductive efficacy of a living organism. Although the essentiality of non-coding genes has been documented, there are still aspects of non-coding genes' essentiality that are unknown to us. For example, We do not know the contribution of sequence features and network spatial features to essentiality. As a consequence, in this work, we propose DeepHEN that could answer the above question. By buidling a new lncRNA-proteion-protein network and utilizing both representation learning and graph neural network, we successfully build our DeepHEN models that could predict the essentiality of lncRNA genes. Compared to other methods for predicting the essentiality of lncRNA genes, our DeepHEN model not only tells whether sequence features or network spatial features have a greater influence on essentiality but also addresses the overfitting issue of those methods caused by the low number of essential lncRN
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#22521;&#20859;&#21512;&#20316;&#19982;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#29983;&#24577;&#31995;&#32479;&#24320;&#21457;&#20102;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.10007</link><description>&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#38388;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#19982;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem. (arXiv:2309.10007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#22521;&#20859;&#21512;&#20316;&#19982;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#29983;&#24577;&#31995;&#32479;&#24320;&#21457;&#20102;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#22521;&#20859;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#65292;&#24320;&#21457;&#20986;&#19982;&#30495;&#23454;&#30340;Nigel&#21644;F1TENTH&#20004;&#31181;&#27604;&#20363;&#33258;&#20027;&#36710;&#36742;&#24179;&#21488;&#20855;&#26377;&#29420;&#29305;&#29305;&#24615;&#21644;&#33021;&#21147;&#30340;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#29983;&#24577;&#31995;&#32479;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#20132;&#21449;&#36335;&#21475;&#31359;&#36234;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#32452;&#21512;&#20316;&#36710;&#36742;&#65288;Nigel&#65289;&#22312;&#21333;&#20010;&#25110;&#22810;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#65292;&#37319;&#29992;&#19968;&#31181;&#20844;&#20849;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#30340;&#22836;&#23545;&#22836;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#65292;&#20351;&#29992;&#21478;&#19968;&#32452;&#36710;&#36742;&#65288;F1TENTH&#65289;&#22312;&#22810;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#37117;&#37319;&#29992;&#20102;&#20998;&#25955;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.10003</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19987;&#21033;&#26435;&#35201;&#27714;&#30340;&#33539;&#22260;&#27979;&#37327;&#20026;&#35813;&#35201;&#27714;&#25152;&#21253;&#21547;&#30340;&#33258;&#20449;&#24687;&#30340;&#20498;&#25968;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#65292;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#32597;&#35265;&#30340;&#27010;&#24565;&#27604;&#24179;&#24120;&#30340;&#27010;&#24565;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#22240;&#20026;&#23427;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#33258;&#20449;&#24687;&#26159;&#20174;&#35813;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#20854;&#20013;&#27010;&#29575;&#26159;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20116;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65288;&#27599;&#20010;&#21333;&#35789;&#25110;&#23383;&#31526;&#22343;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#21462;&#65289;&#21040;&#20013;&#31561;&#27169;&#22411;&#65288;&#20351;&#29992;&#24179;&#22343;&#35789;&#25110;&#23383;&#31526;&#39057;&#29575;&#65289;&#65292;&#20877;&#21040;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT2&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33539;&#22260;&#24230;&#37327;&#20943;&#23569;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#65292;&#36825;&#26159;&#20808;&#21069;&#20316;&#21697;&#20013;&#24050;&#32463;&#20351;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20061;&#20010;&#31995;&#21015;&#30340;&#38024;&#23545;&#19981;&#21516;&#21457;&#26126;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#65292;&#20854;&#20013;&#27599;&#20010;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#31283;&#23450;&#32593;&#32476;(EStable-Net)&#29992;&#20110;&#35299;&#20915;&#26799;&#24230;&#27969;&#26041;&#31243;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#38477;&#20302;&#31163;&#25955;&#33021;&#37327;&#24182;&#29983;&#25104;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.10002</link><description>&lt;p&gt;
&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#33021;&#37327;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Energy stable neural network for gradient flow equations. (arXiv:2309.10002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#31283;&#23450;&#32593;&#32476;(EStable-Net)&#29992;&#20110;&#35299;&#20915;&#26799;&#24230;&#27969;&#26041;&#31243;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#38477;&#20302;&#31163;&#25955;&#33021;&#37327;&#24182;&#29983;&#25104;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27714;&#35299;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#33021;&#37327;&#31283;&#23450;&#32593;&#32476;&#65288;EStable-Net&#65289;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;EStable-Net&#30340;&#35299;&#26356;&#26032;&#26041;&#26696;&#21463;&#21040;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#22522;&#20110;&#36741;&#21161;&#21464;&#37327;&#30340;&#31561;&#20215;&#24418;&#24335;&#30340;&#21551;&#21457;&#12290;EStable-Net&#33021;&#22815;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#38477;&#20302;&#31163;&#25955;&#33021;&#37327;&#65292;&#19982;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#28436;&#21270;&#36807;&#31243;&#30340;&#24615;&#36136;&#20445;&#25345;&#19968;&#33268;&#12290;&#31070;&#32463;&#32593;&#32476;EStable-Net&#30340;&#26550;&#26500;&#21253;&#25324;&#20960;&#20010;&#33021;&#37327;&#34928;&#20943;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#30340;&#36755;&#20986;&#21487;&#20197;&#35299;&#37322;&#20026;&#26799;&#24230;&#27969;&#26041;&#31243;&#28436;&#21270;&#36807;&#31243;&#30340;&#20013;&#38388;&#29366;&#24577;&#12290;&#36825;&#31181;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#20010;&#31283;&#23450;&#12289;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an energy stable network (EStable-Net) for solving gradient flow equations. The solution update scheme in our neural network EStable-Net is inspired by a proposed auxiliary variable based equivalent form of the gradient flow equation. EStable-Net enables decreasing of a discrete energy along the neural network, which is consistent with the property in the evolution process of the gradient flow equation. The architecture of the neural network EStable-Net consists of a few energy decay blocks, and the output of each block can be interpreted as an intermediate state of the evolution process of the gradient flow equation. This design provides a stable, efficient and interpretable network structure. Numerical experimental results demonstrate that our network is able to generate high accuracy and stable predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#25991;&#26723;&#23884;&#20837;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;&#26041;&#38754;&#65292;&#29305;&#23450;&#30340;&#23884;&#20837;&#26041;&#27861;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#30340;&#32452;&#21512;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10000</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26723;&#23884;&#20837;&#21644;&#38477;&#32500;&#26041;&#27861;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Detecting covariate drift in text data using document embeddings and dimensionality reduction. (arXiv:2309.10000v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#25991;&#26723;&#23884;&#20837;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;&#26041;&#38754;&#65292;&#29305;&#23450;&#30340;&#23884;&#20837;&#26041;&#27861;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#30340;&#32452;&#21512;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;&#23545;&#20110;&#20445;&#25345;&#25991;&#26412;&#20998;&#26512;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#30340;&#25991;&#26723;&#23884;&#20837;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#35782;&#21035;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#28418;&#31227;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#25991;&#26723;&#23884;&#20837;&#26041;&#27861;&#65306;&#20351;&#29992;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#65288;LSA&#65289;&#30340;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#36827;&#34892;&#38477;&#32500;&#65292;&#20197;&#21450;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#36827;&#34892;&#38477;&#32500;&#30340;Doc2Vec&#21644;BERT&#23884;&#20837;&#12290;&#20026;&#20102;&#37327;&#21270;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;Kolmogorov-Smirnov&#65288;KS&#65289;&#32479;&#35745;&#37327;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#26816;&#39564;&#20316;&#20026;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26816;&#27979;&#21327;&#21464;&#28418;&#31227;&#26041;&#38754;&#65292;&#26576;&#20123;&#23884;&#20837;&#26041;&#27861;&#12289;&#38477;&#32500;&#25216;&#26415;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#30340;&#32452;&#21512;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting covariate drift in text data is essential for maintaining the reliability and performance of text analysis models. In this research, we investigate the effectiveness of different document embeddings, dimensionality reduction techniques, and drift detection methods for identifying covariate drift in text data. We explore three popular document embeddings: term frequency-inverse document frequency (TF-IDF) using Latent semantic analysis(LSA) for dimentionality reduction and Doc2Vec, and BERT embeddings, with and without using principal component analysis (PCA) for dimensionality reduction. To quantify the divergence between training and test data distributions, we employ the Kolmogorov-Smirnov (KS) statistic and the Maximum Mean Discrepancy (MMD) test as drift detection methods. Experimental results demonstrate that certain combinations of embeddings, dimensionality reduction techniques, and drift detection methods outperform others in detecting covariate drift. Our findings co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#38899;&#39057;&#20998;&#31867;&#22120;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#32654;&#22269;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;38.5%&#12290;</title><link>http://arxiv.org/abs/2309.09996</link><description>&lt;p&gt;
&#29992;&#38899;&#39057;&#20998;&#31867;&#25913;&#36827;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#30340;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Recognition for African American English With Audio Classification. (arXiv:2309.09996v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09996
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#38899;&#39057;&#20998;&#31867;&#22120;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#32654;&#22269;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;38.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#22312;&#35782;&#21035;&#19981;&#21516;&#35821;&#35328;&#21464;&#31181;&#26102;&#23384;&#22312;&#36739;&#22823;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#12290;&#20294;&#26159;&#26377;&#26102;&#20505;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#20250;&#20351;&#35813;&#26041;&#27861;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#39046;&#22495;&#22806;&#25968;&#25454;(&#38271;&#31687;&#24418;&#24335;&#30340;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;)&#26469;&#25552;&#39640;&#32654;&#22269;&#33521;&#35821;&#30701;&#31687;&#35821;&#38899;&#35782;&#21035;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;CORAAL&#12289;YouTube&#21644;Mozilla Common Voice&#26469;&#35757;&#32451;&#19968;&#20010;&#38899;&#39057;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#21487;&#20197;&#22823;&#33268;&#21028;&#26029;&#19968;&#21477;&#35805;&#26159;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#36824;&#26159;&#20854;&#20182;&#21464;&#31181;&#65292;&#21253;&#25324;&#20027;&#27969;&#32654;&#22269;&#33521;&#35821;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#22120;&#36755;&#20986;&#19982;&#31895;&#30053;&#30340;&#22320;&#29702;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#22823;&#37327;&#26410;&#32763;&#35793;&#30340;&#30701;&#31687;&#26597;&#35810;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;&#35821;&#21477;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#27492;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;38.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems have been shown to have large quality disparities between the language varieties they are intended or expected to recognize. One way to mitigate this is to train or fine-tune models with more representative datasets. But this approach can be hindered by limited in-domain data for training and evaluation. We propose a new way to improve the robustness of a US English short-form speech recognizer using a small amount of out-of-domain (long-form) African American English (AAE) data. We use CORAAL, YouTube and Mozilla Common Voice to train an audio classifier to approximately output whether an utterance is AAE or some other variety including Mainstream American English (MAE). By combining the classifier output with coarse geographic information, we can select a subset of utterances from a large corpus of untranscribed short-form queries for semi-supervised learning at scale. Fine-tuning on this data results in a 38.5% relative word error rate disp
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20102;&#26032;&#20896;&#21518;&#38271;&#26399;&#31070;&#32463;&#24182;&#21457;&#30151;&#30340;&#32467;&#26524;&#65292;&#21457;&#29616;68%&#30340;&#24739;&#32773;&#25253;&#21578;&#26377;&#31070;&#32463;&#30151;&#29366;&#65292;&#20854;&#20013;&#30130;&#21171;&#12289;&#22836;&#30171;&#21644;&#21957;&#35273;&#20007;&#22833;&#26368;&#24120;&#35265;&#12290;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#35782;&#21035;&#24739;&#32773;&#21457;&#23637;&#31070;&#32463;&#30149;&#39118;&#38505;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09993</link><description>&lt;p&gt;
&#26032;&#20896;&#21518;&#38271;&#26399;&#31070;&#32463;&#21518;&#36951;&#30151;&#65306;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#32467;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Long-term Neurological Sequelae in Post-COVID-19 Patients: A Machine Learning Approach to Predict Outcomes. (arXiv:2309.09993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09993
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20102;&#26032;&#20896;&#21518;&#38271;&#26399;&#31070;&#32463;&#24182;&#21457;&#30151;&#30340;&#32467;&#26524;&#65292;&#21457;&#29616;68%&#30340;&#24739;&#32773;&#25253;&#21578;&#26377;&#31070;&#32463;&#30151;&#29366;&#65292;&#20854;&#20013;&#30130;&#21171;&#12289;&#22836;&#30171;&#21644;&#21957;&#35273;&#20007;&#22833;&#26368;&#24120;&#35265;&#12290;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#35782;&#21035;&#24739;&#32773;&#21457;&#23637;&#31070;&#32463;&#30149;&#39118;&#38505;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20896;&#30123;&#24773;&#25581;&#31034;&#20102;&#24247;&#22797;&#21518;&#24739;&#32773;&#38271;&#26399;&#31070;&#32463;&#24182;&#21457;&#30151;&#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#23545;500&#20363;&#26032;&#20896;&#21518;&#24739;&#32773;&#36827;&#34892;&#20102;&#31070;&#32463;&#21518;&#36951;&#30151;&#30340;&#35843;&#26597;&#65292;&#21253;&#25324;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#19981;&#21516;&#30340;&#20010;&#20307;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22522;&#20110;&#22810;&#26679;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#21644;&#31070;&#32463;&#24433;&#20687;&#21442;&#25968;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;68%&#30340;&#26032;&#20896;&#21518;&#24739;&#32773;&#25253;&#21578;&#20986;&#29616;&#31070;&#32463;&#30151;&#29366;&#65292;&#30130;&#21171;&#12289;&#22836;&#30171;&#21644;&#21957;&#35273;&#20007;&#22833;&#26159;&#26368;&#24120;&#35265;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;22%&#30340;&#24739;&#32773;&#20986;&#29616;&#26356;&#20005;&#37325;&#30340;&#31070;&#32463;&#24182;&#21457;&#30151;&#65292;&#21253;&#25324;&#33041;&#30149;&#21644;&#20013;&#39118;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24212;&#29992;&#22312;&#39044;&#27979;&#38271;&#26399;&#31070;&#32463;&#32467;&#26524;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#36776;&#21035;&#21457;&#23637;&#31070;&#32463;&#30149;&#39118;&#38505;&#30340;&#24739;&#32773;&#19978;&#36798;&#21040;&#20102;85%&#30340;&#20934;&#30830;&#24615;&#12289;80%&#30340;&#25935;&#24863;&#24615;&#21644;90%&#30340;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has brought to light a concerning aspect of long-term neurological complications in post-recovery patients. This study delved into the investigation of such neurological sequelae in a cohort of 500 post-COVID-19 patients, encompassing individuals with varying illness severity. The primary aim was to predict outcomes using a machine learning approach based on diverse clinical data and neuroimaging parameters. The results revealed that 68% of the post-COVID-19 patients reported experiencing neurological symptoms, with fatigue, headache, and anosmia being the most common manifestations. Moreover, 22% of the patients exhibited more severe neurological complications, including encephalopathy and stroke. The application of machine learning models showed promising results in predicting long-term neurological outcomes. Notably, the Random Forest model achieved an accuracy of 85%, sensitivity of 80%, and specificity of 90% in identifying patients at risk of developing neur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TCGF&#30340;&#32479;&#19968;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#20010;&#35270;&#22270;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.09987</link><description>&lt;p&gt;
TCGF: &#19968;&#31181;&#32479;&#19968;&#30340;&#24352;&#37327;&#19968;&#33268;&#24615;&#22270;&#26694;&#26550;&#29992;&#20110;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TCGF: A unified tensorized consensus graph framework for multi-view representation learning. (arXiv:2309.09987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09987
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TCGF&#30340;&#32479;&#19968;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#20010;&#35270;&#22270;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#23398;&#20064;&#25216;&#26415;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#21033;&#29992;&#22810;&#20010;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#20114;&#34917;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#23558;&#29616;&#26377;&#24037;&#20316;&#32479;&#19968;&#21040;&#21487;&#25193;&#23637;&#21644;&#40065;&#26834;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#27867;&#21270;&#22810;&#35270;&#22270;&#26694;&#26550;&#30340;&#30740;&#31350;&#36824;&#19981;&#36275;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#24403;&#21069;&#30340;&#24037;&#20316;&#37117;&#19987;&#27880;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#22810;&#35270;&#22270;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29305;&#23450;&#23610;&#24230;&#22330;&#26223;&#19979;&#20381;&#36182;&#36739;&#37325;&#65292;&#24182;&#19988;&#26080;&#27861;&#25972;&#20307;&#26377;&#25928;&#22320;&#29702;&#35299;&#22810;&#31181;&#23610;&#24230;&#12290;&#36825;&#20123;&#38480;&#21046;&#22952;&#30861;&#20102;&#20174;&#22810;&#35270;&#22270;&#20013;&#26377;&#25928;&#34701;&#21512;&#20851;&#38190;&#20449;&#24687;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tensorized Consensus Graph Framework (TCGF)&#30340;&#36890;&#29992;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#39318;&#20808;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#21033;&#29992;&#21508;&#20010;&#35270;&#22270;&#30340;&#34920;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
Multi-view learning techniques have recently gained significant attention in the machine learning domain for their ability to leverage consistency and complementary information across multiple views. However, there remains a lack of sufficient research on generalized multi-view frameworks that unify existing works into a scalable and robust learning framework, as most current works focus on specific styles of multi-view models. Additionally, most multi-view learning works rely heavily on specific-scale scenarios and fail to effectively comprehend multiple scales holistically. These limitations hinder the effective fusion of essential information from multiple views, resulting in poor generalization. To address these limitations, this paper proposes a universal multi-view representation learning framework named Tensorized Consensus Graph Framework (TCGF). Specifically, it first provides a unified framework for existing multi-view works to exploit the representations for individual view,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#21644;&#27604;&#36739;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#33041;&#23545;&#30495;&#23454;&#22270;&#29255;&#30340;&#21453;&#24212;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#20351;&#29992;&#22810;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#19987;&#27880;&#20110;&#21463;&#35797;&#32773;&#22823;&#33041;&#27599;&#20010;&#21322;&#29699;&#30340;&#27599;&#20010;ROI&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.09983</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#27604;&#36739;&#29992;&#20110;&#39044;&#27979;&#22823;&#33041;&#23545;&#30495;&#23454;&#22270;&#29255;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploration and Comparison of Deep Learning Architectures to Predict Brain Response to Realistic Pictures. (arXiv:2309.09983v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#21644;&#27604;&#36739;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#33041;&#23545;&#30495;&#23454;&#22270;&#29255;&#30340;&#21453;&#24212;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#20351;&#29992;&#22810;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#19987;&#27880;&#20110;&#21463;&#35797;&#32773;&#22823;&#33041;&#27599;&#20010;&#21322;&#29699;&#30340;&#27599;&#20010;ROI&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#20851;&#20110;&#39044;&#27979;&#22823;&#33041;&#23545;&#30495;&#23454;&#22270;&#20687;&#21453;&#24212;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#30740;&#31350;&#65292;&#20197;&#24212;&#23545;Algonauts Challenge 2023&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#23545;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36739;&#31616;&#21333;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#22823;&#33041;&#27963;&#21160;&#65292;&#20294;&#36880;&#28176;&#24341;&#20837;&#20102;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#36935;&#21040;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30456;&#20851;&#30340;&#20856;&#22411;&#22256;&#38590;&#65292;&#27604;&#22914;&#27491;&#21017;&#21270;&#21644;&#36807;&#25311;&#21512;&#65292;&#20197;&#21450;&#19982;&#25361;&#25112;&#29305;&#23450;&#30340;&#38382;&#39064;&#65292;&#22914;&#38590;&#20197;&#32467;&#21512;&#22810;&#20010;&#36755;&#20837;&#32534;&#30721;&#65292;&#20197;&#21450;&#36755;&#20986;&#30340;&#39640;&#32500;&#24230;&#12289;&#19981;&#26126;&#30830;&#30340;&#32467;&#26500;&#21644;&#22024;&#26434;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#22522;&#20110;&#21333;&#36793;&#19977;&#32500;&#20301;&#32622;&#12289;&#22810;&#24863;&#20852;&#36259;&#21306;&#22495;(ROI)&#21644;&#21322;&#29699;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22810;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#19987;&#27880;&#20110;&#21463;&#35797;&#32773;&#22823;&#33041;&#27599;&#20010;&#21322;&#29699;&#30340;&#27599;&#20010;ROI&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an exploration of machine learning architectures for predicting brain responses to realistic images on occasion of the Algonauts Challenge 2023. Our research involved extensive experimentation with various pretrained models. Initially, we employed simpler models to predict brain activity but gradually introduced more complex architectures utilizing available data and embeddings generated by large-scale pre-trained models. We encountered typical difficulties related to machine learning problems, e.g. regularization and overfitting, as well as issues specific to the challenge, such as difficulty in combining multiple input encodings, as well as the high dimensionality, unclear structure, and noisy nature of the output. To overcome these issues we tested single edge 3D position-based, multi-region of interest (ROI) and hemisphere predictor models, but we found that employing multiple simple models, each dedicated to a ROI in each hemisphere of the brain of each subject, yielded
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064; (IDML) &#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#27169;&#31946;&#22270;&#20687;&#65292;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.09982</link><description>&lt;p&gt;
&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Introspective Deep Metric Learning. (arXiv:2309.09982v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064; (IDML) &#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#27169;&#31946;&#22270;&#20687;&#65292;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064; (IDML) &#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27604;&#36739;&#12290;&#20256;&#32479;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30528;&#37325;&#20110;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#21306;&#20998;&#24230;&#30340;&#23884;&#20837;&#26469;&#25551;&#36848;&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#30001;&#20110;&#22122;&#22768;&#25110;&#35821;&#20041;&#27169;&#31946;&#24615;&#32780;&#23548;&#33268;&#30340;&#27599;&#20010;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#23384;&#22312;&#12290;&#22312;&#27809;&#26377;&#24847;&#35782;&#21040;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#36807;&#24230;&#25311;&#21512;&#27880;&#37322;&#26631;&#31614;&#65292;&#24182;&#22312;&#25512;&#29702;&#26399;&#38388;&#20135;&#29983;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#21028;&#26029;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#22909;&#30340;&#30456;&#20284;&#24230;&#27169;&#22411;&#24212;&#32771;&#34385;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#27169;&#31946;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#20351;&#29992;&#35821;&#20041;&#23884;&#20837;&#65292;&#36824;&#20351;&#29992;&#20276;&#38543;&#30340;&#19981;&#30830;&#23450;&#24615;&#23884;&#20837;&#65292;&#20998;&#21035;&#25551;&#36848;&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#20869;&#30465;&#24335;&#27604;&#36739;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. Conventional deep metric learning methods focus on learning a discriminative embedding to describe the semantic features of images, which ignore the existence of uncertainty in each image resulting from noise or semantic ambiguity. Training without awareness of these uncertainties causes the model to overfit the annotated labels during training and produce unsatisfactory judgments during inference. Motivated by this, we argue that a good similarity model should consider the semantic discrepancies with awareness of the uncertainty to better deal with ambiguous images for more robust training. To achieve this, we propose to represent an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively. We further propose an introspective similarity metric to make
&lt;/p&gt;</description></item><item><title>Des-q&#26159;&#19968;&#31181;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#12290;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#26641;&#37325;&#26032;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#26032;&#26679;&#26412;&#30340;&#21152;&#36733;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807; k &#20998;&#27573;&#32447;&#24615;&#26641;&#20998;&#35010;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.09976</link><description>&lt;p&gt;
Des-q: &#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#30340;&#26500;&#24314;&#21644;&#39640;&#25928;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#30340;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification. (arXiv:2309.09976v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09976
&lt;/p&gt;
&lt;p&gt;
Des-q&#26159;&#19968;&#31181;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#12290;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#26641;&#37325;&#26032;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#26032;&#26679;&#26412;&#30340;&#21152;&#36733;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807; k &#20998;&#27573;&#32447;&#24615;&#26641;&#20998;&#35010;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#30001;&#20110;&#20854;&#31616;&#21333;&#26500;&#36896;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20256;&#32479;&#30340;&#20915;&#31574;&#26641;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#24930;&#65292;&#19982;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#21576;&#22810;&#39033;&#24335;&#35268;&#27169;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#31639;&#27861;Des-q&#65292;&#29992;&#20110;&#22312;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#12290;&#20551;&#35774;&#25968;&#25454;&#27969;&#20135;&#29983;&#36739;&#23567;&#30340;&#26032;&#35757;&#32451;&#26679;&#26412;&#22686;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;Des-q&#31639;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#26641;&#37325;&#26032;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#21363;&#20351;&#32771;&#34385;&#23558;&#26032;&#26679;&#26412;&#21152;&#36733;&#21040;&#37327;&#23376;&#21487;&#35775;&#38382;&#20869;&#23384;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#20854;&#26102;&#38388;&#22797;&#26434;&#24230;&#20063;&#36798;&#21040;&#20102;&#22810;&#23545;&#25968;&#32423;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#25191;&#34892;k&#20998;&#27573;&#32447;&#24615;&#26641;&#20998;&#35010;&#12290;&#36825;&#20123;&#20998;&#35010;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#36229;&#24179;&#38754;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are widely used in machine learning due to their simplicity in construction and interpretability. However, as data sizes grow, traditional methods for constructing and retraining decision trees become increasingly slow, scaling polynomially with the number of training examples. In this work, we introduce a novel quantum algorithm, named Des-q, for constructing and retraining decision trees in regression and binary classification tasks. Assuming the data stream produces small increments of new training examples, we demonstrate that our Des-q algorithm significantly reduces the time required for tree retraining, achieving a poly-logarithmic time complexity in the number of training examples, even accounting for the time needed to load the new examples into quantum-accessible memory. Our approach involves building a decision tree algorithm to perform k-piecewise linear tree splits at each internal node. These splits simultaneously generate multiple hyperplanes, dividing the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09924</link><description>&lt;p&gt;
&#22522;&#20110;&#28909;&#21644;&#27874;&#21160;&#21160;&#21147;&#23398;&#29305;&#24449;&#30340;&#22270;&#25299;&#25169;&#23646;&#24615;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Graph topological property recovery with heat and wave dynamics-based features on graphsD. (arXiv:2309.09924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#19978;&#30340;PDE&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#33719;&#24471;&#36830;&#32493;&#30340;&#33410;&#28857;&#21644;&#22270;&#32423;&#34920;&#31034;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#19982;&#22270;&#30340;&#35889;&#29305;&#24615;&#20197;&#21450;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#28216;&#36208;&#22312;&#22270;&#19978;&#34892;&#20026;&#20043;&#38388;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#38543;&#26426;&#22270;&#29983;&#25104;&#21442;&#25968;&#12289;Ricci&#26354;&#29575;&#21644;&#25345;&#20037;&#21516;&#35843;&#31561;&#26041;&#24335;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#33021;&#22815;&#25429;&#25417;&#21040;&#22270;&#24418;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#26174;&#33879;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;GDeNet&#22312;&#21253;&#25324;&#24341;&#29992;&#22270;&#12289;&#33647;&#29289;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#22312;&#20869;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Graph Differential Equation Network (GDeNet), an approach that harnesses the expressive power of solutions to PDEs on a graph to obtain continuous node- and graph-level representations for various downstream tasks. We derive theoretical results connecting the dynamics of heat and wave equations to the spectral properties of the graph and to the behavior of continuous-time random walks on graphs. We demonstrate experimentally that these dynamics are able to capture salient aspects of graph geometry and topology by recovering generating parameters of random graphs, Ricci curvature, and persistent homology. Furthermore, we demonstrate the superior performance of GDeNet on real-world datasets including citation graphs, drug-like molecules, and proteins.
&lt;/p&gt;</description></item><item><title>&#22312;&#32570;&#20047;&#25991;&#21270;&#20849;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26080;&#38480;&#28145;&#24230;&#28508;&#22312;&#32467;&#26500;&#30340;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;iDLC-CCT&#65289;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;CCT&#65289;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#23545;&#20849;&#35782;&#20449;&#24565;&#22810;&#26679;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.09787</link><description>&lt;p&gt;
&#22312;&#32570;&#20047;&#25991;&#21270;&#20849;&#35782;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#38598;&#20307;&#26234;&#24935;
&lt;/p&gt;
&lt;p&gt;
Harnessing Collective Intelligence Under a Lack of Cultural Consensus. (arXiv:2309.09787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09787
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#25991;&#21270;&#20849;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26080;&#38480;&#28145;&#24230;&#28508;&#22312;&#32467;&#26500;&#30340;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;iDLC-CCT&#65289;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;CCT&#65289;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#23545;&#20849;&#35782;&#20449;&#24565;&#22810;&#26679;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38598;&#20307;&#26234;&#24935;&#26469;&#25512;&#21160;&#26377;&#25928;&#30340;&#20915;&#31574;&#21644;&#21512;&#20316;&#21463;&#30410;&#20110;&#33021;&#22815;&#26816;&#27979;&#21644;&#25551;&#36848;&#20849;&#35782;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#12290;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;CCT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#25551;&#36848;&#36825;&#20123;&#19981;&#21516;&#30340;&#20849;&#35782;&#20449;&#24565;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#29616;&#20195;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#23545;&#30456;&#20284;&#20449;&#24565;&#30340;&#27010;&#25324;&#33021;&#21147;&#65292;&#23545;&#31232;&#30095;&#25968;&#25454;&#26080;&#25928;&#65292;&#24182;&#19988;&#26080;&#27861;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#25110;&#23398;&#20064;&#21040;&#30340;&#26426;&#22120;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#26080;&#38480;&#28145;&#24230;&#28508;&#22312;&#32467;&#26500;&#30340;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;iDLC-CCT&#65289;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#28508;&#22312;&#32467;&#26500;&#26469;&#25193;&#23637;CCT&#12290;&#35813;&#32467;&#26500;&#20801;&#35768;&#25105;&#20204;&#23558;&#25991;&#21270;&#20849;&#35782;&#30475;&#20316;&#26159;&#19968;&#31995;&#21015;&#26080;&#38480;&#28145;&#24230;&#30340;&#27010;&#24565;&#26500;&#24314;&#22359;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#20449;&#24565;&#22810;&#26679;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing collective intelligence to drive effective decision-making and collaboration benefits from the ability to detect and characterize heterogeneity in consensus beliefs. This is particularly true in domains such as technology acceptance or leadership perception, where a consensus defines an intersubjective truth, leading to the possibility of multiple "ground truths" when subsets of respondents sustain mutually incompatible consensuses. Cultural Consensus Theory (CCT) provides a statistical framework for detecting and characterizing these divergent consensus beliefs. However, it is unworkable in modern applications because it lacks the ability to generalize across even highly similar beliefs, is ineffective with sparse data, and can leverage neither external knowledge bases nor learned machine representations. Here, we overcome these limitations through Infinite Deep Latent Construct Cultural Consensus Theory (iDLC-CCT), a nonparametric Bayesian model that extends CCT with a lat
&lt;/p&gt;</description></item><item><title>FedGKD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#24449;&#24182;&#24341;&#20837;&#24863;&#30693;&#20840;&#23616;&#21327;&#20316;&#32467;&#26500;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09517</link><description>&lt;p&gt;
FedGKD:&#22312;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37322;&#25918;&#21327;&#20316;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks. (arXiv:2309.09517v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09517
&lt;/p&gt;
&lt;p&gt;
FedGKD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#24449;&#24182;&#24341;&#20837;&#24863;&#30693;&#20840;&#23616;&#21327;&#20316;&#32467;&#26500;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#30001;&#20110;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#33021;&#22815;&#22312;&#25968;&#25454;&#38548;&#31163;&#22330;&#26223;&#19979;&#25191;&#34892;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#32852;&#37030;&#35757;&#32451;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#30340;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#32479;&#35745;&#37327;&#26469;&#34920;&#31034;&#23616;&#37096;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#32858;&#21512;&#26426;&#21046;&#23558;&#23427;&#20204;&#32852;&#31995;&#36215;&#26469;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#25928;&#29575;&#26377;&#38480;&#65306;&#20219;&#21153;&#30456;&#20851;&#24615;&#37327;&#21270;&#30340;&#36136;&#37327;&#20302;&#21644;&#21033;&#29992;&#21327;&#20316;&#32467;&#26500;&#30340;&#26080;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedGKD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;GNN&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#22320;&#25551;&#36848;&#20219;&#21153;&#30456;&#20851;&#24615;&#30340;&#20219;&#21153;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#24863;&#30693;&#21040;&#20840;&#23616;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;FedGKD&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated training of Graph Neural Networks (GNN) has become popular in recent years due to its ability to perform graph-related tasks under data isolation scenarios while preserving data privacy. However, graph heterogeneity issues in federated GNN systems continue to pose challenges. Existing frameworks address the problem by representing local tasks using different statistics and relating them through a simple aggregation mechanism. However, these approaches suffer from limited efficiency from two aspects: low quality of task-relatedness quantification and inefficacy of exploiting the collaboration structure. To address these issues, we propose FedGKD, a novel federated GNN framework that utilizes a novel client-side graph dataset distillation method to extract task features that better describe task-relatedness, and introduces a novel server-side aggregation mechanism that is aware of the global collaboration structure. We conduct extensive experiments on six real-world datasets of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20197;&#22312;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#20986;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.09457</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#35745;&#31639;&#22797;&#26434;&#24615;&#26080;&#27861;&#35299;&#20915;&#30340;&#39044;&#35328;&#26426;&#65292;&#22312;&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles. (arXiv:2309.09457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20197;&#22312;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#20986;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#23398;&#20064;&#32773;&#21487;&#20197;&#35775;&#38382;&#24050;&#30693;&#30340;&#29305;&#24449;&#26144;&#23556;$ \phi&#65288;x&#65292;a&#65289;$&#65292;&#35813;&#26144;&#23556;&#23558;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26144;&#23556;&#21040;$d$&#32500;&#21521;&#37327;&#65292;&#24182;&#19988;&#22870;&#21169;&#21644;&#36716;&#25442;&#26159;&#27492;&#34920;&#31034;&#20013;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#20294;&#26159;&#36825;&#20123;&#29305;&#24449;&#20174;&#21738;&#37324;&#26469;&#65311;&#22312;&#27809;&#26377;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#31181;&#35825;&#20154;&#30340;&#31574;&#30053;&#26159;&#20351;&#29992;&#8220;&#21416;&#25151;&#27700;&#27133;&#8221;&#26041;&#27861;&#65292;&#24182;&#24076;&#26395;&#30495;&#23454;&#29305;&#24449;&#21253;&#21547;&#22312;&#19968;&#20010;&#26356;&#22823;&#30340;&#28508;&#22312;&#29305;&#24449;&#38598;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#32447;&#24615;MDP&#12290;&#22312;$k$-&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#65292;&#23384;&#22312;&#19968;&#20010;&#26410;&#30693;&#30340;&#22823;&#23567;&#20026;$k$&#30340;&#23376;&#38598;$S \subset [d]$&#65292;&#20854;&#20013;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#29305;&#24449;&#65292;&#30446;&#26631;&#26159;&#22312;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#20165;&#32463;&#36807;poly$(k,\log d)$&#27425;&#23398;&#20064;&#65292;&#23398;&#20064;&#20986;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#35201;&#20040;&#20570;&#20986;&#20102;&#26126;&#26174;&#30340;&#20551;&#35774;&#65292;&#20351;&#24471;&#25506;&#32034;&#26080;&#20851;&#32039;&#35201;&#65292;&#35201;&#20040;&#25552;&#20379;&#20102;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The key assumption underlying linear Markov Decision Processes (MDPs) is that the learner has access to a known feature map $\phi(x, a)$ that maps state-action pairs to $d$-dimensional vectors, and that the rewards and transitions are linear functions in this representation. But where do these features come from? In the absence of expert domain knowledge, a tempting strategy is to use the ``kitchen sink" approach and hope that the true features are included in a much larger set of potential features. In this paper we revisit linear MDPs from the perspective of feature selection. In a $k$-sparse linear MDP, there is an unknown subset $S \subset [d]$ of size $k$ containing all the relevant features, and the goal is to learn a near-optimal policy in only poly$(k,\log d)$ interactions with the environment. Our main result is the first polynomial-time algorithm for this problem. In contrast, earlier works either made prohibitively strong assumptions that obviated the need for exploration, o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21387;&#32553;&#25216;&#26415;&#65292;&#35299;&#20915;&#25345;&#32493;&#22270;&#23398;&#20064;&#20013;&#23384;&#20648;&#39044;&#31639;&#32039;&#24352;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.09455</link><description>&lt;p&gt;
CaT: &#24102;&#26377;&#22270;&#21387;&#32553;&#30340;&#24179;&#34913;&#25345;&#32493;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CaT: Balanced Continual Graph Learning with Graph Condensation. (arXiv:2309.09455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21387;&#32553;&#25216;&#26415;&#65292;&#35299;&#20915;&#25345;&#32493;&#22270;&#23398;&#20064;&#20013;&#23384;&#20648;&#39044;&#31639;&#32039;&#24352;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#30340;&#30446;&#26631;&#26159;&#20197;&#27969;&#24335;&#26041;&#24335;&#19981;&#26029;&#26356;&#26032;&#22270;&#27169;&#22411;&#12290;&#30001;&#20110;&#27169;&#22411;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#26102;&#23481;&#26131;&#24536;&#35760;&#20043;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#24050;&#25104;&#20026;CGL&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#65288;1&#65289;&#25972;&#20010;&#26032;&#25968;&#25454;&#21644;&#65288;2&#65289;&#23384;&#20648;&#22238;&#25918;&#22270;&#20197;&#36817;&#20284;&#21382;&#21490;&#25968;&#25454;&#20998;&#24067;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#35760;&#24518;&#24211;&#26469;&#26356;&#26032;&#27169;&#22411;&#65292;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26356;&#26032;&#27169;&#22411;&#21518;&#65292;&#20174;&#36755;&#20837;&#22270;&#20013;&#37319;&#26679;&#30340;&#26032;&#22238;&#25918;&#22270;&#23558;&#28155;&#21152;&#21040;&#29616;&#26377;&#30340;&#35760;&#24518;&#24211;&#20013;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#23545;CGL&#26469;&#35828;&#30452;&#35266;&#19988;&#26377;&#25928;&#65292;&#20294;&#26412;&#25991;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#32477;&#22823;&#22810;&#25968;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#22312;&#23384;&#20648;&#39044;&#31639;&#32039;&#24352;&#26102;&#38590;&#20197;&#23436;&#20840;&#25429;&#25417;&#21382;&#21490;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;&#22312;&#22797;&#26434;&#26032;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#29616;&#26377;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual graph learning (CGL) is purposed to continuously update a graph model with graph data being fed in a streaming manner. Since the model easily forgets previously learned knowledge when training with new-coming data, the catastrophic forgetting problem has been the major focus in CGL. Recent replay-based methods intend to solve this problem by updating the model using both (1) the entire new-coming data and (2) a sampling-based memory bank that stores replayed graphs to approximate the distribution of historical data. After updating the model, a new replayed graph sampled from the incoming graph will be added to the existing memory bank. Despite these methods are intuitive and effective for the CGL, two issues are identified in this paper. Firstly, most sampling-based methods struggle to fully capture the historical distribution when the storage budget is tight. Secondly, a significant data imbalance exists in terms of the scales of the complex new-coming graph data and the lig
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Wasserstein&#36317;&#31163;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#20445;&#35777;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#20048;&#38431;&#20013;&#23454;&#38469;&#29615;&#22659;&#19981;&#21305;&#37197;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08748</link><description>&lt;p&gt;
Wasserstein&#20998;&#24067;&#20445;&#35777;&#30340;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#22312;&#19978;&#19979;&#25991;&#20048;&#38431;&#20013;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits. (arXiv:2309.08748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08748
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Wasserstein&#36317;&#31163;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#20445;&#35777;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#20048;&#38431;&#20013;&#23454;&#38469;&#29615;&#22659;&#19981;&#21305;&#37197;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#19982;&#29615;&#22659;&#30452;&#25509;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#25910;&#38598;&#30340;&#29615;&#22659;&#36890;&#24120;&#19982;&#23398;&#20064;&#30340;&#31574;&#30053;&#24212;&#29992;&#30340;&#29615;&#22659;&#19981;&#21516;&#12290;&#20026;&#20102;&#22312;&#23398;&#20064;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#32771;&#34385;&#19981;&#21516;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#26032;&#22411;&#20998;&#24067;&#20445;&#35777;&#20248;&#21270;(DRO)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20551;&#35774;&#26032;&#29615;&#22659;&#30340;&#20998;&#24067;&#20301;&#20110;&#19981;&#30830;&#23450;&#38598;&#21512;&#20869;&#26102;&#65292;&#35745;&#31639;&#31574;&#30053;&#20540;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#30028;&#12290;&#20856;&#22411;&#22320;&#65292;&#36825;&#20010;&#19981;&#30830;&#23450;&#38598;&#21512;&#26159;&#22522;&#20110;&#20174;&#26085;&#24535;&#25968;&#25454;&#38598;&#20013;&#35745;&#31639;&#30340;&#32463;&#39564;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;KL&#19981;&#30830;&#23450;&#38598;&#21512;&#26080;&#27861;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#25903;&#25345;&#30340;&#20998;&#24067;&#65292;&#20063;&#32570;&#20047;&#23545;&#20998;&#24067;&#25903;&#25345;&#30340;&#20960;&#20309;&#24863;&#30693;&#12290;&#32467;&#26524;&#65292;KL&#26041;&#27861;&#22312;&#35299;&#20915;&#23454;&#38469;&#29615;&#22659;&#19981;&#21305;&#37197;&#21644;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#26368;&#22351;&#24773;&#20917;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#26032;&#22411;DRO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08600</link><description>&lt;p&gt;
&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#29702;&#35299;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#22810;&#20041;&#24615;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#22312;&#22810;&#20010;&#35821;&#20041;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#28608;&#27963;&#12290;&#22810;&#20041;&#24615;&#20351;&#25105;&#20204;&#26080;&#27861;&#25214;&#21040;&#31616;&#27905;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#24037;&#20316;&#12290;&#22810;&#20041;&#24615;&#30340;&#19968;&#20010;&#29468;&#27979;&#21407;&#22240;&#26159;&#21472;&#21152;&#25928;&#24212;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#37197;&#32473;&#28608;&#27963;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#36807;&#23436;&#22791;&#26041;&#21521;&#38598;&#21512;&#65292;&#32780;&#19981;&#26159;&#20010;&#21035;&#31070;&#32463;&#20803;&#65292;&#34920;&#31034;&#26356;&#22810;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#36825;&#20123;&#26041;&#21521;&#65292;&#20197;&#37325;&#26500;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#12290;&#36825;&#20123;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#19968;&#32452;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#27604;&#20854;&#20182;&#26041;&#27861;&#37492;&#23450;&#20986;&#30340;&#26041;&#21521;&#26356;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#65292;&#35299;&#37322;&#24615;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#34913;&#37327;&#30340;&#12290;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#20363;&#22914;&#36890;&#36807;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Ablating these features enables precise model editing, for example, by remo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24314;&#27169;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#23545;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#33616;&#25910;&#38598;&#39069;&#22806;&#30340;SPECT MPI&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08415</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;CRT&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification. (arXiv:2309.08415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24314;&#27169;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#23545;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#33616;&#25910;&#38598;&#39069;&#22806;&#30340;SPECT MPI&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#24739;&#32773;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#65288;CRT&#65289;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#25512;&#33616;&#22312;&#22522;&#32447;&#20020;&#24202;&#21464;&#37327;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#30340;&#29305;&#24449;&#19981;&#36275;&#26102;&#25910;&#38598;&#39069;&#22806;&#30340;&#21333;&#20809;&#23376;&#21457;&#23556;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#24515;&#32908;&#28748;&#27880;&#26174;&#20687;&#65288;SPECT MPI&#65289;&#21464;&#37327;&#12290;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#32435;&#20837;&#20102;218&#21517;&#25509;&#21463;&#38745;&#24687;&#38376;&#25511;SPECT MPI&#30340;&#24739;&#32773;&#12290;CRT&#21453;&#24212;&#34987;&#23450;&#20041;&#20026;6&#20010;&#26376;&#38543;&#35775;&#26102;&#24038;&#23460;&#23556;&#34880;&#20998;&#25968;&#65288;LVEF&#65289;&#22686;&#21152;&gt; 5%&#12290;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#38598;&#25104;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#32467;&#26524;&#12290;CRT&#30340;&#21453;&#24212;&#29575;&#20026;55.5%&#65288;n = 121&#65289;&#65292;&#25972;&#20307;&#30007;&#24615;&#21344;61.0%&#65288;n = 133&#65289;&#65292;&#24179;&#22343;&#24180;&#40836;62.0&#23681;&#65292;LVEF&#20026;27.7&#12290;&#35813;&#22810;&#38454;&#27573;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#38598;&#25104;&#27169;&#22411;2&#65288;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;SPECT&#25968;&#25454;&#65289;&#30456;&#20284;&#65292;AUC&#20998;&#21035;&#20026;0.75&#21644;0.77&#65292;&#20934;&#30830;&#24615;&#20998;&#21035;&#20026;0.71&#21644;...
&lt;/p&gt;
&lt;p&gt;
Aims. The purpose of this study is to create a multi-stage machine learning model to predict cardiac resynchronization therapy (CRT) response for heart failure (HF) patients. This model exploits uncertainty quantification to recommend additional collection of single-photon emission computed tomography myocardial perfusion imaging (SPECT MPI) variables if baseline clinical variables and features from electrocardiogram (ECG) are not sufficient. Methods. 218 patients who underwent rest-gated SPECT MPI were enrolled in this study. CRT response was defined as an increase in left ventricular ejection fraction (LVEF) &gt; 5% at a 6 month follow-up. A multi-stage ML model was created by combining two ensemble models. Results. The response rate for CRT was 55.5% (n = 121) with overall male gender 61.0% (n = 133), an average age of 62.0, and LVEF of 27.7. The multi-stage model performed similarly to Ensemble 2 (which utilized the additional SPECT data) with AUC of 0.75 vs. 0.77, accuracy of 0.71 vs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.07315</link><description>&lt;p&gt;
&#26053;&#34892;&#35789;&#65306;&#19968;&#31181;&#21464;&#21387;&#22120;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29702;&#35299;&#20854;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35828;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#23558;&#28508;&#22312;&#29305;&#24449;&#38480;&#21046;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#27880;&#24847;&#21147;&#33021;&#22815;&#22312;&#35813;&#34920;&#38754;&#19978;&#22609;&#36896;&#21333;&#35789;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#36825;&#31181;&#20960;&#20309;&#35270;&#28857;&#26080;&#32541;&#22320;&#36830;&#25509;&#20102;&#36845;&#20195;&#25913;&#36827;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#31561;&#24050;&#30693;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;124M&#21442;&#25968;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#26089;&#26399;&#23618;&#20013;&#28165;&#26224;&#30340;&#26597;&#35810;-&#38190;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#22312;&#26356;&#28145;&#30340;&#23618;&#27425;&#19978;&#24314;&#31435;&#22312;&#20808;&#21069;&#20851;&#20110;&#27880;&#24847;&#22836;&#30340;&#19987;&#38376;&#24615;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#12290;&#21033;&#29992;&#36825;&#20123;&#20960;&#20309;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#65292;&#23558;&#20854;&#25551;&#32472;&#20026;&#22609;&#36896;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;O-RAN&#32593;&#32476;&#20013;&#20351;&#29992;DRL&#31639;&#27861;&#36827;&#34892;&#38381;&#29615;&#25511;&#21046;&#26102;&#36935;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07265</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#21152;&#36895;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;O-RAN&#20999;&#29255;: &#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;O-RAN&#32593;&#32476;&#20013;&#20351;&#29992;DRL&#31639;&#27861;&#36827;&#34892;&#38381;&#29615;&#25511;&#21046;&#26102;&#36935;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#26550;&#26500;&#25903;&#25345;&#26234;&#33021;&#32593;&#32476;&#25511;&#21046;&#31639;&#27861;&#20316;&#20026;&#20854;&#26680;&#24515;&#33021;&#21147;&#20043;&#19968;&#12290;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36890;&#36807;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#26234;&#33021;&#25511;&#21046;&#22120;&#65288;RIC&#65289;&#26469;&#20248;&#21270;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;RAN&#65289;&#21151;&#33021;&#12290;&#22312;O-RAN&#25991;&#29486;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26159;&#35299;&#20915;&#21160;&#24577;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;O-RAN RIC&#24341;&#20837;&#20102;&#35832;&#22810;&#22909;&#22788;&#65292;&#20294;&#22312;&#30495;&#23454;&#32593;&#32476;&#37096;&#32626;&#20013;&#65292;DRL&#31639;&#27861;&#30340;&#23454;&#38469;&#37319;&#29992;&#21364;&#33853;&#21518;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;DRL&#20195;&#29702;&#22312;&#37096;&#32626;&#21644;&#38754;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#32593;&#32476;&#26465;&#20214;&#26102;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#20316;&#20026;O-RAN&#21151;&#33021;&#30340;DRL&#22522;&#20110;&#38381;&#29615;&#25511;&#21046;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#27969;&#31243;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;TL&#36741;&#21161;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The open radio access network (O-RAN) architecture supports intelligent network control algorithms as one of its core capabilities. Data-driven applications incorporate such algorithms to optimize radio access network (RAN) functions via RAN intelligent controllers (RICs). Deep reinforcement learning (DRL) algorithms are among the main approaches adopted in the O-RAN literature to solve dynamic radio resource management problems. However, despite the benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms in real network deployments falls behind. This is primarily due to the slow convergence and unstable performance exhibited by DRL agents upon deployment and when facing previously unseen network conditions. In this paper, we address these challenges by proposing transfer learning (TL) as a core component of the training and deployment workflows for the DRL-based closed-loop control of O-RAN functionalities. To this end, we propose and design a hybrid TL-aided a
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06212</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06212
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20892;&#19994;&#23454;&#36341;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#23545;&#20110;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#23588;&#20854;&#23545;&#20110;&#38271;&#26399;&#20915;&#31574;&#65292;&#25552;&#21069;&#19968;&#24180;&#36827;&#34892;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#21450;&#20854;&#30456;&#37051;&#21306;&#22495;&#20869;&#21508;&#31181;&#22240;&#32032;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#39044;&#27979;&#36825;&#19968;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21508;&#31181;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25152;&#32771;&#34385;&#30340;&#27169;&#22411;&#20027;&#35201;&#26159;&#26681;&#25454;Palmer&#24178;&#26097;&#20005;&#37325;&#25351;&#25968;&#65288;PDSI&#65289;&#39044;&#27979;&#24863;&#20852;&#36259;&#20122;&#21306;&#30340;&#24178;&#26097;&#24378;&#24230;&#65292;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#30340;&#20869;&#22312;&#22240;&#32032;&#21644;&#35265;&#35299;&#26469;&#25552;&#39640;&#24178;&#26097;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#27604;&#36739;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26799;&#24230;&#25552;&#21319;&#21644;&#36923;&#36753;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#21367;&#31215;LSTM&#65288;ConvLSTM&#65289;&#21644;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#21069;&#20004;&#31181;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;ROC AUC&#20998;&#25968;&#65292;&#39640;&#36798;0.90
&lt;/p&gt;
&lt;p&gt;
The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25919;&#27835;&#20559;&#35265;&#39044;&#27979;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20998;&#31867;&#25919;&#27835;&#20542;&#21521;&#65292;&#24182;&#36827;&#34892;&#20102;&#23545;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#39044;&#27979;&#25919;&#27835;&#20542;&#21521;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.05589</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#25919;&#27835;&#20559;&#35265;&#26041;&#38754;&#23545;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias. (arXiv:2309.05589v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25919;&#27835;&#20559;&#35265;&#39044;&#27979;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20998;&#31867;&#25919;&#27835;&#20542;&#21521;&#65292;&#24182;&#36827;&#34892;&#20102;&#23545;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#39044;&#27979;&#25919;&#27835;&#20542;&#21521;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#23569;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#26159;&#23545;&#25239;&#38169;&#35823;&#20449;&#24687;&#21644;&#22238;&#38899;&#23460;&#25928;&#24212;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#35745;&#31639;&#26041;&#27861;&#23545;&#25919;&#27835;&#20559;&#35265;&#36827;&#34892;&#26102;&#38388;&#21270;&#25551;&#36848;&#20250;&#38754;&#20020;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#22122;&#22768;&#39057;&#32321;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25551;&#36848;&#25919;&#27835;&#20559;&#35265;&#65292;&#20294;&#23545;&#20110;&#39044;&#27979;&#25919;&#27835;&#20559;&#35265;&#21644;&#39044;&#27979;&#25919;&#27835;&#23545;&#35805;&#22312;&#19981;&#36828;&#30340;&#23558;&#26469;&#22914;&#20309;&#21457;&#23637;&#30340;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20998;&#31867;&#20026;&#20116;&#31181;&#19981;&#21516;&#30340;&#25919;&#27835;&#20542;&#21521;&#31867;&#21035;&#12290;&#30001;&#20110;&#32570;&#20047;&#20851;&#20110;&#39044;&#27979;&#25919;&#27835;&#20559;&#35265;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#21738;&#31181;&#27169;&#22411;&#26368;&#36866;&#21512;&#39044;&#27979;&#25919;&#27835;&#20542;&#21521;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22312;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#25919;&#27835;&#35266;&#28857;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#19978;&#21033;&#29992;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and mitigating political bias in online social media platforms are crucial tasks to combat misinformation and echo chamber effects. However, characterizing political bias temporally using computational methods presents challenges due to the high frequency of noise in social media datasets. While existing research has explored various approaches to political bias characterization, the ability to forecast political bias and anticipate how political conversations might evolve in the near future has not been extensively studied. In this paper, we propose a heuristic approach to classify social media posts into five distinct political leaning categories. Since there is a lack of prior work on forecasting political bias, we conduct an in-depth analysis of existing baseline models to identify which model best fits to forecast political leaning time series. Our approach involves utilizing existing time series forecasting models on two social media datasets with different politica
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;6G&#29615;&#22659;&#20013;&#25903;&#25345;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#20449;&#26550;&#26500;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#32858;&#21512;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.05525</link><description>&lt;p&gt;
6G&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#21457;&#23637;&#65306;&#22522;&#20110;&#22270;&#20998;&#26512;&#30340;&#21487;&#20449;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis. (arXiv:2309.05525v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05525
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;6G&#29615;&#22659;&#20013;&#25903;&#25345;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#20449;&#26550;&#26500;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#32858;&#21512;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21407;&#29983;AI&#25903;&#25345;&#38598;&#25104;&#21040;&#32593;&#32476;&#26550;&#26500;&#20013;&#26159;6G&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#33539;&#24335;&#20986;&#29616;&#65292;&#21487;&#20197;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#65292;&#20419;&#36827;&#20998;&#25955;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#36328;&#36234;&#22810;&#31181;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#22312;6G&#29615;&#22659;&#19979;&#65292;&#26377;&#20960;&#20010;&#25361;&#25112;&#38459;&#30861;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;&#24694;&#24847;&#25915;&#20987;&#21644;&#23545;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#30417;&#35270;&#65292;&#20197;&#21450;&#38598;&#20013;&#21270;&#30340;&#32570;&#28857;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;FL&#30340;&#21487;&#20449;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21033;&#29992;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#65288;DLT&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#29305;&#24615;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#21516;&#24577;&#21152;&#23494;&#30340;&#39044;&#22788;&#29702;&#23618;&#29992;&#20110;&#23433;&#20840;&#22320;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#20445;&#25252;&#20010;&#20307;&#27169;&#22411;&#30340;&#38544;&#31169;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#39044;&#22788;&#29702;&#23618;&#20013;&#23458;&#25143;&#31471;&#21644;&#33410;&#28857;&#20043;&#38388;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#22270;&#32467;&#26500;&#65292;&#21033;&#29992;GNN&#26469;&#35782;&#21035;&#24322;&#24120;&#26412;&#22320;&#27169;&#22411;&#65292;&#22686;&#24378;&#31995;&#32479;&#23433;&#20840;&#24615;&#12290;&#31532;&#19977;&#65292;&#21033;&#29992;DLT&#26469;&#32500;&#25252;&#32593;&#32476;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#23433;&#20840;&#30340;&#20849;&#35782;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating native AI support into the network architecture is an essential objective of 6G. Federated Learning (FL) emerges as a potential paradigm, facilitating decentralized AI model training across a diverse range of devices under the coordination of a central server. However, several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls. This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology (DLT) and Graph Neural Network (GNN), including three key features. First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models, preserving the privacy of individual models. Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer, GNN is leveraged to identify abnormal local models, enhancing system security. Third, DLT is utilized to d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31034;&#25945;&#23398;&#30340;&#31034;&#25945;&#23398;&#20064;&#30340;&#26367;&#20195;&#33539;&#24335;&#65292;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#22312;&#22330;&#26223;&#30340;&#20108;&#32500;&#22270;&#20687;&#19978;&#21246;&#21202;&#31034;&#33539;&#36712;&#36857;&#26469;&#25945;&#26426;&#22120;&#20154;&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#21512;&#25104;&#20026;&#19977;&#32500;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03835</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#24615;&#22270;&#31034;&#25945;&#23398;&#36827;&#34892;&#31034;&#25945;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration via Probabilistic Diagrammatic Teaching. (arXiv:2309.03835v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31034;&#25945;&#23398;&#30340;&#31034;&#25945;&#23398;&#20064;&#30340;&#26367;&#20195;&#33539;&#24335;&#65292;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#22312;&#22330;&#26223;&#30340;&#20108;&#32500;&#22270;&#20687;&#19978;&#21246;&#21202;&#31034;&#33539;&#36712;&#36857;&#26469;&#25945;&#26426;&#22120;&#20154;&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#21512;&#25104;&#20026;&#19977;&#32500;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31034;&#25945;&#23398;&#20064;&#65288;Learning for Demonstration&#65292;LfD&#65289;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#31034;&#33539;&#26469;&#33719;&#24471;&#26032;&#25216;&#33021;&#65292;&#20801;&#35768;&#29992;&#25143;&#20197;&#30452;&#35266;&#30340;&#26041;&#24335;&#20256;&#36798;&#20182;&#20204;&#30340;&#25351;&#31034;&#12290;&#26368;&#36817;&#22312;LfD&#39046;&#22495;&#30340;&#36827;&#23637;&#24448;&#24448;&#20381;&#36182;&#20110;&#21160;&#20316;&#31034;&#33539;&#25945;&#23398;&#25110;&#36828;&#31243;&#25805;&#20316;&#20316;&#20026;&#29992;&#25143;&#25351;&#23450;&#31034;&#33539;&#30340;&#25163;&#27573;&#12290;&#21160;&#20316;&#31034;&#33539;&#25945;&#23398;&#38656;&#35201;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#29289;&#29702;&#25805;&#32437;&#65292;&#32780;&#36828;&#31243;&#25805;&#20316;&#21017;&#38656;&#35201;&#29087;&#32451;&#25484;&#25569;&#39069;&#22806;&#30340;&#30828;&#20214;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31034;&#25945;&#23398;&#30340;LfD&#30340;&#26367;&#20195;&#33539;&#24335;&#12290;&#22270;&#31034;&#25945;&#23398;&#26088;&#22312;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#22312;&#22330;&#26223;&#30340;&#20108;&#32500;&#22270;&#20687;&#19978;&#21246;&#21202;&#31034;&#33539;&#36712;&#36857;&#26469;&#25945;&#26426;&#22120;&#20154;&#26032;&#30340;&#25216;&#33021;&#65292;&#28982;&#21518;&#36825;&#20123;&#36712;&#36857;&#23558;&#34987;&#21512;&#25104;&#20026;&#19977;&#32500;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#22270;&#31034;&#25945;&#23398;&#30340;&#23556;&#32447;&#36861;&#36394;&#27010;&#29575;&#36712;&#36857;&#23398;&#20064;&#65288;RPTL&#65289;&#26694;&#26550;&#12290;RPTL&#20174;&#20108;&#32500;&#22270;&#31034;&#20013;&#25552;&#21462;&#26102;&#38388;&#21464;&#21270;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#24182;&#24212;&#29992;&#23556;&#32447;&#36861;&#36394;&#26469;&#23547;&#25214;&#30456;&#24212;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning for Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions in an intuitive manner. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, applies ray-tracing to find corresponding regions i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35745;&#31639;&#25928;&#29575;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36879;&#26126;&#21644;&#39640;&#35745;&#31639;&#24320;&#38144;&#31561;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.03694</link><description>&lt;p&gt;
&#20351;&#29992;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#22686;&#24378;&#30340;CNN-LSTM&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network. (arXiv:2309.03694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03694
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35745;&#31639;&#25928;&#29575;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36879;&#26126;&#21644;&#39640;&#35745;&#31639;&#24320;&#38144;&#31561;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#23545;&#20110;&#30005;&#21147;&#31995;&#32479;&#30340;&#39640;&#25928;&#36816;&#34892;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#65292;&#32473;&#23450;&#20854;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36879;&#26126;&#21644;&#23454;&#26102;&#37096;&#32626;&#30340;&#39640;&#35745;&#31639;&#24320;&#38144;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#33258;&#20027;&#22320;&#25506;&#32034;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#35782;&#21035;&#23545;&#20934;&#30830;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#26174;&#33879;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#31616;&#21270;&#30340;&#26694;&#26550;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30495;&#23454;&#30005;&#21147;&#38656;&#27714;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;...
&lt;/p&gt;
&lt;p&gt;
Short-term load forecasting is of paramount importance in the efficient operation and planning of power systems, given its inherent non-linear and dynamic nature. Recent strides in deep learning have shown promise in addressing this challenge. However, these methods often grapple with hyperparameter sensitivity, opaqueness in interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that surmounts these obstacles. Our approach harnesses the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to discern the salient features crucial for accurate forecasting, and a streamlined framework for computational efficiency. Our method undergoes rigorous evaluation using a genuine electricity demand dataset. The results underscore its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error o
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03648</link><description>&lt;p&gt;
GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;Lipschitz&#29305;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03648
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#30028;&#38480;&#26159;&#20174;&#40065;&#26834;&#32479;&#35745;&#23398;&#20013;&#20511;&#37492;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#38480;&#21046;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#26368;&#22823;&#21464;&#21270;&#65292;&#32771;&#34385;&#21040;&#30456;&#20851;&#30340;&#38750;&#20851;&#38190;&#20559;&#20506;&#22240;&#32032;&#12290;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#26597;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#31283;&#23450;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25805;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;GNN&#30340;Lipschitz&#30028;&#38480;&#20197;&#25581;&#31034;&#27169;&#22411;&#36755;&#20986;&#30340;&#31283;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#20559;&#20506;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#12290;&#30001;&#20110;&#24120;&#35265;&#22270;&#24418;&#25968;&#25454;&#22312;GNN&#35757;&#32451;&#20013;&#23384;&#22312;&#22266;&#26377;&#20559;&#24046;&#65292;&#36825;&#32473;&#38480;&#21046;&#30001;&#36755;&#20837;&#20559;&#24046;&#24341;&#36215;&#30340;GNN&#36755;&#20986;&#25200;&#21160;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#38556;&#20844;&#24179;&#24615;&#65292;&#24102;&#26469;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23613;&#31649;Lipschitz&#24120;&#25968;&#22312;&#25511;&#21046;&#27431;&#20960;&#37324;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#26377;&#25152;&#24212;&#29992;&#65292;&#20294;&#31934;&#30830;Lipschitz&#24120;&#25968;&#30340;&#35745;&#31639;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28388;&#27874;/&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.03557</link><description>&lt;p&gt;
&#35770;&#22810;&#26234;&#33021;&#20307;&#38750;&#32447;&#24615;&#28388;&#27874;&#21644;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
On the dynamics of multi agent nonlinear filtering and learning. (arXiv:2309.03557v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28388;&#27874;/&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#36890;&#36807;&#20998;&#25955;&#19968;&#33268;&#24615;&#23547;&#27714;&#21160;&#21147;&#23398;&#26469;&#23436;&#25104;&#39640;&#24230;&#22797;&#26434;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#35745;&#31639;&#26234;&#33021;&#31038;&#21306;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28388;&#27874;/&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#30340;&#19968;&#33324;&#34920;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#29616;&#21327;&#21516;&#23398;&#20064;&#34892;&#20026;&#30340;&#26465;&#20214;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36824;&#20171;&#32461;&#20102;&#35813;&#25512;&#23548;&#26694;&#26550;&#22312;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiagent systems aim to accomplish highly complex learning tasks through decentralised consensus seeking dynamics and their use has garnered a great deal of attention in the signal processing and computational intelligence societies. This article examines the behaviour of multiagent networked systems with nonlinear filtering/learning dynamics. To this end, a general formulation for the actions of an agent in multiagent networked systems is presented and conditions for achieving a cohesive learning behaviour is given. Importantly, application of the so derived framework in distributed and federated learning scenarios are presented.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;</title><link>http://arxiv.org/abs/2308.16375</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#35843;&#26597;&#65306;&#25915;&#20987;&#12289;&#20445;&#25252;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16375
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#25913;&#21892;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#39640;&#25928;&#33021;&#34920;&#29616;&#65292;&#22914;&#20934;&#30830;&#24615;&#65292;&#32780;&#32570;&#20047;&#38544;&#31169;&#32771;&#34385;&#65292;&#36825;&#26159;&#29616;&#20195;&#31038;&#20250;&#38544;&#31169;&#25915;&#20987;&#30427;&#34892;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;GNNs&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22270;&#39046;&#22495;&#32570;&#20047;&#23545;&#25915;&#20987;&#21644;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24635;&#32467;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#25915;&#20987;&#12289;&#23545;GNNs&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#23457;&#26597;&#21487;&#29992;&#20110;&#20998;&#26512;/&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#31243;&#24207;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#20197;&#24314;&#31435;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.15126</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35780;&#20272;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LVLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#24187;&#35273;&#25351;&#30340;&#26159;LVLMs&#21709;&#24212;&#20013;&#19981;&#23384;&#22312;&#20110;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#21518;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#23545;LVLMs&#20013;&#30340;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#24037;&#20316;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#12290;HaELM&#30340;&#24615;&#33021;&#36817;&#20284;&#20110;ChatGPT&#30340;95%&#65292;&#24182;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#21487;&#22797;&#29616;&#12289;&#20445;&#25252;&#38544;&#31169;&#21644;&#26412;&#22320;&#37096;&#32626;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#21033;&#29992;HaELM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LVLMs&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;LVLMs&#20013;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#26377;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
&lt;/p&gt;</description></item><item><title>BayOTIDE&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#20302;&#31209;&#26102;&#24207;&#22240;&#23376;&#32452;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#36827;&#34892;&#25554;&#34917;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20840;&#23616;&#36235;&#21183;&#21644;&#21608;&#26399;&#24615;&#27169;&#24335;&#30340;&#24573;&#30053;&#20197;&#21450;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14906</link><description>&lt;p&gt;
BayOTIDE: &#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. (arXiv:2308.14906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14906
&lt;/p&gt;
&lt;p&gt;
BayOTIDE&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#20302;&#31209;&#26102;&#24207;&#22240;&#23376;&#32452;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#36827;&#34892;&#25554;&#34917;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20840;&#23616;&#36235;&#21183;&#21644;&#21608;&#26399;&#24615;&#27169;&#24335;&#30340;&#24573;&#30053;&#20197;&#21450;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#22914;&#20132;&#36890;&#21644;&#33021;&#28304;&#65292;&#32463;&#24120;&#35266;&#23519;&#21040;&#20855;&#26377;&#32570;&#22833;&#20540;&#21644;&#22122;&#22768;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#29978;&#33267;&#26159;&#19981;&#35268;&#21017;&#37319;&#26679;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25554;&#34917;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#25968;&#21482;&#36866;&#29992;&#20110;&#23616;&#37096;&#35270;&#35282;&#65292;&#21363;&#23558;&#38271;&#24207;&#21015;&#25286;&#20998;&#20026;&#36866;&#24403;&#22823;&#23567;&#30340;&#25209;&#27425;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#23616;&#37096;&#35270;&#35282;&#21487;&#33021;&#20351;&#27169;&#22411;&#24573;&#30053;&#20840;&#23616;&#36235;&#21183;&#25110;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20960;&#20046;&#25152;&#26377;&#26041;&#27861;&#37117;&#20551;&#35774;&#35266;&#27979;&#20540;&#22312;&#35268;&#21017;&#30340;&#26102;&#38388;&#38388;&#38548;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#19988;&#26080;&#27861;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#30340;&#22797;&#26434;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22312;&#31163;&#32447;&#29366;&#24577;&#19979;&#36827;&#34892;&#23398;&#20064;&#30340;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#37027;&#20123;&#26377;&#24555;&#36895;&#21040;&#36798;&#30340;&#27969;&#25968;&#25454;&#30340;&#24212;&#29992;&#26469;&#35828;&#65292;&#23427;&#20204;&#24182;&#19981;&#21512;&#36866;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BayOTIDE&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios like traffic and energy, massive time-series data with missing values and noises are widely observed, even sampled irregularly. While many imputation methods have been proposed, most of them work with a local horizon, which means models are trained by splitting the long sequence into batches of fit-sized patches. This local horizon can make models ignore global trends or periodic patterns. More importantly, almost all methods assume the observations are sampled at regular time stamps, and fail to handle complex irregular sampled time series arising from different applications. Thirdly, most existing methods are learned in an offline manner. Thus, it is not suitable for many applications with fast-arriving streaming data. To overcome these limitations, we propose \ours: Bayesian Online Multivariate Time series Imputation with functional decomposition. We treat the multivariate time series as the weighted combination of groups of low-rank temporal factors with dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extractor&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65292;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;Extractor&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.07661</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19981;&#20877;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#19996;&#35199;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention Is Not All You Need Anymore. (arXiv:2308.07661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extractor&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65292;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;Extractor&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#27969;&#34892;&#30340;Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#24615;&#33021;&#24179;&#34913;&#26469;&#20943;&#23569;Transformer&#20013;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#23545;&#20110;Transformer&#30340;&#25345;&#32493;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65288;Extractor&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Extractor&#26367;&#25442;&#33258;&#27880;&#24847;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;Extractor&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#65292;&#22240;&#27492;&#26377;&#28508;&#21147;&#27604;&#33258;&#27880;&#24847;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20351;&#29992;&#21487;&#21464;&#38271;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#23545;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#38024;&#23545;&#25105;&#20204;&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#23545;Transformer&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our unders
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#21512;&#25104;&#35760;&#24405;&#65292;&#24182;&#20445;&#25345;&#20102;&#22270;&#20687;&#21644;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.07573</link><description>&lt;p&gt;
&#20351;&#29992;&#20004;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23545;&#28151;&#21512;&#22270;&#20687;-&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks. (arXiv:2308.07573v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#21512;&#25104;&#35760;&#24405;&#65292;&#24182;&#20445;&#25345;&#20102;&#22270;&#20687;&#21644;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#35760;&#24405;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#24212;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#38544;&#31169;&#38382;&#39064;&#24182;&#20419;&#36827;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;GAN&#65288;&#945;GAN&#65289;&#21644;&#26465;&#20214;&#34920;&#26684;GAN&#65288;CTGAN&#65289;&#29983;&#25104;&#21512;&#25104;&#30340;&#28151;&#21512;&#21307;&#23398;&#35760;&#24405;&#65292;&#20854;&#20013;&#21253;&#25324;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65288;CXRs&#65289;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#65288;&#21253;&#25324;&#20154;&#20307;&#27979;&#37327;&#25968;&#25454;&#21644;&#23454;&#39564;&#23460;&#27979;&#35797;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#19968;&#20010;&#22823;&#30340;&#20844;&#20849;&#25968;&#25454;&#24211;&#65288;pDB&#65289;&#19978;&#35757;&#32451;&#19968;&#20010;&#945;GAN&#27169;&#22411;&#65292;&#20197;&#38477;&#20302;CXRs&#30340;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;GAN&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#21407;&#22987;&#25968;&#25454;&#24211;&#65288;oDB&#65289;&#20013;&#30340;&#22270;&#20687;&#65292;&#20197;&#33719;&#24471;&#28508;&#22312;&#21521;&#37327;&#12290;&#36825;&#20123;&#28508;&#22312;&#21521;&#37327;&#19982;oDB&#20013;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#32852;&#21512;&#25968;&#25454;&#26469;&#35757;&#32451;CTGAN&#27169;&#22411;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#22810;&#26679;&#21270;&#30340;&#28151;&#21512;CXRs&#21644;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#35760;&#24405;&#65292;&#24182;&#20445;&#25345;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of synthetic medical records using generative adversarial networks (GANs) has become increasingly important for addressing privacy concerns and promoting data sharing in the medical field. In this paper, we propose a novel method for generating synthetic hybrid medical records consisting of chest X-ray images (CXRs) and structured tabular data (including anthropometric data and laboratory tests) using an auto-encoding GAN ({\alpha}GAN) and a conditional tabular GAN (CTGAN). Our approach involves training a {\alpha}GAN model on a large public database (pDB) to reduce the dimensionality of CXRs. We then applied the trained encoder of the GAN model to the images in original database (oDB) to obtain the latent vectors. These latent vectors were combined with tabular data in oDB, and these joint data were used to train the CTGAN model. We successfully generated diverse synthetic records of hybrid CXR and tabular data, maintaining correspondence between them. We evaluated this
&lt;/p&gt;</description></item><item><title>VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.02117</link><description>&lt;p&gt;
VQGraph: &#22270;&#24418;&#21521;&#37327;&#37327;&#21270;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs
&lt;/p&gt;
&lt;p&gt;
VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02117
&lt;/p&gt;
&lt;p&gt;
VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#20449;&#24687;&#20256;&#36882;&#65292;&#32858;&#21512;&#23616;&#37096;&#37051;&#23621;&#20197;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#20449;&#24687;&#20256;&#36882;&#23548;&#33268;&#22312;&#23454;&#38469;&#30340;&#24310;&#36831;&#32422;&#26463;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#27169;&#20223;GNN&#30340;&#36755;&#20986;&#26469;&#23398;&#20064;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#34920;&#31034;&#31354;&#38388;&#21487;&#33021;&#19981;&#36275;&#20197;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#65292;&#36825;&#38480;&#21046;&#20102;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;VQGraph&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#21464;&#20307;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#23427;&#23558;&#22810;&#26679;&#21270;&#30340;&#23616;&#37096;&#32467;&#26500;&#33410;&#28857;&#26126;&#30830;&#34920;&#31034;&#20026;&#22823;&#37327;&#31163;&#25955;&#20196;&#29260;&#65292;&#24182;&#26500;&#25104;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#20195;&#30721;&#20070;&#12290;&#37197;&#22791;&#20102;&#23398;&#20064;&#30340;&#20195;&#30721;&#20070;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propos
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#12290;&#20027;&#35201;&#20851;&#27880;&#30340;&#20004;&#20010;GAMs&#26159;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#25351;&#26631;&#33021;&#22815;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#35780;&#20272;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10112</link><description>&lt;p&gt;
&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Extended Graph Assessment Metrics for Graph Neural Networks. (arXiv:2307.10112v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#12290;&#20027;&#35201;&#20851;&#27880;&#30340;&#20004;&#20010;GAMs&#26159;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#25351;&#26631;&#33021;&#22815;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#35780;&#20272;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#24739;&#32773;&#38431;&#21015;&#37325;&#32452;&#20026;&#25152;&#35859;&#30340;&#20154;&#21475;&#22270;&#26102;&#65292;&#26368;&#21021;&#29420;&#31435;&#30340;&#25968;&#25454;&#28857;&#21487;&#20197;&#21512;&#24182;&#25104;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#22270;&#32467;&#26500;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#21487;&#20197;&#20351;&#29992;&#36825;&#31181;&#20154;&#21475;&#22270;&#36827;&#34892;&#21307;&#23398;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36866;&#21512;&#30340;&#22270;&#32467;&#26500;&#30340;&#26500;&#24314;&#26159;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27493;&#39588;&#65292;&#23427;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#22270;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#26631;&#20165;&#36866;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#21644;&#31163;&#25955;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#21482;&#35206;&#30422;&#20102;&#19968;&#23567;&#37096;&#20998;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#30340;&#25193;&#23637;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20004;&#20010;&#20855;&#20307;&#30340;GAMs&#65306;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#25105;&#20204;&#23558;GAMs&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#20010;&#36339;&#36291;&#65292;&#24182;&#20026;&#22238;&#24402;&#20219;&#21153;&#23450;&#20041;&#20102;&#21516;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When re-structuring patient cohorts into so-called population graphs, initially independent data points can be incorporated into one interconnected graph structure. This population graph can then be used for medical downstream tasks using graph neural networks (GNNs). The construction of a suitable graph structure is a challenging step in the learning pipeline that can have severe impact on model performance. To this end, different graph assessment metrics have been introduced to evaluate graph structures. However, these metrics are limited to classification tasks and discrete adjacency matrices, only covering a small subset of real-world applications. In this work, we introduce extended graph assessment metrics (GAMs) for regression tasks and continuous adjacency matrices. We focus on two GAMs in specific: \textit{homophily} and \textit{cross-class neighbourhood similarity} (CCNS). We extend the notion of GAMs to more than one hop, define homophily for regression tasks, as well as con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07686</link><description>&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#25903;&#25345;OpenMP Fortran&#21644;C++&#20195;&#30721;&#30456;&#20114;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#65288;CodeBLEU&#65289;&#21644;&#23450;&#24615;&#65288;&#20154;&#24037;&#35780;&#20272;&#65289;&#26041;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#32534;&#30721;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;5.1&#20493;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#23450;&#32534;&#30721;&#29087;&#24713;&#24230;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;9.9&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#20195;&#30721;&#32763;&#35793;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#32925;&#30828;&#21270;&#39044;&#27979;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#25972;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.04617</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#65306;&#32925;&#30828;&#21270;&#20998;&#31867;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#32925;&#30828;&#21270;&#39044;&#27979;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#25972;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#36890;&#36807;&#20302;&#32622;&#20449;&#24230;&#30340;&#24369;&#26631;&#31614;&#65288;&#20363;&#22914;&#25918;&#23556;&#23398;&#35780;&#20998;&#65289;&#36827;&#34892;&#24265;&#20215;&#24555;&#36895;&#30340;&#27880;&#37322;&#12290;&#32780;&#39640;&#32622;&#20449;&#24230;&#30340;&#26631;&#31614;&#65288;&#22914;&#22522;&#20110;&#32452;&#32455;&#23398;&#30340;&#35786;&#26029;&#65289;&#24456;&#23569;&#19988;&#26114;&#36149;&#12290;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36739;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65292;&#36825;&#22312;&#22823;&#22411;3D&#22270;&#20687;&#30340;&#20840;&#20998;&#36776;&#29575;&#24773;&#20917;&#19979;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;GPU&#20869;&#23384;&#26377;&#38480;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20851;&#20110;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20307;&#31215;&#20449;&#24687;&#23545;&#20110;&#26576;&#20123;&#21307;&#23398;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#65288;&#21363;&#25918;&#23556;&#23398;&#20302;&#32622;&#20449;&#24230;&#26631;&#27880;&#65289;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32925;&#30828;&#21270;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20998;&#31867;&#20307;&#31995;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#21015;&#20030;&#20102;&#19981;&#21516;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#26680;&#24515;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04370</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey. (arXiv:2307.04370v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20998;&#31867;&#20307;&#31995;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#21015;&#20030;&#20102;&#19981;&#21516;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#26680;&#24515;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#39550;&#39542;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23427;&#36991;&#20813;&#20102;&#27169;&#22359;&#21270;&#31995;&#32479;&#30340;&#32570;&#28857;&#65292;&#22914;&#22797;&#26434;&#24615;&#36807;&#39640;&#21644;&#35823;&#24046;&#20256;&#25773;&#30340;&#20542;&#21521;&#12290;&#33258;&#21160;&#39550;&#39542;&#36890;&#36807;&#39044;&#20808;&#20027;&#21160;&#35782;&#21035;&#20851;&#38190;&#20107;&#20214;&#26469;&#36229;&#36234;&#20256;&#32479;&#20132;&#36890;&#27169;&#24335;&#65292;&#30830;&#20445;&#20056;&#23458;&#30340;&#23433;&#20840;&#24182;&#20026;&#20182;&#20204;&#25552;&#20379;&#33298;&#36866;&#30340;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#21644;&#22810;&#21464;&#30340;&#20132;&#36890;&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#23545;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#31471;&#21040;&#31471;&#26041;&#24335;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20998;&#31867;&#20307;&#31995;&#65292;&#21253;&#25324;&#20174;&#24863;&#30693;&#21040;&#25511;&#21046;&#30340;&#25972;&#20010;&#39550;&#39542;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#26681;&#25454;&#22522;&#26412;&#21407;&#29702;&#12289;&#26041;&#27861;&#35770;&#21644;&#26680;&#24515;&#21151;&#33021;&#23545;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation. Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers' safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings. This paper presents a comprehensive review of the End-to-End autonomous driving stack. It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control, while addressing key challenges encountered in real-world applications. Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality. These categories encompass sensorial input, m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;Cox&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06276</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#30340;&#22522;&#22240;&#34920;&#36798;&#20540;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values. (arXiv:2306.06276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;Cox&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#20102;&#22810;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#20316;&#20026;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#65292;&#20197;&#22522;&#20110;&#32959;&#30244;&#36716;&#24405;&#32452;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#27809;&#26377;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26377;&#35268;&#21017;&#21270;&#30340;Cox&#22238;&#24402;&#26174;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#21644;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;ANN&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22270;&#20687;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#33391;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#32959;&#30244;&#22522;&#22240;&#34920;&#36798;&#21644;&#20020;&#24202;&#25968;&#25454;&#65292;&#20197;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26469;&#35757;&#32451;Cox&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#12290;&#20351;&#29992;&#26469;&#33258;The Cancer Genome Atlas&#65288;TCGA&#65289;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;Cox&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#24449;&#65292;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several artificial neural networks (ANNs) have recently been developed as the Cox proportional hazard model for predicting cancer prognosis based on tumor transcriptome. However, they have not demonstrated significantly better performance than the traditional Cox regression with regularization. Training an ANN with high prediction power is challenging in the presence of a limited number of data samples and a high-dimensional feature space. Recent advancements in image classification have shown that contrastive learning can facilitate further learning tasks by learning good feature representation from a limited number of data samples. In this paper, we applied supervised contrastive learning to tumor gene expression and clinical data to learn feature representations in a low-dimensional space. We then used these learned features to train the Cox model for predicting cancer prognosis. Using data from The Cancer Genome Atlas (TCGA), we demonstrated that our contrastive learning-based Cox 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#32780;&#39640;&#25928;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;&#24515;&#30005;&#22270;(ECG)&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#27169;&#22411;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23454;&#29616;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#26032;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.06196</link><description>&lt;p&gt;
ElectroCardioGuard&#65306;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#38450;&#27490;&#24515;&#30005;&#22270;&#25968;&#25454;&#24211;&#20013;&#24739;&#32773;&#35823;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks. (arXiv:2306.06196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#32780;&#39640;&#25928;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;&#24515;&#30005;&#22270;(ECG)&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#27169;&#22411;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23454;&#29616;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#26032;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;(ECG)&#36890;&#24120;&#34987;&#24515;&#33039;&#30149;&#19987;&#23478;&#29992;&#20110;&#26816;&#27979;&#19982;&#24515;&#33039;&#30456;&#20851;&#30340;&#30149;&#29702;&#24773;&#20917;&#65292;&#32780;&#21487;&#38752;&#30340;ECG&#38598;&#21512;&#23545;&#20110;&#30830;&#35786;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#23558;&#35760;&#24405;&#30340;ECG&#20998;&#37197;&#32473;&#38169;&#35823;&#30340;&#24739;&#32773;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#21457;&#29983;&#12290;&#26412;&#25991;&#19982;&#19968;&#23478;&#20020;&#24202;&#21644;&#30740;&#31350;&#26426;&#26500;&#21512;&#20316;&#65292;&#35813;&#26426;&#26500;&#35748;&#35782;&#21040;&#36825;&#19968;&#25361;&#25112;&#24182;&#32852;&#31995;&#25105;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#24039;&#39640;&#25928;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;ECG&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#21033;&#29992;760&#20493;&#26356;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;PTB-XL&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#30011;&#24266;&#25506;&#38024;&#24739;&#32773;&#35782;&#21035;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#19968;&#20010;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#26032;&#25910;&#38598;&#30340;ECG&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiograms (ECGs) are commonly used by cardiologists to detect heart-related pathological conditions. Reliable collections of ECGs are crucial for precise diagnosis. However, in clinical practice, the assignment of captured ECG recordings to incorrect patients can occur inadvertently. In collaboration with a clinical and research facility which recognized this challenge and reached out to us, we present a study that addresses this issue. In this work, we propose a small and efficient neural-network based model for determining whether two ECGs originate from the same patient. Our model demonstrates great generalization capabilities and achieves state-of-the-art performance in gallery-probe patient identification on PTB-XL while utilizing 760x fewer parameters. Furthermore, we present a technique leveraging our model for detection of recording-assignment mistakes, showcasing its applicability in a realistic scenario. Finally, we evaluate our model on a newly collected ECG dataset
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03763</link><description>&lt;p&gt;
ChatGPT&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20174;&#26102;&#38388;&#25991;&#26412;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#36130;&#32463;&#26032;&#38395;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#32467;&#26500;&#30340;&#28508;&#21147;&#20173;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#22270;&#25512;&#26029;&#33021;&#21147;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#32467;&#26500;&#34701;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36827;&#34892;&#21518;&#32493;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#20135;&#20986;&#26500;&#24314;&#30340;&#32452;&#21512;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#24180;&#21270;&#32047;&#35745;&#22238;&#25253;&#12289;&#26356;&#20302;&#30340;&#27874;&#21160;&#24615;&#21644;&#26368;&#22823;&#22238;&#25764;&#12290;&#36825;&#31181;&#21331;&#36234;&#34920;&#29616;&#31361;&#26174;&#20102;ChatGPT&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#33021;&#37327;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#24191;&#20041;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#22810;&#31181;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#65292;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15639</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24191;&#20041;p-Laplacian&#27491;&#21017;&#21270;&#26694;&#26550;&#22270;&#21367;&#31215;&#32593;&#32476;: &#25910;&#25947;&#24615;&#12289;&#33021;&#37327;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#25193;&#25955;&#35757;&#32451;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion. (arXiv:2305.15639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#33021;&#37327;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#24191;&#20041;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#22810;&#31181;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#65292;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65288;pL-UFG&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#24314;&#31435;&#23545;&#20854;&#24615;&#36136;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;Framelet&#21367;&#31215;&#21518;&#38598;&#25104;p-Laplacian&#30340;&#38544;&#24335;&#23618;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;pL-UFG&#28176;&#36817;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;&#36890;&#36807;&#25506;&#32034;pL-UFG&#30340;&#24191;&#20041;Dirichlet&#33021;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Dirichlet&#33021;&#37327;&#20445;&#25345;&#38750;&#38646;&#65292;&#30830;&#20445;&#22312;pL-UFG&#25509;&#36817;&#25910;&#25947;&#26102;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#33021;&#37327;&#35270;&#35282;&#38416;&#26126;&#20102;pL-UFG&#20013;&#30340;&#38544;&#24335;&#23618;&#19982;&#22270;Framelets&#21327;&#21516;&#24037;&#20316;&#65292;&#22686;&#24378;&#20102;&#35813;&#27169;&#22411;&#23545;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38544;&#24335;&#23618;&#21487;&#20197;&#34987;&#35299;&#37322;&#25104;&#24191;&#20041;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;&#36825;&#20123;&#22810;&#26041;&#38754;&#30340;&#20998;&#26512;&#23548;&#33268;&#20102;&#32479;&#19968;&#30340;&#32467;&#35770;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive theoretical analysis of graph p-Laplacian based framelet network (pL-UFG) to establish a solid understanding of its properties. We begin by conducting a convergence analysis of the p-Laplacian based implicit layer integrated after the framelet convolution, providing insights into the asymptotic behavior of pL-UFG. By exploring the generalized Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains non-zero, ensuring the avoidance of over-smoothing issues in pL-UFG as it approaches convergence. Furthermore, we elucidate the dynamic energy perspective through which the implicit layer in pL-UFG synergizes with graph framelets, enhancing the model's adaptability to both homophilic and heterophilic data. Remarkably, we establish that the implicit layer can be interpreted as a generalized non-linear diffusion process, enabling training using diverse schemes. These multifaceted analyses lead to unified conclusions that provide novel insi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#24378;&#21270;&#20102;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#30340;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06704</link><description>&lt;p&gt;
&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#30340;&#40065;&#26834;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models. (arXiv:2305.06704v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#24378;&#21270;&#20102;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#30340;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#21457;&#29616;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#65292;&#21487;&#20197;&#33719;&#24471;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#25351;&#30340;&#26159;&#20004;&#20010;&#30456;&#23545;&#26102;&#38388;&#20114;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#29992;&#20110;&#25511;&#21046;&#12289;&#39044;&#27979;&#25110;&#32858;&#31867;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25152;&#35774;&#24819;&#30340;&#31649;&#36947;&#25509;&#25910;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20174;&#27599;&#20010;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#19968;&#32452;&#23376;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#65288;&#20363;&#22914;K-means++&#21644;&#35889;&#32858;&#31867;&#65289;&#65292;&#37319;&#29992;&#21508;&#31181;&#25104;&#23545;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#19968;&#26086;&#32858;&#31867;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#36328;&#32858;&#31867;&#30340;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#34987;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;&#30001;&#20110;&#22810;
&lt;/p&gt;
&lt;p&gt;
In multivariate time series systems, key insights can be obtained by discovering lead-lag relationships inherent in the data, which refer to the dependence between two time series shifted in time relative to one another, and which can be leveraged for the purposes of control, forecasting or clustering. We develop a clustering-driven methodology for the robust detection of lead-lag relationships in lagged multi-factor models. Within our framework, the envisioned pipeline takes as input a set of time series, and creates an enlarged universe of extracted subsequence time series from each input time series, by using a sliding window approach. We then apply various clustering techniques (e.g, K-means++ and spectral clustering), employing a variety of pairwise similarity measures, including nonlinear ones. Once the clusters have been extracted, lead-lag estimates across clusters are aggregated to enhance the identification of the consistent relationships in the original universe. Since multi
&lt;/p&gt;</description></item><item><title>ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;</title><link>http://arxiv.org/abs/2305.03513</link><description>&lt;p&gt;
ChatGraph: &#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03513
&lt;/p&gt;
&lt;p&gt;
ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#26368;&#36817;&#25512;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#38459;&#30861;&#20102;&#23427;&#30340;&#28508;&#22312;&#24212;&#29992;&#65306;&#65288;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#30340;&#19981;&#28789;&#27963;&#24615;&#21644;&#65288;2&#65289;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#26469;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#65292;&#21516;&#26102;&#25552;&#39640;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing Cha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12751</link><description>&lt;p&gt;
&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#25913;&#36827;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Node Feature Augmentation Vitaminizes Network Alignment. (arXiv:2304.12751v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#65288;NA&#65289;&#26159;&#36890;&#36807;&#32473;&#23450;&#32593;&#32476;&#30340;&#25299;&#25169;&#21644;/&#25110;&#29305;&#24449;&#20449;&#24687;&#26469;&#21457;&#29616;&#22810;&#20010;&#32593;&#32476;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;NA&#26041;&#27861;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#24182;&#19981;&#24635;&#26159;&#26377;&#39069;&#22806;&#20449;&#24687;&#65292;&#22914;&#20808;&#21069;&#30340;&#38170;&#28857;&#38142;&#25509;&#21644;/&#25110;&#33410;&#28857;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23454;&#38469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grad-Align+&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;NA&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;&#26368;&#36817;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;NA&#26041;&#27861;Grad-Align&#20043;&#19978;&#65292;Grad-Align+&#20165;&#36880;&#27493;&#21457;&#29616;&#37096;&#20998;&#33410;&#28857;&#23545;&#65292;&#30452;&#21040;&#25214;&#21040;&#25152;&#26377;&#33410;&#28857;&#23545;&#12290;&#22312;&#35774;&#35745;Grad-Align+&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#30340;Grad-Align+&#65306;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#65288;CNFA&#65289;&#12289;&#22270;&#20999;&#29255;&#29983;&#25104;&#21644;&#20248;&#21270;&#33410;&#28857;&#23884;&#20837;&#29305;&#24449;&#65288;ONIFE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment (NA) is the task of discovering node correspondences across multiple networks using topological and/or feature information of given networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this practical challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers only a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, we develop Grad-Align+ consisting of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20462;&#27491;Medoid-Shift&#65288;RMS&#65289;&#30340;&#26032;&#32858;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09512</link><description>&lt;p&gt;
&#22522;&#20110;KNN&#30340;&#20462;&#27491;Medoid-Shift&#30340;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Community Detection Using Revised Medoid-Shift Based on KNN. (arXiv:2304.09512v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09512
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20462;&#27491;Medoid-Shift&#65288;RMS&#65289;&#30340;&#26032;&#32858;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#30340;&#20852;&#36215;&#65292;&#31038;&#21306;&#26816;&#27979;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#22343;&#20540;&#28418;&#31227;&#26159;&#19968;&#31181;&#20986;&#33394;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#21482;&#33021;&#22788;&#29702;&#20855;&#26377;&#22352;&#26631;&#30340;&#25968;&#25454;&#65292;&#32780;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#22823;&#22810;&#20197;&#22270;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#21487;&#20197;&#35270;&#20026;&#20855;&#26377;&#36317;&#31163;&#30697;&#38453;&#65288;&#25110;&#30456;&#20284;&#24230;&#30697;&#38453;&#65289;&#30340;&#25968;&#25454;&#65292;&#22240;&#27492;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861; Medoid-Shift&#65292;&#35813;&#31639;&#27861;&#20445;&#30041;&#20102;&#22343;&#20540;&#28418;&#31227;&#30340;&#20248;&#28857;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#36317;&#31163;&#30697;&#38453;&#30340;&#38382;&#39064;&#65292;&#22914;&#31038;&#21306;&#26816;&#27979;&#12290;Medoid-Shift&#31639;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#21487;&#33021;&#22312;&#30001;&#36317;&#31163;&#21442;&#25968;&#23450;&#20041;&#30340;&#37051;&#22495;&#21306;&#22495;&#20869;&#27809;&#26377;&#25968;&#25454;&#28857;&#12290; &#20026;&#20102;&#26356;&#22909;&#22320;&#22788;&#29702;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20462;&#27491;Medoid-Shift&#65288;RMS&#65289;&#30340;&#26032;&#31639;&#27861;&#12290; &#22312;&#23547;&#25214;&#19979;&#19968;&#20010;&#20013;&#24515;&#28857;&#30340;&#36807;&#31243;&#20013;&#65292;RMS&#31639;&#27861;&#22522;&#20110;&#37051;&#22495;&#30340;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection becomes an important problem with the booming of social networks. As an excellent clustering algorithm, Mean-Shift can not be applied directly to community detection, since Mean-Shift can only handle data with coordinates, while the data in the community detection problem is mostly represented by a graph that can be treated as data with a distance matrix (or similarity matrix). Fortunately, a new clustering algorithm called Medoid-Shift is proposed. The Medoid-Shift algorithm preserves the benefits of Mean-Shift and can be applied to problems based on distance matrix, such as community detection. One drawback of the Medoid-Shift algorithm is that there may be no data points within the neighborhood region defined by a distance parameter. To deal with the community detection problem better, a new algorithm called Revised Medoid-Shift (RMS) in this work is thus proposed. During the process of finding the next medoid, the RMS algorithm is based on a neighborhood defined
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#24322;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#36755;&#20986;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#21512;&#35268;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02574</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21512;&#35268;&#24322;&#31574;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Conformal Off-Policy Evaluation in Markov Decision Processes. (arXiv:2304.02574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#24322;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#36755;&#20986;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#21512;&#35268;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#21644;&#35780;&#20272;&#26377;&#25928;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#32773;&#19981;&#33021;&#36827;&#34892;&#23454;&#39564;&#65292;&#20063;&#19981;&#33021;&#20197;&#22312;&#32447;&#26041;&#24335;&#33719;&#21462;&#25968;&#25454;&#65288;&#22312;&#23454;&#39564;&#36153;&#29992;&#39640;&#26114;&#12289;&#39118;&#38505;&#39640;&#25110;&#19981;&#36947;&#24503;&#30340;&#24773;&#20917;&#19979;&#65292;&#23601;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#65289;&#12290;&#38024;&#23545;&#36825;&#31181;&#24212;&#29992;&#65292;&#24517;&#39035;&#20351;&#29992;&#22312;&#19981;&#21516;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#21382;&#21490;&#25968;&#25454;&#65288;&#34892;&#20026;&#31574;&#30053;&#65289;&#26469;&#20272;&#35745;&#32473;&#23450;&#31574;&#30053;&#65288;&#30446;&#26631;&#31574;&#30053;&#65289;&#30340;&#22870;&#21169;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#36825;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21363;&#24322;&#31574;&#35780;&#20272;&#65288;OPE&#65289;&#65292;&#37117;&#27809;&#26377;&#20934;&#30830;&#24615;&#21644;&#30830;&#23450;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#26032;&#22411;OPE&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#19968;&#20010;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#21516;&#26102;&#20855;&#26377;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#12290;OPE&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#30446;&#26631;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;&#22788;&#29702;&#36825;&#31181;&#20559;&#31227;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#20272;&#35745;&#30340;&#22870;&#21169;&#21306;&#38388;&#30340;&#21512;&#35268;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;&#22522;&#20934;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning aims at identifying and evaluating efficient control policies from data. In many real-world applications, the learner is not allowed to experiment and cannot gather data in an online manner (this is the case when experimenting is expensive, risky or unethical). For such applications, the reward of a given policy (the target policy) must be estimated using historical data gathered under a different policy (the behavior policy). Most methods for this learning task, referred to as Off-Policy Evaluation (OPE), do not come with accuracy and certainty guarantees. We present a novel OPE method based on Conformal Prediction that outputs an interval containing the true reward of the target policy with a prescribed level of certainty. The main challenge in OPE stems from the distribution shift due to the discrepancies between the target and the behavior policies. We propose and empirically evaluate different ways to deal with this shift. Some of these methods yield conform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#36716;&#31227;&#30697;&#38453;&#21644;&#22266;&#23450;&#20294;&#26410;&#30693;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;MDP&#23398;&#20064;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#32622;&#20449;&#21306;&#38388;&#26694;&#26550;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.00155</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19978;&#36827;&#34892;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Reinforcement Learning in Markov Decision Process Using Linear Programming. (arXiv:2304.00155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#36716;&#31227;&#30697;&#38453;&#21644;&#22266;&#23450;&#20294;&#26410;&#30693;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;MDP&#23398;&#20064;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#32622;&#20449;&#21306;&#38388;&#26694;&#26550;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#30693;&#36716;&#31227;&#30697;&#38453;&#21644;&#22266;&#23450;&#20294;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#23398;&#20064;&#32773;&#26088;&#22312;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#24182;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#20182;&#20204;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#27169;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#36807;&#28193;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#24182;&#20351;&#29992;&#21344;&#29992;&#24230;&#37327;&#23558;&#22312;&#32447;MDP&#19982;&#32447;&#24615;&#35268;&#21010;&#30456;&#36830;&#25509;&#65292;&#23454;&#29616;&#20102;$\tilde{O}(LX\sqrt{TA})$&#30340;&#39640;&#27010;&#29575;&#36951;&#25022;&#30028;&#12290;&#23427;&#27604;&#29616;&#26377;&#30340;&#20351;&#29992;&#31867;&#20284;&#32622;&#20449;&#21306;&#38388;&#26694;&#26550;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#24182;&#25913;&#21892;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online reinforcement learning in episodic Markov decision process (MDP) with an unknown transition matrix and stochastic rewards drawn from a fixed but unknown distribution. The learner aims to learn the optimal policy and minimize their regret over a finite time horizon through interacting with the environment. We devise a simple and efficient model-based algorithm that achieves $\tilde{O}(LX\sqrt{TA})$ regret with high probability, where $L$ is the episode length, $T$ is the number of episodes, and $X$ and $A$ are the cardinalities of the state space and the action space, respectively. The proposed algorithm, which is based on the concept of "optimism in the face of uncertainty", maintains confidence sets of transition and reward functions and uses occupancy measures to connect the online MDP with linear programming. It achieves a tighter regret bound compared to the existing works that use a similar confidence sets framework and improves the computational effort compared
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20174;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#31354;&#38388;&#32676;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#21462;&#24471;&#20102;&#27604;&#30452;&#25509;&#22312;ICSD&#26230;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11699</link><description>&lt;p&gt;
&#22312;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms. (arXiv:2303.11699v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20174;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#31354;&#38388;&#32676;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#21462;&#24471;&#20102;&#27604;&#30452;&#25509;&#22312;ICSD&#26230;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#26230;&#20307;&#31354;&#38388;&#32676;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#22312;ICSD&#31561;&#25968;&#25454;&#24211;&#30340;&#27169;&#25311;&#34893;&#23556;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#20854;&#35268;&#27169;&#26377;&#38480;&#12289;&#31867;&#21035;&#19981;&#22343;&#21248;&#24182;&#19988;&#20559;&#21521;&#26576;&#20123;&#32467;&#26500;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#27599;&#20010;&#31354;&#38388;&#32676;&#30340;&#23545;&#31216;&#25805;&#20316;&#65292;&#22312;&#38543;&#26426;&#22352;&#26631;&#19979;&#29983;&#25104;&#21512;&#25104;&#26230;&#20307;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;ResNet&#31867;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#65292;&#27599;&#23567;&#26102;&#26368;&#22810;&#21487;&#29983;&#25104;&#23569;&#37327;&#30334;&#19975;&#20010;&#21807;&#19968;&#30340;&#21512;&#25104;&#34893;&#23556;&#22270;&#12290;&#38024;&#23545;&#25105;&#20204;&#36873;&#25321;&#30340;&#31354;&#38388;&#32676;&#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#22823;&#22810;&#25968;&#31354;&#38388;&#32676;&#30340;&#26410;&#35265;ICSD&#32467;&#26500;&#31867;&#22411;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;79.9%&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#36825;&#36229;&#36807;&#20102;&#30452;&#25509;&#22312;ICSD&#26230;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;56.1%&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques have successfully been used to extract structural information such as the crystal space group from powder X-ray diffractograms. However, training directly on simulated diffractograms from databases such as the ICSD is challenging due to its limited size, class-inhomogeneity, and bias toward certain structure types. We propose an alternative approach of generating synthetic crystals with random coordinates by using the symmetry operations of each space group. Based on this approach, we demonstrate online training of deep ResNet-like models on up to a few million unique on-the-fly generated synthetic diffractograms per hour. For our chosen task of space group classification, we achieved a test accuracy of 79.9% on unseen ICSD structure types from most space groups. This surpasses the 56.1% accuracy of the current state-of-the-art approach of training on ICSD crystals directly. Our results demonstrate that synthetically generated crystals can be used to extract
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22534;&#21472;&#20998;&#35299;&#26041;&#27861;&#26469;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.14227</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#23427;&#20204;&#30340;&#26102;&#38388;&#20998;&#35299;&#30340;&#22240;&#26524;&#25195;&#25551;&#31574;&#30053;&#30340;&#32479;&#19968;&#21487;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A unified scalable framework for causal sweeping strategies for Physics-Informed Neural Networks (PINNs) and their temporal decompositions. (arXiv:2302.14227v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14227
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22534;&#21472;&#20998;&#35299;&#26041;&#27861;&#26469;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20316;&#20026;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#65288;CS&amp;E&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36817;&#26399;&#19968;&#20010;&#26377;&#36259;&#30340;&#35805;&#39064;&#26159;&#25506;&#32034;&#21508;&#31181;&#35757;&#32451;&#65288;&#21363;&#20248;&#21270;&#65289;&#25361;&#25112; - &#29305;&#21035;&#26159;&#22312;&#20248;&#21270;&#26223;&#35266;&#20013;&#38519;&#20837;&#20102;&#24046;&#30340;&#23616;&#37096;&#26368;&#23567;&#28857;&#26102;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;PDE&#36817;&#20284;&#32473;&#20986;&#20102;&#19968;&#20010;&#36739;&#24046;&#29978;&#33267;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#35299;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#25968;&#25454;&#20197;&#21069;&#36827;&#30340;&#26102;&#38388;&#30456;&#20851;PDE&#30340;&#27714;&#35299;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#23384;&#22312;&#20110;&#19968;&#20123;&#39046;&#22495;&#20998;&#35299;&#31574;&#30053;&#20013;&#65292;&#22914;&#20351;&#29992;XPINNs&#30340;&#26102;&#38388;&#20998;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19981;&#21516;&#35757;&#32451;&#25361;&#25112;&#30340;&#31034;&#20363;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#21407;&#22240;&#65292;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#19982;&#20449;&#24687;&#20256;&#25773;&#21644;&#26102;&#38388;&#20998;&#35299;&#26377;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22534;&#21472;&#20998;&#35299;&#26041;&#27861;&#65292;&#20197;&#24357;&#21512;&#26102;&#38388;&#27493;&#36827;PINNs&#21644;XPINNs&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) as a means of solving partial differential equations (PDE) have garnered much attention in the Computational Science and Engineering (CS&amp;E) world. However, a recent topic of interest is exploring various training (i.e., optimization) challenges - in particular, arriving at poor local minima in the optimization landscape results in a PINN approximation giving an inferior, and sometimes trivial, solution when solving forward time-dependent PDEs with no data. This problem is also found in, and in some sense more difficult, with domain decomposition strategies such as temporal decomposition using XPINNs. We furnish examples and explanations for different training challenges, their cause, and how they relate to information propagation and temporal decomposition. We then propose a new stacked-decomposition method that bridges the gap between time-marching PINNs and XPINNs. We also introduce significant computational speed-ups by using transfer learnin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#21644;PDE&#29702;&#35770;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;3D&#22343;&#21248;&#26925;&#22278;&#22411;PDE&#35299;&#31639;&#31526;&#30340;&#25968;&#25454;&#39640;&#25928;&#24674;&#22797;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#20197;&#25351;&#25968;&#25910;&#25947;&#29575;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.12888</link><description>&lt;p&gt;
&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#26159;&#21487;&#38752;&#30340;
&lt;/p&gt;
&lt;p&gt;
Elliptic PDE learning is provably data-efficient. (arXiv:2302.12888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#21644;PDE&#29702;&#35770;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;3D&#22343;&#21248;&#26925;&#22278;&#22411;PDE&#35299;&#31639;&#31526;&#30340;&#25968;&#25454;&#39640;&#25928;&#24674;&#22797;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#20197;&#25351;&#25968;&#25910;&#25947;&#29575;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PDE&#23398;&#20064;&#26159;&#19968;&#20010;&#23558;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#24674;&#22797;&#26410;&#30693;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#26368;&#36817;&#30340;PDE&#23398;&#20064;&#25216;&#26415;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#20173;&#28982;&#26159;&#32463;&#39564;&#24615;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;PDE&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#23545;&#25152;&#38656;&#30340;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#23545;&#25968;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#21644;PDE&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#38752;&#25968;&#25454;&#25928;&#29575;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20174;&#36755;&#20837;-&#36755;&#20986;&#25968;&#25454;&#20013;&#24674;&#22797;3D&#22343;&#21248;&#26925;&#22278;&#22411;PDE&#30340;&#35299;&#31639;&#31526;&#65292;&#24182;&#20197;&#26497;&#39640;&#30340;&#25104;&#21151;&#27010;&#29575;&#23454;&#29616;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25351;&#25968;&#25910;&#25947;&#29575;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
PDE learning is an emerging field that combines physics and machine learning to recover unknown physical systems from experimental data. While deep learning models traditionally require copious amounts of training data, recent PDE learning techniques achieve spectacular results with limited data availability. Still, these results are empirical. Our work provides theoretical guarantees on the number of input-output training pairs required in PDE learning. Specifically, we exploit randomized numerical linear algebra and PDE theory to derive a provably data-efficient algorithm that recovers solution operators of 3D uniformly elliptic PDEs from input-output data and achieves an exponential convergence rate of the error with respect to the size of the training dataset with an exceptionally high probability of success.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#26032;&#20852;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#65292;&#20855;&#20307;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31639;&#27861;&#22312;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306;&#22320;&#34920;&#39118;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#29702;&#24819;&#21270;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#31354;&#38388;&#21151;&#29575;&#35889;&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.08720</link><description>&lt;p&gt;
&#36817;&#22320;&#34920;&#39118;&#30340;&#31639;&#27861;&#24187;&#35273;&#65306;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#26032;&#20852;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#65292;&#20855;&#20307;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31639;&#27861;&#22312;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306;&#22320;&#34920;&#39118;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#29702;&#24819;&#21270;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#31354;&#38388;&#21151;&#29575;&#35889;&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#20013;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12290;&#25105;&#20204;&#30340;GANs&#26159;&#36890;&#36807;&#23545;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#36755;&#20837;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#26469;&#29983;&#25104;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306; Weather Research and Forecasting&#65288;WRF&#65289;&#27169;&#22411;&#30340;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22320;&#34920;&#39118;&#12290;&#19982;&#20256;&#32479;&#30340;SR&#27169;&#22411;&#19981;&#21516;&#65292;LR&#36755;&#20837;&#22312;WRF&#27169;&#25311;&#20013;&#20351;&#29992;&#20102;&#38750;&#29702;&#24819;&#21270;&#30340;LR&#21644;HR&#37197;&#23545;&#65292;&#23548;&#33268;&#30001;&#20110;&#20869;&#37096;&#21464;&#24322;&#24341;&#36215;&#30340;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#24403;&#21069;&#22522;&#20110;SR&#30340;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#24182;&#23581;&#35797;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26032;&#39062;&#39057;&#29575;&#20998;&#31163;&#65288;FS&#65289;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;SR&#27169;&#22411;&#30340;&#25216;&#33021;&#65292;&#25105;&#20204;&#31934;&#36873;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#20851;&#27880;&#22522;&#20110;&#31354;&#38388;&#21151;&#29575;&#35889;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;GAN&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of emerging machine learning methods from image super-resolution (SR) to the task of statistical downscaling. We specifically focus on convolutional neural network-based Generative Adversarial Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to generate high-resolution (HR) surface winds emulating Weather Research and Forecasting (WRF) model simulations over North America. Unlike traditional SR models, where LR inputs are idealized coarsened versions of the HR images, WRF emulation involves using non-idealized LR and HR pairs resulting in shared-scale mismatches due to internal variability. Our study builds upon current SR-based statistical downscaling by experimenting with a novel frequency-separation (FS) approach from the computer vision field. To assess the skill of SR models, we carefully select evaluation metrics, and focus on performance measures based on spatial power spectra. Our analyses reveal how GAN configurations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#31895;&#31890;&#21270;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#21147;&#22330;&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#21147;&#22330;&#36755;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#29983;&#29289;&#36807;&#31243;&#30340;&#31934;&#30830;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#34507;&#30333;&#36136;&#27169;&#25311;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.00600</link><description>&lt;p&gt;
&#19968;&#25307;&#20004;&#24471;&#65306;&#31895;&#31890;&#21270;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics. (arXiv:2302.00600v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#31895;&#31890;&#21270;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#21147;&#22330;&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#21147;&#22330;&#36755;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#29983;&#29289;&#36807;&#31243;&#30340;&#31934;&#30830;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#34507;&#30333;&#36136;&#27169;&#25311;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31895;&#31890;&#21270;&#65288;CG&#65289;&#20998;&#23376;&#21160;&#21147;&#23398;&#20351;&#24471;&#22312;&#21407;&#23376;&#20998;&#36776;&#29575;&#19979;&#26080;&#27861;&#35299;&#20915;&#30340;&#29983;&#29289;&#36807;&#31243;&#21487;&#20197;&#24471;&#20197;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#23398;&#20064;CG&#21147;&#22330;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#12289;&#21147;&#22330;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;CG&#21147;&#22330;&#65292;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#20219;&#20309;&#21147;&#22330;&#36755;&#20837;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#20102;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30340;&#35780;&#20998;&#20989;&#25968;&#36817;&#20284;&#19968;&#20010;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#27169;&#25311;CG&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#21147;&#22330;&#12290;&#23613;&#31649;&#30456;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#22823;&#22823;&#31616;&#21270;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#23567;&#22411;&#21040;&#20013;&#22411;&#34507;&#30333;&#36136;&#27169;&#25311;&#20013;&#20855;&#26377;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#37325;&#29616;CG&#24179;&#34913;&#20998;&#24067;&#65292;&#24182;&#20445;&#25345;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#20840;&#21407;&#23376;&#27169;&#25311;&#30340;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coarse-grained (CG) molecular dynamics enables the study of biological processes at temporal and spatial scales that would be intractable at an atomistic resolution. However, accurately learning a CG force field remains a challenge. In this work, we leverage connections between score-based generative models, force fields and molecular dynamics to learn a CG force field without requiring any force inputs during training. Specifically, we train a diffusion generative model on protein structures from molecular dynamics simulations, and we show that its score function approximates a force field that can directly be used to simulate CG molecular dynamics. While having a vastly simplified training setup compared to previous work, we demonstrate that our approach leads to improved performance across several small- to medium-sized protein simulations, reproducing the CG equilibrium distribution, and preserving dynamics of all-atom simulations such as protein folding events.
&lt;/p&gt;</description></item><item><title>RouteNet-Fermi&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#23450;&#20041;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#30340;&#25490;&#38431;&#29702;&#35770;&#30456;&#27604;&#65292;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.12070</link><description>&lt;p&gt;
RouteNet-Fermi: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32593;&#32476;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
RouteNet-Fermi: Network Modeling with Graph Neural Networks. (arXiv:2212.12070v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12070
&lt;/p&gt;
&lt;p&gt;
RouteNet-Fermi&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#23450;&#20041;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#30340;&#25490;&#38431;&#29702;&#35770;&#30456;&#27604;&#65292;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27169;&#22411;&#26159;&#29616;&#20195;&#32593;&#32476;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24191;&#27867;&#29992;&#20110;&#32593;&#32476;&#35268;&#21010;&#21644;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#19968;&#20123;&#27169;&#22411;&#23384;&#22312;&#38480;&#21046;&#65292;&#22914;&#25490;&#38431;&#29702;&#35770;&#27169;&#22411;&#20013;&#23545;&#39532;&#23572;&#21487;&#22827;&#27969;&#37327;&#30340;&#20551;&#35774;&#65292;&#20197;&#21450;&#32593;&#32476;&#27169;&#25311;&#22120;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#27491;&#22312;&#25512;&#21160;&#19968;&#20195;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RouteNet-Fermi&#30340;&#33258;&#23450;&#20041;GNN&#27169;&#22411;&#65292;&#23427;&#19982;&#25490;&#38431;&#29702;&#35770;&#20855;&#26377;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#32593;&#32476;&#35268;&#27169;&#65288;&#26368;&#22823;&#36798;&#21040;300&#20010;&#33410;&#28857;&#65289;&#21644;&#21253;&#25324;&#20855;&#26377;&#28151;&#21512;&#27969;&#37327;&#29305;&#24615;&#30340;&#26679;&#26412;&#65288;&#22914;&#22797;&#26434;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65289;&#20013;&#27979;&#35797;&#20102;RouteNet-Fermi&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models present limitations, such as the assumption of Markovian traffic in queuing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks (GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviors. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goals as Queuing Theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles -- e.g., with complex non-Markovian models -- and arbitrary routin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#25991;&#26412;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968; SoftCTC&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#36716;&#24405;&#21464;&#20307;&#65292;&#36991;&#20813;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#36807;&#28388;&#27493;&#39588;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#25163;&#20889;&#35782;&#21035;&#20219;&#21153;&#19978;&#19982;&#36807;&#28388;&#27969;&#27700;&#32447;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2212.02135</link><description>&lt;p&gt;
SoftCTC -- &#21033;&#29992;&#36719;&#20266;&#26631;&#31614;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SoftCTC -- Semi-Supervised Learning for Text Recognition using Soft Pseudo-Labels. (arXiv:2212.02135v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#25991;&#26412;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968; SoftCTC&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#36716;&#24405;&#21464;&#20307;&#65292;&#36991;&#20813;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#36807;&#28388;&#27493;&#39588;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#25163;&#20889;&#35782;&#21035;&#20219;&#21153;&#19978;&#19982;&#36807;&#28388;&#27969;&#27700;&#32447;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#24207;&#21015;&#20219;&#21153;&#30340;&#21322;&#30417;&#30563;&#35757;&#32451;&#65292;&#22914;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#25110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968; SoftCTC&#65292;&#23427;&#26159;CTC&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#36716;&#24405;&#21464;&#20307;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30465;&#30053;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#36807;&#28388;&#27493;&#39588;&#65292;&#35813;&#27493;&#39588;&#23545;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25163;&#20889;&#35782;&#21035;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;SoftCTC&#21487;&#20197;&#19982;&#31934;&#35843;&#30340;&#22522;&#20110;&#36807;&#28388;&#30340;&#27969;&#27700;&#32447;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;SoftCTC&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24471;&#20986;&#32467;&#35770;&#23427;&#22312;&#35757;&#32451;&#22810;&#20010;&#36716;&#24405;&#21464;&#20307;&#26041;&#38754;&#27604;&#26420;&#32032;&#30340;CTC&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;GPU&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores semi-supervised training for sequence tasks, such as Optical Character Recognition or Automatic Speech Recognition. We propose a novel loss function $\unicode{x2013}$ SoftCTC $\unicode{x2013}$ which is an extension of CTC allowing to consider multiple transcription variants at the same time. This allows to omit the confidence based filtering step which is otherwise a crucial component of pseudo-labeling approaches to semi-supervised learning. We demonstrate the effectiveness of our method on a challenging handwriting recognition task and conclude that SoftCTC matches the performance of a finely-tuned filtering based pipeline. We also evaluated SoftCTC in terms of computational efficiency, concluding that it is significantly more efficient than a na\"ive CTC-based approach for training on multiple transcription variants, and we make our GPU implementation public.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#30340;&#27010;&#24565;&#28418;&#31227;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20840;&#23616;&#32423;&#21035;&#24341;&#20837;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#25110;&#35843;&#25972;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27010;&#24565;&#28418;&#31227;&#30340;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.12989</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#27010;&#24565;&#28418;&#31227;&#21462;&#28040;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Unlearning of Concept Drift with Autoencoders. (arXiv:2211.12989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#30340;&#27010;&#24565;&#28418;&#31227;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20840;&#23616;&#32423;&#21035;&#24341;&#20837;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#25110;&#35843;&#25972;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27010;&#24565;&#28418;&#31227;&#30340;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#28418;&#31227;&#26159;&#25351;&#24433;&#21709;&#26410;&#26469;&#26679;&#26412;&#25968;&#25454;&#27969;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#23545;&#25968;&#25454;&#27969;&#36827;&#34892;&#25805;&#20316;&#30340;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#21464;&#24471;&#36807;&#26102;&#65292;&#38656;&#35201;&#26114;&#36149;&#19988;&#22256;&#38590;&#30340;&#35843;&#25972;&#65292;&#22914;&#37325;&#26032;&#35757;&#32451;&#25110;&#36866;&#24212;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23454;&#26045;&#23616;&#37096;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;&#26041;&#26696;&#65292;&#20854;&#20013;&#35201;&#20040;&#20351;&#29992;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#65292;&#35201;&#20040;&#22312;&#28418;&#31227;&#26816;&#27979;&#26426;&#21046;&#35302;&#21457;&#35686;&#25253;&#26102;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#20840;&#23616;&#32423;&#21035;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#8220;&#21462;&#28040;&#23398;&#20064;&#8221;&#27010;&#24565;&#28418;&#31227;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#35843;&#25972;&#20219;&#20309;&#22312;&#25968;&#25454;&#19978;&#25805;&#20316;&#30340;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#20004;&#20010;&#24212;&#29992;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;30&#20010;&#20197;&#19978;&#27169;&#22411;&#30340;&#30495;&#23454;&#27700;&#37197;&#36865;&#32593;&#32476;&#65292;&#20854;&#20013;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Concept drift refers to a change in the data distribution affecting the data stream of future samples. Consequently, learning models operating on the data stream might become obsolete, and need costly and difficult adjustments such as retraining or adaptation. Existing methods usually implement a local concept drift adaptation scheme, where either incremental learning of the models is used, or the models are completely retrained when a drift detection mechanism triggers an alarm. This paper proposes an alternative approach in which an unsupervised and model-agnostic concept drift adaptation method at the global level is introduced, based on autoencoders. Specifically, the proposed method aims to ``unlearn'' the concept drift without having to retrain or adapt any of the learning models operating on the data. An extensive experimental evaluation is conducted in two application domains. We consider a realistic water distribution network with more than 30 models in-place, from which we cr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#22312;&#26377;&#21644;&#27809;&#26377;&#39069;&#22806;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2211.11435</link><description>&lt;p&gt;
ZigZag: &#36890;&#36807;&#20004;&#27493;&#25512;&#29702;&#23454;&#29616;&#30340;&#36890;&#29992;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference. (arXiv:2211.11435v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#22312;&#26377;&#21644;&#27809;&#26377;&#39069;&#22806;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#32593;&#32476;&#20135;&#29983;&#26377;&#29992;&#30340;&#39044;&#27979;&#30340;&#33021;&#21147;&#24050;&#32463;&#34987;&#20805;&#20998;&#35777;&#26126;&#65292;&#20294;&#26159;&#20272;&#35745;&#36825;&#20123;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35832;&#22914;MC-Dropout&#21644;Deep Ensembles&#20043;&#31867;&#30340;&#37319;&#26679;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#26368;&#27969;&#34892;&#30340;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#26041;&#27861;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#26102;&#38656;&#35201;&#36827;&#34892;&#35768;&#22810;&#21069;&#21521;&#20256;&#36882;&#65292;&#36825;&#20250;&#20943;&#24930;&#36895;&#24230;&#12290;&#26080;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#26356;&#24555;&#65292;&#20294;&#23384;&#22312;&#20854;&#20182;&#32570;&#28857;&#65292;&#20363;&#22914;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21487;&#20449;&#24230;&#36739;&#20302;&#12289;&#20351;&#29992;&#22256;&#38590;&#20197;&#21450;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;&#26080;&#37319;&#26679;&#26041;&#27861;&#65292;&#23427;&#20197;&#26126;&#26174;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#33719;&#24471;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#12290;&#20854;&#22522;&#26412;&#21407;&#29702;&#26159;&#35757;&#32451;&#32593;&#32476;&#22312;&#26377;&#21644;&#27809;&#26377;&#39069;&#22806;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#24403;&#27809;&#26377;&#25552;&#20379;&#20808;&#39564;&#20449;&#24687;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#32593;&#32476;&#30340;&#33258;&#36523;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whereas the ability of deep networks to produce useful predictions has been amply demonstrated, estimating the reliability of these predictions remains challenging. Sampling approaches such as MC-Dropout and Deep Ensembles have emerged as the most popular ones for this purpose. Unfortunately, they require many forward passes at inference time, which slows them down. Sampling-free approaches can be faster but suffer from other drawbacks, such as lower reliability of uncertainty estimates, difficulty of use, and limited applicability to different types of tasks and data.  In this work, we introduce a sampling-free approach that is generic and easy to deploy, while producing reliable uncertainty estimates on par with state-of-the-art methods at a significantly lower computational cost. It is predicated on training the network to produce the same output with and without additional information about it. At inference time, when no prior information is given, we use the network's own predicti
&lt;/p&gt;</description></item><item><title>RCD-SGD&#26159;&#19968;&#20010;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#65292;&#36890;&#36807;&#23376;&#27169;&#22359;&#21010;&#20998;&#23454;&#29616;&#20102;&#31867;&#21035;&#32423;&#29305;&#24449;&#20998;&#24067;&#30340;&#30456;&#20284;&#24615;&#21644;&#31867;&#21035;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2211.00839</link><description>&lt;p&gt;
RCD-SGD: &#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#29615;&#22659;&#20013;&#22522;&#20110;&#23376;&#27169;&#22359;&#21010;&#20998;&#30340;&#20998;&#24067;&#24335; SGD
&lt;/p&gt;
&lt;p&gt;
RCD-SGD: Resource-Constrained Distributed SGD in Heterogeneous Environment via Submodular Partitioning. (arXiv:2211.00839v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00839
&lt;/p&gt;
&lt;p&gt;
RCD-SGD&#26159;&#19968;&#20010;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#65292;&#36890;&#36807;&#23376;&#27169;&#22359;&#21010;&#20998;&#23454;&#29616;&#20102;&#31867;&#21035;&#32423;&#29305;&#24449;&#20998;&#24067;&#30340;&#30456;&#20284;&#24615;&#21644;&#31867;&#21035;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SGD&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#19982;&#24037;&#20316;&#32773;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#20851;&#12290;&#26631;&#20934;&#30340;&#20998;&#21306;&#25216;&#26415;&#35797;&#22270;&#23454;&#29616;&#31561;&#22823;&#23567;&#20998;&#21306;&#65292;&#20854;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#20998;&#24067;&#19982;&#24635;&#25968;&#25454;&#38598;&#25104;&#27604;&#20363;&#12290;&#21363;&#20351;&#20855;&#26377;&#30456;&#21516;&#24635;&#20307;&#20154;&#21475;&#22823;&#23567;&#25110;&#30456;&#21516;&#31867;&#21035;&#27599;&#20010;&#26679;&#26412;&#30340;&#20998;&#21306;&#65292;&#29305;&#24449;&#31354;&#38388;&#20013;&#20173;&#21487;&#33021;&#23384;&#22312;&#38750;&#29420;&#31435;&#20998;&#24067;&#12290;&#22312;&#24322;&#26500;&#35745;&#31639;&#29615;&#22659;&#20013;&#65292;&#24403;&#35774;&#22791;&#20855;&#26377;&#19981;&#21516;&#30340;&#35745;&#31639;&#33021;&#21147;&#26102;&#65292;&#36328;&#35774;&#22791;&#30340;&#22343;&#21248;&#20998;&#21306;&#21487;&#33021;&#23548;&#33268;&#20998;&#24067;&#24335;SGD&#20013;&#30340;&#25302;&#23614;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#26032;&#22411;&#25968;&#25454;&#21010;&#20998;&#31639;&#27861;&#30340;&#24322;&#26500;&#29615;&#22659;&#19979;&#30340;&#20998;&#24067;&#24335;SGD&#26694;&#26550;&#65292;&#35813;&#31639;&#27861;&#26174;&#24335;&#32771;&#34385;&#20102;&#24037;&#20316;&#32773;&#20043;&#38388;&#30340;&#36164;&#28304;&#24322;&#36136;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#31867;&#21035;&#32423;&#29305;&#24449;&#20998;&#24067;&#30340;&#30456;&#20284;&#24615;&#21644;&#31867;&#21035;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of SGD based distributed training algorithms is tied to the data distribution across workers. Standard partitioning techniques try to achieve equal-sized partitions with per-class population distribution in proportion to the total dataset. Partitions having the same overall population size or even the same number of samples per class may still have Non-IID distribution in the feature space. In heterogeneous computing environments, when devices have different computing capabilities, even-sized partitions across devices can lead to the straggler problem in distributed SGD. We develop a framework for distributed SGD in heterogeneous environments based on a novel data partitioning algorithm involving submodular optimization. Our data partitioning algorithm explicitly accounts for resource heterogeneity across workers while achieving similar class-level feature distribution and maintaining class balance. Based on this algorithm, we develop a distributed SGD framework that ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31807;&#35760;&#65288;BK&#65289;&#30340;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#22312;&#22823;&#27169;&#22411;&#21644;&#39640;&#32500;&#25968;&#25454;&#19978;&#30340;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.00038</link><description>&lt;p&gt;
&#22312;&#23567;&#25104;&#26412;&#19978;&#23545;&#22823;&#27169;&#22411;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Optimization on Large Model at Small Cost. (arXiv:2210.00038v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31807;&#35760;&#65288;BK&#65289;&#30340;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#22312;&#22823;&#27169;&#22411;&#21644;&#39640;&#32500;&#25968;&#25454;&#19978;&#30340;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20248;&#21270;&#26159;&#23398;&#20064;&#20934;&#30830;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#20934;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36880;&#26679;&#26412;&#26799;&#24230;&#20462;&#21098;&#65292;DP&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#29616;&#26377;&#30340;DP&#23454;&#29616;&#27604;&#26631;&#20934;&#65288;&#38750;&#31169;&#26377;&#65289;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#39640;2-1000&#20493;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31807;&#35760;&#65288;BK&#65289;&#25216;&#26415;&#65292;&#23427;&#23454;&#29616;&#20102;&#29616;&#26377;&#30340;DP&#20248;&#21270;&#22120;&#65288;&#20174;&#32780;&#23454;&#29616;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#65289;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#26377;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BK&#20351;&#24471;&#23545;&#22823;&#22411;&#27169;&#22411;&#21644;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;DP&#35757;&#32451;&#30340;&#36895;&#24230;&#21644;&#33410;&#30465;&#20869;&#23384;&#19982;&#26631;&#20934;&#35757;&#32451;&#30456;&#24403;&#65292;&#32780;&#20197;&#21069;&#30340;DP&#31639;&#27861;&#21487;&#33021;&#22240;&#20869;&#23384;&#38169;&#35823;&#32780;&#20302;&#25928;&#25110;&#26080;&#27861;&#35757;&#32451;&#12290;&#36890;&#36807;&#22797;&#26434;&#24230;&#20998;&#26512;&#21644;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;BK&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2-1000X more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#35299;&#32806;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20154;&#21338;&#24328;&#20013;&#33021;&#22815;&#20351;&#27599;&#20010;&#29609;&#23478;&#30340;&#35302;&#21457;&#21518;&#24724;&#25353;$O(\log T)$&#22686;&#38271;&#65292;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#24182;&#19988;&#26500;&#24314;&#21033;&#29992;&#20102;&#19968;&#20010;&#26356;&#20026;&#19968;&#33324;&#30340;&#30001;&#20855;&#26377;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26377;&#29702;&#20989;&#25968;&#23548;&#20986;&#30340;&#19981;&#21160;&#28857;&#32467;&#26524;&#65292;&#20197;&#21450;&#19968;&#20010;&#20984;&#21253;&#30340;&#26356;&#32454;&#33268;&#30340;&#21518;&#24724;&#30005;&#36335;&#65292;&#20445;&#30041;&#20102;RVU&#21512;&#36866;&#24615;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.09747</link><description>&lt;p&gt;
&#21338;&#24328;&#26641;&#20013;&#30340;NEAR-OPTIMAL PHI-REGRET&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal $\Phi$-Regret Learning in Extensive-Form Games. (arXiv:2208.09747v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#35299;&#32806;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20154;&#21338;&#24328;&#20013;&#33021;&#22815;&#20351;&#27599;&#20010;&#29609;&#23478;&#30340;&#35302;&#21457;&#21518;&#24724;&#25353;$O(\log T)$&#22686;&#38271;&#65292;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#24182;&#19988;&#26500;&#24314;&#21033;&#29992;&#20102;&#19968;&#20010;&#26356;&#20026;&#19968;&#33324;&#30340;&#30001;&#20855;&#26377;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26377;&#29702;&#20989;&#25968;&#23548;&#20986;&#30340;&#19981;&#21160;&#28857;&#32467;&#26524;&#65292;&#20197;&#21450;&#19968;&#20010;&#20984;&#21253;&#30340;&#26356;&#32454;&#33268;&#30340;&#21518;&#24724;&#30005;&#36335;&#65292;&#20445;&#30041;&#20102;RVU&#21512;&#36866;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#20154;&#23436;&#32654;&#22238;&#24518;&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#20013;&#24314;&#31435;&#20102;&#26377;&#25928;&#21644;&#35299;&#32806;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#20197;&#20415;&#27599;&#20010;&#29609;&#23478;&#30340;&#35302;&#21457;&#21518;&#24724;&#22312;T&#27425;&#28216;&#25103;&#37325;&#22797;&#21518;&#25353;$O(\log T)$&#22686;&#38271;&#12290;&#36825;&#30456;&#23545;&#20110;&#20808;&#21069;&#24050;&#30693;&#30340;&#35302;&#21457;&#21518;&#24724;&#36793;&#30028;$O(T^{1/4})$&#26377;&#25351;&#25968;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#35299;&#20915;&#20102;Bai&#31561;&#20154;&#65288;2022&#65289;&#25552;&#20986;&#30340;&#19968;&#20010;&#26368;&#36817;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#20316;&#20026;&#30452;&#25509;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#20445;&#35777;&#20197;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#24230;$\frac{\log T}{T}$&#25910;&#25947;&#21040;&#24191;&#27867;&#24418;&#24335;&#30340;&#30456;&#20851;&#22343;&#34913;&#21644;&#31895;&#30053;&#30340;&#30456;&#20851;&#22343;&#34913;&#12290;&#22312;&#29616;&#26377;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#26500;&#24314;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26356;&#20026;&#19968;&#33324;&#30340;&#30001;&#20855;&#26377;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26377;&#29702;&#20989;&#25968;&#23548;&#20986;&#30340;&#19981;&#21160;&#28857;&#32467;&#26524;&#65292;&#36825;&#26159;&#25105;&#20204;&#20026;&#35302;&#21457;&#20559;&#24046;&#20989;&#25968;&#65288;&#31895;&#30053;&#30340;&#65289;&#22266;&#23450;&#28857;&#25152;&#24314;&#31435;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26500;&#24314;&#21033;&#29992;&#20102;&#20984;&#21253;&#30340;&#26356;&#32454;&#33268;&#30340;&#21518;&#24724;&#30005;&#36335;&#65292;&#19982;&#20808;&#21069;&#30340;&#20445;&#35777;&#19981;&#21516;&#65292;&#23427;&#20445;&#30041;&#20102;RVU&#21512;&#36866;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we establish efficient and uncoupled learning dynamics so that, when employed by all players in multiplayer perfect-recall imperfect-information extensive-form games, the trigger regret of each player grows as $O(\log T)$ after $T$ repetitions of play. This improves exponentially over the prior best known trigger-regret bound of $O(T^{1/4})$, and settles a recent open question by Bai et al. (2022). As an immediate consequence, we guarantee convergence to the set of extensive-form correlated equilibria and coarse correlated equilibria at a near-optimal rate of $\frac{\log T}{T}$.  Building on prior work, at the heart of our construction lies a more general result regarding fixed points deriving from rational functions with polynomial degree, a property that we establish for the fixed points of (coarse) trigger deviation functions. Moreover, our construction leverages a refined regret circuit for the convex hull, which -- unlike prior guarantees -- preserves the RVU proper
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#36890;&#36807;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#21644;&#22788;&#29702;&#25928;&#24212;&#65292;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.07898</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#21327;&#21516;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Collaborative causal inference on distributed data. (arXiv:2208.07898v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#36890;&#36807;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#21644;&#22788;&#29702;&#25928;&#24212;&#65292;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#39318;&#20808;&#65292;&#26412;&#22320;&#21508;&#26041;&#20174;&#31169;&#26377;&#25968;&#25454;&#20013;&#26500;&#24314;&#38477;&#32500;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#20182;&#20204;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#21518;&#65292;&#20174;&#20849;&#20139;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#12290;&#26368;&#21518;&#65292;&#20174;&#20542;&#21521;&#20998;&#25968;&#20013;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#20943;&#23569;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#38543;&#26426;&#35823;&#24046;&#12290;&#36890;&#36807;&#22312;&#20154;&#24037;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#27604;&#21333;&#29420;&#20998;&#26512;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of technologies for causal inference with the privacy preservation of distributed data has attracted considerable attention in recent years. To address this issue, we propose a data collaboration quasi-experiment (DC-QE) that enables causal inference from distributed data with privacy preservation. In our method, first, local parties construct dimensionality-reduced intermediate representations from the private data. Second, they share intermediate representations, instead of private data for privacy preservation. Third, propensity scores were estimated from the shared intermediate representations. Finally, the treatment effects were estimated from propensity scores. Our method can reduce both random errors and biases, whereas existing methods can only reduce random errors in the estimation of treatment effects. Through numerical experiments on both artificial and real-world data, we confirmed that our method can lead to better estimation results than individual analyse
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;A*Net&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.04798</link><description>&lt;p&gt;
A*Net&#65306;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs. (arXiv:2206.04798v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;A*Net&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#23545;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#29702;&#19968;&#30452;&#30001;&#23884;&#20837;&#26041;&#27861;&#20027;&#23548;&#12290;&#34429;&#28982;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#20855;&#26377;&#23884;&#20837;&#26041;&#27861;&#25152;&#32570;&#20047;&#30340;&#24402;&#32435;&#33021;&#21147;&#65292;&#20294;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#25351;&#25968;&#32423;&#36335;&#24452;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#36335;&#24452;&#26041;&#27861;&#12290;&#21463;&#21040;A*&#31639;&#27861;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;A*Net&#23398;&#20064;&#20102;&#19968;&#20010;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#36793;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#36873;&#25321;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#27604;&#20363;&#21487;&#20197;&#25351;&#23450;&#65292;&#20197;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#20256;&#23548;&#24615;&#21644;&#24402;&#32435;&#24615;&#30693;&#35782;&#22270;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;A*Net&#22312;&#20165;&#35775;&#38382;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;10%&#33410;&#28857;&#21644;10%&#36793;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#22522;&#20110;&#36335;&#24452;&#26041;&#27861;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#30334;&#19975;&#32423;&#25968;&#25454;&#38598;ogbl-wikikg2&#19978;&#65292;A*Net&#19981;&#20165;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#36824;&#23454;&#29616;&#20102;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A*Net not only achieves a new state-of-the-art result, but also converges 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2205.14704</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#36981;&#24490;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#24335;&#65307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#36951;&#24536;&#21644;&#26426;&#26800;&#35760;&#24518;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RetroPrompt&#65292;&#26088;&#22312;&#20174;&#35760;&#24518;&#20013;&#23558;&#30693;&#35782;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#19982;&#20256;&#32479;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;RetroPrompt&#20174;&#35757;&#32451;&#23454;&#20363;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#30693;&#35782;&#24211;&#65292;&#24182;&#22312;&#36755;&#20837;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#26045;&#26816;&#32034;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#29992;&#20110;&#22686;&#24378;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;RetroPrompt&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21508;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#27604;&#36739;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2205.04766</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainable Deep Learning Methods in Medical Image Classification: A Survey. (arXiv:2205.04766v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04766
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21508;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#27604;&#36739;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20854;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#24456;&#38590;&#22312;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#24471;&#21040;&#37319;&#29992;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#38656;&#27714;&#65292;&#36827;&#32780;&#24418;&#25104;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;XAI&#22312;&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#25991;&#26412;&#12289;&#22522;&#20110;&#31034;&#20363;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#30340;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#19968;&#32452;&#22522;&#20110;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box-ness of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation-based methods. Finally, the major 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26681;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#35760;&#24518;&#20449;&#24687;&#25512;&#26029;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2205.02355</link><description>&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#20316;&#20026;&#24320;&#20070;&#32771;&#35797;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02355
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26681;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#35760;&#24518;&#20449;&#24687;&#25512;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#20851;&#31995;&#25277;&#21462;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#26080;&#27861;&#25512;&#24191;&#21040;&#37027;&#20123;&#32597;&#35265;&#25110;&#22256;&#38590;&#30340;&#27169;&#24335;&#20013;&#12290;&#25105;&#20204;&#23558;&#20851;&#31995;&#25277;&#21462;&#35270;&#20026;&#19968;&#31181;&#24320;&#25918;&#24335;&#32771;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#29992;&#20110;&#26816;&#32034;&#22522;&#20110;&#25552;&#31034;&#30340;&#23454;&#20363;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#20851;&#31995;&#26631;&#31614;&#20316;&#20026;&#35760;&#24518;&#30340;&#38190;&#20540;&#23545;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#22522;&#20110;PLM&#30340;&#22522;&#26412;&#36755;&#20986;&#19982;&#23384;&#20648;&#24211;&#19978;&#30340;&#38750;&#21442;&#25968;&#26368;&#36817;&#37051;&#20998;&#24067;&#26469;&#25512;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#30340;&#39640;&#32500;McKean-Vlasov&#27491;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#27714;&#35299;&#20855;&#26377;&#26174;&#24335;&#31995;&#25968;&#20989;&#25968;&#30340;&#26631;&#20934;FBSDEs&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;MV-FBSDEs&#30340;&#27169;&#22411;&#31995;&#25968;&#65292;&#21487;&#20197;&#35299;&#20915;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#23436;&#20840;&#20998;&#24067;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.11924</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#19968;&#33324;&#20998;&#24067;&#20381;&#36182;&#24615;&#30340;&#39640;&#32500;McKean-Vlasov&#27491;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning High-Dimensional McKean-Vlasov Forward-Backward Stochastic Differential Equations with General Distribution Dependence. (arXiv:2204.11924v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#30340;&#39640;&#32500;McKean-Vlasov&#27491;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#27714;&#35299;&#20855;&#26377;&#26174;&#24335;&#31995;&#25968;&#20989;&#25968;&#30340;&#26631;&#20934;FBSDEs&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;MV-FBSDEs&#30340;&#27169;&#22411;&#31995;&#25968;&#65292;&#21487;&#20197;&#35299;&#20915;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#23436;&#20840;&#20998;&#24067;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22343;&#22330;&#25511;&#21046;&#21644;&#22343;&#22330;&#21338;&#24328;&#20013;&#65292;&#35299;&#20915;&#30456;&#24212;&#30340;McKean-Vlasov&#27491;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(MV-FBSDEs)&#26159;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#20165;&#20381;&#36182;&#20110;&#26399;&#26395;&#25110;&#20854;&#20182;&#30697;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#20915;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#23436;&#20840;&#20998;&#24067;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#30340;MV-FBSDEs&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#34394;&#25311;&#21338;&#24328;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#21453;&#22797;&#27714;&#35299;&#20855;&#26377;&#26174;&#24335;&#31995;&#25968;&#20989;&#25968;&#30340;&#26631;&#20934;FBSDEs&#12290;&#36825;&#20123;&#31995;&#25968;&#20989;&#25968;&#29992;&#20110;&#36817;&#20284;&#20855;&#26377;&#23436;&#20840;&#20998;&#24067;&#20381;&#36182;&#24615;&#30340;MV-FBSDEs&#30340;&#27169;&#22411;&#31995;&#25968;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20174;&#19978;&#19968;&#27425;&#36845;&#20195;&#30340;FBSDE&#35299;&#27169;&#25311;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35299;&#20915;&#21478;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#26469;&#26356;&#26032;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#27714;&#35299;&#26631;&#20934;BSDEs&#24182;&#36827;&#34892;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
One of the core problems in mean-field control and mean-field games is to solve the corresponding McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs). Most existing methods are tailored to special cases in which the mean-field interaction only depends on expectation or other moments and thus inadequate to solve problems when the mean-field interaction has full distribution dependence.  In this paper, we propose a novel deep learning method for computing MV-FBSDEs with a general form of mean-field interactions. Specifically, built on fictitious play, we recast the problem into repeatedly solving standard FBSDEs with explicit coefficient functions. These coefficient functions are used to approximate the MV-FBSDEs' model coefficients with full distribution dependence, and are updated by solving another supervising learning problem using training data simulated from the last iteration's FBSDE solutions. We use deep neural networks to solve standard BSDEs and approx
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#26377;&#25928;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.04392</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#26377;&#25928;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#25110;&#28436;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#25628;&#32034;&#31163;&#25955;&#25110;&#36830;&#32493;&#25552;&#31034;&#25110;&#20248;&#21270;&#35821;&#35328;&#34920;&#36798;&#32773;&#65292;&#20294;&#23545;&#20110;&#28436;&#31034;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#28436;&#31034;&#31034;&#20363;&#23545;&#20110;&#26368;&#32456;&#30340;&#25552;&#31034;&#35843;&#20248;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25554;&#25300;&#12289;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#65292;&#23427;&#19981;&#38656;&#35201;&#36827;&#34892;&#28436;&#31034;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#65306;&#65288;i&#65289;&#23884;&#20837;&#21040;&#20219;&#20309;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#20013;&#65307;&#65288;ii&#65289;&#25193;&#23637;&#21040;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#24191;&#27867;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#22312;16&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;LM-BFF&#21644;P-tuning&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited. Concretely, the demonstration examples are crucial for an excellent final performance of prompt-tuning. In this paper, we propose a novel pluggable, extensible, and efficient approach named contrastive demonstration tuning, which is free of demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged into any previous prompt-tuning approaches; (ii) Extended to widespread classification tasks with a large number of categories. Experimental results on 16 datasets illustrate that our method integrated with previous approaches LM-BFF and P-tuning can yield better performance. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#20266;&#36870;&#65292;&#21253;&#25324;&#20854;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#26465;&#20214;&#20197;&#21450;&#24615;&#36136;&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#19968;&#20123;&#20247;&#25152;&#21608;&#30693;&#30340;&#19981;&#21487;&#36870;&#38750;&#32447;&#24615;&#31639;&#23376;PI&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#23567;&#27874;&#38408;&#20540;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2111.10755</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#20266;&#36870;&#30340;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Theoretical Foundations for Pseudo-Inversion of Nonlinear Operators. (arXiv:2111.10755v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#20266;&#36870;&#65292;&#21253;&#25324;&#20854;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#26465;&#20214;&#20197;&#21450;&#24615;&#36136;&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#19968;&#20123;&#20247;&#25152;&#21608;&#30693;&#30340;&#19981;&#21487;&#36870;&#38750;&#32447;&#24615;&#31639;&#23376;PI&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#23567;&#27874;&#38408;&#20540;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Moore-Penrose&#20266;&#36870;&#22312;&#29289;&#29702;&#23398;&#12289;&#32479;&#35745;&#23398;&#21644;&#21508;&#20010;&#24037;&#31243;&#39046;&#22495;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#38750;&#32447;&#24615;&#31639;&#23376;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#20266;&#36870;&#65292;&#24191;&#20041;&#22320;&#23450;&#20041;&#20102;&#36825;&#20010;&#27010;&#24565;&#65292;&#39318;&#20808;&#23545;&#20110;&#19968;&#33324;&#38598;&#21512;&#65292;&#28982;&#21518;&#23545;&#20110;&#36171;&#33539;&#31354;&#38388;&#36827;&#34892;&#20102;&#32454;&#21270;&#12290;&#24403;&#31639;&#23376;&#26159;&#30697;&#38453;&#26102;&#65292;&#36171;&#33539;&#31354;&#38388;&#30340;PI&#20135;&#29983;&#20102;Moore-Penrose&#20266;&#36870;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;PI&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#30340;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#20854;&#24615;&#36136;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#22914;&#36830;&#32493;&#24615;&#12289;&#31639;&#23376;&#32452;&#21512;&#21644;&#25237;&#24433;&#31639;&#23376;&#30340;&#20215;&#20540;&#31561;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#20247;&#25152;&#21608;&#30693;&#30340;&#19981;&#21487;&#36870;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;PI&#32473;&#20986;&#20102;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#20363;&#22914;&#30828;/&#36719;&#38408;&#20540;&#21644;ReLU&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#31070;&#32463;&#23618;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#23567;&#27874;&#38408;&#20540;&#26377;&#20851;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Moore-Penrose inverse is widely used in physics, statistics, and various fields of engineering. It captures well the notion of inversion of linear operators in the case of overcomplete data. In data science, nonlinear operators are extensively used. In this paper we characterize the fundamental properties of a pseudo-inverse (PI) for nonlinear operators.  The concept is defined broadly. First for general sets, and then a refinement for normed spaces. The PI for normed spaces yields the Moore-Penrose inverse when the operator is a matrix. We present conditions for existence and uniqueness of a PI and establish theoretical results investigating its properties, such as continuity, its value for operator compositions and projection operators, and others. Analytic expressions are given for the PI of some well-known, non-invertible, nonlinear operators, such as hard- or soft-thresholding and ReLU. Finally, we analyze a neural layer and discuss relations to wavelet thresholding.
&lt;/p&gt;</description></item><item><title>&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#32676;&#36873;&#25321;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#22238;&#24402;&#20989;&#25968;&#36229;&#36807;&#39044;&#35774;&#38408;&#20540;&#30340;&#29305;&#24449;&#31354;&#38388;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#23450;&#20102;&#22312;&#26679;&#26412;&#35268;&#27169;&#21644;&#31867;&#22411;I&#38169;&#35823;&#27010;&#29575;&#19978;&#36951;&#25022;&#30340;&#26368;&#20339;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2109.01077</link><description>&lt;p&gt;
&#26368;&#20339;&#23376;&#32676;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Optimal subgroup selection. (arXiv:2109.01077v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01077
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#32676;&#36873;&#25321;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#22238;&#24402;&#20989;&#25968;&#36229;&#36807;&#39044;&#35774;&#38408;&#20540;&#30340;&#29305;&#24449;&#31354;&#38388;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#23450;&#20102;&#22312;&#26679;&#26412;&#35268;&#27169;&#21644;&#31867;&#22411;I&#38169;&#35823;&#27010;&#29575;&#19978;&#36951;&#25022;&#30340;&#26368;&#20339;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#30475;&#21040;&#29305;&#24449;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#26377;&#36259;&#30340;&#34892;&#20026;&#21306;&#22495;&#65292;&#20294;&#19981;&#28165;&#26970;&#36825;&#20123;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#26159;&#21542;&#22312;&#24635;&#20307;&#27700;&#24179;&#19978;&#26377;&#25152;&#21453;&#26144;&#12290;&#38024;&#23545;&#22238;&#24402;&#35774;&#32622;&#65292;&#25105;&#20204;&#32771;&#34385;&#23376;&#32676;&#36873;&#25321;&#25361;&#25112;&#65292;&#21363;&#35782;&#21035;&#19968;&#20010;&#29305;&#24449;&#31354;&#38388;&#30340;&#21306;&#22495;&#65292;&#22312;&#35813;&#21306;&#22495;&#19978;&#65292;&#22238;&#24402;&#20989;&#25968;&#36229;&#36807;&#20102;&#39044;&#35774;&#30340;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#23547;&#25214;&#19968;&#20010;&#20302;&#22797;&#26434;&#24230;&#12289;&#25968;&#25454;&#30456;&#20851;&#30340;&#36873;&#25321;&#38598;&#65292;&#22312;&#36825;&#20010;&#36873;&#25321;&#38598;&#19978;&#65292;&#22238;&#24402;&#20989;&#25968;&#26377;&#33267;&#23569;&#19982;&#38408;&#20540;&#19968;&#26679;&#22823;&#30340;&#27010;&#29575;&#65292;&#21516;&#26102;&#35201;&#27714;&#35813;&#21306;&#22495;&#22312;&#36793;&#32536;&#29305;&#24449;&#20998;&#24067;&#19979;&#30340;&#36136;&#37327;&#23613;&#21487;&#33021;&#22823;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#36951;&#25022;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#23450;&#20102;&#36951;&#25022;&#22312;&#26679;&#26412;&#35268;&#27169;&#21644;&#31532;&#19968;&#31867;&#38169;&#35823;&#27010;&#29575;&#19978;&#30340;&#26368;&#20248;&#20540;&#12290;&#36825;&#20010;&#26368;&#20248;&#20540;&#28041;&#21450;&#21040;&#26679;&#26412;&#22823;&#23567;&#21644;&#31867;&#22411;I&#38169;&#35823;&#27010;&#29575;&#30340;&#24494;&#22937;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical trials and other applications, we often see regions of the feature space that appear to exhibit interesting behaviour, but it is unclear whether these observed phenomena are reflected at the population level. Focusing on a regression setting, we consider the subgroup selection challenge of identifying a region of the feature space on which the regression function exceeds a pre-determined threshold. We formulate the problem as one of constrained optimisation, where we seek a low-complexity, data-dependent selection set on which, with a guaranteed probability, the regression function is uniformly at least as large as the threshold; subject to this constraint, we would like the region to contain as much mass under the marginal feature distribution as possible. This leads to a natural notion of regret, and our main contribution is to determine the minimax optimal rate for this regret in both the sample size and the Type I error probability. The rate involves a delicate interpla
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#65292;&#38543;&#30528;&#26356;&#26032;&#27425;&#25968;&#36235;&#36817;&#26080;&#31351;&#22823;&#65292;&#24102;&#26377;&#34920;&#26684;&#21442;&#25968;&#21270;&#30340;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#25910;&#25947;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34892;&#20026;&#21644;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2108.08655</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;ODE&#26497;&#38480;&#20840;&#23616;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. (arXiv:2108.08655v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#65292;&#38543;&#30528;&#26356;&#26032;&#27425;&#25968;&#36235;&#36817;&#26080;&#31351;&#22823;&#65292;&#24102;&#26377;&#34920;&#26684;&#21442;&#25968;&#21270;&#30340;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#25910;&#25947;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34892;&#20026;&#21644;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#22312;&#32447;&#25968;&#25454;&#26679;&#26412;&#30340;&#21040;&#26469;&#65292;&#20854;&#22312;&#25968;&#23398;&#19978;&#20998;&#26512;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25968;&#25454;&#26679;&#26412;&#30340;&#20998;&#24067;&#38543;&#30528;&#27169;&#22411;&#30340;&#26356;&#26032;&#32780;&#21160;&#24577;&#21464;&#21270;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#22797;&#26434;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26102;&#38388;&#37325;&#32553;&#25918;&#19979;&#65292;&#24102;&#26377;&#34920;&#26684;&#21442;&#25968;&#21270;&#30340;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#26356;&#26032;&#27425;&#25968;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#35777;&#26126;&#39318;&#20808;&#22312;&#22266;&#23450;&#30340;&#28436;&#21592;&#31574;&#30053;&#19979;&#24314;&#31435;&#25968;&#25454;&#26679;&#26412;&#30340;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#27850;&#26494;&#26041;&#31243;&#65292;&#25105;&#20204;&#35777;&#26126;&#38543;&#30528;&#26356;&#26032;&#27425;&#25968;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#25968;&#25454;&#26679;&#26412;&#20851;&#20110;&#19968;&#31181;&#21160;&#24577;&#27010;&#29575;&#27979;&#24230;&#30340;&#27874;&#21160;&#22312;&#28436;&#21464;&#30340;&#28436;&#21592;&#27169;&#22411;&#30340;&#20989;&#25968;&#19979;&#28040;&#22833;&#12290;&#19968;&#26086;&#24471;&#21040;ODE&#26497;&#38480;&#65292;&#25105;&#20204;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#20998;&#26512;&#30740;&#31350;&#20854;&#25910;&#25947;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-critic algorithms are widely used in reinforcement learning, but are challenging to mathematically analyse due to the online arrival of non-i.i.d. data samples. The distribution of the data samples dynamically changes as the model is updated, introducing a complex feedback loop between the data distribution and the reinforcement learning algorithm. We prove that, under a time rescaling, the online actor-critic algorithm with tabular parametrization converges to an ordinary differential equation (ODE) as the number of updates becomes large. The proof first establishes the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the data samples around a dynamic probability measure, which is a function of the evolving actor model, vanish as the number of updates become large. Once the ODE limit has been derived, we study its convergence properties using a two time-scale analysis which asymptotically de-co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#31354;&#38388;&#21644;&#35889;&#22495;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#32039;&#23494;&#20851;&#32852;&#21508;&#33258;&#22495;&#20869;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2107.10234</link><description>&lt;p&gt;
&#36328;&#36234;&#31354;&#38388;&#21644;&#20809;&#35889;&#22495;&#30340;&#40511;&#27807;&#65306;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks. (arXiv:2107.10234v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.10234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#31354;&#38388;&#21644;&#35889;&#22495;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#32039;&#23494;&#20851;&#32852;&#21508;&#33258;&#22495;&#20869;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26088;&#22312;&#22788;&#29702;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;GNN&#26159;&#20351;&#29992;&#19981;&#21516;&#30340;&#29702;&#35770;&#21019;&#24314;&#30340;&#65292;&#22240;&#27492;&#26080;&#27861;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#23427;&#20204;&#30340;&#20869;&#22312;&#36830;&#25509;&#20851;&#31995;&#20851;&#27880;&#29978;&#23569;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#24314;&#31435;&#19968;&#20010;&#22522;&#20110;&#35889;&#22270;&#21644;&#36817;&#20284;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#38598;&#25104;&#22522;&#20110;&#31354;&#38388;&#21644;&#35889;&#22495;&#30340;GNN&#65292;&#24182;&#32039;&#23494;&#20851;&#32852;&#21508;&#33258;&#22495;&#20869;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning's performance has been extensively recognized recently. Graph neural networks (GNNs) are designed to deal with graph-structural data that classical deep learning does not easily manage. Since most GNNs were created using distinct theories, direct comparisons are impossible. Prior research has primarily concentrated on categorizing existing models, with little attention paid to their intrinsic connections. The purpose of this study is to establish a unified framework that integrates GNNs based on spectral graph and approximation theory. The framework incorporates a strong integration between spatial- and spectral-based GNNs while tightly associating approaches that exist within each respective domain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#22810;&#32500;&#22797;&#26434;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;ODE&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#32500;&#24230;&#28798;&#38590;&#21644;&#22797;&#26434;ODE&#32467;&#26500;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#21644;&#36866;&#24403;&#36873;&#25321;&#32593;&#32476;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#34987;&#35777;&#26126;&#26159;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2106.03591</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#22810;&#32500;&#22797;&#26434;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#22122;&#22768;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Calibrating multi-dimensional complex ODE from noisy data via deep neural networks. (arXiv:2106.03591v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#22810;&#32500;&#22797;&#26434;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;ODE&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#32500;&#24230;&#28798;&#38590;&#21644;&#22797;&#26434;ODE&#32467;&#26500;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#21644;&#36866;&#24403;&#36873;&#25321;&#32593;&#32476;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#34987;&#35777;&#26126;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#12289;&#24037;&#31243;&#12289;&#37329;&#34701;&#12289;&#29289;&#29702;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#21160;&#24577;&#12290;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#26657;&#20934;&#22797;&#26434;ODE&#31995;&#32479;&#36890;&#24120;&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36793;&#30028;&#26680;&#26041;&#27861;&#25552;&#21462;&#21435;&#22122;&#25968;&#25454;&#21450;&#20854;&#39640;&#38454;&#23548;&#25968;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#36755;&#20837;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31232;&#30095;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;ODE&#31995;&#32479;&#65292;&#32780;&#19981;&#21463;&#32500;&#24230;&#28798;&#38590;&#21644;&#22797;&#26434;ODE&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;&#24403;ODE&#20855;&#26377;&#19968;&#33324;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27599;&#20010;&#27169;&#22359;&#32452;&#20214;&#20165;&#28041;&#21450;&#23569;&#37327;&#36755;&#20837;&#21464;&#37327;&#65292;&#24182;&#19988;&#32593;&#32476;&#26550;&#26500;&#34987;&#36866;&#24403;&#36873;&#25321;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#19968;&#33268;&#30340;&#12290;&#29702;&#35770;&#24615;&#36136;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#24471;&#21040;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ordinary differential equations (ODEs) are widely used to model complex dynamics that arises in biology, chemistry, engineering, finance, physics, etc. Calibration of a complicated ODE system using noisy data is generally very difficult. In this work, we propose a two-stage nonparametric approach to address this problem. We first extract the de-noised data and their higher order derivatives using boundary kernel method, and then feed them into a sparsely connected deep neural network with ReLU activation function. Our method is able to recover the ODE system without being subject to the curse of dimensionality and complicated ODE structure. When the ODE possesses a general modular structure, with each modular component involving only a few input variables, and the network architecture is properly chosen, our method is proven to be consistent. Theoretical properties are corroborated by an extensive simulation study that demonstrates the validity and effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25512;&#24191;&#20102;&#22871;&#32034;&#26041;&#27861;&#22312;&#39640;&#26031;&#30456;&#20851;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#8220;&#22266;&#23450;&#35774;&#35745;&#8221;&#27169;&#22411;&#26469;&#31934;&#30830;&#21051;&#30011;&#22871;&#32034;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2007.13716</link><description>&lt;p&gt;
&#24102;&#26377;&#19968;&#33324;&#39640;&#26031;&#35774;&#35745;&#30340;&#22871;&#32034;&#26041;&#27861;&#21450;&#20854;&#22312;&#20551;&#35774;&#26816;&#39564;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Lasso with general Gaussian designs with applications to hypothesis testing. (arXiv:2007.13716v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.13716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25512;&#24191;&#20102;&#22871;&#32034;&#26041;&#27861;&#22312;&#39640;&#26031;&#30456;&#20851;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#8220;&#22266;&#23450;&#35774;&#35745;&#8221;&#27169;&#22411;&#26469;&#31934;&#30830;&#21051;&#30011;&#22871;&#32034;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22871;&#32034;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#32500;&#22238;&#24402;&#26041;&#27861;&#65292;&#24403;&#33258;&#21464;&#37327;&#30340;&#25968;&#37327;$p$&#19982;&#35266;&#27979;&#25968;&#37327;$n$&#30456;&#21516;&#25110;&#26356;&#22823;&#26102;&#65292;&#29616;&#22312;&#36890;&#24120;&#20351;&#29992;&#12290;&#30001;&#20110;&#20004;&#20010;&#22522;&#26412;&#21407;&#22240;&#65292;&#32463;&#20856;&#30340;&#28176;&#36817;&#27491;&#24577;&#29702;&#35770;&#19981;&#36866;&#29992;&#20110;&#35813;&#27169;&#22411;&#65306;(1) &#27491;&#21017;&#21270;&#39118;&#38505;&#26159;&#38750;&#20809;&#28369;&#30340;&#65307;(2) &#20272;&#35745;&#22120;$\widehat{\boldsymbol{\theta}}$&#19982;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;$\boldsymbol{\theta}^*$&#20043;&#38388;&#30340;&#36317;&#31163;&#19981;&#33021;&#24573;&#30053;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#29702;&#35770;&#30340;&#22522;&#30784;&#8212;&#8212;&#26631;&#20934;&#30340;&#25668;&#21160;&#35770;&#35777;&#22833;&#36133;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;$n$&#21644;$p$&#37117;&#24456;&#22823;&#19988;$n/p$&#20026;1&#38454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#25551;&#36848;&#22871;&#32034;&#20272;&#35745;&#22120;&#12290;&#36825;&#20010;&#25551;&#36848;&#39318;&#20808;&#26159;&#22312;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#33258;&#21464;&#37327;&#30340;&#39640;&#26031;&#35774;&#35745;&#24773;&#20917;&#19979;&#24471;&#21040;&#30340;&#65306;&#25105;&#20204;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#38750;&#22855;&#24322;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#39640;&#26031;&#30456;&#20851;&#35774;&#35745;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#8220;&#22266;&#23450;&#35774;&#35745;&#8221;&#27169;&#22411;&#26469;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lasso is a method for high-dimensional regression, which is now commonly used when the number of covariates $p$ is of the same order or larger than the number of observations $n$. Classical asymptotic normality theory does not apply to this model due to two fundamental reasons: $(1)$ The regularized risk is non-smooth; $(2)$ The distance between the estimator $\widehat{\boldsymbol{\theta}}$ and the true parameters vector $\boldsymbol{\theta}^*$ cannot be neglected. As a consequence, standard perturbative arguments that are the traditional basis for asymptotic normality fail.  On the other hand, the Lasso estimator can be precisely characterized in the regime in which both $n$ and $p$ are large and $n/p$ is of order one. This characterization was first obtained in the case of Gaussian designs with i.i.d. covariates: here we generalize it to Gaussian correlated designs with non-singular covariance structure. This is expressed in terms of a simpler ``fixed-design'' model. We establish
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;NLP&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#35980;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1910.10683</link><description>&lt;p&gt;
&#25506;&#32034;&#20351;&#29992;&#32479;&#19968;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#22120;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (arXiv:1910.10683v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.10683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;NLP&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#35980;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#20043;&#21069;&#39318;&#20808;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20652;&#29983;&#20102;&#22810;&#31181;&#26041;&#27861;&#12289;&#26041;&#27861;&#35770;&#21644;&#23454;&#36341;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23558;&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;NLP&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#35980;&#12290;&#25105;&#20204;&#23545;&#35768;&#22810;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#30446;&#26631;&#12289;&#26550;&#26500;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#12289;&#36801;&#31227;&#26041;&#27861;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#25506;&#32034;&#30340;&#35265;&#35299;&#19982;&#35268;&#27169;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#8220;&#24222;&#22823;&#24178;&#20928;&#25235;&#21462;&#35821;&#26009;&#24211;&#8221;&#30456;&#32467;&#21512;&#65292;&#22312;&#35768;&#22810;&#28041;&#21450;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#20998;&#31867;&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20419;&#36827;NLP&#39046;&#22495;&#30340;&#26410;&#26469;&#36801;&#31227;&#23398;&#20064;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained mode
&lt;/p&gt;</description></item></channel></rss>