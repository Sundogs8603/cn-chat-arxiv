<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#27169;&#22411;&#36873;&#25321;&#22120;&#65288;EMMS&#65289;&#65292;&#23427;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#22522;&#30784;&#27169;&#22411;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#26631;&#31614;&#26684;&#24335;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#22122;&#22768;&#26631;&#31614;&#23884;&#20837;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21152;&#26435;&#32447;&#24615;&#22238;&#24402;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06262</link><description>&lt;p&gt;
Foundation Model&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#27169;&#22411;&#36873;&#25321;&#22120;
&lt;/p&gt;
&lt;p&gt;
Foundation Model is Efficient Multimodal Multitask Model Selector. (arXiv:2308.06262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#27169;&#22411;&#36873;&#25321;&#22120;&#65288;EMMS&#65289;&#65292;&#23427;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#22522;&#30784;&#27169;&#22411;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#26631;&#31614;&#26684;&#24335;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#22122;&#22768;&#26631;&#31614;&#23884;&#20837;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21152;&#26435;&#32447;&#24615;&#22238;&#24402;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#19981;&#24120;&#35265;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#65306;&#22312;&#32473;&#23450;&#19968;&#32452;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#19981;&#23545;&#23427;&#20204;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#23427;&#20204;&#22312;&#27599;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#27604;&#22914;&#22270;&#20687;&#35782;&#21035;&#12289;&#25351;&#20195;&#12289;&#23383;&#24149;&#29983;&#25104;&#12289;&#35270;&#35273;&#38382;&#31572;&#21644;&#25991;&#23383;&#38382;&#31572;&#12290;&#19968;&#31181;&#34542;&#21147;&#30340;&#26041;&#27861;&#26159;&#23545;&#25152;&#26377;&#27169;&#22411;&#22312;&#25152;&#26377;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20250;&#24102;&#26469;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#19968;&#20123;&#20808;&#36827;&#26041;&#27861;&#37319;&#29992;&#36731;&#37327;&#32423;&#25351;&#26631;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#21333;&#20010;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#30340;&#22330;&#26223;&#20013;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#36873;&#25321;&#22120;&#65288;EMMS&#65289;&#65292;&#23427;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#65292;&#23558;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#21508;&#31181;&#26631;&#31614;&#26684;&#24335;&#65292;&#22914;&#31867;&#21035;&#12289;&#25991;&#26412;&#21644;&#36793;&#30028;&#26694;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#22122;&#22768;&#26631;&#31614;&#23884;&#20837;&#12290;EMMS&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#21152;&#26435;&#32447;&#24615;&#22238;&#24402;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression
&lt;/p&gt;</description></item><item><title>FunnyBirds&#26159;&#19968;&#31181;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#26512;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#23545;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#24178;&#39044;&#65292;&#24182;&#21487;&#20197;&#22312;&#37096;&#20998;&#32423;&#21035;&#19978;&#23545;&#35299;&#37322;&#36827;&#34892;&#35780;&#20272;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.06248</link><description>&lt;p&gt;
FunnyBirds: &#19968;&#31181;&#29992;&#20110;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#30340;&#22522;&#20110;&#37096;&#20998;&#20998;&#26512;&#30340;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods. (arXiv:2308.06248v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06248
&lt;/p&gt;
&lt;p&gt;
FunnyBirds&#26159;&#19968;&#31181;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#26512;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#23545;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#24178;&#39044;&#65292;&#24182;&#21487;&#20197;&#22312;&#37096;&#20998;&#32423;&#21035;&#19978;&#23545;&#35299;&#37322;&#36827;&#34892;&#35780;&#20272;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26088;&#22312;&#25581;&#31034;&#22797;&#26434;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#23613;&#31649;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;XAI&#22266;&#26377;&#22320;&#32570;&#20047;&#30495;&#23454;&#35299;&#37322;&#65292;&#20351;&#20854;&#33258;&#21160;&#35780;&#20272;&#25104;&#20026;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#38598;FunnyBirds&#21450;&#20854;&#20276;&#38543;&#30340;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20801;&#35768;&#36827;&#34892;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#24178;&#39044;&#65292;&#20363;&#22914;&#65292;&#31227;&#38500;&#21333;&#20010;&#29289;&#20307;&#37096;&#20998;&#65292;&#36825;&#26377;&#19977;&#20010;&#37325;&#35201;&#30340;&#21547;&#20041;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#24471;&#33021;&#22815;&#23545;&#37096;&#20998;&#32423;&#21035;&#30340;&#35299;&#37322;&#36827;&#34892;&#20998;&#26512;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#20687;&#32032;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#27604;&#36739;&#21435;&#38500;&#37096;&#20998;&#30340;&#36755;&#20837;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#24212;&#35813;&#21453;&#26144;&#22312;&#35299;&#37322;&#20013;&#30340;&#30495;&#23454;&#37096;&#20998;&#37325;&#35201;&#24615;&#12290;&#31532;&#19977;&#65292;&#36890;&#36807;&#23558;&#21333;&#20010;&#35299;&#37322;&#26144;&#23556;&#21040;&#20849;&#21516;&#30340;&#37096;&#20998;&#37325;&#35201;&#24615;&#31354;&#38388;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#19981;&#21516;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21387;&#32553;&#26679;&#26412;&#21644;&#21015;&#34920;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#23545;&#39640;&#26031;&#20998;&#24067;&#20197;&#21450;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#36827;&#34892;&#20102;&#23398;&#20064;&#19978;&#38480;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#19981;&#21487;&#30693;&#23398;&#20064;&#21644;&#20998;&#24067;&#21464;&#21270;&#25269;&#25239;&#23398;&#20064;&#30340;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06239</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#31169;&#26377;&#20998;&#24067;&#23398;&#20064;&#65306;&#22522;&#20110;&#26679;&#26412;&#21387;&#32553;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Private Distribution Learning with Public Data: The View from Sample Compression. (arXiv:2308.06239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21387;&#32553;&#26679;&#26412;&#21644;&#21015;&#34920;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#23545;&#39640;&#26031;&#20998;&#24067;&#20197;&#21450;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#36827;&#34892;&#20102;&#23398;&#20064;&#19978;&#38480;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#19981;&#21487;&#30693;&#23398;&#20064;&#21644;&#20998;&#24067;&#21464;&#21270;&#25269;&#25239;&#23398;&#20064;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21487;&#20197;&#35775;&#38382;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20844;&#31169;&#23398;&#20064;&#65292;&#23398;&#20064;&#22120;&#34987;&#32473;&#20104;&#26469;&#33258;&#26410;&#30693;&#20998;&#24067;p&#30340;&#23646;&#20110;&#31867;$\mathcal Q$&#30340;&#20844;&#20849;&#26679;&#26412;&#21644;&#31169;&#26377;&#26679;&#26412;&#65292;&#30446;&#26631;&#26159;&#36755;&#20986;&#19968;&#20010;&#23545;p&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#36981;&#23432;&#19982;&#31169;&#26377;&#26679;&#26412;&#30456;&#20851;&#30340;&#38544;&#31169;&#32422;&#26463;&#65288;&#36825;&#37324;&#26159;&#32431;&#24046;&#20998;&#38544;&#31169;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;$\mathcal Q$&#30340;&#20844;&#31169;&#21487;&#23398;&#20064;&#24615;&#19982;$\mathcal Q$&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#20197;&#21450;&#20013;&#38388;&#27010;&#24565;&#8212;&#8212;&#21015;&#34920;&#23398;&#20064;&#30340;&#23384;&#22312;&#24615;&#26377;&#20851;&#12290;&#21033;&#29992;&#36825;&#20010;&#32852;&#31995;&#65306;&#65288;1&#65289;&#36817;&#20284;&#24674;&#22797;&#20102;&#20851;&#20110;$\mathbb R^d$&#19978;&#39640;&#26031;&#20998;&#24067;&#30340;&#20808;&#21069;&#32467;&#26524;&#65307;&#65288;2&#65289;&#24471;&#20986;&#20102;&#26032;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#20219;&#24847;$k$-&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#22312;$\mathbb R^d$&#19978;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#19981;&#21487;&#30693;&#21644;&#20998;&#24067;&#21464;&#21270;&#25269;&#25239;&#23398;&#20064;&#22120;&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;&#20844;&#31169;&#21487;&#23398;&#20064;&#24615;&#30340;&#38381;&#21253;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of private distribution learning with access to public data. In this setup, which we refer to as public-private learning, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.  We show that the public-private learnability of a class $\mathcal Q$ is connected to the existence of a sample compression scheme for $\mathcal Q$, as well as to an intermediate notion we refer to as list learning. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability
&lt;/p&gt;</description></item><item><title>MaxFloodCast&#26159;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#27946;&#27700;&#28153;&#27809;&#28145;&#24230;&#21644;&#35299;&#35835;&#24433;&#21709;&#22240;&#32032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30340;&#27700;&#21160;&#21147;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#39044;&#27979;&#23792;&#20540;&#27946;&#27700;&#28153;&#27809;&#28145;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#38752;&#24615;&#12290;&#23427;&#36824;&#26174;&#31034;&#20102;&#22312;&#25903;&#25345;&#27946;&#27700;&#31649;&#29702;&#21644;&#24212;&#24613;&#34892;&#21160;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#24110;&#21161;&#20915;&#31574;&#32773;&#21046;&#23450;&#27946;&#27700;&#38450;&#27835;&#31574;&#30053;&#21644;&#20248;&#20808;&#32771;&#34385;&#20020;&#30028;&#35774;&#26045;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#30740;&#31350;&#20854;&#20182;&#27969;&#22495;&#38477;&#38632;&#23545;&#27946;&#27700;&#26292;&#38706;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.06228</link><description>&lt;p&gt;
MaxFloodCast: &#29992;&#20110;&#39044;&#27979;&#23792;&#20540;&#28153;&#27809;&#28145;&#24230;&#21644;&#35299;&#35835;&#24433;&#21709;&#22240;&#32032;&#30340;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features. (arXiv:2308.06228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06228
&lt;/p&gt;
&lt;p&gt;
MaxFloodCast&#26159;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#27946;&#27700;&#28153;&#27809;&#28145;&#24230;&#21644;&#35299;&#35835;&#24433;&#21709;&#22240;&#32032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30340;&#27700;&#21160;&#21147;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#39044;&#27979;&#23792;&#20540;&#27946;&#27700;&#28153;&#27809;&#28145;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#38752;&#24615;&#12290;&#23427;&#36824;&#26174;&#31034;&#20102;&#22312;&#25903;&#25345;&#27946;&#27700;&#31649;&#29702;&#21644;&#24212;&#24613;&#34892;&#21160;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#24110;&#21161;&#20915;&#31574;&#32773;&#21046;&#23450;&#27946;&#27700;&#38450;&#27835;&#31574;&#30053;&#21644;&#20248;&#20808;&#32771;&#34385;&#20020;&#30028;&#35774;&#26045;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#30740;&#31350;&#20854;&#20182;&#27969;&#22495;&#38477;&#38632;&#23545;&#27946;&#27700;&#26292;&#38706;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#12289;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#20449;&#24687;&#23545;&#20110;&#20915;&#31574;&#32773;&#12289;&#24212;&#24613;&#31649;&#29702;&#20154;&#21592;&#21644;&#22522;&#30784;&#35774;&#26045;&#36816;&#33829;&#21830;&#22312;&#27946;&#28798;&#20107;&#20214;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#28436;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;MaxFloodCast&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21704;&#37324;&#26031;&#21439;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27700;&#21160;&#21147;&#27169;&#25311;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#22320;&#39044;&#27979;&#27946;&#27700;&#28153;&#27809;&#28145;&#24230;&#12290;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#36798;&#21040;0.949&#30340;&#24179;&#22343;R-squared&#21644;0.61&#33521;&#23610;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#65292;&#35777;&#26126;&#20854;&#22312;&#39044;&#27979;&#27946;&#27700;&#23792;&#20540;&#28153;&#27809;&#28145;&#24230;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#19982;&#39123;&#39118;&#21704;&#32500;&#21644;&#20234;&#26757;&#23572;&#36798;&#39118;&#26292;&#36827;&#34892;&#39564;&#35777;&#65292;MaxFloodCast&#26174;&#31034;&#20102;&#22312;&#25903;&#25345;&#36817;&#26102;&#27946;&#27700;&#24179;&#21407;&#31649;&#29702;&#21644;&#32039;&#24613;&#34892;&#21160;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26377;&#21161;&#20110;&#20915;&#31574;&#32773;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#21046;&#23450;&#27946;&#27700;&#38450;&#27835;&#31574;&#30053;&#65292;&#20248;&#20808;&#32771;&#34385;&#20020;&#30028;&#35774;&#26045;&#21306;&#22495;&#65292;&#24182;&#30740;&#31350;&#20854;&#20182;&#27969;&#22495;&#38477;&#38632;&#22914;&#20309;&#24433;&#21709;&#26576;&#19968;&#21306;&#22495;&#30340;&#27946;&#27700;&#26292;&#38706;&#12290;MaxFloodCast&#27169;&#22411;&#20351;&#28153;&#27809;&#28145;&#24230;&#30340;&#39044;&#27979;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timely, accurate, and reliable information is essential for decision-makers, emergency managers, and infrastructure operators during flood events. This study demonstrates a proposed machine learning model, MaxFloodCast, trained on physics-based hydrodynamic simulations in Harris County, offers efficient and interpretable flood inundation depth predictions. Achieving an average R-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data, it proves reliable in forecasting peak flood inundation depths. Validated against Hurricane Harvey and Storm Imelda, MaxFloodCast shows the potential in supporting near-time floodplain management and emergency operations. The model's interpretability aids decision-makers in offering critical information to inform flood mitigation strategies, to prioritize areas with critical facilities and to examine how rainfall in other watersheds influences flood exposure in one area. The MaxFloodCast model enables accurate and interpretable inundation 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#22686;&#38271;&#26041;&#27861;&#20197;&#21450;&#20248;&#21270;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06221</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#38454;&#31639;&#27861;&#33258;&#21160;&#35843;&#25972;&#21644;&#35757;&#32451;&#39640;&#25928;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms. (arXiv:2308.06221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#22686;&#38271;&#26041;&#27861;&#20197;&#21450;&#20248;&#21270;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#24191;&#20041;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#22238;&#24402;&#25214;&#21040;&#19968;&#20010;&#21021;&#22987;&#30340;&#22810;&#31867;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#36890;&#36807;&#20462;&#21098;&#19981;&#24517;&#35201;&#30340;&#36755;&#20837;&#26469;&#26368;&#23567;&#21270;&#39564;&#35777;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#31867;&#20284;&#20110;Ho-Kashyap&#35268;&#21017;&#30340;&#26041;&#27861;&#25913;&#21892;&#26399;&#26395;&#36755;&#20986;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;&#36755;&#20986;&#21028;&#21035;&#24335;&#32553;&#25918;&#20026;&#24191;&#20041;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;S&#22411;&#36755;&#20986;&#21333;&#20803;&#30340;&#32593;&#32476;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#26063;&#25209;&#37327;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#20462;&#21098;&#19982;&#22686;&#38271;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#23558;&#36755;&#20837;&#21333;&#20803;&#32553;&#25918;&#20026;S&#22411;&#36755;&#20986;&#21333;&#20803;&#30340;&#32593;&#32476;&#20989;&#25968;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#39304;&#36865;&#21040;MLP&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#20013;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#26550;&#26500;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#20110;d&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#21407;&#21017;&#21644;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for d
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#22120;&#30697;&#38453;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#29305;&#24449;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#21333;&#21464;&#37327;&#37327;&#21270;&#26469;&#35782;&#21035;&#21464;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#21464;&#28857;&#26816;&#27979;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21487;&#20197;&#25552;&#20379;&#28508;&#22312;&#30340;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24863;&#20852;&#36259;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.06213</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#22120;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Change Point Detection With Conceptors. (arXiv:2308.06213v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06213
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#22120;&#30697;&#38453;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#29305;&#24449;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#21333;&#21464;&#37327;&#37327;&#21270;&#26469;&#35782;&#21035;&#21464;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#21464;&#28857;&#26816;&#27979;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21487;&#20197;&#25552;&#20379;&#28508;&#22312;&#30340;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24863;&#20852;&#36259;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#21464;&#28857;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#20013;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21457;&#29983;&#21464;&#21270;&#30340;&#28857;&#12290;&#23545;&#20110;&#21333;&#21464;&#37327;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#20102;&#36739;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#38543;&#30528;&#32500;&#24230;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#22686;&#21152;&#65292;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#33267;&#22810;&#19968;&#20010;&#21464;&#28857;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27010;&#24565;&#22120;&#30697;&#38453;&#26469;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#25351;&#23450;&#35757;&#32451;&#31383;&#21475;&#30340;&#29305;&#24449;&#21160;&#24577;&#12290;&#30456;&#20851;&#30340;&#38543;&#26426;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#25968;&#25454;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#19988;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#21270;&#19982;&#20195;&#34920;&#24615;&#27010;&#24565;&#22120;&#30697;&#38453;&#25152;&#24352;&#25104;&#31354;&#38388;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#21333;&#21464;&#37327;&#37327;&#21270;&#26469;&#35782;&#21035;&#21464;&#28857;&#12290;&#36825;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#31034;&#21487;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24863;&#20852;&#36259;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#30495;&#23454;&#21464;&#28857;&#30340;&#19968;&#33268;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#31227;&#21160;&#22359;&#33258;&#21161;&#27861;&#20135;&#29983;&#32479;&#35745;&#37327;&#30340;&#20998;&#20301;&#25968;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#21464;&#28857;&#26816;&#27979;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline change point detection seeks to identify points in a time series where the data generating process changes. This problem is well studied for univariate i.i.d. data, but becomes challenging with increasing dimension and temporal dependence. For the at most one change point problem, we propose the use of a conceptor matrix to learn the characteristic dynamics of a specified training window in a time series. The associated random recurrent neural network acts as a featurizer of the data, and change points are identified from a univariate quantification of the distance between the featurization and the space spanned by a representative conceptor matrix. This model agnostic method can suggest potential locations of interest that warrant further study. We prove that, under mild assumptions, the method provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#12289;&#24403;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#20197;&#21450;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;&#36825;&#39033;&#32508;&#36848;&#36824;&#25552;&#20986;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.06204</link><description>&lt;p&gt;
&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Safety in Traffic Management Systems: A Comprehensive Survey. (arXiv:2308.06204v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#12289;&#24403;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#20197;&#21450;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;&#36825;&#39033;&#32508;&#36848;&#36824;&#25552;&#20986;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#22312;&#20445;&#38556;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#20132;&#36890;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#20808;&#36827;&#25216;&#26415;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26159;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#20107;&#25925;&#24182;&#20943;&#23567;&#23545;&#36947;&#36335;&#29992;&#25143;&#30340;&#24433;&#21709;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#20851;&#20110;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#25991;&#29486;&#65292;&#20855;&#20307;&#35752;&#35770;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#20013;&#28041;&#21450;&#30340;&#19981;&#21516;&#23433;&#20840;&#38382;&#39064;&#12289;&#24403;&#21069;&#20851;&#20110;&#36825;&#20123;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;&#29616;&#29366;&#20197;&#21450;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic management systems play a vital role in ensuring safe and efficient transportation on roads. However, the use of advanced technologies in traffic management systems has introduced new safety challenges. Therefore, it is important to ensure the safety of these systems to prevent accidents and minimize their impact on road users. In this survey, we provide a comprehensive review of the literature on safety in traffic management systems. Specifically, we discuss the different safety issues that arise in traffic management systems, the current state of research on safety in these systems, and the techniques and methods proposed to ensure the safety of these systems. We also identify the limitations of the existing research and suggest future research directions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2308.06203</link><description>&lt;p&gt;
&#20026;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#26500;&#24314;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks. (arXiv:2308.06203v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06203
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#21619;&#30528;&#31995;&#32479;&#35774;&#35745;&#32773;&#26080;&#27861;&#39044;&#27979;&#24182;&#26126;&#30830;&#35774;&#35745;&#20986;&#26426;&#22120;&#20154;&#21487;&#33021;&#36935;&#21040;&#30340;&#25152;&#26377;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#20197;&#36825;&#31181;&#26041;&#24335;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#22312;&#39640;&#24230;&#21463;&#25511;&#30340;&#29615;&#22659;&#20043;&#22806;&#23481;&#26131;&#20986;&#29616;&#25925;&#38556;&#12290;&#22240;&#26524;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#26426;&#22120;&#20154;&#19982;&#20854;&#29615;&#22659;&#30456;&#20114;&#20316;&#29992;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#36890;&#24120;&#36935;&#21040;&#30340;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#12290;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#23637;&#31034;&#20102;&#35768;&#22810;&#24212;&#29992;&#25152;&#38656;&#30340;&#22522;&#26412;&#24863;&#30693;&#21644;&#25805;&#20316;&#33021;&#21147;&#65292;&#21253;&#25324;&#20179;&#24211;&#29289;&#27969;&#21644;&#23478;&#24237;&#20154;&#24037;&#25903;&#25345;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#23558;&#29289;&#29702;&#27169;&#25311;&#21151;&#33021;&#23884;&#20837;&#21040;&#36825;&#20010;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#31561;&#26041;&#24335;&#26469;&#25913;&#36827;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06202</link><description>&lt;p&gt;
&#22312;&#26816;&#27979;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#20013;&#25506;&#32034;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
Exploring Predicate Visual Context in Detecting of Human-Object Interactions. (arXiv:2308.06202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#31561;&#26041;&#24335;&#26469;&#25913;&#36827;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DETR&#26694;&#26550;&#24050;&#25104;&#20026;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#30740;&#31350;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#20004;&#38454;&#27573;&#21464;&#25442;&#22120;&#30340;HOI&#26816;&#27979;&#22120;&#26159;&#24615;&#33021;&#26368;&#22909;&#21644;&#35757;&#32451;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20197;&#32570;&#20047;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#29289;&#20307;&#29305;&#24449;&#20316;&#20026;HOI&#20998;&#31867;&#30340;&#26465;&#20214;&#65292;&#32780;&#24573;&#35270;&#20102;&#23039;&#21183;&#21644;&#26041;&#21521;&#20449;&#24687;&#65292;&#32780;&#26356;&#27880;&#37325;&#20851;&#20110;&#29289;&#20307;&#36523;&#20221;&#21644;&#36793;&#30028;&#30340;&#35270;&#35273;&#25552;&#31034;&#12290;&#36825;&#33258;&#28982;&#22320;&#38459;&#30861;&#20102;&#23545;&#22797;&#26434;&#25110;&#27169;&#31946;&#20132;&#20114;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#24341;&#20837;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#25913;&#36827;&#20102;&#26597;&#35810;&#35774;&#35745;&#65292;&#24191;&#27867;&#25506;&#32034;&#20102;&#38190;&#21644;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#20316;&#20026;&#31354;&#38388;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#65288;PViC&#65289;&#27169;&#22411;&#22312;HICO-DET&#21644;V-COCO&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaini
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#39062;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.06197</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#22522;&#26412;&#29305;&#24449;&#36827;&#34892;&#22797;&#26434;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features. (arXiv:2308.06197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06197
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#39062;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24773;&#32490;&#35782;&#21035;&#26159;&#19968;&#39033;&#35748;&#30693;&#20219;&#21153;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20854;&#34920;&#29616;&#22312;&#19982;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#30456;&#31561;&#25110;&#20197;&#19978;&#30340;&#20854;&#20182;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#30340;&#31243;&#24230;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#36739;&#20026;&#22256;&#38590;&#30340;&#12290;&#30001;&#20110;&#20154;&#33080;&#34920;&#36798;&#30340;&#24773;&#32490;&#22797;&#26434;&#24615;&#65292;&#36890;&#36807;&#38754;&#37096;&#34920;&#24773;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#29305;&#21035;&#22256;&#38590;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#22312;&#36825;&#19968;&#39046;&#22495;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#23427;&#21487;&#33021;&#38656;&#35201;&#23454;&#26102;&#21512;&#25104;&#30693;&#35782;&#24182;&#29702;&#35299;&#26032;&#27010;&#24565;&#12290;&#20154;&#31867;&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#20960;&#20010;&#20363;&#23376;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20174;&#35760;&#24518;&#20013;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#24182;&#20002;&#24323;&#20854;&#20313;&#20449;&#24687;&#12290;&#21516;&#26679;&#65292;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#30041;&#24050;&#30693;&#31867;&#21035;&#30693;&#35782;&#30340;&#21516;&#26102;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#32780;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#21644;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#20934;&#30830;&#22320;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#35780;&#35770;&#20013;&#25552;&#21462;&#23458;&#20154;&#22269;&#31821;&#30340;&#24341;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#26550;&#26500;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.06175</link><description>&lt;p&gt;
&#20174;&#37202;&#24215;&#35780;&#35770;&#20013;&#35780;&#20272;&#23458;&#20154;&#30340;&#22269;&#31821;&#26500;&#25104;
&lt;/p&gt;
&lt;p&gt;
Assessing Guest Nationality Composition from Hotel Reviews. (arXiv:2308.06175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#35780;&#35770;&#20013;&#25552;&#21462;&#23458;&#20154;&#22269;&#31821;&#30340;&#24341;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#26550;&#26500;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#37202;&#24215;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#24066;&#22330;&#30340;&#23458;&#25143;&#33719;&#21462;&#21162;&#21147;&#65292;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#23458;&#20154;&#30340;&#20010;&#20154;&#20559;&#22909;&#21644;&#38656;&#27714;&#12290;&#21516;&#26679;&#65292;&#36825;&#31181;&#25112;&#30053;&#23450;&#20301;&#26159;&#26377;&#25928;&#30340;&#33829;&#38144;&#39044;&#31639;&#20998;&#37197;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#23448;&#26041;&#32479;&#35745;&#25968;&#25454;&#25253;&#21578;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#28216;&#23458;&#25968;&#37327;&#65292;&#20294;&#27809;&#26377;&#20851;&#20110;&#20010;&#21035;&#20225;&#19994;&#23458;&#20154;&#26500;&#25104;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#31454;&#20105;&#23545;&#25163;&#12289;&#20379;&#24212;&#21830;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#26222;&#36890;&#20844;&#20247;&#23545;&#36825;&#20123;&#25968;&#25454;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#35780;&#35770;&#20013;&#25552;&#21462;&#23458;&#20154;&#22269;&#31821;&#30340;&#24341;&#29992;&#65292;&#20197;&#21160;&#24577;&#35780;&#20272;&#21644;&#30417;&#27979;&#20010;&#21035;&#20225;&#19994;&#23458;&#20154;&#26500;&#25104;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#22534;&#21472;&#30340;LSTM&#23618;&#30340;&#30456;&#24403;&#31616;&#21333;&#30340;&#26550;&#26500;&#25552;&#20379;&#20102;&#27604;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#36235;&#21183;&#65292;&#20998;&#26512;&#20102;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#30340;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.06173</link><description>&lt;p&gt;
&#22522;&#20110;&#25668;&#20687;&#22836;&#30340;&#26234;&#33021;&#31995;&#32479;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#65306;&#24403;&#21069;&#36235;&#21183;&#65292;&#20998;&#31867;&#65292;&#24212;&#29992;&#65292;&#30740;&#31350;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook. (arXiv:2308.06173v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#36235;&#21183;&#65292;&#20998;&#26512;&#20102;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#30340;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#24403;&#21069;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#27010;&#24565;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#20998;&#26512;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#21306;&#21035;&#24615;&#29305;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#25191;&#34892;&#25915;&#20987;&#25152;&#28041;&#21450;&#30340;&#20855;&#20307;&#38656;&#27714;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26681;&#25454;&#23427;&#20204;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#30446;&#26631;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#20998;&#31867;&#65292;&#26816;&#27979;&#65292;&#20154;&#33080;&#35782;&#21035;&#65292;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#25915;&#20987;&#26041;&#27861;&#22312;&#25928;&#26524;&#12289;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#31181;&#25216;&#26415;&#22914;&#20309;&#21162;&#21147;&#30830;&#20445;&#25104;&#21151;&#25805;&#20316;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20943;&#23567;&#34987;&#26816;&#27979;&#30340;&#39118;&#38505;&#21644;&#25215;&#21463;&#30495;&#23454;&#19990;&#30028;&#30340;&#24178;&#25200;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#65292;&#24182;&#21246;&#30011;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38454;&#27573;&#24615;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22478;&#38469;&#39640;&#36895;&#20844;&#36335;&#30340;&#27599;&#26085;&#20132;&#36890;&#37327;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31934;&#24515;&#30340;&#25968;&#25454;&#35268;&#33539;&#21270;&#12289;&#28151;&#21512;&#27169;&#22411;&#32467;&#21512;&#20197;&#21450;&#32771;&#34385;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#22810;&#20010;&#35201;&#32032;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20132;&#36890;&#37327;&#39044;&#27979;&#20013;&#30340;&#26102;&#31354;&#29305;&#24449;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06155</link><description>&lt;p&gt;
&#22522;&#20110;&#38454;&#27573;&#24615;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#30340;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction. (arXiv:2308.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38454;&#27573;&#24615;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22478;&#38469;&#39640;&#36895;&#20844;&#36335;&#30340;&#27599;&#26085;&#20132;&#36890;&#37327;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31934;&#24515;&#30340;&#25968;&#25454;&#35268;&#33539;&#21270;&#12289;&#28151;&#21512;&#27169;&#22411;&#32467;&#21512;&#20197;&#21450;&#32771;&#34385;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#22810;&#20010;&#35201;&#32032;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20132;&#36890;&#37327;&#39044;&#27979;&#20013;&#30340;&#26102;&#31354;&#29305;&#24449;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#38469;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#23545;&#20110;&#29616;&#20195;&#37117;&#24066;&#29983;&#27963;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#29983;&#25104;&#24102;&#26377;&#26102;&#31354;&#29305;&#24449;&#30340;&#24322;&#26500;&#24863;&#30693;&#25968;&#25454;&#12290;&#20316;&#20026;&#20132;&#36890;&#39046;&#22495;&#30340;&#20363;&#34892;&#20998;&#26512;&#65292;&#27599;&#26085;&#20132;&#36890;&#37327;&#20272;&#35745;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#20047;&#23545;&#30456;&#20851;&#26102;&#31354;&#29305;&#24449;&#30340;&#38271;&#26399;&#32771;&#23519;&#21644;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#26377;&#25928;&#25163;&#27573;&#65292;&#21518;&#32773;&#20250;&#24694;&#21270;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38454;&#27573;&#24615;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#27599;&#26085;&#20132;&#36890;&#37327;&#12290;&#22312;&#29305;&#24449;&#39044;&#22788;&#29702;&#38454;&#27573;&#65292;&#26681;&#25454;&#28508;&#22312;&#30340;&#38271;&#23614;&#20998;&#24067;&#31934;&#24515;&#36827;&#34892;&#25968;&#25454;&#35268;&#33539;&#21270;&#12290;&#22312;&#26102;&#31354;&#23398;&#20064;&#38454;&#27573;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#20840;&#21367;&#31215;&#32593;&#32476;&#65288;FCN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#32771;&#34385;&#20102;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#26102;&#38388;&#12289;&#31354;&#38388;&#12289;&#27668;&#35937;&#21644;&#26085;&#21382;&#20449;&#24687;&#12290;&#22312;&#20915;&#31574;&#38454;&#27573;&#65292;&#39044;&#27979;&#20102;&#32593;&#32476;&#33539;&#22260;&#25910;&#36153;&#21345;&#21475;&#26410;&#26469;&#19968;&#22825;&#30340;&#20132;&#36890;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inter-city highway transportation is significant for citizens' modern urban life and generates heterogeneous sensory data with spatio-temporal characteristics. As a routine analysis in transportation domain, daily traffic volume estimation faces challenges for highway toll stations including lacking of exploration of correlative spatio-temporal features from a long-term perspective and effective means to deal with data imbalance which always deteriorates the predictive performance. In this paper, a deep spatio-temporal learning method is proposed to predict daily traffic volume in three phases. In feature pre-processing phase, data is normalized elaborately according to latent long-tail distribution. In spatio-temporal learning phase, a hybrid model is employed combining fully convolution network (FCN) and long short-term memory (LSTM), which considers time, space, meteorology, and calendar from heterogeneous data. In decision phase, traffic volumes on a coming day at network-wide toll
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#29992;&#20110;&#36817;&#20284;&#26368;&#22823;&#29109;&#20998;&#24067;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65292;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#26469;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#26368;&#22823;&#29109;&#38381;&#21512;&#12290;&#36890;&#36807;&#23545;&#27604;&#22810;&#20010;&#27979;&#35797;&#26696;&#20363;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06149</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#29992;&#20110;&#26368;&#22823;&#29109;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Regression for Maximum Entropy Distribution. (arXiv:2308.06149v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#29992;&#20110;&#36817;&#20284;&#26368;&#22823;&#29109;&#20998;&#24067;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65292;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#26469;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#26368;&#22823;&#29109;&#38381;&#21512;&#12290;&#36890;&#36807;&#23545;&#27604;&#22810;&#20010;&#27979;&#35797;&#26696;&#20363;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#20998;&#24067;&#25552;&#20379;&#20102;&#19968;&#31867;&#36866;&#29992;&#20110;&#30697;&#38381;&#21512;&#38382;&#39064;&#30340;&#21560;&#24341;&#20154;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#21442;&#25968;&#21270;&#36825;&#20123;&#20998;&#24067;&#30340;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#23545;&#20110;&#23454;&#38469;&#38381;&#21512;&#35774;&#32622;&#26469;&#35828;&#21364;&#26159;&#19968;&#20010;&#35745;&#31639;&#29942;&#39048;&#12290;&#21463;&#21040;&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#36817;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#39640;&#26031;&#20808;&#39564;&#26469;&#36817;&#20284;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#20316;&#20026;&#32473;&#23450;&#19968;&#32452;&#30697;&#30340;&#26144;&#23556;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#20248;&#21270;&#20102;&#21508;&#31181;&#26680;&#20989;&#25968;&#30340;&#36229;&#21442;&#25968;&#12290;&#30740;&#31350;&#20102;&#25152;&#35774;&#35745;&#30340;&#25968;&#25454;&#39537;&#21160;&#26368;&#22823;&#29109;&#38381;&#21512;&#22312;&#21253;&#25324;&#30001;Bhatnagar-Gross-Krook&#21644;Boltzmann&#21160;&#21147;&#23398;&#26041;&#31243;&#25511;&#21046;&#30340;&#38750;&#24179;&#34913;&#20998;&#24067;&#26494;&#24347;&#30340;&#20960;&#20010;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum-Entropy Distributions offer an attractive family of probability densities suitable for moment closure problems. Yet finding the Lagrange multipliers which parametrize these distributions, turns out to be a computational bottleneck for practical closure settings. Motivated by recent success of Gaussian processes, we investigate the suitability of Gaussian priors to approximate the Lagrange multipliers as a map of a given set of moments. Examining various kernel functions, the hyperparameters are optimized by maximizing the log-likelihood. The performance of the devised data-driven Maximum-Entropy closure is studied for couple of test cases including relaxation of non-equilibrium distributions governed by Bhatnagar-Gross-Krook and Boltzmann kinetic equations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35789;&#34955;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23545;&#20195;&#30721;&#27880;&#37322;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#24182;&#27604;&#36739;&#20102;&#20256;&#32479;&#35789;&#34955;&#27169;&#22411;&#21644;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06144</link><description>&lt;p&gt;
&#20351;&#29992;&#35789;&#34955;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#35782;&#21035;&#20195;&#30721;&#35780;&#35770;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models. (arXiv:2308.06144v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35789;&#34955;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23545;&#20195;&#30721;&#27880;&#37322;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#24182;&#27604;&#36739;&#20102;&#20256;&#32479;&#35789;&#34955;&#27169;&#22411;&#21644;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#24180;&#65292;&#20449;&#24687;&#26816;&#32034;&#35770;&#22363;(FIRE)&#21551;&#21160;&#20102;&#19968;&#20010;&#20849;&#20139;&#20219;&#21153;&#65292;&#29992;&#20110;&#23545;&#19981;&#21516;&#20195;&#30721;&#27573;&#30340;&#35780;&#35770;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#26159;&#19968;&#20010;&#20108;&#20803;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#32473;&#23450;&#20195;&#30721;&#27573;&#30340;&#35780;&#35770;&#26159;&#21542;&#30456;&#20851;&#12290;&#21360;&#24230;&#31185;&#23398;&#25945;&#32946;&#19982;&#30740;&#31350;&#38498;&#21338;&#24085;&#23572;&#20998;&#38498;(IISERB)&#30340;BioNLP-IISERB&#23567;&#32452;&#21442;&#19982;&#20102;&#36825;&#39033;&#20219;&#21153;&#65292;&#24182;&#20026;&#20116;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#25552;&#20132;&#20102;&#20116;&#31181;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#27010;&#20917;&#21644;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#19978;&#30340;&#20854;&#20182;&#37325;&#35201;&#21457;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#19981;&#21516;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#26696;&#21644;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#12290;&#23545;&#20110;&#35789;&#34955;&#27169;&#22411;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#36923;&#36753;&#22238;&#24402;&#65292;&#20197;&#35782;&#21035;&#32473;&#23450;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;Transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Forum for Information Retrieval (FIRE) started a shared task this year for classification of comments of different code segments. This is binary text classification task where the objective is to identify whether comments given for certain code segments are relevant or not. The BioNLP-IISERB group at the Indian Institute of Science Education and Research Bhopal (IISERB) participated in this task and submitted five runs for five different models. The paper presents the overview of the models and other significant findings on the training corpus. The methods involve different feature engineering schemes and text classification techniques. The performance of the classical bag of words model and transformer-based models were explored to identify significant features from the given training corpus. We have explored different classifiers viz., random forest, support vector machine and logistic regression using the bag of words model. Furthermore, the pre-trained transformer based models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;JPEG&#21387;&#32553;&#31995;&#25968;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25163;&#20889;&#25991;&#20214;&#20013;&#30340;&#25991;&#26412;&#34892;&#23450;&#20301;&#12290;&#36890;&#36807;&#35774;&#35745;&#25913;&#36827;&#30340;CompTLL-UNet&#26550;&#26500;&#65292;&#22312;JPEG&#21387;&#32553;&#22495;&#20013;&#30452;&#25509;&#23450;&#20301;&#25991;&#26412;&#34892;&#65292;&#24182;&#22312;ICDAR2017&#21644;ICDAR2019&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.06142</link><description>&lt;p&gt;
CompTLL-UNet: &#20351;&#29992;JPEG&#31995;&#25968;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#22312;&#20855;&#25361;&#25112;&#24615;&#25163;&#20889;&#25991;&#20214;&#20013;&#21387;&#32553;&#39046;&#22495;&#25991;&#26412;&#34892;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients. (arXiv:2308.06142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;JPEG&#21387;&#32553;&#31995;&#25968;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25163;&#20889;&#25991;&#20214;&#20013;&#30340;&#25991;&#26412;&#34892;&#23450;&#20301;&#12290;&#36890;&#36807;&#35774;&#35745;&#25913;&#36827;&#30340;CompTLL-UNet&#26550;&#26500;&#65292;&#22312;JPEG&#21387;&#32553;&#22495;&#20013;&#30452;&#25509;&#23450;&#20301;&#25991;&#26412;&#34892;&#65292;&#24182;&#22312;ICDAR2017&#21644;ICDAR2019&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23450;&#20301;&#25163;&#20889;&#25991;&#20214;&#20013;&#30340;&#25991;&#26412;&#34892;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#24403;&#32771;&#34385;&#21040;&#22797;&#26434;&#25163;&#20889;&#25991;&#20214;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;&#21508;&#31181;&#20070;&#20889;&#38382;&#39064;&#22914;&#34892;&#38388;&#19981;&#22343;&#21248;&#38388;&#36317;&#12289;&#25391;&#33633;&#21644;&#25509;&#35302;&#25991;&#23383;&#20197;&#21450;&#20542;&#26012;&#30340;&#23384;&#22312;&#65292;&#23558;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22788;&#29702;&#21387;&#32553;&#25991;&#20214;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#35299;&#21387;&#32553;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;JPEG&#21387;&#32553;&#31995;&#25968;&#20013;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#32780;&#26080;&#38656;&#23436;&#20840;&#35299;&#21387;&#32553;&#26469;&#23454;&#29616;JPEG&#21387;&#32553;&#22495;&#20013;&#30340;&#25991;&#26412;&#34892;&#23450;&#20301;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;U-Net&#26550;&#26500;&#65292;&#31216;&#20026;Compressed Text-Line Localization Network&#65288;CompTLL-UNet&#65289;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#20351;&#29992;&#21253;&#25324;ICDAR2017&#65288;cBAD&#65289;&#21644;ICDAR2019&#65288;cBAD&#65289;&#22312;&#20869;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;JPEG&#21387;&#32553;&#29256;&#26412;&#36827;&#34892;&#20102;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic localization of text-lines in handwritten documents is still an open and challenging research problem. Various writing issues such as uneven spacing between the lines, oscillating and touching text, and the presence of skew become much more challenging when the case of complex handwritten document images are considered for segmentation directly in their respective compressed representation. This is because, the conventional way of processing compressed documents is through decompression, but here in this paper, we propose an idea that employs deep feature learning directly from the JPEG compressed coefficients without full decompression to accomplish text-line localization in the JPEG compressed domain. A modified U-Net architecture known as Compressed Text-Line Localization Network (CompTLL-UNet) is designed to accomplish it. The model is trained and tested with JPEG compressed version of benchmark datasets including ICDAR2017 (cBAD) and ICDAR2019 (cBAD), reporting the state
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#38156;&#29983;&#20135;&#21387;&#21147;&#36807;&#28388;&#36807;&#31243;&#20013;&#30340;&#28388;&#39292;&#21547;&#27700;&#29575;&#65292;&#20026;&#38156;&#29983;&#20135;&#24037;&#33402;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#39044;&#27979;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2308.06138</link><description>&lt;p&gt;
&#24212;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#21387;&#21147;&#36807;&#28388;&#24615;&#33021;&#30340;&#25506;&#32034;&#21644;&#38156;&#28024;&#20986;&#28388;&#39292;&#21547;&#27700;&#29575;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling. (arXiv:2308.06138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06138
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#38156;&#29983;&#20135;&#21387;&#21147;&#36807;&#28388;&#36807;&#31243;&#20013;&#30340;&#28388;&#39292;&#21547;&#27700;&#29575;&#65292;&#20026;&#38156;&#29983;&#20135;&#24037;&#33402;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#39044;&#27979;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#26448;&#26009;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#33021;&#22815;&#25552;&#20379;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#38156;&#29983;&#20135;&#30340;&#21387;&#21147;&#36807;&#28388;&#36807;&#31243;&#20013;&#30340;&#28388;&#39292;&#21547;&#27700;&#29575;&#12290;&#28388;&#39292;&#21547;&#27700;&#29575;&#21463;&#21040;&#19971;&#20010;&#21442;&#25968;&#30340;&#24433;&#21709;&#65306;&#28201;&#24230;&#65288;35&#25668;&#27663;&#24230;&#21644;65&#25668;&#27663;&#24230;&#65289;&#65292;&#22266;&#20307;&#27987;&#24230;&#65288;0.2&#20811;/&#21319;&#21644;0.38&#20811;/&#21319;&#65289;&#65292;pH&#20540;&#65288;2&#12289;3.5&#21644;5&#65289;&#65292;&#21561;&#27668;&#26102;&#38388;&#65288;2&#20998;&#38047;&#12289;10&#20998;&#38047;&#21644;15&#20998;&#38047;&#65289;&#65292;&#28388;&#39292;&#21402;&#24230;&#65288;14&#27627;&#31859;&#12289;20&#27627;&#31859;&#12289;26&#27627;&#31859;&#21644;34&#27627;&#31859;&#65289;&#65292;&#21387;&#21147;&#21644;&#36807;&#28388;&#26102;&#38388;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#32455;&#29289;&#36827;&#34892;&#20102;288&#27425;&#27979;&#35797;&#65306;&#32858;&#19993;&#28911;&#65288;S1&#65289;&#21644;&#28068;&#32438;&#65288;S2&#65289;&#12290;&#36890;&#36807;&#20915;&#23450;&#31995;&#25968;&#65288;R2&#65289;&#12289;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#25351;&#26631;&#35780;&#20272;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;S1&#21644;S2&#65292;R2&#20540;&#20998;&#21035;&#20026;0.88&#21644;0.83&#65292;MSE&#20540;&#20998;&#21035;&#20026;6.243x10-07&#21644;1.086x10-06&#65292;MAE&#20540;&#20998;&#21035;&#20026;0.00056&#21644;0.00088&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is a powerful tool for material science applications. Artificial Neural Network (ANN) is a machine learning technique that can provide high prediction accuracy. This study aimed to develop an ANN model to predict the cake moisture of the pressure filtration process of zinc production. The cake moisture was influenced by seven parameters: temperature (35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and 5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm), pressure, and filtration time. The study conducted 288 tests using two types of fabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated by the Coefficient of determination (R2), the Mean Square Error (MSE), and the Mean Absolute Error (MAE) metrics for both datasets. The results showed R2 values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAE values of 0.00056 and 0.00088 for S1 and S2, respectively. These results indicated t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24102;&#26377;Akaike&#20449;&#24687;&#20934;&#21017;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;CPINN-AIC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#36719;&#20256;&#24863;&#22120;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#21457;&#29616;PDE&#30340;&#36866;&#24403;&#32467;&#26500;&#65292;&#21253;&#25324;&#24494;&#20998;&#31639;&#23376;&#21644;&#28304;&#39033;&#65292;&#20197;&#24357;&#34917;&#29702;&#24819;&#21270;&#30340;PDE&#19982;&#23454;&#38469;&#24773;&#20917;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06132</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;Akaike&#20449;&#24687;&#20934;&#21017;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36719;&#20256;&#24863;&#22120;&#30340;PDE&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike's Information Criterion. (arXiv:2308.06132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24102;&#26377;Akaike&#20449;&#24687;&#20934;&#21017;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;CPINN-AIC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#36719;&#20256;&#24863;&#22120;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#21457;&#29616;PDE&#30340;&#36866;&#24403;&#32467;&#26500;&#65292;&#21253;&#25324;&#24494;&#20998;&#31639;&#23376;&#21644;&#28304;&#39033;&#65292;&#20197;&#24357;&#34917;&#29702;&#24819;&#21270;&#30340;PDE&#19982;&#23454;&#38469;&#24773;&#20917;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20256;&#24863;&#22120;&#24191;&#27867;&#24212;&#29992;&#20110;&#20351;&#29992;&#26131;&#20110;&#27979;&#37327;&#30340;&#21464;&#37327;&#21644;&#25968;&#23398;&#27169;&#22411;&#26469;&#30417;&#27979;&#20851;&#38190;&#21464;&#37327;&#12290;&#23545;&#20110;&#20855;&#26377;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#24037;&#19994;&#36807;&#31243;&#20013;&#30340;&#36719;&#20256;&#24863;&#22120;&#65292;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#27169;&#22411;&#20505;&#36873;&#12290;&#28982;&#32780;&#65292;&#29702;&#24819;&#21270;&#30340;PDE&#21644;&#23454;&#38469;&#24773;&#20917;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#24046;&#36317;&#12290;&#21457;&#29616;&#21253;&#25324;&#24494;&#20998;&#31639;&#23376;&#21644;&#28304;&#39033;&#22312;&#20869;&#30340;PDE&#30340;&#36866;&#24403;&#32467;&#26500;&#21487;&#20197;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Akaike&#20934;&#21017;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;CPINN-AIC&#65289;&#26469;&#21457;&#29616;&#36719;&#20256;&#24863;&#22120;&#30340;PDE&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;CPINN&#33719;&#24471;&#28385;&#36275;PDE&#30340;&#35299;&#21644;&#28304;&#39033;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;-&#29289;&#29702;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;CPINN&#65292;&#20854;&#20013;&#28041;&#21450;&#26410;&#30830;&#23450;&#30340;&#24494;&#20998;&#31639;&#23376;&#30340;&#32452;&#21512;&#12290;&#26368;&#21518;&#65292;&#20154;&#24037;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#39564;&#35777;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft sensors have been extensively used to monitor key variables using easy-to-measure variables and mathematical models. Partial differential equations (PDEs) are model candidates for soft sensors in industrial processes with spatiotemporal dependence. However, gaps often exist between idealized PDEs and practical situations. Discovering proper structures of PDEs, including the differential operators and source terms, can remedy the gaps. To this end, a coupled physics-informed neural network with Akaike's criterion information (CPINN-AIC) is proposed for PDE discovery of soft sensors. First, CPINN is adopted for obtaining solutions and source terms satisfying PDEs. Then, we propose a data-physics-hybrid loss function for training CPINN, in which undetermined combinations of differential operators are involved. Consequently, AIC is used to discover the proper combination of differential operators. Finally, the artificial and practical datasets are used to verify the feasibility and ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#22312;&#36328;&#22478;&#24066;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23384;&#22312;&#30340;UQ&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.06129</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#36328;&#22478;&#24066;&#20132;&#36890;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Image-based Traffic Prediction across Cities. (arXiv:2308.06129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#22312;&#36328;&#22478;&#24066;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23384;&#22312;&#30340;UQ&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#36739;&#24378;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#26222;&#36941;&#37096;&#32626;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#24341;&#20837;&#27010;&#29575;&#25512;&#29702;&#12289;&#25913;&#36827;&#20915;&#31574;&#21644;&#25552;&#39640;&#27169;&#22411;&#37096;&#32626;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;UQ&#26041;&#27861;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#26377;&#29992;&#24615;&#20197;&#21450;&#33719;&#24471;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#22478;&#24066;&#33539;&#22260;&#20869;&#20132;&#36890;&#21160;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#23545;&#36328;&#36234;&#22810;&#20010;&#22478;&#24066;&#21644;&#26102;&#38388;&#27573;&#30340;&#22823;&#35268;&#27169;&#22522;&#20110;&#22270;&#20687;&#30340;&#20132;&#36890;&#25968;&#25454;&#38598;&#24212;&#29992;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#20004;&#31181;&#37325;&#35201;&#24615;&#19981;&#30830;&#23450;&#24615;UQ&#26041;&#27861;&#22312;&#26102;&#38388;&#21644;&#26102;&#31354;&#36716;&#25442;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#36827;&#34892;&#22478;&#24066;&#20132;&#36890;&#21160;&#24577;&#21464;&#21270;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22478;&#24066;&#20132;&#36890;&#21160;&#24577;&#21464;&#21270;&#26041;&#38754;&#21457;&#29616;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the strong predictive performance of deep learning models for traffic prediction, their widespread deployment in real-world intelligent transportation systems has been restrained by a lack of interpretability. Uncertainty quantification (UQ) methods provide an approach to induce probabilistic reasoning, improve decision-making and enhance model deployment potential. To gain a comprehensive picture of the usefulness of existing UQ methods for traffic prediction and the relation between obtained uncertainties and city-wide traffic dynamics, we investigate their application to a large-scale image-based traffic dataset spanning multiple cities and time periods. We compare two epistemic and two aleatoric UQ methods on both temporal and spatio-temporal transfer tasks, and find that meaningful uncertainty estimates can be recovered. We further demonstrate how uncertainty estimates can be employed for unsupervised outlier detection on changes in city traffic dynamics. We find that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#30340;&#25193;&#23637;&#8212;&#8212;&#21487;&#21464;&#30446;&#26631;&#31574;&#30053;&#65288;VOP&#65289;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#27867;&#21270;&#22810;&#31181;&#21442;&#25968;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#25913;&#21464;&#31574;&#30053;&#30340;&#36755;&#20837;&#30446;&#26631;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#33258;&#30001;&#35843;&#25972;&#20854;&#34892;&#20026;&#25110;&#37325;&#26032;&#24179;&#34913;&#20248;&#21270;&#30446;&#26631;&#65292;&#26080;&#38656;&#25910;&#38598;&#39069;&#22806;&#30340;&#35266;&#27979;&#25968;&#25454;&#25209;&#27425;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.06127</link><description>&lt;p&gt;
&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#21464;&#30446;&#26631;&#30340;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Control Policies for Variable Objectives from Offline Data. (arXiv:2308.06127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#30340;&#25193;&#23637;&#8212;&#8212;&#21487;&#21464;&#30446;&#26631;&#31574;&#30053;&#65288;VOP&#65289;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#27867;&#21270;&#22810;&#31181;&#21442;&#25968;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#25913;&#21464;&#31574;&#30053;&#30340;&#36755;&#20837;&#30446;&#26631;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#33258;&#30001;&#35843;&#25972;&#20854;&#34892;&#20026;&#25110;&#37325;&#26032;&#24179;&#34913;&#20248;&#21270;&#30446;&#26631;&#65292;&#26080;&#38656;&#25910;&#38598;&#39069;&#22806;&#30340;&#35266;&#27979;&#25968;&#25454;&#25209;&#27425;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#33719;&#21462;&#21160;&#24577;&#31995;&#32479;&#30340;&#20808;&#36827;&#25511;&#21046;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#26080;&#27861;&#19982;&#29615;&#22659;&#30452;&#25509;&#20132;&#20114;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#30340;&#27010;&#24565;&#25193;&#23637;&#65292;&#31216;&#20026;&#21487;&#21464;&#30446;&#26631;&#31574;&#30053;&#65288;VOP&#65289;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#31574;&#30053;&#34987;&#35757;&#32451;&#20197;&#26377;&#25928;&#22320;&#27867;&#21270;&#22810;&#31181;&#21442;&#25968;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#25913;&#21464;&#20316;&#20026;&#31574;&#30053;&#36755;&#20837;&#30340;&#30446;&#26631;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#33258;&#30001;&#35843;&#25972;&#20854;&#34892;&#20026;&#25110;&#37325;&#26032;&#24179;&#34913;&#20248;&#21270;&#30446;&#26631;&#65292;&#32780;&#26080;&#38656;&#25910;&#38598;&#39069;&#22806;&#30340;&#35266;&#27979;&#25968;&#25454;&#25209;&#27425;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning provides a viable approach to obtain advanced control strategies for dynamical systems, in particular when direct interaction with the environment is not available. In this paper, we introduce a conceptual extension for model-based policy search methods, called variable objective policy (VOP). With this approach, policies are trained to generalize efficiently over a variety of objectives, which parameterize the reward function. We demonstrate that by altering the objectives passed as input to the policy, users gain the freedom to adjust its behavior or re-balance optimization targets at runtime, without need for collecting additional observation batches or re-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38669;&#20811;&#26031;&#36807;&#31243;&#30340;&#24310;&#36831;&#26684;&#20848;&#26480;&#22240;&#26524;&#25928;&#24212;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24310;&#36831;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#25512;&#26029;&#20986;&#26102;&#38388;&#24310;&#36831;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#26377;&#21161;&#20110;&#36861;&#36394;&#21407;&#22987;&#22240;&#26524;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.06106</link><description>&lt;p&gt;
&#24310;&#36831;&#26684;&#20848;&#26480;&#22240;&#26524;&#24615;&#30340;&#38669;&#20811;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Hawkes Processes with Delayed Granger Causality. (arXiv:2308.06106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38669;&#20811;&#26031;&#36807;&#31243;&#30340;&#24310;&#36831;&#26684;&#20848;&#26480;&#22240;&#26524;&#25928;&#24212;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24310;&#36831;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#25512;&#26029;&#20986;&#26102;&#38388;&#24310;&#36831;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#26377;&#21161;&#20110;&#36861;&#36394;&#21407;&#22987;&#22240;&#26524;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;&#22810;&#20803;&#38669;&#20811;&#26031;&#36807;&#31243;&#26126;&#30830;&#22320;&#24314;&#27169;&#24310;&#36831;&#30340;&#26684;&#20848;&#26480;&#22240;&#26524;&#25928;&#24212;&#12290;&#36825;&#20010;&#24819;&#27861;&#30340;&#28789;&#24863;&#26469;&#33258;&#20110;&#22240;&#26524;&#20107;&#20214;&#36890;&#24120;&#38656;&#35201;&#19968;&#20123;&#26102;&#38388;&#25165;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#30740;&#31350;&#36825;&#20010;&#26102;&#38388;&#24310;&#36831;&#26412;&#36523;&#23601;&#24456;&#26377;&#24847;&#20041;&#12290;&#22312;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#30340;&#24310;&#36831;&#21442;&#25968;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#20272;&#35745;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#24076;&#26395;&#25512;&#26029;&#20986;&#26102;&#38388;&#24310;&#36831;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#20102;&#35299;&#35813;&#20998;&#24067;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#23558;&#26102;&#38388;&#24310;&#36831;&#35270;&#20026;&#28508;&#22312;&#21464;&#37327;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#31639;&#27861;&#26469;&#36817;&#20284;&#26102;&#38388;&#24310;&#36831;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#26126;&#30830;&#22320;&#24314;&#27169;&#38669;&#20811;&#26031;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#24310;&#36831;&#65292;&#25105;&#20204;&#20026;&#27169;&#22411;&#22686;&#21152;&#20102;&#28789;&#27963;&#24615;&#12290;&#25512;&#26029;&#20986;&#30340;&#26102;&#38388;&#24310;&#36831;&#21518;&#39564;&#20998;&#24067;&#20855;&#26377;&#31185;&#23398;&#24847;&#20041;&#65292;&#24182;&#26377;&#21161;&#20110;&#36861;&#36394;&#25903;&#25345;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#21407;&#22987;&#22240;&#26524;&#26102;&#38388;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to explicitly model the delayed Granger causal effects based on multivariate Hawkes processes. The idea is inspired by the fact that a causal event usually takes some time to exert an effect. Studying this time lag itself is of interest. Given the proposed model, we first prove the identifiability of the delay parameter under mild conditions. We further investigate a model estimation method under a complex setting, where we want to infer the posterior distribution of the time lags and understand how this distribution varies across different scenarios. We treat the time lags as latent variables and formulate a Variational Auto-Encoder (VAE) algorithm to approximate the posterior distribution of the time lags. By explicitly modeling the time lags in Hawkes processes, we add flexibility to the model. The inferred time-lag posterior distributions are of scientific meaning and help trace the original causal time that supports the root cause analysis. We empirically evaluate our model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20845;&#20010;&#21487;&#32452;&#21512;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#36880;&#28176;&#22686;&#21152;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#21151;&#33021;&#65292;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#65292;&#23454;&#29616;&#26356;&#22823;&#21644;&#26356;&#24378;&#22823;&#27169;&#22411;&#30340;&#39640;&#25928;&#22521;&#35757;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.06103</link><description>&lt;p&gt;
Transformer&#26550;&#26500;&#30340;&#21487;&#32452;&#21512;&#20989;&#25968;&#20445;&#25345;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Composable Function-preserving Expansions for Transformer Architectures. (arXiv:2308.06103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06103
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20845;&#20010;&#21487;&#32452;&#21512;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#36880;&#28176;&#22686;&#21152;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#21151;&#33021;&#65292;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#65292;&#23454;&#29616;&#26356;&#22823;&#21644;&#26356;&#24378;&#22823;&#27169;&#22411;&#30340;&#39640;&#25928;&#22521;&#35757;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26368;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#27169;&#22411;&#35268;&#27169;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#21644;&#25552;&#21319;&#26368;&#26032;&#25216;&#26415;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#36890;&#24120;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#65292;&#22240;&#20026;&#36825;&#24847;&#21619;&#30528;&#26550;&#26500;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#19981;&#20801;&#35768;&#20174;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#30452;&#25509;&#20256;&#36882;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#21487;&#32452;&#21512;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#36880;&#28176;&#22686;&#21152;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#21151;&#33021;&#65292;&#20801;&#35768;&#26681;&#25454;&#38656;&#35201;&#25193;&#23637;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#27599;&#20010;&#36716;&#25442;&#30340;&#26368;&#23567;&#21021;&#22987;&#21270;&#32422;&#26463;&#19979;&#31934;&#30830;&#21151;&#33021;&#20445;&#25345;&#30340;&#35777;&#26126;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#25193;&#23637;&#26550;&#26500;&#65292;&#23454;&#29616;&#26356;&#22823;&#21644;&#26356;&#24378;&#22823;&#27169;&#22411;&#30340;&#39640;&#25928;&#22521;&#35757;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training state-of-the-art neural networks requires a high cost in terms of compute and time. Model scale is recognized to be a critical factor to achieve and improve the state-of-the-art. Increasing the scale of a neural network normally requires restarting from scratch by randomly initializing all the parameters of the model, as this implies a change of architecture's parameters that does not allow for a straightforward transfer of knowledge from smaller size models. In this work, we propose six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality, allowing to expand the capacity of the model as needed. We provide proof of exact function preservation under minimal initialization constraints for each transformation. The proposed methods may enable efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23545;&#35270;&#35273;&#23545;&#25239;&#24615;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#23450;&#37327;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#26410;&#26469;&#25913;&#36827;VCE&#26041;&#27861;&#25351;&#26126;&#20102;&#22810;&#20010;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.06100</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#23545;&#25239;&#24615;&#35299;&#37322; - &#23454;&#29616;&#31995;&#32479;&#24615;&#23450;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Visual Counterfactual Explanations -- Towards Systematic Quantitative Evaluation. (arXiv:2308.06100v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06100
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23545;&#35270;&#35273;&#23545;&#25239;&#24615;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#23450;&#37327;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#26410;&#26469;&#25913;&#36827;VCE&#26041;&#27861;&#25351;&#26126;&#20102;&#22810;&#20010;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#35270;&#35273;&#23545;&#25239;&#24615;&#35299;&#37322;(VCE)&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#21512;&#25104;&#20102;&#39640;&#32500;&#22270;&#20687;&#30340;&#26032;&#31034;&#20363;&#65292;&#36136;&#37327;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#38590;&#27604;&#36739;&#36825;&#20123;VCE&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#35780;&#20272;&#31243;&#24207;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#36890;&#24120;&#20165;&#38480;&#20110;&#23545;&#20010;&#21035;&#31034;&#20363;&#30340;&#21487;&#35270;&#21270;&#26816;&#26597;&#21644;&#23567;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;VCE&#26041;&#27861;&#31995;&#32479;&#24615;&#23450;&#37327;&#35780;&#20272;&#30340;&#26694;&#26550;&#21644;&#19968;&#22871;&#26368;&#23567;&#25351;&#26631;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#25506;&#32034;&#20102;&#26368;&#26032;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#33258;&#28982;&#22270;&#20687;&#20998;&#31867;&#65288;ImageNet&#65289;&#20013;VCE&#26041;&#27861;&#30340;&#26576;&#20123;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#31867;&#20284;&#21066;&#24369;&#23454;&#39564;&#30340;&#27979;&#35797;&#65292;&#20026;&#21508;&#31181;&#22797;&#26434;&#24230;&#12289;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#20998;&#31867;&#22120;&#29983;&#25104;&#20102;&#25968;&#21315;&#20010;VCE&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#26410;&#26469;&#25913;&#36827;VCE&#26041;&#27861;&#25552;&#20379;&#20102;&#22810;&#20010;&#26041;&#21521;&#12290;&#36890;&#36807;&#20998;&#20139;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#21644;
&lt;/p&gt;
&lt;p&gt;
Latest methods for visual counterfactual explanations (VCE) harness the power of deep generative models to synthesize new examples of high-dimensional images of impressive quality. However, it is currently difficult to compare the performance of these VCE methods as the evaluation procedures largely vary and often boil down to visual inspection of individual examples and small scale user studies. In this work, we propose a framework for systematic, quantitative evaluation of the VCE methods and a minimal set of metrics to be used. We use this framework to explore the effects of certain crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet). We conduct a battery of ablation-like experiments, generating thousands of VCEs for a suite of classifiers of various complexity, accuracy and robustness. Our findings suggest multiple directions for future advancements and improvements of VCE methods. By sharing our methodology and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20197;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24178;&#39044;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25968;&#25454;&#12289;&#35757;&#32451;&#21046;&#24230;&#25110;&#35299;&#30721;&#65292;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#27969;&#30021;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#36830;&#36143;&#24615;&#20197;&#21450;&#36981;&#24490;&#31038;&#20250;&#20934;&#21017;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06095</link><description>&lt;p&gt;
&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#21450;&#20854;&#25511;&#21046;&#26041;&#27861;&#65306;&#22833;&#36133;&#21644;&#20462;&#22797;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes. (arXiv:2308.06095v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06095
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20197;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24178;&#39044;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25968;&#25454;&#12289;&#35757;&#32451;&#21046;&#24230;&#25110;&#35299;&#30721;&#65292;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#27969;&#30021;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#36830;&#36143;&#24615;&#20197;&#21450;&#36981;&#24490;&#31038;&#20250;&#20934;&#21017;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20197;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#30475;&#20284;&#27969;&#21033;&#30340;&#26041;&#24335;&#24310;&#32493;&#20219;&#20309;&#31867;&#22411;&#30340;&#25991;&#26412;&#26469;&#28304;&#12290;&#36825;&#20010;&#20107;&#23454;&#20419;&#36827;&#20102;&#23545;&#22522;&#20110;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#36866;&#24403;&#30340;&#23545;&#35805;&#20869;&#23481;&#26469;&#27169;&#20223;&#23545;&#35805;&#26041;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20174;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#21442;&#19982;&#23545;&#35805;&#30340;&#22797;&#26434;&#24615;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#36825;&#19968;&#29305;&#23450;&#30740;&#31350;&#39046;&#22495;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;Grice&#30340;&#21512;&#20316;&#24615;&#23545;&#35805;&#26368;&#22823;&#35268;&#21017;&#65292;&#24182;&#23558;&#25991;&#29486;&#31995;&#32479;&#21270;&#22320;&#24402;&#32435;&#20026;&#19968;&#20010;&#36129;&#29486;&#20309;&#31181;&#20869;&#23481;&#26159;&#36866;&#24403;&#30340;&#26041;&#38754;&#65306;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#24517;&#39035;&#27969;&#30021;&#12289;&#20449;&#24687;&#20016;&#23500;&#12289;&#19968;&#33268;&#12289;&#36830;&#36143;&#65292;&#24182;&#36981;&#24490;&#31038;&#20250;&#20934;&#21017;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#29305;&#24615;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#22312;&#25968;&#25454;&#12289;&#35757;&#32451;&#21046;&#24230;&#25110;&#35299;&#30721;&#31561;&#21508;&#20010;&#24178;&#39044;&#28857;&#19978;&#25511;&#21046;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#12290;&#25353;&#29031;&#36825;&#20123;&#31867;&#21035;&#21644;&#24178;&#39044;&#28857;&#36827;&#34892;&#25490;&#24207;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising at
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24378;&#21270;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36880;&#27493;&#20248;&#21270;&#30340;&#26041;&#27861;&#25193;&#23637;&#35299;&#37322;&#24615;&#30340;&#35268;&#21017;&#38598;&#26469;&#35299;&#37322;&#26102;&#38388;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#25628;&#32034;&#31574;&#30053;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#26032;&#30340;&#35268;&#21017;&#20869;&#23481;&#21644;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.06094</link><description>&lt;p&gt;
&#24378;&#21270;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Logic Rule Learning for Temporal Point Processes. (arXiv:2308.06094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24378;&#21270;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36880;&#27493;&#20248;&#21270;&#30340;&#26041;&#27861;&#25193;&#23637;&#35299;&#37322;&#24615;&#30340;&#35268;&#21017;&#38598;&#26469;&#35299;&#37322;&#26102;&#38388;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#25628;&#32034;&#31574;&#30053;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#26032;&#30340;&#35268;&#21017;&#20869;&#23481;&#21644;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#36880;&#27493;&#25193;&#23637;&#35299;&#37322;&#24615;&#30340;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#38598;&#65292;&#20197;&#35299;&#37322;&#26102;&#38388;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#21033;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#24314;&#27169;&#21644;&#23398;&#20064;&#26694;&#26550;&#65292;&#35268;&#21017;&#20869;&#23481;&#21644;&#26435;&#37325;&#23558;&#36880;&#28176;&#20248;&#21270;&#65292;&#30452;&#21040;&#35266;&#27979;&#20107;&#20214;&#24207;&#21015;&#30340;&#20284;&#28982;&#24615;&#26368;&#20248;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24403;&#21069;&#35268;&#21017;&#38598;&#26435;&#37325;&#26356;&#26032;&#30340;&#20027;&#38382;&#39064;&#21644;&#25628;&#32034;&#24182;&#21253;&#21547;&#26368;&#20339;&#22686;&#21152;&#20284;&#28982;&#24615;&#30340;&#26032;&#35268;&#21017;&#30340;&#23376;&#38382;&#39064;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;&#25152;&#21046;&#23450;&#30340;&#20027;&#38382;&#39064;&#26159;&#20984;&#30340;&#65292;&#20351;&#29992;&#36830;&#32493;&#20248;&#21270;&#30456;&#23545;&#23481;&#26131;&#27714;&#35299;&#65292;&#32780;&#23376;&#38382;&#39064;&#21017;&#38656;&#35201;&#25628;&#32034;&#24040;&#22823;&#30340;&#32452;&#21512;&#35268;&#21017;&#35859;&#35789;&#21644;&#20851;&#31995;&#31354;&#38388;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#25628;&#32034;&#31574;&#30053;&#65292;&#23398;&#20064;&#29983;&#25104;&#26032;&#35268;&#21017;&#20869;&#23481;&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#12290;&#31574;&#30053;&#21442;&#25968;&#23558;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20854;&#20013;&#22870;&#21169;&#20449;&#21495;&#21487;&#20197;&#39640;&#25928;&#22320;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework that can incrementally expand the explanatory temporal logic rule set to explain the occurrence of temporal events. Leveraging the temporal point process modeling and learning framework, the rule content and weights will be gradually optimized until the likelihood of the observational event sequences is optimal. The proposed algorithm alternates between a master problem, where the current rule set weights are updated, and a subproblem, where a new rule is searched and included to best increase the likelihood. The formulated master problem is convex and relatively easy to solve using continuous optimization, whereas the subproblem requires searching the huge combinatorial rule predicate and relationship space. To tackle this challenge, we propose a neural search policy to learn to generate the new rule content as a sequence of actions. The policy parameters will be trained end-to-end using the reinforcement learning framework, where the reward signals can be effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#19987;&#23478;&#26435;&#37325;&#24179;&#22343;&#21270;&#23454;&#29616;&#20102;&#23545;ViTs&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#26029;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.06093</link><description>&lt;p&gt;
&#19987;&#23478;&#26435;&#37325;&#24179;&#22343;&#21270;: &#19968;&#31181;&#35270;&#35273;Transformer&#30340;&#26032;&#36890;&#29992;&#35757;&#32451;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Experts Weights Averaging: A New General Training Scheme for Vision Transformers. (arXiv:2308.06093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#19987;&#23478;&#26435;&#37325;&#24179;&#22343;&#21270;&#23454;&#29616;&#20102;&#23545;ViTs&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#26029;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#36890;&#29992;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#25512;&#26029;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#12290;&#38543;&#30528;&#35270;&#35273;Transformer (ViTs)&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#36880;&#28176;&#36229;&#36234;CNNs&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;: &#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;ViTs&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#25512;&#26029;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#65311;&#26368;&#36817;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#26377;&#25928;&#22320;&#25193;&#23637;Transformer&#30340;&#23481;&#37327;&#65292;&#32780;&#25104;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#32771;&#34385;&#21040;MoE&#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#22810;&#25903;&#31995;&#32467;&#26500;&#65292;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;MoE&#26469;&#23454;&#29616;&#31867;&#20284;&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#30340;ViT&#35757;&#32451;&#26041;&#26696;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ViTs&#36890;&#29992;&#35757;&#32451;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20998;&#31163;&#20102;ViTs&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#26367;&#25442;&#20102;&#19968;&#20123;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;
&lt;/p&gt;
&lt;p&gt;
Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the 
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.06091</link><description>&lt;p&gt;
&#23545;&#21327;&#21516;&#36807;&#28388;&#20002;&#22833;&#20989;&#25968;&#30340;&#26356;&#22909;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Toward a Better Understanding of Loss Functions for Collaborative Filtering. (arXiv:2308.06091v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06091
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26159;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;CF&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#20132;&#20114;&#32534;&#30721;&#22120;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#36127;&#37319;&#26679;&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;CF&#27169;&#22411;&#26469;&#35774;&#35745;&#22797;&#26434;&#30340;&#20132;&#20114;&#32534;&#30721;&#22120;&#65292;&#20294;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#21046;&#23450;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#20998;&#26512;&#25581;&#31034;&#20102;&#20808;&#21069;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35299;&#37322;&#20026;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#20989;&#25968;&#65306;&#65288;i&#65289;&#23545;&#40784;&#21305;&#37197;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#22343;&#21248;&#24615;&#20998;&#25955;&#29992;&#25143;&#21644;&#29289;&#21697;&#20998;&#24067;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#31216;&#20026;Margin-aware Alignment and Weighted Uniformity&#65288;MAWU&#65289;&#12290;MAWU&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#33021;&#28304;&#31995;&#32479;&#25511;&#21046;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#21270;&#21152;&#24378;&#23433;&#20840;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#31163;&#25955;&#21270;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#20808;&#36827;&#30340;&#24037;&#31243;&#26041;&#27861;&#24182;&#36827;&#34892;&#24418;&#24335;&#39564;&#35777;&#65292;&#20854;&#20013;&#27010;&#29575;&#20445;&#35777;&#24418;&#25104;&#20102;&#28385;&#36275;&#21407;&#22987;&#23433;&#20840;&#35201;&#27714;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.06069</link><description>&lt;p&gt;
&#29992;&#37319;&#26679;&#35268;&#33539;&#20445;&#25252;&#26234;&#33021;&#33021;&#28304;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications. (arXiv:2308.06069v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#33021;&#28304;&#31995;&#32479;&#25511;&#21046;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#21270;&#21152;&#24378;&#23433;&#20840;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#31163;&#25955;&#21270;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#20808;&#36827;&#30340;&#24037;&#31243;&#26041;&#27861;&#24182;&#36827;&#34892;&#24418;&#24335;&#39564;&#35777;&#65292;&#20854;&#20013;&#27010;&#29575;&#20445;&#35777;&#24418;&#25104;&#20102;&#28385;&#36275;&#21407;&#22987;&#23433;&#20840;&#35201;&#27714;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33021;&#28304;&#31995;&#32479;&#25511;&#21046;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#38500;&#20102;&#24615;&#33021;&#35201;&#27714;&#22806;&#65292;&#36824;&#26377;&#39069;&#22806;&#30340;&#23433;&#20840;&#35201;&#27714;&#65292;&#22914;&#36991;&#20813;&#20572;&#30005;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#23433;&#20840;&#35201;&#27714;&#22312;&#23454;&#26102;&#26102;&#24577;&#36923;&#36753;&#20013;&#36827;&#34892;&#31163;&#25955;&#21270;&#21152;&#24378;&#65292;&#20351;&#24471;LTL&#20844;&#24335;&#30340;&#28385;&#36275;&#24847;&#21619;&#30528;&#28385;&#36275;&#21407;&#22987;&#30340;&#23433;&#20840;&#35201;&#27714;&#12290;&#31163;&#25955;&#21270;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#20808;&#36827;&#30340;&#24037;&#31243;&#26041;&#27861;&#65292;&#22914;&#20026;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#21512;&#25104;&#30462;&#29260;&#20197;&#21450;&#24418;&#24335;&#39564;&#35777;&#65292;&#20854;&#20013;&#23545;&#20110;&#32479;&#35745;&#27169;&#22411;&#26816;&#39564;&#65292;LTL&#27169;&#22411;&#26816;&#39564;&#24471;&#21040;&#30340;&#27010;&#29575;&#20445;&#35777;&#24418;&#25104;&#20102;&#28385;&#36275;&#21407;&#22987;&#23454;&#26102;&#23433;&#20840;&#35201;&#27714;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study challenges using reinforcement learning in controlling energy systems, where apart from performance requirements, one has additional safety requirements such as avoiding blackouts. We detail how these safety requirements in real-time temporal logic can be strengthened via discretization into linear temporal logic (LTL), such that the satisfaction of the LTL formulae implies the satisfaction of the original safety requirements. The discretization enables advanced engineering methods such as synthesizing shields for safe reinforcement learning as well as formal verification, where for statistical model checking, the probabilistic guarantee acquired by LTL model checking forms a lower bound for the satisfaction of the original real-time safety requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AdaSPS&#21644;AdaSLS&#20004;&#31181;&#26032;&#30340;&#21464;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SGD&#22312;&#38750;&#25554;&#20540;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#26102;&#20445;&#25345;&#32447;&#24615;&#21644;&#20122;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.06058</link><description>&lt;p&gt;
&#24102;&#26377;Polyak&#27493;&#38271;&#21644;&#32447;&#24615;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;SGD: &#40065;&#26834;&#25910;&#25947;&#21644;&#26041;&#24046;&#20943;&#23567;
&lt;/p&gt;
&lt;p&gt;
Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction. (arXiv:2308.06058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AdaSPS&#21644;AdaSLS&#20004;&#31181;&#26032;&#30340;&#21464;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SGD&#22312;&#38750;&#25554;&#20540;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#26102;&#20445;&#25345;&#32447;&#24615;&#21644;&#20122;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#38543;&#26426;Polyak&#27493;&#38271; (SPS) &#21644;&#38543;&#26426;&#32447;&#24615;&#25628;&#32034; (SLS) &#22312;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#26102;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#25554;&#20540;&#29615;&#22659;&#19979;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#21482;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#35299;&#30340;&#37051;&#22495;&#65292;&#21487;&#33021;&#23548;&#33268;&#27604;&#21021;&#22987;&#29468;&#27979;&#26356;&#24046;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20154;&#20026;&#20943;&#23567;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064; (Orvieto et al. [2022])&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#20984;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SPS&#21644;SLS&#21464;&#31181;&#65292;&#20998;&#21035;&#31216;&#20026;AdaSPS&#21644;AdaSLS&#65292;&#23427;&#20204;&#22312;&#38750;&#25554;&#20540;&#29615;&#22659;&#20013;&#20445;&#35777;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#26102;&#20445;&#25345;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#20122;&#32447;&#24615;&#21644;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;AdaSLS&#19981;&#38656;&#35201;&#23545;&#38382;&#39064;&#30456;&#20851;&#21442;&#25968;&#30340;&#20102;&#35299;&#65292;&#32780;AdaSPS&#21482;&#38656;&#35201;&#26368;&#20248;&#20989;&#25968;&#20540;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al. [2022]), this approach results in slower convergence rates for convex and over-parameterized models. In this work, we make two contributions: Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06053</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#19978;&#20855;&#26377;MiRo&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#22791;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20174;&#25345;&#32493;&#30340;&#20219;&#21153;&#27969;&#20013;&#36880;&#27493;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#35760;&#20303;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#26087;&#26679;&#26412;&#23384;&#20648;&#22312;&#19968;&#20010;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#21040;&#26469;&#26102;&#36827;&#34892;&#22238;&#25918;&#12290;&#37319;&#29992;&#25345;&#32493;&#23398;&#20064;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#23545;&#33021;&#28304;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#25439;&#23475;&#33021;&#28304;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#27169;&#22411;&#20934;&#30830;&#24230;&#65292;&#21363;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20197;&#33719;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#36890;&#36807;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#36164;&#28304;&#29366;&#24577;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#65292;&#20174;&#32780;&#23558;&#25105;&#20204;&#30340;&#35265;&#35299;&#31934;&#30830;&#22320;&#25972;&#21512;&#21040;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#25104;&#26412;&#25928;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;Miro&#36824;&#23545;&#24102;&#26377;&#26126;&#30830;&#20934;&#30830;&#24230;-&#33021;&#37327;&#24179;&#34913;&#30340;&#21442;&#25968;&#36827;&#34892;&#22312;&#32447;&#20998;&#26512;&#65292;&#24182;&#20197;&#20302;&#24320;&#38144;&#22320;&#36866;&#24212;&#26368;&#20339;&#20540;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23454;&#20363;&#33258;&#36866;&#24212;&#25512;&#29702;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32553;&#25918;&#21644;&#20301;&#31227;&#28145;&#24230;&#29305;&#24449;&#65288;SSF&#65289;&#23454;&#29616;&#20102;&#22788;&#29702;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06051</link><description>&lt;p&gt;
&#38754;&#21521;&#23454;&#20363;&#33258;&#36866;&#24212;&#25512;&#29702;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Instance-adaptive Inference for Federated Learning. (arXiv:2308.06051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23454;&#20363;&#33258;&#36866;&#24212;&#25512;&#29702;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32553;&#25918;&#21644;&#20301;&#31227;&#28145;&#24230;&#29305;&#24449;&#65288;SSF&#65289;&#23454;&#29616;&#20102;&#22788;&#29702;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#27719;&#38598;&#26412;&#22320;&#35757;&#32451;&#26469;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#20943;&#36731;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;&#36229;&#36234;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#20063;&#21487;&#20197;&#35266;&#23519;&#21040;&#23458;&#25143;&#31471;&#20869;&#37096;&#30340;&#24322;&#36136;&#24615;&#65292;&#20005;&#37325;&#24433;&#21709;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;FedIns&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#23454;&#20363;&#33258;&#36866;&#24212;&#25512;&#29702;&#26469;&#22788;&#29702;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#24222;&#22823;&#30340;&#23454;&#20363;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;&#32553;&#25918;&#21644;&#20301;&#31227;&#28145;&#24230;&#29305;&#24449;&#65288;SSF&#65289;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#35757;&#32451;&#19968;&#20010;SSF&#27744;&#65292;&#22312;&#26381;&#21153;&#22120;&#31471;&#27719;&#38598;&#36825;&#20123;SSF&#27744;&#65292;&#20174;&#32780;&#20173;&#28982;&#20445;&#25345;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;VQ-VAE&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#20196;&#29260;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39030;&#37096;&#27880;&#20837;&#39640;&#32423;&#20808;&#39564;&#27169;&#22411;&#26469;&#29983;&#25104;&#26080;&#38480;&#38271;&#19988;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2308.06025</link><description>&lt;p&gt;
&#19981;&#21487;&#35266;&#27979;&#30340;&#39537;&#21160;&#28304;&#19979;&#25511;&#21046;&#35282;&#33394;&#21160;&#20316;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Controlling Character Motions without Observable Driving Source. (arXiv:2308.06025v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;VQ-VAE&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#20196;&#29260;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39030;&#37096;&#27880;&#20837;&#39640;&#32423;&#20808;&#39564;&#27169;&#22411;&#26469;&#29983;&#25104;&#26080;&#38480;&#38271;&#19988;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#27809;&#26377;&#20219;&#20309;&#39537;&#21160;&#28304;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#36924;&#30495;&#19988;&#26080;&#38480;&#38271;&#30340;&#22836;&#37096;/&#36523;&#20307;&#24207;&#21015;&#65311;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#26410;&#32463;&#20805;&#20998;&#25506;&#31350;&#30340;&#30740;&#31350;&#38382;&#39064;&#24182;&#19981;&#26159;&#19968;&#20214;&#36731;&#26494;&#30340;&#20107;&#65292;&#24182;&#19988;&#23384;&#22312;&#30528;&#29420;&#29305;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#22312;&#27809;&#26377;&#26469;&#33258;&#39537;&#21160;&#28304;&#30340;&#35821;&#20041;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#38271;&#24207;&#21015;&#24456;&#23481;&#26131;&#23548;&#33268;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#30001;&#20110;&#32047;&#31215;&#35823;&#24046;&#20135;&#29983;&#20102;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#38382;&#39064;&#65307;2&#65289;&#29983;&#25104;&#30340;&#36816;&#21160;&#24207;&#21015;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#26080;&#27861;&#20135;&#29983;&#36924;&#30495;&#21644;&#29983;&#21160;&#30340;&#21160;&#20316;&#24207;&#21015;&#65307;3&#65289;&#26102;&#38388;&#19978;&#20986;&#29616;&#20102;&#19981;&#26399;&#26395;&#30340;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#23558;VQ-VAE&#30340;&#20248;&#28857;&#19982;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#20196;&#29260;&#32423;&#25511;&#21046;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#39030;&#37096;&#27880;&#20837;&#39640;&#32423;&#20808;&#39564;&#27169;&#22411;&#26469;&#29983;&#25104;&#26080;&#38480;&#38271;&#19988;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#12290;&#23613;&#31649;&#25105;&#20204;&#29616;&#22312;&#20851;&#27880;&#20110;&#27809;&#26377;&#39537;&#21160;&#28304;&#65292;&#20294;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25512;&#24191;&#21040;&#21463;&#25511;&#21512;&#25104;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthe
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#23558;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#31616;&#21270;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#20351;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.06013</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#30340;&#26410;&#26469;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Telecom: Forthcoming Impact on the Industry. (arXiv:2308.06013v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06013
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#23558;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#31616;&#21270;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#20351;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#32929;&#21464;&#38761;&#30340;&#21147;&#37327;&#65292;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20256;&#32479;&#39046;&#22495;&#20043;&#22806;&#65292;&#36824;&#22312;&#35768;&#22810;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#20851;&#27880;&#12290;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#30005;&#20449;&#34892;&#19994;&#38754;&#20020;&#30528;&#28508;&#22312;&#24433;&#21709;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#24433;&#21709;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;LLMs&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20204;&#30446;&#21069;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#30005;&#20449;&#34892;&#19994;&#21487;&#20197;&#26041;&#20415;&#23454;&#26045;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#31616;&#21270;&#20102;&#30446;&#21069;&#22952;&#30861;&#36816;&#33829;&#25928;&#29575;&#24182;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22312;&#30005;&#20449;&#39046;&#22495;&#21033;&#29992;LLMs&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26159;&#20805;&#20998;&#21033;&#29992;LLMs&#28508;&#21147;&#21644;&#21457;&#25381;&#20854;&#33021;&#21147;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fulles
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#20026;&#31185;&#23398;&#26159;&#21542;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#20197;&#26426;&#22120;&#23398;&#20064;&#21183;&#22330;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#21457;&#29616;&#24182;&#35299;&#20915;&#22312;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;MLFF&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#26679;&#26412;&#25928;&#29575;&#12289;&#26102;&#38388;&#22495;&#25935;&#24863;&#24615;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05999</link><description>&lt;p&gt;
AI&#20026;&#31185;&#23398;&#26159;&#21542;&#38656;&#35201;&#21478;&#19968;&#20010;ImageNet&#25110;&#23436;&#20840;&#19981;&#21516;&#30340;&#22522;&#20934;&#65311;&#26426;&#22120;&#23398;&#20064;&#21183;&#22330;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields. (arXiv:2308.05999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#20026;&#31185;&#23398;&#26159;&#21542;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#20197;&#26426;&#22120;&#23398;&#20064;&#21183;&#22330;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#21457;&#29616;&#24182;&#35299;&#20915;&#22312;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;MLFF&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#26679;&#26412;&#25928;&#29575;&#12289;&#26102;&#38388;&#22495;&#25935;&#24863;&#24615;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI for science (AI4S)&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#31185;&#23398;&#35745;&#31639;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;&#20256;&#32479;&#30340;AI&#22522;&#20934;&#26041;&#27861;&#22312;&#36866;&#24212;AI4S&#39046;&#22495;&#30340;&#29420;&#29305;&#25361;&#25112;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#26410;&#26469;&#30340;&#30495;&#23454;&#26597;&#35810;&#25968;&#25454;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#32780;AI4S&#24037;&#20316;&#36127;&#36733;&#21017;&#39044;&#26399;&#23384;&#22312;&#20998;&#24067;&#19981;&#21516;&#30340;&#38382;&#39064;&#23454;&#20363;&#12290;&#26412;&#25991;&#20197;&#26426;&#22120;&#23398;&#20064;&#21183;&#22330;&#65288;MLFF&#65289;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#26377;&#25928;&#35780;&#20272;AI for science&#38656;&#35201;&#26032;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;MLFF&#26159;&#19968;&#31181;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#39640;&#31934;&#24230;&#30340;&#21152;&#36895;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#31185;&#23398;&#24847;&#20041;&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#21508;&#31181;&#38169;&#22833;&#30340;&#26426;&#20250;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#35780;&#20272;MLFF&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#12289;&#26102;&#38388;&#22495;&#25935;&#24863;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI for science (AI4S) is an emerging research field that aims to enhance the accuracy and speed of scientific computing tasks using machine learning methods. Traditional AI benchmarking methods struggle to adapt to the unique challenges posed by AI4S because they assume data in training, testing, and future real-world queries are independent and identically distributed, while AI4S workloads anticipate out-of-distribution problem instances. This paper investigates the need for a novel approach to effectively benchmark AI for science, using the machine learning force field (MLFF) as a case study. MLFF is a method to accelerate molecular dynamics (MD) simulation with low computational cost and high accuracy. We identify various missed opportunities in scientifically meaningful benchmarking and propose solutions to evaluate MLFF models, specifically in the aspects of sample efficiency, time domain sensitivity, and cross-dataset generalization capabilities. By setting up the problem instant
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35780;&#20272;&#31867;&#20869;&#29305;&#24449;&#26041;&#24046;&#24555;&#36895;&#20934;&#30830;&#22320;&#27979;&#37327;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29992;&#20110;&#36873;&#25321;&#26368;&#36866;&#21512;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21253;&#25324;&#26377;&#21644;&#26080;&#20998;&#31867;&#22120;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#36873;&#25321;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20339;&#36716;&#31227;&#23618;&#12290;</title><link>http://arxiv.org/abs/2308.05986</link><description>&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#31867;&#20869;&#29305;&#24449;&#26041;&#24046;&#24555;&#36895;&#20934;&#30830;&#22320;&#27979;&#37327;&#21487;&#36716;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance. (arXiv:2308.05986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35780;&#20272;&#31867;&#20869;&#29305;&#24449;&#26041;&#24046;&#24555;&#36895;&#20934;&#30830;&#22320;&#27979;&#37327;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29992;&#20110;&#36873;&#25321;&#26368;&#36866;&#21512;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21253;&#25324;&#26377;&#21644;&#26080;&#20998;&#31867;&#22120;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#36873;&#25321;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20339;&#36716;&#31227;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;&#20309;&#24555;&#36895;&#20934;&#30830;&#22320;&#25214;&#21040;&#26368;&#36866;&#21512;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65311;&#21487;&#36716;&#31227;&#24615;&#27979;&#37327;&#26159;&#35780;&#20272;&#19968;&#20010;&#22312;&#28304;&#20219;&#21153;&#19978;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#21487;&#36716;&#31227;&#24615;&#30340;&#25351;&#26631;&#12290;&#23427;&#29992;&#20110;&#24555;&#36895;&#23545;&#32473;&#23450;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#65292;&#22240;&#27492;&#25104;&#20026;&#36801;&#31227;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#21487;&#36716;&#31227;&#24615;&#23450;&#20041;&#20026;&#22312;&#36801;&#31227;&#23398;&#20064;&#20043;&#21069;&#65292;&#28304;&#27169;&#22411;&#23545;&#20110;&#30446;&#26631;&#25968;&#25454;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#26080;&#27861;&#20934;&#30830;&#20272;&#35745;&#24494;&#35843;&#24615;&#33021;&#12290;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#38480;&#21046;&#20102;&#21487;&#36716;&#31227;&#24615;&#27979;&#37327;&#22312;&#36873;&#25321;&#20855;&#26377;&#20998;&#31867;&#22120;&#30340;&#26368;&#20339;&#26377;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#33021;&#22815;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#24212;&#29992;&#30340;&#36890;&#29992;&#21487;&#36716;&#31227;&#24615;&#27979;&#37327;&#26041;&#27861;&#21313;&#20998;&#37325;&#35201;&#65292;&#20363;&#22914;&#36873;&#25321;&#27809;&#26377;&#20998;&#31867;&#22120;&#30340;&#26368;&#20339;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#36873;&#25321;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20339;&#36716;&#31227;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a set of pre-trained models, how can we quickly and accurately find the most useful pre-trained model for a downstream task? Transferability measurement is to quantify how transferable is a pre-trained model learned on a source task to a target task. It is used for quickly ranking pre-trained models for a given task and thus becomes a crucial step for transfer learning. Existing methods measure transferability as the discrimination ability of a source model for a target data before transfer learning, which cannot accurately estimate the fine-tuning performance. Some of them restrict the application of transferability measurement in selecting the best supervised pre-trained models that have classifiers. It is important to have a general method for measuring transferability that can be applied in a variety of situations, such as selecting the best self-supervised pre-trained models that do not have classifiers, and selecting the best transferring layer for a target task. In this wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#38454;HSIC&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;Bayesian&#32593;&#32476;&#20013;&#35299;&#20915;&#20102;&#23616;&#37096;&#21464;&#37327;&#21516;&#26102;&#20855;&#26377;&#30452;&#25509;&#21644;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#30830;&#23450;&#23376;&#38598;&#21644;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#36827;&#34892;&#23616;&#37096;&#20462;&#27491;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05969</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;HSIC&#23398;&#20064;&#20855;&#26377;&#22686;&#37327;&#20449;&#24687;&#30340;&#38750;&#21442;&#25968;DAGs
&lt;/p&gt;
&lt;p&gt;
Learning nonparametric DAGs with incremental information via high-order HSIC. (arXiv:2308.05969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#38454;HSIC&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;Bayesian&#32593;&#32476;&#20013;&#35299;&#20915;&#20102;&#23616;&#37096;&#21464;&#37327;&#21516;&#26102;&#20855;&#26377;&#30452;&#25509;&#21644;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#30830;&#23450;&#23376;&#38598;&#21644;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#36827;&#34892;&#23616;&#37096;&#20462;&#27491;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20840;&#23616;&#35780;&#20998;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23616;&#37096;&#21464;&#37327;&#21516;&#26102;&#20855;&#26377;&#30452;&#25509;&#21644;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#65292;&#37027;&#20040;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#23558;&#24573;&#30053;&#20855;&#26377;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#36793;&#32536;&#65292;&#20854;&#24471;&#20998;&#23567;&#20110;&#20855;&#26377;&#30452;&#25509;&#20381;&#36182;&#20851;&#31995;&#30340;&#36793;&#32536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30830;&#23450;&#23376;&#38598;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;DAG&#12290;&#36890;&#36807;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#21363;&#26368;&#20248;&#35843;&#25972;&#65288;OT&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#20840;&#23616;&#20248;&#21270;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#23616;&#37096;&#20462;&#27491;&#12290;&#22312;&#26368;&#20248;&#38454;&#27573;&#65292;&#22522;&#20110;&#19968;&#38454;Hilbert-Schmidt&#29420;&#31435;&#24615;&#20934;&#21017;&#65288;HSIC&#65289;&#30340;&#20248;&#21270;&#38382;&#39064;&#32473;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#30340;&#39592;&#26550;&#20316;&#20026;&#21021;&#22987;&#30830;&#23450;&#30340;&#29238;&#33410;&#28857;&#23376;&#38598;&#12290;&#22312;&#35843;&#25972;&#38454;&#27573;&#65292;&#26681;&#25454;&#39640;&#38454;HSIC&#30340;&#29702;&#35770;&#35777;&#26126;&#22686;&#37327;&#29305;&#24615;&#65292;&#23545;&#39592;&#26550;&#36827;&#34892;&#23616;&#37096;&#35843;&#25972;&#65292;&#21253;&#25324;&#21024;&#38500;&#12289;&#28155;&#21152;&#21644;DAG&#26684;&#24335;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based methods for learning Bayesain networks(BN) aim to maximizing the global score functions. However, if local variables have direct and indirect dependence simultaneously, the global optimization on score functions misses edges between variables with indirect dependent relationship, of which scores are smaller than those with direct dependent relationship. In this paper, we present an identifiability condition based on a determined subset of parents to identify the underlying DAG. By the identifiability condition, we develop a two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the global optimization. In the optimal phase, an optimization problem based on first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated skeleton as the initial determined parents subset. In the tuning phase, the skeleton is locally tuned by deletion, addition and DAG-formalization strategies using the theoretically proved incremental properties of high-order HS
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#22411;&#28857;&#20113;&#32534;&#35299;&#30721;&#22120;&#65292;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#23545;&#27604;&#24230;-&#31934;&#24230;&#24179;&#34913;&#65292;&#24182;&#19988;&#22312;ModelNet40&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;94%&#30340;BD&#27604;&#29305;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2308.05959</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#28857;&#20113;&#21387;&#32553;&#29992;&#20110;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learned Point Cloud Compression for Classification. (arXiv:2308.05959v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05959
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#22411;&#28857;&#20113;&#32534;&#35299;&#30721;&#22120;&#65292;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#23545;&#27604;&#24230;-&#31934;&#24230;&#24179;&#34913;&#65292;&#24182;&#19988;&#22312;ModelNet40&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;94%&#30340;BD&#27604;&#29305;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#22312;3D&#28857;&#20113;&#25968;&#25454;&#19978;&#36827;&#34892;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#26426;&#22120;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#25512;&#26029;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#32456;&#31471;&#35774;&#22791;&#30340;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#32534;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#32593;&#32476;&#19978;&#20256;&#36755;&#28857;&#20113;&#25968;&#25454;&#36827;&#34892;&#26381;&#21153;&#22120;&#31471;&#22788;&#29702;&#12290;&#36825;&#26679;&#30340;&#32534;&#35299;&#30721;&#22120;&#24517;&#39035;&#36731;&#24039;&#19988;&#33021;&#22815;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#21387;&#32553;&#27604;&#12290;&#21463;&#27492;&#28608;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#32534;&#35299;&#30721;&#22120;&#65292;&#19987;&#38376;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32534;&#35299;&#30721;&#22120;&#22522;&#20110;PointNet&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#23545;&#27604;&#24230;-&#31934;&#24230;&#24179;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;ModelNet40&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#38750;&#19987;&#29992;&#32534;&#35299;&#30721;&#22120;&#30456;&#27604;&#65292;BD&#27604;&#29305;&#29575;&#38477;&#20302;&#20102;94%&#12290;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#32456;&#31471;&#35774;&#22791;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#37197;&#32622;&#30340;&#32534;&#30721;&#22120;&#65292;&#20854;BD&#27604;&#29305;&#29575;&#21066;&#20943;&#29575;&#20998;&#21035;&#20026;93%&#21644;92%&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is increasingly being used to perform machine vision tasks such as classification, object detection, and segmentation on 3D point cloud data. However, deep learning inference is computationally expensive. The limited computational capabilities of end devices thus necessitate a codec for transmitting point cloud data over the network for server-side processing. Such a codec must be lightweight and capable of achieving high compression ratios without sacrificing accuracy. Motivated by this, we present a novel point cloud codec that is highly specialized for the machine task of classification. Our codec, based on PointNet, achieves a significantly better rate-accuracy trade-off in comparison to alternative methods. In particular, it achieves a 94% reduction in BD-bitrate over non-specialized codecs on the ModelNet40 dataset. For low-resource end devices, we also propose two lightweight configurations of our encoder that achieve similar BD-bitrate reductions of 93% and 92% wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ARGEW&#30340;&#22686;&#24378;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33021;&#22815;&#26356;&#22909;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#36793;&#26435;&#37325;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.05957</link><description>&lt;p&gt;
&#24102;&#26377;ARGEW&#30340;&#21516;&#36136;&#22270;&#33410;&#28857;&#23884;&#20837;&#65306;&#36890;&#36807;&#22270;&#36793;&#26435;&#37325;&#22686;&#24378;&#30340;&#38543;&#26426;&#28216;&#36208;
&lt;/p&gt;
&lt;p&gt;
Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights. (arXiv:2308.05957v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05957
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ARGEW&#30340;&#22686;&#24378;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33021;&#22815;&#26356;&#22909;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#36793;&#26435;&#37325;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#20026;&#23494;&#38598;&#21521;&#37327;&#33410;&#28857;&#23884;&#20837;&#23545;&#20110;&#29702;&#35299;&#32473;&#23450;&#32593;&#32476;&#21644;&#35299;&#20915;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#26435;&#37325;&#21516;&#36136;&#22270;&#65292;&#20854;&#20013;&#20855;&#26377;&#30456;&#20284;&#33410;&#28857;&#30340;&#36793;&#32536;&#26435;&#37325;&#36739;&#22823;&#65292;&#25105;&#20204;&#24076;&#26395;&#33410;&#28857;&#23884;&#20837;&#20013;&#20855;&#26377;&#24378;&#26435;&#37325;&#30340;&#33410;&#28857;&#23545;&#20855;&#26377;&#26356;&#25509;&#36817;&#30340;&#23884;&#20837;&#12290;&#34429;&#28982;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#22914;node2vec&#21644;node2vec+&#65292;&#36890;&#36807;&#23558;&#36793;&#26435;&#37325;&#21253;&#21547;&#22312;&#34892;&#36208;&#36716;&#31227;&#27010;&#29575;&#20013;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#21152;&#26435;&#32593;&#32476;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;&#32467;&#26524;&#19981;&#36275;&#20197;&#21453;&#26144;&#36793;&#26435;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARGEW&#65288;&#36890;&#36807;&#22270;&#36793;&#26435;&#37325;&#22686;&#24378;&#30340;&#38543;&#26426;&#28216;&#36208;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#28216;&#36208;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#35821;&#26009;&#24211;&#20351;&#20855;&#26377;&#36739;&#22823;&#36793;&#26435;&#37325;&#30340;&#33410;&#28857;&#26368;&#32456;&#20855;&#26377;&#26356;&#25509;&#36817;&#30340;&#23884;&#20837;&#12290;ARGEW&#21487;&#20197;&#19982;&#20219;&#20309;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#29420;&#31435;&#20110;&#38543;&#26426;&#37319;&#26679;&#31574;&#30053;&#26412;&#36523;&#24182;&#22312;&#24050;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing nodes in a network as dense vectors node embeddings is important for understanding a given network and solving many downstream tasks. In particular, for weighted homophilous graphs where similar nodes are connected with larger edge weights, we desire node embeddings where node pairs with strong weights have closer embeddings. Although random walk based node embedding methods like node2vec and node2vec+ do work for weighted networks via including edge weights in the walk transition probabilities, our experiments show that the embedding result does not adequately reflect edge weights. In this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge Weights), a novel augmentation method for random walks that expands the corpus in such a way that nodes with larger edge weights end up with closer embeddings. ARGEW can work with any random walk based node embedding method, because it is independent of the random sampling strategy itself and works on top of the already
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;INR-Arch&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#24449;&#22788;&#29702;&#20013;&#20219;&#24847;&#38454;&#26799;&#24230;&#35745;&#31639;&#30340;&#25968;&#25454;&#27969;&#26550;&#26500;&#21644;&#32534;&#35793;&#22120;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#23558;&#35745;&#31639;&#22270;&#36716;&#21270;&#20026;&#30828;&#20214;&#20248;&#21270;&#30340;&#25968;&#25454;&#27969;&#26550;&#26500;&#26469;&#35299;&#20915;&#20102;&#20256;&#32479;&#26550;&#26500;&#22312;&#39640;&#38454;&#26799;&#24230;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#65292;&#20026;FPGA&#21152;&#36895;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.05930</link><description>&lt;p&gt;
INR-Arch&#65306;&#19968;&#31181;&#29992;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#24449;&#22788;&#29702;&#20013;&#20219;&#24847;&#38454;&#26799;&#24230;&#35745;&#31639;&#30340;&#25968;&#25454;&#27969;&#26550;&#26500;&#21644;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing. (arXiv:2308.05930v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;INR-Arch&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#24449;&#22788;&#29702;&#20013;&#20219;&#24847;&#38454;&#26799;&#24230;&#35745;&#31639;&#30340;&#25968;&#25454;&#27969;&#26550;&#26500;&#21644;&#32534;&#35793;&#22120;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#23558;&#35745;&#31639;&#22270;&#36716;&#21270;&#20026;&#30828;&#20214;&#20248;&#21270;&#30340;&#25968;&#25454;&#27969;&#26550;&#26500;&#26469;&#35299;&#20915;&#20102;&#20256;&#32479;&#26550;&#26500;&#22312;&#39640;&#38454;&#26799;&#24230;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#65292;&#20026;FPGA&#21152;&#36895;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#20102;&#23545;&#21508;&#31181;&#24212;&#29992;&#30340;n&#38454;&#26799;&#24230;&#35745;&#31639;&#30340;&#29992;&#36884;&#65292;&#21253;&#25324;&#22270;&#24418;&#12289;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12289;&#31185;&#23398;&#35745;&#31639;&#20197;&#21450;&#26368;&#36817;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#24449;&#65288;INR&#65289;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;INR&#30340;&#26799;&#24230;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#32534;&#36753;&#20854;&#25152;&#20195;&#34920;&#30340;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#23558;&#20854;&#36716;&#25442;&#22238;&#31163;&#25955;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#34920;&#31034;&#20026;&#35745;&#31639;&#22270;&#30340;&#20989;&#25968;&#65292;&#20256;&#32479;&#26550;&#26500;&#22312;&#39640;&#38454;&#26799;&#24230;&#30340;&#39640;&#38656;&#27714;&#21644;&#25968;&#25454;&#31227;&#21160;&#30340;&#39640;&#22797;&#26434;&#24230;&#19979;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;FPGA&#21152;&#36895;&#30340;&#26377;&#24076;&#26395;&#30340;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;INR-Arch&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;n&#38454;&#26799;&#24230;&#30340;&#35745;&#31639;&#22270;&#36716;&#21270;&#20026;&#30828;&#20214;&#20248;&#21270;&#30340;&#25968;&#25454;&#27969;&#26550;&#26500;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20351;&#29992;FIFO&#27969;&#21644;&#20248;&#21270;&#30340;&#35745;&#31639;&#20869;&#26680;&#30340;&#25968;&#25454;&#27969;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing number of researchers are finding use for nth-order gradient computations for a wide variety of applications, including graphics, meta-learning (MAML), scientific computing, and most recently, implicit neural representations (INRs). Recent work shows that the gradient of an INR can be used to edit the data it represents directly without needing to convert it back to a discrete representation. However, given a function represented as a computation graph, traditional architectures face challenges in efficiently computing its nth-order gradient due to the higher demand for computing power and higher complexity in data movement. This makes it a promising target for FPGA acceleration. In this work, we introduce INR-Arch, a framework that transforms the computation graph of an nth-order gradient into a hardware-optimized dataflow architecture. We address this problem in two phases. First, we design a dataflow architecture that uses FIFO streams and an optimized computation kern
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#29420;&#31435;&#20110;&#948;&#30340;&#22797;&#26434;&#24230;&#30340;Occam&#31639;&#27861;&#65292;&#20854;&#37096;&#20998;&#36870;&#21629;&#39064;&#36866;&#29992;&#20110;&#38381;&#21512;&#20110;&#20363;&#22806;&#21015;&#34920;&#30340;&#27010;&#24565;&#31867;&#65292;&#20174;&#32780;&#20026;&#30456;&#20851;&#30340;&#29702;&#35770;&#32467;&#26524;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#25552;&#20379;&#20102;&#20107;&#21518;&#30340;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.05906</link><description>&lt;p&gt;
&#20851;&#20110;Occam&#31639;&#27861;&#30340;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the equivalence of Occam algorithms. (arXiv:2308.05906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05906
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#29420;&#31435;&#20110;&#948;&#30340;&#22797;&#26434;&#24230;&#30340;Occam&#31639;&#27861;&#65292;&#20854;&#37096;&#20998;&#36870;&#21629;&#39064;&#36866;&#29992;&#20110;&#38381;&#21512;&#20110;&#20363;&#22806;&#21015;&#34920;&#30340;&#27010;&#24565;&#31867;&#65292;&#20174;&#32780;&#20026;&#30456;&#20851;&#30340;&#29702;&#35770;&#32467;&#26524;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#25552;&#20379;&#20102;&#20107;&#21518;&#30340;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Blumer&#31561;&#20154;&#65288;1987&#24180;&#12289;1989&#24180;&#65289;&#35777;&#26126;&#20102;&#20219;&#20309;&#21487;&#20197;&#30001;Occam&#31639;&#27861;&#23398;&#20064;&#30340;&#27010;&#24565;&#31867;&#37117;&#26159;PAC&#21487;&#23398;&#20064;&#30340;&#12290;Board&#21644;Pitt&#65288;1990&#24180;&#65289;&#35777;&#26126;&#20102;&#36825;&#20010;&#23450;&#29702;&#30340;&#19968;&#20010;&#37096;&#20998;&#36870;&#21629;&#39064;&#65306;&#23545;&#20110;&#38381;&#21512;&#20110;&#20363;&#22806;&#21015;&#34920;&#30340;&#27010;&#24565;&#31867;&#65292;&#20219;&#20309;&#21487;&#20197;PAC&#23398;&#20064;&#30340;&#31867;&#37117;&#21487;&#20197;&#36890;&#36807;Occam&#31639;&#27861;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;Occam&#31639;&#27861;&#36755;&#20986;&#30340;&#20551;&#35774;&#22797;&#26434;&#24230;&#20381;&#36182;&#20110;&#948;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20182;&#20204;&#30340;&#37096;&#20998;&#36870;&#21629;&#39064;&#20063;&#36866;&#29992;&#20110;&#20855;&#26377;&#29420;&#31435;&#20110;&#948;&#30340;&#22797;&#26434;&#24230;&#30340;Occam&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20026;&#20351;&#29992;&#37096;&#20998;&#36870;&#21629;&#39064;&#20316;&#20026;&#20854;&#24037;&#20316;&#22522;&#30784;&#30340;&#21508;&#31181;&#29702;&#35770;&#32467;&#26524;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#25552;&#20379;&#20102;&#20107;&#21518;&#30340;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blumer et al. (1987, 1989) showed that any concept class that is learnable by Occam algorithms is PAC learnable. Board and Pitt (1990) showed a partial converse of this theorem: for concept classes that are closed under exception lists, any class that is PAC learnable is learnable by an Occam algorithm. However, their Occam algorithm outputs a hypothesis whose complexity is $\delta$-dependent, which is an important limitation. In this paper, we show that their partial converse applies to Occam algorithms with $\delta$-independent complexities as well. Thus, we provide a posteriori justification of various theoretical results and algorithm design methods which use the partial converse as a basis for their work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#21644;&#25351;&#26631;&#23545;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#36825;&#20123;&#20272;&#35745;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05903</link><description>&lt;p&gt;
&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Comparing the quality of neural network uncertainty estimates for classification problems. (arXiv:2308.05903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#21644;&#25351;&#26631;&#23545;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#36825;&#20123;&#20272;&#35745;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#20998;&#31867;&#22120;&#65292;&#20294;&#35768;&#22810;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#23545;&#20854;&#20272;&#35745;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20915;&#31574;&#20013;&#30340;&#26377;&#29992;&#24615;&#24341;&#36215;&#20102;&#25991;&#29486;&#20013;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#36136;&#37327;&#35780;&#20272;&#30740;&#31350;&#24456;&#23569;&#12290;&#25105;&#20204;&#20351;&#29992;&#32463;&#39564;&#20027;&#20041;&#32622;&#20449;&#21306;&#38388;&#35206;&#30422;&#29575;&#21644;&#21306;&#38388;&#23485;&#24230;&#30340;&#32479;&#35745;&#26041;&#27861;&#26469;&#35780;&#20272;&#32622;&#20449;&#21306;&#38388;&#30340;&#36136;&#37327;&#65292;&#24182;&#20351;&#29992;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#35780;&#20272;&#20998;&#31867;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#19981;&#21516;&#30340;UQ&#26041;&#27861;&#24212;&#29992;&#20110;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#21644;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#25311;&#21512;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#65292;&#33258;&#21161;&#24335;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#65292;&#28145;&#24230;&#38598;&#25104;&#65288;DE&#65289;&#21644;&#33945;&#29305;&#21345;&#27931;&#65288;MC&#65289;dropout&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#26041;&#27861;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional deep learning (DL) models are powerful classifiers, but many approaches do not provide uncertainties for their estimates. Uncertainty quantification (UQ) methods for DL models have received increased attention in the literature due to their usefulness in decision making, particularly for high-consequence decisions. However, there has been little research done on how to evaluate the quality of such methods. We use statistical methods of frequentist interval coverage and interval width to evaluate the quality of credible intervals, and expected calibration error to evaluate classification predicted confidence. These metrics are evaluated on Bayesian neural networks (BNN) fit using Markov Chain Monte Carlo (MCMC) and variational inference (VI), bootstrapped neural networks (NN), Deep Ensembles (DE), and Monte Carlo (MC) dropout. We apply these different UQ for DL methods to a hyperspectral image target detection problem and show the inconsistency of the different methods' resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#35299;&#20915;&#20102;MAPF&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#32479;&#19968;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#20316;&#20026;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;MAPF&#24403;&#21069;&#25361;&#25112;&#25152;&#38656;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.05893</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#22242;&#38431;&#23548;&#33322;&#65306;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding. (arXiv:2308.05893v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#35299;&#20915;&#20102;MAPF&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#32479;&#19968;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#20316;&#20026;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;MAPF&#24403;&#21069;&#25361;&#25112;&#25152;&#38656;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#26159;&#35768;&#22810;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#36890;&#24120;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#21644;&#25317;&#25380;&#30340;&#29615;&#22659;&#20013;&#65292;MAPF&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#24050;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#38477;&#20302;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#30446;&#21069;&#22312;&#35780;&#20272;MAPF&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#32570;&#21475;&#65292;&#36890;&#36807;&#35299;&#20915;&#32570;&#20047;&#32479;&#19968;&#35780;&#20272;&#25351;&#26631;&#30340;&#38382;&#39064;&#24182;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20840;&#38754;&#38416;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#35752;&#35770;&#20102;&#20316;&#20026;&#26410;&#26469;&#26041;&#21521;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#22522;&#30784;&#29702;&#35299;&#20197;&#24212;&#23545;MAPF&#20013;&#30340;&#24403;&#21069;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our o
&lt;/p&gt;</description></item><item><title>DF2&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#19977;&#20010;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05889</link><description>&lt;p&gt;
DF2: &#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DF2: Distribution-Free Decision-Focused Learning. (arXiv:2308.05889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05889
&lt;/p&gt;
&lt;p&gt;
DF2&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#19977;&#20010;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#39044;&#27979;-&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#27169;&#22411;&#23450;&#21046;&#21040;&#19968;&#20010;&#19979;&#28216;&#20248;&#21270;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;DFL&#26041;&#27861;&#21463;&#21040;&#19977;&#20010;&#37325;&#35201;&#29942;&#39048;&#30340;&#21046;&#32422;&#65306;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#12290;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#28304;&#20110;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#20998;&#24067;&#19982;&#30495;&#23454;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#12290;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#26159;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#26469;&#36817;&#20284;&#26399;&#26395;&#20248;&#21270;&#30446;&#26631;&#26102;&#20135;&#29983;&#30340;&#12290;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#21457;&#29983;&#22312;DFL&#20381;&#38752;KKT&#26465;&#20214;&#36827;&#34892;&#31934;&#30830;&#26799;&#24230;&#35745;&#31639;&#26102;&#65292;&#32780;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#38750;&#20984;&#30446;&#26631;&#20013;&#36817;&#20284;&#26799;&#24230;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DF2 - &#31532;&#19968;&#20010;&#26126;&#30830;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#19977;&#20010;&#29942;&#39048;&#30340;&#26080;&#20998;&#24067;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending 
&lt;/p&gt;</description></item><item><title>GPLaSDI&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#23436;&#20840;&#38454;&#25968;&#30340;PDE&#35299;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#25554;&#20540;&#21644;&#35299;&#20915;ODE&#31995;&#32479;&#36827;&#34892;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;ROM&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05882</link><description>&lt;p&gt;
GPLaSDI: &#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder. (arXiv:2308.05882v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05882
&lt;/p&gt;
&lt;p&gt;
GPLaSDI&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#23436;&#20840;&#38454;&#25968;&#30340;PDE&#35299;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#25554;&#20540;&#21644;&#35299;&#20915;ODE&#31995;&#32479;&#36827;&#34892;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;ROM&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#36825;&#23548;&#33268;&#20102;&#20943;&#23569;&#38454;&#25968;&#27169;&#22411;(ROMs)&#30340;&#21457;&#23637;&#65292;&#20854;&#31934;&#30830;&#24615;&#39640;&#20110;&#23436;&#20840;&#38454;&#25968;&#27169;&#22411;(FOMs)&#20294;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#23454;&#29616;&#20102;&#38750;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#30340;&#21019;&#24314;&#65292;&#20363;&#22914;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;(LaSDI)&#12290;LaSDI&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#23436;&#20840;&#38454;&#25968;&#30340;PDE&#35299;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#23398;&#20064;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;ODE&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#20943;&#23569;&#30340;&#28508;&#31354;&#38388;&#20013;&#25554;&#20540;&#21644;&#35299;&#20915;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#36755;&#20837;&#35299;&#30721;&#22120;&#26469;&#36827;&#34892;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;ROM&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;(GP)&#30340;&#26032;&#22411;LaSDI&#26694;&#26550;&#65292;&#29992;&#20110;&#28508;&#31354;&#38388;ODE&#25554;&#20540;&#12290;&#20351;&#29992;GP&#24102;&#26469;&#20004;&#20010;&#37325;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#33021;&#22815;&#37327;&#21270;ROM&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#36825;&#20010;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerically solving partial differential equations (PDEs) can be challenging and computationally expensive. This has led to the development of reduced-order models (ROMs) that are accurate but faster than full order models (FOMs). Recently, machine learning advances have enabled the creation of non-linear projection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI maps full-order PDE solutions to a latent space using autoencoders and learns the system of ODEs governing the latent space dynamics. By interpolating and solving the ODE system in the reduced latent space, fast and accurate ROM predictions can be made by feeding the predicted latent space dynamics into the decoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework that relies on Gaussian process (GP) for latent space ODE interpolations. Using GPs offers two significant advantages. First, it enables the quantification of uncertainty over the ROM predictions. Second, leveraging this predict
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#21644;&#26816;&#27979;&#37326;&#22806;&#30340;&#34460;&#34411;&#38598;&#32676;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#24863;&#26579;&#27700;&#24179;&#65292;&#24182;&#36890;&#36807;&#37319;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.05881</link><description>&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#37326;&#22806;&#35782;&#21035;&#21644;&#26816;&#27979;&#34460;&#34411;&#38598;&#32676;
&lt;/p&gt;
&lt;p&gt;
Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models. (arXiv:2308.05881v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#21644;&#26816;&#27979;&#37326;&#22806;&#30340;&#34460;&#34411;&#38598;&#32676;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#24863;&#26579;&#27700;&#24179;&#65292;&#24182;&#36890;&#36807;&#37319;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34460;&#34411;&#20405;&#23475;&#23545;&#20892;&#20316;&#29289;&#29983;&#20135;&#12289;&#20892;&#26449;&#31038;&#21306;&#21644;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#34429;&#28982;&#21270;&#23398;&#26432;&#34411;&#21058;&#23545;&#20110;&#26368;&#22823;&#21270;&#20135;&#37327;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#25972;&#20010;&#30000;&#22320;&#19978;&#26045;&#33647;&#26082;&#19981;&#29615;&#20445;&#20063;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#23450;&#20301;&#21644;&#31649;&#29702;&#34460;&#34411;&#23545;&#20110;&#26377;&#38024;&#23545;&#24615;&#22320;&#26045;&#33647;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#35770;&#25991;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#34460;&#34411;&#38598;&#32676;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#34460;&#34411;&#38598;&#32676;&#26469;&#20272;&#35745;&#24863;&#26579;&#27700;&#24179;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#20174;&#39640;&#31921;&#30000;&#37319;&#38598;&#20102;&#19968;&#32452;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25163;&#21160;&#36873;&#25321;&#20102;5,447&#24352;&#21253;&#21547;&#34460;&#34411;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#36825;&#20123;&#22270;&#20687;&#20013;&#27880;&#37322;&#20102;&#27599;&#20010;&#21333;&#29420;&#30340;&#34460;&#34411;&#38598;&#32676;&#12290;&#20026;&#20102;&#26041;&#20415;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#36825;&#20123;&#22270;&#20687;&#35009;&#21098;&#25104;&#23567;&#22359;&#36827;&#34892;&#22788;&#29702;&#65292;&#24471;&#21040;&#19968;&#20010;&#21253;&#21547;151,380&#20010;&#22270;&#20687;&#22359;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#27604;&#36739;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aphid infestation poses a significant threat to crop production, rural communities, and global food security. While chemical pest control is crucial for maximizing yields, applying chemicals across entire fields is both environmentally unsustainable and costly. Hence, precise localization and management of aphids are essential for targeted pesticide application. The paper primarily focuses on using deep learning models for detecting aphid clusters. We propose a novel approach for estimating infection levels by detecting aphid clusters. To facilitate this research, we have captured a large-scale dataset from sorghum fields, manually selected 5,447 images containing aphids, and annotated each individual aphid cluster within these images. To facilitate the use of machine learning models, we further process the images by cropping them into patches, resulting in a labeled dataset comprising 151,380 image patches. Then, we implemented and compared the performance of four state-of-the-art obj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27969;&#24335;&#25968;&#25454;&#30340;&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#26500;&#24314;&#31639;&#27861;&#65292;&#20854;&#26680;&#24515;&#23376;&#38598;&#21487;&#20197;&#30456;&#20114;&#21512;&#24182;&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#12290;&#36825;&#31181;&#26680;&#24515;&#38598;&#21487;&#20197;&#36890;&#36807;&#32467;&#21512;CRAIG&#21644;&#21152;&#36895;&#26500;&#24314;&#30340;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#29992;&#20110;&#23454;&#26102;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.05878</link><description>&lt;p&gt;
&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#29992;&#20110;&#22810;&#25968;&#25454;&#38598;&#27969;&#30340;&#22810;&#26679;&#24615;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Composable Core-sets for Diversity Approximation on Multi-Dataset Streams. (arXiv:2308.05878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27969;&#24335;&#25968;&#25454;&#30340;&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#26500;&#24314;&#31639;&#27861;&#65292;&#20854;&#26680;&#24515;&#23376;&#38598;&#21487;&#20197;&#30456;&#20114;&#21512;&#24182;&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#12290;&#36825;&#31181;&#26680;&#24515;&#38598;&#21487;&#20197;&#36890;&#36807;&#32467;&#21512;CRAIG&#21644;&#21152;&#36895;&#26500;&#24314;&#30340;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#29992;&#20110;&#23454;&#26102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#24515;&#38598;&#26159;&#25351;&#26368;&#22823;&#21270;&#26576;&#20123;&#21151;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#24120;&#29992;&#20110;&#22810;&#26679;&#24615;&#25110;&#32676;&#32452;&#35201;&#27714;&#12290;&#36825;&#20123;&#23376;&#38598;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#25191;&#34892;&#32473;&#23450;&#20219;&#21153;&#65292;&#22914;&#26524;&#21435;&#38500;&#20559;&#35265;&#65292;&#24615;&#33021;&#21487;&#23218;&#32654;&#29978;&#33267;&#20248;&#20110;&#21407;&#22987;&#25968;&#25454;&#12290;&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#26159;&#19968;&#31181;&#20855;&#26377;&#23646;&#24615;&#30340;&#26680;&#24515;&#38598;&#65292;&#20854;&#26680;&#24515;&#23376;&#38598;&#21487;&#20197;&#30456;&#20114;&#21512;&#24182;&#20197;&#33719;&#24471;&#23545;&#21407;&#22987;&#25968;&#25454;&#30340;&#36817;&#20284;&#65292;&#36866;&#29992;&#20110;&#27969;&#24335;&#25110;&#20998;&#24067;&#24335;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#20351;&#29992;&#26680;&#24515;&#38598;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20197;CRAIG&#20026;&#20195;&#34920;&#30340;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#24182;&#25552;&#20379;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#30340;&#26680;&#24515;&#38598;&#26500;&#24314;&#31639;&#27861;&#65292;&#20197;&#27010;&#25324;&#27969;&#24335;&#25968;&#25454;&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#12290;&#22914;&#26524;&#32467;&#21512;CRAIG&#21644;&#21152;&#36895;&#26500;&#24314;&#30340;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#21487;&#20197;&#29992;&#20110;&#23454;&#26102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Core-sets refer to subsets of data that maximize some function that is commonly a diversity or group requirement. These subsets are used in place of the original data to accomplish a given task with comparable or even enhanced performance if biases are removed. Composable core-sets are core-sets with the property that subsets of the core set can be unioned together to obtain an approximation for the original data; lending themselves to be used for streamed or distributed data. Recent work has focused on the use of core-sets for training machine learning models. Preceding solutions such as CRAIG have been proven to approximate gradient descent while providing a reduced training time. In this paper, we introduce a core-set construction algorithm for constructing composable core-sets to summarize streamed data for use in active learning environments. If combined with techniques such as CRAIG and heuristics to enhance construction speed, composable core-sets could be used for real time tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20248;&#21270;N-CNN&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#24212;&#29992;&#36719;&#26631;&#31614;&#24471;&#21040;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#26032;&#29983;&#20799;&#30140;&#30171;&#35780;&#20272;&#30340;&#38754;&#37096;&#34920;&#24773;&#20998;&#31867;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#20998;&#31867;&#25351;&#26631;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#25913;&#21892;&#65292;&#20294;&#27809;&#26377;&#30452;&#25509;&#36716;&#21270;&#20026;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05877</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;N-CNN&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting N-CNN for Clinical Practice. (arXiv:2308.05877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20248;&#21270;N-CNN&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#24212;&#29992;&#36719;&#26631;&#31614;&#24471;&#21040;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#26032;&#29983;&#20799;&#30140;&#30171;&#35780;&#20272;&#30340;&#38754;&#37096;&#34920;&#24773;&#20998;&#31867;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#20998;&#31867;&#25351;&#26631;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#25913;&#21892;&#65292;&#20294;&#27809;&#26377;&#30452;&#25509;&#36716;&#21270;&#20026;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20248;&#21270;N-CNN&#30340;&#36229;&#21442;&#25968;&#24182;&#35780;&#20272;&#20854;&#23545;&#20998;&#31867;&#25351;&#26631;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19981;&#25913;&#21464;&#21407;&#22987;N-CNN&#26550;&#26500;&#30340;&#36229;&#21442;&#25968;&#65292;&#20027;&#35201;&#20462;&#25913;&#20102;&#20854;&#23398;&#20064;&#29575;&#21644;&#35757;&#32451;&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#35780;&#20272;&#27599;&#20010;&#36229;&#21442;&#25968;&#23545;F1&#24471;&#20998;&#30340;&#25913;&#36827;&#65292;&#36873;&#25321;&#20102;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#21019;&#24314;&#35843;&#25972;&#21518;&#30340;N-CNN&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#26681;&#25454;&#26032;&#29983;&#20799;&#38754;&#37096;&#32534;&#30721;&#31995;&#32479;&#24471;&#20986;&#30340;&#36719;&#26631;&#31614;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#26032;&#29983;&#20799;&#30140;&#30171;&#35780;&#20272;&#30340;&#38754;&#37096;&#34920;&#24773;&#20998;&#31867;&#27169;&#22411;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23613;&#31649;&#35843;&#25972;&#21518;&#30340;N-CNN&#30340;&#32467;&#26524;&#34920;&#26126;&#22312;&#20998;&#31867;&#25351;&#26631;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#25913;&#21892;&#65292;&#20294;&#36825;&#20123;&#25913;&#21892;&#24182;&#27809;&#26377;&#30452;&#25509;&#36716;&#21270;&#20026;&#26657;&#20934;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#26679;&#30340;&#35265;&#35299;&#21487;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the Neonatal Convolutional Neural Network (N-CNN) by optimizing its hyperparameters and evaluating how they affect its classification metrics, explainability and reliability, discussing their potential impact in clinical practice. We have chosen hyperparameters that do not modify the original N-CNN architecture, but mainly modify its learning rate and training regularization. The optimization was done by evaluating the improvement in F1 Score for each hyperparameter individually, and the best hyperparameters were chosen to create a Tuned N-CNN. We also applied soft labels derived from the Neonatal Facial Coding System, proposing a novel approach for training facial expression classification models for neonatal pain assessment. Interestingly, while the Tuned N-CNN results point towards improvements in classification metrics and explainability, these improvements did not directly translate to calibration performance. We believe that such insights might have the potent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UFed-GAN&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#38480;&#21046;&#35745;&#31639;&#36164;&#28304;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2308.05870</link><description>&lt;p&gt;
UFed-GAN&#65306;&#19968;&#31181;&#20855;&#26377;&#21463;&#38480;&#35745;&#31639;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data. (arXiv:2308.05870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UFed-GAN&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#38480;&#21046;&#35745;&#31639;&#36164;&#28304;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;&#24191;&#27867;&#30340;&#24212;&#29992;&#38656;&#27714;&#21644;&#23545;&#22312;&#22522;&#20110;&#20113;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#20302;&#24310;&#36831;&#22810;&#23186;&#20307;&#25968;&#25454;&#20998;&#31867;&#21644;&#25968;&#25454;&#38544;&#31169;&#30340;&#38656;&#27714;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#38024;&#23545;&#35768;&#22810;&#26080;&#32447;&#36890;&#20449;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#35745;&#31639;&#33021;&#21147;&#21644;&#20165;&#26377;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#23454;&#38469;&#26696;&#20363;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#36164;&#28304;&#21463;&#38480;&#21644;&#26631;&#31614;&#32570;&#22833;&#29615;&#22659;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;UFed-GAN&#26694;&#26550;&#65306;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#32780;&#26080;&#38656;&#36827;&#34892;&#26412;&#22320;&#20998;&#31867;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;UFed-GAN&#30340;&#25910;&#25947;&#24615;&#21644;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UFed-GAN&#22312;&#22788;&#29702;&#38480;&#21046;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To satisfy the broad applications and insatiable hunger for deploying low latency multimedia data classification and data privacy in a cloud-based setting, federated learning (FL) has emerged as an important learning paradigm. For the practical cases involving limited computational power and only unlabeled data in many wireless communications applications, this work investigates FL paradigm in a resource-constrained and label-missing environment. Specifically, we propose a novel framework of UFed-GAN: Unsupervised Federated Generative Adversarial Network, which can capture user-side data distribution without local classification training. We also analyze the convergence and privacy of the proposed UFed-GAN. Our experimental results demonstrate the strong potential of UFed-GAN in addressing limited computational resources and unlabeled data while preserving privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#33258;&#28982;&#28798;&#23475;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25214;&#21040;&#20102;Twitter&#25968;&#25454;&#19982;&#39123;&#39118;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#30340;Twitter&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#39123;&#39118;&#31561;&#32423;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05866</link><description>&lt;p&gt;
&#20351;&#29992;Twitter&#25968;&#25454;&#30830;&#23450;&#39123;&#39118;&#31561;&#32423;&#65306;&#19968;&#20010;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Using Twitter Data to Determine Hurricane Category: An Experiment. (arXiv:2308.05866v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#33258;&#28982;&#28798;&#23475;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25214;&#21040;&#20102;Twitter&#25968;&#25454;&#19982;&#39123;&#39118;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#30340;Twitter&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#39123;&#39118;&#31561;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21253;&#21547;&#22823;&#37327;&#20851;&#20110;&#20844;&#20247;&#23545;&#37325;&#22823;&#20107;&#20214;&#30340;&#35266;&#28857;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#28798;&#23475;&#22914;&#39123;&#39118;&#12290;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#24086;&#23376;&#36890;&#24120;&#26159;&#30001;&#23621;&#20303;&#22312;&#20107;&#20214;&#21457;&#29983;&#22320;&#38468;&#36817;&#30340;&#29992;&#25143;&#22312;&#20107;&#20214;&#21457;&#29983;&#26102;&#21457;&#24067;&#30340;&#12290;&#21487;&#20197;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#33719;&#24471;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19982;&#20107;&#20214;&#20043;&#38388;&#30340;&#29305;&#27530;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#30740;&#31350;&#24037;&#20316;&#65292;&#26088;&#22312;&#25214;&#21040;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19982;&#28798;&#23475;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#39123;&#39118;&#21704;&#32500;&#21644;&#27431;&#25991;&#26399;&#38388;&#21457;&#24067;&#30340;Twitter&#25968;&#25454;&#65292;&#24182;&#23581;&#35797;&#25214;&#21040;&#29305;&#23450;&#22320;&#21306;&#30340;Twitter&#25968;&#25454;&#19982;&#35813;&#22320;&#21306;&#30340;&#39123;&#39118;&#32423;&#21035;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#30340;Twitter&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#39123;&#39118;&#31561;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media posts contain an abundant amount of information about public opinion on major events, especially natural disasters such as hurricanes. Posts related to an event, are usually published by the users who live near the place of the event at the time of the event. Special correlation between the social media data and the events can be obtained using data mining approaches. This paper presents research work to find the mappings between social media data and the severity level of a disaster. Specifically, we have investigated the Twitter data posted during hurricanes Harvey and Irma, and attempted to find the correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#32454;&#32990;&#20998;&#21106;&#22522;&#20934;&#65292;&#37319;&#29992;Transformer-based&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#26174;&#24494;&#25104;&#20687;&#24179;&#21488;&#21644;&#32452;&#32455;&#31867;&#22411;&#30340;&#22270;&#20687;&#65292;&#26080;&#38656;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#65292;&#20026;&#26174;&#24494;&#25104;&#20687;&#20013;&#26356;&#20934;&#30830;&#21644;&#22810;&#21151;&#33021;&#30340;&#32454;&#32990;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.05864</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#32454;&#32990;&#20998;&#21106;&#25361;&#25112;&#65306;&#36808;&#21521;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions. (arXiv:2308.05864v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#32454;&#32990;&#20998;&#21106;&#22522;&#20934;&#65292;&#37319;&#29992;Transformer-based&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#26174;&#24494;&#25104;&#20687;&#24179;&#21488;&#21644;&#32452;&#32455;&#31867;&#22411;&#30340;&#22270;&#20687;&#65292;&#26080;&#38656;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#65292;&#20026;&#26174;&#24494;&#25104;&#20687;&#20013;&#26356;&#20934;&#30830;&#21644;&#22810;&#21151;&#33021;&#30340;&#32454;&#32990;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#20998;&#21106;&#26159;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36827;&#34892;&#23450;&#37327;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#29616;&#26377;&#30340;&#32454;&#32990;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#27169;&#24577;&#25110;&#38656;&#35201;&#25163;&#21160;&#24178;&#39044;&#26469;&#25351;&#23450;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#20013;&#30340;&#36229;&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#32454;&#32990;&#20998;&#21106;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;50&#22810;&#20010;&#19981;&#21516;&#29983;&#29289;&#23454;&#39564;&#30340;1500&#22810;&#20010;&#26631;&#35760;&#22270;&#20687;&#12290;&#21069;&#20960;&#21517;&#21442;&#19982;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#26174;&#24494;&#25104;&#20687;&#24179;&#21488;&#21644;&#32452;&#32455;&#31867;&#22411;&#30340;&#22810;&#26679;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#26080;&#38656;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#12290;&#36825;&#20010;&#22522;&#20934;&#21644;&#25913;&#36827;&#30340;&#31639;&#27861;&#20026;&#26174;&#24494;&#25104;&#20687;&#20013;&#26356;&#20934;&#30830;&#21644;&#22810;&#21151;&#33021;&#30340;&#32454;&#32990;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#22270;&#19978;&#36827;&#34892;&#30693;&#35782;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;Cora&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05857</link><description>&lt;p&gt;
&#22312;&#26465;&#20214;&#29420;&#31435;&#22270;&#19978;&#30340;&#30693;&#35782;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Knowledge Propagation over Conditional Independence Graphs. (arXiv:2308.05857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05857
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#22270;&#19978;&#36827;&#34892;&#30693;&#35782;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;Cora&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#65288;CI&#65289;&#22270;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#65292;&#20854;&#20013;&#29305;&#24449;&#36830;&#25509;&#20351;&#29992;&#26080;&#21521;&#22270;&#24314;&#27169;&#65292;&#36793;&#26435;&#37325;&#34920;&#31034;&#29305;&#24449;&#20043;&#38388;&#30340;&#37096;&#20998;&#30456;&#20851;&#24615;&#24378;&#24230;&#12290;&#30001;&#20110;CI&#22270;&#25429;&#25417;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#30452;&#25509;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#20204;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#29616;&#39046;&#22495;&#25299;&#25169;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;CI&#22270;&#19978;&#25191;&#34892;&#30693;&#35782;&#20256;&#25773;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#20844;&#24320;&#30340;Cora&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Independence (CI) graph is a special type of a Probabilistic Graphical Model (PGM) where the feature connections are modeled using an undirected graph and the edge weights show the partial correlation strength between the features. Since the CI graphs capture direct dependence between features, they have been garnering increasing interest within the research community for gaining insights into the systems from various domains, in particular discovering the domain topology. In this work, we propose algorithms for performing knowledge propagation over the CI graphs. Our experiments demonstrate that our techniques improve upon the state-of-the-art on the publicly available Cora and PubMed datasets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20056;&#27861;&#28388;&#27874;&#32593;&#32476;&#21644;&#23884;&#20837;&#27874;&#22330;&#30340;&#24050;&#30693;&#29305;&#24615;&#65292;GaborPINN&#26041;&#27861;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;PINN&#26041;&#27861;&#39640;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25910;&#25947;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.05843</link><description>&lt;p&gt;
GaborPINN: &#20351;&#29992;&#20056;&#27861;&#28388;&#27874;&#32593;&#32476;&#30340;&#39640;&#25928;&#29289;&#29702;&#20855;&#20307;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GaborPINN: Efficient physics informed neural networks using multiplicative filtered networks. (arXiv:2308.05843v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05843
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20056;&#27861;&#28388;&#27874;&#32593;&#32476;&#21644;&#23884;&#20837;&#27874;&#22330;&#30340;&#24050;&#30693;&#29305;&#24615;&#65292;GaborPINN&#26041;&#27861;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;PINN&#26041;&#27861;&#39640;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25910;&#25947;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27714;&#35299; Helmholtz &#26041;&#31243;&#35745;&#31639;&#22320;&#38663;&#27874;&#22330;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#20840;&#27874;&#24418;&#21453;&#28436;&#12290;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#34920;&#31034;&#27874;&#22330;&#35299;&#30340;&#20989;&#25968;&#24418;&#24335;&#65292;&#20294;&#20854;&#25910;&#25947;&#36895;&#24230;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340; PINN &#26041;&#27861;&#65292;&#20351;&#29992;&#20056;&#27861;&#28388;&#27874;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#20013;&#23884;&#20837;&#20102;&#19968;&#20123;&#27874;&#22330;&#30340;&#24050;&#30693;&#29305;&#24615;&#65292;&#20363;&#22914;&#39057;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992; Gabor &#22522;&#20989;&#25968;&#65292;&#22240;&#20026;&#20854;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#20934;&#30830;&#34920;&#31034;&#27874;&#22330;&#65292;&#24182;&#23558;&#20854;&#23454;&#29616;&#31216;&#20026; GaborPINN&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23558;&#27874;&#22330;&#39057;&#29575;&#30340;&#20808;&#39564;&#20449;&#24687;&#32435;&#20837;&#26041;&#27861;&#30340;&#35774;&#35745;&#20013;&#65292;&#20197;&#20943;&#36731; GaborPINN &#34920;&#31034;&#30340;&#27874;&#22330;&#30340;&#19981;&#36830;&#32493;&#24615;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479; PINN &#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25910;&#25947;&#36895;&#24230;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation of the seismic wavefield by solving the Helmholtz equation is crucial to many practical applications, e.g., full waveform inversion. Physics-informed neural networks (PINNs) provide functional wavefield solutions represented by neural networks (NNs), but their convergence is slow. To address this problem, we propose a modified PINN using multiplicative filtered networks, which embeds some of the known characteristics of the wavefield in training, e.g., frequency, to achieve much faster convergence. Specifically, we use the Gabor basis function due to its proven ability to represent wavefields accurately and refer to the implementation as GaborPINN. Meanwhile, we incorporate prior information on the frequency of the wavefield into the design of the method to mitigate the influence of the discontinuity of the represented wavefield by GaborPINN. The proposed method achieves up to a two-magnitude increase in the speed of convergence as compared with conventional PINNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLShield&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;FL&#21442;&#19982;&#32773;&#30340;&#33391;&#24615;&#25968;&#25454;&#23545;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#39564;&#35777;&#65292;&#20197;&#38450;&#24481;&#24694;&#24847;&#21442;&#19982;&#32773;&#30340;&#25237;&#27602;&#25915;&#20987;&#65292;&#24182;&#30830;&#20445;FL&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.05832</link><description>&lt;p&gt;
FLShield&#65306;&#19968;&#31181;&#22522;&#20110;&#39564;&#35777;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#38450;&#24481;&#25237;&#27602;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks. (arXiv:2308.05832v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLShield&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;FL&#21442;&#19982;&#32773;&#30340;&#33391;&#24615;&#25968;&#25454;&#23545;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#39564;&#35777;&#65292;&#20197;&#38450;&#24481;&#24694;&#24847;&#21442;&#19982;&#32773;&#30340;&#25237;&#27602;&#25915;&#20987;&#65292;&#24182;&#30830;&#20445;FL&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27491;&#22312;&#25913;&#21464;&#25105;&#20204;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;&#38543;&#30528;&#23427;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#23427;&#29616;&#22312;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#39046;&#22495;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#30001;&#20110;&#25104;&#21315;&#19978;&#19975;&#30340;&#21442;&#19982;&#32773;&#21487;&#20197;&#22312;&#36825;&#31181;&#21327;&#20316;&#29615;&#22659;&#20013;&#36129;&#29486;&#25968;&#25454;&#65292;&#30830;&#20445;&#36825;&#31181;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31361;&#26174;&#20102;&#35774;&#35745;&#23433;&#20840;&#21644;&#40065;&#26834;&#30340;FL&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#20197;&#25269;&#24481;&#24694;&#24847;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#30830;&#20445;&#39640;&#25928;&#30340;&#25928;&#29992;&#12289;&#26412;&#22320;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#31216;&#20026;FLShield&#65292;&#23427;&#21033;&#29992;FL&#21442;&#19982;&#32773;&#30340;&#33391;&#24615;&#25968;&#25454;&#22312;&#23558;&#20854;&#32771;&#34385;&#36827;&#20840;&#23616;&#27169;&#22411;&#29983;&#25104;&#20043;&#21069;&#23545;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#39564;&#35777;&#12290;&#36825;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#26381;&#21153;&#22120;&#23545;&#28165;&#27905;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#30340;&#38450;&#24481;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#24182;&#19988;&#19982;FL&#30340;&#22522;&#26412;&#21407;&#29702;&#30456;&#20914;&#31361;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;FLShield&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#35299;&#35835;&#20957;&#35270;&#20449;&#24687;&#65292;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05768</link><description>&lt;p&gt;
&#20351;&#29992;&#33041;&#30005;&#22270;&#30340;&#21487;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20957;&#35270;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Interpretable and Attention-based Method for Gaze Estimation Using Electroencephalography. (arXiv:2308.05768v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#35299;&#35835;&#20957;&#35270;&#20449;&#24687;&#65292;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#21160;&#21487;&#20197;&#25581;&#31034;&#20154;&#31867;&#24515;&#29702;&#36807;&#31243;&#12289;&#36523;&#20307;&#20581;&#24247;&#21644;&#34892;&#20026;&#30340;&#23453;&#36149;&#27934;&#23519;&#21147;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#21516;&#26102;&#35760;&#24405;&#33041;&#30005;&#27963;&#21160;&#21644;&#30524;&#21160;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#36825;&#24341;&#21457;&#20102;&#22522;&#20110;&#33041;&#27963;&#21160;&#39044;&#27979;&#20957;&#35270;&#26041;&#21521;&#30340;&#21508;&#31181;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#25216;&#26415;&#21487;&#25509;&#21463;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21516;&#26102;&#27979;&#37327;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#30524;&#21160;&#36319;&#36394;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;EEG&#25968;&#25454;&#20272;&#35745;&#20957;&#35270;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#20998;&#26512;EEG&#20449;&#21495;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#32593;&#32476;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#20449;&#21495;&#20013;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#19978;&#65292;&#24182;&#19988;&#24573;&#30053;&#26377;&#38382;&#39064;&#30340;&#36890;&#36947;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye movements can reveal valuable insights into various aspects of human mental processes, physical well-being, and actions. Recently, several datasets have been made available that simultaneously record EEG activity and eye movements. This has triggered the development of various methods to predict gaze direction based on brain activity. However, most of these methods lack interpretability, which limits their technology acceptance. In this paper, we leverage a large data set of simultaneously measured Electroencephalography (EEG) and Eye tracking, proposing an interpretable model for gaze estimation from EEG data. More specifically, we present a novel attention-based deep learning framework for EEG signal analysis, which allows the network to focus on the most relevant information in the signal and discard problematic channels. Additionally, we provide a comprehensive evaluation of the presented framework, demonstrating its superiority over current methods in terms of accuracy and rob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30005;&#30340;&#24773;&#32490;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476; (E2STN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#25968;&#25454;&#38598;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#26679;&#26412;&#26679;&#24335;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#36716;&#25442;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#39118;&#26684;&#21270;&#30340;&#24773;&#32490;&#33041;&#30005;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#36827;&#34892;&#36328;&#25968;&#25454;&#38598;&#30340;&#21028;&#21035;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05767</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#30340;&#24773;&#32490;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#29992;&#20110;&#36328;&#25968;&#25454;&#38598;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EEG-based Emotion Style Transfer Network for Cross-dataset Emotion Recognition. (arXiv:2308.05767v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30005;&#30340;&#24773;&#32490;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476; (E2STN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#25968;&#25454;&#38598;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#26679;&#26412;&#26679;&#24335;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#36716;&#25442;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#39118;&#26684;&#21270;&#30340;&#24773;&#32490;&#33041;&#30005;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#36827;&#34892;&#36328;&#25968;&#25454;&#38598;&#30340;&#21028;&#21035;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23454;&#29616;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;(BCIs)&#30340;&#20851;&#38190;&#65292;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#24050;&#32463;&#34987;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#30740;&#31350;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#21516;&#19968;&#21463;&#35797;&#32773;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#36328;&#39046;&#22495;&#24046;&#24322;&#23548;&#33268;&#30340;&#28304;&#22495;(&#35757;&#32451;&#25968;&#25454;)&#21644;&#30446;&#26631;&#22495;(&#27979;&#35797;&#25968;&#25454;)&#33041;&#30005;&#26679;&#26412;&#20043;&#38388;&#30340;&#26679;&#24335;&#19981;&#21305;&#37197;&#20173;&#28982;&#26159;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36328;&#25968;&#25454;&#38598;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30005;&#30340;&#24773;&#32490;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65288;E2STN&#65289;&#65292;&#29992;&#20110;&#33719;&#24471;&#21253;&#21547;&#28304;&#22495;&#20869;&#23481;&#20449;&#24687;&#21644;&#30446;&#26631;&#22495;&#39118;&#26684;&#20449;&#24687;&#30340;&#33041;&#30005;&#34920;&#31034;&#65292;&#34987;&#31216;&#20026;&#39118;&#26684;&#21270;&#30340;&#24773;&#32490;&#33041;&#30005;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#23545;&#20110;&#36328;&#25968;&#25454;&#38598;&#30340;&#21028;&#21035;&#39044;&#27979;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;E2STN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21363;&#36716;&#25442;&#27169;&#22359;&#65292;&#36716;&#25442;&#35780;&#20272;&#27169;&#22359;&#21644;&#21028;&#21035;&#39044;&#27979;&#27169;&#22359;&#12290;&#36716;&#25442;&#27169;&#22359;&#23545;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and targe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;Extra-Tree&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#19982;Random Forest&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#39640;&#20102;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;98.33%&#30340;&#20934;&#30830;&#29575;</title><link>http://arxiv.org/abs/2308.05765</link><description>&lt;p&gt;
&#21457;&#25381;&#29305;&#24449;&#36873;&#25321;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#20316;&#29992;&#65292;&#25552;&#39640;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Extra-Tree Feature Selection and Random Forest Classifier for Improved Survival Prediction in Heart Failure Patients. (arXiv:2308.05765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05765
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;Extra-Tree&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#19982;Random Forest&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#39640;&#20102;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;98.33%&#30340;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#26159;&#19968;&#31181;&#21361;&#21450;&#29983;&#21629;&#30340;&#30142;&#30149;&#65292;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#21463;&#21040;&#24433;&#21709;&#12290;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#29983;&#23384;&#33021;&#21147;&#21487;&#20197;&#24110;&#21161;&#21450;&#26089;&#24178;&#39044;&#65292;&#24182;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;Extra-Tree (ET) &#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#19982;Random Forest (RF) &#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#29983;&#23384;&#39044;&#27979;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#21457;&#25381;ET&#29305;&#24449;&#36873;&#25321;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#19982;&#24515;&#21147;&#34928;&#31469;&#29983;&#23384;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#39044;&#27979;&#22240;&#23376;&#12290;&#21033;&#29992;&#20844;&#24320;&#30340;UCL&#24515;&#21147;&#34928;&#31469; (HF) &#29983;&#23384;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#37319;&#29992;ET&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#26469;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#29305;&#24449;&#29992;&#20316;RF&#30340;&#32593;&#26684;&#25628;&#32034;&#30340;&#36755;&#20837;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#25351;&#26631;&#23545;&#35843;&#20248;&#21518;&#30340;RF&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25152;&#37319;&#29992;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;98.33%&#30340;&#20934;&#30830;&#29575;&#65292;&#26159;&#30446;&#21069;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart failure is a life-threatening condition that affects millions of people worldwide. The ability to accurately predict patient survival can aid in early intervention and improve patient outcomes. In this study, we explore the potential of utilizing data pre-processing techniques and the Extra-Tree (ET) feature selection method in conjunction with the Random Forest (RF) classifier to improve survival prediction in heart failure patients. By leveraging the strengths of ET feature selection, we aim to identify the most significant predictors associated with heart failure survival. Using the public UCL Heart failure (HF) survival dataset, we employ the ET feature selection algorithm to identify the most informative features. These features are then used as input for grid search of RF. Finally, the tuned RF Model was trained and evaluated using different matrices. The approach was achieved 98.33% accuracy that is the highest over the exiting work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#65292;&#24182;&#33021;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#21644;&#30830;&#23450;&#24515;&#33039;&#34920;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.05764</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI. (arXiv:2308.05764v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#65292;&#24182;&#33021;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#21644;&#30830;&#23450;&#24515;&#33039;&#34920;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270; (ECG) &#26159;&#19968;&#31181;&#24191;&#27867;&#21487;&#29992;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#21487;&#20197;&#24555;&#36895;&#21644;&#32463;&#27982;&#39640;&#25928;&#22320;&#35780;&#20272;&#24515;&#34880;&#31649;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#35786;&#26029;&#20013;&#65292;&#36890;&#24120;&#26356;&#21916;&#27426;&#20351;&#29992;&#26114;&#36149;&#30340;&#24515;&#33039;&#30913;&#20849;&#25391; (CMR) &#25104;&#20687;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#26816;&#26597;&#12290;&#34429;&#28982; CMR &#25104;&#20687;&#21487;&#20197;&#25552;&#20379;&#35814;&#32454;&#30340;&#24515;&#33039;&#35299;&#21078;&#21487;&#35270;&#21270;&#65292;&#20294;&#30001;&#20110;&#38271;&#26102;&#38388;&#25195;&#25551;&#21644;&#39640;&#26114;&#30340;&#36153;&#29992;&#65292;&#23427;&#24182;&#19981;&#24191;&#27867;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#26041;&#27861;&#65292;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#19982;&#23631;&#34109;&#25968;&#25454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#12290;&#22312;&#20351;&#29992;&#26469;&#33258;40044&#21517;UK Biobank&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;&#25105;&#20204;&#39044;&#27979;&#20102;&#21508;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#65292;&#24182;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#30830;&#23450;&#20102;&#19981;&#21516;&#30340;&#24515;&#33039;&#34920;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electrocardiogram (ECG) is a widely available diagnostic tool that allows for a cost-effective and fast assessment of the cardiovascular health. However, more detailed examination with expensive cardiac magnetic resonance (CMR) imaging is often preferred for the diagnosis of cardiovascular diseases. While providing detailed visualization of the cardiac anatomy, CMR imaging is not widely available due to long scan times and high costs. To address this issue, we propose the first self-supervised contrastive approach that transfers domain-specific information from CMR images to ECG embeddings. Our approach combines multimodal contrastive learning with masked data modeling to enable holistic cardiac screening solely from ECG data. In extensive experiments using data from 40,044 UK Biobank subjects, we demonstrate the utility and generalizability of our method. We predict the subject-specific risk of various cardiovascular diseases and determine distinct cardiac phenotypes solely from E
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30561;&#30496;-&#28165;&#37266;&#20998;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#20809;&#30005;&#27969;&#34880;&#23481;&#31215;&#27874;&#24418;&#21644;&#27963;&#21160;&#20449;&#21495;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#65292;&#35782;&#21035;&#30561;&#30496;&#38382;&#39064;&#21644;&#25913;&#21892;&#25972;&#20307;&#20581;&#24247;&#12290;</title><link>http://arxiv.org/abs/2308.05759</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#20174;&#20809;&#30005;&#27969;&#34880;&#23481;&#31215;&#27874;&#24418;&#21644;&#27963;&#21160;&#20449;&#21495;&#20013;&#25552;&#21462;&#30340;&#23569;&#37327;&#29305;&#24449;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30561;&#30496;-&#28165;&#37266;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals. (arXiv:2308.05759v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30561;&#30496;-&#28165;&#37266;&#20998;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#20809;&#30005;&#27969;&#34880;&#23481;&#31215;&#27874;&#24418;&#21644;&#27963;&#21160;&#20449;&#21495;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#65292;&#35782;&#21035;&#30561;&#30496;&#38382;&#39064;&#21644;&#25913;&#21892;&#25972;&#20307;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#23545;&#25105;&#20204;&#30340;&#25972;&#20307;&#20581;&#24247;&#21644;&#24184;&#31119;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#22312;&#35843;&#33410;&#25105;&#20204;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23545;&#25105;&#20204;&#30340;&#24773;&#32490;&#12289;&#35760;&#24518;&#21644;&#35748;&#30693;&#21151;&#33021;&#20197;&#21450;&#36523;&#20307;&#30340;&#38887;&#24615;&#21644;&#20813;&#30123;&#31995;&#32479;&#37117;&#26377;&#24433;&#21709;&#12290;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#26159;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#20272;&#35745;&#30561;&#30496;&#36136;&#37327;&#21644;&#25105;&#20204;&#30340;&#36523;&#20307;&#22312;&#36825;&#20010;&#37325;&#35201;&#30340;&#20241;&#24687;&#26102;&#26399;&#20869;&#21151;&#33021;&#22914;&#20309;&#30340;&#25351;&#26631;&#12290;&#20809;&#30005;&#27969;&#34880;&#23481;&#31215;&#27874;&#24418;&#65288;PPG&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#30561;&#30496;&#38454;&#27573;&#25512;&#26029;&#20449;&#21495;&#65292;&#24847;&#21619;&#30528;&#23427;&#21487;&#20197;&#21333;&#29420;&#20351;&#29992;&#25110;&#19982;&#20854;&#20182;&#20449;&#21495;&#32467;&#21512;&#20351;&#29992;&#26469;&#30830;&#23450;&#30561;&#30496;&#38454;&#27573;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#35782;&#21035;&#28508;&#22312;&#30340;&#30561;&#30496;&#38382;&#39064;&#21644;&#21046;&#23450;&#25913;&#21892;&#30561;&#30496;&#36136;&#37327;&#21644;&#25972;&#20307;&#20581;&#24247;&#31574;&#30053;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;eXtreme Gradient Boosting&#65288;XGBoost&#65289;&#31639;&#27861;&#21644;&#20174;PPG&#20449;&#21495;&#21644;&#27963;&#21160;&#35745;&#25968;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#30561;&#30496;-&#28165;&#37266;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep is a crucial aspect of our overall health and well-being. It plays a vital role in regulating our mental and physical health, impacting our mood, memory, and cognitive function to our physical resilience and immune system. The classification of sleep stages is a mandatory step to assess sleep quality, providing the metrics to estimate the quality of sleep and how well our body is functioning during this essential period of rest. Photoplethysmography (PPG) has been demonstrated to be an effective signal for sleep stage inference, meaning it can be used on its own or in a combination with others signals to determine sleep stage. This information is valuable in identifying potential sleep issues and developing strategies to improve sleep quality and overall health. In this work, we present a machine learning sleep-wake classification model based on the eXtreme Gradient Boosting (XGBoost) algorithm and features extracted from PPG signal and activity counts. The performance of our met
&lt;/p&gt;</description></item><item><title>OrcoDCS&#26159;&#19968;&#31181;&#29289;&#32852;&#32593;&#36793;&#32536;&#32534;&#25490;&#30340;&#22312;&#32447;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#22788;&#29702;&#19981;&#21516;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#32452;&#21644;&#23427;&#20204;&#30340;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#22312;&#21518;&#32493;&#24212;&#29992;&#20013;&#25552;&#20379;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05757</link><description>&lt;p&gt;
OrcoDCS:&#19968;&#31181;&#29289;&#32852;&#32593;&#36793;&#32536;&#32534;&#25490;&#30340;&#22312;&#32447;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OrcoDCS: An IoT-Edge Orchestrated Online Deep Compressed Sensing Framework. (arXiv:2308.05757v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05757
&lt;/p&gt;
&lt;p&gt;
OrcoDCS&#26159;&#19968;&#31181;&#29289;&#32852;&#32593;&#36793;&#32536;&#32534;&#25490;&#30340;&#22312;&#32447;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#22788;&#29702;&#19981;&#21516;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#32452;&#21644;&#23427;&#20204;&#30340;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#22312;&#21518;&#32493;&#24212;&#29992;&#20013;&#25552;&#20379;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#35774;&#35745;&#30340;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#32852;&#32593;&#36793;&#32536;&#32534;&#25490;&#30340;&#22312;&#32447;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26694;&#26550;OrcoDCS&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#32452;&#21644;&#23427;&#20204;&#30340;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#33021;&#22312;&#21518;&#32493;&#24212;&#29992;&#20013;&#25552;&#20379;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressed data aggregation (CDA) over wireless sensor networks (WSNs) is task-specific and subject to environmental changes. However, the existing compressed data aggregation (CDA) frameworks (e.g., compressed sensing-based data aggregation, deep learning(DL)-based data aggregation) do not possess the flexibility and adaptivity required to handle distinct sensing tasks and environmental changes. Additionally, they do not consider the performance of follow-up IoT data-driven deep learning (DL)-based applications. To address these shortcomings, we propose OrcoDCS, an IoT-Edge orchestrated online deep compressed sensing framework that offers high flexibility and adaptability to distinct IoT device groups and their sensing tasks, as well as high performance for follow-up applications. The novelty of our work is the design and deployment of IoT-Edge orchestrated online training framework over WSNs by leveraging an specially-designed asymmetric autoencoder, which can largely reduce the enco
&lt;/p&gt;</description></item><item><title>WeldMon&#26159;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#36229;&#22768;&#27874;&#28938;&#25509;&#26426;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#23450;&#20041;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#21644;&#23454;&#26102;&#20998;&#26512;&#27969;&#31243;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#28938;&#25509;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#21160;&#29983;&#25104;&#30340;&#29305;&#24449;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#29366;&#24577;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36824;&#20943;&#36731;&#20102;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#24037;&#20855;&#29366;&#24577;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05756</link><description>&lt;p&gt;
WeldMon: &#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#36229;&#22768;&#27874;&#28938;&#25509;&#26426;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
WeldMon: A Cost-effective Ultrasonic Welding Machine Condition Monitoring System. (arXiv:2308.05756v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05756
&lt;/p&gt;
&lt;p&gt;
WeldMon&#26159;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#36229;&#22768;&#27874;&#28938;&#25509;&#26426;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#23450;&#20041;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#21644;&#23454;&#26102;&#20998;&#26512;&#27969;&#31243;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#28938;&#25509;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#21160;&#29983;&#25104;&#30340;&#29305;&#24449;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#29366;&#24577;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36824;&#20943;&#36731;&#20102;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#24037;&#20855;&#29366;&#24577;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#27874;&#28938;&#25509;&#26426;&#22312;&#38146;&#30005;&#27744;&#34892;&#19994;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#23558;&#30005;&#27744;&#19982;&#23548;&#20307;&#36827;&#34892;&#36830;&#25509;&#12290;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#28938;&#25509;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#24037;&#20855;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#23545;&#20110;&#26089;&#26399;&#36136;&#37327;&#25511;&#21046;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30417;&#27979;&#26041;&#27861;&#22312;&#25104;&#26412;&#12289;&#20572;&#26426;&#26102;&#38388;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#36229;&#22768;&#27874;&#28938;&#25509;&#26426;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;WeldMon&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#23450;&#20041;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#21644;&#23454;&#26102;&#20998;&#26512;&#35774;&#35745;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#31639;&#27861;&#32467;&#21512;&#20102;&#33258;&#21160;&#29983;&#25104;&#30340;&#29305;&#24449;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#65292;&#22312;&#29366;&#24577;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#20132;&#21449;&#39564;&#35777;&#20934;&#30830;&#24615;(&#24179;&#22343;&#36798;&#21040;95.8%&#30340;&#27979;&#35797;&#20219;&#21153;)&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;(92.5%)&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20943;&#36731;&#20102;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#23558;&#24037;&#20855;&#29366;&#24577;&#20998;&#31867;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;8.3%&#12290;&#25152;&#26377;&#31639;&#27861;&#37117;&#22312;&#26412;&#22320;&#36816;&#34892;&#65292;&#20165;&#38656;&#35201;385&#27627;&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasonic welding machines play a critical role in the lithium battery industry, facilitating the bonding of batteries with conductors. Ensuring high-quality welding is vital, making tool condition monitoring systems essential for early-stage quality control. However, existing monitoring methods face challenges in cost, downtime, and adaptability. In this paper, we present WeldMon, an affordable ultrasonic welding machine condition monitoring system that utilizes a custom data acquisition system and a data analysis pipeline designed for real-time analysis. Our classification algorithm combines auto-generated features and hand-crafted features, achieving superior cross-validation accuracy (95.8% on average over all testing tasks) compared to the state-of-the-art method (92.5%) in condition classification tasks. Our data augmentation approach alleviates the concept drift problem, enhancing tool condition classification accuracy by 8.3%. All algorithms run locally, requiring only 385 mil
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#36827;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21361;&#38505;&#30340;&#25381;&#21457;&#29289;&#21270;&#21512;&#29289;&#30340;&#20652;&#21270;&#33976;&#27773;&#37325;&#25972;&#36827;&#34892;&#24314;&#27169;&#12289;&#29702;&#35299;&#21644;&#20248;&#21270;&#12290;&#36890;&#36807;&#30002;&#33519;&#20652;&#21270;&#33976;&#27773;&#37325;&#25972;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#21270;&#23398;/&#32441;&#29702;&#20998;&#26512;&#26469;&#33719;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#20845;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#29992;&#26469;&#23545;&#35813;&#36807;&#31243;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2308.05750</link><description>&lt;p&gt;
&#23558;&#21361;&#38505;&#30340;&#25381;&#21457;&#29289;&#21270;&#21512;&#29289;&#36716;&#21270;&#20026;&#29123;&#26009;&#30340;&#20652;&#21270;&#33976;&#27773;&#37325;&#25972;&#65306;&#19968;&#31181;&#36827;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Turning hazardous volatile matter compounds into fuel by catalytic steam reforming: An evolutionary machine learning approach. (arXiv:2308.05750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#36827;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21361;&#38505;&#30340;&#25381;&#21457;&#29289;&#21270;&#21512;&#29289;&#30340;&#20652;&#21270;&#33976;&#27773;&#37325;&#25972;&#36827;&#34892;&#24314;&#27169;&#12289;&#29702;&#35299;&#21644;&#20248;&#21270;&#12290;&#36890;&#36807;&#30002;&#33519;&#20652;&#21270;&#33976;&#27773;&#37325;&#25972;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#21270;&#23398;/&#32441;&#29702;&#20998;&#26512;&#26469;&#33719;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#20845;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#29992;&#26469;&#23545;&#35813;&#36807;&#31243;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21644;&#29983;&#29289;&#36136;&#22788;&#29702;&#31995;&#32479;&#27599;&#22825;&#37117;&#23558;&#25381;&#21457;&#24615;&#29289;&#36136;&#21270;&#21512;&#29289;&#25490;&#25918;&#21040;&#29615;&#22659;&#20013;&#12290;&#20652;&#21270;&#37325;&#25972;&#21487;&#20197;&#23558;&#36825;&#20123;&#21270;&#21512;&#29289;&#36716;&#21270;&#20026;&#26377;&#20215;&#20540;&#30340;&#29123;&#26009;&#65292;&#20294;&#24320;&#21457;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#24182;&#20248;&#21270;&#21453;&#24212;&#26465;&#20214;&#65292;&#22240;&#27492;&#26159;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#25381;&#21457;&#24615;&#29289;&#36136;&#21270;&#21512;&#29289;&#30340;&#20652;&#21270;&#33976;&#27773;&#37325;&#25972;&#36827;&#34892;&#24314;&#27169;&#12289;&#29702;&#35299;&#21644;&#20248;&#21270;&#12290;&#20197;&#30002;&#33519;&#20652;&#21270;&#33976;&#27773;&#37325;&#25972;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21270;&#23398;/&#32441;&#29702;&#20998;&#26512;&#65288;&#20363;&#22914;X&#23556;&#32447;&#34893;&#23556;&#20998;&#26512;&#65289;&#22914;&#20309;&#29992;&#20110;&#33719;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#21033;&#29992;&#25991;&#29486;&#32534;&#21046;&#20102;&#19968;&#20010;&#28085;&#30422;&#21508;&#31181;&#20652;&#21270;&#21058;&#29305;&#24615;&#21644;&#21453;&#24212;&#26465;&#20214;&#30340;&#25968;&#25454;&#24211;&#12290;&#36890;&#36807;&#20845;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#35813;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12289;&#26426;&#29702;&#35752;&#35770;&#21644;&#24314;&#27169;&#65292;&#26368;&#32456;&#24471;&#20986;&#30456;&#20851;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical and biomass processing systems release volatile matter compounds into the environment daily. Catalytic reforming can convert these compounds into valuable fuels, but developing stable and efficient catalysts is challenging. Machine learning can handle complex relationships in big data and optimize reaction conditions, making it an effective solution for addressing the mentioned issues. This study is the first to develop a machine-learning-based research framework for modeling, understanding, and optimizing the catalytic steam reforming of volatile matter compounds. Toluene catalytic steam reforming is used as a case study to show how chemical/textural analyses (e.g., X-ray diffraction analysis) can be used to obtain input features for machine learning models. Literature is used to compile a database covering a variety of catalyst characteristics and reaction conditions. The process is thoroughly analyzed, mechanistically discussed, modeled by six machine learning models, and o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#28151;&#21512;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#30340;&#21160;&#21147;&#23398;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#30452;&#25509;&#37096;&#32626;&#40657;&#31665;&#27169;&#22411;&#30340;&#23433;&#20840;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;&#30456;&#27604;&#31616;&#21333;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#26368;&#36817;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#33021;&#26356;&#22909;&#22320;&#39044;&#27979;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#21033;&#29992;&#36807;&#31243;&#21160;&#21147;&#23398;&#36712;&#36857;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.05749</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;-Transformer&#24341;&#20837;&#28151;&#21512;&#24314;&#27169;&#65306;&#20851;&#20110;&#20018;&#34892;&#19982;&#24182;&#34892;&#26041;&#27861;&#22312;&#25209;&#32467;&#26230;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization. (arXiv:2308.05749v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#28151;&#21512;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#30340;&#21160;&#21147;&#23398;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#30452;&#25509;&#37096;&#32626;&#40657;&#31665;&#27169;&#22411;&#30340;&#23433;&#20840;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;&#30456;&#27604;&#31616;&#21333;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#26368;&#36817;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#33021;&#26356;&#22909;&#22320;&#39044;&#27979;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#21033;&#29992;&#36807;&#31243;&#21160;&#21147;&#23398;&#36712;&#36857;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25968;&#23383;&#23402;&#29983;&#20381;&#36182;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#40657;&#31665;&#27169;&#22411;&#65292;&#20027;&#35201;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65292;RNN&#21644;CNN&#65289;&#26469;&#25429;&#25417;&#21270;&#23398;&#31995;&#32479;&#30340;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#23433;&#20840;&#21644;&#25805;&#20316;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#34987;&#23454;&#38469;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#23558;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#27169;&#22411;&#22240;&#20854;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#8220;&#20004;&#20840;&#20854;&#32654;&#8221;&#30340;&#26041;&#27861;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31616;&#21333;&#30340;DNN&#27169;&#22411;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#21033;&#29992;&#36807;&#31243;&#21160;&#21147;&#23398;&#36712;&#36857;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#24182;&#19981;&#25797;&#38271;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;TST&#65289;&#22312;&#25429;&#25417;&#36807;&#31243;&#29366;&#24577;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#21464;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing digital twins rely on data-driven black-box models, predominantly using deep neural recurrent, and convolutional neural networks (DNNs, RNNs, and CNNs) to capture the dynamics of chemical systems. However, these models have not seen the light of day, given the hesitance of directly deploying a black-box tool in practice due to safety and operational issues. To tackle this conundrum, hybrid models combining first-principles physics-based dynamics with machine learning (ML) models have increased in popularity as they are considered a 'best of both worlds' approach. That said, existing simple DNN models are not adept at long-term time-series predictions and utilizing contextual information on the trajectory of the process dynamics. Recently, attention-based time-series transformers (TSTs) that leverage multi-headed attention mechanism and positional encoding to capture long-term and short-term changes in process states have shown high predictive performance. Thus, a first-of
&lt;/p&gt;</description></item><item><title>LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2308.05481</link><description>&lt;p&gt;
LLM&#21464;&#25104;DBA
&lt;/p&gt;
&lt;p&gt;
LLM As DBA. (arXiv:2308.05481v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05481
&lt;/p&gt;
&lt;p&gt;
LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65288;DBA&#65289;&#22312;&#31649;&#29702;&#12289;&#32500;&#25252;&#21644;&#20248;&#21270;&#25968;&#25454;&#24211;&#31995;&#32479;&#20197;&#30830;&#20445;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;DBA&#26469;&#35828;&#65292;&#31649;&#29702;&#22823;&#37327;&#25968;&#25454;&#24211;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#20113;&#25968;&#25454;&#24211;&#19978;&#30340;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#65289;&#26159;&#22256;&#38590;&#21644;&#32321;&#29712;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#29702;&#35299;&#26377;&#20215;&#20540;&#25991;&#20214;&#24182;&#29983;&#25104;&#21512;&#29702;&#31572;&#26696;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D-Bot&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65292;&#23427;&#21487;&#20197;&#25345;&#32493;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#25968;&#25454;&#24211;&#32500;&#25252;&#32463;&#39564;&#65292;&#24182;&#20026;&#30446;&#26631;&#25968;&#25454;&#24211;&#25552;&#20379;&#21512;&#29702;&#12289;&#26377;&#29702;&#12289;&#21450;&#26102;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#20197;LLM&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#24211;&#32500;&#25252;&#26694;&#26550;&#65292;&#21253;&#25324;&#65288;i&#65289;&#20174;&#25991;&#26723;&#21644;&#24037;&#20855;&#20013;&#26816;&#27979;&#25968;&#25454;&#24211;&#32500;&#25252;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#24605;&#32500;&#26641;&#65292;&#21644;&#65288;iii&#65289;&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05476</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#29992;&#20110;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#65306;&#19968;&#39033;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis. (arXiv:2308.05476v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#27450;&#35784;&#25110;&#27450;&#39575;&#24615;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#29992;&#20110;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#65288;&#22914;BERT&#65292;XLNET&#65292;DistilBERT&#21644;RoBERTa&#65289;&#22312;&#26816;&#27979;&#27450;&#35784;&#24615;&#25991;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27450;&#35784;&#24615;&#21644;&#38750;&#27450;&#35784;&#24615;&#25991;&#26412;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30446;&#30340;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#21253;&#25324;&#20934;&#30830;&#29575;&#65292;&#31934;&#30830;&#29575;&#65292;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#33021;&#22815;&#22312;&#22788;&#29702;&#27450;&#35784;&#20869;&#23481;&#26102;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deceptive text classification is a critical task in natural language processing that aims to identify deceptive or fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05309</link><description>&lt;p&gt;
&#22270;&#32858;&#31867;&#30340;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#22270;&#32467;&#26500;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#31232;&#30095;&#24615;&#21644;&#22810;&#26679;&#24615;&#25152;&#22266;&#26377;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22270;&#32467;&#26500;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#36830;&#25509;&#21644;&#21024;&#38500;&#38169;&#35823;&#30340;&#36830;&#25509;&#26469;&#20248;&#21270;&#36755;&#20837;&#22270;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#30417;&#30563;&#30340;&#35774;&#32622;&#19978;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#65292;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#29305;&#23450;&#32858;&#31867;&#20219;&#21153;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#22270;&#32858;&#31867;&#65288;HoLe&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;&#35266;&#23519;&#21040;&#65292;&#24494;&#22937;&#22320;&#22686;&#24378;&#22270;&#32467;&#26500;&#20013;&#30340;&#21516;&#31867;&#24615;&#31243;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;GNNs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve G
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.04522</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#38544;&#20889;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#26159;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#30340;&#20004;&#20010;&#30456;&#20851;&#26041;&#38754;&#12290;&#38544;&#20889;&#26415;&#26088;&#22312;&#38544;&#34255;&#36890;&#20449;&#65292;&#32780;&#38544;&#20889;&#20998;&#26512;&#21017;&#26088;&#22312;&#25214;&#21040;&#36825;&#20123;&#38544;&#34255;&#20449;&#24687;&#65292;&#29978;&#33267;&#23581;&#35797;&#24674;&#22797;&#20854;&#25152;&#21253;&#21547;&#30340;&#25968;&#25454;&#12290;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#21463;&#21040;&#25191;&#27861;&#37096;&#38376;&#30340;&#20851;&#27880;&#12290;&#38544;&#20889;&#26415;&#24120;&#34987;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#29978;&#33267;&#24656;&#24598;&#20998;&#23376;&#29992;&#26469;&#36991;&#20813;&#22312;&#25317;&#26377;&#35777;&#25454;&#26102;&#34987;&#25429;&#65292;&#21363;&#20351;&#21152;&#23494;&#20063;&#19968;&#26679;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#22269;&#23478;&#31105;&#27490;&#25110;&#38480;&#21046;&#20351;&#29992;&#23494;&#30721;&#23398;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#25581;&#31034;&#38544;&#34255;&#20449;&#24687;&#30340;&#23574;&#31471;&#25216;&#26415;&#23545;&#25581;&#38706;&#38750;&#27861;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#35768;&#22810;&#24378;&#22823;&#21487;&#38752;&#30340;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
&lt;/p&gt;</description></item><item><title>SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04365</link><description>&lt;p&gt;
SLEM&#65306;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#36335;&#24452;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04365
&lt;/p&gt;
&lt;p&gt;
SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#31185;&#23398;&#30340;&#20851;&#38190;&#30446;&#26631;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24471;&#20986;&#20851;&#20110;&#23545;&#20551;&#23450;&#24178;&#39044;&#30340;&#39044;&#27979;&#30340;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#36335;&#24452;&#27169;&#22411;&#12289;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#33021;&#22815;&#26126;&#30830;&#22320;&#25351;&#23450;&#20851;&#20110;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#20551;&#35774;&#12290;&#19982;DAGs&#19981;&#21516;&#65292;SEMs&#20551;&#35774;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20989;&#25968;&#38169;&#35823;&#35268;&#33539;&#65292;&#20174;&#32780;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#21487;&#38752;&#30340;&#25928;&#26524;&#22823;&#23567;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;&#65288;SLEM&#65289;&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#38598;&#25104;&#30340;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;SLEM&#33021;&#22815;&#25552;&#20379;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#22312;&#19982;SEMs&#36827;&#34892;&#32447;&#24615;&#27169;&#22411;&#27604;&#36739;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#20248;&#20110;SEMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
&lt;/p&gt;</description></item><item><title>Spellburst&#26159;&#19968;&#20010;&#22522;&#20110;&#33410;&#28857;&#30340;&#30028;&#38754;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#21019;&#24847;&#32534;&#30721;&#29615;&#22659;&#65292;&#33402;&#26415;&#23478;&#21487;&#20197;&#36890;&#36807;&#20998;&#25903;&#21644;&#21512;&#24182;&#25805;&#20316;&#25506;&#32034;&#29983;&#25104;&#33402;&#26415;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22522;&#20110;&#34920;&#36798;&#24335;&#30340;&#25552;&#31034;&#20132;&#20114;&#36827;&#34892;&#35821;&#20041;&#32534;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35821;&#20041;&#21644;&#21477;&#27861;&#25506;&#32034;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#12290;</title><link>http://arxiv.org/abs/2308.03921</link><description>&lt;p&gt;
Spellburst&#65306;&#22522;&#20110;&#33410;&#28857;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#25506;&#32034;&#24615;&#21019;&#24847;&#32534;&#30721;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts. (arXiv:2308.03921v1 [cs.SE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03921
&lt;/p&gt;
&lt;p&gt;
Spellburst&#26159;&#19968;&#20010;&#22522;&#20110;&#33410;&#28857;&#30340;&#30028;&#38754;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#21019;&#24847;&#32534;&#30721;&#29615;&#22659;&#65292;&#33402;&#26415;&#23478;&#21487;&#20197;&#36890;&#36807;&#20998;&#25903;&#21644;&#21512;&#24182;&#25805;&#20316;&#25506;&#32034;&#29983;&#25104;&#33402;&#26415;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22522;&#20110;&#34920;&#36798;&#24335;&#30340;&#25552;&#31034;&#20132;&#20114;&#36827;&#34892;&#35821;&#20041;&#32534;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35821;&#20041;&#21644;&#21477;&#27861;&#25506;&#32034;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24847;&#32534;&#30721;&#20219;&#21153;&#36890;&#24120;&#26159;&#25506;&#32034;&#24615;&#30340;&#12290;&#22312;&#21046;&#20316;&#25968;&#23383;&#33402;&#26415;&#21697;&#26102;&#65292;&#33402;&#26415;&#23478;&#36890;&#24120;&#20174;&#39640;&#32423;&#35821;&#20041;&#26500;&#36896;&#24320;&#22987;&#65292;&#22914;&#8220;&#29627;&#29827;&#31383;&#28388;&#38236;&#8221;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#20195;&#30721;&#21442;&#25968;&#65288;&#22914;&#24418;&#29366;&#12289;&#39068;&#33394;&#12289;&#32447;&#26465;&#21644;&#19981;&#36879;&#26126;&#24230;&#65289;&#26469;&#23454;&#29616;&#23427;&#20197;&#20135;&#29983;&#35270;&#35273;&#21560;&#24341;&#21147;&#30340;&#32467;&#26524;&#12290;&#26681;&#25454;&#19982;&#33402;&#26415;&#23478;&#30340;&#35775;&#35848;&#65292;&#23558;&#35821;&#20041;&#26500;&#36896;&#36716;&#21270;&#20026;&#31243;&#24207;&#35821;&#27861;&#21487;&#33021;&#26159;&#36153;&#21147;&#30340;&#65292;&#30446;&#21069;&#30340;&#32534;&#31243;&#24037;&#20855;&#19981;&#21033;&#20110;&#24555;&#36895;&#30340;&#21019;&#36896;&#24615;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Spellburst&#65292;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#21160;&#21147;&#30340;&#21019;&#24847;&#32534;&#30721;&#29615;&#22659;&#12290;Spellburst&#25552;&#20379;&#65288;1&#65289;&#22522;&#20110;&#33410;&#28857;&#30340;&#30028;&#38754;&#65292;&#20801;&#35768;&#33402;&#26415;&#23478;&#21019;&#24314;&#29983;&#25104;&#33402;&#26415;&#24182;&#36890;&#36807;&#20998;&#25903;&#21644;&#21512;&#24182;&#25805;&#20316;&#36827;&#34892;&#21464;&#21270;&#25506;&#32034;&#65292;&#65288;2&#65289;&#22522;&#20110;&#34920;&#36798;&#24335;&#30340;&#25552;&#31034;&#20132;&#20114;&#65292;&#29992;&#20110;&#36827;&#34892;&#35821;&#20041;&#32534;&#31243;&#20132;&#20114;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22522;&#20110;&#25552;&#31034;&#39537;&#21160;&#30340;&#30028;&#38754;&#21644;&#30452;&#25509;&#20195;&#30721;&#32534;&#36753;&#65292;&#20197;&#22312;&#35821;&#20041;&#21644;&#21477;&#27861;&#25506;&#32034;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creative coding tasks are often exploratory in nature. When producing digital artwork, artists usually begin with a high-level semantic construct such as a "stained glass filter" and programmatically implement it by varying code parameters such as shape, color, lines, and opacity to produce visually appealing results. Based on interviews with artists, it can be effortful to translate semantic constructs to program syntax, and current programming tools don't lend well to rapid creative exploration. To address these challenges, we introduce Spellburst, a large language model (LLM) powered creative-coding environment. Spellburst provides (1) a node-based interface that allows artists to create generative art and explore variations through branching and merging operations, (2) expressive prompt-based interactions to engage in semantic programming, and (3) dynamic prompt-driven interfaces and direct code editing to seamlessly switch between semantic and syntactic exploration. Our evaluation
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;PanStarrs DR1&#12289;2MASS&#21644;WISE&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21306;&#20998;L&#31867;&#21644;T&#31867;&#26837;&#30702;&#26143;&#21644;&#20854;&#20182;&#20809;&#35889;&#21644;&#20142;&#24230;&#31867;&#21035;&#30340;&#22825;&#20307;&#12290;&#36825;&#26377;&#21161;&#20110;&#24314;&#31435;&#19968;&#20010;&#22343;&#21248;&#19988;&#23436;&#25972;&#30340;&#26837;&#30702;&#26143;&#26679;&#26412;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#21487;&#38752;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.03045</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#29616;&#20195;&#26143;&#31354;&#35843;&#26597;&#25968;&#25454;&#20013;&#25628;&#32034;L&#65286;T&#31867;&#26837;&#30702;&#26143;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys. (arXiv:2308.03045v2 [astro-ph.SR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03045
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;PanStarrs DR1&#12289;2MASS&#21644;WISE&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21306;&#20998;L&#31867;&#21644;T&#31867;&#26837;&#30702;&#26143;&#21644;&#20854;&#20182;&#20809;&#35889;&#21644;&#20142;&#24230;&#31867;&#21035;&#30340;&#22825;&#20307;&#12290;&#36825;&#26377;&#21161;&#20110;&#24314;&#31435;&#19968;&#20010;&#22343;&#21248;&#19988;&#23436;&#25972;&#30340;&#26837;&#30702;&#26143;&#26679;&#26412;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#21487;&#38752;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#21508;&#31181;&#20272;&#35745;&#65292;&#26837;&#30702;&#26143;&#65288;BD&#65289;&#24212;&#21344;&#38134;&#27827;&#31995;&#20013;&#25152;&#26377;&#22825;&#20307;&#30340;25&#65285;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26837;&#30702;&#26143;&#34987;&#21457;&#29616;&#24182;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#65292;&#26080;&#35770;&#26159;&#20010;&#20307;&#36824;&#26159;&#25972;&#20307;&#12290;&#36825;&#20123;&#30740;&#31350;&#38656;&#35201;&#22343;&#21248;&#21644;&#23436;&#25972;&#30340;&#26837;&#30702;&#26143;&#26679;&#26412;&#12290;&#30001;&#20110;&#20854;&#24369;&#20449;&#21495;&#65292;&#26837;&#30702;&#26143;&#30340;&#20809;&#35889;&#30740;&#31350;&#30456;&#24403;&#32791;&#36153;&#31934;&#21147;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20809;&#35889;&#35266;&#27979;&#30830;&#35748;&#30340;&#22823;&#37327;&#21487;&#38752;&#30340;&#26837;&#30702;&#26143;&#26679;&#26412;&#20284;&#20046;&#22312;&#30446;&#21069;&#26159;&#19981;&#21487;&#20225;&#21450;&#30340;&#12290;&#24050;&#32463;&#23581;&#35797;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21033;&#29992;&#20854;&#39068;&#33394;&#20316;&#20026;&#20915;&#31574;&#35268;&#21017;&#24212;&#29992;&#20110;&#22823;&#37327;&#30340;&#21208;&#27979;&#25968;&#25454;&#26469;&#25628;&#32034;&#21644;&#21019;&#24314;&#19968;&#32452;&#26837;&#30702;&#26143;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#12289;XGBoost&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#21644;TabNet&#31561;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;PanStarrs DR1&#12289;2MASS&#21644;WISE&#25968;&#25454;&#19978;&#21306;&#20998;L&#31867;&#21644;T&#31867;&#26837;&#30702;&#26143;&#19982;&#20854;&#20182;&#20809;&#35889;&#21644;&#20142;&#24230;&#31867;&#21035;&#30340;&#22825;&#20307;&#12290;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to various estimates, brown dwarfs (BD) should account for up to 25 percent of all objects in the Galaxy. However, few of them are discovered and well-studied, both individually and as a population. Homogeneous and complete samples of brown dwarfs are needed for these kinds of studies. Due to their weakness, spectral studies of brown dwarfs are rather laborious. For this reason, creating a significant reliable sample of brown dwarfs, confirmed by spectroscopic observations, seems unattainable at the moment. Numerous attempts have been made to search for and create a set of brown dwarfs using their colours as a decision rule applied to a vast amount of survey data. In this work, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier and TabNet on PanStarrs DR1, 2MASS and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. The explanation of the models is discussed. We also compare our models wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#25273;&#38500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#24425;&#31080;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#37325;&#26032;&#32771;&#34385;&#20462;&#21098;&#20449;&#24687;&#20013;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACE-GLT&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#22270;&#24425;&#31080;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02916</link><description>&lt;p&gt;
&#21033;&#29992;&#20462;&#21098;&#20803;&#32032;&#30340;&#23545;&#25239;&#25273;&#38500;&#65306;&#36808;&#21521;&#26356;&#22909;&#30340;&#22270;&#24425;&#31080;
&lt;/p&gt;
&lt;p&gt;
Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket. (arXiv:2308.02916v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#25273;&#38500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#24425;&#31080;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#37325;&#26032;&#32771;&#34385;&#20462;&#21098;&#20449;&#24687;&#20013;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACE-GLT&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#22270;&#24425;&#31080;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24425;&#31080;&#65288;GLT&#65289;&#26159;&#26680;&#24515;&#23376;&#22270;&#21644;&#31232;&#30095;&#23376;&#32593;&#32476;&#30340;&#32452;&#21512;&#65292;&#26088;&#22312;&#20943;&#36731;&#22823;&#22411;&#36755;&#20837;&#22270;&#19978;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#33719;&#32988;&#30340;GLT&#26159;&#36890;&#36807;&#24212;&#29992;&#36845;&#20195;&#24133;&#20540;&#20462;&#21098;&#65288;IMP&#65289;&#32780;&#24471;&#21040;&#30340;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35780;&#20272;&#21644;&#37325;&#26032;&#32771;&#34385;&#20462;&#21098;&#20449;&#24687;&#65292;&#36825;&#24573;&#35270;&#20102;&#22312;&#22270;/&#27169;&#22411;&#32467;&#26500;&#20462;&#21098;&#36807;&#31243;&#20013;&#36793;&#32536;/&#26435;&#37325;&#37325;&#35201;&#24615;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#33719;&#32988;&#30340;&#24425;&#31080;&#30340;&#21560;&#24341;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29468;&#24819;&#65292;&#21363;&#20462;&#21098;&#22270;&#36830;&#25509;&#21644;&#27169;&#22411;&#21442;&#25968;&#20013;&#23384;&#22312;&#34987;&#24573;&#35270;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#37325;&#26032;&#20998;&#32452;&#21040;GLT&#20013;&#20197;&#22686;&#24378;&#26368;&#32456;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#34917;&#20805;&#25273;&#38500;&#65288;ACE&#65289;&#26694;&#26550;&#65292;&#20197;&#20174;&#20462;&#21098;&#32452;&#20214;&#20013;&#25506;&#32034;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#24320;&#21457;&#20986;&#26356;&#24378;&#22823;&#30340;GLT&#65292;&#31216;&#20026;ACE-GLT&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Lottery Ticket (GLT), a combination of core subgraph and sparse subnetwork, has been proposed to mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance. However, the winning GLTs in exisiting studies are obtained by applying iterative magnitude-based pruning (IMP) without re-evaluating and re-considering the pruned information, which disregards the dynamic changes in the significance of edges/weights during graph/model structure pruning, and thus limits the appeal of the winning tickets. In this paper, we formulate a conjecture, i.e., existing overlooked valuable information in the pruned graph connections and model parameters which can be re-grouped into GLT to enhance the final performance. Specifically, we propose an adversarial complementary erasing (ACE) framework to explore the valuable information from the pruned components, thereby developing a more powerful GLT, referred to as the ACE-GLT. The main
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02080</link><description>&lt;p&gt;
&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#22240;&#26524;&#24341;&#23548;&#35299;&#32544;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#20419;&#36827;&#20844;&#24320;&#23545;&#35805;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#20182;&#20204;&#32463;&#24120;&#34987;&#21033;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;&#36825;&#31181;&#26377;&#23475;&#20869;&#23481;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#24433;&#21709;&#21040;&#20102;&#23427;&#20204;&#36866;&#24212;&#27867;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#36807;&#20110;&#29421;&#38552;&#22320;&#20851;&#27880;&#29305;&#23450;&#30340;&#35821;&#35328;&#20449;&#21495;&#25110;&#26576;&#20123;&#35789;&#35821;&#31867;&#21035;&#30340;&#20351;&#29992;&#12290;&#24403;&#24179;&#21488;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20986;&#29616;&#20102;&#65292;&#38656;&#35201;&#36328;&#24179;&#21488;&#27169;&#22411;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#20998;&#24067;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#24182;&#25512;&#24191;&#21040;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#19981;&#21516;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#36755;&#20837;&#34920;&#31034;&#35299;&#32544;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#26159;&#25552;&#20379;&#26356;&#22909;&#35299;&#32544;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;</title><link>http://arxiv.org/abs/2308.01118</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#24110;&#21161;&#20154;&#20204;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#25215;&#35834;&#26159;&#33021;&#22815;&#22686;&#21152;&#30446;&#24405;&#20013;&#36739;&#23569;&#30693;&#21517;&#30340;&#29289;&#21697;&#30340;&#21487;&#35265;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29616;&#20170;&#30340;&#25512;&#33616;&#31639;&#27861;&#21453;&#32780;&#34920;&#29616;&#20986;&#27969;&#34892;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#22312;&#25512;&#33616;&#20013;&#32463;&#24120;&#20851;&#27880;&#30456;&#24403;&#27969;&#34892;&#30340;&#29289;&#21697;&#12290;&#36825;&#31181;&#20559;&#24046;&#19981;&#20165;&#21487;&#33021;&#23548;&#33268;&#30701;&#26399;&#20869;&#23545;&#28040;&#36153;&#32773;&#21644;&#25552;&#20379;&#32773;&#30340;&#25512;&#33616;&#20215;&#20540;&#26377;&#38480;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#24341;&#36215;&#19981;&#24076;&#26395;&#30340;&#24378;&#21270;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27969;&#34892;&#20559;&#24046;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#27969;&#34892;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#26082;&#21253;&#25324;&#20102;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#65292;&#20063;&#21253;&#25324;&#20102;&#20943;&#23569;&#20559;&#24046;&#30340;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.01097</link><description>&lt;p&gt;
&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#30001;&#20110;&#26410;&#26469;&#23039;&#21183;&#30340;&#38543;&#26426;&#21644;&#19981;&#35268;&#21017;&#24615;&#36136;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24448;&#24448;&#38590;&#20197;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#30340;&#26102;&#31354;&#34920;&#31034;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#24573;&#35270;&#36816;&#21160;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#39592;&#26550;&#33410;&#28857;&#30340;&#26102;&#22495;&#21644;&#31354;&#22495;&#20381;&#36182;&#24615;&#26159;&#19981;&#21516;&#30340;&#12290;&#26102;&#22495;&#20851;&#31995;&#25429;&#25417;&#21040;&#38543;&#26102;&#38388;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#31354;&#22495;&#20851;&#31995;&#25551;&#36848;&#20102;&#36523;&#20307;&#32467;&#26500;&#21644;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#22686;&#37327;&#20449;&#24687;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#23427;&#35299;&#32806;&#20102;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#20102;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CLAMS&#30340;&#32858;&#31867;&#27169;&#31946;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#35270;&#35273;&#32858;&#31867;&#20013;&#30340;&#24863;&#30693;&#21464;&#24322;&#24615;&#12290;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#32858;&#31867;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22238;&#24402;&#27169;&#22359;&#23545;&#32858;&#31867;&#30340;&#27169;&#31946;&#24230;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.00284</link><description>&lt;p&gt;
CLAMS:&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#35270;&#35273;&#32858;&#31867;&#20013;&#24863;&#30693;&#21464;&#24322;&#24615;&#30340;&#32858;&#31867;&#27169;&#31946;&#24230;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering. (arXiv:2308.00284v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CLAMS&#30340;&#32858;&#31867;&#27169;&#31946;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#35270;&#35273;&#32858;&#31867;&#20013;&#30340;&#24863;&#30693;&#21464;&#24322;&#24615;&#12290;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#32858;&#31867;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22238;&#24402;&#27169;&#22359;&#23545;&#32858;&#31867;&#30340;&#27169;&#31946;&#24230;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#32858;&#31867;&#26159;&#25955;&#28857;&#22270;&#20013;&#24120;&#35265;&#30340;&#24863;&#30693;&#20219;&#21153;&#65292;&#25903;&#25345;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#65288;&#20363;&#22914;&#32858;&#31867;&#35782;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20351;&#29992;&#30456;&#21516;&#30340;&#25955;&#28857;&#22270;&#65292;&#30001;&#20110;&#20010;&#20307;&#38388;&#30340;&#24046;&#24322;&#21644;&#27169;&#31946;&#30340;&#32858;&#31867;&#36793;&#30028;&#65292;&#24863;&#30693;&#32858;&#31867;&#30340;&#26041;&#24335;&#65288;&#21363;&#36827;&#34892;&#35270;&#35273;&#32858;&#31867;&#65289;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#23613;&#31649;&#36825;&#31181;&#24863;&#30693;&#21464;&#24322;&#24615;&#23545;&#20110;&#22522;&#20110;&#35270;&#35273;&#32858;&#31867;&#30340;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#30097;&#38382;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;&#36825;&#31181;&#21464;&#24322;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36827;&#34892;&#35270;&#35273;&#32858;&#31867;&#20013;&#30340;&#24863;&#30693;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32858;&#31867;&#27169;&#31946;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;CLAMS&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39044;&#27979;&#21333;&#33394;&#25955;&#28857;&#22270;&#20013;&#32858;&#31867;&#27169;&#31946;&#24230;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#35270;&#35273;&#36136;&#37327;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#23450;&#24615;&#30740;&#31350;&#65292;&#20197;&#30830;&#23450;&#24433;&#21709;&#32858;&#31867;&#30340;&#35270;&#35273;&#20998;&#31163;&#30340;&#20851;&#38190;&#22240;&#32032;&#65288;&#20363;&#22914;&#32858;&#31867;&#38388;&#30340;&#25509;&#36817;&#24230;&#25110;&#22823;&#23567;&#24046;&#24322;&#65289;&#12290;&#22522;&#20110;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#37096;&#32626;&#20102;&#19968;&#20010;&#22238;&#24402;&#27169;&#22359;&#26469;&#20272;&#35745;&#32858;&#31867;&#30340;&#27169;&#31946;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual clustering is a common perceptual task in scatterplots that supports diverse analytics tasks (e.g., cluster identification). However, even with the same scatterplot, the ways of perceiving clusters (i.e., conducting visual clustering) can differ due to the differences among individuals and ambiguous cluster boundaries. Although such perceptual variability casts doubt on the reliability of data analysis based on visual clustering, we lack a systematic way to efficiently assess this variability. In this research, we study perceptual variability in conducting visual clustering, which we call Cluster Ambiguity. To this end, we introduce CLAMS, a data-driven visual quality measure for automatically predicting cluster ambiguity in monochrome scatterplots. We first conduct a qualitative study to identify key factors that affect the visual separation of clusters (e.g., proximity or size difference between clusters). Based on study findings, we deploy a regression module that estimates t
&lt;/p&gt;</description></item><item><title>ZADU&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25191;&#34892;&#21644;&#20998;&#26512;&#21508;&#20010;&#25968;&#25454;&#28857;&#30340;&#36129;&#29486;&#25552;&#20379;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.00282</link><description>&lt;p&gt;
ZADU: &#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#21487;&#38752;&#24615;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings. (arXiv:2308.00282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00282
&lt;/p&gt;
&lt;p&gt;
ZADU&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25191;&#34892;&#21644;&#20998;&#26512;&#21508;&#20010;&#25968;&#25454;&#28857;&#30340;&#36129;&#29486;&#25552;&#20379;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#25216;&#26415;&#26412;&#36136;&#19978;&#20250;&#25197;&#26354;&#21407;&#22987;&#39640;&#32500;&#25968;&#25454;&#30340;&#32467;&#26500;&#65292;&#20135;&#29983;&#19981;&#23436;&#32654;&#30340;&#20302;&#32500;&#23884;&#20837;&#12290;&#20026;&#20102;&#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#30340;&#21487;&#38752;&#24615;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#21644;&#25191;&#34892;&#25197;&#26354;&#24230;&#37327;&#19968;&#30452;&#26159;&#32791;&#26102;&#19988;&#32321;&#29712;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;Python&#24211;ZADU&#65292;&#25552;&#20379;&#20102;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#12290;ZADU&#19981;&#20165;&#26131;&#20110;&#23433;&#35013;&#21644;&#25191;&#34892;&#65292;&#36824;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#29305;&#24615;&#23454;&#29616;&#20102;&#23545;&#38477;&#32500;&#23884;&#20837;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#35813;&#24211;&#28085;&#30422;&#20102;&#21508;&#31181;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#23427;&#33258;&#21160;&#20248;&#21270;&#25197;&#26354;&#24230;&#37327;&#30340;&#25191;&#34892;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#25191;&#34892;&#22810;&#20010;&#24230;&#37327;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#35813;&#24211;&#21487;&#26174;&#31034;&#20010;&#21035;&#25968;&#25454;&#28857;&#23545;&#25972;&#20307;&#25197;&#26354;&#30340;&#36129;&#29486;&#65292;&#20415;&#20110;&#23545;&#38477;&#32500;&#23884;&#20837;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) techniques inherently distort the original structure of input high-dimensional data, producing imperfect low-dimensional embeddings. Diverse distortion measures have thus been proposed to evaluate the reliability of DR embeddings. However, implementing and executing distortion measures in practice has so far been time-consuming and tedious. To address this issue, we present ZADU, a Python library that provides distortion measures. ZADU is not only easy to install and execute but also enables comprehensive evaluation of DR embeddings through three key features. First, the library covers a wide range of distortion measures. Second, it automatically optimizes the execution of distortion measures, substantially reducing the running time required to execute multiple measures. Last, the library informs how individual points contribute to the overall distortions, facilitating the detailed analysis of DR embeddings. By simulating a real-world scenario of optimizin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;--&#26631;&#31614;&#21487;&#20449;&#24230;&#21644;&#26631;&#31614;&#36830;&#32493;&#24615;&#65288;Label-T&amp;C&#65289;--&#25913;&#36827;&#20102;&#22522;&#20110;&#31867;&#21035;&#26631;&#31614;&#30340;&#38477;&#32500;&#35780;&#20272;&#30340;&#36807;&#31243;&#65292;&#19981;&#20877;&#20551;&#35774;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#24418;&#25104;&#33391;&#22909;&#30340;&#32858;&#31867;&#65292;&#32780;&#26159;&#36890;&#36807;&#20272;&#35745;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#24418;&#25104;&#32858;&#31867;&#30340;&#31243;&#24230;&#21644;&#35780;&#20272;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.00278</link><description>&lt;p&gt;
&#31867;&#21035;&#19981;&#31561;&#20110;&#32858;&#31867;&#65306;&#25913;&#36827;&#22522;&#20110;&#26631;&#31614;&#30340;&#38477;&#32500;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction. (arXiv:2308.00278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;--&#26631;&#31614;&#21487;&#20449;&#24230;&#21644;&#26631;&#31614;&#36830;&#32493;&#24615;&#65288;Label-T&amp;C&#65289;--&#25913;&#36827;&#20102;&#22522;&#20110;&#31867;&#21035;&#26631;&#31614;&#30340;&#38477;&#32500;&#35780;&#20272;&#30340;&#36807;&#31243;&#65292;&#19981;&#20877;&#20551;&#35774;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#24418;&#25104;&#33391;&#22909;&#30340;&#32858;&#31867;&#65292;&#32780;&#26159;&#36890;&#36807;&#20272;&#35745;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#24418;&#25104;&#32858;&#31867;&#30340;&#31243;&#24230;&#21644;&#35780;&#20272;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#30340;&#21487;&#38752;&#24615;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#37327;&#21270;&#26631;&#35760;&#31867;&#21035;&#22312;&#23884;&#20837;&#20013;&#22914;&#20309;&#24418;&#25104;&#32039;&#20945;&#19988;&#30456;&#20114;&#20998;&#31163;&#30340;&#32858;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#31867;&#21035;&#22312;&#21407;&#22987;&#30340;&#39640;&#32500;&#31354;&#38388;&#20013;&#20173;&#28982;&#26159;&#28165;&#26224;&#30340;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65307;&#19968;&#20010;&#31867;&#21035;&#21487;&#33021;&#34987;&#20998;&#35299;&#25104;&#22810;&#20010;&#20998;&#31163;&#30340;&#32858;&#31867;&#65292;&#22810;&#20010;&#31867;&#21035;&#21487;&#33021;&#21512;&#24182;&#25104;&#19968;&#20010;&#32858;&#31867;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#33021;&#24635;&#26159;&#20445;&#35777;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#36827;&#34892;&#35780;&#20272;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;--&#26631;&#31614;&#21487;&#20449;&#24230;&#21644;&#26631;&#31614;&#36830;&#32493;&#24615;&#65288;Label-T&amp;C&#65289;--&#25913;&#36827;&#20102;&#22522;&#20110;&#31867;&#21035;&#26631;&#31614;&#30340;&#38477;&#32500;&#35780;&#20272;&#30340;&#36807;&#31243;&#12290;Label-T&amp;C&#19981;&#20877;&#20551;&#35774;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#24418;&#25104;&#33391;&#22909;&#30340;&#32858;&#31867;&#65292;&#32780;&#26159;&#36890;&#36807;&#65288;1&#65289;&#20272;&#35745;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#24418;&#25104;&#32858;&#31867;&#30340;&#31243;&#24230;&#21644;&#65288;2&#65289;&#35780;&#20272;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures -- Label-Trustworthiness and Label-Continuity (Label-T&amp;C) -- advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&amp;C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.16104</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#20840;&#29699;&#21487;&#38752;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27700;&#26159;&#26368;&#24120;&#35265;&#21644;&#24433;&#21709;&#26368;&#22823;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#23545;&#21457;&#23637;&#20013;&#22269;&#23478;&#23588;&#20854;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22269;&#23478;&#24448;&#24448;&#32570;&#20047;&#23494;&#38598;&#30340;&#27700;&#27969;&#30417;&#27979;&#32593;&#32476;&#12290;&#20934;&#30830;&#21450;&#26102;&#30340;&#39044;&#35686;&#23545;&#20110;&#20943;&#36731;&#27946;&#27700;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20934;&#30830;&#30340;&#27700;&#25991;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26681;&#25454;&#27599;&#20010;&#24212;&#29992;&#30340;&#27969;&#22495;&#20013;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#35760;&#24405;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;7&#22825;&#20869;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#22823;&#27954;&#12289;&#21069;&#23548;&#26102;&#38388;&#21644;&#37325;&#29616;&#26399;&#20013;&#22343;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#27700;&#25991;&#27169;&#22411;&#65288;Copernicus&#24212;&#24613;&#31649;&#29702;&#26381;&#21153;&#20840;&#29699;&#27946;&#27700;&#24847;&#35782;&#31995;&#32479;&#65289;&#12290;AI&#22312;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#20013;&#30340;&#39044;&#27979;&#23588;&#20854;&#26377;&#25928;&#65292;&#36825;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20840;&#29699;&#21482;&#26377;&#30334;&#20998;&#20043;&#20960;&#30340;&#27969;&#22495;&#20855;&#26377;&#27969;&#37327;&#35266;&#27979;&#31449;&#65292;&#32780;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#25968;&#37327;&#21344;&#27604;&#24456;&#39640;&#65292;&#23545;&#20154;&#31867;&#29305;&#21035;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#20223;&#23398;&#20064;&#20013;&#22240;&#26524;&#28151;&#28102;&#38382;&#39064;&#30340;&#21021;&#22987;&#21270;&#29366;&#24577;&#24178;&#39044;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36974;&#34109;&#35266;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#24182;&#25552;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.15980</link><description>&lt;p&gt;
&#21021;&#22987;&#21270;&#29366;&#24577;&#24178;&#39044;&#23545;&#35299;&#20915;&#21435;&#38500;&#28151;&#28102;&#30340;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initial State Interventions for Deconfounded Imitation Learning. (arXiv:2307.15980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#20223;&#23398;&#20064;&#20013;&#22240;&#26524;&#28151;&#28102;&#38382;&#39064;&#30340;&#21021;&#22987;&#21270;&#29366;&#24577;&#24178;&#39044;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36974;&#34109;&#35266;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#24182;&#25552;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#23384;&#22312;&#22240;&#26524;&#28151;&#28102;&#38382;&#39064;&#65292;&#21363;&#23398;&#20064;&#31574;&#30053;&#20851;&#27880;&#30340;&#29305;&#24449;&#24182;&#19981;&#22240;&#26524;&#22320;&#24433;&#21709;&#19987;&#23478;&#30340;&#34892;&#20026;&#65292;&#32780;&#26159;&#34920;&#38754;&#19978;&#30340;&#30456;&#20851;&#24615;&#12290;&#22240;&#26524;&#28151;&#28102;&#30340;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#20013;&#20135;&#29983;&#20102;&#20302;&#30340;&#24320;&#29615;&#30417;&#30563;&#25439;&#22833;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#34920;&#29616;&#20986;&#24046;&#30340;&#38381;&#29615;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#35266;&#27979;&#31354;&#38388;&#30340;&#20998;&#31163;&#34920;&#31034;&#20013;&#36974;&#34109;&#24050;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36974;&#34109;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#23545;&#21021;&#22987;&#31995;&#32479;&#29366;&#24577;&#36827;&#34892;&#24178;&#39044;&#30340;&#33021;&#21147;&#65292;&#36991;&#20813;&#20102;&#23545;&#19987;&#23478;&#26597;&#35810;&#12289;&#19987;&#23478;&#22870;&#21169;&#20989;&#25968;&#25110;&#22240;&#26524;&#22270;&#35268;&#33539;&#30340;&#20219;&#20309;&#35201;&#27714;&#12290;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26159;&#20445;&#23432;&#30340;&#65292;&#21363;&#19981;&#20250;&#38169;&#35823;&#22320;&#36974;&#34109;&#22240;&#26524;&#30456;&#20851;&#30340;&#35266;&#27979;&#65307;&#27492;&#22806;&#65292;&#23545;&#21021;&#22987;&#29366;&#24577;&#30340;&#24178;&#39044;&#33021;&#22815;&#20005;&#26684;&#20943;&#23569;&#36807;&#24230;&#20445;&#23432;&#24615;&#12290;&#35813;&#36974;&#34109;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#20004;&#20010;&#31034;&#20363;&#25511;&#21046;&#31995;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning suffers from causal confusion. This phenomenon occurs when learned policies attend to features that do not causally influence the expert actions but are instead spuriously correlated. Causally confused agents produce low open-loop supervised loss but poor closed-loop performance upon deployment. We consider the problem of masking observed confounders in a disentangled representation of the observation space. Our novel masking algorithm leverages the usual ability to intervene in the initial system state, avoiding any requirement involving expert querying, expert reward functions, or causal graph specification. Under certain assumptions, we theoretically prove that this algorithm is conservative in the sense that it does not incorrectly mask observations that causally influence the expert; furthermore, intervening on the initial state serves to strictly reduce excess conservatism. The masking algorithm is applied to behavior cloning for two illustrative control system
&lt;/p&gt;</description></item><item><title>NIPD&#26159;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#38750;&#29420;&#31435;&#19982;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#20154;&#20307;&#26816;&#27979;&#22522;&#20934;&#65292;&#24320;&#28304;&#20102;&#19968;&#20010;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#29289;&#32852;&#32593;&#20154;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.15932</link><description>&lt;p&gt;
NIPD&#65288;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#38750;&#29420;&#31435;&#19982;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#20154;&#20307;&#26816;&#27979;&#22522;&#20934;&#65289;
&lt;/p&gt;
&lt;p&gt;
NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data. (arXiv:2306.15932v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15932
&lt;/p&gt;
&lt;p&gt;
NIPD&#26159;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#38750;&#29420;&#31435;&#19982;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#20154;&#20307;&#26816;&#27979;&#22522;&#20934;&#65292;&#24320;&#28304;&#20102;&#19968;&#20010;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#29289;&#32852;&#32593;&#20154;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#12290;FL&#20351;&#24471;&#29289;&#32852;&#32593;&#23458;&#25143;&#31471;&#21487;&#20197;&#22312;&#38450;&#27490;&#38544;&#31169;&#27844;&#28431;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#33391;&#22909;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#22914;&#26524;&#23558;FL&#19982;&#36793;&#32536;&#35774;&#22791;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#22312;&#36793;&#32536;&#30452;&#25509;&#22788;&#29702;&#35270;&#39057;&#25968;&#25454;&#65292;&#20174;&#32780;&#23558;&#20154;&#21592;&#26816;&#27979;&#37096;&#32626;&#22312;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30456;&#26426;&#30340;&#30828;&#20214;&#21644;&#37096;&#32626;&#22330;&#26223;&#19981;&#21516;&#65292;&#30456;&#26426;&#25152;&#25910;&#38598;&#30340;&#25968;&#25454;&#21576;&#29616;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#30340;&#29305;&#24615;&#65292;FL&#32858;&#21512;&#24471;&#21040;&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#25928;&#26524;&#36739;&#24046;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#29992;&#20110;&#30740;&#31350;&#29289;&#32852;&#32593;&#30456;&#26426;&#19978;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#38382;&#39064;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#19968;&#20010;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#29289;&#32852;&#32593;&#20154;&#20307;&#26816;&#27979;&#65288;NIPD&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#20116;&#20010;&#19981;&#21516;&#30340;&#30456;&#26426;&#25910;&#38598;&#32780;&#26469;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30495;&#27491;&#22522;&#20110;&#35774;&#22791;&#30340;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#20154;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), a privacy-preserving distributed machine learning, has been rapidly applied in wireless communication networks. FL enables Internet of Things (IoT) clients to obtain well-trained models while preventing privacy leakage. Person detection can be deployed on edge devices with limited computing power if combined with FL to process the video data directly at the edge. However, due to the different hardware and deployment scenarios of different cameras, the data collected by the camera present non-independent and identically distributed (non-IID), and the global model derived from FL aggregation is less effective. Meanwhile, existing research lacks public data set for real-world FL object detection, which is not conducive to studying the non-IID problem on IoT cameras. Therefore, we open source a non-IID IoT person detection (NIPD) data set, which is collected from five different cameras. To our knowledge, this is the first true device-based non-IID person detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#30417;&#30563;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39118;&#21147;&#21457;&#30005;&#26426;&#26728;&#21494;&#31995;&#32479;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#35782;&#21035;&#22256;&#38590;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22256;&#38590;&#26679;&#26412;&#23545;&#26469;&#23398;&#20064;&#26356;&#20855;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.14701</link><description>&lt;p&gt;
&#22522;&#20110;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#30417;&#30563;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#29992;&#20110;&#39118;&#21147;&#21457;&#30005;&#26426;&#26728;&#21494;&#31995;&#32479;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Hard Sample Mining Enabled Supervised Contrastive Feature Learning for Wind Turbine Pitch System Fault Diagnosis. (arXiv:2306.14701v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#30417;&#30563;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39118;&#21147;&#21457;&#30005;&#26426;&#26728;&#21494;&#31995;&#32479;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#35782;&#21035;&#22256;&#38590;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22256;&#38590;&#26679;&#26412;&#23545;&#26469;&#23398;&#20064;&#26356;&#20855;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#21147;&#21457;&#30005;&#26426;&#26377;&#25928;&#21033;&#29992;&#39118;&#33021;&#20381;&#36182;&#20110;&#20854;&#26728;&#21494;&#31995;&#32479;&#33021;&#22815;&#26681;&#25454;&#39118;&#36895;&#21464;&#21270;&#35843;&#25972;&#26728;&#21494;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38271;&#26399;&#30952;&#25439;&#23548;&#33268;&#30340;&#26728;&#21494;&#31995;&#32479;&#20013;&#23384;&#22312;&#22810;&#31181;&#20581;&#24247;&#38382;&#39064;&#65292;&#32473;&#20934;&#30830;&#20998;&#31867;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#36827;&#32780;&#22686;&#21152;&#20102;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#32500;&#25252;&#25104;&#26412;&#29978;&#33267;&#21487;&#33021;&#25439;&#22351;&#23427;&#20204;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;HSMSCL&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#35782;&#21035;&#22256;&#38590;&#26679;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36890;&#36807;&#26500;&#24314;&#22256;&#38590;&#26679;&#26412;&#23545;&#26469;&#23398;&#20064;&#26356;&#20855;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#20013;&#30340;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#26694;&#26550;&#36824;&#29992;&#23398;&#21040;&#30340;&#34920;&#31034;&#26500;&#24314;&#22256;&#38590;&#26679;&#26412;&#65292;&#20351;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#26356;&#20855;&#25361;&#25112;&#24615;&#24182;&#20351;&#20854;&#25104;&#20026;&#26356;&#26377;&#25928;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficient utilization of wind power by wind turbines relies on the ability of their pitch systems to adjust blade pitch angles in response to varying wind speeds. However, the presence of multiple health conditions in the pitch system due to the long-term wear and tear poses challenges in accurately classifying them, thus increasing the maintenance cost of wind turbines or even damaging them. This paper proposes a novel method based on hard sample mining-enabled supervised contrastive learning (HSMSCL) to address this problem. The proposed method employs cosine similarity to identify hard samples and subsequently, leverages supervised contrastive learning to learn more discriminative representations by constructing hard sample pairs. Furthermore, the hard sample mining framework in the proposed method also constructs hard samples with learned representations to make the training process of the multilayer perceptron (MLP) more challenging and make it a more effective classifier. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RANS-PINN&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;2&#26041;&#31243;&#28065;&#31896;&#24230;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#27969;&#22330;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06034</link><description>&lt;p&gt;
&#22522;&#20110;RANS-PINN&#30340;&#27169;&#25311;&#20195;&#29702;&#39044;&#27979;&#28237;&#27969;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows. (arXiv:2306.06034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RANS-PINN&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;2&#26041;&#31243;&#28065;&#31896;&#24230;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#27969;&#22330;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20026;&#24314;&#31435;&#30001;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#20102;&#26694;&#26550;&#12290; PINN&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20250;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#27491;&#21017;&#21270;&#39033;&#26469;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#12290;&#30001;&#20110;&#27169;&#25311;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25511;&#21046;&#30340;&#21160;&#24577;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#65292;PINN&#24050;&#32463;&#22312;&#23398;&#20064;&#30001;Navier-Stokes&#26041;&#31243;&#25511;&#21046;&#30340;&#28082;&#20307;&#27969;&#21160;&#38382;&#39064;&#30340;&#21442;&#25968;&#20195;&#29702;&#26041;&#38754;&#24191;&#21463;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RANS-PINN&#65292;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;PINN&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#27969;&#22330;&#65288;&#21363;&#36895;&#24230;&#21644;&#21387;&#21147;&#65289;&#12290;&#20026;&#20102;&#32771;&#34385;&#28237;&#27969;&#24341;&#20837;&#30340;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;RANS-PINN&#37319;&#29992;&#22522;&#20110;&#38647;&#35834;&#24179;&#22343;Navier-Stokes&#65288;RANS&#65289;&#30340;2&#26041;&#31243;&#28065;&#31896;&#24230;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#30830;&#20445;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#26377;&#25928;&#21021;&#22987;&#21270;&#21644;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) provide a framework to build surrogate models for dynamical systems governed by differential equations. During the learning process, PINNs incorporate a physics-based regularization term within the loss function to enhance generalization performance. Since simulating dynamics controlled by partial differential equations (PDEs) can be computationally expensive, PINNs have gained popularity in learning parametric surrogates for fluid flow problems governed by Navier-Stokes equations. In this work, we introduce RANS-PINN, a modified PINN framework, to predict flow fields (i.e., velocity and pressure) in high Reynolds number turbulent flow regime. To account for the additional complexity introduced by turbulence, RANS-PINN employs a 2-equation eddy viscosity model based on a Reynolds-averaged Navier-Stokes (RANS) formulation. Furthermore, we adopt a novel training approach that ensures effective initialization and balance among the various component
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36941;&#21382;&#31639;&#27861;&#20013;&#30340;&#35206;&#30422;&#26102;&#38388;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#35745;&#25968;&#30340;&#36127;&#21453;&#39304;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#22270;&#20013;&#23616;&#37096;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#65292;&#24182;&#22312;&#29305;&#27530;&#30340;&#22270;&#24418;&#20013;&#23454;&#29616;&#26356;&#23567;&#30340;&#35206;&#30422;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.04902</link><description>&lt;p&gt;
&#19968;&#31181;&#38750;&#39532;&#23572;&#21487;&#22827;&#31639;&#27861;&#30340;&#35206;&#30422;&#26102;&#38388;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Cover Time Study of a non-Markovian Algorithm. (arXiv:2306.04902v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36941;&#21382;&#31639;&#27861;&#20013;&#30340;&#35206;&#30422;&#26102;&#38388;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#35745;&#25968;&#30340;&#36127;&#21453;&#39304;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#22270;&#20013;&#23616;&#37096;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#65292;&#24182;&#22312;&#29305;&#27530;&#30340;&#22270;&#24418;&#20013;&#23454;&#29616;&#26356;&#23567;&#30340;&#35206;&#30422;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#65292;&#35206;&#30422;&#26102;&#38388;&#26159;&#35775;&#38382;&#25152;&#26377;&#33410;&#28857;&#25152;&#38656;&#30340;&#26399;&#26395;&#27493;&#25968;&#12290;&#26356;&#23567;&#30340;&#35206;&#30422;&#26102;&#38388;&#24847;&#21619;&#30528;&#36941;&#21382;&#31639;&#27861;&#30340;&#25506;&#32034;&#25928;&#29575;&#26356;&#39640;&#12290;&#23613;&#31649;&#23545;&#20110;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#38750;&#39532;&#23572;&#21487;&#22827;&#26041;&#27861;&#23578;&#26080;&#35206;&#30422;&#26102;&#38388;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20986;&#21457;&#65292;&#34920;&#26126;&#36127;&#21453;&#39304;&#31574;&#30053;&#65288;&#19968;&#31181;&#22522;&#20110;&#35745;&#25968;&#30340;&#25506;&#32034;&#26041;&#27861;&#65289;&#27604;&#26420;&#32032;&#30340;&#38543;&#26426;&#28459;&#27493;&#25628;&#32034;&#31574;&#30053;&#26356;&#22909;&#12290;&#29305;&#21035;&#22320;&#65292;&#21069;&#32773;&#21487;&#20197;&#22312;&#20219;&#24847;&#22270;&#20013;&#23616;&#37096;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#23427;&#36824;&#21487;&#20197;&#22312;&#29305;&#27530;&#20294;&#37325;&#35201;&#30340;&#22270;&#24418;&#20013;&#23454;&#29616;&#26356;&#23567;&#30340;&#35206;&#30422;&#26102;&#38388;&#65292;&#21253;&#25324;&#22242;&#31751;&#22270;&#21644;&#26641;&#22270;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#25581;&#31034;&#20026;&#20160;&#20040;&#32463;&#20856;&#30340;UCB&#21644;MCTS&#31639;&#27861;&#22914;&#27492;&#26377;&#29992;&#12290;&#21508;&#31181;&#25968;&#20540;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a traversal algorithm, cover time is the expected number of steps needed to visit all nodes in a given graph. A smaller cover time means a higher exploration efficiency of traversal algorithm. Although random walk algorithms have been studied extensively in the existing literature, there has been no cover time result for any non-Markovian method. In this work, we stand on a theoretical perspective and show that the negative feedback strategy (a count-based exploration method) is better than the naive random walk search. In particular, the former strategy can locally improve the search efficiency for an arbitrary graph. It also achieves smaller cover times for special but important graphs, including clique graphs, tree graphs, etc. Moreover, we make connections between our results and reinforcement learning literature to give new insights on why classical UCB and MCTS algorithms are so useful. Various numerical results corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;</title><link>http://arxiv.org/abs/2306.04542</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#21644;&#21024;&#38500;&#22122;&#22768;&#26469;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#20197;&#29983;&#25104;&#25968;&#25454;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#34987;&#25552;&#20986;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#39640;&#23618;&#27425;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#32452;&#20214;&#30340;&#35774;&#35745;&#22522;&#30784;&#35206;&#30422;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#32780;&#36830;&#36143;&#30340;&#32508;&#36848;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#20214;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32508;&#36848;&#25353;&#29031;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#32452;&#32455;&#65292;&#21363;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#25193;&#25955;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#30740;&#31350;&#20998;&#26512;&#20010;&#20307;&#32452;&#20214;&#12289;&#35774;&#35745;&#36873;&#25321;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#33976;&#39311;&#20013;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#30456;&#23545;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24544;&#23454;&#30340;&#27169;&#20223;&#26694;&#26550;&#26469;&#35299;&#20915;&#23398;&#29983;&#32622;&#20449;&#24230;&#21644;&#36719;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#35777;&#21644;&#35748;&#35777;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#23398;&#29983;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04431</link><description>&lt;p&gt;
&#24544;&#23454;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Faithful Knowledge Distillation. (arXiv:2306.04431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#33976;&#39311;&#20013;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#30456;&#23545;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24544;&#23454;&#30340;&#27169;&#20223;&#26694;&#26550;&#26469;&#35299;&#20915;&#23398;&#29983;&#32622;&#20449;&#24230;&#21644;&#36719;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#35777;&#21644;&#35748;&#35777;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#23398;&#29983;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#20351;&#20854;&#33021;&#22815;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#25104;&#21151;&#26041;&#27861;&#65292;&#20294;&#36807;&#21435;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#25945;&#24072;&#19982;&#23398;&#29983;&#20043;&#38388;&#22312;&#36719;&#32622;&#20449;&#24230;&#26041;&#38754;&#30340;&#30456;&#23545;&#26657;&#20934;&#38382;&#39064;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#23545;&#20013;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;i&#65289;&#25945;&#24072;&#21644;&#23398;&#29983;&#26159;&#21542;&#22312;&#25509;&#36817;&#27491;&#30830;&#20998;&#31867;&#30340;&#25968;&#25454;&#26679;&#26412;&#26102;&#23384;&#22312;&#20998;&#27495;&#65292;&#65288;ii&#65289;&#22312;&#25968;&#25454;&#26679;&#26412;&#21608;&#22260;&#65292;&#32463;&#36807;&#33976;&#39311;&#30340;&#23398;&#29983;&#26159;&#21542;&#20687;&#25945;&#24072;&#19968;&#26679;&#33258;&#20449;&#12290;&#36825;&#20123;&#37117;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#32771;&#34385;&#20174;&#40065;&#26834;&#25945;&#24072;&#20013;&#35757;&#32451;&#36739;&#23567;&#23398;&#29983;&#32593;&#32476;&#30340;&#37096;&#32626;&#26102;&#38750;&#24120;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24544;&#23454;&#30340;&#27169;&#20223;&#26694;&#26550;&#26469;&#35752;&#35770;&#32622;&#20449;&#24230;&#30340;&#30456;&#23545;&#26657;&#20934;&#65292;&#24182;&#25552;&#20379;&#23454;&#35777;&#21644;&#35748;&#35777;&#26041;&#27861;&#26469;&#35780;&#20272;&#23398;&#29983;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has received much attention due to its success in compressing networks to allow for their deployment in resource-constrained systems. While the problem of adversarial robustness has been studied before in the KD setting, previous works overlook what we term the relative calibration of the student network with respect to its teacher in terms of soft confidences. In particular, we focus on two crucial questions with regard to a teacher-student pair: (i) do the teacher and student disagree at points close to correctly classified dataset examples, and (ii) is the distilled student as confident as the teacher around dataset examples? These are critical questions when considering the deployment of a smaller student network trained from a robust teacher within a safety-critical setting. To address these questions, we introduce a faithful imitation framework to discuss the relative calibration of confidences, as well as provide empirical and certified methods to eva
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#21253;&#25324;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#33180;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17271</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss. (arXiv:2305.17271v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#21253;&#25324;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#33180;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36947;&#26816;&#27979;&#26159;&#36710;&#36742;&#23450;&#20301;&#30340;&#20851;&#38190;&#65292;&#26159;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#21644;&#35768;&#22810;&#26234;&#33021;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#21644;&#32858;&#21512;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#36710;&#36947;&#32447;&#21644;&#22270;&#20687;&#20013;&#20854;&#20182;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#24182;&#25552;&#21319;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#27169;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#22312;&#32454;&#35843;&#20998;&#21106;&#38454;&#27573;&#20013;&#65292;&#36830;&#32493;&#30340;&#22270;&#20687;&#24103;&#34987;&#29992;&#20316;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
Lane detection is crucial for vehicle localization which makes it the foundation for automated driving and many intelligent and advanced driving assistant systems. Available vision-based lane detection methods do not make full use of the valuable features and aggregate contextual information, especially the interrelationships between lane lines and other regions of the images in continuous frames. To fill this research gap and upgrade lane detection performance, this paper proposes a pipeline consisting of self pre-training with masked sequential autoencoders and fine-tuning with customized PolyLoss for the end-to-end neural network models using multi-continuous image frames. The masked sequential autoencoders are adopted to pre-train the neural network models with reconstructing the missing pixels from a random masked image as the objective. Then, in the fine-tuning segmentation phase where lane detection segmentation is performed, the continuous image frames are served as the inputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;OT&#38382;&#39064;&#30340;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#30340;&#65292;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.07500</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21487;&#23545;&#40784;&#30340;&#28508;&#22312;&#31354;&#38388;&#29992;&#20110;&#39640;&#25928;&#38381;&#21512;&#24418;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation. (arXiv:2305.07500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;OT&#38382;&#39064;&#30340;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#30340;&#65292;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20960;&#20309;&#24037;&#20855;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#23545;&#40784;&#27010;&#29575;&#27979;&#24230;&#65292;&#36981;&#24490;&#26368;&#23567;&#21162;&#21147;&#21407;&#21017;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#65292;OT&#30340;&#35768;&#22810;&#25104;&#21151;&#24212;&#29992;&#20043;&#19968;&#26159;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#30446;&#26631;&#26159;&#23558;&#20998;&#31867;&#22120;&#20174;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#26410;&#26631;&#35760;&#25110;&#31232;&#30095;&#26631;&#35760;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;OT&#30340;DA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30001;&#20223;&#23556;&#26144;&#23556;&#32473;&#20986;&#30340;OT&#38382;&#39064;&#30340;&#38381;&#24335;&#35299;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;&#35813;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Among many successful applications of OT in machine learning (ML), domain adaptation (DA) -- a field of study where the goal is to transfer a classifier from one labelled domain to another similar, yet different unlabelled or scarcely labelled domain -- has been historically among the most investigated ones. This success is due to the ability of OT to provide both a meaningful discrepancy measure to assess the similarity of two domains' distributions and a mapping that can project source domain data onto the target one. In this paper, we propose a principally new OT-based approach applied to DA that uses the closed-form solution of the OT problem given by an affine mapping and learns an embedding space for which this solution is optimal and computationally less complex. We show that our approach works in both homogeneous and heterogeneous DA settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.02640</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#23384;&#22312;&#38544;&#24615;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#25968;&#25454;&#33539;&#24335;&#65306;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#21333;&#20540;&#30340;&#21333;&#20010;&#39592;&#26550;&#32467;&#26500;&#65292;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#22810;&#20540;&#30340;&#19968;&#32452;&#22810;&#39592;&#26550;&#32467;&#26500;&#12290;&#22810;&#20010;&#39592;&#26550;&#24341;&#20837;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#22810;&#20010;&#20540;&#24341;&#20837;&#20102;&#20998;&#24067;&#20551;&#35774;&#30340;&#26080;&#33021;&#21147;&#65292;&#36825;&#20004;&#32773;&#23548;&#33268;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#33267;&#20170;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#24378;&#24230;&#32780;&#19981;&#26159;&#29420;&#31435;&#22122;&#22768;&#20316;&#20026;&#28508;&#21464;&#37327;&#26469;&#35843;&#33410;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#36825;&#31181;&#35774;&#35745;&#24605;&#24819;&#65292;&#19981;&#21516;&#39592;&#26550;&#30340;&#22240;&#26524;&#24378;&#24230;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#34920;&#31034;&#20026;&#21333;&#20540;&#22240;&#26524;&#22270;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#22270;G&#20998;&#35299;&#20026;&#20004;&#20010;&#30456;&#20851;&#23376;&#22270;O&#21644;C&#12290;O&#21253;&#21547;&#35266;&#23519;&#33410;&#28857;&#20043;&#38388;&#30340;&#32431;&#20851;&#31995;&#65292;&#32780;C&#34920;&#31034;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAMAF-Net&#30340;&#32593;&#32476;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#26816;&#26597;&#35270;&#39057;&#20013;&#30340;&#20013;&#39118;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#25968;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#30340;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#20013;&#39118;&#26816;&#27979;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.09466</link><description>&lt;p&gt;
MAMAF-Net&#65306;&#29992;&#20110;&#20013;&#39118;&#35786;&#26029;&#30340;&#36816;&#21160;&#24863;&#30693;&#21644;&#22810;&#20851;&#27880;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MAMAF-Net: Motion-Aware and Multi-Attention Fusion Network for Stroke Diagnosis. (arXiv:2304.09466v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAMAF-Net&#30340;&#32593;&#32476;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#26816;&#26597;&#35270;&#39057;&#20013;&#30340;&#20013;&#39118;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#25968;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#30340;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#20013;&#39118;&#26816;&#27979;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#39118;&#26159;&#20840;&#29699;&#27515;&#20129;&#29575;&#21644;&#27531;&#30142;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#27599;&#22235;&#20154;&#20013;&#23601;&#26377;&#19968;&#20154;&#26377;&#20013;&#39118;&#30340;&#21361;&#38505;&#12290;&#39044;&#20808;&#21307;&#38498;&#20013;&#39118;&#35780;&#20272;&#22312;&#20934;&#30830;&#35782;&#21035;&#20013;&#39118;&#24739;&#32773;&#20197;&#21152;&#36895;&#36827;&#19968;&#27493;&#26816;&#26597;&#21644;&#27835;&#30103;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAMAF-Net&#30340;&#36816;&#21160;&#24863;&#30693;&#21644;&#22810;&#20851;&#27880;&#34701;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#26816;&#27979;&#22810;&#27169;&#24335;&#26816;&#26597;&#35270;&#39057;&#20013;&#30340;&#20013;&#39118;&#24773;&#20917;&#12290;&#19982;&#20854;&#20182;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#20013;&#39118;&#26816;&#27979;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#25968;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#27599;&#20010;&#34987;&#35797;&#32773;&#30340;&#22810;&#20010;&#35270;&#39057;&#35760;&#24405;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20013;&#39118;&#12289;&#30701;&#26242;&#24615;&#32570;&#34880;&#24615;&#21457;&#20316;&#65288;TIA&#65289;&#21644;&#20581;&#24247;&#23545;&#29031;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stroke is a major cause of mortality and disability worldwide from which one in four people are in danger of incurring in their lifetime. The pre-hospital stroke assessment plays a vital role in identifying stroke patients accurately to accelerate further examination and treatment in hospitals. Accordingly, the National Institutes of Health Stroke Scale (NIHSS), Cincinnati Pre-hospital Stroke Scale (CPSS) and Face Arm Speed Time (F.A.S.T.) are globally known tests for stroke assessment. However, the validity of these tests is skeptical in the absence of neurologists. Therefore, in this study, we propose a motion-aware and multi-attention fusion network (MAMAF-Net) that can detect stroke from multimodal examination videos. Contrary to other studies on stroke detection from video analysis, our study for the first time proposes an end-to-end solution from multiple video recordings of each subject with a dataset encapsulating stroke, transient ischemic attack (TIA), and healthy controls. T
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16618</link><description>&lt;p&gt;
&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20026;&#23545;&#35805;&#25935;&#24863;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#29305;&#24449;&#30340;&#20154;&#21592;&#21644;/&#25110;&#29305;&#23450;&#29615;&#22659;&#20013;&#30340;&#35828;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#35282;&#33394;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#21644;&#25104;&#21151;&#21033;&#29992;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27969;&#34892;&#30340; Cornell &#30005;&#24433;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340; 863 &#21517;&#28436;&#35762;&#32773;&#65292;&#21253;&#25324;&#29305;&#24449;&#24341;&#29992;&#21644;&#35282;&#33394;&#25551;&#36848;&#65292;&#20197;&#21450;&#36229;&#36807; 95&#65285; &#30340;&#29305;&#33394;&#30005;&#24433;&#30340;&#19968;&#32452;&#33258;&#21160;&#25552;&#21462;&#30340;&#20845;&#20010;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#27492;&#31867;&#27880;&#37322;&#26469;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22256;&#24785;&#20943;&#23569;&#39640;&#36798; 8.5&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35762;&#32773;&#65292;&#21363;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#22521;&#35757;&#25968;&#25454;&#30340;&#28436;&#35762;&#32773;&#65292;&#20381;&#36182;&#20110;&#35282;&#33394;&#30340;&#20154;&#21475;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#25910;&#38598;&#27492;&#31867;&#20803;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#39033;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#20197;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#24341;&#23548;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#20154;&#36896;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292; &#20197;&#35757;&#32451;&#26356;&#31934;&#30830;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#35780;&#20272;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.15939</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20154;&#36896;&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Physics-guided adversarial networks for artificial digital image correlation data generation. (arXiv:2303.15939v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#24341;&#23548;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#20154;&#36896;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292; &#20197;&#35757;&#32451;&#26356;&#31934;&#30830;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#35780;&#20272;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#65288;DIC&#65289;&#24050;&#25104;&#20026;&#35780;&#20272;&#21147;&#23398;&#23454;&#39564;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#23454;&#39564;&#12290;&#35780;&#20272;&#38656;&#35201;&#20934;&#30830;&#30340;&#35010;&#32441;&#36335;&#24452;&#21644;&#35010;&#32441;&#23574;&#31471;&#20301;&#32622;&#20449;&#24687;&#65292;&#30001;&#20110;&#22266;&#26377;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#21407;&#22240;&#65292;&#36825;&#24456;&#38590;&#33719;&#24471;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#32473;&#23450;&#26631;&#35760;&#30340;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#27492;&#30456;&#20851;&#20449;&#24687;&#38750;&#24120;&#25104;&#21151;&#12290;&#20026;&#20102;&#35757;&#32451;&#20855;&#26377;&#24191;&#27867;&#27867;&#21270;&#33021;&#21147;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#38656;&#35201;&#22823;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#26448;&#26009;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#25968;&#25454;&#36890;&#24120;&#24456;&#23569;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#24341;&#23548;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#20154;&#36896;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20915;&#23450;&#25968;&#25454;&#26679;&#26412;&#26159;&#30495;&#23454;&#36824;&#26159;&#20551;&#30340;&#65292;&#35813;&#37492;&#21035;&#22120;&#21478;&#22806;&#25509;&#25910;&#23548;&#20986;&#30340;von Mises&#31561;&#25928;&#24212;&#21464;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#36825;&#31181;&#29289;&#29702;&#24341;&#23548;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;GAN&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20135;&#29983;&#22823;&#37327;&#30340;&#20154;&#36896;DIC&#25968;&#25454;&#65292;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#35780;&#20272;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital image correlation (DIC) has become a valuable tool in the evaluation of mechanical experiments, particularly fatigue crack growth experiments. The evaluation requires accurate information of the crack path and crack tip position, which is difficult to obtain due to inherent noise and artefacts. Machine learning models have been extremely successful in recognizing this relevant information given labelled DIC displacement data. For the training of robust models, which generalize well, big data is needed. However, data is typically scarce in the field of material science and engineering because experiments are expensive and time-consuming. We present a method to generate synthetic DIC displacement data using generative adversarial networks with a physics-guided discriminator. To decide whether data samples are real or fake, this discriminator additionally receives the derived von Mises equivalent strain. We show that this physics-guided approach leads to improved results in terms 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36801;&#31227;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#26041;&#21521;&#35843;&#25972;&#26469;&#25913;&#36827;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#35813;&#26041;&#27861;&#26082;&#33021;&#20943;&#23567;&#22823;&#27493;&#38271;&#20013;&#30340;&#26356;&#26032;&#20559;&#24046;&#65292;&#21448;&#33021;&#20943;&#36731;&#23567;&#27493;&#38271;&#20013;&#30340;&#26356;&#26032;&#25391;&#33633;&#12290;</title><link>http://arxiv.org/abs/2303.15109</link><description>&lt;p&gt;
&#36890;&#36807;&#26041;&#21521;&#35843;&#25972;&#25913;&#36827;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Transferability of Adversarial Examples via Direction Tuning. (arXiv:2303.15109v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15109
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36801;&#31227;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#26041;&#21521;&#35843;&#25972;&#26469;&#25913;&#36827;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#35813;&#26041;&#27861;&#26082;&#33021;&#20943;&#23567;&#22823;&#27493;&#38271;&#20013;&#30340;&#26356;&#26032;&#20559;&#24046;&#65292;&#21448;&#33021;&#20943;&#36731;&#23567;&#27493;&#38271;&#20013;&#30340;&#26356;&#26032;&#25391;&#33633;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#65292;&#23545;&#25239;&#24615;&#26679;&#26412;&#20165;&#30001;&#26367;&#20195;&#27169;&#22411;&#29983;&#25104;&#65292;&#24182;&#22312;&#21463;&#23475;&#27169;&#22411;&#20013;&#23454;&#29616;&#26377;&#25928;&#25200;&#21160;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#24037;&#20316;&#22312;&#25913;&#36827;&#22522;&#20110;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#22522;&#20110;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#23454;&#38469;&#26356;&#26032;&#26041;&#21521;&#19982;&#26368;&#38497;&#30340;&#26356;&#26032;&#26041;&#21521;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#20559;&#24046;&#65292;&#36825;&#26159;&#30001;&#20110;&#22823;&#30340;&#26356;&#26032;&#27493;&#38271;&#23548;&#33268;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26080;&#27861;&#24456;&#22909;&#22320;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20943;&#23567;&#26356;&#26032;&#27493;&#38271;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#26356;&#26032;&#25391;&#33633;&#65292;&#20174;&#32780;&#20351;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#20063;&#26080;&#27861;&#22312;&#21463;&#23475;&#27169;&#22411;&#20013;&#36798;&#21040;&#24456;&#22909;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36801;&#31227;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21363;&#26041;&#21521;&#35843;&#25972;&#25915;&#20987;&#65292;&#26088;&#22312;&#20943;&#23567;&#22823;&#27493;&#38271;&#20013;&#30340;&#26356;&#26032;&#20559;&#24046;&#65292;&#24182;&#20943;&#36731;&#23567;&#27493;&#38271;&#20013;&#30340;&#26356;&#26032;&#25391;&#33633;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the transfer-based adversarial attacks, adversarial examples are only generated by the surrogate models and achieve effective perturbation in the victim models. Although considerable efforts have been developed on improving the transferability of adversarial examples generated by transfer-based adversarial attacks, our investigation found that, the big deviation between the actual and steepest update directions of the current transfer-based adversarial attacks is caused by the large update step length, resulting in the generated adversarial examples can not converge well. However, directly reducing the update step length will lead to serious update oscillation so that the generated adversarial examples also can not achieve great transferability to the victim models. To address these issues, a novel transfer-based attack, namely direction tuning attack, is proposed to not only decrease the update deviation in the large step length, but also mitigate the update oscillation in the smal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PENTACET&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;2300&#19975;&#20010;&#19978;&#19979;&#25991;&#20195;&#30721;&#27880;&#37322;&#21644;50&#19975;&#20010;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#27880;&#37322;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#23558;&#36827;&#19968;&#27493;&#25512;&#21160;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;SATD&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.14029</link><description>&lt;p&gt;
PENTACET&#25968;&#25454;&#8212;&#8212;2300&#19975;&#20010;&#19978;&#19979;&#25991;&#20195;&#30721;&#27880;&#37322;&#21644;50&#19975;&#20010;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
PENTACET data -- 23 Million Contextual Code Comments and 500,000 SATD comments. (arXiv:2303.14029v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PENTACET&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;2300&#19975;&#20010;&#19978;&#19979;&#25991;&#20195;&#30721;&#27880;&#37322;&#21644;50&#19975;&#20010;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#27880;&#37322;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#23558;&#36827;&#19968;&#27493;&#25512;&#21160;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;SATD&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#65288;SATD&#65289;&#30740;&#31350;&#20351;&#29992;&#22914;&#8220;TODO&#8221;&#21644;&#8220;FIXME&#8221;&#20043;&#31867;&#30340;&#26174;&#24335;SATD&#29305;&#24449;&#36827;&#34892;SATD&#26816;&#27979;&#12290;&#26356;&#20180;&#32454;&#22320;&#35266;&#23519;&#21457;&#29616;&#65292;&#19968;&#20123;SATD&#30740;&#31350;&#20351;&#29992;&#31616;&#21333;&#30340;SATD&#65288;&#8220;&#26131;&#20110;&#21457;&#29616;&#30340;&#8221;&#65289;&#20195;&#30721;&#27880;&#37322;&#32780;&#27809;&#26377;&#19978;&#19979;&#25991;&#25968;&#25454;&#65288;&#21069;&#25991;&#21644;&#21518;&#25991;&#28304;&#20195;&#30721;&#19978;&#19979;&#25991;&#65289;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;PENTACET&#65288;&#25110;5C&#25968;&#25454;&#38598;&#65289;&#25968;&#25454;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;PENTACET&#26159;&#19968;&#20010;&#30001;&#36129;&#29486;&#32773;&#36827;&#34892;&#31579;&#36873;&#30340;&#22823;&#22411;&#19978;&#19979;&#25991;&#20195;&#30721;&#27880;&#37322;&#25968;&#25454;&#24211;&#65292;&#26159;&#26368;&#20840;&#38754;&#30340;SATD&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;9,096&#20010;&#24320;&#28304;&#36719;&#20214;Java&#39033;&#30446;&#20013;&#25366;&#25496;&#20102;&#24635;&#20849;435&#30334;&#19975;&#34892;&#20195;&#30721;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;2300&#19975;&#20010;&#20195;&#30721;&#27880;&#37322;&#65292;&#24182;&#20026;&#27599;&#20010;&#27880;&#37322;&#25552;&#20379;&#20102;&#21069;&#21518;&#28304;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;50&#19975;&#20010;&#34987;&#26631;&#35760;&#20026;SATD&#30340;&#27880;&#37322;&#65292;&#21253;&#25324;&#8220;&#26131;&#20110;&#21457;&#29616;&#30340;&#8221;&#21644;&#8220;&#38590;&#20110;&#21457;&#29616;&#30340;&#8221; SATD&#12290;&#25105;&#20204;&#30456;&#20449;PENTACET&#25968;&#25454;&#38598;&#23558;&#36827;&#19968;&#27493;&#25512;&#21160;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;SATD&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Self-Admitted Technical Debt (SATD) research utilizes explicit SATD features such as 'TODO' and 'FIXME' for SATD detection. A closer look reveals several SATD research uses simple SATD ('Easy to Find') code comments without the contextual data (preceding and succeeding source code context). This work addresses this gap through PENTACET (or 5C dataset) data. PENTACET is a large Curated Contextual Code Comments per Contributor and the most extensive SATD data. We mine 9,096 Open Source Software Java projects with a total of 435 million LOC. The outcome is a dataset with 23 million code comments, preceding and succeeding source code context for each comment, and more than 500,000 comments labeled as SATD, including both 'Easy to Find' and 'Hard to Find' SATD. We believe PENTACET data will further SATD research using Artificial Intelligence techniques.
&lt;/p&gt;</description></item><item><title>ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.12364</link><description>&lt;p&gt;
ExBEHRT&#65306;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#25193;&#23637;Transformer&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes &amp; Progressions. (arXiv:2303.12364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12364
&lt;/p&gt;
&lt;p&gt;
ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ExBEHRT&#65292;&#23427;&#26159;BEHRT&#65288;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;BERT&#65289;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#29305;&#24449;&#31354;&#38388;&#20174;&#20165;&#32771;&#34385;&#35786;&#26029;&#21644;&#24739;&#32773;&#24180;&#40836;&#25193;&#23637;&#21040;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#65292;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#20020;&#24202;&#29305;&#24449;&#12289;&#29983;&#21629;&#20307;&#24449;&#12289;&#21560;&#28895;&#29366;&#24577;&#12289;&#35786;&#26029;&#12289;&#25163;&#26415;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#32479;&#19968;&#19981;&#21516;&#29305;&#24449;&#30340;&#39057;&#29575;&#21644;&#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38468;&#21152;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#20445;&#35777;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#26399;&#26799;&#24230;&#30340;&#25913;&#36827;&#26041;&#27861;&#23545;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#20197;&#21069;&#26410;&#24212;&#29992;&#20110;&#23558;EHR&#25968;&#25454;&#19982;Transformer&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#65292;&#22914;&#29305;&#24449;&#21644;&#20196;&#29260;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32959;&#30244;&#23398;&#24739;&#32773;&#30340;&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ExBEHRT&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health records), and apply different algorithms to interpret its results. While BEHRT considers only diagnoses and patient age, we extend the feature space to several multimodal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications, and laboratory tests, by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various downstream tasks in different diseases. To ensure robustness, we interpret model predictions using an adaptation of expected gradients, which has not been previously applied to transformers with EHR data and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the model representations of oncology patients, we show tha
&lt;/p&gt;</description></item><item><title>BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.08032</link><description>&lt;p&gt;
BODEGA: &#38024;&#23545;&#21487;&#20449;&#24230;&#35780;&#20272;&#20013;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08032
&lt;/p&gt;
&lt;p&gt;
BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26816;&#27979;&#19981;&#21487;&#20449;&#20869;&#23481;&#65292;&#22914;&#20551;&#26032;&#38395;&#12289;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12289;&#23459;&#20256;&#31561;&#12290;&#36739;&#20026;&#20934;&#30830;&#30340;&#27169;&#22411;&#65288;&#21487;&#33021;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#26377;&#21161;&#20110;&#31649;&#29702;&#20844;&#20849;&#30005;&#23376;&#24179;&#21488;&#65292;&#24182;&#32463;&#24120;&#23548;&#33268;&#20869;&#23481;&#21019;&#24314;&#32773;&#38754;&#20020;&#25552;&#20132;&#25298;&#32477;&#25110;&#24050;&#21457;&#24067;&#25991;&#26412;&#30340;&#25764;&#19979;&#12290;&#20026;&#20102;&#36991;&#20813;&#36827;&#19968;&#27493;&#34987;&#26816;&#27979;&#65292;&#20869;&#23481;&#21019;&#24314;&#32773;&#23581;&#35797;&#20135;&#29983;&#19968;&#20010;&#31245;&#24494;&#20462;&#25913;&#36807;&#30340;&#25991;&#26412;&#29256;&#26412;&#65288;&#21363;&#25915;&#20987;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#23548;&#33268;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BODEGA&#65306;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#27169;&#25311;&#20869;&#23481;&#31649;&#29702;&#30340;&#30495;&#23454;&#29992;&#20363;&#20013;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21463;&#27426;&#36814;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#21487;&#29992;&#25915;&#20987;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;ZSCL&#65292;&#26088;&#22312;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#38646;&#26679;&#20363;&#36716;&#31227;&#38477;&#32423;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24341;&#20837;&#21442;&#32771;&#25968;&#25454;&#38598;&#36827;&#34892;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#27169;&#22411;&#30340;&#38646;&#26679;&#20363;&#36716;&#31227;&#33021;&#21147;&#30340;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2303.06628</link><description>&lt;p&gt;
&#38450;&#27490;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#20986;&#29616;&#38646;&#26679;&#20363;&#36716;&#31227;&#38477;&#32423;
&lt;/p&gt;
&lt;p&gt;
Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models. (arXiv:2303.06628v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;ZSCL&#65292;&#26088;&#22312;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#38646;&#26679;&#20363;&#36716;&#31227;&#38477;&#32423;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24341;&#20837;&#21442;&#32771;&#25968;&#25454;&#38598;&#36827;&#34892;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#27169;&#22411;&#30340;&#38646;&#26679;&#20363;&#36716;&#31227;&#33021;&#21147;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#21487;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36866;&#24212;&#26032;&#30340;&#25110;&#26410;&#32463;&#35757;&#32451;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#36830;&#32493;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#27169;&#22411;&#30340;&#38646;&#26679;&#20363;&#36716;&#31227;&#33021;&#21147;&#26174;&#33879;&#38477;&#20302;&#12290;&#29616;&#26377;&#30340;CL&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22238;&#25918;&#20808;&#21069;&#30340;&#25968;&#25454;&#26469;&#20943;&#36731;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;CLIP&#25968;&#25454;&#38598;&#26159;&#31169;&#26377;&#30340;&#65292;&#22238;&#25918;&#26041;&#27861;&#26080;&#27861;&#35775;&#38382;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#22238;&#25918;&#20808;&#21069;&#23398;&#20064;&#30340;&#19979;&#28216;&#20219;&#21153;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#25439;&#32791;&#38646;&#26679;&#20363;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#21644;&#21442;&#25968;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;ZSCL&#65292;&#20197;&#38450;&#27490;&#22312;&#36830;&#32493;&#23398;&#20064;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26102;&#20986;&#29616;&#38646;&#26679;&#20363;&#36716;&#31227;&#38477;&#32423;&#12290;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21442;&#32771;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24403;&#21069;&#21644;&#21021;&#22987;&#27169;&#22411;&#20043;&#38388;&#30340;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) can help pre-trained vision-language models efficiently adapt to new or under-trained data distributions without re-training. Nevertheless, during the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model's zero-shot transfer ability significantly degrades due to catastrophic forgetting. Existing CL methods can mitigate forgetting by replaying previous data. However, since the CLIP dataset is private, replay methods cannot access the pre-training dataset. In addition, replaying data of previously learned downstream tasks can enhance their performance but comes at the cost of sacrificing zero-shot performance. To address this challenge, we propose a novel method ZSCL to prevent zero-shot transfer degradation in the continual learning of vision-language models in both feature and parameter space. In the feature space, a reference dataset is introduced for distillation between the current and initial models. The r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#20154;&#26426;&#36741;&#21161;&#30340;&#21327;&#21516;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26234;&#33021;&#35774;&#22791;&#32676;&#19982;&#26080;&#20154;&#26426;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32771;&#34385;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#36890;&#20449;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#23548;&#20986;&#20102;&#21327;&#21516;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#26469;&#25552;&#39640;&#35757;&#32451;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.02266</link><description>&lt;p&gt;
&#37319;&#29992;&#26426;&#36733;&#21327;&#35843;&#22120;&#36827;&#34892;&#21327;&#21516;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning with a Drone Orchestrator. (arXiv:2303.02266v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#20154;&#26426;&#36741;&#21161;&#30340;&#21327;&#21516;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26234;&#33021;&#35774;&#22791;&#32676;&#19982;&#26080;&#20154;&#26426;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32771;&#34385;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#36890;&#20449;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#23548;&#20986;&#20102;&#21327;&#21516;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#26469;&#25552;&#39640;&#35757;&#32451;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26080;&#20154;&#26426;&#36741;&#21161;&#21327;&#21516;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#65292;&#26234;&#33021;&#26080;&#32447;&#35774;&#22791;&#32676;&#36890;&#36807;&#26080;&#20154;&#26426;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#27599;&#20010;&#35774;&#22791;&#20351;&#29992;&#20854;&#20256;&#24863;&#22120;&#35760;&#24405;&#26469;&#33258;&#29615;&#22659;&#30340;&#26679;&#26412;&#65292;&#20197;&#33719;&#21462;&#29992;&#20110;&#35757;&#32451;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#30001;&#20110;&#21508;&#35774;&#22791;&#30340;&#25968;&#25454;&#37327;&#21644;&#20256;&#24863;&#22120;&#22122;&#22768;&#27700;&#24179;&#19981;&#21516;&#65292;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#20005;&#37325;&#30340;&#24322;&#36136;&#24615;&#12290;&#26234;&#33021;&#35774;&#22791;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#38598;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#24182;&#23558;&#27169;&#22411;&#21442;&#25968;&#19982;&#26080;&#20154;&#26426;&#36827;&#34892;&#20132;&#25442;&#20197;&#36827;&#34892;&#32858;&#21512;&#12290;&#22312;&#32771;&#34385;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#20256;&#24863;&#22120;&#22122;&#22768;&#27700;&#24179;&#21644;&#36890;&#20449;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#23548;&#20986;&#20102;&#21327;&#21516;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#32456;&#20934;&#30830;&#29575;&#30340;&#26080;&#20154;&#26426;&#36712;&#36857;&#12290;&#25152;&#25552;&#20986;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#32771;&#34385;&#20102;&#35774;&#22791;&#30340;&#25968;&#25454;&#29305;&#24615;&#65288;&#21363;&#26412;&#22320;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#22122;&#22768;&#27700;&#24179;&#65289;&#20197;&#21450;&#20854;&#26080;&#32447;&#36890;&#20449;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the problem of drone-assisted collaborative learning is considered. In this scenario, swarm of intelligent wireless devices train a shared neural network (NN) model with the help of a drone. Using its sensors, each device records samples from its environment to gather a local dataset for training. The training data is severely heterogeneous as various devices have different amount of data and sensor noise level. The intelligent devices iteratively train the NN on their local datasets and exchange the model parameters with the drone for aggregation. For this system, the convergence rate of collaborative learning is derived while considering data heterogeneity, sensor noise levels, and communication errors, then, the drone trajectory that maximizes the final accuracy of the trained NN is obtained. The proposed trajectory optimization approach is aware of both the devices data characteristics (i.e., local dataset size and noise level) and their wireless channel conditions, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#33719;&#21462;&#20840;&#38754;&#23545;&#31216;&#30340;&#28857;&#31890;&#23376;&#32676;&#20307;&#65288;&#22914;&#20998;&#23376;&#20013;&#30340;&#21407;&#23376;&#65289;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26377;&#38480;&#23376;&#38598;&#25551;&#36848;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.14770</link><description>&lt;p&gt;
&#21407;&#23376;&#32467;&#26500;&#34920;&#24449;&#30340;&#23436;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
Completeness of Atomic Structure Representations. (arXiv:2302.14770v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33719;&#21462;&#20840;&#38754;&#23545;&#31216;&#30340;&#28857;&#31890;&#23376;&#32676;&#20307;&#65288;&#22914;&#20998;&#23376;&#20013;&#30340;&#21407;&#23376;&#65289;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26377;&#38480;&#23376;&#38598;&#25551;&#36848;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33719;&#21462;&#20840;&#38754;&#23545;&#31216;&#30340;&#28857;&#31890;&#23376;&#32676;&#20307;&#65288;&#22914;&#20998;&#23376;&#20013;&#30340;&#21407;&#23376;&#65289;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#29289;&#29702;&#23398;&#21644;&#29702;&#35770;&#21270;&#23398;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#31185;&#23398;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#25903;&#25745;&#20102;&#27169;&#22411;&#20934;&#30830;&#22797;&#29616;&#29289;&#29702;&#20851;&#31995;&#24182;&#19982;&#22522;&#26412;&#23545;&#31216;&#24615;&#21644;&#23432;&#24658;&#23450;&#24459;&#19968;&#33268;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#29992;&#20110;&#34920;&#31034;&#28857;&#20113;&#30340;&#25551;&#36848;&#31526;&#65288;&#23588;&#20854;&#26159;&#29992;&#20110;&#25551;&#36848;&#21407;&#23376;&#23610;&#24230;&#19978;&#29289;&#36136;&#30340;&#25551;&#36848;&#31526;&#65289;&#26080;&#27861;&#21306;&#20998;&#29305;&#27530;&#30340;&#31890;&#23376;&#25490;&#21015;&#12290;&#36825;&#20351;&#24471;&#26080;&#27861;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#23398;&#20064;&#23427;&#20204;&#30340;&#23646;&#24615;&#12290;&#23384;&#22312;&#21487;&#35777;&#26126;&#23436;&#22791;&#24615;&#30340;&#26694;&#26550;&#65292;&#20294;&#20165;&#22312;&#21516;&#26102;&#25551;&#36848;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#25165;&#26159;&#22914;&#27492;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26377;&#38480;&#23376;&#38598;&#25551;&#36848;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenge of obtaining a comprehensive and symmetric representation of point particle groups, such as atoms in a molecule, which is crucial in physics and theoretical chemistry. The problem has become even more important with the widespread adoption of machine-learning techniques in science, as it underpins the capacity of models to accurately reproduce physical relationships while being consistent with fundamental symmetries and conservation laws. However, the descriptors that are commonly used to represent point clouds -- most notably those adopted to describe matter at the atomic scale -- are unable to distinguish between special arrangements of particles. This makes it impossible to machine learn their properties. Frameworks that are provably complete exist but are only so in the limit in which they simultaneously describe the mutual relationship between all atoms, which is impractical. We present a novel approach to construct descriptors of finite cor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;COOLANT&#65292;&#19968;&#20010;&#29992;&#20110;&#36328;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#21319;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23545;&#40784;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#36328;&#27169;&#24577;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.14057</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;COOLANT&#65292;&#19968;&#20010;&#29992;&#20110;&#36328;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#21319;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23545;&#40784;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#36328;&#27169;&#24577;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#34701;&#21512;&#21333;&#27169;&#29305;&#24449;&#20197;&#20135;&#29983;&#22810;&#27169;&#24577;&#26032;&#38395;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#32858;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#20197;&#25552;&#21319;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COOLANT&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#40784;&#31934;&#24230;&#65292;&#25105;&#20204;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#22312;&#23545;&#27604;&#36807;&#31243;&#20013;&#36719;&#21270;&#36127;&#26679;&#26412;&#30340;&#25439;&#22833;&#39033;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#21147;&#24341;&#23548;&#27169;&#22359;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#24110;&#21161;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#22320;&#32858;&#21512;&#23545;&#40784;&#30340;&#21333;&#27169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic detection of multimodal fake news has gained a widespread attention recently. Many existing approaches seek to fuse unimodal features to produce multimodal news representations. However, the potential of powerful cross-modal contrastive learning methods for fake news detection has not been well exploited. Besides, how to aggregate features from different modalities to boost the performance of the decision-making process is still an open question. To address that, we propose COOLANT, a cross-modal contrastive learning framework for multimodal fake news detection, aiming to achieve more accurate image-text alignment. To further improve the alignment precision, we leverage an auxiliary task to soften the loss term of negative samples during the contrast process. A cross-modal fusion module is developed to learn the cross-modality correlations. An attention mechanism with an attention guidance module is implemented to help effectively and interpretably aggregate the aligned unimo
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#33258;&#28982;&#38899;&#39057;&#24405;&#38899;&#20013;&#25552;&#21462;&#22768;&#38899;&#20135;&#29983;&#24182;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#37325;&#35201;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#19968;&#20010;&#20934;&#30830;&#29575;&#36739;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#19981;&#21516;&#24405;&#38899;&#26465;&#20214;&#19979;&#30340;&#22024;&#26434;&#24405;&#38899;&#12290;</title><link>http://arxiv.org/abs/2302.07640</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#38899;&#39057;&#24405;&#38899;&#30340;&#22768;&#38899;&#20135;&#29983;&#26816;&#27979;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Detection and classification of vocal productions in large scale audio recordings. (arXiv:2302.07640v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07640
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#33258;&#28982;&#38899;&#39057;&#24405;&#38899;&#20013;&#25552;&#21462;&#22768;&#38899;&#20135;&#29983;&#24182;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#37325;&#35201;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#19968;&#20010;&#20934;&#30830;&#29575;&#36739;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#19981;&#21516;&#24405;&#38899;&#26465;&#20214;&#19979;&#30340;&#22024;&#26434;&#24405;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#33258;&#28982;&#38899;&#39057;&#24405;&#38899;&#20013;&#25552;&#21462;&#22768;&#38899;&#20135;&#29983;&#24182;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#27969;&#31243;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#35745;&#31639;&#27493;&#39588;&#65288;&#31383;&#21475;&#21270;&#12289;&#22122;&#38899;&#31867;&#21035;&#21019;&#24314;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#37325;&#37319;&#26679;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#37325;&#35201;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22312;&#19981;&#21516;&#24405;&#38899;&#26465;&#20214;&#19979;&#37319;&#38598;&#30340;&#22024;&#26434;&#24405;&#38899;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#33258;&#28982;&#38899;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#19968;&#20010;&#26159;&#26469;&#33258;&#20960;&#20869;&#20122;&#29394;&#29394;&#22312;&#28789;&#38271;&#31867;&#30740;&#31350;&#20013;&#24515;&#30340;&#24405;&#38899;&#65292;&#21478;&#19968;&#20010;&#26159;&#26469;&#33258;&#22312;&#23478;&#20013;&#24405;&#21046;&#30340;&#23156;&#20799;&#30340;&#24405;&#38899;&#12290;&#35813;&#27969;&#31243;&#20351;&#29992;&#20102;72&#20998;&#38047;&#21644;77&#20998;&#38047;&#30340;&#26631;&#27880;&#38899;&#39057;&#24405;&#38899;&#35757;&#32451;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;94.58%&#21644;99.76%&#12290;&#28982;&#21518;&#65292;&#23427;&#34987;&#29992;&#20110;&#22788;&#29702;443&#23567;&#26102;&#21644;174&#23567;&#26102;&#30340;&#33258;&#28982;&#36830;&#32493;&#24405;&#38899;&#65292;&#24182;&#21019;&#24314;/&gt;
&lt;/p&gt;
&lt;p&gt;
We propose an automatic data processing pipeline to extract vocal productions from large-scale natural audio recordings and classify these vocal productions. The pipeline is based on a deep neural network and adresses both issues simultaneously. Though a series of computationel steps (windowing, creation of a noise class, data augmentation, re-sampling, transfer learning, Bayesian optimisation), it automatically trains a neural network without requiring a large sample of labeled data and important computing resources. Our end-to-end methodology can handle noisy recordings made under different recording conditions. We test it on two different natural audio data sets, one from a group of Guinea baboons recorded from a primate research center and one from human babies recorded at home. The pipeline trains a model on 72 and 77 minutes of labeled audio recordings, with an accuracy of 94.58% and 99.76%. It is then used to process 443 and 174 hours of natural continuous recordings and it crea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#28041;&#21450;&#26080;&#32422;&#26463;&#38382;&#39064;&#21644;&#21160;&#24577;&#36951;&#25022;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#65292;&#22312;&#36866;&#24212;&#24615;&#21644;&#24212;&#29992;&#19978;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2301.13349</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23454;&#29616;&#26080;&#32422;&#26463;&#21160;&#24577;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Unconstrained Dynamic Regret via Sparse Coding. (arXiv:2301.13349v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#28041;&#21450;&#26080;&#32422;&#26463;&#38382;&#39064;&#21644;&#21160;&#24577;&#36951;&#25022;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#65292;&#22312;&#36866;&#24212;&#24615;&#21644;&#24212;&#29992;&#19978;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#22312;&#20004;&#20010;&#38382;&#39064;&#32467;&#26500;&#30340;&#32806;&#21512;&#19979;&#30340;&#24773;&#20917;&#65306;&#22495;&#26080;&#30028;&#65292;&#32780;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#36890;&#36807;&#21160;&#24577;&#36951;&#25022;&#26469;&#34913;&#37327;&#30340;&#12290;&#22788;&#29702;&#20219;&#19968;&#38382;&#39064;&#37117;&#35201;&#27714;&#36951;&#25022;&#30028;&#38480;&#20381;&#36182;&#20110;&#27604;&#36739;&#24207;&#21015;&#30340;&#26576;&#20123;&#22797;&#26434;&#24230;&#37327;&#24230; - &#29305;&#21035;&#26159;&#26080;&#32422;&#26463;OLO&#20013;&#30340;&#27604;&#36739;&#22120;&#33539;&#25968;&#65292;&#20197;&#21450;&#21160;&#24577;&#36951;&#25022;&#20013;&#30340;&#36335;&#24452;&#38271;&#24230;&#12290;&#19982;&#26368;&#36817;&#19968;&#31687;&#25991;&#31456;(Jacobsen&amp; Cutkosky&#65292;2022)&#36866;&#24212;&#36825;&#20004;&#20010;&#22797;&#26434;&#24230;&#37327;&#24230;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#12290;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#23454;&#29616;&#36866;&#24212;&#24615;&#65292;&#36825;&#20010;&#26694;&#26550;&#33258;&#28982;&#22320;&#21033;&#29992;&#20102;&#29615;&#22659;&#26356;&#22797;&#26434;&#30340;&#21069;&#32622;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38745;&#24577;&#26080;&#32422;&#26463;OLO&#26799;&#24230;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#26426;&#21046;&#35774;&#35745;&#12290;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by time series forecasting, we study Online Linear Optimization (OLO) under the coupling of two problem structures: the domain is unbounded, and the performance of an algorithm is measured by its dynamic regret. Handling either of them requires the regret bound to depend on certain complexity measure of the comparator sequence -- specifically, the comparator norm in unconstrained OLO, and the path length in dynamic regret. In contrast to a recent work (Jacobsen &amp; Cutkosky, 2022) that adapts to the combination of these two complexity measures, we propose an alternative complexity measure by recasting the problem into sparse coding. Adaptivity can be achieved by a simple modular framework, which naturally exploits more intricate prior knowledge of the environment. Along the way, we also present a new gradient adaptive algorithm for static unconstrained OLO, designed using novel continuous time machinery. This could be of independent interest.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;CodeBert&#22312;&#28304;&#30721;&#34920;&#31034;&#23398;&#20064;&#20013;&#23398;&#21040;&#20102;&#21738;&#20123;&#29305;&#24449;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#29702;&#35299;&#28304;&#20195;&#30721;&#30340;&#36923;&#36753;&#65292;&#32780;&#28304;&#20195;&#30721;&#30340;&#34920;&#31034;&#20381;&#36182;&#20110;&#31243;&#24207;&#21592;&#23450;&#20041;&#30340;&#21464;&#37327;&#21644;&#20989;&#25968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2301.08427</link><description>&lt;p&gt;
CodeBert&#33021;&#23398;&#21040;&#21738;&#20123;&#29305;&#24449;&#65306;BERT&#22522;&#20110;&#28304;&#30721;&#34920;&#31034;&#23398;&#20064;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning. (arXiv:2301.08427v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;CodeBert&#22312;&#28304;&#30721;&#34920;&#31034;&#23398;&#20064;&#20013;&#23398;&#21040;&#20102;&#21738;&#20123;&#29305;&#24449;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#29702;&#35299;&#28304;&#20195;&#30721;&#30340;&#36923;&#36753;&#65292;&#32780;&#28304;&#20195;&#30721;&#30340;&#34920;&#31034;&#20381;&#36182;&#20110;&#31243;&#24207;&#21592;&#23450;&#20041;&#30340;&#21464;&#37327;&#21644;&#20989;&#25968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;(BERT)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#25552;&#20986;&#65292;&#24182;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;BERT&#24212;&#29992;&#20110;&#28304;&#30721;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#22909;&#28040;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#29702;&#35299;&#28304;&#20195;&#30721;&#30340;&#36923;&#36753;&#12290;&#28304;&#20195;&#30721;&#30340;&#34920;&#31034;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#31243;&#24207;&#21592;&#23450;&#20041;&#30340;&#21464;&#37327;&#21644;&#20989;&#25968;&#21517;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29468;&#24819;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bidirectional Encoder Representations from Transformers (BERT) were proposed in the natural language process (NLP) and shows promising results. Recently researchers applied the BERT to source-code representation learning and reported some good news on several downstream tasks. However, in this paper, we illustrated that current methods cannot effectively understand the logic of source codes. The representation of source code heavily relies on the programmer-defined variable and function names. We design and implement a set of experiments to demonstrate our conjecture and provide some insights for future works.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#26799;&#24230;&#21160;&#21147;&#23398;&#21644;&#26223;&#35266;&#20998;&#26512;&#25581;&#31034;&#20102;&#28145;&#24230;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.14150</link><description>&lt;p&gt;
&#28145;&#24230;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#20013;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#21160;&#21147;&#23398;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Dynamics Theory of Implicit Regularization in Deep Low-Rank Matrix Factorization. (arXiv:2212.14150v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#26799;&#24230;&#21160;&#21147;&#23398;&#21644;&#26223;&#35266;&#20998;&#26512;&#25581;&#31034;&#20102;&#28145;&#24230;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#27491;&#21017;&#21270;&#26159;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#24320;&#22987;&#29992;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;(DMF)&#26469;&#35299;&#37322;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#24182;&#20998;&#26512;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#31163;&#25955;&#26799;&#24230;&#21160;&#21147;&#23398;&#30340;&#36712;&#36857;&#12290;&#36825;&#20123;&#31163;&#25955;&#26799;&#24230;&#21160;&#21147;&#23398;&#30456;&#23545;&#36739;&#23567;&#20294;&#19981;&#26159;&#26080;&#31351;&#23567;&#65292;&#22240;&#27492;&#24456;&#22909;&#22320;&#36866;&#24212;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#38469;&#23454;&#29616;&#12290;&#30446;&#21069;&#65292;&#31163;&#25955;&#26799;&#24230;&#21160;&#21147;&#23398;&#20998;&#26512;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#27973;&#23618;&#32593;&#32476;&#65292;&#20294;&#22312;&#28145;&#23618;&#32593;&#32476;&#20013;&#36935;&#21040;&#20102;&#22797;&#26434;&#35745;&#31639;&#30340;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21478;&#19968;&#31181;&#35299;&#37322;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#31163;&#25955;&#26799;&#24230;&#21160;&#21147;&#23398;&#26041;&#27861;&#65292;&#21363;&#26223;&#35266;&#20998;&#26512;&#12290;&#23427;&#20027;&#35201;&#20851;&#27880;&#26799;&#24230;&#21306;&#22495;&#65292;&#22914;&#38797;&#28857;&#21644;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#38797;&#28857;&#36867;&#36920;(SPE)&#38454;&#27573;&#21644;DMF&#20013;&#30697;&#38453;&#31209;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#31209;&#20026;R&#30340;&#30697;&#38453;&#37325;&#26500;&#65292;DMF&#23558;&#25910;&#25947;&#21040;&#20108;&#38454;&#20020;&#30028;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit regularization is an important way to interpret neural networks. Recent theory starts to explain implicit regularization with the model of deep matrix factorization (DMF) and analyze the trajectory of discrete gradient dynamics in the optimization process. These discrete gradient dynamics are relatively small but not infinitesimal, thus fitting well with the practical implementation of neural networks. Currently, discrete gradient dynamics analysis has been successfully applied to shallow networks but encounters the difficulty of complex computation for deep networks. In this work, we introduce another discrete gradient dynamics approach to explain implicit regularization, i.e. landscape analysis. It mainly focuses on gradient regions, such as saddle points and local minima. We theoretically establish the connection between saddle point escaping (SPE) stages and the matrix rank in DMF. We prove that, for a rank-R matrix reconstruction, DMF will converge to a second-order criti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#39640;&#23481;&#37327;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#25511;&#21046;&#39046;&#22495;&#30340;&#39640;&#24615;&#33021;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.06817</link><description>&lt;p&gt;
RT-1: &#29992;&#20110;&#23454;&#38469;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;.
&lt;/p&gt;
&lt;p&gt;
RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#39640;&#23481;&#37327;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#25511;&#21046;&#39046;&#22495;&#30340;&#39640;&#24615;&#33021;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#25110;&#20351;&#29992;&#23569;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26469;&#39640;&#27700;&#24179;&#22320;&#35299;&#20915;&#20855;&#20307;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;&#36825;&#31181;&#33021;&#21147;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25110;&#35821;&#38899;&#35782;&#21035;&#31561;&#20854;&#20182;&#39046;&#22495;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23578;&#26410;&#23637;&#31034;&#20986;&#26469;&#12290;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23588;&#20026;&#20851;&#38190;&#65292;&#30001;&#20110;&#25910;&#38598;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#38590;&#24230;&#36739;&#22823;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#25104;&#21151;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#22312;&#20110;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#24320;&#25918;&#24335;&#35757;&#32451;&#65292;&#32467;&#21512;&#21487;&#20197;&#21560;&#25910;&#25152;&#26377;&#22810;&#26679;&#21270;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#39640;&#23481;&#37327;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#31867;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#27169;&#22411;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#27169;&#22411;&#31867;&#21035;&#21450;&#20854;&#38543;&#25968;&#25454;&#22823;&#23567;&#32780;&#25512;&#24191;&#30340;&#33021;&#21147;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, mod
&lt;/p&gt;</description></item><item><title>&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#26159;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#32536;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.02374</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#26435;&#34913;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Trade-off between Over-smoothing and Over-squashing in Deep Graph Neural Networks. (arXiv:2212.02374v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02374
&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#26159;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#32536;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#31185;&#23398;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#28145;&#24230;GNN&#22312;&#24212;&#29992;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#24403;&#22534;&#21472;&#22270;&#21367;&#31215;&#23618;&#26102;&#65292;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#26159;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#21644;&#20174;&#36828;&#22788;&#33410;&#28857;&#20256;&#25773;&#20449;&#24687;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#19982;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#31526;&#30340;&#35889;&#38388;&#38548;&#26377;&#20869;&#22312;&#32852;&#31995;&#65292;&#23548;&#33268;&#36825;&#20004;&#20010;&#38382;&#39064;&#20043;&#38388;&#23384;&#22312;&#24517;&#28982;&#30340;&#26435;&#34913;&#65292;&#26080;&#27861;&#21516;&#26102;&#32531;&#35299;&#12290;&#20026;&#20102;&#36798;&#21040;&#21512;&#36866;&#30340;&#25240;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#32536;&#20316;&#20026;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38543;&#26426;Jost&#21644;Liu&#26354;&#29575;&#37325;&#36830;&#65288;SJLR&#65289;&#31639;&#27861;&#65292;&#23427;&#22312;&#36895;&#24230;&#19978;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19982;&#20197;&#21069;&#22522;&#20110;&#26354;&#29575;&#30340;&#26041;&#27861;&#30456;&#27604;&#20445;&#25345;&#20102;&#22522;&#26412;&#29305;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;SJLR&#22312;GNN&#35757;&#32451;&#36807;&#31243;&#20013;&#25191;&#34892;&#36793;&#32536;&#28155;&#21152;&#21644;&#21024;&#38500;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22522;&#26412;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have succeeded in various computer science applications, yet deep GNNs underperform their shallow counterparts despite deep learning's success in other domains. Over-smoothing and over-squashing are key challenges when stacking graph convolutional layers, hindering deep representation learning and information propagation from distant nodes. Our work reveals that over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues, as they cannot be alleviated simultaneously. To achieve a suitable compromise, we propose adding and removing edges as a viable approach. We introduce the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which is computationally efficient and preserves fundamental properties compared to previous curvature-based methods. Unlike existing approaches, SJLR performs edge addition and removal during GNN training while maintaining
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#35268;&#21017;&#21270;&#22120;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#30001;&#20984;&#33034;&#20989;&#25968;&#32452;&#25104;&#30340;&#35268;&#21017;&#21270;&#22120;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#21333;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#26799;&#24230;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21435;&#22122;&#12289;CT&#21644;MRI&#37325;&#24314;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#31867;&#20284;&#21487;&#38752;&#24615;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.12461</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#35268;&#21017;&#21270;&#22120;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
A Neural-Network-Based Convex Regularizer for Image Reconstruction. (arXiv:2211.12461v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#35268;&#21017;&#21270;&#22120;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#30001;&#20984;&#33034;&#20989;&#25968;&#32452;&#25104;&#30340;&#35268;&#21017;&#21270;&#22120;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#21333;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#26799;&#24230;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21435;&#22122;&#12289;CT&#21644;MRI&#37325;&#24314;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#31867;&#20284;&#21487;&#38752;&#24615;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#19978;&#30340;&#20986;&#29616;&#65292;&#20351;&#24471;&#37325;&#24314;&#36136;&#37327;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#27492;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#22914;&#20309;&#22312;&#20445;&#25345;&#24615;&#33021;&#25552;&#21319;&#30340;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#30001;&#20984;&#33034;&#20989;&#25968;&#32452;&#25104;&#30340;&#35268;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#35268;&#21017;&#21270;&#22120;&#30340;&#26799;&#24230;&#30001;&#19968;&#20010;&#20855;&#26377;&#21333;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20854;&#20013;&#21253;&#21547;&#36880;&#28176;&#22686;&#21152;&#21644;&#21487;&#23398;&#20064;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#35813;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22810;&#27493;&#39640;&#26031;&#21435;&#22122;&#22120;&#22312;&#20960;&#20998;&#38047;&#20869;&#36827;&#34892;&#35757;&#32451;&#12290;&#21435;&#22122;&#12289;CT&#21644;MRI&#37325;&#24314;&#30340;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#25552;&#20379;&#31867;&#20284;&#21487;&#38752;&#24615;&#20445;&#35777;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#20855;&#26377;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of deep-learning-based methods to solve image-reconstruction problems has enabled a significant increase in reconstruction quality. Unfortunately, these new methods often lack reliability and explainability, and there is a growing interest to address these shortcomings while retaining the boost in performance. In this work, we tackle this issue by revisiting regularizers that are the sum of convex-ridge functions. The gradient of such regularizers is parameterized by a neural network that has a single hidden layer with increasing and learnable activation functions. This neural network is trained within a few minutes as a multistep Gaussian denoiser. The numerical experiments for denoising, CT, and MRI reconstruction show improvements over methods that offer similar reliability guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#8212;&#8212;&#36870;&#26680;&#20998;&#35299;&#65288;IKD&#65289;&#65292;&#36890;&#36807;&#29305;&#24449;&#20540;&#20998;&#35299;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;GPLVMs&#65289;&#30340;&#21551;&#21457;&#65292;&#24182;&#22312;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#26041;&#38754;&#25552;&#20379;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.05961</link><description>&lt;p&gt;
&#21453;&#21521;&#26680;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Inverse Kernel Decomposition. (arXiv:2211.05961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#8212;&#8212;&#36870;&#26680;&#20998;&#35299;&#65288;IKD&#65289;&#65292;&#36890;&#36807;&#29305;&#24449;&#20540;&#20998;&#35299;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;GPLVMs&#65289;&#30340;&#21551;&#21457;&#65292;&#24182;&#22312;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#26041;&#38754;&#25552;&#20379;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#38477;&#32500;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#32780;&#20165;&#38656;&#29305;&#24449;&#20540;&#20998;&#35299;&#30340;&#38381;&#24335;&#26041;&#27861;&#22312;&#22797;&#26434;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#38754;&#19981;&#22815;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#8212;&#8212;&#36870;&#26680;&#20998;&#35299;&#65288;IKD&#65289;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;GPLVMs&#65289;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#19982;GPLVMs&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22788;&#29702;&#20855;&#26377;&#36739;&#24369;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#20998;&#22359;&#21644;&#27979;&#22320;&#32447;&#8212;&#8212;&#20197;&#21033;&#29992;&#23616;&#37096;&#30456;&#20851;&#30340;&#25968;&#25454;&#28857;&#24182;&#25552;&#20379;&#26356;&#22909;&#21644;&#25968;&#20540;&#19978;&#26356;&#31283;&#23450;&#30340;&#28508;&#21464;&#37327;&#20272;&#35745;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#34920;&#26126;&#65292;IKD&#26159;&#19968;&#31181;&#27604;&#20854;&#20182;&#22522;&#20110;&#29305;&#24449;&#20540;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#20248;&#21270;&#26041;&#27861;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state-of-the-art dimensionality reduction approaches largely rely on complicated optimization procedures. On the other hand, closed-form approaches requiring merely eigen-decomposition do not have enough sophistication and nonlinearity. In this paper, we propose a novel nonlinear dimensionality reduction method -- Inverse Kernel Decomposition (IKD) -- based on an eigen-decomposition of the sample covariance matrix of data. The method is inspired by Gaussian process latent variable models (GPLVMs) and has comparable performance with GPLVMs. To deal with very noisy data with weak correlations, we propose two solutions -- blockwise and geodesic -- to make use of locally correlated data points and provide better and numerically more stable latent estimations. We use synthetic datasets and four real-world datasets to show that IKD is a better dimensionality reduction method than other eigen-decomposition-based methods, and achieves comparable performance against optimization-based metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;Whisper&#27169;&#22411;&#34429;&#28982;&#22312;&#20998;&#24067;&#22806;&#36755;&#20837;&#21644;&#38543;&#26426;&#22122;&#22768;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#21364;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#26497;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#26469;&#22823;&#24133;&#38477;&#20302;Whisper&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#23454;&#38469;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#23545;&#25239;&#24615;&#31283;&#20581;ASR&#30340;&#38656;&#27714;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2210.17316</link><description>&lt;p&gt;
&#26377;&#19981;&#27490;&#19968;&#31181;&#31283;&#20581;&#24615;&#65306;&#29992;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;Whisper&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
There is more than one kind of robustness: Fooling Whisper with adversarial examples. (arXiv:2210.17316v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;Whisper&#27169;&#22411;&#34429;&#28982;&#22312;&#20998;&#24067;&#22806;&#36755;&#20837;&#21644;&#38543;&#26426;&#22122;&#22768;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#21364;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#26497;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#26469;&#22823;&#24133;&#38477;&#20302;Whisper&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#23454;&#38469;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#23545;&#25239;&#24615;&#31283;&#20581;ASR&#30340;&#38656;&#27714;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#23545;&#20110;&#20998;&#24067;&#22806;&#36755;&#20837;&#21644;&#38543;&#26426;&#22122;&#22768;&#37117;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31283;&#20581;&#24615;&#22312;&#23545;&#25239;&#24178;&#25200;&#19979;&#24182;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29983;&#25104;&#26497;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#65288;&#20449;&#22122;&#27604;&#20026;35-45dB&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;Whisper&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#36716;&#24405;&#25105;&#20204;&#36873;&#25321;&#30340;&#30446;&#26631;&#21477;&#23376;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#27450;&#39575;Whisper&#35821;&#35328;&#26816;&#27979;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#38477;&#20302;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#23545;&#19968;&#20010;&#24191;&#21463;&#27426;&#36814;&#30340;&#24320;&#28304;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#20855;&#26377;&#23454;&#38469;&#30340;&#23433;&#20840;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#25239;&#24615;&#31283;&#20581;ASR&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whisper is a recent Automatic Speech Recognition (ASR) model displaying impressive robustness to both out-of-distribution inputs and random noise. In this work, we show that this robustness does not carry over to adversarial noise. We show that we can degrade Whisper performance dramatically, or even transcribe a target sentence of our choice, by generating very small input perturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling the Whisper language detector we can very easily degrade the performance of multilingual models. These vulnerabilities of a widely popular open-source model have practical security implications and emphasize the need for adversarially robust ASR.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#31616;&#21333;&#32780;&#23450;&#37327;&#30340;&#25968;&#25454;&#20998;&#31163;&#23450;&#24459;&#65292;&#27599;&#19968;&#23618;&#37117;&#20197;&#24658;&#23450;&#30340;&#20960;&#20309;&#36895;&#29575;&#25913;&#21892;&#25968;&#25454;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36825;&#20010;&#23450;&#24459;&#20026;&#26550;&#26500;&#35774;&#35745;&#12289;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#26679;&#26412;&#22806;&#24615;&#33021;&#20197;&#21450;&#39044;&#27979;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.17020</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#31163;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
A Law of Data Separation in Deep Learning. (arXiv:2210.17020v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17020
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#31616;&#21333;&#32780;&#23450;&#37327;&#30340;&#25968;&#25454;&#20998;&#31163;&#23450;&#24459;&#65292;&#27599;&#19968;&#23618;&#37117;&#20197;&#24658;&#23450;&#30340;&#20960;&#20309;&#36895;&#29575;&#25913;&#21892;&#25968;&#25454;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36825;&#20010;&#23450;&#24459;&#20026;&#26550;&#26500;&#35774;&#35745;&#12289;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#26679;&#26412;&#22806;&#24615;&#33021;&#20197;&#21450;&#39044;&#27979;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#31185;&#23398;&#30340;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20854;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#39640;&#39118;&#38505;&#20915;&#31574;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20013;&#38388;&#23618;&#20013;&#22914;&#20309;&#22788;&#29702;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#23450;&#37327;&#30340;&#23450;&#24459;&#65292;&#23427;&#35268;&#23450;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#26681;&#25454;&#31867;&#21035;&#25104;&#21592;&#23558;&#25968;&#25454;&#22312;&#25152;&#26377;&#23618;&#20013;&#20998;&#31163;&#20986;&#26469;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#20010;&#23450;&#24459;&#34920;&#26126;&#65292;&#27599;&#19968;&#23618;&#37117;&#20197;&#24658;&#23450;&#30340;&#20960;&#20309;&#36895;&#29575;&#25913;&#21892;&#25968;&#25454;&#30340;&#20998;&#31163;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#20102;&#23427;&#30340;&#20986;&#29616;&#65292;&#26080;&#35770;&#26159;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#26550;&#26500;&#36824;&#26159;&#25968;&#25454;&#38598;&#19978;&#12290;&#36825;&#20010;&#23450;&#24459;&#20026;&#26550;&#26500;&#35774;&#35745;&#12289;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#26679;&#26412;&#22806;&#24615;&#33021;&#20197;&#21450;&#39044;&#27979;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning has enabled significant advances in many areas of science, its black-box nature hinders architecture design for future artificial intelligence applications and interpretation for high-stakes decision makings. We addressed this issue by studying the fundamental question of how deep neural networks process data in the intermediate layers. Our finding is a simple and quantitative law that governs how deep neural networks separate data according to class membership throughout all layers for classification. This law shows that each layer improves data separation at a constant geometric rate, and its emergence is observed in a collection of network architectures and datasets during training. This law offers practical guidelines for designing architectures, improving model robustness and out-of-sample performance, as well as interpreting the predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#31867;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2210.16192</link><description>&lt;p&gt;
&#21033;&#29992;&#20803;&#25968;&#25454;&#21644;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Audio Features with Metadata and Contrastive Learning. (arXiv:2210.16192v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#31867;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uses supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. Learning representations using only metadata obtains similar performance as using cross entropy with class labels only. State-of-the-art score is obtained when combining class labels with metadata using multiple supervised contrastive learning.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#37322;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19968;&#30452;&#26159;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;ICBHI&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#21512;&#36825;&#31181;&#24773;&#20917;&#30340;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#65292;&#32780;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#30417;&#30563;&#23545;&#27604;&#35774;&#32622;&#20013;&#20351;&#29992;&#22810;&#20010;&#20803;&#25968;&#25454;&#28304;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#19981;&#24179;&#34913;&#21644;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods based on supervised learning using annotations in an end-to-end fashion have been the state-of-the-art for classification problems. However, they may be limited in their generalization capability, especially in the low data regime. In this study, we address this issue using supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. We apply our approach on ICBHI, a respiratory sound classification dataset suited for this setting. We show that learning representations using only metadata, without class labels, obtains similar performance as using cross entropy with those labels only. In addition, we obtain state-of-the-art score when combining class labels with metadata using multiple supervised contrastive learning. This work suggests the potential of using multiple metadata sources in supervised contrastive settings, in particular in settings with class imbalance and few data. Our code is released 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#20809;&#21488;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32918;&#20687;&#22270;&#20687;&#37325;&#26032;&#29031;&#26126;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.10510</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#20809;&#21488;&#21644;&#21512;&#25104;&#21040;&#23454;&#38469;&#36866;&#24212;&#23398;&#20064;&#23545;&#32918;&#20687;&#22270;&#20687;&#37325;&#26032;&#29031;&#26126;
&lt;/p&gt;
&lt;p&gt;
Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation. (arXiv:2209.10510v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10510
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#20809;&#21488;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32918;&#20687;&#22270;&#20687;&#37325;&#26032;&#29031;&#26126;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#20154;&#30340;&#32918;&#20687;&#22270;&#20687;&#21644;&#30446;&#26631;&#29031;&#26126;&#30340;&#29615;&#22659;&#22270;&#65292;&#32918;&#20687;&#29031;&#26126;&#26088;&#22312;&#37325;&#26032;&#29031;&#26126;&#22270;&#20687;&#20013;&#30340;&#20154;&#29289;&#65292;&#22909;&#20687;&#35813;&#20154;&#20986;&#29616;&#22312;&#20855;&#26377;&#30446;&#26631;&#29031;&#26126;&#30340;&#29615;&#22659;&#20013;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#36755;&#20837;-&#36755;&#20986;&#37197;&#23545;&#30340;&#25968;&#25454;&#38598;&#26469;&#30417;&#30563;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#21033;&#29992;&#20809;&#21488;&#25429;&#25417;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#38656;&#35201;&#26114;&#36149;&#30340;&#29305;&#27530;&#25429;&#25417;&#35774;&#22791;&#21644;&#32791;&#26102;&#30340;&#21162;&#21147;&#65292;&#38480;&#21046;&#21482;&#26377;&#23569;&#25968;&#36164;&#28304;&#20016;&#23500;&#30340;&#23454;&#39564;&#23460;&#25165;&#33021;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20809;&#21488;&#30340;&#24773;&#20917;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#37325;&#26032;&#29031;&#26126;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#35748;&#35782;&#65292;&#21363;&#32918;&#20687;&#22270;&#20687;&#30340;&#25104;&#21151;&#37325;&#26032;&#29031;&#26126;&#21462;&#20915;&#20110;&#20004;&#20010;&#26465;&#20214;&#12290;&#39318;&#20808;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#27169;&#20223;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#30340;&#37325;&#26032;&#29031;&#26126;&#30340;&#34892;&#20026;&#12290;&#20854;&#27425;&#65292;&#36755;&#20986;&#24517;&#39035;&#26159;&#29031;&#29255;&#36136;&#37327;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a portrait image of a person and an environment map of the target lighting, portrait relighting aims to re-illuminate the person in the image as if the person appeared in an environment with the target lighting. To achieve high-quality results, recent methods rely on deep learning. An effective approach is to supervise the training of deep neural networks with a high-fidelity dataset of desired input-output pairs, captured with a light stage. However, acquiring such data requires an expensive special capture rig and time-consuming efforts, limiting access to only a few resourceful laboratories. To address the limitation, we propose a new approach that can perform on par with the state-of-the-art (SOTA) relighting methods without requiring a light stage. Our approach is based on the realization that a successful relighting of a portrait image depends on two conditions. First, the method needs to mimic the behaviors of physically-based relighting. Second, the output has to be photo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22352;&#26631;&#25237;&#24433;&#32593;&#32476;&#65288;SCOPE&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#37325;&#25237;&#24433;&#31574;&#30053;&#25913;&#21892;&#20102;&#31232;&#30095;&#35270;&#35282;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#30340;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.05483</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22352;&#26631;&#25237;&#24433;&#32593;&#32476;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#30340;&#32763;&#35793;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Coordinate Projection Network for Sparse-View Computed Tomography. (arXiv:2209.05483v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22352;&#26631;&#25237;&#24433;&#32593;&#32476;&#65288;SCOPE&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#37325;&#25237;&#24433;&#31574;&#30053;&#25913;&#21892;&#20102;&#31232;&#30095;&#35270;&#35282;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#30340;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22352;&#26631;&#25237;&#24433;&#32593;&#32476;&#65288;SCOPE&#65289;&#65292;&#36890;&#36807;&#35299;&#20915;&#36870;&#20307;&#23618;&#25668;&#24433;&#25104;&#20687;&#38382;&#39064;&#20174;&#21333;&#20010;&#31232;&#30095;&#35270;&#35282;&#27491;&#24358;&#22270;&#20013;&#37325;&#24314;&#26080;&#20266;&#24433;&#30340;CT&#22270;&#20687;&#12290;&#19982;&#26368;&#36817;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#32593;&#32476;&#65288;INR&#65289;&#35299;&#20915;&#31867;&#20284;&#38382;&#39064;&#30340;&#30456;&#20851;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26377;&#25928;&#32780;&#31616;&#21333;&#30340;&#37325;&#25237;&#24433;&#31574;&#30053;&#65292;&#33021;&#22815;&#25552;&#39640;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;CT&#37325;&#24314;&#24037;&#20316;&#30340;&#25668;&#24433;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#21463;&#21040;&#32447;&#24615;&#20195;&#25968;&#21644;&#36870;&#38382;&#39064;&#20043;&#38388;&#31616;&#21333;&#20851;&#31995;&#30340;&#21551;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#27424;&#23450;&#32447;&#24615;&#26041;&#31243;&#32452;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;INR&#36890;&#36807;&#22270;&#20687;&#36830;&#32493;&#24615;&#20808;&#39564;&#32422;&#26463;&#35299;&#31354;&#38388;&#24182;&#33719;&#24471;&#31895;&#31961;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#29983;&#25104;&#19968;&#31181;&#23494;&#38598;&#35270;&#35282;&#27491;&#24358;&#22270;&#65292;&#20197;&#25913;&#21892;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#31209;&#65292;&#24182;&#20135;&#29983;&#26356;&#31283;&#23450;&#30340;CT&#22270;&#20687;&#35299;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#26032;&#25237;&#24433;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#31232;&#30095;&#35270;&#35282;CT&#22270;&#20687;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present work, we propose a Self-supervised COordinate Projection nEtwork (SCOPE) to reconstruct the artifacts-free CT image from a single SV sinogram by solving the inverse tomography imaging problem. Compared with recent related works that solve similar problems using implicit neural representation network (INR), our essential contribution is an effective and simple re-projection strategy that pushes the tomography image reconstruction quality over supervised deep learning CT reconstruction works. The proposed strategy is inspired by the simple relationship between linear algebra and inverse problems. To solve the under-determined linear equation system, we first introduce INR to constrain the solution space via image continuity prior and achieve a rough solution. And secondly, we propose to generate a dense view sinogram that improves the rank of the linear equation system and produces a more stable CT image solution space. Our experiment results demonstrate that the re-projec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#21644;&#32852;&#21512;&#26102;&#31354;&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrendGCN&#26469;&#22686;&#24378;&#20132;&#36890;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#24179;&#34913;&#21160;&#24577;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22788;&#29702;&#20855;&#26377;&#32479;&#35745;&#30456;&#20851;&#24615;&#30340;&#39034;&#24207;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.03063</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#21644;&#32852;&#21512;&#26102;&#31354;&#23884;&#20837;&#22686;&#24378;&#20132;&#36890;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Robustness via Adversarial Learning and Joint Spatial-Temporal Embeddings in Traffic Forecasting. (arXiv:2208.03063v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#21644;&#32852;&#21512;&#26102;&#31354;&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrendGCN&#26469;&#22686;&#24378;&#20132;&#36890;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#24179;&#34913;&#21160;&#24577;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22788;&#29702;&#20855;&#26377;&#32479;&#35745;&#30456;&#20851;&#24615;&#30340;&#39034;&#24207;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#22478;&#24066;&#35268;&#21010;&#21644;&#35745;&#31639;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20132;&#36890;&#23545;&#35937;&#65288;&#20363;&#22914;&#20256;&#24863;&#22120;&#21644;&#36947;&#36335;&#27573;&#65289;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#21160;&#24577;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#35201;&#27714;&#20351;&#29992;&#39640;&#24230;&#28789;&#27963;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#27169;&#22411;&#22312;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#36235;&#21183;&#65288;&#19968;&#38454;&#23548;&#25968;&#38543;&#26102;&#38388;&#21464;&#21270;&#65289;&#26041;&#38754;&#21487;&#33021;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#29616;&#23454;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#21160;&#24577;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrendGCN&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#25193;&#23637;&#20102;GCN&#30340;&#28789;&#27963;&#24615;&#21644;&#29983;&#25104;&#23545;&#25239;&#25439;&#22833;&#30340;&#20998;&#24067;&#20445;&#25345;&#33021;&#21147;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#32479;&#35745;&#30456;&#20851;&#24615;&#30340;&#39034;&#24207;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21516;&#26102;&#32467;&#21512;&#20102;&#31354;&#38388;&#65288;&#33410;&#28857;&#32423;&#65289;&#23884;&#20837;&#21644;&#26102;&#38388;&#65288;&#26102;&#38388;&#32423;&#65289;&#23884;&#20837;&#65292;&#20197;&#32771;&#34385;&#24322;&#36136;&#30340;&#26102;&#31354;&#21367;&#31215;&#65307;&#21516;&#26102;&#65292;&#23427;&#20351;&#29992;GAN&#32467;&#26500;&#26469;&#31995;&#32479;&#35780;&#20272;&#23454;&#38469;&#25968;&#25454;&#21644;&#39044;&#27979;&#25968;&#25454;&#20043;&#38388;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is an essential problem in urban planning and computing. The complex dynamic spatial-temporal dependencies among traffic objects (e.g., sensors and road segments) have been calling for highly flexible models; unfortunately, sophisticated models may suffer from poor robustness especially in capturing the trend of the time series (1st-order derivatives with time), leading to unrealistic forecasts. To address the challenge of balancing dynamics and robustness, we propose TrendGCN, a new scheme that extends the flexibility of GCNs and the distribution-preserving capacity of generative and adversarial loss for handling sequential data with inherent statistical correlations. On the one hand, our model simultaneously incorporates spatial (node-wise) embeddings and temporal (time-wise) embeddings to account for heterogeneous space-and-time convolutions; on the other hand, it uses GAN structure to systematically evaluate statistical consistencies between the real and the pre
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;-&#23545;&#25239;&#31232;&#30095;&#24615;&#65292;&#23427;&#37327;&#21270;&#20102;&#25104;&#21151;&#25200;&#21160;&#30340;&#38590;&#24230;&#12290;&#31232;&#30095;&#24615;&#25581;&#31034;&#20102;&#40065;&#26834;&#27169;&#22411;&#20043;&#38388;&#30340;&#37325;&#35201;&#24046;&#24322;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.04129</link><description>&lt;p&gt;
&#36825;&#20010;&#27169;&#22411;&#26377;&#22810;&#23569;&#25200;&#21160;&#20250;&#30772;&#22351;&#23427;&#65311;&#35780;&#20272;&#36229;&#36234;&#23545;&#25239;&#20934;&#30830;&#24230;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
How many perturbations break this model? Evaluating robustness beyond adversarial accuracy. (arXiv:2207.04129v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;-&#23545;&#25239;&#31232;&#30095;&#24615;&#65292;&#23427;&#37327;&#21270;&#20102;&#25104;&#21151;&#25200;&#21160;&#30340;&#38590;&#24230;&#12290;&#31232;&#30095;&#24615;&#25581;&#31034;&#20102;&#40065;&#26834;&#27169;&#22411;&#20043;&#38388;&#30340;&#37325;&#35201;&#24046;&#24322;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#20934;&#30830;&#24230;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#24230;&#37327;&#26631;&#20934;&#24182;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#21040;&#40065;&#26834;&#24615;&#30340;&#25152;&#26377;&#26041;&#38754;&#65292;&#23588;&#20854;&#26159;&#24573;&#30053;&#20102;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#28857;&#21487;&#20197;&#25214;&#21040;&#22810;&#23569;&#25200;&#21160;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#23545;&#25239;&#31232;&#30095;&#24615;&#65292;&#23427;&#37327;&#21270;&#20102;&#22312;&#32473;&#23450;&#36755;&#20837;&#28857;&#21644;&#25200;&#21160;&#26041;&#21521;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#25104;&#21151;&#25200;&#21160;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#24615;&#22312;&#22810;&#20010;&#26041;&#38754;&#23545;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#65306;&#20363;&#22914;&#65292;&#23427;&#25581;&#31034;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#27169;&#22411;&#20043;&#38388;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#36825;&#26159;&#31934;&#30830;&#24230;&#20998;&#26512;&#25152;&#19981;&#20855;&#22791;&#30340;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#25552;&#39640;&#23427;&#20204;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#24403;&#24212;&#29992;&#23545;&#24369;&#25915;&#20987;&#26377;&#25928;&#20294;&#23545;&#24378;&#25915;&#20987;&#26080;&#25928;&#30340;&#30772;&#35299;&#38450;&#24481;&#26102;&#65292;&#31232;&#30095;&#24615;&#21487;&#20197;&#21306;&#20998;&#23436;&#20840;&#26080;&#25928;&#21644;&#37096;&#20998;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#24230;&#37327;&#40065;&#26834;&#24615;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness to adversarial attacks is typically evaluated with adversarial accuracy. While essential, this metric does not capture all aspects of robustness and in particular leaves out the question of how many perturbations can be found for each point. In this work, we introduce an alternative approach, adversarial sparsity, which quantifies how difficult it is to find a successful perturbation given both an input point and a constraint on the direction of the perturbation. We show that sparsity provides valuable insight into neural networks in multiple ways: for instance, it illustrates important differences between current state-of-the-art robust models them that accuracy analysis does not, and suggests approaches for improving their robustness. When applying broken defenses effective against weak attacks but not strong ones, sparsity can discriminate between the totally ineffective and the partially effective defenses. Finally, with sparsity we can measure increases in robustness th
&lt;/p&gt;</description></item><item><title>HRFuser&#26159;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20256;&#24863;&#22120;&#34701;&#21512;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#20108;&#32500;&#29289;&#20307;&#26816;&#27979;&#65292;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#32593;&#32476;&#21644;&#26032;&#22411;&#22810;&#31383;&#21475;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#36827;&#34892;&#22810;&#27169;&#24577;&#22810;&#20998;&#36776;&#29575;&#34701;&#21512;&#12290;&#22312;nuScenes&#21644;DENSE&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.15157</link><description>&lt;p&gt;
HRFuser: &#19968;&#31181;&#29992;&#20110;&#20108;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#20998;&#36776;&#29575;&#20256;&#24863;&#22120;&#34701;&#21512;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection. (arXiv:2206.15157v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15157
&lt;/p&gt;
&lt;p&gt;
HRFuser&#26159;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20256;&#24863;&#22120;&#34701;&#21512;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#20108;&#32500;&#29289;&#20307;&#26816;&#27979;&#65292;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#32593;&#32476;&#21644;&#26032;&#22411;&#22810;&#31383;&#21475;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#36827;&#34892;&#22810;&#27169;&#24577;&#22810;&#20998;&#36776;&#29575;&#34701;&#21512;&#12290;&#22312;nuScenes&#21644;DENSE&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38500;&#20102;&#26631;&#20934;&#30456;&#26426;&#22806;&#65292;&#36890;&#24120;&#36824;&#21253;&#25324;&#22810;&#20010;&#20854;&#20182;&#20256;&#24863;&#22120;&#65292;&#22914;&#28608;&#20809;&#38647;&#36798;&#21644;&#38647;&#36798;&#65292;&#36825;&#20123;&#20256;&#24863;&#22120;&#24110;&#21161;&#33719;&#21462;&#24863;&#30693;&#39550;&#39542;&#22330;&#26223;&#30340;&#26356;&#20016;&#23500;&#20449;&#24687;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#20960;&#39033;&#24037;&#20316;&#30528;&#37325;&#20110;&#23558;&#26576;&#20123;&#20256;&#24863;&#22120;&#23545;&#36827;&#34892;&#34701;&#21512;&#65292;&#22914;&#30456;&#26426;&#19982;&#28608;&#20809;&#38647;&#36798;&#25110;&#38647;&#36798;&#65292;&#20294;&#32570;&#20047;&#19968;&#31181;&#36890;&#29992;&#19988;&#27169;&#22359;&#21270;&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HRFuser&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#20108;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#27169;&#22359;&#21270;&#26550;&#26500;&#12290;&#23427;&#20197;&#22810;&#20998;&#36776;&#29575;&#26041;&#24335;&#34701;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#24182;&#21487;&#25193;&#23637;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#36755;&#20837;&#27169;&#24335;&#12290;HRFuser&#30340;&#35774;&#35745;&#22522;&#20110;&#29992;&#20110;&#20165;&#22270;&#20687;&#23494;&#38598;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#39640;&#20998;&#36776;&#29575;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#30340;&#22810;&#31383;&#21475;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#20316;&#20026;&#25191;&#34892;&#22810;&#27169;&#24577;&#22810;&#20998;&#36776;&#29575;&#34701;&#21512;&#30340;&#25163;&#27573;&#12290;&#36890;&#36807;&#23545;nuScenes&#21644;&#24694;&#21155;&#26465;&#20214;DENSE&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;HRFuser&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Besides standard cameras, autonomous vehicles typically include multiple additional sensors, such as lidars and radars, which help acquire richer information for perceiving the content of the driving scene. While several recent works focus on fusing certain pairs of sensors - such as camera with lidar or radar - by using architectural components specific to the examined setting, a generic and modular sensor fusion architecture is missing from the literature. In this work, we propose HRFuser, a modular architecture for multi-modal 2D object detection. It fuses multiple sensors in a multi-resolution fashion and scales to an arbitrary number of input modalities. The design of HRFuser is based on state-of-the-art high-resolution networks for image-only dense prediction and incorporates a novel multi-window cross-attention block as the means to perform fusion of multiple modalities at multiple resolutions. We demonstrate via extensive experiments on nuScenes and the adverse conditions DENSE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#22320;&#32858;&#21512;&#25551;&#36848;&#31526;&#25552;&#21462;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#36807;&#31243;&#65292;&#29992;&#20110;&#20943;&#23569;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#30340;&#20154;&#24037;&#24178;&#39044;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2206.04531</link><description>&lt;p&gt;
ECLAD: &#20351;&#29992;&#26412;&#22320;&#32858;&#21512;&#25551;&#36848;&#31526;&#25552;&#21462;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#22320;&#32858;&#21512;&#25551;&#36848;&#31526;&#25552;&#21462;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#36807;&#31243;&#65292;&#29992;&#20110;&#20943;&#23569;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#30340;&#20154;&#24037;&#24178;&#39044;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#20854;&#20013;&#40065;&#26834;&#24615;&#21644;&#23545;&#40784;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#25552;&#20986;&#20102;&#36890;&#36807;&#27010;&#24565;&#25552;&#21462;&#26469;&#29983;&#25104;CNN&#39044;&#27979;&#36807;&#31243;&#30340;&#39640;&#32423;&#35299;&#37322;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#22270;&#20687;&#20013;&#26159;&#21542;&#23384;&#22312;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#30830;&#23450;&#20854;&#20301;&#32622;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#24456;&#38590;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#28608;&#27963;&#22270;&#20687;&#30340;&#20687;&#32032;&#32858;&#21512;&#34920;&#31034;&#30340;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#23450;&#20301;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#25552;&#21462;&#25216;&#26415;&#39564;&#35777;&#36807;&#31243;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#20854;&#20027;&#35201;&#32452;&#20214;&#30340;&#20687;&#32032;&#27880;&#37322;&#65292;&#20943;&#23569;&#20102;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are increasingly being used in critical systems, where robustness and alignment are crucial. In this context, the field of explainable artificial intelligence has proposed the generation of high-level explanations of the prediction process of CNNs through concept extraction. While these methods can detect whether or not a concept is present in an image, they are unable to determine its location. What is more, a fair comparison of such approaches is difficult due to a lack of proper validation procedures. To address these issues, we propose a novel method for automatic concept extraction and localization based on representations obtained through pixel-wise aggregations of CNN activation maps. Further, we introduce a process for the validation of concept-extraction techniques based on synthetic datasets with pixel-wise annotations of their main components, reducing the need for human intervention. Extensive experimentation on both synthetic and real-w
&lt;/p&gt;</description></item><item><title>&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23376;&#31354;&#38388;&#35757;&#32451;&#21644;&#26435;&#37325;&#24179;&#22343;&#20540;&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#26524;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2205.13104</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#65306;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;&#19968;&#33324;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trainable Weight Averaging: A General Approach for Subspace Training. (arXiv:2205.13104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13104
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23376;&#31354;&#38388;&#35757;&#32451;&#21644;&#26435;&#37325;&#24179;&#22343;&#20540;&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#26524;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26159;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#25110;&#22312;&#35757;&#32451;&#36712;&#36857;&#19978;&#25191;&#34892;&#38477;&#32500;&#26041;&#27861;&#26469;&#25552;&#21462;&#23376;&#31354;&#38388;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#32500;&#24230;&#21644;&#25968;&#20540;&#36816;&#31639;&#26041;&#38754;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#25110;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23376;&#31354;&#38388;&#35757;&#32451;&#19982;&#26435;&#37325;&#24179;&#22343;&#20540;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35757;&#32451;&#26435;&#37325;&#24179;&#22343;&#20540;(TWA)&#65292;&#36825;&#26159;&#19968;&#31181;&#27867;&#21270;&#20197;&#21069;&#21162;&#21147;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;TWA&#22312;&#32500;&#24230;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#26696;&#26469;&#24212;&#23545;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#65292;&#23427;&#20801;&#35768;&#22810;&#20010;&#33410;&#28857;&#19978;&#30340;&#24182;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#20869;&#23384;&#21644;&#35745;&#31639;&#36127;&#25285;&#22343;&#21248;&#20998;&#37197;&#32473;&#27599;&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#23558;TWA&#24212;&#29992;&#20110;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) in low-dimensional subspaces is a promising direction for achieving efficient training and better generalization performance. Previous works extract the subspaces by using random projection or performing dimensionality reduction method on the training trajectory, but these methods can be inefficient or unstable in terms of dimensionality and numerical operations. In this paper, we connect subspace training to weight averaging and propose Trainable Weight Averaging (TWA), a general approach for subspace training that generalizes the previous efforts. TWA is efficient in terms of dimensionality and also easy to use, making it a promising new method for subspace training. We further design an efficient scheme for subspace training to cope with large-scale problems, which allows parallel training across multiple nodes and evenly distributing the memory and computation burden to each node. We apply TWA to efficient neural network training and improving f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#21644;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#20132;&#20998;&#35299;&#24314;&#27169;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#65292;&#24212;&#29992;&#36328;&#31354;&#38388;&#23616;&#37096;&#20445;&#25345;&#36827;&#34892;&#32858;&#31867;&#32467;&#26500;&#23398;&#20064;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2204.08247</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#19982;&#22270;&#23398;&#20064;&#30340;&#32852;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Joint Multi-view Unsupervised Feature Selection and Graph Learning. (arXiv:2204.08247v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#21644;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#20132;&#20998;&#35299;&#24314;&#27169;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#65292;&#24212;&#29992;&#36328;&#31354;&#38388;&#23616;&#37096;&#20445;&#25345;&#36827;&#34892;&#32858;&#31867;&#32467;&#26500;&#23398;&#20064;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#20043;&#21069;&#30340;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20027;&#35201;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#20351;&#29992;&#32858;&#31867;&#32467;&#26500;&#25110;&#30456;&#20284;&#24615;&#32467;&#26500;&#26469;&#25351;&#23548;&#29305;&#24449;&#36873;&#25321;&#65292;&#24573;&#30053;&#20102;&#32852;&#21512;&#20844;&#24335;&#21487;&#33021;&#24102;&#26469;&#30340;&#20114;&#24800;&#25928;&#30410;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#20840;&#23616;&#32467;&#26500;&#23398;&#20064;&#25110;&#23616;&#37096;&#32467;&#26500;&#23398;&#20064;&#26469;&#23398;&#20064;&#30456;&#20284;&#24615;&#32467;&#26500;&#65292;&#32570;&#20047;&#21516;&#26102;&#20855;&#22791;&#20840;&#23616;&#21644;&#23616;&#37096;&#32467;&#26500;&#24863;&#30693;&#30340;&#22270;&#23398;&#20064;&#33021;&#21147;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#21644;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;JMVFG&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#27491;&#20132;&#20998;&#35299;&#23545;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#65292;&#20854;&#20013;&#27599;&#20010;&#30446;&#26631;&#30697;&#38453;&#34987;&#20998;&#35299;&#20026;&#19968;&#20010;&#35270;&#22270;&#29305;&#23450;&#30340;&#22522;&#30697;&#38453;&#21644;&#19968;&#20010;&#35270;&#22270;&#19968;&#33268;&#30340;&#32858;&#31867;&#25351;&#31034;&#22120;&#12290;&#36328;&#31354;&#38388;&#23616;&#37096;&#20445;&#25345;&#34987;&#24212;&#29992;&#20110;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#32858;&#31867;&#32467;&#26500;&#23398;&#20064;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress, previous multi-view unsupervised feature selection methods mostly suffer from two limitations. First, they generally utilize either cluster structure or similarity structure to guide the feature selection, which neglect the possibility of a joint formulation with mutual benefits. Second, they often learn the similarity structure by either global structure learning or local structure learning, which lack the capability of graph learning with both global and local structural awareness. In light of this, this paper presents a joint multi-view unsupervised feature selection and graph learning (JMVFG) approach. Particularly, we formulate the multi-view feature selection with orthogonal decomposition, where each target matrix is decomposed into a view-specific basis matrix and a view-consistent cluster indicator. The cross-space locality preservation is incorporated to bridge the cluster structure learning in the projected space and the similarity learning (i.e.
&lt;/p&gt;</description></item><item><title>Con$^{2}$DA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#21644;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.01558</link><description>&lt;p&gt;
Con$^{2}$DA&#65306;&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#21644;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#31616;&#21270;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Con$^{2}$DA: Simplifying Semi-supervised Domain Adaptation by Learning Consistent and Contrastive Feature Representations. (arXiv:2204.01558v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01558
&lt;/p&gt;
&lt;p&gt;
Con$^{2}$DA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#21644;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Con$^{2}$DA&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#25193;&#23637;&#21040;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23545;&#32473;&#23450;&#36755;&#20837;&#36827;&#34892;&#38543;&#26426;&#25968;&#25454;&#36716;&#25442;&#29983;&#25104;&#37197;&#23545;&#30340;&#30456;&#20851;&#26679;&#26412;&#12290;&#20851;&#32852;&#30340;&#25968;&#25454;&#23545;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#26144;&#23556;&#21040;&#29305;&#24449;&#34920;&#31034;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24378;&#21046;&#20445;&#25345;&#20851;&#32852;&#25968;&#25454;&#23545;&#26679;&#26412;&#30340;&#29305;&#24449;&#34920;&#31034;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#23398;&#21040;&#30340;&#34920;&#31034;&#23545;&#20110;&#22788;&#29702;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#26159;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#25105;&#20204;&#27169;&#22411;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#24182;&#23637;&#31034;&#20102;&#65306;(i) &#23398;&#20064;&#19968;&#33268;&#24615;&#21644;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#23545;&#20110;&#36328;&#19981;&#21516;&#39046;&#22495;&#25552;&#21462;&#22909;&#30340;&#21028;&#21035;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#65292;&#21644;(ii) &#25105;&#20204;&#30340;&#27169;&#22411;&#20174;&#20351;&#29992;&#24378;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20013;&#33719;&#30410;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present Con$^{2}$DA, a simple framework that extends recent advances in semi-supervised learning to the semi-supervised domain adaptation (SSDA) problem. Our framework generates pairs of associated samples by performing stochastic data transformations to a given input. Associated data pairs are mapped to a feature representation space using a feature extractor. We use different loss functions to enforce consistency between the feature representations of associated data pairs of samples. We show that these learned representations are useful to deal with differences in data distributions in the domain adaptation problem. We performed experiments to study the main components of our model and we show that (i) learning of the consistent and contrastive feature representations is crucial to extract good discriminative features across different domains, and ii) our model benefits from the use of strong augmentation policies. With these findings, our method achieves state-of-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#35823;&#24046;&#27169;&#22411;&#19979;&#37051;&#25509;&#30697;&#38453;&#30340;&#21463;&#38480;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#30740;&#31350;&#20102;GCN&#22312;&#36825;&#31181;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#20934;&#30830;&#24615;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.07831</link><description>&lt;p&gt;
&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#22312;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network Sensitivity Under Probabilistic Error Model. (arXiv:2203.07831v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#35823;&#24046;&#27169;&#22411;&#19979;&#37051;&#25509;&#30697;&#38453;&#30340;&#21463;&#38480;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#30740;&#31350;&#20102;GCN&#22312;&#36825;&#31181;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#20934;&#30830;&#24615;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21487;&#20197;&#36890;&#36807;&#22270;&#21367;&#31215;&#25104;&#21151;&#23398;&#20064;&#22270;&#20449;&#21495;&#34920;&#31034;&#12290;&#22270;&#21367;&#31215;&#20381;&#36182;&#20110;&#22270;&#28388;&#27874;&#22120;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#25454;&#30340;&#25299;&#25169;&#20381;&#36182;&#20851;&#31995;&#24182;&#20256;&#25773;&#25968;&#25454;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#20256;&#25773;&#30697;&#38453;&#65288;&#20363;&#22914;&#37051;&#25509;&#30697;&#38453;&#65289;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#21487;&#33021;&#23545;&#22270;&#28388;&#27874;&#22120;&#21644;GCNs&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#27010;&#29575;&#22270;&#35823;&#24046;&#27169;&#22411;&#23545;GCN&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#37051;&#25509;&#30697;&#38453;&#21463;&#21040;&#22270;&#22823;&#23567;&#21644;&#35823;&#24046;&#27010;&#29575;&#20989;&#25968;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#24102;&#26377;&#33258;&#24490;&#29615;&#30340;&#24402;&#19968;&#21270;&#37051;&#25509;&#30697;&#38453;&#30340;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#23454;&#39564;&#26469;&#35828;&#26126;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#30740;&#31350;&#31616;&#21333;GCN&#22312;&#36825;&#31181;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#20934;&#30830;&#24615;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) can successfully learn the graph signal representation by graph convolution. The graph convolution depends on the graph filter, which contains the topological dependency of data and propagates data features. However, the estimation errors in the propagation matrix (e.g., the adjacency matrix) can have a significant impact on graph filters and GCNs. In this paper, we study the effect of a probabilistic graph error model on the performance of the GCNs. We prove that the adjacency matrix under the error model is bounded by a function of graph size and error probability. We further analytically specify the upper bound of a normalized adjacency matrix with self-loop added. Finally, we illustrate the error bounds by running experiments on a synthetic dataset and study the sensitivity of a simple GCN under this probabilistic error model on accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#23616;&#37096;&#24322;&#24120;&#21644;&#24674;&#22797;&#40065;&#26834;&#23884;&#20837;&#26469;&#25552;&#21319;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#26816;&#27979;&#25805;&#20316;&#65292;&#36890;&#36807;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#24674;&#22797;&#40065;&#26834;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#40657;&#30418;&#25915;&#20987;&#19979;&#24674;&#22797;&#40065;&#26834;&#30340;&#22270;&#34920;&#31034;&#24182;&#21462;&#24471;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.04936</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#26412;&#22320;&#38169;&#35823;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Robust Graph Representation Learning for Local Corruption Recovery. (arXiv:2202.04936v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#23616;&#37096;&#24322;&#24120;&#21644;&#24674;&#22797;&#40065;&#26834;&#23884;&#20837;&#26469;&#25552;&#21319;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#26816;&#27979;&#25805;&#20316;&#65292;&#36890;&#36807;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#24674;&#22797;&#40065;&#26834;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#40657;&#30418;&#25915;&#20987;&#19979;&#24674;&#22797;&#40065;&#26834;&#30340;&#22270;&#34920;&#31034;&#24182;&#21462;&#24471;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#21463;&#21040;&#22270;&#36755;&#20837;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#36861;&#27714;&#20840;&#23616;&#24179;&#28369;&#30340;&#22270;&#23884;&#20837;&#65292;&#28982;&#32780;&#25105;&#20204;&#35748;&#20026;&#24456;&#23569;&#35266;&#23519;&#21040;&#30340;&#24322;&#24120;&#21516;&#26679;&#23545;&#20934;&#30830;&#39044;&#27979;&#26377;&#23475;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22270;&#23398;&#20064;&#26041;&#26696;&#65292;&#33258;&#21160;&#26816;&#27979;&#65288;&#23616;&#37096;&#65289;&#25439;&#22351;&#30340;&#29305;&#24449;&#23646;&#24615;&#65292;&#24182;&#20026;&#39044;&#27979;&#20219;&#21153;&#24674;&#22797;&#40065;&#26834;&#23884;&#20837;&#12290;&#26816;&#27979;&#25805;&#20316;&#21033;&#29992;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#19981;&#23545;&#26412;&#22320;&#25439;&#22351;&#30340;&#20998;&#24067;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#23427;&#22312;&#19968;&#20010;&#26080;&#20559;&#30340;&#25513;&#30721;&#30697;&#38453;&#20013;&#23450;&#20301;&#24322;&#24120;&#33410;&#28857;&#23646;&#24615;&#30340;&#20301;&#32622;&#65292;&#36890;&#36807;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#24674;&#22797;&#40065;&#26834;&#20272;&#35745;&#12290;&#20248;&#21270;&#22120;&#25509;&#36817;&#20110;&#22312;Framelet&#22495;&#20013;&#31232;&#30095;&#19988;&#22312;&#36755;&#20837;&#35266;&#27979;&#26465;&#20214;&#19979;&#25509;&#36817;&#30340;&#26032;&#23884;&#20837;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#40657;&#30418;&#20013;&#24674;&#22797;&#40065;&#26834;&#30340;&#22270;&#34920;&#31034;&#24182;&#23454;&#29616;&#24322;&#24120;&#27745;&#26579;&#19979;&#30340;&#20248;&#31168;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of graph representation learning is affected by the quality of graph input. While existing research usually pursues a globally smoothed graph embedding, we believe the rarely observed anomalies are as well harmful to an accurate prediction. This work establishes a graph learning scheme that automatically detects (locally) corrupted feature attributes and recovers robust embedding for prediction tasks. The detection operation leverages a graph autoencoder, which does not make any assumptions about the distribution of the local corruptions. It pinpoints the positions of the anomalous node attributes in an unbiased mask matrix, where robust estimations are recovered with sparsity promoting regularizer. The optimizer approaches a new embedding that is sparse in the framelet domain and conditionally close to input observations. Extensive experiments are provided to validate our proposed model can recover a robust graph representation from black-box poisoning and achieve exce
&lt;/p&gt;</description></item><item><title>Oracle Teacher&#26159;&#19968;&#31181;&#26032;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#20110;CTC&#30340;&#24207;&#21015;&#27169;&#22411;&#30340;&#30446;&#26631;&#20449;&#24687;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#30693;&#35782;&#33976;&#39311;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2111.03664</link><description>&lt;p&gt;
Oracle Teacher: &#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#26356;&#22909;&#22320;&#36827;&#34892;&#22522;&#20110;CTC&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Oracle Teacher: Leveraging Target Information for Better Knowledge Distillation of CTC Models. (arXiv:2111.03664v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03664
&lt;/p&gt;
&lt;p&gt;
Oracle Teacher&#26159;&#19968;&#31181;&#26032;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#20110;CTC&#30340;&#24207;&#21015;&#27169;&#22411;&#30340;&#30446;&#26631;&#20449;&#24687;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#30693;&#35782;&#33976;&#39311;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#26356;&#22823;&#30340;&#32593;&#32476;&#65288;&#25945;&#24072;&#65289;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#26356;&#23567;&#30340;&#32593;&#32476;&#65288;&#23398;&#29983;&#65289;&#12290;&#20256;&#32479;&#30340;KD&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#20197;&#30417;&#30563;&#26041;&#24335;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#20854;&#20013;&#36755;&#20986;&#26631;&#31614;&#20165;&#34987;&#35270;&#20026;&#30446;&#26631;&#12290;&#22312;&#36827;&#19968;&#27493;&#25193;&#23637;&#36825;&#31181;&#30417;&#30563;&#26041;&#26696;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#30340;&#24207;&#21015;&#27169;&#22411;&#30340;&#26032;&#31867;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#21363;Oracle Teacher&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#28304;&#36755;&#20837;&#21644;&#36755;&#20986;&#26631;&#31614;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#30001;&#20110;Oracle Teacher&#36890;&#36807;&#21442;&#32771;&#30446;&#26631;&#20449;&#24687;&#23398;&#20064;&#26356;&#20934;&#30830;&#30340;CTC&#23545;&#40784;&#65292;&#22240;&#27492;&#23427;&#21487;&#20197;&#20026;&#23398;&#29983;&#25552;&#20379;&#26356;&#20248;&#21270;&#30340;&#25351;&#23548;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#19968;&#20010;&#28508;&#22312;&#39118;&#38505;&#26159;&#27169;&#22411;&#30340;&#36755;&#20986;&#30452;&#25509;&#22797;&#21046;&#30446;&#26631;&#36755;&#20837;&#12290;&#22522;&#20110;CTC&#31639;&#27861;&#30340;&#22810;&#23545;&#19968;&#26144;&#23556;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#36825;&#31181;&#24773;&#20917;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD), best known as an effective method for model compression, aims at transferring the knowledge of a bigger network (teacher) to a much smaller network (student). Conventional KD methods usually employ the teacher model trained in a supervised manner, where output labels are treated only as targets. Extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (CTC)-based sequence models, namely Oracle Teacher, that leverages both the source inputs and the output labels as the teacher model's input. Since the Oracle Teacher learns a more accurate CTC alignment by referring to the target information, it can provide the student with more optimal guidance. One potential risk for the proposed approach is a trivial solution that the model's output directly copies the target input. Based on a many-to-one mapping property of the CTC algorithm, we present a training strategy that can effectively prevent the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#21028;&#21035;&#24471;&#20998;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#32858;&#31867;&#25968;&#30446;&#12289;&#32858;&#31867;&#27169;&#22411;&#21644;&#31639;&#27861;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22522;&#20110;&#20108;&#27425;&#21028;&#21035;&#24471;&#20998;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#21442;&#32771;&#32858;&#31867;&#27010;&#24565;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#19968;&#33268;&#24615;&#20934;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#21487;&#20197;&#36890;&#36807;&#20108;&#27425;&#25110;&#32447;&#24615;&#36793;&#30028;&#24456;&#22909;&#20998;&#38548;&#30340;&#32676;&#32452;&#65292;&#23545;&#20110;&#24212;&#29992;&#20013;&#23547;&#25214;&#36825;&#31181;&#31867;&#22411;&#30340;&#32676;&#32452;&#24456;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2111.02302</link><description>&lt;p&gt;
&#36873;&#25321;&#32858;&#31867;&#25968;&#30446;&#12289;&#32858;&#31867;&#27169;&#22411;&#21644;&#31639;&#27861;&#65306;&#22522;&#20110;&#20108;&#27425;&#21028;&#21035;&#24471;&#20998;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Selecting the number of clusters, clustering models, and algorithms. A unifying approach based on the quadratic discriminant score. (arXiv:2111.02302v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#21028;&#21035;&#24471;&#20998;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#32858;&#31867;&#25968;&#30446;&#12289;&#32858;&#31867;&#27169;&#22411;&#21644;&#31639;&#27861;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22522;&#20110;&#20108;&#27425;&#21028;&#21035;&#24471;&#20998;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#21442;&#32771;&#32858;&#31867;&#27010;&#24565;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#19968;&#33268;&#24615;&#20934;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#21487;&#20197;&#36890;&#36807;&#20108;&#27425;&#25110;&#32447;&#24615;&#36793;&#30028;&#24456;&#22909;&#20998;&#38548;&#30340;&#32676;&#32452;&#65292;&#23545;&#20110;&#24212;&#29992;&#20013;&#23547;&#25214;&#36825;&#31181;&#31867;&#22411;&#30340;&#32676;&#32452;&#24456;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#20998;&#26512;&#38656;&#35201;&#20570;&#20986;&#35768;&#22810;&#20915;&#31574;&#65306;&#32858;&#31867;&#26041;&#27861;&#21644;&#38544;&#21547;&#30340;&#21442;&#32771;&#27169;&#22411;&#12289;&#32858;&#31867;&#25968;&#30446;&#65292;&#20197;&#21450;&#36890;&#24120;&#36824;&#26377;&#19968;&#20123;&#36229;&#21442;&#25968;&#21644;&#31639;&#27861;&#30340;&#35843;&#25972;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#20250;&#24471;&#21040;&#22810;&#20010;&#21010;&#20998;&#65292;&#26368;&#32456;&#26681;&#25454;&#39564;&#35777;&#25110;&#36873;&#25321;&#20934;&#21017;&#36873;&#25321;&#19968;&#20010;&#26368;&#32456;&#30340;&#21010;&#20998;&#12290;&#23384;&#22312;&#22823;&#37327;&#30340;&#39564;&#35777;&#26041;&#27861;&#65292;&#23427;&#20204;&#38544;&#24335;&#25110;&#26174;&#24335;&#22320;&#20551;&#35774;&#26576;&#31181;&#32858;&#31867;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24120;&#24120;&#34987;&#38480;&#21046;&#22312;&#29305;&#23450;&#26041;&#27861;&#24471;&#21040;&#30340;&#21010;&#20998;&#19978;&#36827;&#34892;&#25805;&#20316;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#21487;&#20197;&#36890;&#36807;&#20108;&#27425;&#25110;&#32447;&#24615;&#36793;&#30028;&#24456;&#22909;&#20998;&#38548;&#30340;&#32676;&#32452;&#12290;&#22522;&#20110;&#20108;&#27425;&#21028;&#21035;&#24471;&#20998;&#20989;&#25968;&#21644;&#25551;&#36848;&#32858;&#31867;&#22823;&#23567;&#12289;&#20013;&#24515;&#21644;&#25955;&#24067;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21442;&#32771;&#32858;&#31867;&#27010;&#24565;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#32858;&#31867;&#36136;&#37327;&#20934;&#21017;&#65292;&#31216;&#20026;&#20108;&#27425;&#24471;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20934;&#21017;&#19982;&#19968;&#33324;&#31867;&#21035;&#30340;&#26925;&#22278;&#23545;&#31216;&#20998;&#24067;&#29983;&#25104;&#30340;&#32676;&#32452;&#19968;&#33268;&#12290;&#22312;&#24212;&#29992;&#20013;&#65292;&#23547;&#25214;&#36825;&#31181;&#31867;&#22411;&#30340;&#32676;&#32452;&#26159;&#24120;&#35265;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cluster analysis requires many decisions: the clustering method and the implied reference model, the number of clusters and, often, several hyper-parameters and algorithms' tunings. In practice, one produces several partitions, and a final one is chosen based on validation or selection criteria. There exist an abundance of validation methods that, implicitly or explicitly, assume a certain clustering notion. Moreover, they are often restricted to operate on partitions obtained from a specific method. In this paper, we focus on groups that can be well separated by quadratic or linear boundaries. The reference cluster concept is defined through the quadratic discriminant score function and parameters describing clusters' size, center and scatter. We develop two cluster-quality criteria called quadratic scores. We show that these criteria are consistent with groups generated from a general class of elliptically-symmetric distributions. The quest for this type of groups is common in applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.03894</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification. (arXiv:2110.03894v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#37325;&#32534;&#31243;&#65288;AR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#65288;SCR&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;AR-SCR&#31995;&#32479;&#12290;AR&#36807;&#31243;&#26088;&#22312;&#20462;&#25913;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#22768;&#23398;&#20449;&#21495;&#65292;&#20197;&#37325;&#26032;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;SCR&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#37325;&#32534;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26631;&#31614;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;AR&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#26144;&#23556;&#25216;&#26415;&#26469;&#23545;&#40784;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#25216;&#26415;&#19982;&#21407;&#22987;AR&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20302;&#36164;&#28304;SCR&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;AR-SCR&#31995;&#32479;&#65292;&#21253;&#25324;&#38463;&#25289;&#20271;&#35821;&#12289;&#31435;&#38518;&#23451;&#35821;&#21644;&#35328;&#35821;&#38556;&#30861;&#24615;&#26222;&#36890;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;AM&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#30340;AR-SCR&#31995;&#32479;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#19988;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Arabic and Lithuanian speech commands datasets, with only a limited amount of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#26377;&#25928;&#29305;&#24449;&#25552;&#21462;&#36827;&#34892;&#32929;&#31080;&#20132;&#26131;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24615;&#33021;&#31579;&#36873;&#20986;&#26368;&#20339;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#22235;&#20010;&#20998;&#31867;&#22120;&#36827;&#34892;&#20132;&#26131;&#20915;&#31574;&#12290;&#26368;&#20339;&#27169;&#22411;&#22312;2011&#24180;7&#26376;&#33267;2019&#24180;1&#26376;&#26399;&#38388;&#27599;&#26085;&#20132;&#26131;&#20013;&#33719;&#24471;&#20102;54.35%&#30340;&#21033;&#28070;&#12290;</title><link>http://arxiv.org/abs/2107.13148</link><description>&lt;p&gt;
&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#19982;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#36827;&#34892;&#32929;&#31080;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Combining Machine Learning Classifiers for Stock Trading with Effective Feature Extraction. (arXiv:2107.13148v3 [q-fin.TR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.13148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#26377;&#25928;&#29305;&#24449;&#25552;&#21462;&#36827;&#34892;&#32929;&#31080;&#20132;&#26131;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24615;&#33021;&#31579;&#36873;&#20986;&#26368;&#20339;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#22235;&#20010;&#20998;&#31867;&#22120;&#36827;&#34892;&#20132;&#26131;&#20915;&#31574;&#12290;&#26368;&#20339;&#27169;&#22411;&#22312;2011&#24180;7&#26376;&#33267;2019&#24180;1&#26376;&#26399;&#38388;&#27599;&#26085;&#20132;&#26131;&#20013;&#33719;&#24471;&#20102;54.35%&#30340;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#24066;&#22330;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21644;&#27874;&#21160;&#24615;&#20351;&#24471;&#29992;&#20219;&#20309;&#27867;&#21270;&#26041;&#26696;&#33719;&#24471;&#21487;&#35266;&#21033;&#28070;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#26102;&#20132;&#26131;&#22312;&#32654;&#22269;&#32929;&#24066;&#20013;&#33719;&#24471;&#21487;&#35266;&#21033;&#28070;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#25214;&#21040;&#29305;&#23450;&#20132;&#26131;&#26399;&#38388;&#30340;&#26368;&#20339;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#36890;&#36807;&#24615;&#33021;&#26469;&#23558;&#29305;&#24449;&#20174;148&#20010;&#32553;&#23567;&#21040;&#32422;30&#20010;&#12290;&#27492;&#22806;&#65292;&#22312;&#27599;&#27425;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#21069;&#65292;&#25105;&#20204;&#20250;&#21160;&#24577;&#36873;&#25321;&#21069;25&#20010;&#29305;&#24449;&#12290;&#23427;&#20351;&#29992;&#20102;&#22235;&#20010;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#23398;&#20064;&#65306;&#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#20915;&#31574;&#26641;&#12289;&#20855;&#26377;L1&#27491;&#21017;&#21270;&#30340;&#36923;&#36753;&#22238;&#24402;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#20197;&#20915;&#23450;&#26159;&#21542;&#23545;&#29305;&#23450;&#32929;&#31080;&#36827;&#34892;&#20570;&#22810;&#25110;&#20570;&#31354;&#20132;&#26131;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;2011&#24180;7&#26376;&#33267;2019&#24180;1&#26376;&#20043;&#38388;&#36827;&#34892;&#27599;&#26085;&#20132;&#26131;&#65292;&#33719;&#24471;&#20102;54.35%&#30340;&#21033;&#28070;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#19982;&#26377;&#25928;&#29305;&#24449;&#25552;&#21462;&#22312;&#32929;&#31080;&#20132;&#26131;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unpredictability and volatility of the stock market render it challenging to make a substantial profit using any generalised scheme. Many previous studies tried different techniques to build a machine learning model, which can make a significant profit in the US stock market by performing live trading. However, very few studies have focused on the importance of finding the best features for a particular trading period. Our top approach used the performance to narrow down the features from a total of 148 to about 30. Furthermore, the top 25 features were dynamically selected before each time training our machine learning model. It uses ensemble learning with four classifiers: Gaussian Naive Bayes, Decision Tree, Logistic Regression with L1 regularization, and Stochastic Gradient Descent, to decide whether to go long or short on a particular stock. Our best model performed daily trade between July 2011 and January 2019, generating 54.35% profit. Finally, our work showcased that mixtu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#38142;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRFs&#65289;&#38480;&#21046;&#21040;&#27491;&#21017;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#24191;&#27867;&#30340;&#32422;&#26463;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#36825;&#20123;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2106.07306</link><description>&lt;p&gt;
&#38480;&#21046;&#32447;&#24615;&#38142;&#26465;&#20214;&#38543;&#26426;&#22330;&#21040;&#27491;&#21017;&#35821;&#35328;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#38142;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRFs&#65289;&#38480;&#21046;&#21040;&#27491;&#21017;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#24191;&#27867;&#30340;&#32422;&#26463;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#36825;&#20123;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#34920;&#31034;&#36755;&#20986;&#32467;&#26500;&#20013;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#24403;&#36755;&#20986;&#20197;&#24207;&#21015;&#24418;&#24335;&#32467;&#26500;&#21270;&#26102;&#65292;&#32447;&#24615;&#38142;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRFs&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#31867;&#65292;&#21487;&#20197;&#23398;&#20064;&#36755;&#20986;&#20013;&#30340;&#8220;&#23616;&#37096;&#8221;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;CRF&#30340;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#20351;&#24471;CRFs&#26080;&#27861;&#34920;&#31034;&#20855;&#26377;&#8220;&#38750;&#23616;&#37096;&#8221;&#20381;&#36182;&#20851;&#31995;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#26631;&#20934;CRFs&#26080;&#27861;&#28385;&#36275;&#25968;&#25454;&#30340;&#38750;&#23616;&#37096;&#32422;&#26463;&#65288;&#20363;&#22914;&#36755;&#20986;&#26631;&#31614;&#30340;&#20840;&#23616;&#24615;&#32422;&#26463;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;CRFs&#30340;&#25512;&#24191;&#24418;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#21487;&#33021;&#30340;&#36755;&#20986;&#32467;&#26500;&#31354;&#38388;&#25351;&#23450;&#20026;&#27491;&#21017;&#35821;&#35328;$\mathcal{L}$&#26469;&#24378;&#21046;&#25191;&#34892;&#24191;&#27867;&#30340;&#32422;&#26463;&#65292;&#21253;&#25324;&#38750;&#23616;&#37096;&#32422;&#26463;&#12290;&#32467;&#26524;&#30340;&#27491;&#21017;&#32422;&#26463;CRF&#65288;RegCCRF&#65289;&#20855;&#26377;&#19982;&#26631;&#20934;CRF&#30456;&#21516;&#30340;&#24418;&#24335;&#23646;&#24615;&#65292;&#20294;&#23545;&#20110;&#19981;&#22312;$\mathcal{L}$&#20013;&#30340;&#25152;&#26377;&#26631;&#31614;&#24207;&#21015;&#20998;&#37197;&#38646;&#27010;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RegCCRFs&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#32422;&#26463;&#65292;&#19982;&#30456;&#20851;&#27169;&#22411;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in structured prediction is to represent the interdependencies within output structures. When outputs are structured as sequences, linear-chain conditional random fields (CRFs) are a widely used model class which can learn \textit{local} dependencies in the output. However, the CRF's Markov assumption makes it impossible for CRFs to represent distributions with \textit{nonlocal} dependencies, and standard CRFs are unable to respect nonlocal constraints of the data (such as global arity constraints on output labels). We present a generalization of CRFs that can enforce a broad class of constraints, including nonlocal ones, by specifying the space of possible output structures as a regular language $\mathcal{L}$. The resulting regular-constrained CRF (RegCCRF) has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$. Notably, RegCCRFs can incorporate their constraints during training, while related models
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22235;&#36275;&#36339;&#36291;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#36339;&#36291;&#19981;&#21516;&#36317;&#31163;&#21644;&#39640;&#24230;&#65292;&#24182;&#33021;&#24212;&#23545;&#19981;&#24179;&#22374;&#22320;&#24418;&#21644;&#21487;&#21464;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2011.07089</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#40065;&#26834;&#24615;&#22235;&#36275;&#36339;&#36291;
&lt;/p&gt;
&lt;p&gt;
Robust Quadruped Jumping via Deep Reinforcement Learning. (arXiv:2011.07089v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.07089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22235;&#36275;&#36339;&#36291;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#36339;&#36291;&#19981;&#21516;&#36317;&#31163;&#21644;&#39640;&#24230;&#65292;&#24182;&#33021;&#24212;&#23545;&#19981;&#24179;&#22374;&#22320;&#24418;&#21644;&#21487;&#21464;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22235;&#36275;&#26426;&#22120;&#20154;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#36339;&#36291;&#19981;&#21516;&#36317;&#31163;&#21644;&#39640;&#24230;&#30340;&#19968;&#33324;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22914;&#19981;&#24179;&#22374;&#22320;&#24418;&#21644;&#21487;&#21464;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#21442;&#25968;&#12290;&#20026;&#20102;&#22312;&#36825;&#26679;&#30340;&#26465;&#20214;&#19979;&#20934;&#30830;&#36339;&#36291;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20511;&#37492;&#24182;&#22686;&#24378;&#20102;&#22235;&#36275;&#36339;&#36291;&#30340;&#38750;&#32447;&#24615;&#36712;&#36857;&#20248;&#21270;&#30340;&#22797;&#26434;&#35299;&#20915;&#26041;&#26696;&#12290;&#19982;&#29420;&#31435;&#20248;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#38480;&#21046;&#20102;&#36339;&#36291;&#36215;&#36339;&#28857;&#24517;&#39035;&#22312;&#24179;&#22374;&#22320;&#38754;&#19978;&#65292;&#36824;&#35201;&#27714;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#23545;&#26497;&#19981;&#24179;&#22374;&#22320;&#24418;&#21644;&#21487;&#21464;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#21442;&#25968;&#20197;&#21450;&#29615;&#22659;&#26465;&#20214;&#30340;&#36339;&#36291;&#40065;&#26834;&#24615;&#12290;&#19982;&#34892;&#36208;&#21644;&#22868;&#36305;&#30456;&#27604;&#65292;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#31215;&#26497;&#36339;&#36291;&#38656;&#35201;&#32771;&#34385;&#30005;&#26426;&#30340;&#25197;&#30697;-&#36895;&#24230;&#20851;&#31995;&#20197;&#21450;&#26426;&#22120;&#20154;&#30340;&#24635;&#21151;&#29575;&#38480;&#21046;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#32435;&#20837;&#25105;&#20204;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#37096;&#32626;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a general task of jumping varying distances and heights for a quadrupedal robot in noisy environments, such as off of uneven terrain and with variable robot dynamics parameters. To accurately jump in such conditions, we propose a framework using deep reinforcement learning that leverages and augments the complex solution of nonlinear trajectory optimization for quadrupedal jumping. While the standalone optimization limits jumping to take-off from flat ground and requires accurate assumptions of robot dynamics, our proposed approach improves the robustness to allow jumping off of significantly uneven terrain with variable robot dynamical parameters and environmental conditions. Compared with walking and running, the realization of aggressive jumping on hardware necessitates accounting for the motors' torque-speed relationship as well as the robot's total power limits. By incorporating these constraints into our learning framework, we successfully deploy our po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#36827;&#34892;&#20102;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2010.11750</link><description>&lt;p&gt;
&#37327;&#21270;&#24322;&#26500;&#36716;&#31227;&#30340;&#31934;&#30830;&#39640;&#32500;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Precise High-Dimensional Asymptotics for Quantifying Heterogeneous Transfers. (arXiv:2010.11750v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.11750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#36827;&#34892;&#20102;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26102;&#20351;&#29992;&#26469;&#33258;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#26679;&#26412;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#20160;&#20040;&#26102;&#20505;&#23558;&#26469;&#33258;&#20004;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#21512;&#24182;&#27604;&#21333;&#29420;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26356;&#22909;&#65311;&#30452;&#35266;&#19978;&#65292;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#36716;&#31227;&#25928;&#24212;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#65292;&#22914;&#26679;&#26412;&#22823;&#23567;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#36825;&#31181;&#36716;&#31227;&#25928;&#24212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#27604;&#36739;&#32852;&#21512;&#23398;&#20064;&#21644;&#21333;&#20219;&#21153;&#23398;&#20064;&#20043;&#38388;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#19968;&#20010;&#20219;&#21153;&#26159;&#21542;&#27604;&#21478;&#19968;&#20010;&#20219;&#21153;&#20855;&#26377;&#27604;&#36739;&#20248;&#21183;&#21462;&#20915;&#20110;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30830;&#20999;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#31867;&#22411;&#12290;&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#19968;&#20123;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#30340;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#65292;&#24403;&#26679;&#26412;&#22823;&#23567;&#19982;&#29305;&#24449;&#32500;&#24230;&#25104;&#27604;&#20363;&#22686;&#21152;&#26102;&#65292;&#22266;&#23450;&#27604;&#20363;&#12290;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#20197;&#26679;&#26412;&#22823;&#23567;&#30340;&#20989;&#25968;&#24418;&#24335;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of learning one task with samples from another task has received much interest recently. In this paper, we ask a fundamental question: when is combining data from two tasks better than learning one task alone? Intuitively, the transfer effect from one task to another task depends on dataset shifts such as sample sizes and covariance matrices. However, quantifying such a transfer effect is challenging since we need to compare the risks between joint learning and single-task learning, and the comparative advantage of one over the other depends on the exact kind of dataset shift between both tasks. This paper uses random matrix theory to tackle this challenge in a linear regression setting with two tasks. We give precise asymptotics about the excess risks of some commonly used estimators in the high-dimensional regime, when the sample sizes increase proportionally with the feature dimension at fixed ratios. The precise asymptotics is provided as a function of the sample sizes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36867;&#36920;&#35757;&#32451;GAN&#20013;&#30340;&#26497;&#38480;&#21608;&#26399;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#31163;&#24515;&#21152;&#36895;&#24230;&#31639;&#27861;&#65288;PCAA&#65289;&#21644;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#31639;&#27861;&#65288;Adam&#65289;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26497;&#38480;&#21608;&#26399;&#34892;&#20026;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2010.03322</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36867;&#36920;&#35757;&#32451;GAN&#20013;&#30340;&#26497;&#38480;&#21608;&#26399;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A method for escaping limit cycles in training GANs. (arXiv:2010.03322v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36867;&#36920;&#35757;&#32451;GAN&#20013;&#30340;&#26497;&#38480;&#21608;&#26399;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#31163;&#24515;&#21152;&#36895;&#24230;&#31639;&#27861;&#65288;PCAA&#65289;&#21644;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#31639;&#27861;&#65288;Adam&#65289;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26497;&#38480;&#21608;&#26399;&#34892;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;&#39044;&#27979;&#31163;&#24515;&#21152;&#36895;&#24230;&#31639;&#27861;&#65288;PCAA&#65289;&#65292;&#23545;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20013;&#26497;&#38480;&#21608;&#26399;&#34892;&#20026;&#38382;&#39064;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;PCAA&#22312;&#19968;&#33324;&#21452;&#32447;&#24615;&#21338;&#24328;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#30340;&#19978;&#19979;&#30028;&#65292;&#22312;&#19978;&#30028;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;PCAA&#19982;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#31639;&#27861;&#65288;Adam&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;GANs&#30340;&#23454;&#29992;&#26041;&#27861;PCAA-Adam&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21452;&#32447;&#24615;&#21338;&#24328;&#12289;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#21644;CelebA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper mainly conducts further research to alleviate the issue of limit cycling behavior in training generative adversarial networks (GANs) through the proposed predictive centripetal acceleration algorithm (PCAA). Specifically, we first derive the upper and lower bounds on the last-iterate convergence rates of PCAA for the general bilinear game, with the upper bound notably improving upon previous results. Then, we combine PCAA with the adaptive moment estimation algorithm (Adam) to propose PCAA-Adam, a practical approach for training GANs. Finally, we validate the effectiveness of the proposed algorithm through experiments conducted on bilinear games, multivariate Gaussian distributions, and the CelebA dataset, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#27169;&#22411;&#21644;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#20803;&#23398;&#20064;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2003.03229</link><description>&lt;p&gt;
&#20855;&#26377;&#31867;&#20154;&#31867;&#26641;&#31361;&#28608;&#27963;&#30340;&#38750;&#32447;&#24615;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Non-linear Neurons with Human-like Apical Dendrite Activations. (arXiv:2003.03229v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.03229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#27169;&#22411;&#21644;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#20803;&#23398;&#20064;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#32447;&#24615;&#19981;&#21487;&#20998;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#24120;&#23558;&#31070;&#32463;&#20803;&#32452;&#32455;&#25104;&#33267;&#23569;&#21253;&#21547;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#21463;&#31070;&#32463;&#31185;&#23398;&#30340;&#19968;&#20123;&#26368;&#26032;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#20803;&#23398;&#20064;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26631;&#20934;&#31070;&#32463;&#20803;&#25509;&#19978;&#25105;&#20204;&#30340;&#26032;&#22411;&#26641;&#31361;&#28608;&#27963;&#20989;&#25968;&#65288;ADA&#65289;&#21487;&#20197;&#20197;100%&#30340;&#20934;&#30830;&#29575;&#23398;&#20064;XOR&#36923;&#36753;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21363;MOROCO&#12289;UTKFace&#12289;CREMA-D&#12289;Fashion-MNIST&#12289;Tiny ImageNet&#21644;ImageNet&#65292;&#32467;&#26524;&#26174;&#31034;ADA&#21644;&#28431;&#30005;ADA&#20989;&#25968;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65288;&#22914;&#19968;&#23618;&#25110;&#20004;&#23618;&#38544;&#34255;&#23618;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#19978;&#20248;&#20110;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#12289;&#28431;&#30005;ReLU&#12289;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#21644;Swish&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to classify linearly non-separable data, neurons are typically organized into multi-layer neural networks that are equipped with at least one hidden layer. Inspired by some recent discoveries in neuroscience, we propose a new model of artificial neuron along with a novel activation function enabling the learning of nonlinear decision boundaries using a single neuron. We show that a standard neuron followed by our novel apical dendrite activation (ADA) can learn the XOR logical function with 100% accuracy. Furthermore, we conduct experiments on six benchmark data sets from computer vision, signal processing and natural language processing, i.e. MOROCO, UTKFace, CREMA-D, Fashion-MNIST, Tiny ImageNet and ImageNet, showing that the ADA and the leaky ADA functions provide superior results to Rectified Linear Units (ReLU), leaky ReLU, RBF and Swish, for various neural network architectures, e.g. one-hidden-layer or two-hidden-layer multi-layer perceptrons (MLPs) and convolutional ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;B&#20301;&#37327;&#21270;&#30340;&#38750;&#21442;&#25968;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#23545;&#26679;&#26412;&#36827;&#34892;&#37327;&#21270;&#22788;&#29702;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;B&#36229;&#36807;&#38408;&#20540;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26679;&#26465;&#27169;&#22411;&#20013;&#33021;&#22815;&#36798;&#21040;&#32463;&#20856;&#26497;&#23567;&#26497;&#20540;&#29575;&#30340;&#27979;&#35797;&#27700;&#24179;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25299;&#23637;&#20102;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#21253;&#25324;&#38750;&#21442;&#25968;&#30452;&#32447;&#24615;&#26816;&#39564;&#21644;&#33258;&#36866;&#24212;&#38750;&#21442;&#25968;&#26816;&#39564;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/1901.08571</link><description>&lt;p&gt;
B&#20301;&#37327;&#21270;&#19979;&#30340;&#38750;&#21442;&#25968;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Inference under B-bits Quantization. (arXiv:1901.08571v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.08571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;B&#20301;&#37327;&#21270;&#30340;&#38750;&#21442;&#25968;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#23545;&#26679;&#26412;&#36827;&#34892;&#37327;&#21270;&#22788;&#29702;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;B&#36229;&#36807;&#38408;&#20540;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26679;&#26465;&#27169;&#22411;&#20013;&#33021;&#22815;&#36798;&#21040;&#32463;&#20856;&#26497;&#23567;&#26497;&#20540;&#29575;&#30340;&#27979;&#35797;&#27700;&#24179;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25299;&#23637;&#20102;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#21253;&#25324;&#38750;&#21442;&#25968;&#30452;&#32447;&#24615;&#26816;&#39564;&#21644;&#33258;&#36866;&#24212;&#38750;&#21442;&#25968;&#26816;&#39564;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#21495;/&#22270;&#20687;&#22788;&#29702;&#12289;&#21307;&#23398;&#22270;&#20687;&#23384;&#20648;&#12289;&#36965;&#24863;&#12289;&#20449;&#21495;&#20256;&#36755;&#31561;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#24120;&#24120;&#38656;&#35201;&#22522;&#20110;&#26377;&#25439;&#25110;&#19981;&#23436;&#25972;&#26679;&#26412;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#23545;&#26679;&#26412;&#36827;&#34892;B&#20301;&#37327;&#21270;&#30340;&#38750;&#21442;&#25968;&#26816;&#39564;&#31243;&#24207;&#12290;&#22312;&#19968;&#20123;&#28201;&#21644;&#25216;&#26415;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#26816;&#39564;&#21151;&#29575;&#22312;B&#22686;&#21152;&#26102;&#30340;&#21464;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#22914;&#26524;B&#36229;&#36807;&#26576;&#20010;&#38408;&#20540;&#65292;&#21017;&#25152;&#25552;&#20986;&#30340;&#38750;&#21442;&#25968;&#26816;&#39564;&#31243;&#24207;&#22312;&#26679;&#26465;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#32463;&#20856;&#26497;&#23567;&#26497;&#20540;&#29575;&#30340;&#27979;&#35797;&#65288;Shang&#21644;Cheng&#65292;2015&#65289;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#23558;&#29702;&#35770;&#30740;&#31350;&#25193;&#23637;&#21040;&#20102;&#38750;&#21442;&#25968;&#30452;&#32447;&#24615;&#26816;&#39564;&#21644;&#33258;&#36866;&#24212;&#38750;&#21442;&#25968;&#26816;&#39564;&#65292;&#25299;&#23637;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#26469;&#35777;&#26126;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical inference based on lossy or incomplete samples is often needed in research areas such as signal/image processing, medical image storage, remote sensing, signal transmission. In this paper, we propose a nonparametric testing procedure based on samples quantized to $B$ bits through a computationally efficient algorithm. Under mild technical conditions, we establish the asymptotic properties of the proposed test statistic and investigate how the testing power changes as $B$ increases. In particular, we show that if $B$ exceeds a certain threshold, the proposed nonparametric testing procedure achieves the classical minimax rate of testing (Shang and Cheng, 2015) for spline models. We further extend our theoretical investigations to a nonparametric linearity test and an adaptive nonparametric test, expanding the applicability of the proposed methods. Extensive simulation studies {together with a real-data analysis} are used to demonstrate the validity and effectiveness of the pr
&lt;/p&gt;</description></item></channel></rss>