<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.07644</link><description>&lt;p&gt;
&#35686;&#24789;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687; -- &#19982; GAN &#22312;&#35760;&#24518;&#33041;&#32959;&#30244;&#22270;&#20687;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images. (arXiv:2305.07644v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07644
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#32780;&#24320;&#21457;&#30340;&#65292;&#29616;&#22312;&#20063;&#34987;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#22312; GAN &#20043;&#21069;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20351;&#29992;&#20102;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#25351;&#26631;&#22914; FID &#21644; IS &#24182;&#19981;&#36866;&#21512;&#30830;&#23450;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#21482;&#26159;&#22797;&#21046;&#20102;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992; BRATS20 &#21644; BRATS21 &#25968;&#25454;&#38598;&#35757;&#32451; StyleGAN &#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#33041;&#32959;&#30244;&#22270;&#20687;&#65292;&#24182;&#27979;&#37327;&#21512;&#25104;&#22270;&#20687;&#19982;&#25152;&#26377;&#35757;&#32451;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#25968;&#25454;&#38598;&#12290;&#22914;&#26524;&#26368;&#32456;&#30446;&#26631;&#26159;&#20849;&#20139;&#21512;&#25104;&#30340;&#22270;&#20687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#25104;&#20687;&#26102;&#24212;&#35813;&#23567;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models were initially developed for text-to-image generation and are now being utilized to generate high quality synthetic images. Preceded by GANs, diffusion models have shown impressive results using various evaluation metrics. However, commonly used metrics such as FID and IS are not suitable for determining whether diffusion models are simply reproducing the training images. Here we train StyleGAN and diffusion models, using BRATS20 and BRATS21 datasets, to synthesize brain tumor images, and measure the correlation between the synthetic images and all training images. Our results show that diffusion models are much more likely to memorize the training images, especially for small datasets. Researchers should be careful when using diffusion models for medical imaging, if the final goal is to share the synthetic images.
&lt;/p&gt;</description></item><item><title>ASNR-MICCAI&#33041;&#32959;&#30244;&#20998;&#21106;&#25361;&#25112;2023&#23558;&#25552;&#20379;&#19968;&#20010;&#36866;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#39045;&#20869;&#33041;&#33180;&#30244;&#30340;&#26368;&#20808;&#36827;&#33258;&#21160;&#21270;&#39045;&#20869;&#33041;&#33180;&#30244;&#20998;&#21106;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.07642</link><description>&lt;p&gt;
ASNR-MICCAI&#33041;&#32959;&#30244;&#20998;&#21106;&#25361;&#25112;2023&#65306;&#39045;&#20869;&#33041;&#33180;&#30244;
&lt;/p&gt;
&lt;p&gt;
The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma. (arXiv:2305.07642v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07642
&lt;/p&gt;
&lt;p&gt;
ASNR-MICCAI&#33041;&#32959;&#30244;&#20998;&#21106;&#25361;&#25112;2023&#23558;&#25552;&#20379;&#19968;&#20010;&#36866;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#39045;&#20869;&#33041;&#33180;&#30244;&#30340;&#26368;&#20808;&#36827;&#33258;&#21160;&#21270;&#39045;&#20869;&#33041;&#33180;&#30244;&#20998;&#21106;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#33180;&#30244;&#26159;&#25104;&#20154;&#39045;&#20869;&#26368;&#24120;&#35265;&#30340;&#21407;&#21457;&#24615;&#32959;&#30244;&#65292;&#21487;&#33021;&#19982;&#37325;&#22823;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#26377;&#20851;&#12290;&#25918;&#23556;&#31185;&#21307;&#29983;&#12289;&#31070;&#32463;&#22806;&#31185;&#21307;&#29983;&#12289;&#31070;&#32463;&#32959;&#30244;&#23398;&#23478;&#21644;&#25918;&#23556;&#32959;&#30244;&#31185;&#21307;&#29983;&#20381;&#38752;&#22810;&#21442;&#25968;MRI&#65288;mpMRI&#65289;&#36827;&#34892;&#35786;&#26029;&#12289;&#27835;&#30103;&#35268;&#21010;&#21644;&#38271;&#26399;&#27835;&#30103;&#30417;&#27979;&#65307;&#28982;&#32780;&#65292;&#32570;&#20047;&#33258;&#21160;&#21270;&#12289;&#23458;&#35266;&#21270;&#21644;&#23450;&#37327;&#21270;&#30340;&#24037;&#20855;&#26469;&#23545;mpMRI&#20013;&#30340;&#33041;&#33180;&#30244;&#36827;&#34892;&#38750;&#20405;&#20837;&#24615;&#35780;&#20272;&#12290;BraTS&#33041;&#33180;&#30244;2023&#25361;&#25112;&#23558;&#25552;&#20379;&#19968;&#20010;&#31038;&#21306;&#26631;&#20934;&#21644;&#22522;&#20110;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#33041;&#33180;&#30244;mpMRI&#25968;&#25454;&#38598;&#30340;&#26368;&#20808;&#36827;&#33258;&#21160;&#21270;&#39045;&#20869;&#33041;&#33180;&#30244;&#20998;&#21106;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25361;&#25112;&#21442;&#36187;&#32773;&#23558;&#24320;&#21457;&#33258;&#21160;&#21270;&#20998;&#21106;&#27169;&#22411;&#65292;&#39044;&#27979;MRI&#19978;&#30340;&#19977;&#20010;&#19981;&#21516;&#30340;&#33041;&#33180;&#30244;&#20122;&#21306;&#22495;&#65292;&#21253;&#25324;&#22686;&#24378;&#32959;&#30244;&#12289;&#38750;&#22686;&#24378;&#32959;&#30244;&#26680;&#24515;&#21644;&#21608;&#22260;&#26080;&#22686;&#24378;T2/FLAIR&#39640;&#20449;&#21495;&#21306;&#12290;&#27169;&#22411;&#23558;&#20351;&#29992;&#26631;&#20934;&#21270;&#25351;&#26631;&#22312;&#21333;&#29420;&#30340;&#39564;&#35777;&#21644;&#20445;&#30041;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meningiomas are the most common primary intracranial tumor in adults and can be associated with significant morbidity and mortality. Radiologists, neurosurgeons, neuro-oncologists, and radiation oncologists rely on multiparametric MRI (mpMRI) for diagnosis, treatment planning, and longitudinal treatment monitoring; yet automated, objective, and quantitative tools for non-invasive assessment of meningiomas on mpMRI are lacking. The BraTS meningioma 2023 challenge will provide a community standard and benchmark for state-of-the-art automated intracranial meningioma segmentation models based on the largest expert annotated multilabel meningioma mpMRI dataset to date. Challenge competitors will develop automated segmentation models to predict three distinct meningioma sub-regions on MRI including enhancing tumor, non-enhancing tumor core, and surrounding nonenhancing T2/FLAIR hyperintensity. Models will be evaluated on separate validation and held-out test datasets using standardized metri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#25216;&#26415;&#21644;&#31070;&#32463;&#32676;&#32452;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22270;&#20687;&#23457;&#26680;&#24341;&#25806;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#22270;&#20687;&#23457;&#26680;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07639</link><description>&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#24863;&#30693;&#21644;&#32676;&#32452;&#26816;&#27979;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#22270;&#20687;&#20998;&#31867;&#21644;&#31163;&#32676;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Neural Network based Classification and Outlier Detection for Image Moderation using Compressed Sensing and Group Testing. (arXiv:2305.07639v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07639
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#25216;&#26415;&#21644;&#31070;&#32463;&#32676;&#32452;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22270;&#20687;&#23457;&#26680;&#24341;&#25806;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#22270;&#20687;&#23457;&#26680;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21033;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#23457;&#26680;&#24341;&#25806;&#65292;&#23545;&#19978;&#20256;&#30340;&#22270;&#29255;&#36827;&#34892;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#20869;&#23481;&#30340;&#20998;&#31867;&#12290;&#36825;&#26679;&#30340;&#23457;&#26680;&#24341;&#25806;&#24517;&#39035;&#22238;&#31572;&#22823;&#37327;&#26597;&#35810;&#24182;&#20855;&#26377;&#37325;&#35745;&#31639;&#25104;&#26412;&#65292;&#23613;&#31649;&#20855;&#26377;&#38382;&#39064;&#20869;&#23481;&#30340;&#23454;&#38469;&#22270;&#20687;&#25968;&#37327;&#36890;&#24120;&#21482;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#19968;&#37096;&#20998;&#12290;&#21463;&#31070;&#32463;&#32676;&#32452;&#27979;&#35797;&#30340;&#26368;&#26032;&#24037;&#20316;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#25216;&#26415;&#26469;&#20943;&#23569;&#27492;&#31867;&#24341;&#25806;&#30340;&#25972;&#20307;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#37327;&#30697;&#38453;&#27744;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QMPNN&#65289;&#65292;&#20854;&#36755;&#20837;&#20026;n&#20010;&#22270;&#20687;&#21644;&#19968;&#20010;m&#215;n&#30340;&#20108;&#20803;&#27719;&#24635;&#30697;&#38453;&#65292;&#20854;&#20013;m&lt;n&#65292;&#34892;&#34920;&#31034;m&#20010;&#22270;&#20687;&#27744;&#65292;&#21363;&#20174;n&#20010;&#22270;&#20687;&#20013;&#36873;&#25321;r&#20010;&#22270;&#20687;&#30340;&#36873;&#25321;&#12290; QMPNN&#26377;&#25928;&#22320;&#36755;&#20986;&#35813;&#30697;&#38453;&#19982;&#34920;&#31034;&#27599;&#20010;&#22270;&#20687;&#26159;&#21542;&#23384;&#22312;&#38382;&#39064;&#30340;&#26410;&#30693;&#31232;&#30095;&#20108;&#36827;&#21046;&#21521;&#37327;&#30340;&#20056;&#31215;&#65292;&#21363;&#36755;&#20986;&#27599;&#20010;&#27744;&#20013;&#23384;&#22312;&#38382;&#39064;&#30340;&#22270;&#20687;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular social media platforms employ neural network based image moderation engines to classify images uploaded on them as having potentially objectionable content. Such moderation engines must answer a large number of queries with heavy computational cost, even though the actual number of images with objectionable content is usually a tiny fraction. Inspired by recent work on Neural Group Testing, we propose an approach which exploits this fact to reduce the overall computational cost of such engines using the technique of Compressed Sensing (CS). We present the quantitative matrix-pooled neural network (QMPNN), which takes as input $n$ images, and a $m \times n$ binary pooling matrix with $m &lt; n$, whose rows indicate $m$ pools of images i.e. selections of $r$ images out of $n$. The QMPNN efficiently outputs the product of this matrix with the unknown sparse binary vector indicating whether each image is objectionable or not, i.e. it outputs the number of objectionable images in each 
&lt;/p&gt;</description></item><item><title>Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.07637</link><description>&lt;p&gt;
Text2Cohort: &#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#23545;&#30284;&#30151;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07637
&lt;/p&gt;
&lt;p&gt;
Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;(IDC)&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#25968;&#25454;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24320;&#25918;&#33719;&#21462;&#30340;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#20419;&#36827;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#36136;&#65292;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#20197;&#36827;&#34892;&#38431;&#21015;&#21457;&#29616;&#21644;&#35775;&#38382;&#25104;&#20687;&#25968;&#25454;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#26174;&#33879;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Text2Cohort&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#23558;&#26597;&#35810;&#30340;&#21709;&#24212;&#36820;&#22238;&#32473;&#29992;&#25143;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26657;&#27491;&#20197;&#35299;&#20915;&#26597;&#35810;&#20013;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#36890;&#36807;&#23558;&#38169;&#35823;&#20256;&#22238;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#21644;&#26657;&#27491;&#12290;&#25105;&#20204;&#23545;50&#20010;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#20102;Text2Cohort&#35780;&#20272;&#65292;&#33539;&#22260;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#38431;&#21015;&#21457;&#29616;&#12290;&#32467;&#26524;&#26597;&#35810;&#21644;&#36755;&#20986;&#30001;&#20004;&#20301;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#39033;&#30446;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#38646;&#26679;&#26412;&#39033;&#30446;&#25512;&#33616;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;&#22235;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#36866;&#24212;&#23618;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#27169;&#22411;&#24494;&#35843;&#21040;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.07633</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20219;&#21153;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#22522;&#20110;&#39033;&#30446;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training. (arXiv:2305.07633v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#39033;&#30446;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#38646;&#26679;&#26412;&#39033;&#30446;&#25512;&#33616;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;&#22235;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#36866;&#24212;&#23618;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#27169;&#22411;&#24494;&#35843;&#21040;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#38646;&#26679;&#26412;&#39033;&#30446;&#65288;&#21363;&#22312;&#35757;&#32451;&#38454;&#27573;&#27809;&#26377;&#19982;&#29992;&#25143;&#36827;&#34892;&#36807;&#21382;&#21490;&#20114;&#21160;&#30340;&#39033;&#30446;&#65289;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#25552;&#21462;&#36890;&#29992;&#39033;&#30446;&#34920;&#31034;&#65292;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#39033;&#30446;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#20174;PLMs&#20013;&#25552;&#28860;&#20986;&#39033;&#30446;&#29305;&#24449;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#39033;&#30446;&#25512;&#33616;&#65288;ZSIR&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#39044;&#35757;&#32451;PKG&#30340;&#19977;&#20010;&#25361;&#25112;&#65292;&#21363;PKG&#20013;&#30340;&#22810;&#31867;&#22411;&#20851;&#31995;&#65292;&#39033;&#30446;&#36890;&#29992;&#20449;&#24687;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#20197;&#21450;&#20174;PKG&#21040;&#19979;&#28216;ZSIR&#20219;&#21153;&#30340;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#22235;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#36866;&#24212;&#65288;ToA&#65289;&#23618;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#23545;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#24471;ToA&#23618;&#36866;&#24212;&#20110;ZSIR&#20219;&#21153;&#12290;&#22312;18&#20010;&#24066;&#22330;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage. Though recent works extract universal item representation via pre-trained language models (PLMs), they ignore the crucial item relationships. This paper presents a novel paradigm for the Zero-Shot Item-based Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph (PKG) to refine the item features from PLMs. We identify three challenges for pre-training PKG, which are multi-type relations in PKG, semantic divergence between item generic information and relations and domain discrepancy from PKG to downstream ZSIR task. We address the challenges by proposing four pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover, this paper discusses how to fine-tune the model on new recommendation task such that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18 markets dataset ar
&lt;/p&gt;</description></item><item><title>Meta Omnium&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#65292;&#20351;&#24471;&#23398;&#26415;&#30028;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#22810;&#39033;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#65292;&#26469;&#35780;&#20272;&#20803;&#23398;&#20064;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.07625</link><description>&lt;p&gt;
Meta Omnium: &#19968;&#39033;&#36890;&#29992;&#23398;&#20064;-&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Meta Omnium: A Benchmark for General-Purpose Learning-to-Learn. (arXiv:2305.07625v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07625
&lt;/p&gt;
&lt;p&gt;
Meta Omnium&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#65292;&#20351;&#24471;&#23398;&#26415;&#30028;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#22810;&#39033;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#65292;&#26469;&#35780;&#20272;&#20803;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#20197;&#21450;&#20854;&#20182;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#65292;&#21516;&#26102;&#20063;&#36234;&#26469;&#36234;&#24212;&#29992;&#20110;&#20854;&#20182;&#35270;&#35273;&#20219;&#21153;&#65292;&#22914;&#23039;&#24577;&#20272;&#35745;&#21644;&#23494;&#38598;&#39044;&#27979;&#12290;&#36825;&#33258;&#28982;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#27867;&#21270;&#21040;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#31867;&#22411;&#20043;&#38388;&#65311;&#20026;&#20102;&#24110;&#21161;&#23398;&#26415;&#30028;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Meta Omnium&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#23427;&#28085;&#30422;&#20102;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#65292;&#21253;&#25324;&#35782;&#21035;&#12289;&#20851;&#38190;&#28857;&#23450;&#20301;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#22238;&#24402;&#12290;&#25105;&#20204;&#35797;&#39564;&#20102;&#21463;&#27426;&#36814;&#30340;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#22522;&#32447;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#27867;&#21270;&#21040;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#23427;&#20204;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;Meta Omnium&#20351;&#24471;&#23398;&#26415;&#30028;&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#22810;&#39033;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#27604;&#20197;&#21069;&#26356;&#21152;&#24191;&#27867;&#65292;&#21516;&#26102;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#19981;&#21516;&#35270;&#35273;&#24212;&#29992;&#20013;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#35780;&#20272;&#20803;&#23398;&#20064;&#32773;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning and other approaches to few-shot learning are widely studied for image recognition, and are increasingly applied to other vision tasks such as pose estimation and dense prediction. This naturally raises the question of whether there is any few-shot meta-learning algorithm capable of generalizing across these diverse task types? To support the community in answering this question, we introduce Meta Omnium, a dataset-of-datasets spanning multiple vision tasks including recognition, keypoint localization, semantic segmentation and regression. We experiment with popular few-shot meta-learning baselines and analyze their ability to generalize across tasks and to transfer knowledge between them. Meta Omnium enables meta-learning researchers to evaluate model generalization to a much wider array of tasks than previously possible, and provides a single framework for evaluating meta-learners across a wide suite of vision applications in a consistent manner.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30005;&#23481;&#20256;&#24863;&#22120;&#20449;&#21495;&#35782;&#21035;&#25163;&#21183;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#23454;&#29616;&#65292;&#24182;&#22312;500&#27627;&#31186;&#20869;&#33021;&#22815;&#30830;&#23450;5&#20010;&#25163;&#25351;&#30340;&#19977;&#20010;&#29305;&#24449;&#65292;&#23558;&#25163;&#21183;&#30340;&#35782;&#21035;&#25928;&#29575;&#25552;&#39640;&#20102;&#12290;</title><link>http://arxiv.org/abs/2305.07624</link><description>&lt;p&gt;
&#30005;&#23481;&#20256;&#24863;&#35774;&#22791;&#30340;&#25935;&#25463;&#25163;&#21183;&#35782;&#21035;&#65306;&#22312;&#24037;&#20316;&#20013;&#30340;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Agile gesture recognition for capacitive sensing devices: adapting on-the-job. (arXiv:2305.07624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30005;&#23481;&#20256;&#24863;&#22120;&#20449;&#21495;&#35782;&#21035;&#25163;&#21183;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#23454;&#29616;&#65292;&#24182;&#22312;500&#27627;&#31186;&#20869;&#33021;&#22815;&#30830;&#23450;5&#20010;&#25163;&#25351;&#30340;&#19977;&#20010;&#29305;&#24449;&#65292;&#23558;&#25163;&#21183;&#30340;&#35782;&#21035;&#25928;&#29575;&#25552;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25163;&#21183;&#35782;&#21035;&#26159;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20960;&#21313;&#24180;&#26469;&#30340;&#20851;&#27880;&#28966;&#28857;&#12290;&#20256;&#32479;&#19978;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#22260;&#32469;&#20551;&#35774;&#29992;&#25143;&#25163;&#37096;&#22270;&#20687;&#27969;&#30340;&#21487;&#29992;&#24615;&#23637;&#24320;&#12290;&#36825;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#22522;&#20110;&#30456;&#26426;&#30340;&#35774;&#22791;&#30340;&#26222;&#21450;&#21644;&#22270;&#20687;&#25968;&#25454;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#38656;&#27714;&#38656;&#35201;&#25163;&#21183;&#35782;&#21035;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#32780;&#19981;&#26159;&#39640;&#32500;&#24230;&#30340;&#36755;&#20837;(&#22914;&#25163;&#37096;&#22270;&#20687;)&#26469;&#23454;&#29616;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#21644;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#23884;&#20837;&#21040;etee&#25163;&#26564;&#25511;&#21046;&#22120;&#20013;&#30340;&#30005;&#23481;&#20256;&#24863;&#22120;&#30340;&#20449;&#21495;&#12290;&#35813;&#25511;&#21046;&#22120;&#20174;&#20329;&#25140;&#32773;&#30340;&#20116;&#20010;&#25163;&#25351;&#20013;&#20135;&#29983;&#23454;&#26102;&#20449;&#21495;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20998;&#26512;&#26102;&#24207;&#20449;&#21495;&#65292;&#24182;&#30830;&#23450;&#21487;&#20197;&#22312;500&#27627;&#31186;&#20869;&#34920;&#31034;5&#20010;&#25163;&#25351;&#30340;&#19977;&#20010;&#29305;&#24449;&#12290;&#20998;&#26512;&#30001;&#20004;&#20010;&#38454;&#27573;&#30340;&#22521;&#35757;&#31574;&#30053;&#32452;&#25104;&#65292;&#21253;&#25324;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#36827;&#34892;&#30340;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
Automated hand gesture recognition has been a focus of the AI community for decades. Traditionally, work in this domain revolved largely around scenarios assuming the availability of the flow of images of the user hands. This has partly been due to the prevalence of camera-based devices and the wide availability of image data. However, there is growing demand for gesture recognition technology that can be implemented on low-power devices using limited sensor data instead of high-dimensional inputs like hand images. In this work, we demonstrate a hand gesture recognition system and method that uses signals from capacitive sensors embedded into the etee hand controller. The controller generates real-time signals from each of the wearer five fingers. We use a machine learning technique to analyse the time series signals and identify three features that can represent 5 fingers within 500 ms. The analysis is composed of a two stage training strategy, including dimension reduction through pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#30830;&#23450;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#35782;&#21035;&#20998;&#24067;&#22806;&#30340;&#27979;&#35797;&#26679;&#26412;&#24182;&#25351;&#23548;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation for Deep Learning Image Reconstruction using a Local Lipschitz Metric. (arXiv:2305.07618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#30830;&#23450;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#35782;&#21035;&#20998;&#24067;&#22806;&#30340;&#27979;&#35797;&#26679;&#26412;&#24182;&#25351;&#23548;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#19982;&#25104;&#20687;&#30456;&#20851;&#30340;&#21453;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22270;&#20687;&#37325;&#24314;&#12290;&#22312;&#27169;&#22411;&#37096;&#32626;&#26102;&#65292;&#21487;&#33021;&#20250;&#36935;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#24046;&#24322;&#36739;&#22823;&#30340;&#36755;&#20837;&#20998;&#24067;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#25968;&#25454;&#20559;&#24046;&#25110;&#28418;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20174;&#21333;&#20010;&#35757;&#32451;&#27169;&#22411;&#20013;&#30830;&#23450;&#30340;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#22270;&#20687;&#37325;&#24314;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;Lipschitz&#20540;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#21333;&#35843;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#27492;&#26041;&#27861;&#25552;&#20379;&#30830;&#23450;&#26159;&#21542;&#36866;&#21512;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#30340;&#38408;&#20540;&#12290;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21487;&#29992;&#20110;&#35782;&#21035;&#20998;&#24067;&#22806;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#20851;&#32852;&#20851;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#65292;&#24182;&#25351;&#23548;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#20013;&#29305;&#21035;&#37325;&#35201;&#30340;&#26159;&#65292;&#37327;&#21270;&#23398;&#20064;&#37325;&#26500;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#20026;&#35786;&#26029;&#21644;&#27835;&#30103;&#21487;&#33021;&#20250;&#21463;&#21040;&#37325;&#24314;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of deep learning approaches for image reconstruction is of contemporary interest in radiology, especially for approaches that solve inverse problems associated with imaging. In deployment, these models may be exposed to input distributions that are widely shifted from training data, due in part to data biases or drifts. We propose a metric based on local Lipschitz determined from a single trained model that can be used to estimate the model uncertainty for image reconstructions. We demonstrate a monotonic relationship between the local Lipschitz value and Mean Absolute Error and show that this method can be used to provide a threshold that determines whether a given DL reconstruction approach was well suited to the task. Our uncertainty estimation method can be used to identify out-of-distribution test samples, relate information regarding epistemic uncertainties, and guide proper data augmentation. Quantifying uncertainty of learned reconstruction approaches is especially pert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;NP-hard&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#31163;&#25955;&#22270;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#21516;&#26102;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20855;&#26377;&#23545;&#39044;&#27979;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.07617</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;NP-hard&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#31163;&#25955;&#22270;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#21516;&#26102;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20855;&#26377;&#23545;&#39044;&#27979;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#31163;&#25955;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#28151;&#21512;&#30340;&#19981;&#26029;&#25506;&#32034;&#20013;&#65292;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#23545;&#31070;&#32463;&#32467;&#26500;&#20855;&#22791;&#20174;&#33258;&#28982;&#36755;&#20837;&#20013;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#31163;&#25955;&#25512;&#29702;&#25110;&#20248;&#21270;&#38382;&#39064;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32467;&#26500;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#23398;&#20064;&#34987;&#34920;&#31034;&#20026;&#31163;&#25955;&#22270;&#27169;&#22411;&#30340; NP-hard &#25512;&#29702;&#38382;&#39064;&#30340;&#32422;&#26463;&#21644;&#26631;&#20934;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#35299;&#20915;&#20102; Besag &#30340;&#20266;&#23545;&#25968;&#20284;&#28982;&#30340;&#20027;&#35201;&#38480;&#21046;&#20043;&#19968;&#65292;&#33021;&#22815;&#23398;&#20064;&#39640;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#36755;&#20837;&#20013;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915; NP-hard &#25512;&#29702;&#38382;&#39064;&#65292;&#22914;&#31526;&#21495;&#12289;&#35270;&#35273;&#25110;&#22810;&#35299;&#25968;&#25968;&#29420;&#38382;&#39064;&#65292;&#20197;&#21450;&#34507;&#30333;&#36136;&#35774;&#35745;&#38382;&#39064;&#30340;&#33021;&#37327;&#20248;&#21270;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#23545;&#39044;&#27979;&#30340; \textit{a posteriori} &#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag's pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and \textit{a posteriori} control over predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spider GAN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23547;&#25214;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21451;&#22909;&#37051;&#23621;&#26469;&#25552;&#39640;GAN&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#21152;&#36895;&#25910;&#25947;&#65292;&#21363;&#20351;&#26159;&#19981;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#20063;&#21487;&#20197;&#21457;&#29616;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.07613</link><description>&lt;p&gt;
Spider GAN:&#21033;&#29992;&#21451;&#22909;&#37051;&#23621;&#21152;&#36895;GAN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training. (arXiv:2305.07613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spider GAN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23547;&#25214;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21451;&#22909;&#37051;&#23621;&#26469;&#25552;&#39640;GAN&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#21152;&#36895;&#25910;&#25947;&#65292;&#21363;&#20351;&#26159;&#19981;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#20063;&#21487;&#20197;&#21457;&#29616;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GAN&#30340;&#35757;&#32451;&#26159;&#20010;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Spider GAN&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#32467;&#26500;&#30340;&#29305;&#28857;&#20248;&#21270;&#29983;&#25104;&#22120;&#30340;&#36716;&#25442;&#65292;&#36890;&#36807;&#23450;&#20041;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#24335;&#65292;&#21363;&#26377;&#31526;&#21495;&#21551;&#21160;&#36317;&#31163;&#65288;SID&#65289;&#65292;&#20351;&#20854;&#26356;&#39640;&#25928;&#22320;&#23547;&#25214;&#21451;&#22909;&#37051;&#23621;&#65292;&#32467;&#26524;&#23548;&#33268;&#26356;&#24555;&#30340;&#25910;&#25947;&#65292;&#21363;&#20351;&#22312;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#20063;&#21487;&#20197;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training Generative adversarial networks (GANs) stably is a challenging task. The generator in GANs transform noise vectors, typically Gaussian distributed, into realistic data such as images. In this paper, we propose a novel approach for training GANs with images as inputs, but without enforcing any pairwise constraints. The intuition is that images are more structured than noise, which the generator can leverage to learn a more robust transformation. The process can be made efficient by identifying closely related datasets, or a ``friendly neighborhood'' of the target distribution, inspiring the moniker, Spider GAN. To define friendly neighborhoods leveraging proximity between datasets, we propose a new measure called the signed inception distance (SID), inspired by the polyharmonic kernel. We show that the Spider GAN formulation results in faster convergence, as the generator can discover correspondence even between seemingly unrelated datasets, for instance, between Tiny-ImageNet 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#37319;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38480;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NEOLITHIC&#30340;&#26032;&#22411;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#32553;&#23567;&#19979;&#38480;&#21644;&#29616;&#26377;&#31639;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.07612</link><description>&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#19979;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#19979;&#38480;&#21644;&#21152;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lower Bounds and Accelerated Algorithms in Distributed Stochastic Optimization with Communication Compression. (arXiv:2305.07612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37319;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38480;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NEOLITHIC&#30340;&#26032;&#22411;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#32553;&#23567;&#19979;&#38480;&#21644;&#29616;&#26377;&#31639;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#26159;&#20943;&#36731;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#20013;&#35745;&#31639;&#33410;&#28857;&#38388;&#20449;&#24687;&#20132;&#25442;&#37327;&#30340;&#37325;&#35201;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38480;&#65292;&#24182;&#20851;&#27880;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#21387;&#32553;&#22120;&#65306;&#26080;&#20559;&#21644;&#21387;&#32553;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#21387;&#32553;&#22120;&#33719;&#24471;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#20845;&#31181;&#19981;&#21516;&#35774;&#32622;&#65292;&#32467;&#21512;&#24378;&#20984;&#12289;&#19968;&#33324;&#20984;&#25110;&#38750;&#20984;&#20989;&#25968;&#65292;&#24182;&#29992;&#26080;&#20559;&#25110;&#21387;&#32553;&#22411;&#21387;&#32553;&#22120;&#24314;&#31435;&#20102;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#29575;&#19979;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NEOLITHIC&#30340;&#26032;&#22411;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#30456;&#27604;&#32463;&#20856;&#26041;&#27861;&#65292;&#32553;&#23567;&#20102;&#19979;&#38480;&#21644;&#29616;&#26377;&#31639;&#27861;&#30340;&#24046;&#36317;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#20013;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#33021;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication compression is an essential strategy for alleviating communication overhead by reducing the volume of information exchanged between computing nodes in large-scale distributed stochastic optimization. Although numerous algorithms with convergence guarantees have been obtained, the optimal performance limit under communication compression remains unclear.  In this paper, we investigate the performance limit of distributed stochastic optimization algorithms employing communication compression. We focus on two main types of compressors, unbiased and contractive, and address the best-possible convergence rates one can obtain with these compressors. We establish the lower bounds for the convergence rates of distributed stochastic optimization in six different settings, combining strongly-convex, generally-convex, or non-convex functions with unbiased or contractive compressor types. To bridge the gap between lower bounds and existing algorithms' rates, we propose NEOLITHIC, a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07598</link><description>&lt;p&gt;
RHINO&#65306;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#30340;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#30340;&#26059;&#36716;DETR
&lt;/p&gt;
&lt;p&gt;
RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;DINO&#30340;&#21457;&#24067;&#65292;&#19968;&#31181;DETR&#30340;&#21464;&#20307;&#65292;&#26816;&#27979;&#21464;&#21387;&#22120;&#27491;&#22312;&#36890;&#36807;&#20854;&#31471;&#21040;&#31471;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;&#30446;&#26631;&#26816;&#27979;&#22522;&#20934;&#20013;&#21047;&#26032;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#39044;&#35745;&#20174;&#20854;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#33719;&#24471;&#26356;&#22810;&#30340;&#22909;&#22788;&#65292;&#22914;&#28040;&#38500;NMS&#21644;&#19982;&#38170;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#20294;&#23578;&#26410;&#24443;&#24213;&#30740;&#31350;DETR&#22312;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20351;&#29992;DETR&#36827;&#34892;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#24182;&#19981;&#33021;&#20445;&#35777;&#19981;&#37325;&#22797;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25104;&#26412;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#21256;&#29273;&#21033;&#21305;&#37197;&#26469;&#36807;&#28388;&#20887;&#20313;&#30340;&#24102;&#22122;&#22768;&#30340;&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#26597;&#35810;&#23545;&#40784;&#26469;&#20445;&#25345;Transformer&#35299;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26059;&#36716;DETR&#21644;&#20854;&#20182;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a novel denoising strategy that uses Hungarian matching to filter redundant noised queries and query alignment to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#19982;&#20219;&#20309;&#21160;&#37327;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#36890;&#36807;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#27169;&#22411;&#24182;&#20351;&#29992;&#19979;&#38480;&#25130;&#26029;&#65292;&#20197;&#21450;&#21363;&#26102;&#20272;&#35745;&#26410;&#30693;&#19979;&#38480;&#65292;&#26469;&#36817;&#20284;&#26368;&#23567;&#21270;&#35813;&#27169;&#22411;&#20197;&#35745;&#31639;&#19979;&#19968;&#27493;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;SGDM&#21644;Adam&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.07583</link><description>&lt;p&gt;
MoMo: &#21160;&#37327;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;
&lt;/p&gt;
&lt;p&gt;
MoMo: Momentum Models for Adaptive Learning Rates. (arXiv:2305.07583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#19982;&#20219;&#20309;&#21160;&#37327;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#36890;&#36807;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#27169;&#22411;&#24182;&#20351;&#29992;&#19979;&#38480;&#25130;&#26029;&#65292;&#20197;&#21450;&#21363;&#26102;&#20272;&#35745;&#26410;&#30693;&#19979;&#38480;&#65292;&#26469;&#36817;&#20284;&#26368;&#23567;&#21270;&#35813;&#27169;&#22411;&#20197;&#35745;&#31639;&#19979;&#19968;&#27493;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;SGDM&#21644;Adam&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#19982;&#20219;&#20309;&#21160;&#37327;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26032;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MoMo&#21644;MoMo-Adam&#65292;&#23427;&#20204;&#26159;&#20855;&#26377;&#21160;&#37327;&#65288;SGDM&#65289;&#30340;SGD&#21644;Adam&#26041;&#27861;&#19982;&#25105;&#20204;&#30340;&#26032;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;MoMo&#26041;&#27861;&#26159;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#38543;&#26426;&#20248;&#21270;&#26469;&#28608;&#21457;&#30340;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#27599;&#27425;&#36845;&#20195;&#37319;&#26679;&#30340;&#25209;&#37327;&#25439;&#22833;&#21644;&#26799;&#24230;&#30340;&#21160;&#37327;&#20272;&#35745;&#26469;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21033;&#29992;&#20102;&#24050;&#30693;&#25439;&#22833;&#20989;&#25968;&#19979;&#38480;&#30340;&#25130;&#26029;&#26041;&#27861;&#12290;&#23454;&#38469;&#19978;&#65292;&#22823;&#22810;&#25968;&#25439;&#22833;&#37117;&#34987;&#19979;&#38480;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#36817;&#20284;&#26368;&#23567;&#21270;&#27492;&#27169;&#22411;&#20197;&#35745;&#31639;&#19979;&#19968;&#27493;&#12290;&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#19979;&#38480;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#21363;&#26102;&#19979;&#38480;&#20272;&#35745;&#65292;&#36825;&#20123;&#20272;&#35745;&#29992;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MoMo&#26041;&#27861;&#22312;MNIST&#12289;CIFAR10&#12289;CIFAR100&#21644;Imagenet32&#31561;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#20998;&#31867;&#35757;&#32451;&#20013;&#65292;&#30456;&#36739;&#20110;SGDM&#21644;Adam&#65292;&#22312;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new adaptive learning rates that can be used with any momentum method. To showcase our new learning rates we develop MoMo and MoMo-Adam, which are SGD with momentum (SGDM) and Adam together with our new adaptive learning rates. Our MoMo methods are motivated through model-based stochastic optimization, wherein we use momentum estimates of the batch losses and gradients sampled at each iteration to build a model of the loss function. Our model also makes use of any known lower bound of the loss function by using truncation. Indeed most losses are bounded below by zero. We then approximately minimize this model at each iteration to compute the next step. For losses with unknown lower bounds, we develop new on-the-fly estimates of the lower bound that we use in our model. Numerical experiments show that our MoMo methods improve over SGDM and Adam in terms of accuracy and robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR10, CIFAR100, Imagenet32, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GNN&#12290;</title><link>http://arxiv.org/abs/2305.07580</link><description>&lt;p&gt;
&#22522;&#20110;Fisher&#20449;&#24687;&#23884;&#20837;&#30340;&#33410;&#28857;&#21644;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fisher Information Embedding for Node and Graph Learning. (arXiv:2305.07580v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#20363;&#22914;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#65292;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#21644;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#27969;&#34892;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#26631;&#27880;&#25968;&#25454;&#65292;&#19988;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#23646;&#24615;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#31181;&#22810;&#37325;&#38598;&#21512;&#20869;&#33410;&#28857;&#21608;&#22260;&#23376;&#22270;&#30340;&#20998;&#23618;&#26680;&#20043;&#19978;&#65288;&#20363;&#22914;&#65292;&#37051;&#22495;&#65289;&#65292;&#24182;&#19988;&#27599;&#20010;&#26680;&#21033;&#29992;&#24179;&#28369;&#32479;&#35745;&#27969;&#24418;&#30340;&#20960;&#20309;&#26469;&#27604;&#36739;&#22810;&#37325;&#38598;&#21512;&#30340;&#25104;&#23545;&#24046;&#24322;&#65292;&#36890;&#36807;&#23558;&#22810;&#37325;&#38598;&#21512;&#8220;&#26144;&#23556;&#8221;&#21040;&#27969;&#24418;&#19978;&#12290;&#36890;&#36807;&#26174;&#24335;&#35745;&#31639;&#39640;&#26031;&#28151;&#21512;&#29289;&#27969;&#24418;&#20013;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#23548;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#26426;&#21046;&#36827;&#34892;&#37051;&#22495;&#32858;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#23884;&#20837;&#30340;&#27867;&#21270;&#21644;&#34920;&#36798;&#33021;&#21147;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#20026;&#26356;&#28145;&#20837;&#29702;&#35299;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GNN&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based graph neural networks (GNNs), such as graph attention networks (GATs), have become popular neural architectures for processing graph-structured data and learning node embeddings. Despite their empirical success, these models rely on labeled data and the theoretical properties of these models have yet to be fully understood. In this work, we propose a novel attention-based node embedding framework for graphs. Our framework builds upon a hierarchical kernel for multisets of subgraphs around nodes (e.g. neighborhoods) and each kernel leverages the geometry of a smooth statistical manifold to compare pairs of multisets, by "projecting" the multisets onto the manifold. By explicitly computing node embeddings with a manifold of Gaussian mixtures, our method leads to a new attention mechanism for neighborhood aggregation. We provide theoretical insights into genralizability and expressivity of our embeddings, contributing to a deeper understanding of attention-based GNNs. We p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#26032;&#39062;&#30340;Voronoi Loss&#20989;&#25968;&#26469;&#35299;&#20915;&#39640;&#26031;&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#38376;&#25511;&#32593;&#32476;&#19979;&#25552;&#20379;&#29702;&#35770;&#25910;&#25947;&#36895;&#29575;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.07572</link><description>&lt;p&gt;
&#39640;&#26031;&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts. (arXiv:2305.07572v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#26032;&#39062;&#30340;Voronoi Loss&#20989;&#25968;&#26469;&#35299;&#20915;&#39640;&#26031;&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#38376;&#25511;&#32593;&#32476;&#19979;&#25552;&#20379;&#29702;&#35770;&#25910;&#25947;&#36895;&#29575;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#22240;&#20854;&#22312;&#38598;&#25104;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#32780;&#34987;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36817;&#24180;&#26469;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#20998;&#26512;&#30340;&#22522;&#26412;&#26500;&#20214;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#26031;&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#34892;&#20026;&#30340;&#29702;&#35299;&#36824;&#19981;&#20805;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;Voronoi Loss&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#25910;&#25947;&#36895;&#29575;&#30340;&#35777;&#26126;&#65292;&#25581;&#31034;&#20102;&#22312;&#20004;&#31181;&#20998;&#31163;&#30340;&#38376;&#25511;&#32593;&#32476;&#19979;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originally introduced as a neural network for ensemble learning, mixture of experts (MoE) has recently become a fundamental building block of highly successful modern deep neural networks for heterogeneous data analysis in several applications, including those in machine learning, statistics, bioinformatics, economics, and medicine. Despite its popularity in practice, a satisfactory level of understanding of the convergence behavior of Gaussian-gated MoE parameter estimation is far from complete. The underlying reason for this challenge is the inclusion of covariates in the Gaussian gating and expert networks, which leads to their intrinsically complex interactions via partial differential equations with respect to their parameters. We address these issues by designing novel Voronoi loss functions to accurately capture heterogeneity in the maximum likelihood estimator (MLE) for resolving parameter estimation in these models. Our results reveal distinct behaviors of the MLE under two se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#25490;&#32451;&#21644;&#39044;&#26399;&#26469;&#35760;&#24518;&#26377;&#20851;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#24212;&#29992;&#33258;&#30417;&#30563;&#26426;&#21046;&#65292;&#36890;&#36807;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#23631;&#34109;&#24314;&#27169;&#20219;&#21153;&#35757;&#32451;&#65292;&#25104;&#21151;&#36890;&#36807;&#30701;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.07565</link><description>&lt;p&gt;
&#19968;&#31181;&#25903;&#25345;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#38382;&#31572;&#27969;&#24335;&#25968;&#25454;&#35760;&#24518;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information. (arXiv:2305.07565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#25490;&#32451;&#21644;&#39044;&#26399;&#26469;&#35760;&#24518;&#26377;&#20851;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#24212;&#29992;&#33258;&#30417;&#30563;&#26426;&#21046;&#65292;&#36890;&#36807;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#23631;&#34109;&#24314;&#27169;&#20219;&#21153;&#35757;&#32451;&#65292;&#25104;&#21151;&#36890;&#36807;&#30701;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38382;&#31572;&#26041;&#27861;&#24448;&#24448;&#20551;&#35774;&#36755;&#20837;&#20869;&#23481;&#65288;&#22914;&#25991;&#20214;&#25110;&#35270;&#39057;&#65289;&#24635;&#26159;&#21487;&#35775;&#38382;&#30340;&#65292;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#35760;&#24518;&#32593;&#32476;&#34987;&#24341;&#20837;&#26469;&#27169;&#20223;&#20154;&#31867;&#36880;&#27493;&#29702;&#35299;&#21644;&#21387;&#32553;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#23398;&#20064;&#22914;&#20309;&#36890;&#36807;&#25972;&#20010;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#38169;&#35823;&#26469;&#32500;&#25252;&#20869;&#23384;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#20855;&#26377;&#25552;&#39640;&#35760;&#24518;&#23481;&#37327;&#30340;&#26377;&#25928;&#26426;&#21046;&#65292;&#20363;&#22914;&#25490;&#32451;&#21644;&#39044;&#26399;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#32451;&#21644;&#39044;&#26399;&#26469;&#22788;&#29702;&#36755;&#20837;&#20197;&#35760;&#24518;&#26377;&#20851;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#35757;&#32451;&#26399;&#38388;&#36890;&#36807;&#38024;&#23545;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#23631;&#34109;&#24314;&#27169;&#20219;&#21153;&#36827;&#34892;&#33258;&#30417;&#30563;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;&#30701;&#24207;&#21015;&#65288;bAbI&#65289;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#39281;&#21644;&#38750;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;&#65288;SGELU&#12289;SSiLU&#21644;SMish&#65289;&#65292;&#23427;&#20204;&#30001;GELU&#12289;SiLU&#12289;Mish&#21450;ReLU&#30340;&#27491;&#37096;&#20998;&#32452;&#25104;&#65292;&#33021;&#22815;&#22312;CIFAR-100&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#20013;&#23637;&#29616;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07537</link><description>&lt;p&gt;
&#39281;&#21644;&#38750;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Saturated Non-Monotonic Activation Functions. (arXiv:2305.07537v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#39281;&#21644;&#38750;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;&#65288;SGELU&#12289;SSiLU&#21644;SMish&#65289;&#65292;&#23427;&#20204;&#30001;GELU&#12289;SiLU&#12289;Mish&#21450;ReLU&#30340;&#27491;&#37096;&#20998;&#32452;&#25104;&#65292;&#33021;&#22815;&#22312;CIFAR-100&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#20013;&#23637;&#29616;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;&#12289;&#28789;&#27963;&#24615;&#24378;&#30340;&#28608;&#27963;&#20989;&#25968;&#37117;&#26159;&#21333;&#35843;&#20989;&#25968;&#65292;&#20294;&#19968;&#20123;&#38750;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;&#27491;&#22312;&#34987;&#25506;&#32034;&#24182;&#23637;&#29616;&#20986;&#24456;&#26377;&#21069;&#26223;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;: SGELU&#12289;SSiLU&#21644;SMish&#12290;&#36825;&#20123;&#28608;&#27963;&#20989;&#25968;&#26159;&#30001;GELU&#12289;SiLU&#12289;Mish&#20197;&#21450;ReLU&#30340;&#27491;&#37096;&#20998;&#32452;&#25104;&#65292;&#24182;&#22312;CIFAR-100&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions are essential to deep learning networks. Popular and versatile activation functions are mostly monotonic functions, some non-monotonic activation functions are being explored and show promising performance. But by introducing non-monotonicity, they also alter the positive input, which is proved to be unnecessary by the success of ReLU and its variants. In this paper, we double down on the non-monotonic activation functions' development and propose the Saturated Gaussian Error Linear Units by combining the characteristics of ReLU and non-monotonic activation functions. We present three new activation functions built with our proposed method: SGELU, SSiLU, and SMish, which are composed of the negative portion of GELU, SiLU, and Mish, respectively, and ReLU's positive portion. The results of image classification experiments on CIFAR-100 indicate that our proposed activation functions are highly effective and outperform state-of-the-art baselines across multiple deep l
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;ML&#27169;&#22411;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07512</link><description>&lt;p&gt;
&#23398;&#20064;&#21435;&#23398;&#20064;&#65306;&#26426;&#22120;&#21435;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learn to Unlearn: A Survey on Machine Unlearning. (arXiv:2305.07512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;ML&#27169;&#22411;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#23454;&#29616;&#34987;&#36951;&#24536;&#26435;&#26159;&#35768;&#22810;&#25968;&#25454;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;&#26426;&#22120;&#21435;&#23398;&#20064;&#24050;&#25104;&#20026;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#30340;&#31616;&#35201;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#31934;&#30830;&#21644;&#36817;&#20284;&#26041;&#27861;&#12289;&#21487;&#33021;&#30340;&#25915;&#20987;&#20197;&#21450;&#39564;&#35777;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#27604;&#36739;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20351;&#29992;Deltagrad&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#36824;&#24378;&#35843;&#20102;&#25361;&#25112;&#65292;&#22914;&#38750;IID&#21024;&#38500;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#21644;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25104;&#20026;&#23547;&#27714;&#26426;&#22120;&#21435;&#23398;&#20064;&#36164;&#26009;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) models contain private information, and implementing the right to be forgotten is a challenging privacy issue in many data applications. Machine unlearning has emerged as an alternative to remove sensitive data from a trained model, but completely retraining ML models is often not feasible. This survey provides a concise appraisal of Machine Unlearning techniques, encompassing both exact and approximate methods, probable attacks, and verification approaches. The survey compares the merits and limitations each method and evaluates their performance using the Deltagrad exact machine unlearning method. The survey also highlights challenges like the pressing need for a robust model for non-IID deletion to mitigate fairness issues. Overall, the survey provides a thorough synopsis of machine unlearning techniques and applications, noting future research directions in this evolving field. The survey aims to be a valuable resource for researchers and practitioners seeking
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#20998;&#26512;&#20102;XAI&#39046;&#22495;&#20013;&#24212;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#30340;&#30740;&#31350;&#65292;&#20197;&#20415;&#35299;&#37322;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#38024;&#23545;&#30340;&#30142;&#30149;&#21253;&#25324;&#30284;&#30151;&#21644;COVID-19&#12290;</title><link>http://arxiv.org/abs/2305.07511</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence on Medical Images: A Survey. (arXiv:2305.07511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07511
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#20998;&#26512;&#20102;XAI&#39046;&#22495;&#20013;&#24212;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#30340;&#30740;&#31350;&#65292;&#20197;&#20415;&#35299;&#37322;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#38024;&#23545;&#30340;&#30142;&#30149;&#21253;&#25324;&#30284;&#30151;&#21644;COVID-19&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#20110;&#21307;&#30103;&#39046;&#22495;&#30340;&#30740;&#31350;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#12290;&#20026;&#20102;&#21521;&#21442;&#19982;&#21307;&#23398;&#26816;&#26597;&#30340;&#25152;&#26377;&#20154;&#35299;&#37322;&#36825;&#20123;&#32467;&#26524;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;&#26368;&#36817;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20063;&#31216;&#20026;XAI&#65292;&#26088;&#22312;&#35299;&#37322;&#36825;&#20123;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#20197;&#20415;&#36827;&#34892;&#25152;&#38656;&#35780;&#20272;&#12290;&#36825;&#39033;&#35843;&#26597;&#20998;&#26512;&#20102;XAI&#39046;&#22495;&#20013;&#38024;&#23545;&#21307;&#23398;&#35786;&#26029;&#30740;&#31350;&#30340;&#20960;&#39033;&#26368;&#26032;&#30740;&#31350;&#65292;&#20801;&#35768;&#26377;&#20851;&#22810;&#31181;&#19981;&#21516;&#30142;&#30149;&#65288;&#22914;&#30284;&#30151;&#21644;COVID-19&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, the number of works about deep learning applied to the medical field has increased enormously. The necessity of a rigorous assessment of these models is required to explain these results to all people involved in medical exams. A recent field in the machine learning area is explainable artificial intelligence, also known as XAI, which targets to explain the results of such black box models to permit the desired assessment. This survey analyses several recent studies in the XAI field applied to medical diagnosis research, allowing some explainability of the machine learning results in several different diseases, such as cancers and COVID-19.
&lt;/p&gt;</description></item><item><title>MolDiff &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#29983;&#25104;&#21407;&#23376;&#21644;&#38190;&#65292;&#24182;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#20445;&#25345;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#21407;&#23376;&#38190;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3D&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.07508</link><description>&lt;p&gt;
MolDiff&#65306;&#22312;3D&#20998;&#23376;&#25193;&#25955;&#29983;&#25104;&#20013;&#35299;&#20915;&#21407;&#23376;&#38190;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MolDiff: Addressing the Atom-Bond Inconsistency Problem in 3D Molecule Diffusion Generation. (arXiv:2305.07508v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07508
&lt;/p&gt;
&lt;p&gt;
MolDiff &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#29983;&#25104;&#21407;&#23376;&#21644;&#38190;&#65292;&#24182;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#20445;&#25345;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#21407;&#23376;&#38190;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3D&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;3D&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;&#20854;&#20013;&#22823;&#22810;&#25968;&#27169;&#22411;&#39318;&#20808;&#29983;&#25104;&#21407;&#23376;&#65292;&#28982;&#21518;&#22522;&#20110;&#29983;&#25104;&#30340;&#21407;&#23376;&#20197;&#19968;&#31181;&#21518;&#22788;&#29702;&#30340;&#26041;&#24335;&#28155;&#21152;&#21270;&#23398;&#38190;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#29983;&#25104;&#30340;&#21407;&#23376;&#20301;&#32622;&#26159;&#22312;&#19981;&#32771;&#34385;&#21487;&#33021;&#30340;&#38190;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#65292;&#25152;&#20197;&#36825;&#20123;&#21407;&#23376;&#21487;&#33021;&#27809;&#26377;&#30456;&#24212;&#30340;&#38190;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#23450;&#20041;&#20026;&#21407;&#23376;&#38190;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#22768;&#31216;&#36825;&#26159;&#30446;&#21069;&#29983;&#25104;&#19981;&#36924;&#30495;3D&#20998;&#23376;&#26041;&#27861;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;MolDiff&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#29983;&#25104;&#21407;&#23376;&#21644;&#38190;&#65292;&#24182;&#36890;&#36807;&#26174;&#24335;&#22320;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#20381;&#36182;&#24615;&#26469;&#20445;&#25345;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#20960;&#20309;&#21644;&#21270;&#23398;&#23646;&#24615;&#30456;&#20851;&#30340;&#26631;&#20934;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#29983;&#25104;&#20998;&#23376;&#30340;&#36136;&#37327;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;3D&#20998;&#23376;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have recently achieved superior performance in 3D molecule generation. Most of them first generate atoms and then add chemical bonds based on the generated atoms in a post-processing manner. However, there might be no corresponding bond solution for the temporally generated atoms as their locations are generated without considering potential bonds. We define this problem as the atom-bond inconsistency problem and claim it is the main reason for current approaches to generating unrealistic 3D molecules. To overcome this problem, we propose a new diffusion model called MolDiff which can generate atoms and bonds simultaneously while still maintaining their consistency by explicitly modeling the dependence between their relationships. We evaluated the generation ability of our proposed model and the quality of the generated molecules using criteria related to both geometry and chemical properties. The empirical studies showed that our model outperforms previous appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#31216;&#20026;&#26657;&#20934;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476; (CAB)&#65292;&#29992;&#20110;&#20849;&#21516;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26657;&#20934;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07504</link><description>&lt;p&gt;
&#26657;&#20934;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Calibration-Aware Bayesian Learning. (arXiv:2305.07504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#31216;&#20026;&#26657;&#20934;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476; (CAB)&#65292;&#29992;&#20110;&#20849;&#21516;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26657;&#20934;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#29616;&#20195;&#31995;&#32479;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25552;&#20379;&#20854;&#20915;&#31574;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#32622;&#20449;&#27700;&#24179;&#65288;&#20063;&#31216;&#20026;&#26657;&#20934;&#65289;&#30340;&#36136;&#37327;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#21253;&#25324;&#21521;&#35757;&#32451;&#25439;&#22833;&#28155;&#21152;&#22522;&#20110;&#25968;&#25454;&#30340;&#25110;&#22522;&#20110;&#25968;&#25454;&#26080;&#20851;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#22312;&#20256;&#32479;&#30340;&#39057;&#29575;&#27966;&#23398;&#20064;&#19978;&#26368;&#36817;&#24341;&#20837;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#24809;&#32602;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24230;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#26080;&#20851;&#30340;&#27491;&#21017;&#21270;&#22120;&#22312;&#36125;&#21494;&#26031;&#23398;&#20064;&#30340;&#26680;&#24515;&#65292;&#24378;&#21046;&#20351;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#21464;&#20998;&#20998;&#24067;&#26381;&#20174;&#20808;&#39564;&#23494;&#24230;&#12290;&#21069;&#19968;&#31181;&#26041;&#27861;&#26080;&#27861;&#37327;&#21270;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#21518;&#32773;&#21017;&#20005;&#37325;&#21463;&#21040;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#30340;&#24433;&#21709;&#12290;&#37492;&#20110;&#20004;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#31216;&#20026;&#26657;&#20934;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476; (CAB)&#65292;&#29992;&#20110;&#20849;&#21516;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26657;&#20934;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#28041;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#30456;&#20851;&#24809;&#32602;&#39033;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models, including modern systems like large language models, are well known to offer unreliable estimates of the uncertainty of their decisions. In order to improve the quality of the confidence levels, also known as calibration, of a model, common approaches entail the addition of either data-dependent or data-independent regularization terms to the training loss. Data-dependent regularizers have been recently introduced in the context of conventional frequentist learning to penalize deviations between confidence and accuracy. In contrast, data-independent regularizers are at the core of Bayesian learning, enforcing adherence of the variational distribution in the model parameter space to a prior density. The former approach is unable to quantify epistemic uncertainty, while the latter is severely affected by model misspecification. In light of the limitations of both methods, this paper proposes an integrated framework, referred to as calibration-aware Bayesian neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;OT&#38382;&#39064;&#30340;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#30340;&#65292;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.07500</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21487;&#23545;&#40784;&#30340;&#28508;&#22312;&#31354;&#38388;&#29992;&#20110;&#39640;&#25928;&#38381;&#21512;&#24418;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation. (arXiv:2305.07500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;OT&#38382;&#39064;&#30340;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#30340;&#65292;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20960;&#20309;&#24037;&#20855;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#23545;&#40784;&#27010;&#29575;&#27979;&#24230;&#65292;&#36981;&#24490;&#26368;&#23567;&#21162;&#21147;&#21407;&#21017;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#65292;OT&#30340;&#35768;&#22810;&#25104;&#21151;&#24212;&#29992;&#20043;&#19968;&#26159;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#30446;&#26631;&#26159;&#23558;&#20998;&#31867;&#22120;&#20174;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#26410;&#26631;&#35760;&#25110;&#31232;&#30095;&#26631;&#35760;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;OT&#30340;DA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30001;&#20223;&#23556;&#26144;&#23556;&#32473;&#20986;&#30340;OT&#38382;&#39064;&#30340;&#38381;&#24335;&#35299;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;&#35813;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Among many successful applications of OT in machine learning (ML), domain adaptation (DA) -- a field of study where the goal is to transfer a classifier from one labelled domain to another similar, yet different unlabelled or scarcely labelled domain -- has been historically among the most investigated ones. This success is due to the ability of OT to provide both a meaningful discrepancy measure to assess the similarity of two domains' distributions and a mapping that can project source domain data onto the target one. In this paper, we propose a principally new OT-based approach applied to DA that uses the closed-form solution of the OT problem given by an affine mapping and learns an embedding space for which this solution is optimal and computationally less complex. We show that our approach works in both homogeneous and heterogeneous DA settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38899;&#39057;&#20998;&#31867;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#34987;&#35757;&#32451;&#35774;&#22791;&#19978;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07499</link><description>&lt;p&gt;
&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#22686;&#24378;&#30340;&#35774;&#22791;&#40065;&#26834;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Device-Robust Acoustic Scene Classification via Impulse Response Augmentation. (arXiv:2305.07499v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38899;&#39057;&#20998;&#31867;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#34987;&#35757;&#32451;&#35774;&#22791;&#19978;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38899;&#39057;&#20998;&#31867;&#27169;&#22411;&#32780;&#35328;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#24405;&#38899;&#35774;&#22791;&#26159;&#20851;&#38190;&#24615;&#33021;&#22240;&#32032;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#40614;&#20811;&#39118;&#29305;&#24615;&#30001;&#20110;&#20854;&#19981;&#21516;&#30340;&#39057;&#29575;&#21709;&#24212;&#65292;&#20250;&#24341;&#20837;&#25968;&#23383;&#21270;&#38899;&#39057;&#20449;&#21495;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#22914;&#26524;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#32771;&#34385;&#27492;&#39046;&#22495;&#20559;&#31227;&#65292;&#37027;&#20040;&#24403;&#23427;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#35774;&#22791;&#35760;&#24405;&#38899;&#39057;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20005;&#37325;&#19979;&#38477;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#23569;&#25968;&#19981;&#21516;&#40614;&#20811;&#39118;&#19978;&#24405;&#21046;&#38899;&#39057;&#20449;&#21495;&#30340;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#20250;&#20351;&#27867;&#21270;&#21040;&#26410;&#34987;&#35757;&#32451;&#30340;&#35774;&#22791;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#24405;&#21046;&#35774;&#22791;&#33033;&#20914;&#21709;&#24212;(DIR)&#21367;&#31215;&#35757;&#32451;&#38598;&#20013;&#30340;&#38899;&#39057;&#20449;&#21495;&#65292;&#20174;&#32780;&#20154;&#24037;&#22686;&#21152;&#24405;&#38899;&#35774;&#22791;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20351;&#29992;CNN&#21644;&#38899;&#39057;&#20809;&#35889;&#21464;&#25442;&#36827;&#34892;Acoustic Scene Classification&#20219;&#21153;&#30340;DIR&#22686;&#24378;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;DIR&#22686;&#24378;&#23601;&#33021;&#25552;&#21319;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalize to a wide range of recording devices is a crucial performance factor for audio classification models. The characteristics of different types of microphones introduce distributional shifts in the digitized audio signals due to their varying frequency responses. If this domain shift is not taken into account during training, the model's performance could degrade severely when it is applied to signals recorded by unseen devices. In particular, training a model on audio signals recorded with a small number of different microphones can make generalization to unseen devices difficult. To tackle this problem, we convolve audio signals in the training set with pre-recorded device impulse responses (DIRs) to artificially increase the diversity of recording devices. We systematically study the effect of DIR augmentation on the task of Acoustic Scene Classification using CNNs and Audio Spectrogram Transformers. The results show that DIR augmentation in isolation performs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#24555;&#36895;&#30340;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22270;&#24211;&#25968;&#25454;&#36827;&#34892;&#37319;&#26679;&#22788;&#29702;&#65292;&#22312;&#20943;&#23569;&#25628;&#32034;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#23545;&#24322;&#24120;&#22270;&#20687;&#22914;&#38169;&#35823;&#26631;&#35760;&#12289;&#20302;&#36136;&#37327;&#21644;&#20449;&#24687;&#36739;&#23569;&#30340;&#22270;&#20687;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;5.4M&#32593;&#32476;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FNIR&#26041;&#38754;&#36798;&#21040;&#20102;0.0975&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#20026;0.3891&#12290;</title><link>http://arxiv.org/abs/2305.07495</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24211;&#37319;&#26679;&#30340;&#20154;&#33080;&#35782;&#21035;&#24555;&#36895;&#20934;&#30830;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gallery Sampling for Robust and Fast Face Identification. (arXiv:2305.07495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#24555;&#36895;&#30340;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22270;&#24211;&#25968;&#25454;&#36827;&#34892;&#37319;&#26679;&#22788;&#29702;&#65292;&#22312;&#20943;&#23569;&#25628;&#32034;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#23545;&#24322;&#24120;&#22270;&#20687;&#22914;&#38169;&#35823;&#26631;&#35760;&#12289;&#20302;&#36136;&#37327;&#21644;&#20449;&#24687;&#36739;&#23569;&#30340;&#22270;&#20687;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;5.4M&#32593;&#32476;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FNIR&#26041;&#38754;&#36798;&#21040;&#20102;0.0975&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#20026;0.3891&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26497;&#22823;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#23613;&#21487;&#33021;&#22810;&#30340;&#22270;&#20687;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26631;&#35782;&#25968;&#25454;&#21644;&#26816;&#26597;&#22823;&#37327;&#22270;&#20687;&#25968;&#25454;&#30340;&#36136;&#37327;&#26159;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22823;&#25968;&#25454;&#26102;&#19981;&#33021;&#36991;&#20813;&#38169;&#35823;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#19968;&#30452;&#35797;&#22270;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#22914;&#26524;&#38169;&#35823;&#20986;&#29616;&#22312;&#20154;&#33080;&#35782;&#21035;&#30340;&#22270;&#24211;&#25968;&#25454;&#20013;&#65292;&#20250;&#24102;&#26469;&#26356;&#20026;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22270;&#24211;&#25968;&#25454;&#37319;&#26679;&#26041;&#27861;&#65292;&#21253;&#25324;&#38169;&#35823;&#26631;&#35760;&#12289;&#20302;&#36136;&#37327;&#21644;&#20449;&#24687;&#36739;&#23569;&#30340;&#22270;&#20687;&#65292;&#24182;&#20943;&#23569;&#20102;&#25628;&#32034;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#37319;&#26679;-&#20462;&#21098;&#21644;&#37319;&#26679;-&#29983;&#25104;&#26041;&#27861;&#22312;5.4M&#20010;&#21517;&#20154;&#32593;&#32476;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#33080;&#35782;&#21035;&#24615;&#33021;&#12290;&#22312;FPIR=0.01&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FNIR&#26041;&#38754;&#36798;&#21040;&#20102;0.0975&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#21017;&#26174;&#31034;&#20026;0.3891&#12290;&#24179;&#22343;&#29305;&#24449;&#21521;&#37327;&#25968;&#37327;&#20943;&#23569;&#20102;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have been achieved brilliant results in face recognition. One of the important tasks to improve the performance is to collect and label images as many as possible. However, labeling identities and checking qualities of large image data are difficult task and mistakes cannot be avoided in processing large data. Previous works have been trying to deal with the problem only in training domain, however it can cause much serious problem if the mistakes are in gallery data of face identification. We proposed gallery data sampling methods which are robust to outliers including wrong labeled, low quality, and less-informative images and reduce searching time. The proposed sampling-by-pruning and sampling-by-generating methods significantly improved face identification performance on our 5.4M web image dataset of celebrities. The proposed method achieved 0.0975 in terms of FNIR at FPIR=0.01, while conventional method showed 0.3891. The average number of feature vectors for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#22768;&#28304;&#20998;&#31163;&#20219;&#21153;&#22522;&#20934;&#65292;&#24182;&#23558;&#27969;&#34892;&#30340;&#27169;&#22411;&#21450;&#20854;&#38598;&#25104;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20182;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#20998;&#31163;&#26041;&#27861;&#65292;&#22522;&#20110;&#36866;&#21512;&#29305;&#23450;&#38899;&#36712;&#30340;&#19981;&#21516;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#22312;2023&#24180;&#38899;&#20048;&#20998;&#31163;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#25104;&#32489;&#65292;&#24182;&#24320;&#28304;&#20102;&#20195;&#30721;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07489</link><description>&lt;p&gt;
&#22768;&#38899;&#25286;&#20998;&#20219;&#21153;&#30340;&#22522;&#20934;&#21644;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Benchmarks and leaderboards for sound demixing tasks. (arXiv:2305.07489v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#22768;&#28304;&#20998;&#31163;&#20219;&#21153;&#22522;&#20934;&#65292;&#24182;&#23558;&#27969;&#34892;&#30340;&#27169;&#22411;&#21450;&#20854;&#38598;&#25104;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20182;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#20998;&#31163;&#26041;&#27861;&#65292;&#22522;&#20110;&#36866;&#21512;&#29305;&#23450;&#38899;&#36712;&#30340;&#19981;&#21516;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#22312;2023&#24180;&#38899;&#20048;&#20998;&#31163;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#25104;&#32489;&#65292;&#24182;&#24320;&#28304;&#20102;&#20195;&#30721;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#25286;&#20998;&#26159;&#23558;&#32473;&#23450;&#30340;&#21333;&#38899;&#39057;&#20449;&#21495;&#20998;&#31163;&#25104;&#32452;&#25104;&#37096;&#20998;&#65288;&#20363;&#22914;&#40723;&#12289;&#20302;&#38899;&#21644;&#20154;&#22768;&#31561;&#65289;&#19982;&#20854;&#20182;&#20276;&#22863;&#38899;&#20048;&#20998;&#31163;&#30340;&#20219;&#21153;&#12290;&#28304;&#20998;&#31163;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#21313;&#20998;&#26377;&#29992;&#65292;&#21253;&#25324;&#23089;&#20048;&#21644;&#21161;&#21548;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22768;&#28304;&#20998;&#31163;&#20219;&#21153;&#22522;&#20934;&#65292;&#24182;&#27604;&#36739;&#20102;&#27969;&#34892;&#30340;&#22768;&#38899;&#25286;&#20998;&#27169;&#22411;&#21450;&#20854;&#38598;&#25104;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#25490;&#34892;&#27036; https://mvsep.com/quality_checker/&#65292;&#20197;&#23545;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21487;&#20379;&#19979;&#36733;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#20998;&#31163;&#26041;&#27861;&#65292;&#22522;&#20110;&#36866;&#21512;&#29305;&#23450;&#38899;&#36712;&#30340;&#19981;&#21516;&#27169;&#22411;&#30340;&#38598;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;2023&#38899;&#20048;&#20998;&#31163;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#25104;&#32489;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#26041;&#27861;&#22312;GitHub&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music demixing is the task of separating different tracks from the given single audio signal into components, such as drums, bass, and vocals from the rest of the accompaniment. Separation of sources is useful for a range of areas, including entertainment and hearing aids. In this paper, we introduce two new benchmarks for the sound source separation tasks and compare popular models for sound demixing, as well as their ensembles, on these benchmarks. For the models' assessments, we provide the leaderboard at https://mvsep.com/quality_checker/, giving a comparison for a range of models. The new benchmark datasets are available for download. We also develop a novel approach for audio separation, based on the ensembling of different models that are suited best for the particular stem. The proposed solution was evaluated in the context of the Music Demixing Challenge 2023 and achieved top results in different tracks of the challenge. The code and the approach are open-sourced on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#33258;&#21160;&#39550;&#39542;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#19981;&#21487;&#38752;&#24615;&#65292;&#20197;&#20445;&#25252;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#21644;&#38480;&#21046;&#31574;&#30053;&#30340;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07487</link><description>&lt;p&gt;
&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#30340;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#36793;&#30028;&#30830;&#23450; (arXiv:2305.07487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Identify, Estimate and Bound the Uncertainty of Reinforcement Learning for Autonomous Driving. (arXiv:2305.07487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#33258;&#21160;&#39550;&#39542;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#19981;&#21487;&#38752;&#24615;&#65292;&#20197;&#20445;&#25252;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#21644;&#38480;&#21046;&#31574;&#30053;&#30340;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#24320;&#21457;&#26356;&#26234;&#33021;&#21270;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;(AVs)&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;AVs&#19978;&#30340;&#20856;&#22411;DRL&#24212;&#29992;&#26159;&#35757;&#32451;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#20915;&#31574;&#22833;&#35823;&#65292;&#20351;&#36825;&#20123;AVs&#19981;&#21487;&#38752;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#21644;&#20445;&#25252;DRL&#39550;&#39542;&#31574;&#30053;&#30340;&#19981;&#21487;&#38752;&#20915;&#31574;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#20272;&#35745;&#21644;&#38480;&#21046;&#31574;&#30053;&#30340;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#25110;&#32593;&#32476;&#25311;&#21512;&#35823;&#24046;&#23548;&#33268;&#30340;&#28508;&#22312;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#38480;&#21046;&#19981;&#30830;&#23450;&#24615;&#65292;DRL&#27169;&#22411;&#30340;&#24615;&#33021;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#31574;&#30053;&#12290;&#30001;&#19981;&#36275;&#30340;&#25968;&#25454;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#29992;&#33258;&#21161;&#27861;&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#38598;&#25104;&#32593;&#32476;&#20272;&#35745;&#30001;&#32593;&#32476;&#25311;&#21512;&#35823;&#24046;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#23558;&#22522;&#32447;&#31574;&#30053;&#28155;&#21152;&#20026;&#24615;&#33021;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has emerged as a promising approach for developing more intelligent autonomous vehicles (AVs). A typical DRL application on AVs is to train a neural network-based driving policy. However, the black-box nature of neural networks can result in unpredictable decision failures, making such AVs unreliable. To this end, this work proposes a method to identify and protect unreliable decisions of a DRL driving policy. The basic idea is to estimate and constrain the policy's performance uncertainty, which quantifies potential performance drop due to insufficient training data or network fitting errors. By constraining the uncertainty, the DRL model's performance is always greater than that of a baseline policy. The uncertainty caused by insufficient data is estimated by the bootstrapped method. Then, the uncertainty caused by the network fitting error is estimated using an ensemble network. Finally, a baseline policy is added as the performance lower bound to a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#26631;&#31614;&#22797;&#26434;&#24230;&#24182;&#23454;&#29616;&#32039;&#23494;&#30340;&#26368;&#20248;&#36817;&#20284;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.07486</link><description>&lt;p&gt;
&#20943;&#23569;&#32039;$\ell_2$&#22238;&#24402;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reduced Label Complexity For Tight $\ell_2$ Regression. (arXiv:2305.07486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07486
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#26631;&#31614;&#22797;&#26434;&#24230;&#24182;&#23454;&#29616;&#32039;&#23494;&#30340;&#26368;&#20248;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#25968;&#25454;${\rm X}\in\mathbb{R}^{n\times d}$&#21644;&#26631;&#31614;$\mathbf{y}\in\mathbb{R}^{n}$&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;$\mathbf{w}\in\mathbb{R}^d$&#65292;&#20351;$\Vert{\rm X}\mathbf{w}-\mathbf{y}\Vert^2$&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#31639;&#27861;&#65292;&#23427;&#26080;&#35270;$\mathbf{y}$&#65292;&#33293;&#24323;$n/(d+\sqrt{n})$&#20010;&#25968;&#25454;&#28857;&#65292;&#24182;&#22312;&#26399;&#26395;&#19979;&#26159;$(1+d/n)$&#26368;&#20248;&#30340;&#36817;&#20284;&#20540;&#12290;&#20854;&#21160;&#26426;&#26159;&#22312;&#20943;&#23569;&#26631;&#31614;&#22797;&#26434;&#31243;&#24230;&#65288;&#25152;&#38656;&#25581;&#31034;&#30340;&#26631;&#31614;&#25968;&#65289;&#30340;&#21516;&#26102;&#23454;&#29616;&#32039;&#23494;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;$\Omega(\sqrt{n})$&#20943;&#23569;&#26631;&#31614;&#22797;&#26434;&#24230;&#12290;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#32039;&#23494;&#30340;$(1+d/n)$-&#36817;&#20284;&#26469;&#38477;&#20302;&#26631;&#31614;&#22797;&#26434;&#24230;$\Omega(n)$&#65311;
&lt;/p&gt;
&lt;p&gt;
Given data ${\rm X}\in\mathbb{R}^{n\times d}$ and labels $\mathbf{y}\in\mathbb{R}^{n}$ the goal is find $\mathbf{w}\in\mathbb{R}^d$ to minimize $\Vert{\rm X}\mathbf{w}-\mathbf{y}\Vert^2$. We give a polynomial algorithm that, \emph{oblivious to $\mathbf{y}$}, throws out $n/(d+\sqrt{n})$ data points and is a $(1+d/n)$-approximation to optimal in expectation. The motivation is tight approximation with reduced label complexity (number of labels revealed). We reduce label complexity by $\Omega(\sqrt{n})$. Open question: Can label complexity be reduced by $\Omega(n)$ with tight $(1+d/n)$-approximation?
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#65292;&#20351;&#29992;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20998;&#21035;&#26356;&#26032;&#27169;&#22411;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#21442;&#25968;&#12290;&#27492;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#39640;&#25928;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07484</link><description>&lt;p&gt;
&#20998;&#31163;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Learning Under A Separable Stochastic Approximation Framework. (arXiv:2305.07484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#65292;&#20351;&#29992;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20998;&#21035;&#26356;&#26032;&#27169;&#22411;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#21442;&#25968;&#12290;&#27492;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#39640;&#25928;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#31163;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#19968;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#30340;&#37325;&#28857;&#22312;&#20110;&#35266;&#23519;&#27169;&#22411;&#20013;&#26576;&#20123;&#21442;&#25968;&#27604;&#20854;&#20182;&#21442;&#25968;&#26356;&#23481;&#26131;&#20248;&#21270;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#19968;&#31867;&#32447;&#24615;&#21442;&#25968;&#36739;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#65288;RLS&#65289;&#31639;&#27861;&#26469;&#26356;&#26032;&#32447;&#24615;&#21442;&#25968;&#65292;&#28982;&#21518;&#22522;&#20110;&#26356;&#26032;&#21518;&#30340;&#32447;&#24615;&#21442;&#25968;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#26469;&#26356;&#26032;&#38750;&#32447;&#24615;&#21442;&#25968;&#12290;&#36825;&#20010;&#31639;&#27861;&#21487;&#20197;&#29702;&#35299;&#20026;&#22359;&#22352;&#26631;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#38543;&#26426;&#36924;&#36817;&#29256;&#26412;&#65292;&#22312;&#36825;&#20010;&#29256;&#26412;&#20013;&#65292;&#20854;&#20013;&#19968;&#37096;&#20998;&#21442;&#25968;&#20351;&#29992;&#20108;&#38454;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26356;&#26032;&#65292;&#32780;&#21478;&#19968;&#37096;&#20998;&#21442;&#25968;&#20351;&#29992;&#19968;&#38454;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#12290;&#34429;&#28982;&#35813;&#31639;&#27861;&#23545;&#20110;&#38750;&#20984;&#38382;&#39064;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#27809;&#26377;&#35752;&#35770;&#65292;&#20294;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an online learning algorithm for a class of machine learning models under a separable stochastic approximation framework. The essence of our idea lies in the observation that certain parameters in the models are easier to optimize than others. In this paper, we focus on models where some parameters have a linear nature, which is common in machine learning. In one routine of the proposed algorithm, the linear parameters are updated by the recursive least squares (RLS) algorithm, which is equivalent to a stochastic Newton method; then, based on the updated linear parameters, the nonlinear parameters are updated by the stochastic gradient method (SGD). The proposed algorithm can be understood as a stochastic approximation version of block coordinate gradient descent approach in which one part of the parameters is updated by a second-order SGD method while the other part is updated by a first-order SGD. Global convergence of the proposed online algorithm for non-convex cases is 
&lt;/p&gt;</description></item><item><title>BactInt&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;&#24182;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;BactInt&#35821;&#26009;&#24211;&#26631;&#27880;&#20102;1200&#31687;PubMed&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.07468</link><description>&lt;p&gt;
BactInt:&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#20010;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text. (arXiv:2305.07468v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07468
&lt;/p&gt;
&lt;p&gt;
BactInt&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;&#24182;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;BactInt&#35821;&#26009;&#24211;&#26631;&#27880;&#20102;1200&#31687;PubMed&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#39046;&#22495;&#20013;&#19981;&#21516;&#31867;&#22411;&#24494;&#29983;&#29289;&#22312;&#29983;&#29289;&#23398;&#31354;&#38388;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#20123;&#24494;&#29983;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#24494;&#29983;&#29289;&#32676;&#33853;&#32467;&#26500;&#30340;&#22522;&#26412;&#26500;&#24314;&#21333;&#20803;&#12290;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#21487;&#20316;&#20026;&#39044;&#27979;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#21487;&#38752;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#38405;&#35835;&#28023;&#37327;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26159;&#19968;&#39033;&#32791;&#26102;&#24182;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#24037;&#20316;&#12290;&#36825;&#23601;&#24517;&#28982;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#20934;&#30830;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25152;&#25253;&#36947;&#30340;&#32454;&#33740;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#24494;&#29983;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;&#29305;&#21035;&#26159;&#32454;&#33740;&#20043;&#38388;&#65289;&#30340;&#26041;&#27861;&#20197;&#21450;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#31649;&#36947;&#65292;&#29992;&#20110;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;Bacterial Interaction (BactInt)&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;1200&#31687;PubMed&#25688;&#35201;&#65292;&#27880;&#37322;&#26377;&#32454;&#33740;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The community of different types of microbes present in a biological niche plays a very important role in functioning of the system. The crosstalk or interactions among the different microbes contributes to the building blocks of such microbial community structures. Evidence reported in biomedical text serves as a reliable source for predicting such interactions. However, going through the vast and ever-increasing volume of biomedical literature is an intimidating and time consuming process. This necessitates development of automated methods capable of accurately extracting bacterial relations reported in biomedical literature. In this paper, we introduce a method for automated extraction of microbial interactions (specifically between bacteria) from biomedical literature along with ways of using transfer learning to improve its accuracy. We also describe a pipeline using which relations among specific bacteria groups can be mined. Additionally, we introduce the first publicly availabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#24378;&#21270;&#23398;&#20064;&#22312;&#37329;&#34701;&#31185;&#25216;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32452;&#21512;&#20248;&#21270;&#12289;&#38477;&#20302;&#20449;&#29992;&#39118;&#38505;&#12289;&#25237;&#36164;&#36164;&#26412;&#31649;&#29702;&#12289;&#21033;&#28070;&#26368;&#22823;&#21270;&#12289;&#26377;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26356;&#22909;&#30340;&#20215;&#26684;&#31574;&#30053;&#30830;&#23450;&#12290;&#26412;&#25991;&#36890;&#36807;PRISMA&#25216;&#26415;&#31579;&#36873;&#25991;&#29486;&#65292;&#31361;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;Fintech&#20013;&#30340;&#39044;&#27979;&#31934;&#24230;&#12289;&#22797;&#26434;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#39118;&#38505;&#12289;&#30408;&#21033;&#33021;&#21147;&#21644;&#19994;&#32489;&#65292;&#26088;&#22312;&#25506;&#35752;&#20854;&#22312;Fintech&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.07466</link><description>&lt;p&gt;
&#37329;&#34701;&#31185;&#25216;&#39046;&#22495;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Systematic Review on Reinforcement Learning in the Field of Fintech. (arXiv:2305.07466v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#24378;&#21270;&#23398;&#20064;&#22312;&#37329;&#34701;&#31185;&#25216;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32452;&#21512;&#20248;&#21270;&#12289;&#38477;&#20302;&#20449;&#29992;&#39118;&#38505;&#12289;&#25237;&#36164;&#36164;&#26412;&#31649;&#29702;&#12289;&#21033;&#28070;&#26368;&#22823;&#21270;&#12289;&#26377;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26356;&#22909;&#30340;&#20215;&#26684;&#31574;&#30053;&#30830;&#23450;&#12290;&#26412;&#25991;&#36890;&#36807;PRISMA&#25216;&#26415;&#31579;&#36873;&#25991;&#29486;&#65292;&#31361;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;Fintech&#20013;&#30340;&#39044;&#27979;&#31934;&#24230;&#12289;&#22797;&#26434;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#39118;&#38505;&#12289;&#30408;&#21033;&#33021;&#21147;&#21644;&#19994;&#32489;&#65292;&#26088;&#22312;&#25506;&#35752;&#20854;&#22312;Fintech&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#37329;&#34701;&#31185;&#25216;&#65288;Fintech&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;&#36890;&#36807;&#20854;&#20016;&#23500;&#30340;&#33021;&#21147;&#21644;&#39640;&#25928;&#24615;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;Fintech&#39046;&#22495;&#21462;&#24471;&#20102;&#21331;&#36234;&#25104;&#26524;&#12290;&#26412;&#31995;&#32479;&#24615;&#32508;&#36848;&#30340;&#30446;&#30340;&#26159;&#24320;&#23637;&#19968;&#39033;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#19982;Fintech&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#31361;&#20986;&#20854;&#39044;&#27979;&#31934;&#24230;&#12289;&#22797;&#26434;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#39118;&#38505;&#12289;&#30408;&#21033;&#33021;&#21147;&#21644;&#19994;&#32489;&#12290;&#24378;&#21270;&#23398;&#20064;&#22312;&#37329;&#34701;&#25110;Fintech&#39046;&#22495;&#30340;&#20027;&#35201;&#29992;&#36884;&#21253;&#25324;&#32452;&#21512;&#20248;&#21270;&#12289;&#38477;&#20302;&#20449;&#29992;&#39118;&#38505;&#12289;&#25237;&#36164;&#36164;&#26412;&#31649;&#29702;&#12289;&#21033;&#28070;&#26368;&#22823;&#21270;&#12289;&#26377;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26356;&#22909;&#30340;&#20215;&#26684;&#31574;&#30053;&#30830;&#23450;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#23545;&#37329;&#34701;&#26426;&#26500;&#19994;&#32489;&#30340;&#23454;&#38469;&#36129;&#29486;&#12290;&#26412;&#32508;&#36848;&#21253;&#21547;&#20102;2018&#24180;&#20197;&#26469;&#21457;&#34920;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20351;&#29992;PRISMA&#25216;&#26415;&#24320;&#23637;&#20102;&#26412;&#27425;&#32508;&#36848;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#21457;&#29616;&#21644;&#31579;&#36873;&#21463;&#32435;&#20837;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications of Reinforcement Learning in the Finance Technology (Fintech) have acquired a lot of admiration lately. Undoubtedly Reinforcement Learning, through its vast competence and proficiency, has aided remarkable results in the field of Fintech. The objective of this systematic survey is to perform an exploratory study on a correlation between reinforcement learning and Fintech to highlight the prediction accuracy, complexity, scalability, risks, profitability and performance. Major uses of reinforcement learning in finance or Fintech include portfolio optimization, credit risk reduction, investment capital management, profit maximization, effective recommendation systems, and better price setting strategies. Several studies have addressed the actual contribution of reinforcement learning to the performance of financial institutions. The latest studies included in this survey are publications from 2018 onward. The survey is conducted using PRISMA technique which focuses on the re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#26080;&#20808;&#39564;&#20449;&#36947;&#30693;&#35782;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#20013;&#21457;&#23556;&#22120;&#21644;&#25509;&#25910;&#22120;&#32852;&#21512;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#26696;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07448</link><description>&lt;p&gt;
&#26080;&#20808;&#39564;&#20449;&#36947;&#30693;&#35782;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#30340;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep Deterministic Policy Gradient for End-to-End Communication Systems without Prior Channel Knowledge. (arXiv:2305.07448v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#26080;&#20808;&#39564;&#20449;&#36947;&#30693;&#35782;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#20013;&#21457;&#23556;&#22120;&#21644;&#25509;&#25910;&#22120;&#32852;&#21512;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#26696;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#20102;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#22522;&#20110;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20197;&#20849;&#21516;&#20248;&#21270;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#21457;&#36865;&#22120;&#21644;&#25509;&#25910;&#22120;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;E2E&#23398;&#20064;&#26550;&#26500;&#38656;&#35201;&#20808;&#21069;&#30340;&#21487;&#24494;&#20998;&#20449;&#36947;&#27169;&#22411;&#26469;&#20849;&#21516;&#35757;&#32451;&#21457;&#23556;&#25509;&#25910;&#22120;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#20960;&#20046;&#19981;&#21487;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#25509;&#25910;&#22120;DNN&#30340;&#25439;&#22833;&#20540;&#20316;&#20026;&#22870;&#21169;&#26469;&#35757;&#32451;&#21457;&#23556;&#22120;DNN&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#20197;&#20849;&#21516;&#35757;&#32451;&#21457;&#23556;&#22120;&#21644;&#25509;&#25910;&#22120;&#32780;&#19981;&#38656;&#35201;&#20808;&#21069;&#30340;&#20449;&#36947;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;DDPG&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#23454;&#29616;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-End (E2E) learning-based concept has been recently introduced to jointly optimize both the transmitter and the receiver in wireless communication systems. Unfortunately, this E2E learning architecture requires a prior differentiable channel model to jointly train the deep neural networks (DNNs) at the transceivers, which is hardly obtained in practice. This paper aims to solve this issue by developing a deep deterministic policy gradient (DDPG)-based framework. In particular, the proposed solution uses the loss value of the receiver DNN as the reward to train the transmitter DNN. The simulation results then show that our proposed solution can jointly train the transmitter and the receiver without requiring the prior channel model. In addition, we demonstrate that the proposed DDPG-based solution can achieve better detection performance compared to the state-of-the-art solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#36731;&#37327;&#32423;&#39046;&#22495;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21319;&#22522;&#20110;EEG&#30340;&#36328;&#20027;&#20307;&#24773;&#32490;&#35782;&#21035;&#12290;&#25945;&#24072;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#30340;EEG&#29305;&#24449;&#65292;&#25351;&#23548;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#26356;&#25361;&#25112;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.07446</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#36731;&#37327;&#32423;&#39046;&#22495;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;EEG&#30340;&#36328;&#20027;&#20307;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Lightweight Domain Adversarial Neural Network Based on Knowledge Distillation for EEG-based Cross-subject Emotion Recognition. (arXiv:2305.07446v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#36731;&#37327;&#32423;&#39046;&#22495;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21319;&#22522;&#20110;EEG&#30340;&#36328;&#20027;&#20307;&#24773;&#32490;&#35782;&#21035;&#12290;&#25945;&#24072;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#30340;EEG&#29305;&#24449;&#65292;&#25351;&#23548;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#26356;&#25361;&#25112;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#24046;&#24322;&#20250;&#23548;&#33268;&#36328;&#20027;&#20307;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#39046;&#22495;&#20559;&#31227;&#65292;&#24433;&#21709;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#25552;&#20986;&#36731;&#37327;&#32423;&#39046;&#22495;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#30340;EEG&#26102;&#31354;&#21160;&#24577;&#21644;&#31354;&#38388;&#20851;&#32852;&#65292;&#25351;&#23548;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#37319;&#29992;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#20998;&#23618;&#26102;&#31354;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual differences of Electroencephalogram (EEG) could cause the domain shift which would significantly degrade the performance of cross-subject strategy. The domain adversarial neural networks (DANN), where the classification loss and domain loss jointly update the parameters of feature extractor, are adopted to deal with the domain shift. However, limited EEG data quantity and strong individual difference are challenges for the DANN with cumbersome feature extractor. In this work, we propose knowledge distillation (KD) based lightweight DANN to enhance cross-subject EEG-based emotion recognition. Specifically, the teacher model with strong context learning ability is utilized to learn complex temporal dynamics and spatial correlations of EEG, and robust lightweight student model is guided by the teacher model to learn more difficult domain-invariant features. In the feature-based KD framework, a transformer-based hierarchical temporalspatial learning model is served as the teache
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07440</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#20869;&#23384;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Optimizing Memory Mapping Using Deep Reinforcement Learning. (arXiv:2305.07440v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#35843;&#24230;&#21644;&#20998;&#37197;&#26159;&#35768;&#22810;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#28085;&#30422;&#25317;&#22622;&#25511;&#21046;&#21040;&#20113;&#35745;&#31639;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35843;&#24230;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#21363;&#32534;&#35793;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#26399;&#38388;&#20986;&#29616;&#30340;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#65306;&#21363;&#23558;&#24352;&#37327;&#26144;&#23556;&#21040;&#19981;&#21516;&#30340;&#20869;&#23384;&#23618;&#20197;&#20248;&#21270;&#25191;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#21644;&#39640;&#32500;&#25968;&#25454;&#36755;&#20837;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource scheduling and allocation is a critical component of many high impact systems ranging from congestion control to cloud computing. Finding more optimal solutions to these problems often has significant impact on resource and time savings, reducing device wear-and-tear, and even potentially improving carbon emissions. In this paper, we focus on a specific instance of a scheduling problem, namely the memory mapping problem that occurs during compilation of machine learning programs: That is, mapping tensors to different memory layers to optimize execution time.  We introduce an approach for solving the memory mapping problem using Reinforcement Learning. RL is a solution paradigm well-suited for sequential decision making problems that are amenable to planning, and combinatorial search spaces with high-dimensional data inputs. We formulate the problem as a single-player game, which we call the mallocGame, such that high-reward trajectories of the game correspond to efficient memo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2305.07437</link><description>&lt;p&gt;
&#24102;&#26377;&#38750;&#23545;&#35282;&#20449;&#24687;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#36861;&#36394;&#36830;&#32493;&#26356;&#26032;&#30340;CLIP&#27169;&#22411;&#20013;&#34920;&#31034;&#21521;&#37327;&#30340;&#26041;&#21521;&#21464;&#21270;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#24635;&#32467;&#20102;&#36825;&#20123;&#31354;&#38388;&#21464;&#21270;&#65292;&#31216;&#20026;&#31354;&#38388;&#28151;&#20081;&#65288;SD&#65289;&#65292;&#21487;&#20197;&#20998;&#20026;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#22914;&#20309;&#23548;&#33268;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#31354;&#38388;&#28151;&#20081;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X: &#32500;&#25252;&#38750;&#23545;&#35282;&#20449;&#24687;&#30697;&#38453;&#12290;&#22312;&#21508;&#31181;&#19981;&#21516;&#35268;&#27169;&#21644;&#33539;&#22260;&#30340;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19987;&#23478;&#35780;&#20272;&#30340;&#21152;&#26435;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#35266;&#28857;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20272;&#35745;&#27599;&#20010;&#19987;&#23478;&#30340;&#19987;&#19994;&#31243;&#24230;&#21644;&#32467;&#21512;&#20182;&#20204;&#30340;&#24847;&#35265;&#65292;&#28982;&#21518;&#23558;&#21152;&#26435;&#24179;&#22343;&#29992;&#20110;&#22238;&#24402;&#24314;&#27169;&#12290;&#26412;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#20855;&#26377;&#31616;&#21333;&#12289;&#24555;&#36895;&#21644;&#26377;&#25928;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.07430</link><description>&lt;p&gt;
&#22522;&#20110;&#19987;&#23478;&#35780;&#20272;&#30340;&#21152;&#26435;&#22238;&#24402;&#27169;&#22411;&#22788;&#29702;&#22024;&#26434;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Expertise-based Weighting for Regression Models with Noisy Labels. (arXiv:2305.07430v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19987;&#23478;&#35780;&#20272;&#30340;&#21152;&#26435;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#35266;&#28857;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20272;&#35745;&#27599;&#20010;&#19987;&#23478;&#30340;&#19987;&#19994;&#31243;&#24230;&#21644;&#32467;&#21512;&#20182;&#20204;&#30340;&#24847;&#35265;&#65292;&#28982;&#21518;&#23558;&#21152;&#26435;&#24179;&#22343;&#29992;&#20110;&#22238;&#24402;&#24314;&#27169;&#12290;&#26412;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#20855;&#26377;&#31616;&#21333;&#12289;&#24555;&#36895;&#21644;&#26377;&#25928;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#30340;&#26631;&#31614;&#26159;&#20934;&#30830;&#30340;&#65292;&#28982;&#32780;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#33719;&#21462;&#20934;&#30830;&#30340;&#26631;&#31614;&#24182;&#19981;&#21487;&#34892;&#65292;&#22240;&#27492;&#38656;&#35201;&#20381;&#36182;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#35266;&#28857;&#30340;&#19987;&#23478;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#28789;&#27963;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22024;&#26434;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20808;&#20272;&#35745;&#27599;&#20010;&#19987;&#23478;&#30340;&#19987;&#19994;&#31243;&#24230;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#32467;&#21512;&#20182;&#20204;&#30340;&#24847;&#35265;&#12290;&#25509;&#30528;&#65292;&#23558;&#21152;&#26435;&#24179;&#22343;&#29992;&#20110;&#22238;&#24402;&#24314;&#27169;&#12290;&#26412;&#25991;&#26041;&#27861;&#32463;&#36807;&#27491;&#24335;&#39564;&#35777;&#65292;&#35777;&#26126;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#20854;&#28789;&#27963;&#24615;&#20351;&#24471;&#21487;&#20197;&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#21033;&#29992;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#24635;&#20043;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#21644;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#24182;&#22788;&#29702;&#33719;&#21462;&#33258;&#19981;&#21516;&#19987;&#19994;&#26469;&#28304;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression methods assume that accurate labels are available for training. However, in certain scenarios, obtaining accurate labels may not be feasible, and relying on multiple specialists with differing opinions becomes necessary. Existing approaches addressing noisy labels often impose restrictive assumptions on the regression function. In contrast, this paper presents a novel, more flexible approach. Our method consists of two steps: estimating each labeler's expertise and combining their opinions using learned weights. We then regress the weighted average against the input features to build the prediction model. The proposed method is formally justified and empirically demonstrated to outperform existing techniques on simulated and real data. Furthermore, its flexibility enables the utilization of any machine learning technique in both steps. In summary, this method offers a simple, fast, and effective solution for training regression models with noisy labels derived from diverse e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;ChatGPT&#29983;&#25104;&#33258;&#21160;&#35786;&#26029;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#31649;&#29702;&#20581;&#24247;&#29366;&#20917;&#26041;&#38754;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2305.07429</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#30340;&#26234;&#33021;&#35786;&#26029;&#65292;&#21457;&#25496;&#21307;&#23398;&#24433;&#20687;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Medical Imaging with ChatGPT's Intelligent Diagnostics. (arXiv:2305.07429v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;ChatGPT&#29983;&#25104;&#33258;&#21160;&#35786;&#26029;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#31649;&#29702;&#20581;&#24247;&#29366;&#20917;&#26041;&#38754;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#26159;&#35786;&#26029;&#21508;&#31181;&#20581;&#24247;&#30142;&#30149;&#21644;&#29366;&#20917;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#26159;&#19968;&#39033;&#22797;&#26434;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#20570;&#20986;&#20851;&#20110;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#31649;&#29702;&#20581;&#24247;&#29366;&#20917;&#30340;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#65306;1&#65289;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#35760;&#12289;2&#65289;&#27169;&#22411;&#35757;&#32451;&#21644;3&#65289;&#35786;&#26029;&#25253;&#21578;&#29983;&#25104;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#21462;&#22235;&#31181;&#31867;&#22411;&#30340;&#20449;&#24687;&#65306;&#22270;&#20687;&#25195;&#25551;&#31867;&#22411;&#12289;&#36523;&#20307;&#37096;&#20301;&#12289;&#27979;&#35797;&#22270;&#20687;&#21644;&#32467;&#26524;&#12290;&#36825;&#20123;&#20449;&#24687;&#38543;&#21518;&#34987;&#36755;&#20837;&#21040;ChatGPT&#20013;&#65292;&#20197;&#29983;&#25104;&#33258;&#21160;&#35786;&#26029;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#26377;&#21487;&#33021;&#22686;&#24378;&#20915;&#31574;&#21046;&#23450;&#12289;&#38477;&#20302;&#25104;&#26412;&#21644;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;&#25152;&#25552;&#20986;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging is an essential tool for diagnosing various healthcare diseases and conditions. However, analyzing medical images is a complex and time-consuming task that requires expertise and experience. This article aims to design a decision support system to assist healthcare providers and patients in making decisions about diagnosing, treating, and managing health conditions. The proposed architecture contains three stages: 1) data collection and labeling, 2) model training, and 3) diagnosis report generation. The key idea is to train a deep learning model on a medical image dataset to extract four types of information: the type of image scan, the body part, the test image, and the results. This information is then fed into ChatGPT to generate automatic diagnostics. The proposed system has the potential to enhance decision-making, reduce costs, and improve the capabilities of healthcare providers. The efficacy of the proposed system is analyzed by conducting extensive experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;IS-CSE&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#24179;&#28369;&#23884;&#20837;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26631;&#20934;&#30340;STS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.07424</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#30340;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;IS-CSE&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#24179;&#28369;&#23884;&#20837;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26631;&#20934;&#30340;STS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;unsup-SimCSE&#65292;&#22312;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27599;&#20010;&#23884;&#20837;&#20165;&#26469;&#33258;&#20110;&#19968;&#20010;&#21477;&#23376;&#23454;&#20363;&#65292;&#25105;&#20204;&#31216;&#36825;&#20123;&#23884;&#20837;&#20026;&#23454;&#20363;&#32423;&#23884;&#20837;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#23884;&#20837;&#34987;&#35270;&#20026;&#26159;&#19968;&#31867;&#29420;&#29305;&#30340;&#31867;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IS-CSE&#65288;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#65289;&#26469;&#24179;&#28369;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#36793;&#30028;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#35821;&#20041;&#30456;&#20284;&#24615;&#20174;&#21160;&#24577;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#26816;&#32034;&#23884;&#20837;&#20197;&#33719;&#24471;&#27491;&#23884;&#20837;&#32452;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#23545;&#32452;&#20013;&#30340;&#23884;&#20837;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#29983;&#25104;&#24179;&#28369;&#23454;&#20363;&#23884;&#20837;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#22343;78.30&#65285;&#65292;79.47&#65285;&#65292;77.73&#65285;&#21644;79.42&#65285;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning-based methods, such as unsup-SimCSE, have achieved state-of-the-art (SOTA) performances in learning unsupervised sentence embeddings. However, in previous studies, each embedding used for contrastive learning only derived from one sentence instance, and we call these embeddings instance-level embeddings. In other words, each embedding is regarded as a unique class of its own, whichmay hurt the generalization performance. In this study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to smooth the boundaries of embeddings in the feature space. Specifically, we retrieve embeddings from a dynamic memory buffer according to the semantic similarity to get a positive embedding group. Then embeddings in the group are aggregated by a self-attention operation to produce a smoothed instance embedding for further analysis. We evaluate our method on standard semantic text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%, and 79.42% 
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#22312;&#27169;&#20223;&#21035;&#20154;&#26102;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#37027;&#20123;&#34987;&#35748;&#20026;&#19982;&#33258;&#24049;&#22870;&#21169;&#20989;&#25968;&#30456;&#20284;&#30340;&#20154;&#12290;</title><link>http://arxiv.org/abs/2305.07421</link><description>&lt;p&gt;
&#22522;&#20110;&#22870;&#21169;&#20989;&#25968;&#30456;&#20284;&#24615;&#30340;&#36873;&#25321;&#24615;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Selective imitation on the basis of reward function similarity. (arXiv:2305.07421v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07421
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#22312;&#27169;&#20223;&#21035;&#20154;&#26102;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#37027;&#20123;&#34987;&#35748;&#20026;&#19982;&#33258;&#24049;&#22870;&#21169;&#20989;&#25968;&#30456;&#20284;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#26159;&#20154;&#31867;&#31038;&#20250;&#34892;&#20026;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#34987;&#20799;&#31461;&#21644;&#25104;&#20154;&#24191;&#27867;&#29992;&#20316;&#22312;&#19981;&#30830;&#23450;&#25110;&#38476;&#29983;&#24773;&#20917;&#19979;&#23548;&#33322;&#30340;&#26041;&#24335;&#12290;&#20294;&#22312;&#30001;&#22810;&#20010;&#36861;&#27714;&#19981;&#21516;&#30446;&#26631;&#25110;&#30446;&#30340;&#30340;&#24322;&#36136;&#22240;&#20307;&#25152;&#21344;&#30340;&#29615;&#22659;&#20013;&#65292;&#30450;&#30446;&#27169;&#20223;&#19981;&#22826;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#8212;&#8212;&#27169;&#20223;&#32773;&#24517;&#39035;&#30830;&#23450;&#35841;&#26159;&#26368;&#26377;&#29992;&#30340;&#22797;&#21046;&#23545;&#35937;&#12290;&#22312;&#36825;&#20123;&#20915;&#31574;&#20013;&#21487;&#33021;&#26377;&#35768;&#22810;&#22240;&#32032;&#65292;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#21644;&#20449;&#24687;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#20551;&#35774;&#65306;&#36825;&#20123;&#20915;&#31574;&#28041;&#21450;&#23545;&#20854;&#20182;&#26234;&#20307;&#22870;&#21169;&#20989;&#25968;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation is a key component of human social behavior, and is widely used by both children and adults as a way to navigate uncertain or unfamiliar situations. But in an environment populated by multiple heterogeneous agents pursuing different goals or objectives, indiscriminate imitation is unlikely to be an effective strategy -- the imitator must instead determine who is most useful to copy. There are likely many factors that play into these judgements, depending on context and availability of information. Here we investigate the hypothesis that these decisions involve inferences about other agents' reward functions. We suggest that people preferentially imitate the behavior of others they deem to have similar reward functions to their own. We further argue that these inferences can be made on the basis of very sparse or indirect data, by leveraging an inductive bias toward positing the existence of different \textit{groups} or \textit{types} of people with similar reward functions, a
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;(GFTNN)&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#36827;&#34892;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#24378;&#22823;&#30340;&#25805;&#20316;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#21464;&#25442;(GFT)&#65292;&#21487;&#20197;&#27719;&#24635;&#22330;&#26223;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#22312;&#39640;&#36895;&#20844;&#36335;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#21253;&#21547;&#20219;&#20309;&#24490;&#29615;&#20803;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.07416</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#22312;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Multidimensional Graph Fourier Transformation Neural Network for Vehicle Trajectory Prediction. (arXiv:2305.07416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07416
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;(GFTNN)&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#36827;&#34892;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#24378;&#22823;&#30340;&#25805;&#20316;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#21464;&#25442;(GFT)&#65292;&#21487;&#20197;&#27719;&#24635;&#22330;&#26223;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#22312;&#39640;&#36895;&#20844;&#36335;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#21253;&#21547;&#20219;&#20309;&#24490;&#29615;&#20803;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;(GFTNN)&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#36827;&#34892;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#12290;&#31867;&#20284;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#65292;GFTNN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#22312;&#22270;&#32467;&#26500;&#19978;&#25805;&#20316;&#12290;&#36890;&#36807;&#24378;&#22823;&#30340;&#25805;&#20316;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#21464;&#25442;(GFT)&#65292;&#27169;&#22411;&#23558;&#22330;&#26223;&#23646;&#24615;&#27719;&#24635;&#21040;&#19968;&#36215;&#12290;&#36890;&#36807;&#20351;&#29992;GFT&#65292;&#36710;&#36742;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20132;&#20114;&#22270;&#34987;&#36716;&#25442;&#25104;&#20026;&#35889;&#22495;&#22330;&#26223;&#34920;&#31034;&#12290;&#35813;&#26377;&#30410;&#30340;&#34920;&#31034;&#34987;&#36755;&#20837;&#21040;&#30001;&#31070;&#32463;&#32593;&#32476;&#21644;&#25551;&#36848;&#32534;&#30721;&#22120;&#32452;&#25104;&#30340;&#39044;&#27979;&#26694;&#26550;&#20013;&#12290;&#23613;&#31649;&#25152;&#25552;&#20986;&#30340;GFTNN&#27809;&#26377;&#21253;&#21547;&#20219;&#20309;&#24490;&#29615;&#20803;&#20214;&#65292;&#20294;&#22312;&#39640;&#36895;&#20844;&#36335;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#20844;&#24320;&#25968;&#25454;&#38598;highD&#21644;NGSIM&#34987;&#29992;&#20110;&#23454;&#39564;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the multidimensional Graph Fourier Transformation Neural Network (GFTNN) for long-term trajectory predictions on highways. Similar to Graph Neural Networks (GNNs), the GFTNN is a novel network architecture that operates on graph structures. While several GNNs lack discriminative power due to suboptimal aggregation schemes, the proposed model aggregates scenario properties through a powerful operation: the multidimensional Graph Fourier Transformation (GFT). The spatio-temporal vehicle interaction graph of a scenario is converted into a spectral scenario representation using the GFT. This beneficial representation is input to the prediction framework composed of a neural network and a descriptive decoder. Even though the proposed GFTNN does not include any recurrent element, it outperforms state-of-the-art models in the task of highway trajectory prediction. For experiments and evaluation, the publicly available datasets highD and NGSIM are used
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22235;&#31181;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#21311;&#21517;&#21270;&#25216;&#26415;&#19979;&#23545;&#25104;&#20154;&#25968;&#25454;&#38598;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#21311;&#21517;&#21270;&#25216;&#26415;&#21644;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.07415</link><description>&lt;p&gt;
&#24212;&#29992;&#19981;&#21516;&#25216;&#26415;&#23545;&#21311;&#21517;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of machine learning models applied on anonymized data with different techniques. (arXiv:2305.07415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22235;&#31181;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#21311;&#21517;&#21270;&#25216;&#26415;&#19979;&#23545;&#25104;&#20154;&#25968;&#25454;&#38598;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#21311;&#21517;&#21270;&#25216;&#26415;&#21644;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#20540;&#27010;&#25324;&#23618;&#27425;&#32467;&#26500;&#30340;&#20559;&#35782;&#26631;&#35782;&#31526;&#65288;quasi-identifiers&#65289;&#36827;&#34892;&#28151;&#28102;&#30340;&#21311;&#21517;&#21270;&#25216;&#26415;&#34987;&#24191;&#27867;&#29992;&#20110;&#23454;&#29616;&#39044;&#35774;&#30340;&#38544;&#31169;&#32423;&#21035;&#65292;&#20026;&#20102;&#38450;&#27490;&#23545;&#25968;&#25454;&#24211;&#38544;&#31169;&#30340;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#65292;&#38656;&#35201;&#24212;&#29992;&#22810;&#31181;&#21311;&#21517;&#21270;&#25216;&#26415;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#19982;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#20219;&#21153;&#30340;&#25928;&#29992;&#38477;&#20302;&#30452;&#25509;&#30456;&#20851;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22235;&#31181;&#29992;&#20110;&#20998;&#31867;&#30446;&#30340;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20998;&#26512;&#19982;&#24212;&#29992;&#30340;&#21311;&#21517;&#21270;&#25216;&#26415;&#21644;&#27599;&#31181;&#26041;&#27861;&#36873;&#23450;&#30340;&#21442;&#25968;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24403;&#25913;&#21464;k&#20540;&#29992;&#20110;k-&#21311;&#21517;&#24615;&#65292;&#24182;&#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#25104;&#20154;&#25968;&#25454;&#38598;&#19978;&#37096;&#32626;&#20102;$\ell$-&#22810;&#26679;&#24615;&#12289;t-&#23494;&#20999;&#24230;&#21644;$\delta$-&#27844;&#38706;&#38544;&#31169;&#31561;&#38468;&#21152;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anonymization techniques based on obfuscating the quasi-identifiers by means of value generalization hierarchies are widely used to achieve preset levels of privacy. To prevent different types of attacks against database privacy it is necessary to apply several anonymization techniques beyond the classical k-anonymity or $\ell$-diversity. However, the application of these methods is directly connected to a reduction of their utility in prediction and decision making tasks. In this work we study four classical machine learning methods currently used for classification purposes in order to analyze the results as a function of the anonymization techniques applied and the parameters selected for each of them. The performance of these models is studied when varying the value of k for k-anonymity and additional tools such as $\ell$-diversity, t-closeness and $\delta$-disclosure privacy are also deployed on the well-known adult dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20989;&#25968;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#19979;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#24471;&#21040;&#20102;&#35813;&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#39281;&#21644;&#36793;&#30028;&#30340;&#32622;&#20449;&#24230;&#26368;&#20248;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07408</link><description>&lt;p&gt;
&#38754;&#21521;&#20989;&#25968;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Gradient Descent for Functional Learning. (arXiv:2305.07408v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20989;&#25968;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#19979;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#24471;&#21040;&#20102;&#35813;&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#39281;&#21644;&#36793;&#30028;&#30340;&#32622;&#20449;&#24230;&#26368;&#20248;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#22240;&#20854;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#20449;&#24687;&#26041;&#38754;&#30340;&#24040;&#22823;&#20248;&#21183;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#38024;&#23545;&#26368;&#36817;&#20174;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#20013;&#20135;&#29983;&#30340;&#22823;&#25968;&#25454;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#65288;DGDFL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26469;&#33258;&#20247;&#22810;&#26412;&#22320;&#26426;&#22120;&#65288;&#22788;&#29702;&#22120;&#65289;&#30340;&#20989;&#25968;&#25968;&#25454;&#12290;&#22522;&#20110;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;DGDFL&#31639;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#35768;&#22810;&#26041;&#38754;&#30340;&#31532;&#19968;&#20010;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#29702;&#35299;DGDFL&#30340;&#36807;&#31243;&#20013;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20840;&#38754;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#28176;&#36827;&#24335;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#65288;GDFL&#65289;&#31639;&#27861;&#19982;&#21333;&#26426;&#27169;&#22411;&#30456;&#20851;&#32852;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#24471;&#21040;&#20102;DGDFL&#30340;&#32622;&#20449;&#24230;&#26368;&#20248;&#23398;&#20064;&#29575;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#22312;&#27491;&#21017;&#24615;&#32034;&#24341;&#19978;&#36973;&#21463;&#30340;&#39281;&#21644;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, different types of distributed learning schemes have received increasing attention for their strong advantages in handling large-scale data information. In the information era, to face the big data challenges which stem from functional data analysis very recently, we propose a novel distributed gradient descent functional learning (DGDFL) algorithm to tackle functional data across numerous local machines (processors) in the framework of reproducing kernel Hilbert space. Based on integral operator approaches, we provide the first theoretical understanding of the DGDFL algorithm in many different aspects in the literature. On the way of understanding DGDFL, firstly, a data-based gradient descent functional learning (GDFL) algorithm associated with a single-machine model is proposed and comprehensively studied. Under mild conditions, confidence-based optimal learning rates of DGDFL are obtained without the saturation boundary on the regularity index suffered in previous w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;&#30340;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#21163;&#25345;&#20026;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25915;&#20987;&#30340;&#25104;&#21151;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07406</link><description>&lt;p&gt;
&#20004;&#21512;&#19968;&#65306;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Two-in-One: A Model Hijacking Attack Against Text Generation Models. (arXiv:2305.07406v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;&#30340;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#21163;&#25345;&#20026;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25915;&#20987;&#30340;&#25104;&#21151;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#20154;&#33080;&#35782;&#21035;&#21040;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#25104;&#21151;&#20063;&#20276;&#38543;&#30528;&#21508;&#31181;&#25915;&#20987;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#65292;&#21363;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#25552;&#39640;&#20102;&#38382;&#36131;&#21644;&#23492;&#29983;&#35745;&#31639;&#30340;&#39118;&#38505;&#12290;&#20294;&#26159;&#65292;&#35813;&#25915;&#20987;&#20165;&#38598;&#20013;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27492;&#25915;&#20987;&#30340;&#33539;&#22260;&#25193;&#22823;&#21040;&#21253;&#25324;&#25991;&#26412;&#29983;&#25104;&#21644;&#20998;&#31867;&#27169;&#22411;&#65292;&#20174;&#32780;&#23637;&#31034;&#20854;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;&#8212;&#8212;Ditto&#65292;&#23427;&#21487;&#20197;&#23558;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#21163;&#25345;&#20026;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#20363;&#22914;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#25991;&#26412;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#22914;SST-2&#12289;TweetEval&#12289;AGnews&#12289;QNLI&#21644;IMDB&#65289;&#26469;&#35780;&#20272;&#25105;&#20204;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Ditto&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25104;&#21151;&#22320;&#21163;&#25345;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has progressed significantly in various applications ranging from face recognition to text generation. However, its success has been accompanied by different attacks. Recently a new attack has been proposed which raises both accountability and parasitic computing risks, namely the model hijacking attack. Nevertheless, this attack has only focused on image classification tasks. In this work, we broaden the scope of this attack to include text generation and classification models, hence showing its broader applicability. More concretely, we propose a new model hijacking attack, Ditto, that can hijack different text classification tasks into multiple generation ones, e.g., language translation, text summarization, and language modeling. We use a range of text benchmark datasets such as SST-2, TweetEval, AGnews, QNLI, and IMDB to evaluate the performance of our attacks. Our results show that by using Ditto, an adversary can successfully hijack text generation models withou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#33394;&#24425;&#21435;&#21367;&#31215;&#25216;&#26415;&#21644;Pix2Pix GAN&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#19981;&#21516;HER2&#21697;&#29260;&#20043;&#38388;&#39068;&#33394;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#32454;&#32990;&#30340;HER2&#20998;&#25968;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#32959;&#30244;&#35786;&#26029;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07404</link><description>&lt;p&gt;
&#22522;&#20110;&#33394;&#24425;&#21435;&#21367;&#31215;&#30340;HER2&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#39046;&#22495;&#33258;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Color Deconvolution applied to Domain Adaptation in HER2 histopathological images. (arXiv:2305.07404v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#33394;&#24425;&#21435;&#21367;&#31215;&#25216;&#26415;&#21644;Pix2Pix GAN&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#19981;&#21516;HER2&#21697;&#29260;&#20043;&#38388;&#39068;&#33394;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#32454;&#32990;&#30340;HER2&#20998;&#25968;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#32959;&#30244;&#35786;&#26029;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#21457;&#29616;&#20083;&#33146;&#30284;&#23545;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#33267;&#20851;&#37325;&#35201;&#12290; Institut Catal&#224; de la Salut&#65288;ICS&#65289;&#21551;&#21160;&#20102;DigiPatICS&#39033;&#30446;&#65292;&#20197;&#24320;&#21457;&#21644;&#23454;&#26045;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#21327;&#21161;&#30284;&#30151;&#30340;&#35786;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20083;&#33146;&#30284;&#32452;&#32455;&#20013;HER2&#26579;&#33394;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#34987;&#34920;&#31034;&#20026;&#26679;&#24335;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#33394;&#24425;&#21435;&#21367;&#31215;&#25216;&#26415;&#19982;Pix2Pix GAN&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;&#19981;&#21516;HER2&#21697;&#29260;&#20043;&#38388;&#39068;&#33394;&#21464;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20445;&#25345;&#21464;&#25442;&#21518;&#22270;&#20687;&#20013;&#32454;&#32990;&#30340;HER2&#20998;&#25968;&#65292;&#36825;&#23545;&#20110;HER2&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#22312;&#20445;&#25345;&#21464;&#25442;&#21518;&#22270;&#20687;&#30340;&#32454;&#32990;&#31867;&#21035;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#26679;&#24335;&#36716;&#31227;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#19982;&#23427;&#20204;&#19968;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer early detection is crucial for improving patient outcomes. The Institut Catal\`a de la Salut (ICS) has launched the DigiPatICS project to develop and implement artificial intelligence algorithms to assist with the diagnosis of cancer. In this paper, we propose a new approach for facing the color normalization problem in HER2-stained histopathological images of breast cancer tissue, posed as an style transfer problem. We combine the Color Deconvolution technique with the Pix2Pix GAN network to present a novel approach to correct the color variations between different HER2 stain brands. Our approach focuses on maintaining the HER2 score of the cells in the transformed images, which is crucial for the HER2 analysis. Results demonstrate that our final model outperforms the state-of-the-art image style transfer methods in maintaining the cell classes in the transformed images and is as effective as them in generating realistic images.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#27493;&#20108;&#20998;&#22270;&#20999;&#21106;&#20934;&#21017;&#65288;OBCut&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#19982;&#19968;&#20010;&#36857;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#31561;&#20215;&#24615;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20351;&#29992;k-means&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#25193;&#23637;&#23376;&#31354;&#38388;&#32858;&#31867;&#38382;&#39064;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.07386</link><description>&lt;p&gt;
&#19968;&#27493;&#20108;&#20998;&#22270;&#20999;&#21106;&#65306;&#35268;&#33539;&#21270;&#20844;&#24335;&#21450;&#20854;&#22312;&#21487;&#25193;&#23637;&#23376;&#31354;&#38388;&#32858;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-step Bipartite Graph Cut: A Normalized Formulation and Its Application to Scalable Subspace Clustering. (arXiv:2305.07386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#27493;&#20108;&#20998;&#22270;&#20999;&#21106;&#20934;&#21017;&#65288;OBCut&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#19982;&#19968;&#20010;&#36857;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#31561;&#20215;&#24615;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20351;&#29992;k-means&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#25193;&#23637;&#23376;&#31354;&#38388;&#32858;&#31867;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20998;&#22270;&#32467;&#26500;&#22312;&#20419;&#36827;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#23376;&#31354;&#38388;&#32858;&#31867;&#21644;&#35889;&#32858;&#31867;&#31639;&#27861;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36991;&#20813;&#20108;&#20998;&#22270;&#21010;&#20998;&#36807;&#31243;&#20013;&#38656;&#35201;&#20351;&#29992;k-means&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#25105;&#20204;&#36890;&#24120;&#37319;&#29992;&#32422;&#26463;&#25289;&#26222;&#25289;&#26031;&#31209;&#65288;CLR&#65289;&#26469;&#38480;&#21046;&#20108;&#20998;&#22270;&#20013;&#36830;&#25509;&#25104;&#20998;&#65288;&#21363;&#31751;&#65289;&#30340;&#25968;&#37327;&#65292;&#20294;CLR&#24573;&#30053;&#20102;&#36825;&#20123;&#36830;&#25509;&#25104;&#20998;&#30340;&#20998;&#24067;&#65288;&#25110;&#35268;&#33539;&#21270;&#65289;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#24179;&#34913;&#29978;&#33267;&#19981;&#33391;&#31751;&#12290;&#23613;&#31649;&#35268;&#33539;&#21270;&#20999;&#21106;&#65288;Ncut&#65289;&#22312;&#19968;&#33324;&#22270;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22914;&#20309;&#24378;&#21046;&#25191;&#34892;&#20108;&#20998;&#22270;&#30340;&#19968;&#27493;&#35268;&#33539;&#21270;&#20999;&#21106;&#65292;&#29305;&#21035;&#26159;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26377;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#21033;&#29992;&#35268;&#33539;&#21270;&#32422;&#26463;&#34920;&#24449;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#27493;&#20108;&#20998;&#22270;&#20999;&#21106;(OBCut)&#20934;&#21017;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#20934;&#21017;&#19982;&#19968;&#20010;&#36857;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#31561;&#20215;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20999;&#21106;&#20934;&#21017;&#25512;&#24191;&#21040;&#21487;&#25193;&#23637;&#23376;&#31354;&#38388;&#32858;&#31867;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bipartite graph structure has shown its promising ability in facilitating the subspace clustering and spectral clustering algorithms for large-scale datasets. To avoid the post-processing via k-means during the bipartite graph partitioning, the constrained Laplacian rank (CLR) is often utilized for constraining the number of connected components (i.e., clusters) in the bipartite graph, which, however, neglects the distribution (or normalization) of these connected components and may lead to imbalanced or even ill clusters. Despite the significant success of normalized cut (Ncut) in general graphs, it remains surprisingly an open problem how to enforce a one-step normalized cut for bipartite graphs, especially with linear-time complexity. In this paper, we first characterize a novel one-step bipartite graph cut (OBCut) criterion with normalized constraints, and theoretically prove its equivalence to a trace maximization problem. Then we extend this cut criterion to a scalable subspa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;&#23545;&#27604;&#36755;&#20837;&#35299;&#30721;&#65288;CID&#65289;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#32473;&#23450;&#20004;&#20010;&#36755;&#20837;&#30340;&#25991;&#26412;&#65292;&#33021;&#22815;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24494;&#22937;&#20559;&#35265;&#21644;&#36755;&#20986;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.07378</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#36755;&#20837;&#35299;&#30721;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Surfacing Biases in Large Language Models using Contrastive Input Decoding. (arXiv:2305.07378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;&#23545;&#27604;&#36755;&#20837;&#35299;&#30721;&#65288;CID&#65289;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#32473;&#23450;&#20004;&#20010;&#36755;&#20837;&#30340;&#25991;&#26412;&#65292;&#33021;&#22815;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24494;&#22937;&#20559;&#35265;&#21644;&#36755;&#20986;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20844;&#24179;&#12289;&#24378;&#22766;&#21644;&#26377;&#29992;&#65292;&#38656;&#35201;&#20102;&#35299;&#23545;&#23427;&#20204;&#36755;&#20837;&#30340;&#19981;&#21516;&#20462;&#25913;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#25918;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#26679;&#30340;&#35780;&#20272;&#24182;&#19981;&#31616;&#21333;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#24403;&#24341;&#20837;&#19968;&#20010;&#24102;&#26377;&#36755;&#20837;&#25991;&#26412;&#21644;&#25200;&#21160;&#8220;&#23545;&#29031;&#8221;&#29256;&#26412;&#30340;&#27169;&#22411;&#26102;&#65292;&#26631;&#20934;&#35299;&#30721;&#31574;&#30053;&#21487;&#33021;&#19981;&#33021;&#25581;&#31034;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#23454;&#36136;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#36755;&#20837;&#35299;&#30721;&#65288;CID&#65289;&#65306;&#19968;&#31181;&#35299;&#30721;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#32473;&#23450;&#20004;&#20010;&#36755;&#20837;&#30340;&#25991;&#26412;&#65292;&#20854;&#20013;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#22522;&#20110;&#19968;&#20010;&#36755;&#20837;&#32780;&#19981;&#21487;&#33021;&#22522;&#20110;&#21478;&#19968;&#20010;&#36755;&#20837;&#12290;&#36825;&#26679;&#65292;&#23545;&#27604;&#29983;&#25104;&#21487;&#20197;&#20197;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#31361;&#26174;&#20986;LM&#23545;&#36825;&#20004;&#20010;&#36755;&#20837;&#30340;&#36755;&#20986;&#24046;&#24322;&#21487;&#33021;&#23384;&#22312;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;CID&#26469;&#31361;&#26174;&#26631;&#20934;&#35299;&#30721;&#31574;&#30053;&#38590;&#20197;&#26816;&#27979;&#21040;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#20559;&#35265;&#65292;&#24182;&#37327;&#21270;&#19981;&#21516;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour. In the context of open-text generation tasks, however, such an evaluation is not trivial. For example, when introducing a model with an input text and a perturbed, "contrastive" version of it, meaningful differences in the next-token predictions may not be revealed with standard decoding strategies. With this motivation in mind, we propose Contrastive Input Decoding (CID): a decoding algorithm to generate text given two inputs, where the generated text is likely given one input but unlikely given the other. In this way, the contrastive generations can highlight potentially subtle differences in how the LM output differs for the two inputs in a simple and interpretable manner. We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#36817;&#20284;&#30340;SRAM&#20869;&#25968;&#23383;&#20056;&#27861;&#22120;&#65292;&#22312;&#19981;&#20381;&#36182;&#20110;&#26032;&#22411;&#23384;&#20648;&#25216;&#26415;&#21644;&#36991;&#20813;&#20102;&#27604;&#29305;&#20018;&#34892;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20869;&#23384;&#25191;&#34892;GEMM&#35745;&#31639;&#65292;&#20174;&#32780;&#20026;DNN&#35757;&#32451;&#21644;&#25512;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#21152;&#36895;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.07376</link><description>&lt;p&gt;
DAISM&#65306;&#22522;&#20110;&#22810;&#39033;&#24335;&#36817;&#20284;&#30340;SRAM&#20869;&#25968;&#23383;&#20056;&#27861;&#22120;&#30340;DNN&#35757;&#32451;&#21644;&#25512;&#29702;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
DAISM: Digital Approximate In-SRAM Multiplier-based Accelerator for DNN Training and Inference. (arXiv:2305.07376v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#36817;&#20284;&#30340;SRAM&#20869;&#25968;&#23383;&#20056;&#27861;&#22120;&#65292;&#22312;&#19981;&#20381;&#36182;&#20110;&#26032;&#22411;&#23384;&#20648;&#25216;&#26415;&#21644;&#36991;&#20813;&#20102;&#27604;&#29305;&#20018;&#34892;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20869;&#23384;&#25191;&#34892;GEMM&#35745;&#31639;&#65292;&#20174;&#32780;&#20026;DNN&#35757;&#32451;&#21644;&#25512;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#21152;&#36895;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#26159;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#12290;&#23545;&#20110;DNN&#30340;&#30697;&#38453;&#20056;&#27861;&#36816;&#31639;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#21463;&#38480;&#20110;&#20869;&#23384;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#20026;&#20102;&#20248;&#21270;&#30697;&#38453;&#20056;&#27861;&#36816;&#31639;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#19987;&#38376;&#30340;&#21152;&#36895;&#22120;&#12290;&#19968;&#31181;&#27969;&#34892;&#30340;&#24819;&#27861;&#26159;&#20351;&#29992;PIM&#65288;Processing-in-Memory&#65289;&#65292;&#20854;&#20013;&#35745;&#31639;&#26159;&#30001;&#20869;&#23384;&#23384;&#20648;&#20803;&#20214;&#25191;&#34892;&#30340;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#22788;&#29702;&#22120;&#21644;&#35760;&#24518;&#20307;&#20043;&#38388;&#25968;&#25454;&#20256;&#36755;&#30340;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;PIM&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20381;&#36182;&#20110;&#23578;&#26410;&#25104;&#29087;&#30340;&#26032;&#22411;&#23384;&#20648;&#25216;&#26415;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#27604;&#29305;&#20018;&#34892;&#35745;&#31639;&#65292;&#21518;&#32773;&#20855;&#26377;&#37325;&#22823;&#24615;&#33021;&#24320;&#38144;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;SRAM&#20869;&#25968;&#23383;&#20056;&#27861;&#22120;&#26469;&#37319;&#29992;&#20808;&#36827;&#30340;GEMM&#35745;&#31639;&#25216;&#26415;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#27604;&#29305;&#20018;&#34892;&#35745;&#31639;&#30340;&#32570;&#28857;&#12290;&#36825;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#25216;&#26415;&#32780;&#23454;&#29616;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#30340;&#31995;&#32479;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNNs are one of the most widely used Deep Learning models. The matrix multiplication operations for DNNs incur significant computational costs and are bottlenecked by data movement between the memory and the processing elements. Many specialized accelerators have been proposed to optimize matrix multiplication operations. One popular idea is to use Processing-in-Memory where computations are performed by the memory storage element, thereby reducing the overhead of data movement between processor and memory. However, most PIM solutions rely either on novel memory technologies that have yet to mature or bit-serial computations which have significant performance overhead and scalability issues. In this work, an in-SRAM digital multiplier is proposed to take the best of both worlds, i.e. performing GEMM in memory but using only conventional SRAMs without the drawbacks of bit-serial computations. This allows the user to design systems with significant performance gains using existing techno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#38382;&#39064;&#65292;&#21457;&#29616;&#20248;&#21270;&#25509;&#20837;&#27010;&#29575;&#20197;&#26368;&#22823;&#21270;&#25104;&#21151;&#38142;&#36335;&#25968;&#30340;&#26399;&#26395;&#20540;&#26159;&#25552;&#21319;&#31995;&#32479;&#25910;&#25947;&#36895;&#24230;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.07368</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;: &#24191;&#25773;&#19982;&#38543;&#26426;&#25509;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Decentralized Learning over Wireless Networks: The Effect of Broadcast with Random Access. (arXiv:2305.07368v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#38382;&#39064;&#65292;&#21457;&#29616;&#20248;&#21270;&#25509;&#20837;&#27010;&#29575;&#20197;&#26368;&#22823;&#21270;&#25104;&#21151;&#38142;&#36335;&#25968;&#30340;&#26399;&#26395;&#20540;&#26159;&#25552;&#21319;&#31995;&#32479;&#25910;&#25947;&#36895;&#24230;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#36890;&#20449;&#26041;&#38754;&#65292;&#28041;&#21450;&#22810;&#20010;&#20195;&#29702;&#20351;&#29992;&#20998;&#25955;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;(D-SGD)&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#32447;&#20449;&#36947;&#30340;&#24191;&#25773;&#24615;&#36136;&#20197;&#21450;&#36890;&#20449;&#25299;&#25169;&#20013;&#30340;&#38142;&#36335;&#21160;&#24577;&#65292;&#25506;&#31350;&#24191;&#25773;&#20256;&#36755;&#21644;&#27010;&#29575;&#24615;&#38543;&#26426;&#25509;&#20837;&#31574;&#30053;&#23545;D-SGD&#25910;&#25947;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#25509;&#20837;&#27010;&#29575;&#20197;&#26368;&#22823;&#21270;&#25104;&#21151;&#38142;&#36335;&#25968;&#30340;&#26399;&#26395;&#20540;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25552;&#21319;&#31995;&#32479;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we focus on the communication aspect of decentralized learning, which involves multiple agents training a shared machine learning model using decentralized stochastic gradient descent (D-SGD) over distributed data. In particular, we investigate the impact of broadcast transmission and probabilistic random access policy on the convergence performance of D-SGD, considering the broadcast nature of wireless channels and the link dynamics in the communication topology. Our results demonstrate that optimizing the access probability to maximize the expected number of successful links is a highly effective strategy for accelerating the system convergence.
&lt;/p&gt;</description></item><item><title>S-REINFORCE&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#22120;&#29983;&#25104;&#25968;&#23383;&#21644;&#31526;&#21495;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#21487;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.07367</link><description>&lt;p&gt;
S-REINFORCE&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning. (arXiv:2305.07367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07367
&lt;/p&gt;
&lt;p&gt;
S-REINFORCE&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#22120;&#29983;&#25104;&#25968;&#23383;&#21644;&#31526;&#21495;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#21487;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;S-REINFORCE&#65292;&#26088;&#22312;&#20026;&#21160;&#24577;&#20915;&#31574;&#20219;&#21153;&#29983;&#25104;&#21487;&#35299;&#37322;&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#31526;&#21495;&#22238;&#24402;&#22120;&#65288;SR&#65289;&#65292;&#20998;&#21035;&#29983;&#25104;&#25968;&#23383;&#21644;&#31526;&#21495;&#31574;&#30053;&#12290;NN&#32452;&#20214;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#29983;&#25104;&#21487;&#33021;&#25805;&#20316;&#30340;&#25968;&#23383;&#27010;&#29575;&#20998;&#24067;&#65292;&#32780;SR&#32452;&#20214;&#21017;&#25429;&#33719;&#19982;&#25805;&#20316;&#27010;&#29575;&#30456;&#20851;&#30340;&#29366;&#24577;&#38388;&#20851;&#31995;&#30340;&#20989;&#25968;&#24418;&#24335;&#12290;&#28982;&#21518;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#21033;&#29992;SR&#29983;&#25104;&#30340;&#31574;&#30053;&#34920;&#36798;&#24335;&#25913;&#36827;&#23398;&#20064;&#36807;&#31243;&#20013;&#25509;&#25910;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#20302;&#21644;&#39640;&#32500;&#34892;&#21160;&#31354;&#38388;&#30340;&#21508;&#31181;&#21160;&#24577;&#20915;&#31574;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#25552;&#20986;&#30340;S-REINFORCE&#31639;&#27861;&#65292;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#23454;&#29616;&#21487;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;NN&#21644;SR&#30340;&#20248;&#21183;&#65292;S-REINFORCE&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel RL algorithm, S-REINFORCE, which is designed to generate interpretable policies for dynamic decision-making tasks. The proposed algorithm leverages two types of function approximators, namely Neural Network (NN) and Symbolic Regressor (SR), to produce numerical and symbolic policies, respectively. The NN component learns to generate a numerical probability distribution over the possible actions using a policy gradient, while the SR component captures the functional form that relates the associated states with the action probabilities. The SR-generated policy expressions are then utilized through importance sampling to improve the rewards received during the learning process. We have tested the proposed S-REINFORCE algorithm on various dynamic decision-making problems with low and high dimensional action spaces, and the results demonstrate its effectiveness and impact in achieving interpretable solutions. By leveraging the strengths of both NN and SR, S-REINF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#25512;&#20986;&#30340;M&#35821;&#35328;&#23558;&#27169;&#22411;&#20316;&#20026;&#22522;&#26412;&#30340;&#35745;&#31639;&#21333;&#20301;&#65292;&#21152;&#24378;&#20102;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20851;&#38190;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#23558;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.07341</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#65306;&#20026;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#37325;&#26032;&#23450;&#20041;&#31243;&#24207;&#30340;&#22522;&#26412;&#21333;&#20301;
&lt;/p&gt;
&lt;p&gt;
Model-based Programming: Redefining the Atomic Unit of Programming for the Deep Learning Era. (arXiv:2305.07341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#25512;&#20986;&#30340;M&#35821;&#35328;&#23558;&#27169;&#22411;&#20316;&#20026;&#22522;&#26412;&#30340;&#35745;&#31639;&#21333;&#20301;&#65292;&#21152;&#24378;&#20102;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20851;&#38190;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#23558;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#20294;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#23454;&#38469;&#19994;&#21153;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#22914;&#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#12289;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#20197;&#21450;&#19982;&#29616;&#26377;&#32534;&#31243;&#35821;&#35328;&#30340;&#38598;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#8221;&#27010;&#24565;&#65292;&#24182;&#25512;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#31243;&#35821;&#35328;&#8212;&#8212;M&#35821;&#35328;&#65292;&#35813;&#35821;&#35328;&#38024;&#23545;&#39044;&#26399;&#30340;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#32534;&#31243;&#33539;&#24335;&#32780;&#35774;&#35745;&#12290;M&#35821;&#35328;&#23558;&#27169;&#22411;&#35270;&#20026;&#22522;&#26412;&#30340;&#35745;&#31639;&#21333;&#20301;&#65292;&#20351;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#20851;&#38190;&#20219;&#21153;&#65292;&#22914;&#27169;&#22411;&#21152;&#36733;&#12289;&#24494;&#35843;&#12289;&#35780;&#20272;&#21644;&#37096;&#32626;&#65292;&#20174;&#32780;&#22686;&#24378;&#21019;&#24314;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#23558;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces and explores a new programming paradigm, Model-based Programming, designed to address the challenges inherent in applying deep learning models to real-world applications. Despite recent significant successes of deep learning models across a range of tasks, their deployment in real business scenarios remains fraught with difficulties, such as complex model training, large computational resource requirements, and integration issues with existing programming languages. To ameliorate these challenges, we propose the concept of 'Model-based Programming' and present a novel programming language - M Language, tailored to a prospective model-centered programming paradigm. M Language treats models as basic computational units, enabling developers to concentrate more on crucial tasks such as model loading, fine-tuning, evaluation, and deployment, thereby enhancing the efficiency of creating deep learning applications. We posit that this innovative programming paradigm will 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#32452;&#21512;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#25968;&#27744;&#21270;&#21644;&#36229;&#21472;&#21152;&#26469;&#32452;&#21512;&#21518;&#39564;&#23494;&#24230;&#65292;&#36991;&#20813;&#20102;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#36127;&#25285;&#65292;&#24182;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07334</link><description>&lt;p&gt;
&#38145;&#23450;&#19982;&#21472;&#23618;&#65306;&#36890;&#36807;&#23545;&#25968;&#27744;&#21270;&#21644;&#36229;&#21472;&#21152;&#22534;&#21472;&#36125;&#21494;&#26031;&#27169;&#22411;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Locking and Quacking: Stacking Bayesian model predictions by log-pooling and superposition. (arXiv:2305.07334v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#32452;&#21512;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#25968;&#27744;&#21270;&#21644;&#36229;&#21472;&#21152;&#26469;&#32452;&#21512;&#21518;&#39564;&#23494;&#24230;&#65292;&#36991;&#20813;&#20102;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#36127;&#25285;&#65292;&#24182;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#21512;&#36215;&#26469;&#26159;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#39044;&#27979;&#20998;&#24067;&#20960;&#20046;&#20165;&#20351;&#29992;&#32447;&#24615;&#32452;&#21512;&#36827;&#34892;&#32452;&#21512;&#65292;&#20363;&#22914;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36125;&#21494;&#26031;&#22534;&#21472;&#21644;&#19987;&#23478;&#28151;&#21512;&#12290;&#36825;&#31181;&#32447;&#24615;&#28151;&#21512;&#21487;&#33021;&#23545;&#26576;&#20123;&#24212;&#29992;&#31243;&#24207;&#19981;&#21033;&#65292;&#20363;&#22914;&#22810;&#23792;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#32452;&#21512;&#24037;&#20855;&#12290;&#36825;&#20123;&#24037;&#20855;&#26159;&#27169;&#22411;&#22534;&#21472;&#30340;&#25512;&#24191;&#65292;&#20294;&#26159;&#36890;&#36807;&#23545;&#25968;&#32447;&#24615;&#27719;&#38598;&#65288;&#38145;&#23450;&#65289;&#21644;&#37327;&#23376;&#21472;&#21152;&#65288;quacking&#65289;&#26469;&#21512;&#24182;&#21518;&#39564;&#23494;&#24230;&#12290;&#20026;&#20102;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#32780;&#36991;&#20813;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#36127;&#25285;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32452;&#21512;&#21518;&#39564;&#39044;&#27979;&#30340;Hyvarinen&#24471;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31034;&#20363;&#35828;&#26126;&#20102;&#38145;&#23450;&#65292;&#24182;&#23558;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;&#19981;&#21516;&#36830;&#32493;&#23494;&#24230;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#65292;&#23558;&#23427;&#20204;&#19982;&#20256;&#32479;&#30340;&#27169;&#22411;&#32452;&#21512;&#24037;&#20855;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining predictions from different models is a central problem in Bayesian inference and machine learning more broadly. Currently, these predictive distributions are almost exclusively combined using linear mixtures such as Bayesian model averaging, Bayesian stacking, and mixture of experts. Such linear mixtures impose idiosyncrasies that might be undesirable for some applications, such as multi-modality. While there exist alternative strategies (e.g. geometric bridge or superposition), optimising their parameters usually involves computing an intractable normalising constant repeatedly. We present two novel Bayesian model combination tools. These are generalisations of model stacking, but combine posterior densities by log-linear pooling (locking) and quantum superposition (quacking). To optimise model weights while avoiding the burden of normalising constants, we investigate the Hyvarinen score of the combined posterior predictions. We demonstrate locking with an illustrative examp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;tSNE&#21644;UMAP&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#21457;&#29616;&#24402;&#19968;&#21270;&#21442;&#25968;&#21487;&#20197;&#22312;&#20004;&#32773;&#20043;&#38388;&#20999;&#25442;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;(\ourmethod)&#21487;&#20197;&#32467;&#21512;&#20043;&#21069;&#19981;&#20860;&#23481;&#30340;tSNE&#21644;UMAP&#25216;&#26415;&#65292;&#21363;&#21487;&#20197;&#22797;&#21046;&#20219;&#24847;&#31639;&#27861;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#21152;&#36895;&#33719;&#24471;UMAP&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07320</link><description>&lt;p&gt;
ActUp: &#20998;&#26512;&#21644;&#25972;&#21512;tSNE&#21644;UMAP
&lt;/p&gt;
&lt;p&gt;
ActUp: Analyzing and Consolidating tSNE and UMAP. (arXiv:2305.07320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;tSNE&#21644;UMAP&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#21457;&#29616;&#24402;&#19968;&#21270;&#21442;&#25968;&#21487;&#20197;&#22312;&#20004;&#32773;&#20043;&#38388;&#20999;&#25442;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;(\ourmethod)&#21487;&#20197;&#32467;&#21512;&#20043;&#21069;&#19981;&#20860;&#23481;&#30340;tSNE&#21644;UMAP&#25216;&#26415;&#65292;&#21363;&#21487;&#20197;&#22797;&#21046;&#20219;&#24847;&#31639;&#27861;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#21152;&#36895;&#33719;&#24471;UMAP&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
tSNE&#21644;UMAP&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#36895;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#23884;&#20837;&#32780;&#21463;&#27426;&#36814;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;&#23427;&#20204;&#30340;&#20840;&#37096;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#35780;&#20272;&#20102;tSNE&#21644;UMAP&#20013;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;&#19968;&#20010;&#21442;&#25968;-&#35268;&#33539;&#21270;-&#21487;&#20197;&#22312;&#23427;&#20204;&#20043;&#38388;&#20999;&#25442;&#12290;&#36825;&#21453;&#36807;&#26469;&#24847;&#21619;&#30528;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#24046;&#24322;&#21487;&#20197;&#20999;&#25442;&#32780;&#19981;&#24433;&#21709;&#23884;&#20837;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#23545;UMAP&#32972;&#21518;&#30340;&#20960;&#20010;&#29702;&#35770;&#20027;&#24352;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#30340;tSNE&#35299;&#37322;&#30456;&#21644;&#35856;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;(\ourmethod)&#65292;&#23427;&#32467;&#21512;&#20102;&#20043;&#21069;&#19981;&#20860;&#23481;&#30340;tSNE&#21644;UMAP&#25216;&#26415;&#65292;&#24182;&#21487;&#20197;&#22797;&#21046;&#20219;&#19968;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#20363;&#22914;&#21152;&#36895;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#33719;&#24471;UMAP&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
tSNE and UMAP are popular dimensionality reduction algorithms due to their speed and interpretable low-dimensional embeddings. Despite their popularity, however, little work has been done to study their full span of differences. We theoretically and experimentally evaluate the space of parameters in both tSNE and UMAP and observe that a single one -- the normalization -- is responsible for switching between them. This, in turn, implies that a majority of the algorithmic differences can be toggled without affecting the embeddings. We discuss the implications this has on several theoretic claims behind UMAP, as well as how to reconcile them with existing tSNE interpretations.  Based on our analysis, we provide a method (\ourmethod) that combines previously incompatible techniques from tSNE and UMAP and can replicate the results of either algorithm. This allows our method to incorporate further improvements, such as an acceleration that obtains either method's outputs faster than UMAP. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Robust $(k, z)$-Clustering&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#31163;&#25955;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#35299;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#33719;&#24471;$O(\log m/\log\log m)$&#30340;&#36817;&#20284;&#22240;&#23376;&#65292;&#22312;FPT&#26102;&#38388;&#20869;&#21487;&#20197;&#33719;&#24471;$(3^z+\epsilon)$&#30340;&#36817;&#20284;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.07316</link><description>&lt;p&gt;
&#31163;&#25955;&#20960;&#20309;&#31354;&#38388;&#20013;&#25239;&#24178;&#25200;&#32858;&#31867;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Parameterized Approximation for Robust Clustering in Discrete Geometric Spaces. (arXiv:2305.07316v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Robust $(k, z)$-Clustering&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#31163;&#25955;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#35299;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#33719;&#24471;$O(\log m/\log\log m)$&#30340;&#36817;&#20284;&#22240;&#23376;&#65292;&#22312;FPT&#26102;&#38388;&#20869;&#21487;&#20197;&#33719;&#24471;$(3^z+\epsilon)$&#30340;&#36817;&#20284;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Robust $(k, z)$-Clustering&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#20986;&#29616;&#22312;&#40065;&#26834;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#39046;&#22495;&#20013;&#12290;&#24050;&#30693;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35813;&#38382;&#39064;&#20855;&#26377;$O(\log m/\log\log m)$&#30340;&#36817;&#20284;&#22240;&#23376;&#65292;&#22312;FPT&#26102;&#38388;&#20869;&#20855;&#26377;$(3^z+\epsilon)$&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the well-studied Robust $(k, z)$-Clustering problem, which generalizes the classic $k$-Median, $k$-Means, and $k$-Center problems. Given a constant $z\ge 1$, the input to Robust $(k, z)$-Clustering is a set $P$ of $n$ weighted points in a metric space $(M,\delta)$ and a positive integer $k$. Further, each point belongs to one (or more) of the $m$ many different groups $S_1,S_2,\ldots,S_m$. Our goal is to find a set $X$ of $k$ centers such that $\max_{i \in [m]} \sum_{p \in S_i} w(p) \delta(p,X)^z$ is minimized.  This problem arises in the domains of robust optimization [Anthony, Goyal, Gupta, Nagarajan, Math. Oper. Res. 2010] and in algorithmic fairness. For polynomial time computation, an approximation factor of $O(\log m/\log\log m)$ is known [Makarychev, Vakilian, COLT $2021$], which is tight under a plausible complexity assumption even in the line metrics. For FPT time, there is a $(3^z+\epsilon)$-approximation algorithm, which is tight under GAP-ETH [Goyal, Jaiswal, In
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31163;&#25955;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476; $\partial\mathbb{B}$ &#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#36719;&#32593;&#32476;&#21644;&#30828;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#23398;&#20064;&#21040;&#30340;&#31163;&#25955;&#20989;&#25968;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#36739;&#39640;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.07315</link><description>&lt;p&gt;
$\partial\mathbb{B}$ &#31070;&#32463;&#32593;&#32476;&#65306;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31163;&#25955;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
$\partial\mathbb{B}$ nets: learning discrete functions by gradient descent. (arXiv:2305.07315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07315
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31163;&#25955;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476; $\partial\mathbb{B}$ &#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#36719;&#32593;&#32476;&#21644;&#30828;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#23398;&#20064;&#21040;&#30340;&#31163;&#25955;&#20989;&#25968;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#36739;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$\partial\mathbb{B}$ &#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31163;&#25955;&#24067;&#23572;&#22411;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#12290;$\partial\mathbb{B}$ &#32593;&#32476;&#26377;&#20004;&#20010;&#35821;&#20041;&#19978;&#31561;&#25928;&#30340;&#26041;&#38754;&#65306;&#21487;&#24494;&#20998;&#30340;&#36719;&#32593;&#32476;&#21644;&#24067;&#23572;&#26435;&#37325;&#30340;&#19981;&#21487;&#24494;&#30828;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#36719;&#32593;&#32476;&#65292;&#28982;&#21518;&#8220;&#30828;&#21270;&#8221;&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#65292;&#20197;&#33719;&#24471;&#19982;&#30828;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#24067;&#23572;&#26435;&#37325;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#31163;&#25955;&#20989;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#20108;&#20540;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#30828;&#21270;&#19981;&#20250;&#23548;&#33268;&#31934;&#24230;&#25439;&#22833;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;$\partial\mathbb{B}$ &#32593;&#32476;&#22312;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#32039;&#20945;&#24615;&#65288;&#30001;&#20110; 1 &#20301;&#26435;&#37325;&#65289;&#21644;&#21487;&#35299;&#37322;&#24615;&#65288;&#30001;&#20110;&#23398;&#20064;&#21040;&#20989;&#25968;&#30340;&#36923;&#36753;&#24615;&#36136;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
$\partial\mathbb{B}$ nets are differentiable neural networks that learn discrete boolean-valued functions by gradient descent. $\partial\mathbb{B}$ nets have two semantically equivalent aspects: a differentiable soft-net, with real weights, and a non-differentiable hard-net, with boolean weights. We train the soft-net by backpropagation and then `harden' the learned weights to yield boolean weights that bind with the hard-net. The result is a learned discrete function. `Hardening' involves no loss of accuracy, unlike existing approaches to neural network binarization. Preliminary experiments demonstrate that $\partial\mathbb{B}$ nets achieve comparable performance on standard machine learning problems yet are compact (due to 1-bit weights) and interpretable (due to the logical nature of the learnt functions).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#20013;&#20122;&#22320;&#21306;&#39135;&#21697;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;42&#20010;&#31867;&#21035;&#21644;&#36229;&#36807;16,000&#24352;&#23646;&#20110;&#35813;&#22320;&#21306;&#29420;&#29305;&#30340;&#22269;&#23478;&#32654;&#39135;&#30340;&#22270;&#20687;&#65292;&#24182;&#36798;&#21040;&#20102;88.70&#65285;&#65288;42&#20010;&#31867;&#21035;&#65289;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07257</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#20010;&#24615;&#21270;&#33203;&#39135;&#24178;&#39044;&#30340;&#20013;&#20122;&#39135;&#21697;&#25968;&#25454;&#38598;&#65292;&#25193;&#23637;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
A Central Asian Food Dataset for Personalized Dietary Interventions, Extended Abstract. (arXiv:2305.07257v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#20013;&#20122;&#22320;&#21306;&#39135;&#21697;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;42&#20010;&#31867;&#21035;&#21644;&#36229;&#36807;16,000&#24352;&#23646;&#20110;&#35813;&#22320;&#21306;&#29420;&#29305;&#30340;&#22269;&#23478;&#32654;&#39135;&#30340;&#22270;&#20687;&#65292;&#24182;&#36798;&#21040;&#20102;88.70&#65285;&#65288;42&#20010;&#31867;&#21035;&#65289;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#20154;&#20204;&#26222;&#36941;&#20250;&#25293;&#25668;&#20182;&#20204;&#25152;&#21507;&#30340;&#27599;&#19968;&#31181;&#39278;&#26009;&#12289;&#38646;&#39135;&#25110;&#39184;&#28857;&#30340;&#29031;&#29255;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#29031;&#29255;&#21457;&#24067;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#12290;&#21033;&#29992;&#36825;&#20123;&#31038;&#20132;&#36235;&#21183;&#65292;&#23454;&#26102;&#30340;&#39135;&#21697;&#35782;&#21035;&#21644;&#36825;&#20123;&#25429;&#33719;&#30340;&#39135;&#21697;&#22270;&#20687;&#30340;&#21487;&#38752;&#20998;&#31867;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26367;&#25442;&#19968;&#20123;&#21333;&#35843;&#30340;&#33203;&#39135;&#35760;&#24405;&#21644;&#32534;&#30721;&#65292;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#33203;&#39135;&#24178;&#39044;&#12290;&#34429;&#28982;&#20013;&#20122;&#33756;&#32948;&#22312;&#25991;&#21270;&#21644;&#21382;&#21490;&#19978;&#20855;&#26377;&#29420;&#29305;&#24615;&#65292;&#20294;&#23545;&#36825;&#20010;&#22320;&#21306;&#30340;&#20154;&#20204;&#30340;&#39135;&#21697;&#21644;&#33203;&#39135;&#20064;&#24815;&#30340;&#21457;&#34920;&#25968;&#25454;&#24456;&#23569;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#22320;&#21306;&#39135;&#21697;&#25968;&#25454;&#38598;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#23545;&#24191;&#22823;&#28040;&#36153;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#24456;&#23481;&#26131;&#33719;&#21462;&#12290;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#20851;&#20110;&#21019;&#24314;&#20013;&#20122;&#39135;&#21697;&#25968;&#25454;&#38598;&#65288;CAFD&#65289;&#30340;&#24037;&#20316;&#12290;&#26368;&#32456;&#25968;&#25454;&#38598;&#21253;&#21547;42&#20010;&#39135;&#21697;&#31867;&#21035;&#21644;&#36229;&#36807;16,000&#24352;&#23646;&#20110;&#35813;&#22320;&#21306;&#29420;&#29305;&#30340;&#22269;&#23478;&#32654;&#39135;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;CAFD&#19978;&#23454;&#29616;&#20102;88.70&#65285;&#65288;42&#20010;&#31867;&#21035;&#65289;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, it is common for people to take photographs of every beverage, snack, or meal they eat and then post these photographs on social media platforms. Leveraging these social trends, real-time food recognition and reliable classification of these captured food images can potentially help replace some of the tedious recording and coding of food diaries to enable personalized dietary interventions. Although Central Asian cuisine is culturally and historically distinct, there has been little published data on the food and dietary habits of people in this region. To fill this gap, we aim to create a reliable dataset of regional foods that is easily accessible to both public consumers and researchers. To the best of our knowledge, this is the first work on creating a Central Asian Food Dataset (CAFD). The final dataset contains 42 food categories and over 16,000 images of national dishes unique to this region. We achieved a classification accuracy of 88.70\% (42 classes) on the CAFD us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35745;&#31639;&#29615;&#36335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22810;&#32452;&#20998;&#26448;&#26009;&#30340;&#34920;&#38754;&#30456;&#22270;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20132;&#20114;&#20316;&#29992;&#21183;&#21152;&#36895;&#20102;&#33021;&#37327;&#35780;&#20998;&#21644;&#32479;&#35745;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;NiAl (110)&#19978;&#39044;&#27979;&#20102;&#26032;&#39062;&#30340;&#34920;&#38754;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07251</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#30340;&#27169;&#25311;&#20351;&#24471;&#26080;&#32463;&#39564;&#34920;&#38754;&#37325;&#24314;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Machine-learning-accelerated simulations enable heuristic-free surface reconstruction. (arXiv:2305.07251v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35745;&#31639;&#29615;&#36335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22810;&#32452;&#20998;&#26448;&#26009;&#30340;&#34920;&#38754;&#30456;&#22270;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20132;&#20114;&#20316;&#29992;&#21183;&#21152;&#36895;&#20102;&#33021;&#37327;&#35780;&#20998;&#21644;&#32479;&#35745;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;NiAl (110)&#19978;&#39044;&#27979;&#20102;&#26032;&#39062;&#30340;&#34920;&#38754;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20652;&#21270;&#25110;&#30005;&#23376;&#31561;&#39046;&#22495;&#65292;&#29702;&#35299;&#29289;&#36136;&#34920;&#38754;&#21644;&#30028;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#30005;&#23376;&#32467;&#26500;&#20013;&#30340;&#33021;&#37327;&#21644;&#32479;&#35745;&#21147;&#23398;&#30456;&#32467;&#21512;&#30340;&#31532;&#19968;&#24615;&#21407;&#29702;&#27169;&#25311;&#21487;&#20197;&#22312;&#21407;&#21017;&#19978;&#39044;&#27979;&#26448;&#26009;&#34920;&#38754;&#30340;&#32467;&#26500;&#19982;&#28909;&#21147;&#23398;&#21464;&#37327;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#24517;&#39035;&#36827;&#34892;&#32479;&#35745;&#37319;&#26679;&#30340;&#24191;&#38420;&#30456;&#31354;&#38388;&#32806;&#21512;&#26102;&#65292;&#31934;&#30830;&#30340;&#33021;&#37327;&#27169;&#25311;&#26159;&#31105;&#27490;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#35745;&#31639;&#29615;&#36335;&#26469;&#39044;&#27979;&#22810;&#32452;&#20998;&#26448;&#26009;&#30340;&#34920;&#38754;&#30456;&#22270;&#65292;&#21516;&#26102;&#21152;&#36895;&#33021;&#37327;&#35780;&#20998;&#21644;&#32479;&#35745;&#37319;&#26679;&#26041;&#27861;&#12290;&#36890;&#36807;&#38381;&#29615;&#20027;&#21160;&#23398;&#20064;&#65292;&#20351;&#29992;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#22312;&#39640;&#36890;&#37327;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#35745;&#31639;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#20551;&#24819;&#34920;&#38754;&#20301;&#28857;&#19978;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#65292;&#22312;&#21322;&#27491;&#21017;&#31995;&#32508;&#20013;&#23454;&#29616;&#37319;&#26679;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#30340;&#24341;&#23548;&#19979;&#65292;&#39044;&#27979;&#30340;GaN&#65288;0001&#65289;&#21644;SrTiO3&#65288;001&#65289;&#34920;&#38754;&#19982;&#36807;&#21435;&#30340;&#24037;&#20316;&#19968;&#33268;&#65292;&#24182;&#19988;&#22312;NiAl(110)&#19978;&#39044;&#27979;&#20102;&#26032;&#39062;&#30340;&#34920;&#38754;&#32467;&#26500;&#65292;&#20854;&#20013;&#21457;&#29616;&#22312;&#31354;&#20301;&#30340;&#23384;&#22312;&#19979;&#65292;&#21453;&#20301;&#28857;&#26434;&#36136;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding material surfaces and interfaces is vital in applications like catalysis or electronics. Ab initio simulations, combining energies from electronic structure with statistical mechanics, can, in principle, predict the structure of material surfaces as a function of thermodynamic variables. However, accurate energy simulations are prohibitive when coupled to the vast phase space that must be statistically sampled. Here, we present a bi-faceted computational loop to predict surface phase diagrams of multi-component materials that accelerates both the energy scoring and statistical sampling methods. Fast, scalable, and data-efficient machine learning interatomic potentials are trained on high-throughput density-functional theory calculations through closed-loop active learning. Markov-chain Monte Carlo sampling in the semi-grand canonical ensemble is enabled by using virtual surface sites. The predicted surfaces for GaN(0001) and SrTiO3(001) are in agreement with past work and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20248;&#21270;&#32047;&#31215;&#22870;&#21169;&#20998;&#20301;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;QPO&#21644;&#20854;&#21464;&#20307;QPPO&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25511;&#21046;&#21160;&#20316;&#30340;&#31574;&#30053;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07248</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#20004;&#26102;&#38388;&#26631;&#24230;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantile-Based Deep Reinforcement Learning using Two-Timescale Policy Gradient Algorithms. (arXiv:2305.07248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20248;&#21270;&#32047;&#31215;&#22870;&#21169;&#20998;&#20301;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;QPO&#21644;&#20854;&#21464;&#20307;QPPO&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25511;&#21046;&#21160;&#20316;&#30340;&#31574;&#30053;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26088;&#22312;&#20248;&#21270;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20248;&#21270;&#32047;&#31215;&#22870;&#21169;&#20998;&#20301;&#25968;&#30340;RL&#35774;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25511;&#21046;&#21160;&#20316;&#30340;&#31574;&#30053;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Quantile-Based Policy Optimization&#65288;QPO&#65289;&#30340;&#26032;&#22411;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;Quantile-Based Proximal Policy Optimization&#65288;QPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37327;&#21270;&#30446;&#26631;&#30340;&#28145;&#24230;RL&#38382;&#39064;&#12290;QPO&#20351;&#29992;&#20004;&#20010;&#32806;&#21512;&#36845;&#20195;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#26631;&#24230;&#19978;&#26356;&#26032;&#20998;&#20301;&#25968;&#21644;&#31574;&#30053;&#21442;&#25968;&#65292;&#32780;QPPO&#26159;QPO&#30340;&#31163;&#32447;&#29256;&#26412;&#65292;&#20801;&#35768;&#22312;&#19968;&#20010;&#27169;&#25311;&#22238;&#21512;&#20013;&#22810;&#27425;&#26356;&#26032;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20998;&#20301;&#25968;&#26631;&#20934;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical reinforcement learning (RL) aims to optimize the expected cumulative reward. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative reward. We parameterize the policy controlling actions by neural networks, and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) for solving deep RL problems with quantile objectives. QPO uses two coupled iterations running at different timescales for simultaneously updating quantiles and policy parameters, whereas QPPO is an off-policy version of QPO that allows multiple updates of parameters during one simulation episode, leading to improved algorithm efficiency. Our numerical results indicate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07247</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#30340;Schr\"odinger bridge&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation. (arXiv:2305.07247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Schr\"odinger bridge&#38382;&#39064;&#65288;SBP&#65289;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#30340;&#25237;&#24433;&#26159;&#21807;&#19968;&#21487;&#29992;&#30340;&#65292;&#20854;&#25910;&#25947;&#24615;&#36824;&#19981;&#26159;&#21313;&#20998;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;SBP&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#23637;&#31034;&#20102;&#20248;&#21270;&#20256;&#36755;&#25104;&#26412;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Schr\"odinger bridge problem (SBP) is gaining increasing attention in generative modeling and showing promising potential even in comparison with the score-based generative models (SGMs). SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, only approximated projections are accessible and their convergence is not well understood. To fill this gap, we present a first convergence analysis of the Schr\"odinger bridge algorithm based on approximated projections. As for its practical applications, we apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed algorithm achieves the state-of-the-art result in healthcare and environmental data while exhibiting the advantage of exploring both temporal and feature patterns in probabilistic time ser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#24403;&#20351;&#29992;Sobolev RKHS&#26102;&#65292;&#23545;&#20110;&#35823;&#21305;&#37197;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;KRR&#23545;&#20110;&#20219;&#20309;$s\in (0,1)$&#37117;&#26159;&#26368;&#20248;&#30340;</title><link>http://arxiv.org/abs/2305.07241</link><description>&lt;p&gt;
&#20851;&#20110;&#35823;&#21305;&#37197;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#20248;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Optimality of Misspecified Kernel Ridge Regression. (arXiv:2305.07241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#24403;&#20351;&#29992;Sobolev RKHS&#26102;&#65292;&#23545;&#20110;&#35823;&#21305;&#37197;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;KRR&#23545;&#20110;&#20219;&#20309;$s\in (0,1)$&#37117;&#26159;&#26368;&#20248;&#30340;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35823;&#21305;&#37197;&#30340;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20551;&#23450;&#22320;&#19979;&#30495;&#23454;&#20989;&#25968;$f_{\rho}^{*} \in [\mathcal{H}]^{s}$&#65292;&#20854;&#20013;$[\mathcal{H}]^{s}$&#26159;&#19968;&#20010;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;$\mathcal{H}$&#30340;&#36739;&#24179;&#28369;&#25554;&#20540;&#31354;&#38388;&#65292;$s\in (0,1)$&#12290;&#29616;&#26377;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#32467;&#26524;&#35201;&#27714;$\|f_{\rho}^{*}\|_{L^{\infty}}&lt;\infty$&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;$s &gt; \alpha_{0}$&#65292;&#20854;&#20013;$\alpha_{0}\in (0,1)$&#26159;&#23884;&#20837;&#25351;&#25968;&#65292;&#26159;&#19968;&#20010;&#20381;&#36182;&#20110;$\mathcal{H}$&#30340;&#24120;&#25968;&#12290;KRR&#26159;&#21542;&#23545;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;$\mathcal{H}$&#26159;Sobolev RKHS&#30340;&#26102;&#20505;&#65292;KRR&#23545;&#20110;&#20219;&#20309;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the misspecified kernel ridge regression problem, researchers usually assume the underground true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, a less-smooth interpolation space of a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ for some $s\in (0,1)$. The existing minimax optimal results require $\|f_{\rho}^{*}\|_{L^{\infty}}&lt;\infty$ which implicitly requires $s &gt; \alpha_{0}$ where $\alpha_{0}\in (0,1)$ is the embedding index, a constant depending on $\mathcal{H}$. Whether the KRR is optimal for all $s\in (0,1)$ is an outstanding problem lasting for years. In this paper, we show that KRR is minimax optimal for any $s\in (0,1)$ when the $\mathcal{H}$ is a Sobolev RKHS.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#35270;&#21548;&#23398;&#20064;&#65288;VAVL&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#24773;&#24863;&#22238;&#24402;&#21644;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#21363;&#20351;&#25968;&#25454;&#32570;&#22833;&#25110;&#19981;&#21305;&#37197;&#20063;&#33021;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#21644;&#20999;&#25442;&#12290;</title><link>http://arxiv.org/abs/2305.07216</link><description>&lt;p&gt;
&#22788;&#29702;&#24773;&#24863;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#36890;&#29992;&#35270;&#21548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks. (arXiv:2305.07216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#35270;&#21548;&#23398;&#20064;&#65288;VAVL&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#24773;&#24863;&#22238;&#24402;&#21644;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#21363;&#20351;&#25968;&#25454;&#32570;&#22833;&#25110;&#19981;&#21305;&#37197;&#20063;&#33021;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#21644;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#32570;&#20047;&#23454;&#38469;&#24212;&#29992;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#27169;&#24577;&#21487;&#29992;&#65292;&#20063;&#21487;&#20197;&#20114;&#25442;&#22320;&#23454;&#29616;&#39044;&#27979;&#24773;&#24863;&#23646;&#24615;&#25110;&#35782;&#21035;&#20998;&#31867;&#24773;&#24863;&#12290;&#22312;&#19968;&#20010;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#20013;&#23454;&#29616;&#36825;&#26679;&#30340;&#28789;&#27963;&#24615;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#20934;&#30830;&#35299;&#37322;&#21644;&#25972;&#21512;&#21508;&#31181;&#25968;&#25454;&#26469;&#28304;&#26159;&#22256;&#38590;&#30340;&#12290;&#21516;&#26102;&#65292;&#20801;&#35768;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30452;&#25509;&#20999;&#25442;&#65292;&#21516;&#26102;&#22788;&#29702;&#32570;&#22833;&#25110;&#37096;&#20998;&#20449;&#24687;&#20063;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#24773;&#24863;&#22238;&#24402;&#21644;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#30340;&#36890;&#29992;&#35270;&#21548;&#23398;&#20064;&#65288;VAVL&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22788;&#29702;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#38899;&#35270;&#39057;&#26694;&#26550;&#65292;&#21363;&#20351;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#19981;&#21305;&#37197;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current audio-visual emotion recognition models lack the flexibility needed for deployment in practical applications. We envision a multimodal system that works even when only one modality is available and can be implemented interchangeably for either predicting emotional attributes or recognizing categorical emotions. Achieving such flexibility in a multimodal emotion recognition system is difficult due to the inherent challenges in accurately interpreting and integrating varied data sources. It is also a challenge to robustly handle missing or partial information while allowing direct switch between regression and classification tasks. This study proposes a \emph{versatile audio-visual learning} (VAVL) framework for handling unimodal and multimodal systems for emotion regression and emotion classification tasks. We implement an audio-visual framework that can be trained even when audio and visual paired data is not available for part of the training set (i.e., audio only or only
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#27969;&#24418;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#37325;&#26032;&#24605;&#32771;k-means&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#26816;&#27979;&#25968;&#25454;&#30340;&#32858;&#31867;&#32780;&#19981;&#38656;&#35201;&#22343;&#20540;&#20272;&#35745;&#65292;&#24182;&#19988;&#33021;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#35270;&#22270;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.07213</link><description>&lt;p&gt;
&#20174;&#27969;&#24418;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;k-means
&lt;/p&gt;
&lt;p&gt;
Rethinking k-means from manifold learning perspective. (arXiv:2305.07213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#27969;&#24418;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#37325;&#26032;&#24605;&#32771;k-means&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#26816;&#27979;&#25968;&#25454;&#30340;&#32858;&#31867;&#32780;&#19981;&#38656;&#35201;&#22343;&#20540;&#20272;&#35745;&#65292;&#24182;&#19988;&#33021;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#35270;&#22270;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#32858;&#31867;&#31639;&#27861;&#65292;&#20294;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#21033;&#29992;k-means&#25216;&#26415;&#26469;&#26816;&#27979;&#25968;&#25454;&#28857;&#30340;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;k-means&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32858;&#31867;&#20013;&#24515;&#30340;&#20272;&#35745;&#65292;&#36825;&#24456;&#38590;&#36798;&#21040;&#26368;&#20248;&#35299;&#12290;&#21478;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23545;&#22122;&#22768;&#21644;&#24322;&#24120;&#25968;&#25454;&#25935;&#24863;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20174;&#27969;&#24418;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;k-means&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#26816;&#27979;&#25968;&#25454;&#30340;&#32858;&#31867;&#32780;&#19981;&#38656;&#35201;&#22343;&#20540;&#20272;&#35745;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;Butterworth&#28388;&#27874;&#22120;&#26500;&#36896;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#30697;&#38453;&#65292;&#20351;&#24471;&#21516;&#19968;&#32858;&#31867;&#20013;&#20219;&#24847;&#20004;&#20010;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#31561;&#20110;&#19968;&#20010;&#23567;&#24120;&#25968;&#65292;&#21516;&#26102;&#22686;&#21152;&#26469;&#33258;&#19981;&#21516;&#32858;&#31867;&#30340;&#20854;&#20182;&#25968;&#25454;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#23884;&#20837;&#22312;&#19981;&#21516;&#35270;&#22270;&#20013;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#25105;&#20204;&#22312;&#30001;&#25351;&#31034;&#22120;&#30697;&#38453;&#32452;&#25104;&#30340;&#19977;&#38454;&#24352;&#37327;&#19978;&#21033;&#29992;&#20102;&#24352;&#37327;Schatten p-&#33539;&#25968;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although numerous clustering algorithms have been developed, many existing methods still leverage k-means technique to detect clusters of data points. However, the performance of k-means heavily depends on the estimation of centers of clusters, which is very difficult to achieve an optimal solution. Another major drawback is that it is sensitive to noise and outlier data. In this paper, from manifold learning perspective, we rethink k-means and present a new clustering algorithm which directly detects clusters of data without mean estimation. Specifically, we construct distance matrix between data points by Butterworth filter such that distance between any two data points in the same clusters equals to a small constant, while increasing the distance between other data pairs from different clusters. To well exploit the complementary information embedded in different views, we leverage the tensor Schatten p-norm regularization on the 3rd-order tensor which consists of indicator matrices 
&lt;/p&gt;</description></item><item><title>MEGABYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;Transformer&#30340;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#23383;&#33410;&#30340;&#24207;&#21015;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#24314;&#27169;&#65292;&#22312;&#35757;&#32451;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#26080;&#38656;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07185</link><description>&lt;p&gt;
MEGABYTE: &#22522;&#20110;&#22810;&#23610;&#24230;Transformer&#30340;&#30334;&#19975;&#23383;&#33410;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers. (arXiv:2305.07185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07185
&lt;/p&gt;
&lt;p&gt;
MEGABYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;Transformer&#30340;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#23383;&#33410;&#30340;&#24207;&#21015;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#24314;&#27169;&#65292;&#22312;&#35757;&#32451;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#26080;&#38656;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#22312;&#30701;&#24207;&#21015;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12289;&#25773;&#23458;&#12289;&#20195;&#30721;&#25110;&#22270;&#20070;&#31561;&#38271;&#24207;&#21015;&#30340;&#22788;&#29702;&#33021;&#21147;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Megabyte&#65292;&#19968;&#31181;&#22810;&#23610;&#24230;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#23383;&#33410;&#30340;&#24207;&#21015;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#24314;&#27169;&#12290;Megabyte&#23558;&#24207;&#21015;&#20998;&#20026;&#22270;&#22359;&#65292;&#24182;&#22312;&#22270;&#22359;&#20869;&#20351;&#29992;&#23616;&#37096;&#23376;&#27169;&#22411;&#65292;&#22312;&#22270;&#22359;&#20043;&#38388;&#20351;&#29992;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#23376;&#20108;&#27425;&#33258;&#27880;&#24847;&#12289;&#26356;&#22823;&#30340;&#21069;&#39304;&#23618;&#21644;&#26356;&#22909;&#30340;&#35299;&#30721;&#24182;&#34892;&#24615;&#24471;&#20197;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#29983;&#25104;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Megabyte&#21487;&#20197;&#20351;&#22522;&#20110;&#23383;&#33410;&#30340;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#19982;&#22522;&#20110;&#23376;&#35789;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#22312;ImageNet&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#23494;&#24230;&#20272;&#35745;&#65292;&#21487;&#20197;&#27169;&#25311;&#26469;&#33258;&#21407;&#22987;&#25991;&#20214;&#30340;&#38899;&#39057;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#26080;&#38656;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;UNSR&#30340;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#65292;&#32531;&#35299;&#20102;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23616;&#37096;&#21487;&#35266;&#23519;&#24615;&#38382;&#39064;&#65292;&#37319;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#20379;&#26377;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07182</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#20803;&#32423;&#27880;&#24847;&#21147;&#29366;&#24577;&#34920;&#31034;&#30340;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#25552;&#21319;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Value Decomposition via Unit-Wise Attentive State Representation for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2305.07182v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;UNSR&#30340;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#65292;&#32531;&#35299;&#20102;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23616;&#37096;&#21487;&#35266;&#23519;&#24615;&#38382;&#39064;&#65292;&#37319;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#20379;&#26377;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#29615;&#22659;&#30340;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20250;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#36825;&#23601;&#23545;&#22914;&#20309;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#24471;&#20986;&#32039;&#20945;&#30340;&#28508;&#22312;&#34920;&#31034;&#20197;&#25552;&#21319;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#25552;&#20986;&#20102;&#38590;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21333;&#20803;&#32423;&#27880;&#24847;&#21147;&#29366;&#24577;&#34920;&#31034;&#65288;UNSR&#65289;&#65292;&#32531;&#35299;&#20102;&#23616;&#37096;&#21487;&#35266;&#23519;&#24615;&#24182;&#26377;&#25928;&#20419;&#36827;&#21327;&#21516;&#24615;&#12290;&#22312;UNSR&#20013;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#32780;&#35299;&#32806;&#30340;&#21333;&#20803;&#32423;&#29366;&#24577;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#30001;Transformer&#22359;&#36755;&#20986;&#65292;&#24182;&#29983;&#25104;&#20854;&#23616;&#37096;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;&#25552;&#20986;&#30340;UNSR&#29992;&#20110;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21319;&#20215;&#20540;&#20998;&#35299;&#65292;&#20197;&#22312;&#28151;&#21512;&#32593;&#32476;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#20026;&#20010;&#20307;&#20540;&#20989;&#25968;&#21644;&#32852;&#21512;&#20540;&#20989;&#25968;&#20043;&#38388;&#30340;&#26377;&#25928;&#25512;&#29702;&#36335;&#24452;&#25552;&#20379;&#25903;&#25345;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cooperative multi-agent reinforcement learning (MARL), the environmental stochasticity and uncertainties will increase exponentially when the number of agents increases, which puts hard pressure on how to come up with a compact latent representation from partial observation for boosting value decomposition. To tackle these issues, we propose a simple yet powerful method that alleviates partial observability and efficiently promotes coordination by introducing the UNit-wise attentive State Representation (UNSR). In UNSR, each agent learns a compact and disentangled unit-wise state representation outputted from transformer blocks, and produces its local action-value function. The proposed UNSR is used to boost the value decomposition with a multi-head attention mechanism for producing efficient credit assignment in the mixing network, providing an efficient reasoning path between the individual value function and joint value function. Experimental results demonstrate that our method a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#19981;&#26029;&#22256;&#38590;&#36127;&#26679;&#26412;&#30340;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#20197;&#33719;&#24471;&#26356;&#20855;&#21028;&#21035;&#33021;&#21147;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#36991;&#20813;&#20135;&#29983;&#19981;&#26399;&#26395;&#25110;&#19981;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2305.07176</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19981;&#26029;&#22256;&#38590;&#36127;&#26679;&#26412;&#23454;&#29616;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automatic Radiology Report Generation by Learning with Increasingly Hard Negatives. (arXiv:2305.07176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#19981;&#26029;&#22256;&#38590;&#36127;&#26679;&#26412;&#30340;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#20197;&#33719;&#24471;&#26356;&#20855;&#21028;&#21035;&#33021;&#21147;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#36991;&#20813;&#20135;&#29983;&#19981;&#26399;&#26395;&#25110;&#19981;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#21307;&#23398;&#22270;&#20687;&#25110;&#25253;&#21578;&#36890;&#24120;&#30001;&#20110;&#35299;&#21078;&#32467;&#26500;&#30340;&#30456;&#21516;&#20869;&#23481;&#32780;&#30456;&#20284;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#20010;&#20307;&#22270;&#20687;&#30340;&#29420;&#29305;&#24615;&#65292;&#23481;&#26131;&#20135;&#29983;&#19981;&#26399;&#26395;&#30340;&#36890;&#29992;&#25110;&#19981;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21306;&#20998;&#26368;&#25509;&#36817;&#30340;&#36127;&#26679;&#26412;&#65292;&#21363;&#22256;&#38590;&#36127;&#26679;&#26412;&#26469;&#23398;&#20064;&#21028;&#21035;&#22270;&#20687;&#21644;&#25253;&#21578;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#20026;&#20102;&#33719;&#24471;&#26356;&#20855;&#21028;&#21035;&#33021;&#21147;&#30340;&#29305;&#24449;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#36880;&#28176;&#25552;&#39640;&#36825;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#20026;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#21019;&#24314;&#36234;&#26469;&#36234;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#19981;&#26029;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#35270;&#20026;&#36741;&#21161;&#21464;&#37327;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#21046;&#23450;&#20026;&#19968;&#20010;&#26368;&#23567;-&#26368;&#22823;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Automatic radiology report generation is challenging as medical images or reports are usually similar to each other due to the common content of anatomy. This makes a model hard to capture the uniqueness of individual images and is prone to producing undesired generic or mismatched reports. This situation calls for learning more discriminative features that could capture even fine-grained mismatches between images and reports. To achieve this, this paper proposes a novel framework to learn discriminative image and report features by distinguishing them from their closest peers, i.e., hard negatives. Especially, to attain more discriminative features, we gradually raise the difficulty of such a learning task by creating increasingly hard negative reports for each image in the feature space during training, respectively. By treating the increasingly hard negatives as auxiliary variables, we formulate this process as a min-max alternating optimisation problem. At each iteration, condition
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#20808;&#22238;&#25918;&#35757;&#32451;&#12289;&#30456;&#23545;&#36793;&#32536;&#27969;&#12289;&#24341;&#23548;&#36712;&#36857;&#24179;&#34913;&#30446;&#26631;&#65292;&#24182;&#22312;&#22823;&#24133;&#24230;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#20195;&#29702;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07170</link><description>&lt;p&gt;
&#20851;&#20110;&#29702;&#35299;&#21644;&#25913;&#21892;GFlowNet&#35757;&#32451;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding and Improving GFlowNet Training. (arXiv:2305.07170v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07170
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#20808;&#22238;&#25918;&#35757;&#32451;&#12289;&#30456;&#23545;&#36793;&#32536;&#27969;&#12289;&#24341;&#23548;&#36712;&#36857;&#24179;&#34913;&#30446;&#26631;&#65292;&#24182;&#22312;&#22823;&#24133;&#24230;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#20195;&#29702;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31867;&#31639;&#27861;&#65292;&#23398;&#20064;&#29983;&#25104;&#31574;&#30053;&#20197;&#20174;&#38750;&#36127;&#22870;&#21169;x&#30340;&#30446;&#26631;&#20998;&#24067;$p^*&#65288;x&#65289;\propto R&#65288;x&#65289;$&#20013;&#37319;&#26679;&#31163;&#25955;&#23545;&#35937;&#12290;&#23398;&#20064;&#30446;&#26631;&#20445;&#35777;GFlowNet&#23545;&#25152;&#26377;&#29366;&#24577;&#25110;&#36712;&#36857;&#20840;&#23616;&#26368;&#23567;&#21270;&#25439;&#22833;&#26102;&#65292;&#20174;&#30446;&#26631;&#20998;&#24067;$p^*&#65288;x&#65289;$&#20013;&#37319;&#26679;x&#65292;&#20294;&#22312;&#35757;&#32451;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#24615;&#33021;&#22914;&#20309;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#23558;&#25152;&#23398;&#30340;&#37319;&#26679;&#20998;&#24067;&#19982;&#30446;&#26631;&#22870;&#21169;&#20998;&#24067;&#36827;&#34892;&#27604;&#36739;&#12290;&#30001;&#20110;&#27969;&#22312;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#26102;&#21487;&#33021;&#34987;&#27424;&#23450;&#65292;&#22240;&#27492;&#25105;&#20204;&#38416;&#26126;&#20102;&#23398;&#20064;&#27969;&#23545;&#20110;&#23454;&#36341;&#20013;&#30340;&#27867;&#21270;&#21644;&#21305;&#37197;$p^*&#65288;x&#65289;$&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23398;&#20064;&#26356;&#22909;&#30340;&#27969;&#65292;&#24182;&#25552;&#20986;&#20102;&#65288;i&#65289;&#20248;&#20808;&#22238;&#25918;&#35757;&#32451;&#39640;&#22870;&#21169;$x$&#65292;&#65288;ii&#65289;&#30456;&#23545;&#36793;&#32536;&#27969;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#21644;&#65288;iii&#65289;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#36712;&#36857;&#24179;&#34913;&#30446;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#35299;&#20915;&#23376;&#32467;&#26500;&#23398;&#20998;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#26174;&#30528;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#29702;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative flow networks (GFlowNets) are a family of algorithms that learn a generative policy to sample discrete objects $x$ with non-negative reward $R(x)$. Learning objectives guarantee the GFlowNet samples $x$ from the target distribution $p^*(x) \propto R(x)$ when loss is globally minimized over all states or trajectories, but it is unclear how well they perform with practical limits on training resources. We introduce an efficient evaluation strategy to compare the learned sampling distribution to the target reward distribution. As flows can be underdetermined given training data, we clarify the importance of learned flows to generalization and matching $p^*(x)$ in practice. We investigate how to learn better flows, and propose (i) prioritized replay training of high-reward $x$, (ii) relative edge flow policy parametrization, and (iii) a novel guided trajectory balance objective, and show how it can solve a substructure credit assignment problem. We substantially improve sample e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#26694;&#26550;OneCAD&#65292;&#36890;&#36807;Mask-Image-Modeling(MIM)&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#35299;&#20915;&#20102;&#24403;&#21069;&#26550;&#26500;(&#22914;ViTs&#21644;CNNs)&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#19988;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;DNN&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07167</link><description>&lt;p&gt;
OneCAD: &#20351;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#21333;&#20998;&#31867;&#22120;&#36866;&#29992;&#20110;&#25152;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OneCAD: One Classifier for All image Datasets using multimodal learning. (arXiv:2305.07167v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#26694;&#26550;OneCAD&#65292;&#36890;&#36807;Mask-Image-Modeling(MIM)&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#35299;&#20915;&#20102;&#24403;&#21069;&#26550;&#26500;(&#22914;ViTs&#21644;CNNs)&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#19988;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;DNN&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;(ViTs)&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#12290;&#36825;&#20123;&#27169;&#22411;&#26550;&#26500;&#20381;&#36182;&#20110;&#23427;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#20013;&#31867;&#21035;&#25968;&#30340;&#25968;&#37327;&#12290;&#31867;&#21035;&#25968;&#30340;&#20219;&#20309;&#25913;&#21464;&#37117;&#20250;&#23548;&#33268;&#27169;&#22411;&#26550;&#26500;&#30340;&#25913;&#21464;(&#37096;&#20998;&#25110;&#20840;&#37096;)&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#21019;&#24314;&#19968;&#20010;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;&#27169;&#22411;&#26550;&#26500;&#65311;&#36825;&#26679;&#21487;&#20197;&#20351;&#27169;&#22411;&#26550;&#26500;&#19982;&#20854;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#26080;&#20851;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#24403;&#21069;&#26550;&#26500;(ViTs&#21644;CNNs)&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35757;&#32451;&#21644;&#25512;&#26029;&#26694;&#26550;- OneCAD(&#36866;&#29992;&#20110;&#25152;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#21333;&#20998;&#31867;&#22120;)&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;Mask-Image-Modeling(MIM)&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;DNN&#27169;&#22411;&#26550;&#26500;&#30340;&#24037;&#20316;&#12290;&#21021;&#27493;&#32467;&#26524;&#24050;&#22312;&#33258;&#28982;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Transformers (ViTs) and Convolutional neural networks (CNNs) are widely used Deep Neural Networks (DNNs) for classification task. These model architectures are dependent on the number of classes in the dataset it was trained on. Any change in number of classes leads to change (partial or full) in the model's architecture. This work addresses the question: Is it possible to create a number-of-class-agnostic model architecture?. This allows model's architecture to be independent of the dataset it is trained on. This work highlights the issues with the current architectures (ViTs and CNNs). Also, proposes a training and inference framework OneCAD (One Classifier for All image Datasets) to achieve close-to number-of-class-agnostic transformer model. To best of our knowledge this is the first work to use Mask-Image-Modeling (MIM) with multimodal learning for classification task to create a DNN model architecture agnostic to the number of classes. Preliminary results are shown on natu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#25968;&#25454;&#21253;&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#32593;&#32476;&#32531;&#20914;&#21306;&#31649;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07164</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#25968;&#25454;&#21253;&#35843;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Online Packet Scheduling with Deadlines. (arXiv:2305.07164v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#25968;&#25454;&#21253;&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#32593;&#32476;&#32531;&#20914;&#21306;&#31649;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#32593;&#32476;&#30340;&#30446;&#26631;&#26159;&#20248;&#20808;&#22788;&#29702;&#20851;&#38190;&#27969;&#37327;&#24182;&#26377;&#25928;&#22320;&#31649;&#29702;&#27969;&#37327;&#12290;&#36825;&#38656;&#35201;&#36866;&#24403;&#30340;&#32531;&#20914;&#21306;&#31649;&#29702;&#20197;&#38450;&#27490;&#20002;&#22833;&#37325;&#35201;&#27969;&#37327;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#38750;&#20851;&#38190;&#27969;&#37327;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#25511;&#21046;&#27599;&#27493;&#35201;&#20256;&#36755;&#21738;&#20123;&#25968;&#25454;&#21253;&#12289;&#21738;&#20123;&#25968;&#25454;&#21253;&#35201;&#20002;&#24323;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#25968;&#25454;&#21253;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#24212;&#23545;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#24403;&#39044;&#27979;&#35823;&#24046;&#24456;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20250;&#25552;&#39640;&#31454;&#20105;&#27604;&#29575;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#26377;&#30028;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modern network aims to prioritize critical traffic over non-critical traffic and effectively manage traffic flow. This necessitates proper buffer management to prevent the loss of crucial traffic while minimizing the impact on non-critical traffic. Therefore, the algorithm's objective is to control which packets to transmit and which to discard at each step. In this study, we initiate the learning-augmented online packet scheduling with deadlines and provide a novel algorithmic framework to cope with the prediction. We show that when the prediction error is small, our algorithm improves the competitive ratio while still maintaining a bounded competitive ratio, regardless of the prediction error.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21387;&#32553;&#32452;&#32455;&#23398;&#22270;&#20687;&#24182;&#20445;&#30041;&#26356;&#26377;&#24847;&#20041;&#34920;&#24449;&#30340;&#33258;&#32534;&#30721;&#22120;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20840;&#32452;&#32455;&#20999;&#29255;&#22270;&#20687;&#12290;&#27979;&#35797;&#32467;&#26524;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07161</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20840;&#32452;&#32455;&#20999;&#29255;&#32452;&#32455;&#23398;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#19982;&#20998;&#31867;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning-based Compression and Classification Technique for Whole Slide Histopathology Images. (arXiv:2305.07161v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21387;&#32553;&#32452;&#32455;&#23398;&#22270;&#20687;&#24182;&#20445;&#30041;&#26356;&#26377;&#24847;&#20041;&#34920;&#24449;&#30340;&#33258;&#32534;&#30721;&#22120;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20840;&#32452;&#32455;&#20999;&#29255;&#22270;&#20687;&#12290;&#27979;&#35797;&#32467;&#26524;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#21387;&#32553;&#32452;&#32455;&#23398;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#26356;&#23494;&#38598;&#12289;&#26356;&#26377;&#24847;&#20041;&#30340;&#34920;&#24449;&#12290;&#24403;&#21069;&#25913;&#36827;&#21387;&#32553;&#31639;&#27861;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20801;&#35768;&#26356;&#20302;&#30340;&#21306;&#22495;&#24863;&#20852;&#36259;&#65288;ROI&#65289;&#21387;&#32553;&#29575;&#30340;&#26041;&#27861;&#19978;&#12290;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#34920;&#24449;&#65292;&#22240;&#27492;&#33021;&#22815;&#36873;&#25321;&#35201;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#32771;&#34385;&#30340;&#21306;&#22495;&#12290;&#26412;&#25991;&#20851;&#27880;&#20840;&#32452;&#32455;&#20999;&#29255;&#32452;&#32455;&#23398;&#22270;&#20687;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#20351;&#26377;&#30417;&#30563;&#30340;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#20197;&#26356;&#23494;&#38598;&#12289;&#26356;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#20445;&#30041;&#36755;&#20837;&#32452;&#32455;&#23398;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#30417;&#30563;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#27979;&#35797;&#21387;&#32553;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20102;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an autoencoder-based neural network architecture to compress histopathological images while retaining the denser and more meaningful representation of the original images. Current research into improving compression algorithms is focused on methods allowing lower compression rates for Regions of Interest (ROI-based approaches). Neural networks are great at extracting meaningful semantic representations from images, therefore are able to select the regions to be considered of interest for the compression process. In this work, we focus on the compression of whole slide histopathology images. The objective is to build an ensemble of neural networks that enables a compressive autoencoder in a supervised fashion to retain a denser and more meaningful representation of the input histology images. Our proposed system is a simple and novel method to supervise compressive neural networks. We test the compressed images using transfer learning-based classifiers and show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#23454;&#20363;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;ANN&#12289;RFC&#21644;SVM&#39044;&#27979;&#28183;&#36879;&#29575;&#26354;&#32447;&#65292;&#24182;&#23558;&#20854;&#19982;&#23721;&#24515;&#25968;&#25454;&#30456;&#21305;&#37197;&#65292;&#32467;&#26524;&#34920;&#26126;FZI&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25913;&#21892;&#20648;&#23618;&#39044;&#27979;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.07145</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#23721;&#30707;&#29289;&#29702;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65306;&#24322;&#36136;&#20648;&#23618;&#28183;&#36879;&#29575;&#39044;&#27979;&#23454;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Petrophysical Studies with Machine Learning: A Field Case Study on Permeability Prediction in Heterogeneous Reservoirs. (arXiv:2305.07145v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#23454;&#20363;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;ANN&#12289;RFC&#21644;SVM&#39044;&#27979;&#28183;&#36879;&#29575;&#26354;&#32447;&#65292;&#24182;&#23558;&#20854;&#19982;&#23721;&#24515;&#25968;&#25454;&#30456;&#21305;&#37197;&#65292;&#32467;&#26524;&#34920;&#26126;FZI&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25913;&#21892;&#20648;&#23618;&#39044;&#27979;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#24322;&#36136;&#20648;&#23618;&#22320;&#23618;&#20013;&#20934;&#30830;&#39044;&#27979;&#23721;&#30707;&#29289;&#29702;&#24615;&#36136;&#30340;&#25361;&#25112;&#65292;&#35813;&#38382;&#39064;&#20250;&#20005;&#37325;&#24433;&#21709;&#20648;&#23618;&#24615;&#33021;&#39044;&#27979;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65288;RFC&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20197;&#39044;&#27979;&#24120;&#35268;&#27979;&#20117;&#25968;&#25454;&#20013;&#30340;&#28183;&#36879;&#29575;&#26354;&#32447;&#65292;&#24182;&#23558;&#20854;&#19982;&#23721;&#24515;&#25968;&#25454;&#30456;&#21305;&#37197;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#27604;&#36739;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#28183;&#36879;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#26368;&#20339;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#21033;&#29992;&#27969;&#21160;&#21306;&#24102;&#25351;&#25968;&#65288;FZI&#65289;&#23721;&#30707;&#20998;&#31867;&#25216;&#26415;&#26469;&#20102;&#35299;&#24433;&#21709;&#20648;&#38598;&#23618;&#36136;&#37327;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#23558;&#29992;&#20110;&#25913;&#36827;&#20648;&#23618;&#27169;&#25311;&#65292;&#26356;&#31934;&#30830;&#22320;&#23450;&#20301;&#26410;&#26469;&#30340;&#20117;&#20301;&#12290;&#30740;&#31350;&#24471;&#20986;&#32467;&#35770;&#65292;FZI&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#28183;&#36879;&#29575;&#26354;&#32447;&#21644;&#25913;&#21892;&#20648;&#23618;&#39044;&#27979;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This field case study aims to address the challenge of accurately predicting petrophysical properties in heterogeneous reservoir formations, which can significantly impact reservoir performance predictions. The study employed three machine learning algorithms, namely Artificial Neural Network (ANN), Random Forest Classifier (RFC), and Support Vector Machine (SVM), to predict permeability log from conventional logs and match it with core data. The primary objective of this study was to compare the effectiveness of the three machine learning algorithms in predicting permeability and determine the optimal prediction method. The study utilized the Flow Zone Indicator (FZI) rock typing technique to understand the factors influencing reservoir quality. The findings will be used to improve reservoir simulation and locate future wells more accurately. The study concluded that the FZI approach and machine learning algorithms are effective in predicting permeability log and improving reservoir p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptARC&#65292;&#38024;&#23545;ARC&#39046;&#22495;&#30340;&#25277;&#35937;&#21644;&#25512;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25277;&#35937;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.07141</link><description>&lt;p&gt;
ConceptARC&#22522;&#20934;&#65306;&#35780;&#20272;ARC&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain. (arXiv:2305.07141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptARC&#65292;&#38024;&#23545;ARC&#39046;&#22495;&#30340;&#25277;&#35937;&#21644;&#25512;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25277;&#35937;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#25104;&#21644;&#25277;&#35937;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20851;&#38190;&#65292;&#20294;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#27424;&#32570;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#20851;&#20110;&#27010;&#24565;&#25277;&#35937;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#29702;&#24819;&#21270;&#30340;&#39046;&#22495;&#65292;&#22914;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#21644;Bongard&#38382;&#39064;&#65292;&#20294;&#21363;&#20351;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25104;&#21151;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#29702;&#35299;&#24773;&#20917;&#20063;&#24456;&#23569;&#34987;&#35780;&#20272;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#38024;&#23545;&#25277;&#35937;&#21644;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;ARC&#65289;&#30340;&#28145;&#20837;&#35780;&#20272;&#22522;&#20934;&#65292;ARC&#26159;Chollet [2019]&#24320;&#21457;&#30340;&#19968;&#32452;&#23569;&#37327;&#25277;&#35937;&#21644;&#31867;&#27604;&#38382;&#39064;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;ConceptARC&#30340;&#26032;&#30340;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;ARC&#22522;&#20934;&#65292;&#23427;&#22312;&#35768;&#22810;&#22522;&#26412;&#31354;&#38388;&#21644;&#35821;&#20041;&#27010;&#24565;&#19978;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#25277;&#35937;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#19982;&#21407;&#22987;&#30340;ARC&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;ConceptARC&#29305;&#21035;&#22260;&#32469;&#8220;&#27010;&#24565;&#32452;&#8221;&#36827;&#34892;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture.  In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around "concept groups" -- se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#20248;&#36755;&#36816;&#26694;&#26550;&#30340;&#22270;&#24418;&#27010;&#25324;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#33410;&#28857;&#12289;&#36793;&#32536;&#21644;&#23646;&#24615;&#37325;&#35201;&#24615;&#32435;&#20837;&#27010;&#25324;&#36807;&#31243;&#20013;&#12290;&#20026;&#35299;&#20915;&#21463;&#30417;&#30563;&#22270;&#24418;&#27010;&#25324;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#20854;&#21046;&#23450;&#20026;&#26368;&#22823;&#21270;&#25152;&#27010;&#25324;&#22270;&#24418;&#19982;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#39321;&#20892;&#20114;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#20102;&#20854;&#36817;&#20284;&#30340;NP&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07138</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#35770;&#24230;&#37327;&#30340;&#21463;&#30417;&#30563;&#26368;&#20248;&#36755;&#36816;&#22270;&#24418;&#27010;&#25324;&#30340;&#25215;&#35834;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Promise and Limitations of Supervised Optimal Transport-Based Graph Summarization via Information Theoretic Measures. (arXiv:2305.07138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#20248;&#36755;&#36816;&#26694;&#26550;&#30340;&#22270;&#24418;&#27010;&#25324;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#33410;&#28857;&#12289;&#36793;&#32536;&#21644;&#23646;&#24615;&#37325;&#35201;&#24615;&#32435;&#20837;&#27010;&#25324;&#36807;&#31243;&#20013;&#12290;&#20026;&#35299;&#20915;&#21463;&#30417;&#30563;&#22270;&#24418;&#27010;&#25324;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#20854;&#21046;&#23450;&#20026;&#26368;&#22823;&#21270;&#25152;&#27010;&#25324;&#22270;&#24418;&#19982;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#39321;&#20892;&#20114;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#20102;&#20854;&#36817;&#20284;&#30340;NP&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#27010;&#25324;&#26159;&#23558;&#36755;&#20837;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#20135;&#29983;&#26356;&#23567;&#30340;&#22270;&#24418;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#20197;&#20351;&#36739;&#23567;&#30340;&#21387;&#32553;&#22270;&#24418;&#23545;&#19979;&#28216;&#20219;&#21153;&#25429;&#33719;&#30456;&#20851;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#26368;&#36817;&#26377;&#19968;&#31181;&#22270;&#24418;&#27010;&#25324;&#26041;&#27861;&#65292;&#23427;&#21046;&#23450;&#20102;&#19968;&#31181;&#26368;&#20248;&#36755;&#36816;&#26694;&#26550;&#65292;&#20801;&#35768;&#23558;&#26377;&#20851;&#33410;&#28857;&#12289;&#36793;&#32536;&#21644;&#23646;&#24615;&#37325;&#35201;&#24615;&#65288;&#22312;&#35813;&#24037;&#20316;&#20013;&#26410;&#23450;&#20041;&#65289;&#30340;&#20808;&#21069;&#20449;&#24687;&#32435;&#20837;&#22270;&#24418;&#27010;&#25324;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#26694;&#26550;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#25105;&#20204;&#30693;&#36947;&#30340;&#24456;&#23569;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#21463;&#30417;&#30563;&#30340;&#22270;&#24418;&#27010;&#25324;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#35770;&#24230;&#37327;&#65292;&#25105;&#20204;&#23547;&#27714;&#20445;&#30041;&#19982;&#31867;&#26631;&#31614;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20174;&#29702;&#35770;&#19978;&#30475;&#24453;&#21463;&#30417;&#30563;&#27010;&#25324;&#38382;&#39064;&#26412;&#36523;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#21046;&#23450;&#20026;&#26368;&#22823;&#21270;&#25152;&#27010;&#25324;&#22270;&#24418;&#19982;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#39321;&#20892;&#20114;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36817;&#20284;&#30340;NP&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph summarization is the problem of producing smaller graph representations of an input graph dataset, in such a way that the smaller compressed graphs capture relevant structural information for downstream tasks. There is a recent graph summarization method that formulates an optimal transport-based framework that allows prior information about node, edge, and attribute importance (never defined in that work) to be incorporated into the graph summarization process. However, very little is known about the statistical properties of this framework. To elucidate this question, we consider the problem of supervised graph summarization, wherein by using information theoretic measures we seek to preserve relevant information about a class label. To gain a theoretical perspective on the supervised summarization problem itself, we first formulate it in terms of maximizing the Shannon mutual information between the summarized graph and the class label. We show an NP-hardness of approximation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#35299;&#37322;&#22120;&#35774;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07132</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#35299;&#20915;&#38899;&#39057;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Interpretability in Audio Classification Networks with Non-negative Matrix Factorization. (arXiv:2305.07132v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#35299;&#37322;&#22120;&#35774;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38899;&#39057;&#22788;&#29702;&#32593;&#32476;&#35299;&#37322;&#24615;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#20107;&#21518;&#21644;&#35774;&#35745;&#26102;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#20102;&#22788;&#29702;&#26041;&#27861;&#12290;&#38024;&#23545;&#20107;&#21518;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#37322;&#32593;&#32476;&#20915;&#31574;&#26041;&#38754;&#30340;&#39640;&#32423;&#38899;&#39057;&#23545;&#35937;&#65292;&#26368;&#32456;&#20026;&#32456;&#31471;&#29992;&#25143;&#25552;&#20379;&#21487;&#21548;&#30340;&#23545;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#35299;&#37322;&#22120;&#35774;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35299;&#37322;&#22120;&#36890;&#36807;&#23398;&#20064;&#39044;&#20808;&#23398;&#20064;NMF&#23383;&#20856;&#30340;&#26102;&#38388;&#28608;&#27963;&#20316;&#20026;&#30446;&#26631;&#32593;&#32476;&#38544;&#34255;&#23618;&#30340;&#35268;&#21017;&#21270;&#20013;&#38388;&#23884;&#20837;&#26469;&#29983;&#25104;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#29983;&#25104;&#30452;&#35266;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#35299;&#37322;&#65292;&#26126;&#30830;&#22686;&#24378;&#19982;&#32593;&#32476;&#20915;&#31574;&#26368;&#30456;&#20851;&#30340;&#36755;&#20837;&#20449;&#21495;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#21644;&#38899;&#20048;&#22810;&#26631;&#31614;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles two major problem settings for interpretability of audio processing networks, post-hoc and by-design interpretation. For post-hoc interpretation, we aim to interpret decisions of a network in terms of high-level audio objects that are also listenable for the end-user. This is extended to present an inherently interpretable model with high performance. To this end, we propose a novel interpreter design that incorporates non-negative matrix factorization (NMF). In particular, an interpreter is trained to generate a regularized intermediate embedding from hidden layers of a target network, learnt as time-activations of a pre-learnt NMF dictionary. Our methodology allows us to generate intuitive audio-based interpretations that explicitly enhance parts of the input signal most relevant for a network's decision. We demonstrate our method's applicability on a variety of classification tasks, including multi-label data for real-world audio and music.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;k-&#21311;&#21517;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#33021;&#28304;&#28040;&#32791;&#36739;&#23569;&#65292;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#33021;&#28304;&#28040;&#32791;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07116</link><description>&lt;p&gt;
k-&#21311;&#21517;&#21644;&#21512;&#25104;&#25968;&#25454;&#25216;&#26415;&#30340;&#33021;&#37327;&#25104;&#26412;&#21644;&#26426;&#22120;&#23398;&#20064;&#20934;&#30830;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques. (arXiv:2305.07116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;k-&#21311;&#21517;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#33021;&#28304;&#28040;&#32791;&#36739;&#23569;&#65292;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#33021;&#28304;&#28040;&#32791;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#19982;&#38544;&#31169;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#20851;&#30340;&#24840;&#21457;&#22686;&#38271;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#27431;&#30431;&#39041;&#24067;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#24182;&#25215;&#35834;&#20102;&#32511;&#33394;&#21327;&#35758;&#12290;&#22823;&#37327;&#30740;&#31350;&#25506;&#31350;&#20102;&#36816;&#29992;&#21311;&#21517;&#25968;&#25454;&#38598;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#25928;&#21644;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#25506;&#31350;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;k-&#21311;&#21517;&#12290;&#30001;&#20110;&#21512;&#25104;&#25968;&#25454;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#27492;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#20934;&#30830;&#24615;&#65306;a&#65289;&#23558;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#24212;&#29992;&#20110;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;b&#65289;&#22312;&#30456;&#20851;&#38544;&#31169;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65306;k-&#21311;&#21517;&#21270;&#65288;&#20351;&#29992;&#27867;&#21270;&#21644;&#25233;&#21046;&#65289;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#21450;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#27599;&#20010;&#27169;&#22411;&#37117;&#22312;&#27599;&#20010;&#38544;&#31169;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32463;&#36807;k-&#21311;&#21517;&#21270;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#28040;&#32791;&#30340;&#33021;&#37327;&#36739;&#23569;&#65292;&#19982;&#22312;&#38750;&#21311;&#21517;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;k-&#21311;&#21517;&#21270;&#36807;&#31243;&#20013;&#28040;&#32791;&#30340;&#33021;&#37327;&#38750;&#24120;&#21487;&#35266;&#65292;&#22312;&#35780;&#20272;&#20854;&#26377;&#29992;&#24615;&#26102;&#24517;&#39035;&#23558;&#20854;&#32771;&#34385;&#22312;&#20869;&#12290;&#21512;&#25104;&#25968;&#25454;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#22312;&#28040;&#32791;&#26356;&#23569;&#33021;&#28304;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#38750;&#21311;&#21517;&#21270;&#25968;&#25454;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on both the energy consumption and accuracy of the machine learning models, focusing on k-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on k-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#19968;&#31181;&#21516;&#26102;&#23558;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#21644;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#32467;&#21512;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#21033;&#29992;&#20960;&#20309;&#20449;&#24687;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07100</link><description>&lt;p&gt;
$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
$\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks. (arXiv:2305.07100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#19968;&#31181;&#21516;&#26102;&#23558;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#21644;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#32467;&#21512;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#21033;&#29992;&#20960;&#20309;&#20449;&#24687;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#22312;&#20960;&#20309;&#22270;&#24418;&#21644;&#28857;&#20113;&#19978;&#30340;&#26041;&#27861;&#65292;&#20854;&#31561;&#21464;&#20110;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#21453;&#23556;&#12290;EMPSNs&#21487;&#20197;&#23398;&#20064;&#22312;&#22270;&#24418;&#20013;&#30340;&#39640;&#32500;&#21333;&#32431;&#38754;&#65288;&#22914;&#19977;&#35282;&#24418;&#65289;&#65292;&#24182;&#20197;$\mathrm{E}(n)$&#31561;&#21464;&#26041;&#24335;&#21033;&#29992;&#26356;&#39640;&#32500;&#21333;&#32431;&#20307;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;EMPSNs&#21516;&#26102;&#23558;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#26356;&#21152;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#39046;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#20013;&#21253;&#21547;&#20960;&#20309;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EMPSNs&#21487;&#20197;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#21333;&#29420;&#20351;&#29992;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#65292;&#24615;&#33021;&#26377;&#20102;&#26222;&#36941;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#32500;&#25805;&#20316;&#20013;&#65292;&#21253;&#21547;&#20960;&#20309;&#20449;&#24687;&#26159;&#38450;&#27490;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#36807;&#24230;&#24179;&#28369;&#30340;&#26377;&#25928;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents $\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks (EMPSNs), a novel approach to learning on geometric graphs and point clouds that is equivariant to rotations, translations, and reflections. EMPSNs can learn high-dimensional simplex features in graphs (e.g. triangles), and use the increase of geometric information of higher-dimensional simplices in an $\mathrm{E}(n)$ equivariant fashion. EMPSNs simultaneously generalize $\mathrm{E}(n)$ Equivariant Graph Neural Networks to a topologically more elaborate counterpart and provide an approach for including geometric information in Message Passing Simplicial Networks. The results indicate that EMPSNs can leverage the benefits of both approaches, leading to a general increase in performance when compared to either method. Furthermore, the results suggest that incorporating geometric information serves as an effective measure against over-smoothing in message passing networks, especially when operating on high
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20135;&#29983;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#23545;&#20154;&#31867;&#26159;&#21542;&#26377;&#29992;&#65292;&#21457;&#29616;&#29616;&#26377;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#36828;&#20302;&#20110;&#29702;&#24819;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#20272;&#35745;&#29702;&#30001;&#22312;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.07095</link><description>&lt;p&gt;
&#26426;&#22120;&#29702;&#30001;&#23545;&#20154;&#31867;&#26159;&#21542;&#26377;&#29992;&#65311;&#35780;&#20272;&#21644;&#25552;&#39640;&#33258;&#28982;&#25991;&#26412;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales. (arXiv:2305.07095v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20135;&#29983;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#23545;&#20154;&#31867;&#26159;&#21542;&#26377;&#29992;&#65292;&#21457;&#29616;&#29616;&#26377;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#36828;&#20302;&#20110;&#29702;&#24819;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#20272;&#35745;&#29702;&#30001;&#22312;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#26174;&#30528;&#20986;&#29616;&#33021;&#21147;&#20013;&#65292;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#26159;&#20854;&#20013;&#20043;&#19968;&#65307;&#36229;&#36807;&#26576;&#20010;&#35268;&#27169;&#21518;&#65292;&#22823;&#22411;LMs&#33021;&#22815;&#29983;&#25104;&#30475;&#20284;&#26377;&#29992;&#30340;&#29702;&#30001;&#65292;&#36827;&#32780;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#23427;&#20204;&#22312;&#39046;&#23548;&#27036;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#29616;&#35937;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26426;&#22120;&#29983;&#25104;&#30340;&#29702;&#30001;&#26159;&#21542;&#20063;&#33021;&#23545;&#20154;&#31867;&#26377;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#26222;&#36890;&#20154;&#23581;&#35797;&#26681;&#25454;&#36825;&#20123;&#26426;&#22120;&#29702;&#30001;&#22238;&#31572;&#38382;&#39064;&#26102;&#65311;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#36828;&#26410;&#20196;&#20154;&#28385;&#24847;&#65292;&#24182;&#19988;&#26114;&#36149;&#30340;&#20154;&#31867;&#30740;&#31350;&#25165;&#33021;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#29983;&#25104;&#29702;&#30001;LM&#30340;&#20219;&#21153;&#34920;&#29616;&#25110;&#29983;&#25104;&#29702;&#30001;&#19982;&#40644;&#37329;&#29702;&#30001;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#26126;&#23427;&#20204;&#30340;&#20154;&#31867;&#25928;&#29992;&#12290;&#34429;&#28982;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29702;&#30001;&#30340;&#26576;&#20123;&#23646;&#24615;&#65292;&#22914;&#31616;&#27905;&#24615;&#21644;&#26032;&#39062;&#24615;&#65292;&#19982;&#23427;&#20204;&#30340;&#20154;&#31867;&#25928;&#29992;&#26377;&#20851;&#65292;&#20294;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#23427;&#20204;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20272;&#35745;&#29702;&#30001;&#22312;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond a certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory, and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales, or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale's helpfulness in ans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#24180;&#40836;&#36807;&#31243;&#65292;&#23558;Borkar-Meyn&#31283;&#23450;&#24615;&#23450;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#20219;&#24847;&#30697;&#30028;&#30340;&#20449;&#24687;&#24310;&#36831;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#36924;&#36817;&#20013;&#65292;&#24182;&#35752;&#35770;&#20102;&#20998;&#24067;&#24335;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#21644;&#20998;&#26512;SA&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07091</link><description>&lt;p&gt;
&#20855;&#26377;&#22823;&#22411;&#26080;&#30028;&#38543;&#26426;&#20449;&#24687;&#24310;&#36831;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#36924;&#36817;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability and Convergence of Distributed Stochastic Approximations with large Unbounded Stochastic Information Delays. (arXiv:2305.07091v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#24180;&#40836;&#36807;&#31243;&#65292;&#23558;Borkar-Meyn&#31283;&#23450;&#24615;&#23450;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#20219;&#24847;&#30697;&#30028;&#30340;&#20449;&#24687;&#24310;&#36831;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#36924;&#36817;&#20013;&#65292;&#24182;&#35752;&#35770;&#20102;&#20998;&#24067;&#24335;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#21644;&#20998;&#26512;SA&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;Borkar-Meyn&#31283;&#23450;&#24615;&#23450;&#29702;&#65288;BMT&#65289;&#25512;&#24191;&#21040;&#20855;&#26377;&#20219;&#24847;&#30697;&#30028;&#30340;&#20449;&#24687;&#24310;&#36831;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#20013;&#12290;&#20026;&#20102;&#27169;&#25311;&#24310;&#36831;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20449;&#24687;&#24180;&#40836;&#36807;&#31243;&#65288;AoIP&#65289;&#65306;&#20855;&#26377;&#21333;&#20301;&#22686;&#38271;&#23646;&#24615;&#30340;&#38750;&#36127;&#25972;&#25968;&#19978;&#30340;&#38543;&#26426;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;AoIP&#19981;&#33021;&#26080;&#38480;&#22320;&#36229;&#36807;&#20219;&#20309;&#26102;&#27573;&#25152;&#21344;&#27604;&#20363;&#12290;&#32467;&#21512;&#36866;&#24403;&#36873;&#25321;&#30340;&#27493;&#38271;&#65292;&#36825;&#20010;&#24615;&#36136;&#36275;&#20197;&#20445;&#35777;&#20998;&#24067;&#24335;SA&#30340;&#31283;&#23450;&#24615;&#12290;&#19982;BMT&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#38656;&#35201;&#20851;&#38190;&#20462;&#27491;&#21644;&#26032;&#30340;&#35770;&#35777;&#26041;&#24335;&#26469;&#22788;&#29702;&#30001;AoI&#23548;&#33268;&#30340;SA&#35823;&#24046;&#12290;&#22312;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#36825;&#20123;SA&#35823;&#24046;&#28385;&#36275;&#36882;&#24402;&#19981;&#31561;&#24335;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20010;&#36882;&#24402;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Gr&#246;nwall-type&#19981;&#31561;&#24335;&#65292;&#29992;&#20110;&#26102;&#21464;&#19979;&#38480;&#27714;&#21644;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#24067;&#24335;BMT&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20998;&#24067;&#24335;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#21644;&#20998;&#26512;SA&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize the Borkar-Meyn stability Theorem (BMT) to distributed stochastic approximations (SAs) with information delays that possess an arbitrary moment bound. To model the delays, we introduce Age of Information Processes (AoIPs): stochastic processes on the non-negative integers with a unit growth property. We show that AoIPs with an arbitrary moment bound cannot exceed any fraction of time infinitely often. In combination with a suitably chosen stepsize, this property turns out to be sufficient for the stability of distributed SAs. Compared to the BMT, our analysis requires crucial modifications and a new line of argument to handle the SA errors caused by AoI. In our analysis, we show that these SA errors satisfy a recursive inequality. To evaluate this recursion, we propose a new Gronwall-type inequality for time-varying lower limits of summations. As applications to our distributed BMT, we discuss distributed gradient-based optimization and a new approach to analyzing SAs wit
&lt;/p&gt;</description></item><item><title>HINT&#26159;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#22411;&#27169;&#22411;&#26063;&#65292;&#33021;&#22815;&#26377;&#25928;&#12289;&#20934;&#30830;&#22320;&#36827;&#34892;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;Bootstrap&#26041;&#27861;&#24182;&#20026;&#32593;&#32476;&#21152;&#20837;&#35268;&#33539;&#21270;&#29305;&#24449;&#25552;&#21462;&#21644;&#36755;&#20986;&#35268;&#33539;&#21270;&#26469;&#20445;&#35777;&#20854;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#31934;&#24230;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.07089</link><description>&lt;p&gt;
HINT:&#23618;&#27425;&#28151;&#21512;&#32593;&#32476;&#29992;&#20110;&#19968;&#33268;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HINT: Hierarchical Mixture Networks For Coherent Probabilistic Forecasting. (arXiv:2305.07089v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07089
&lt;/p&gt;
&lt;p&gt;
HINT&#26159;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#22411;&#27169;&#22411;&#26063;&#65292;&#33021;&#22815;&#26377;&#25928;&#12289;&#20934;&#30830;&#22320;&#36827;&#34892;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;Bootstrap&#26041;&#27861;&#24182;&#20026;&#32593;&#32476;&#21152;&#20837;&#35268;&#33539;&#21270;&#29305;&#24449;&#25552;&#21462;&#21644;&#36755;&#20986;&#35268;&#33539;&#21270;&#26469;&#20445;&#35777;&#20854;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#31934;&#24230;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Hierarchical Mixture Networks"&#65288;HINT&#65289;&#30340;&#27169;&#22411;&#26063;&#65292;&#29992;&#20110;&#26377;&#25928;&#32780;&#20934;&#30830;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20803;&#28151;&#21512;&#24182;&#20351;&#29992;&#22797;&#21512;&#20284;&#28982;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#26469;&#19987;&#38376;&#38024;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#32593;&#32476;&#29305;&#21270;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;Bootstrap&#26041;&#27861;&#21152;&#20197;&#21327;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#35268;&#33539;&#21270;&#29305;&#24449;&#25552;&#21462;&#21644;&#36755;&#20986;&#35268;&#33539;&#21270;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#23610;&#24230;&#21464;&#21270;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;8&#65285; sCRPS&#22686;&#24378;&#31934;&#24230;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#37096;&#20214;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#24182;&#24191;&#27867;&#30740;&#31350;&#20102;&#22810;&#20803;&#28151;&#21512;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290; HINT&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/Nixtla/neuralforecast&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Hierarchical Mixture Networks (HINT), a model family for efficient and accurate coherent forecasting. We specialize the networks on the task via a multivariate mixture optimized with composite likelihood and made coherent via bootstrap reconciliation. Additionally, we robustify the networks to stark time series scale variations, incorporating normalized feature extraction and recomposition of output scales within their architecture. We demonstrate 8% sCRPS improved accuracy across five datasets compared to the existing state-of-the-art. We conduct ablation studies on our model's components and extensively investigate the theoretical properties of the multivariate mixture. HINT's code is available at this https://github.com/Nixtla/neuralforecast.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36719;&#20214;&#24037;&#31243;&#21407;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#20844;&#27491;&#30340;&#21516;&#26102;&#65292;&#35782;&#21035;&#21644;&#20943;&#36731;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26088;&#22312;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#27979;&#35797;&#21644;&#39564;&#35777;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.07041</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#27491;&#24615;&#19982;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24179;&#31561;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
Fairness in Machine Learning meets with Equity in Healthcare. (arXiv:2305.07041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36719;&#20214;&#24037;&#31243;&#21407;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#20844;&#27491;&#30340;&#21516;&#26102;&#65292;&#35782;&#21035;&#21644;&#20943;&#36731;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26088;&#22312;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#27979;&#35797;&#21644;&#39564;&#35777;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#26085;&#30410;&#24212;&#29992;&#65292;&#25552;&#39640;&#21307;&#30103;&#20445;&#20581;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#28508;&#21147;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#21363;&#22312;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#35745;&#20013;&#24310;&#32493;&#20559;&#35265;&#65292;&#20174;&#32780;&#20260;&#23475;&#26576;&#20123;&#21463;&#20445;&#25252;&#32676;&#20307;&#65292;&#22914;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36719;&#20214;&#24037;&#31243;&#21407;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#20844;&#27491;&#30340;&#21516;&#26102;&#65292;&#35782;&#21035;&#21644;&#20943;&#36731;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25968;&#25454;&#20013;&#31995;&#32479;&#24615;&#20559;&#35265;&#22914;&#20309;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#25918;&#22823;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#39044;&#38450;&#27492;&#31867;&#20559;&#35265;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26088;&#22312;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#27979;&#35797;&#21644;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;ML&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing utilization of machine learning in healthcare, there is increasing potential to enhance healthcare outcomes and efficiency. However, this also brings the risk of perpetuating biases in data and model design that can harm certain protected groups based on factors such as age, gender, and race. This study proposes an artificial intelligence framework, grounded in software engineering principles, for identifying and mitigating biases in data and models while ensuring fairness in healthcare settings. A case study is presented to demonstrate how systematic biases in data can lead to amplified biases in model predictions, and machine learning methods are suggested to prevent such biases. Future research aims to test and validate the proposed ML framework in real-world clinical settings to evaluate its impact on promoting health equity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21442;&#25968;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20809;&#35889;&#27979;&#37327;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;&#23454;&#39564;&#25928;&#29575;&#65292;&#32553;&#30701;&#20102;&#27979;&#37327;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.07040</link><description>&lt;p&gt;
&#24212;&#29992;&#21442;&#25968;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#65306;&#22522;&#20110;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#30340;&#20809;&#35889;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Sequential Experimental Design for Spectral Measurement: Active Learning Using a Parametric Model. (arXiv:2305.07040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21442;&#25968;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20809;&#35889;&#27979;&#37327;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;&#23454;&#39564;&#25928;&#29575;&#65292;&#32553;&#30701;&#20102;&#27979;&#37327;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21442;&#25968;&#27169;&#22411;&#20316;&#20026;&#39044;&#27979;&#22240;&#32032;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#26469;&#36827;&#34892;&#20809;&#35889;&#27979;&#37327;&#12290;&#22312;&#20809;&#35889;&#27979;&#37327;&#20013;&#65292;&#30001;&#20110;&#26679;&#26412;&#26131;&#25439;&#22351;&#21644;&#39640;&#33021;&#37327;&#25104;&#26412;&#30340;&#21407;&#22240;&#65292;&#38656;&#35201;&#32553;&#30701;&#27979;&#37327;&#26102;&#38388;&#12290;&#20026;&#20102;&#25552;&#39640;&#23454;&#39564;&#25928;&#29575;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#65292;&#21033;&#29992;&#20043;&#21069;&#33719;&#24471;&#30340;&#25968;&#25454;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#35774;&#35745;&#21518;&#32493;&#27979;&#37327;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20844;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#19968;&#33324;&#30340;&#21442;&#25968;&#27169;&#22411;&#22312;&#23454;&#29616;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;Bayesian&#25512;&#26029;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#20132;&#25442;Monte Carlo&#26041;&#27861;&#23454;&#29616;&#20102;&#20351;&#29992;&#19968;&#33324;&#21442;&#25968;&#27169;&#22411;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we demonstrate a sequential experimental design for spectral measurements by active learning using parametric models as predictors. In spectral measurements, it is necessary to reduce the measurement time because of sample fragility and high energy costs. To improve the efficiency of experiments, sequential experimental designs are proposed, in which the subsequent measurement is designed by active learning using the data obtained before the measurement. Conventionally, parametric models are employed in data analysis; when employed for active learning, they are expected to afford a sequential experimental design that improves the accuracy of data analysis. However, due to the complexity of the formulas, a sequential experimental design using general parametric models has not been realized. Therefore, we applied Bayesian inference-based data analysis using the exchange Monte Carlo method to realize a sequential experimental design with general parametric models. In this s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#26469;&#35299;&#20915;&#20540;&#36845;&#20195;&#32593;&#32476;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#21644;&#20943;&#36731;&#32047;&#31215;&#35823;&#24046;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#21644;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07039</link><description>&lt;p&gt;
&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Value Iteration Networks with Gated Summarization Module. (arXiv:2305.07039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#26469;&#35299;&#20915;&#20540;&#36845;&#20195;&#32593;&#32476;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#21644;&#20943;&#36731;&#32047;&#31215;&#35823;&#24046;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#21644;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;VIN&#65289;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#65292;&#20943;&#36731;&#30001;&#22686;&#21152;&#36845;&#20195;&#27425;&#25968;&#24341;&#36215;&#30340;&#32047;&#31215;&#35823;&#24046;&#30340;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#21367;&#31215;&#26680;&#20943;&#23569;&#36845;&#20195;&#27425;&#25968;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#65292;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#21010;&#36807;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#20351;&#24471;&#32593;&#32476;&#21487;&#20197;&#24378;&#35843;&#25972;&#20010;&#35268;&#21010;&#36807;&#31243;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#26368;&#32456;&#30340;&#20840;&#23616;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;(CVAEs)&#26816;&#27979;&#21644;&#37327;&#21270;DaT&#30340;&#27987;&#24230;&#21450;&#20854;&#31354;&#38388;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#22238;&#24402;&#31639;&#27861;&#23558;&#20854;&#19982;&#19981;&#21516;&#30151;&#29366;&#31867;&#21035;&#30456;&#20851;&#32852;&#65292;&#25104;&#21151;&#22320;&#23558;UPDRS&#19982;&#20302;&#32500;&#34920;&#31034;&#30456;&#20851;&#32852;&#65292;&#26377;&#26395;&#22312;PD&#30340;&#26089;&#26399;&#35786;&#26029;&#21450;&#29702;&#35299;&#31070;&#32463;&#36864;&#34892;&#24615;&#21644;&#30151;&#29366;&#23398;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.07038</link><description>&lt;p&gt;
&#29992;3D&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25581;&#31034;&#24085;&#37329;&#26862;&#30149;&#30340;&#30151;&#29366;&#27169;&#24335;&#65306;&#28508;&#31354;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Revealing Patterns of Symptomatology in Parkinson's Disease: A Latent Space Analysis with 3D Convolutional Autoencoders. (arXiv:2305.07038v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;(CVAEs)&#26816;&#27979;&#21644;&#37327;&#21270;DaT&#30340;&#27987;&#24230;&#21450;&#20854;&#31354;&#38388;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#22238;&#24402;&#31639;&#27861;&#23558;&#20854;&#19982;&#19981;&#21516;&#30151;&#29366;&#31867;&#21035;&#30456;&#20851;&#32852;&#65292;&#25104;&#21151;&#22320;&#23558;UPDRS&#19982;&#20302;&#32500;&#34920;&#31034;&#30456;&#20851;&#32852;&#65292;&#26377;&#26395;&#22312;PD&#30340;&#26089;&#26399;&#35786;&#26029;&#21450;&#29702;&#35299;&#31070;&#32463;&#36864;&#34892;&#24615;&#21644;&#30151;&#29366;&#23398;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;3D&#21367;&#31215;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(CVAEs)&#36861;&#36394;&#24085;&#37329;&#26862;&#30149;(PD)&#31070;&#32463;&#36864;&#21270;&#20135;&#29983;&#30340;&#21464;&#21270;&#21644;&#30151;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;3D CVAEs&#26816;&#27979;&#21644;&#37327;&#21270;&#22810;&#24052;&#33018;&#36716;&#36816;&#20307;(DaT)&#27987;&#24230;&#21450;&#20854;&#31354;&#38388;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23398;&#20064;&#33041;&#37096;&#25104;&#20687;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#22238;&#24402;&#31639;&#27861;&#23558;&#20854;&#19982;&#19981;&#21516;&#30340;&#30151;&#29366;&#31867;&#21035;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#22312;PD&#24739;&#32773;&#21644;&#20581;&#24247;&#23545;&#29031;&#32452;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#26222;&#36941;&#30151;&#29366;(UPDRS)&#36890;&#36807;&#20855;&#26377;R2&gt;0.25&#30340;CVAE&#30340;d&#32500;&#20998;&#35299;&#19982;&#20043;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#26089;&#26399;&#35786;&#26029;&#20197;&#21450;&#29702;&#35299;&#31070;&#32463;&#36864;&#34892;&#24615;&#21644;&#30151;&#29366;&#23398;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes the use of 3D convolutional variational autoencoders (CVAEs) to trace the changes and symptomatology produced by neurodegeneration in Parkinson's disease (PD). In this work, we present a novel approach to detect and quantify changes in dopamine transporter (DaT) concentration and its spatial patterns using 3D CVAEs on Ioflupane (FPCIT) imaging. Our approach leverages the power of deep learning to learn a low-dimensional representation of the brain imaging data, which then is linked to different symptom categories using regression algorithms. We demonstrate the effectiveness of our approach on a dataset of PD patients and healthy controls, and show that general symptomatology (UPDRS) is linked to a d-dimensional decomposition via the CVAE with R2&gt;0.25. Our work shows the potential of representation learning not only in early diagnosis but in understanding neurodegeneration processes and symptomatology.
&lt;/p&gt;</description></item><item><title>&#28155;&#21152;&#20869;&#37096;&#23618;&#36830;&#25509;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20462;&#25913;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#65292;&#20351;&#24471;&#24102;&#26377;&#20869;&#37096;&#23618;&#36830;&#25509;&#30340;&#27973;&#23618;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#28145;&#23618;&#32593;&#32476;&#30340;&#19968;&#20123;&#22256;&#38590;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.07037</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#37096;&#23618;&#36830;&#25509;&#37325;&#26032;&#24605;&#32771;&#28145;&#24230;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Rethink Depth Separation with Intra-layer Links. (arXiv:2305.07037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07037
&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#20869;&#37096;&#23618;&#36830;&#25509;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20462;&#25913;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#65292;&#20351;&#24471;&#24102;&#26377;&#20869;&#37096;&#23618;&#36830;&#25509;&#30340;&#27973;&#23618;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#28145;&#23618;&#32593;&#32476;&#30340;&#19968;&#20123;&#22256;&#38590;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#29616;&#22312;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#19968;&#20010;&#26377;&#25928;&#35299;&#37322;&#65292;&#23427;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;i&#65289;&#23384;&#22312;&#19968;&#31181;&#21487;&#20197;&#30001;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#30340;&#20989;&#25968;&#65307;ii&#65289;&#36825;&#26679;&#30340;&#20989;&#25968;&#19981;&#33021;&#30001;&#23485;&#24230;&#20302;&#20110;&#26576;&#19968;&#38408;&#20540;&#30340;&#27973;&#23618;&#32593;&#32476;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#29702;&#35770;&#26159;&#24314;&#31435;&#22312;&#21069;&#39304;&#32593;&#32476;&#19978;&#30340;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#22312;&#21521;&#35299;&#20915;&#29616;&#23454;&#38382;&#39064;&#30340;&#26368;&#24120;&#35265;&#30340;&#32593;&#32476;&#31867;&#22411;&#8212;&#8212;&#24555;&#25463;&#32593;&#32476;&#20013;&#32771;&#34385;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#28155;&#21152;&#20869;&#37096;&#23618;&#36830;&#25509;&#21487;&#20197;&#20462;&#25913;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#36890;&#36807;&#30028;&#38480;&#20272;&#35745;&#12289;&#26174;&#24335;&#26500;&#36896;&#21644;&#21151;&#33021;&#31354;&#38388;&#20998;&#26512;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#20869;&#37096;&#23618;&#36830;&#25509;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#19968;&#20010;&#24102;&#26377;&#20869;&#37096;&#23618;&#36830;&#25509;&#30340;&#27973;&#23618;&#32593;&#32476;&#19981;&#38656;&#35201;&#20687;&#20043;&#21069;&#19968;&#26679;&#21464;&#24471;&#23485;&#26469;&#34920;&#31034;&#30001;&#28145;&#23618;&#32593;&#32476;&#26500;&#36896;&#30340;&#19968;&#20123;&#22256;&#38590;&#20989;&#25968;&#26469;&#20462;&#25913;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The depth separation theory is nowadays widely accepted as an effective explanation for the power of depth, which consists of two parts: i) there exists a function representable by a deep network; ii) such a function cannot be represented by a shallow network whose width is lower than a threshold. However, this theory is established for feedforward networks. Few studies, if not none, considered the depth separation theory in the context of shortcuts which are the most common network types in solving real-world problems. Here, we find that adding intra-layer links can modify the depth separation theory. First, we report that adding intra-layer links can greatly improve a network's representation capability through bound estimation, explicit construction, and functional space analysis. Then, we modify the depth separation theory by showing that a shallow network with intra-layer links does not need to go as wide as before to express some hard functions constructed by a deep network. Such
&lt;/p&gt;</description></item><item><title>GFlowNets&#26694;&#26550;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#36712;&#36857;&#19978;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#20005;&#26684;&#19982;&#20154;&#31867;&#35780;&#32423;&#25104;&#27604;&#20363;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27604;RLHF&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.07036</link><description>&lt;p&gt;
&#24102;&#20154;&#31867;&#21453;&#39304;&#30340;GFlowNets
&lt;/p&gt;
&lt;p&gt;
GFlowNets with Human Feedback. (arXiv:2305.07036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07036
&lt;/p&gt;
&lt;p&gt;
GFlowNets&#26694;&#26550;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#36712;&#36857;&#19978;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#20005;&#26684;&#19982;&#20154;&#31867;&#35780;&#32423;&#25104;&#27604;&#20363;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27604;RLHF&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;GFlowNets (GFlowHF) &#26694;&#26550;&#26469;&#25913;&#36827;&#35757;&#32451; AI &#27169;&#22411;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23545;&#20110;&#22870;&#21169;&#26410;&#30693;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#36712;&#36857;&#19978;&#30340;&#20154;&#31867;&#35780;&#20272;&#26469;&#36866;&#24212;&#22870;&#21169;&#20989;&#25968;&#12290;GFlowHF &#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#20005;&#26684;&#19982;&#20154;&#31867;&#35780;&#32423;&#25104;&#27604;&#20363;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#20110;&#31867;&#20284; RLHF &#30340;&#20154;&#31867;&#21916;&#22909;&#35780;&#32423;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;GFlowHF &#27604; RLHF &#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the GFlowNets with Human Feedback (GFlowHF) framework to improve the exploration ability when training AI models. For tasks where the reward is unknown, we fit the reward function through human evaluations on different trajectories. The goal of GFlowHF is to learn a policy that is strictly proportional to human ratings, instead of only focusing on human favorite ratings like RLHF. Experiments show that GFlowHF can achieve better exploration ability than RLHF.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;CTC&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#26469;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#12290;&#37319;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.07034</link><description>&lt;p&gt;
&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#30340;&#21476;&#20848;&#32463;&#26391;&#35829;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Quran Recitation Recognition using End-to-End Deep Learning. (arXiv:2305.07034v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;CTC&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#26469;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#12290;&#37319;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21476;&#20848;&#32463;&#26159;&#20234;&#26031;&#20848;&#25945;&#30340;&#22307;&#20070;&#65292;&#20854;&#26391;&#35829;&#26159;&#35813;&#23447;&#25945;&#20449;&#20208;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#30001;&#20110;&#21476;&#20848;&#32463;&#30340;&#29420;&#29305;&#35268;&#21017;&#19981;&#36866;&#29992;&#20110;&#27491;&#24120;&#30340;&#28436;&#35762;&#65292;&#25152;&#20197;&#33258;&#21160;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#24050;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#65292;&#20294;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#26391;&#35829;&#38169;&#35823;&#26816;&#27979;&#35270;&#20026;&#20998;&#31867;&#20219;&#21153;&#25110;&#20351;&#29992;&#20256;&#32479;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#12290;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;CNN-Bidirectional GRU&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;CTC&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#21644;&#22522;&#20110;&#23383;&#31526;&#30340;&#35299;&#30721;&#22120;&#65292;&#21363;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#26159;&#22312;&#30001;&#30701;&#33410;&#21644;&#20960;&#31456;&#21476;&#20848;&#32463;&#32452;&#25104;&#30340;&#23567;&#22411;&#31169;&#20154;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#12290;&#30001;&#20110;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;Ar-DAD&#65289;&#20316;&#20026;&#23454;&#39564;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Quran is the holy scripture of Islam, and its recitation is an important aspect of the religion. Recognizing the recitation of the Holy Quran automatically is a challenging task due to its unique rules that are not applied in normal speaking speeches. A lot of research has been done in this domain, but previous works have detected recitation errors as a classification task or used traditional automatic speech recognition (ASR). In this paper, we proposed a novel end-to-end deep learning model for recognizing the recitation of the Holy Quran. The proposed model is a CNN-Bidirectional GRU encoder that uses CTC as an objective function, and a character-based decoder which is a beam search decoder. Moreover, all previous works were done on small private datasets consisting of short verses and a few chapters of the Holy Quran. As a result of using private datasets, no comparisons were done. To overcome this issue, we used a public dataset that has recently been published (Ar-DAD) and co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#31934;&#30830;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#65292;&#24182;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#31038;&#20250;&#25193;&#25955;&#21644;&#22320;&#38663;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.07031</link><description>&lt;p&gt;
&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Hawkes Process based on Controlled Differential Equations. (arXiv:2305.07031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#31934;&#30830;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#65292;&#24182;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#31038;&#20250;&#25193;&#25955;&#21644;&#22320;&#38663;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hawkes&#36807;&#31243;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22810;&#20010;&#39046;&#22495;&#30340;&#24207;&#36143;&#20107;&#20214;&#21457;&#29983;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#31038;&#20250;&#25193;&#25955;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#20107;&#20214;&#20043;&#38388;&#30340;&#38388;&#38548;&#26102;&#38388;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#19981;&#20165;&#38590;&#20197;&#25429;&#25417;&#36825;&#31181;&#22797;&#26434;&#30340;&#19981;&#35268;&#21017;&#21160;&#24577;&#65292;&#32780;&#19988;&#36824;&#20250;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#35745;&#31639;&#20107;&#20214;&#30340;&#23545;&#25968;&#20284;&#28982;&#65292;&#22240;&#20026;&#23427;&#20204;&#22823;&#22810;&#22522;&#20110;&#35774;&#35745;&#29992;&#20110;&#35268;&#21017;&#31163;&#25955;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;(CDE)&#30340;Hawkes&#36807;&#31243;&#27010;&#24565;&#65292;&#36890;&#36807;&#37319;&#29992;&#31867;&#20284;&#20110;&#36830;&#32493;RNN&#30340;&#31070;&#32463;CDE&#25216;&#26415;&#12290;&#30001;&#20110;HP-CDE&#19981;&#26029;&#22320;&#35835;&#21462;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#36866;&#24403;&#22320;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#23427;&#20204;&#30340;&#19981;&#22343;&#21248;&#26102;&#38388;&#31354;&#38388;&#65292;&#24182;&#19988;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;Hawkes&#36807;&#31243;&#21644;&#31070;&#32463;CDE&#37117;&#26159;&#22312;&#36830;&#32493;&#30340;&#26102;&#38388;&#22495;&#20013;&#39318;&#20808;&#24320;&#21457;&#30340;&#65292;&#23427;&#20204;&#20855;&#26377;&#30456;&#20284;&#30340;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;HP-CDE&#20855;&#26377;&#36879;&#26126;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#23454;&#38469;&#22330;&#26223;&#65292;&#20363;&#22914;&#31038;&#20250;&#25193;&#25955;&#65292;&#20854;&#20013;&#20107;&#20214;&#20043;&#38388;&#30340;&#38388;&#38548;&#26102;&#38388;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#31038;&#20132;&#25193;&#25955;&#21644;&#22320;&#38663;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hawkes processes are a popular framework to model the occurrence of sequential events, i.e., occurrence dynamics, in several fields such as social diffusion. In real-world scenarios, the inter-arrival time among events is irregular. However, existing neural network-based Hawkes process models not only i) fail to capture such complicated irregular dynamics, but also ii) resort to heuristics to calculate the log-likelihood of events since they are mostly based on neural networks designed for regular discrete inputs. To this end, we present the concept of Hawkes process based on controlled differential equations (HP-CDE), by adopting the neural controlled differential equation (neural CDE) technology which is an analogue to continuous RNNs. Since HP-CDE continuously reads data, i) irregular time-series datasets can be properly treated preserving their uneven temporal spaces, and ii) the log-likelihood can be exactly computed. Moreover, as both Hawkes processes and neural CDEs are first de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#23454;&#29616;&#20102;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;&#65292;&#38024;&#23545;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#20808;&#39564;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.06924</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#22312;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#23454;&#29616;&#20808;&#39564;&#30693;&#35782;&#20256;&#36882;&#20197;&#36827;&#34892;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation. (arXiv:2305.06924v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#23454;&#29616;&#20102;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;&#65292;&#38024;&#23545;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#20808;&#39564;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21147;&#24066;&#22330;&#30340;&#25237;&#26631;&#28216;&#25103;&#20013;&#65292;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#20272;&#35745;&#26159;&#21457;&#30005;&#20844;&#21496;&#65288;GENCO&#65289;&#36827;&#34892;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#29420;&#31435;&#31995;&#32479;&#36816;&#33829;&#21830;&#65288;ISO&#65289;&#36827;&#34892;&#24066;&#22330;&#30417;&#35270;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NE&#20272;&#35745;&#26041;&#27861;&#22312;&#26032;&#20852;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#65288;FEM&#65289;&#20013;&#26159;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22312;&#20219;&#20309;&#29615;&#22659;&#21464;&#21270;&#20043;&#21069;&#65292;&#22914;&#36127;&#36733;&#38656;&#27714;&#21464;&#21270;&#12289;&#32593;&#32476;&#25317;&#22581;&#21644;&#24066;&#22330;&#35774;&#35745;&#30340;&#20462;&#25913;&#65292;&#25237;&#26631;&#31574;&#30053;&#30340;&#20808;&#39564;&#30693;&#35782;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#38024;&#23545;FEM&#24320;&#21457;&#20102;Bayes&#33258;&#36866;&#24212;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;BAMDP-FEM&#65289;&#65292;&#20197;&#32771;&#34385;&#20808;&#39564;&#30693;&#35782;&#26469;&#24314;&#27169;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#12290;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65288;MAGAIL-FEM&#65289;&#65292;&#20351;GENCO&#33021;&#22815;&#21516;&#26102;&#20174;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#24471;&#21040;&#30340;NE&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#65288;BNE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Nash Equilibrium (NE) estimation in bidding games of electricity markets is the key concern of both generation companies (GENCOs) for bidding strategy optimization and the Independent System Operator (ISO) for market surveillance. However, existing methods for NE estimation in emerging modern electricity markets (FEM) are inaccurate and inefficient because the priori knowledge of bidding strategies before any environment changes, such as load demand variations, network congestion, and modifications of market design, is not fully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM (BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategy optimization considering the priori knowledge. A novel Multi-Agent Generative Adversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed to enable GENCOs to learn simultaneously from priori knowledge and interactions with changing environments. The obtained NE is a Bayesian Nash Equilibrium (BNE) with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26469;&#32852;&#21512;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#21644;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#20135;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2305.06921</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#26410;&#26469;&#30340;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#65311;&#31532;&#20108;&#37096;&#20998;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications. (arXiv:2305.06921v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26469;&#32852;&#21512;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#21644;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#20135;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20026;&#20004;&#37096;&#20998;&#30340;&#35770;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#33539;&#24335;&#29702;&#35770;&#21644;&#35814;&#32454;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#25311;&#26469;&#32852;&#21512;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#36890;&#36807;&#38416;&#36848;&#35814;&#32454;&#30340;&#26041;&#27861;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#65288;ESM&#65289;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#65288;ASM&#65289;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#65288;RC&#65289;&#21644;&#37329;&#34701;&#24066;&#22330;&#65288;FM&#65289;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#65288;VB&#65289;&#20135;&#21697;&#26469;&#36827;&#19968;&#27493;&#28436;&#31034;&#36825;&#19968;&#29702;&#35770;&#12290;&#26681;&#25454;&#31532;&#19968;&#37096;&#20998;&#25552;&#20986;&#30340;&#29702;&#35770;&#65292;&#39318;&#20808;&#30830;&#23450;&#32852;&#21512;&#24066;&#22330;&#20013;&#30340;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;&#25509;&#30528;&#65292;&#24320;&#21457;&#20102;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#21644;&#19981;&#30830;&#23450;&#39118;&#38505;&#32435;&#20837;&#27169;&#22411;&#20844;&#24335;&#20013;&#12290;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#36817;&#31471;&#20248;&#21270;&#65288;MAPPO&#65289;&#31639;&#27861;&#65292;&#20316;&#20026;&#31532;&#19968;&#37096;&#20998;&#24320;&#21457;&#30340;&#24191;&#20041;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;&#30340;&#23454;&#38469;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20123;&#24066;&#22330;&#36816;&#34892;&#32489;&#25928;&#25351;&#26631;&#65292;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
This two-part paper develops a paradigmatic theory and detailed methods of the joint electricity market design using reinforcement-learning (RL)-based simulation. In Part 2, this theory is further demonstrated by elaborating detailed methods of designing an electricity spot market (ESM), together with a reserved capacity product (RC) in the ancillary service market (ASM) and a virtual bidding (VB) product in the financial market (FM). Following the theory proposed in Part 1, firstly, market design options in the joint market are specified. Then, the Markov game model is developed, in which we show how to incorporate market design options and uncertain risks in model formulation. A multi-agent policy proximal optimization (MAPPO) algorithm is elaborated, as a practical implementation of the generalized market simulation method developed in Part 1. Finally, the case study demonstrates how to pick the best market design options by using some of the market operation performance indicators 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.06472</link><description>&lt;p&gt;
ChatGPT&#24335;&#30340;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#25216;&#26415;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#35774;&#22791;&#32500;&#25252;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;PHM&#25216;&#26415;&#35782;&#21035;&#21644;&#39044;&#27979;&#35774;&#22791;&#25925;&#38556;&#21644;&#25439;&#22351;&#12290;&#29616;&#22312;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;AI&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#36825;&#31181;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24037;&#19994;&#39046;&#22495;&#65292;&#22914;&#38081;&#36335;&#12289;&#33021;&#28304;&#21644;&#33322;&#31354;&#31561;&#65292;&#20197;&#25552;&#39640;&#35774;&#22791;&#30340;&#26381;&#21153;&#23551;&#21629;&#21644;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#29983;&#20135;&#25104;&#26412;&#21644;&#20572;&#26426;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.06446</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;: &#24322;&#27493;&#36890;&#20449;&#21644;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35774;&#32622;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20197;&#21512;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#21516;&#26102;&#30830;&#20445;&#21512;&#20316;&#20248;&#21183;&#19988;&#36890;&#20449;&#24320;&#38144;&#20302;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377; $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ &#30340;&#36951;&#25022;&#20540;&#21644; $\tilde{\mathcal{O}}(dHM^2)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013; $d$ &#26159;&#29305;&#24449;&#32500;&#25968;&#65292;$H$ &#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$M$ &#26159;&#26234;&#33021;&#20307;&#24635;&#25968;&#65292;$K$ &#26159;&#24635;&#24773;&#33410;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#38480;&#35777;&#26126;&#65292;&#34920;&#26126;&#36890;&#36807;&#21327;&#20316;&#33267;&#23569;&#38656;&#35201; $\Omega(dM)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#25165;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.06435</link><description>&lt;p&gt;
&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#29616;&#22312;&#38750;&#24120;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#23569;&#23450;&#37327;&#35299;&#37322;&#26368;&#20339;&#23567;&#25209;&#37327;&#22823;&#23567;&#24212;&#35813;&#26159;&#22810;&#22823;&#30340;&#29702;&#35770;&#12290;&#26412;&#25991;&#23581;&#35797;&#31995;&#32479;&#22320;&#29702;&#35299;&#23567;&#25209;&#37327;&#22823;&#23567;&#22312;&#35757;&#32451;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#12290;&#22312;&#25945;&#24072;-&#23398;&#29983;&#24773;&#22659;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#25945;&#24072;&#65292;&#24182;&#32858;&#28966;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;m&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#30340;&#27867;&#21270;&#24615;&#33021;&#24378;&#28872;&#20381;&#36182;&#20110;m&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#20020;&#30028;&#20540;mc&#22788;&#32463;&#21382;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#36825;&#26679;&#24403;m&lt; mc&#26102;&#65292;&#35757;&#32451;&#36807;&#31243;&#22833;&#36133;&#65292;&#32780;&#24403;m&gt; mc&#26102;&#65292;&#23398;&#29983;&#21487;&#20197;&#23436;&#32654;&#22320;&#23398;&#20064;&#25110;&#24456;&#22909;&#22320;&#27867;&#21270;&#25945;&#24072;&#12290;&#30456;&#21464;&#26159;&#30001;&#32479;&#35745;&#21147;&#23398;&#39318;&#27425;&#21457;&#29616;&#30340;&#38598;&#20307;&#29616;&#35937;&#65292;&#24182;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#35266;&#23519;&#21040;&#12290;&#25214;&#21040;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;&#65292;&#21487;&#20197;&#38416;&#26126;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m&lt;m_c$ the training process fails, while for $m&gt;m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#23646;&#20110;&#32463;&#20856;Sobolev&#31354;&#38388;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#30495;&#23454;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#36991;&#20813;&#36807;&#25311;&#21512;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03531</link><description>&lt;p&gt;
&#26680;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20013;&#30340;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Random Smoothing Regularization in Kernel Gradient Descent Learning. (arXiv:2305.03531v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#23646;&#20110;&#32463;&#20856;Sobolev&#31354;&#38388;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#30495;&#23454;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#36991;&#20813;&#36807;&#25311;&#21512;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#27491;&#21017;&#21270;&#24418;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#36755;&#20837;&#25968;&#25454;&#24341;&#20837;&#22122;&#22768;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#40723;&#21169;&#27169;&#22411;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#29305;&#24449;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#26426;&#24179;&#28369;&#30340;&#27491;&#21017;&#21270;&#33021;&#21147;&#32570;&#20047;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#23646;&#20110;&#32463;&#20856; Sobolev &#31354;&#38388;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#30495;&#23454;&#20989;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#22522;&#30784;&#30340;&#20989;&#25968;&#31354;&#38388;&#65306;&#20302;&#22266;&#26377;&#32500;&#24230;&#30340; Sobolev &#31354;&#38388;&#65292;&#20854;&#20013;&#21253;&#25324; $D$ &#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#25110;&#20302;&#32500;&#23376;&#27969;&#24418;&#20316;&#20026;&#29305;&#20363;&#65292;&#20197;&#21450;&#20855;&#26377;&#24352;&#37327;&#32467;&#26500;&#30340;&#28151;&#21512;&#24179;&#28369; Sobolev &#31354;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#27491;&#21017;&#21270;&#20316;&#20026;&#26032;&#22411;&#21367;&#31215;&#24179;&#28369;&#26680;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random smoothing data augmentation is a unique form of regularization that can prevent overfitting by introducing noise to the input data, encouraging the model to learn more generalized features. Despite its success in various applications, there has been a lack of systematic study on the regularization ability of random smoothing. In this paper, we aim to bridge this gap by presenting a framework for random smoothing regularization that can adaptively and effectively learn a wide range of ground truth functions belonging to the classical Sobolev spaces. Specifically, we investigate two underlying function spaces: the Sobolev space of low intrinsic dimension, which includes the Sobolev space in $D$-dimensional Euclidean space or low-dimensional sub-manifolds as special cases, and the mixed smooth Sobolev space with a tensor structure. By using random smoothing regularization as novel convolution-based smoothing kernels, we can attain optimal convergence rates in these cases using a ke
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#20197;&#24212;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#65292;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#65292;&#24182;&#20026;&#29615;&#22659;&#20570;&#20986;&#36129;&#29486;&#12290;&#35813;&#33539;&#22411;&#29702;&#35770;&#30340;&#26694;&#26550;&#23558;&#22312;&#20004;&#37096;&#20998;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;</title><link>http://arxiv.org/abs/2305.02485</link><description>&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#26410;&#26469;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#65311;&#31532;&#19968;&#37096;&#20998;&#65306;&#33539;&#22411;&#29702;&#35770;&#12290;&#65288;arXiv:2305.02485v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory. (arXiv:2305.02485v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#20197;&#24212;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#65292;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#65292;&#24182;&#20026;&#29615;&#22659;&#20570;&#20986;&#36129;&#29486;&#12290;&#35813;&#33539;&#22411;&#29702;&#35770;&#30340;&#26694;&#26550;&#23558;&#22312;&#20004;&#37096;&#20998;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#37325;&#26032;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#26159;&#19968;&#31181;&#23439;&#35266;&#23618;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#39640;&#28183;&#36879;&#29575;&#65292;&#24182;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#25805;&#20316;&#23433;&#20840;&#12289;&#32463;&#27982;&#25928;&#29575;&#21644;&#29615;&#22659;&#21451;&#22909;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24066;&#22330;&#35774;&#35745;&#26041;&#27861;&#23398;&#23384;&#22312;&#20110;&#33021;&#28304;&#29616;&#36135;&#24066;&#22330;&#65288;ESM&#65289;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#65288;ASM&#65289;&#21644;&#37329;&#34701;&#24066;&#22330;&#65288;FM&#65289;&#20043;&#38388;&#21327;&#35843;&#19981;&#36275;&#65292;&#21363;&#8220;&#32852;&#21512;&#24066;&#22330;&#8221;&#65292;&#20197;&#21450;&#32570;&#20047;&#21487;&#38752;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#26412;&#25991;&#23558;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#25311;&#65292;&#24320;&#21457;&#32852;&#21512;&#24066;&#22330;&#35774;&#35745;&#30340;&#33539;&#22411;&#29702;&#35770;&#21644;&#35814;&#32454;&#26041;&#27861;&#12290;&#31532;&#19968;&#37096;&#20998;&#25552;&#20986;&#20102;&#36825;&#31181;&#26032;&#22411;&#24066;&#22330;&#35774;&#35745;&#21746;&#23398;&#30340;&#29702;&#35770;&#21644;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#24635;&#32467;&#20102;&#22312;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#26102;&#23384;&#22312;&#30340;&#26377;&#20105;&#35758;&#30340;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#20316;&#20026;&#30446;&#26631;&#30740;&#31350;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In face of the pressing need of decarbonization in the power sector, the re-design of electricity market is necessary as a Marco-level approach to accommodate the high penetration of renewable generations, and to achieve power system operation security, economic efficiency, and environmental friendliness. However, existing market design methodologies suffer from the lack of coordination among energy spot market (ESM), ancillary service market (ASM) and financial market (FM), i.e., the "joint market", and the lack of reliable simulation-based verification. To tackle these deficiencies, this two-part paper develops a paradigmatic theory and detailed methods of the joint market design using reinforcement-learning (RL)-based simulation. In Part 1, the theory and framework of this novel market design philosophy are proposed. First, the controversial market design options while designing the joint market are summarized as the targeted research questions. Second, the Markov game model is deve
&lt;/p&gt;</description></item><item><title>GAMIVAL&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28216;&#25103;&#19987;&#29992;&#26080;&#21442;&#32771;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22810;&#31181;&#20248;&#28857;&#12290;&#22312;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#30340;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;NR VQA&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02422</link><description>&lt;p&gt;
GAMIVAL&#65306;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#30340;&#35270;&#39057;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content. (arXiv:2305.02422v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02422
&lt;/p&gt;
&lt;p&gt;
GAMIVAL&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28216;&#25103;&#19987;&#29992;&#26080;&#21442;&#32771;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22810;&#31181;&#20248;&#28857;&#12290;&#22312;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#30340;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;NR VQA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31227;&#21160;&#20113;&#28216;&#25103;&#34892;&#19994;&#36805;&#36895;&#22686;&#38271;&#12290;&#24403;&#28216;&#25103;&#35270;&#39057;&#20174;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;&#21040;&#23458;&#25143;&#31471;&#35774;&#22791;&#26102;&#65292;&#38656;&#35201;&#19968;&#31181;&#21487;&#20197;&#30417;&#27979;&#22833;&#30495;&#35270;&#39057;&#36136;&#37327;&#32780;&#26080;&#38656;&#21442;&#32771;&#35270;&#39057;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#30001;&#35745;&#31639;&#26426;&#22270;&#24418;&#24341;&#25806;&#28210;&#26579;&#30340;&#27969;&#24335;&#28216;&#25103;&#35270;&#39057;&#36136;&#37327;&#30340;&#26080;&#21442;&#32771;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;NR VQA&#65289;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#28216;&#25103;&#20869;&#23481;&#36890;&#24120;&#22312;&#32479;&#35745;&#19978;&#19982;&#33258;&#28982;&#35270;&#39057;&#19981;&#21516;&#65292;&#32570;&#20047;&#32454;&#33410;&#65292;&#24182;&#21253;&#21547;&#35768;&#22810;&#24179;&#28369;&#21306;&#22495;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#21517;&#20026;Gaming Video Quality Evaluator&#65288;GAMIVAL&#65289;&#30340;&#26032;&#22411;&#28216;&#25103;&#19987;&#29992;NR VQA&#27169;&#22411;&#65292;&#32467;&#21512;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#28216;&#25103;&#22833;&#30495;&#22330;&#26223;&#32479;&#35745;&#27169;&#22411;&#12289;&#31070;&#32463;&#22122;&#22768;&#27169;&#22411;&#21644;&#23458;&#35266;&#36136;&#37327;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;GAMIVAL&#24050;&#32463;&#22312;&#19968;&#20010;&#22823;&#22411;&#30340;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#22312;&#28216;&#25103;&#20869;&#23481;&#30340;NR VQA&#27169;&#22411;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mobile cloud gaming industry has been rapidly growing over the last decade. When streaming gaming videos are transmitted to customers' client devices from cloud servers, algorithms that can monitor distorted video quality without having any reference video available are desirable tools. However, creating No-Reference Video Quality Assessment (NR VQA) models that can accurately predict the quality of streaming gaming videos rendered by computer graphics engines is a challenging problem, since gaming content generally differs statistically from naturalistic videos, often lacks detail, and contains many smooth regions. Until recently, the problem has been further complicated by the lack of adequate subjective quality databases of mobile gaming content. We have created a new gaming-specific NR VQA model called the Gaming Video Quality Evaluator (GAMIVAL), which combines and leverages the advantages of spatial and temporal gaming distorted scene statistics models, a neural noise model, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#23637;&#24179;&#27969;&#24418;&#30340;&#31639;&#27861;FlatNet&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01777</link><description>&lt;p&gt;
&#36890;&#36807;&#27969;&#24418;&#23637;&#24179;&#21644;&#37325;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Manifold Flattening and Reconstruction. (arXiv:2305.01777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#23637;&#24179;&#27969;&#24418;&#30340;&#31639;&#27861;FlatNet&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#27969;&#24418;&#30340;&#26377;&#38480;&#26679;&#26412;&#20013;&#26174;&#24335;&#26500;&#24314;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#32447;&#24615;&#21270;&#21644;&#37325;&#26500;&#23884;&#20837;&#23376;&#27969;&#24418;&#12290;&#25105;&#20204;&#25152;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#31216;&#20026;&#23637;&#24179;&#32593;&#32476;&#65288;FlatNet&#65289;&#65292;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#35745;&#31639;&#19978;&#21487;&#25193;&#23637;&#24615;&#24378;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#31181;&#24179;&#34913;&#36890;&#24120;&#22312;&#22522;&#20110;&#27969;&#24418;&#30340;&#23398;&#20064;&#26041;&#27861;&#20013;&#38590;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#22522;&#20110;&#21512;&#25104;&#30340;&#39640;&#32500;&#27969;&#24418;&#25968;&#25454;&#21644;2D&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#65292;&#24182;&#19982;&#20854;&#20182;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes an algorithm for explicitly constructing a pair of neural networks that linearize and reconstruct an embedded submanifold, from finite samples of this manifold. Our such-generated neural networks, called flattening networks (FlatNet), are theoretically interpretable, computationally feasible at scale, and generalize well to test data, a balance not typically found in manifold-based learning methods. We present empirical results and comparisons to other models on synthetic high-dimensional manifold data and 2D image data. Our code is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#19968;&#33268;&#39044;&#27979;&#30340;&#25298;&#32477;&#23545;&#25239;&#35757;&#32451;&#65288;CPR&#65289;&#65292;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20998;&#23618;&#25298;&#32477;&#35774;&#32622;&#19979;&#36827;&#34892;&#23545;&#25239;&#40065;&#26834;&#20998;&#31867;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01139</link><description>&lt;p&gt;
&#20998;&#23618;&#23545;&#25239;&#40065;&#26834;&#24615;&#19982;&#25298;&#32477;
&lt;/p&gt;
&lt;p&gt;
Stratified Adversarial Robustness with Rejection. (arXiv:2305.01139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#19968;&#33268;&#39044;&#27979;&#30340;&#25298;&#32477;&#23545;&#25239;&#35757;&#32451;&#65288;CPR&#65289;&#65292;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20998;&#23618;&#25298;&#32477;&#35774;&#32622;&#19979;&#36827;&#34892;&#23545;&#25239;&#40065;&#26834;&#20998;&#31867;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#22320;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#26041;&#27861;&#8212;&#8212;&#25298;&#32477;&#39044;&#27979;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#25298;&#32477;&#39044;&#27979;&#20250;&#24102;&#26469;&#19968;&#23450;&#30340;&#25104;&#26412;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#23558;&#34987;&#25200;&#21160;&#30340;&#36755;&#20837;&#30340;&#25298;&#32477;&#19982;&#38646;&#25104;&#26412;&#30456;&#20851;&#32852;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25298;&#32477;&#22823;&#37327;&#21487;&#20197;&#34987;&#27491;&#30830;&#20998;&#31867;&#30340;&#36731;&#24230;&#25200;&#21160;&#36755;&#20837;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#23618;&#25298;&#32477;&#35774;&#32622;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#20998;&#31867;&#65292;&#24182;&#19988;&#36890;&#36807;&#25298;&#32477;&#25439;&#22833;&#20989;&#25968;&#22312;&#25200;&#21160;&#24133;&#24230;&#19978;&#21333;&#35843;&#19981;&#20943;&#22320;&#24314;&#27169;&#26469;&#27169;&#25311;&#25298;&#32477;&#25104;&#26412;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#20998;&#23618;&#25298;&#32477;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#19968;&#33268;&#39044;&#27979;&#30340;&#25298;&#32477;&#23545;&#25239;&#35757;&#32451;&#65288;CPR&#65289;&#8212;&#8212;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#22120;&#12290;&#38024;&#23545;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24378;&#36866;&#24212;&#24615;&#19979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there is an emerging interest in adversarially training a classifier with a rejection option (also known as a selective classifier) for boosting adversarial robustness. While rejection can incur a cost in many applications, existing studies typically associate zero cost with rejecting perturbed inputs, which can result in the rejection of numerous slightly-perturbed inputs that could be correctly classified. In this work, we study adversarially-robust classification with rejection in the stratified rejection setting, where the rejection cost is modeled by rejection loss functions monotonically non-increasing in the perturbation magnitude. We theoretically analyze the stratified rejection setting and propose a novel defense method -- Adversarial Training with Consistent Prediction-based Rejection (CPR) -- for building a robust selective classifier. Experiments on image datasets demonstrate that the proposed method significantly outperforms existing methods under strong adaptiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#20551;&#35774;&#65292;&#21457;&#29616;&#20102;Grassmannian Frame&#32467;&#26500;&#21644;&#23545;&#31216;&#27867;&#21270;&#29616;&#35937;&#65292;&#36825;&#23545;&#29305;&#24449;&#36873;&#25321;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#37117;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.08914</link><description>&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#30740;&#31350;&#65306;Grassmannian Frame&#12289;&#23545;&#31216;&#24615;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization. (arXiv:2304.08914v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#20551;&#35774;&#65292;&#21457;&#29616;&#20102;Grassmannian Frame&#32467;&#26500;&#21644;&#23545;&#31216;&#27867;&#21270;&#29616;&#35937;&#65292;&#36825;&#23545;&#29305;&#24449;&#36873;&#25321;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#37117;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#20551;&#35774;&#25512;&#24191;&#20102;&#21407;&#22987;&#30340;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#31867;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#24471;&#21040;&#20102;Grassmannian Frame&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#22312;&#29699;&#38754;&#19978;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#20102;&#27599;&#20004;&#20010;&#31867;&#21035;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#19968;&#20010;&#26356;&#22823;&#30340;&#29305;&#24449;&#32500;&#24230;&#12290;&#20986;&#20110;&#23545;Grassmannian Frame&#23545;&#31216;&#24615;&#30340;&#22909;&#22855;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25506;&#32034;&#19981;&#21516;Grassmannian Frame&#27169;&#22411;&#26159;&#21542;&#20250;&#20135;&#29983;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#31216;&#27867;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#32622;&#25442;&#23545;&#31216;&#27867;&#21270;&#30340;&#23450;&#29702;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#29305;&#24449;&#30340;&#19981;&#21516;&#26041;&#21521;&#20250;&#23548;&#33268;&#22914;&#27492;&#19981;&#21516;&#30340;&#27867;&#21270;&#29616;&#35937;&#30340;&#38382;&#39064;&#20173;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we extends original Neural Collapse Phenomenon by proving Generalized Neural Collapse hypothesis. We obtain Grassmannian Frame structure from the optimization and generalization of classification. This structure maximally separates features of every two classes on a sphere and does not require a larger feature dimension than the number of classes. Out of curiosity about the symmetry of Grassmannian Frame, we conduct experiments to explore if models with different Grassmannian Frames have different performance. As a result, we discover the Symmetric Generalization phenomenon. We provide a theorem to explain Symmetric Generalization of permutation. However, the question of why different directions of features can lead to such different generalization is still open for future investigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.06715</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#24403;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#22320;&#25551;&#36848;&#25152;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;&#65292;&#35299;&#37322;&#26041;&#27861;&#25165;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#39044;&#27979;&#22312;&#29305;&#23450;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#36825;&#21253;&#25324;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#26550;&#26500;&#12290;&#20219;&#20309;&#24544;&#23454;&#25551;&#36848;&#36825;&#31181;&#31867;&#22411;&#27169;&#22411;&#30340;&#35299;&#37322;&#37117;&#38656;&#35201;&#19982;&#35813;&#19981;&#21464;&#24615;&#23646;&#24615;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#36816;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#26469;&#24418;&#24335;&#21270;&#36825;&#31181;&#30452;&#35273;&#12290;&#36890;&#36807;&#36825;&#31181;&#20005;&#26684;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#65288;1&#65289;&#20004;&#20010;&#24230;&#37327;&#26469;&#34913;&#37327;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#27169;&#22411;&#23545;&#31216;&#32676;&#30340;&#20581;&#22766;&#24615;;&#65288;2&#65289;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#65307;&#65288;3&#65289;&#25552;&#39640;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#23545;&#31216;&#32676;&#30340;&#19981;&#21464;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#19982;&#19981;&#21516;&#23545;&#31216;&#32676;&#30456;&#20851;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#32463;&#39564;&#22320;&#27979;&#37327;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#23545;&#20110;&#24378;&#22823;&#30340;&#35299;&#37322;&#26041;&#27861;&#26159;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22833;&#21435;&#21487;&#22609;&#24615;&#38382;&#39064;&#36827;&#34892;&#31995;&#32479;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#28145;&#24230;&#19982;&#25439;&#22833;&#26799;&#24230;&#26354;&#29575;&#21464;&#21270;&#23494;&#20999;&#30456;&#20851;&#65292;&#39281;&#21644;&#21333;&#20803;&#25110;&#21457;&#25955;&#26799;&#24230;&#33539;&#25968;&#24182;&#38750;&#21407;&#22240;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#35782;&#21035;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20445;&#25345;&#21487;&#22609;&#24615;&#30340;&#33021;&#21147;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01486</link><description>&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding plasticity in neural networks. (arXiv:2303.01486v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22833;&#21435;&#21487;&#22609;&#24615;&#38382;&#39064;&#36827;&#34892;&#31995;&#32479;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#28145;&#24230;&#19982;&#25439;&#22833;&#26799;&#24230;&#26354;&#29575;&#21464;&#21270;&#23494;&#20999;&#30456;&#20851;&#65292;&#39281;&#21644;&#21333;&#20803;&#25110;&#21457;&#25955;&#26799;&#24230;&#33539;&#25968;&#24182;&#38750;&#21407;&#22240;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#35782;&#21035;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20445;&#25345;&#21487;&#22609;&#24615;&#30340;&#33021;&#21147;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#26159;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#24555;&#36895;&#26681;&#25454;&#26032;&#20449;&#24687;&#26356;&#25913;&#20854;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21363;&#20351;&#22312;&#30456;&#23545;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#20063;&#20250;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22833;&#21435;&#21487;&#22609;&#24615;&#65292;&#20294;&#39537;&#21160;&#36825;&#31181;&#29616;&#35937;&#30340;&#26426;&#21046;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#26088;&#22312;&#28145;&#24230;&#29702;&#35299;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#65292;&#20197;&#24341;&#23548;&#26410;&#26469;&#23545;&#26377;&#38024;&#23545;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#19982;&#25439;&#22833;&#26799;&#24230;&#26354;&#29575;&#30340;&#21464;&#21270;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#36890;&#24120;&#21457;&#29983;&#22312;&#26080;&#39281;&#21644;&#21333;&#20803;&#25110;&#21457;&#25955;&#26799;&#24230;&#33539;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#19968;&#20123;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#35774;&#35745;&#36873;&#25321;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#20445;&#25345;&#21487;&#22609;&#24615;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#22522;&#20110;&#29305;&#24449;&#30340;&#24178;&#39044;&#25514;&#26045;&#22312;&#19968;&#31995;&#21015;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25928;&#29992;&#65292;&#35777;&#26126;&#23427;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#31995;&#32479;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it typically occurs in the absence of saturated units or divergent gradient norms. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#31639;&#27861; LDECC&#65292;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#23454;&#29616;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2302.08070</link><description>&lt;p&gt;
&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#26412;&#22320;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Causal Discovery for Estimating Causal Effects. (arXiv:2302.08070v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#31639;&#27861; LDECC&#65292;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#23454;&#29616;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#25105;&#20204;&#30340;&#25968;&#25454;&#32972;&#21518;&#30340;&#22240;&#26524;&#22270;&#24418;&#26159;&#26410;&#30693;&#30340;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#26469;&#32553;&#23567;&#21487;&#33021;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21462;&#20540;&#33539;&#22260;&#65292;&#26041;&#24335;&#26159;&#65288;1&#65289;&#35782;&#21035;&#21040;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;; &#21644;&#65288;2&#65289;&#20272;&#35745;&#35813;&#31867;&#20013;&#27599;&#20010;&#22270;&#30340;ATE&#12290;&#23613;&#31649;PC&#31639;&#27861;&#22312;&#24378;&#20445;&#30495;&#24230;&#20551;&#35774;&#19979;&#21487;&#20197;&#35782;&#21035;&#35813;&#31867;&#21035;&#65292;&#20294;&#35745;&#31639;&#19978;&#30340;&#38480;&#21046;&#35753;&#20154;&#26395;&#32780;&#21364;&#27493;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20165;&#38656;&#35201;&#20851;&#20110;&#22788;&#29702;&#30340;&#23616;&#37096;&#22270;&#32467;&#26500;&#21363;&#21487;&#35782;&#21035;&#20986;&#21487;&#33021;&#30340;ATE&#20540;&#38598;&#65292;&#36825;&#26159;&#30001;&#26412;&#22320;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#20107;&#23454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#31639;&#27861;&#8212;&#8212;&#20351;&#29992;&#24613;&#20999;&#30896;&#25758;&#26816;&#26597;&#30340;&#26412;&#22320;&#21457;&#29616;&#65288;LDECC&#65289;&#65292;&#20351;&#29992;&#26410;&#23631;&#34109;&#30340;&#30896;&#25758;&#22120;&#26469;&#20351;&#22788;&#29702;&#30340;&#29238;&#39033;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#22270;&#24418;&#65292;&#22312;&#36825;&#20123;&#22270;&#24418;&#20013;&#65292;LDECC&#21576;&#25351;&#25968;&#32423;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#26412;&#22320;&#21457;&#29616;&#31639;&#27861;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;LDECC&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#20272;&#35745;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#39640;&#26031;&#35823;&#24046;&#19979;&#38480;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even when the causal graph underlying our data is unknown, we can use observational data to narrow down the possible values that an average treatment effect (ATE) can take by (1) identifying the graph up to a Markov equivalence class; and (2) estimating that ATE for each graph in the class. While the PC algorithm can identify this class under strong faithfulness assumptions, it can be computationally prohibitive. Fortunately, only the local graph structure around the treatment is required to identify the set of possible ATE values, a fact exploited by local discovery algorithms to improve computational efficiency. In this paper, we introduce Local Discovery using Eager Collider Checks (LDECC), a new local causal discovery algorithm that leverages unshielded colliders to orient the treatment's parents differently from existing methods. We show that there exist graphs where LDECC exponentially outperforms existing local discovery algorithms and vice versa. Moreover, we show that LDECC an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07260</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20808;&#39564;&#32593;&#32476;&#30340;&#39640;&#32500;&#36755;&#20986;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#28041;&#21450;&#21040;&#26410;&#30693;&#30340;&#39640;&#32500;&#24230;&#26144;&#23556;&#19968;&#32452;&#21487;&#25511;&#21464;&#37327;&#21040;&#26114;&#36149;&#23454;&#39564;&#32467;&#26524;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#20219;&#21153;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#26102;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24403;&#22788;&#29702;&#39640;&#32500;&#36755;&#20986;&#26102;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#32500;&#24230;&#20027;&#35201;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#20030;&#38598;&#25104;&#30340;BO&#21644;&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#20307;&#31995;&#32467;&#26500;&#36873;&#25321;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36817;&#20284;&#35774;&#35745;&#21464;&#37327;&#21644;&#24863;&#20852;&#36259;&#37327;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#21363;&#20351;&#22312;&#21518;&#32773;&#21462;&#20540;&#20110;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#25110;&#29978;&#33267;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22788;&#29702;&#39640;&#32500;&#24230;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#22362;&#22266;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#35270;&#35282;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#22362;&#22266;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#24471;&#20998;&#21305;&#37197;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25152;&#24471;&#31639;&#27861;&#26159;&#31934;&#30830;&#30340;&#65292;&#26356;&#26032;&#31616;&#21333;&#65292;&#36895;&#24230;&#36739;&#20043;&#21069;&#30340;&#31639;&#27861;&#24555;10&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2302.04759</link><description>&lt;p&gt;
&#22362;&#22266;&#19988;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust and Scalable Bayesian Online Changepoint Detection. (arXiv:2302.04759v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04759
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#22362;&#22266;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#35270;&#35282;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#22362;&#22266;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#24471;&#20998;&#21305;&#37197;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25152;&#24471;&#31639;&#27861;&#26159;&#31934;&#30830;&#30340;&#65292;&#26356;&#26032;&#31616;&#21333;&#65292;&#36895;&#24230;&#36739;&#20043;&#21069;&#30340;&#31639;&#27861;&#24555;10&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#12289;&#21487;&#35777;&#26126;&#22362;&#22266;&#19988;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#21464;&#28857;&#26816;&#27979;&#12290;&#25152;&#24471;&#31639;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#20855;&#26377;&#37325;&#35201;&#20248;&#21183;&#65306;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#35270;&#35282;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#22362;&#22266;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20043;&#21069;&#23581;&#35797;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#24191;&#20041;&#36125;&#21494;&#26031;&#24418;&#24335;&#20027;&#20041;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#24471;&#20998;&#21305;&#37197;&#23548;&#33268;&#20849;&#36717;&#21518;&#39564;&#30340;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#33719;&#24471;&#12290;&#25152;&#24471;&#31639;&#27861;&#26159;&#31934;&#30830;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#20195;&#25968;&#26356;&#26032;&#65292;&#24182;&#19988;&#27604;&#20854;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#23545;&#25163;&#24555;10&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an online, provably robust, and scalable Bayesian approach for changepoint detection. The resulting algorithm has key advantages over previous work: it provides provable robustness by leveraging the generalised Bayesian perspective, and also addresses the scalability issues of previous attempts. Specifically, the proposed generalised Bayesian formalism leads to conjugate posteriors whose parameters are available in closed form by leveraging diffusion score matching. The resulting algorithm is exact, can be updated through simple algebra, and is more than 10 times faster than its closest competitor.
&lt;/p&gt;</description></item><item><title>GPS++&#26159;&#19968;&#31181;&#28151;&#21512;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;/&#22270;&#24418;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#26412;&#22320;&#28040;&#24687;&#20256;&#36882;&#32452;&#20214;&#21644;&#20840;&#23616;&#20851;&#27880;&#65292;&#20197;&#21450;&#36807;&#21435;&#25991;&#29486;&#20013;&#30340;&#20854;&#20182;&#20851;&#38190;&#24605;&#24819;&#65292;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.02947</link><description>&lt;p&gt;
GPS++: &#24674;&#22797;&#28040;&#24687;&#20256;&#36882;&#33402;&#26415;&#20197;&#36827;&#34892;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GPS++: Reviving the Art of Message Passing for Molecular Property Prediction. (arXiv:2302.02947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02947
&lt;/p&gt;
&lt;p&gt;
GPS++&#26159;&#19968;&#31181;&#28151;&#21512;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;/&#22270;&#24418;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#26412;&#22320;&#28040;&#24687;&#20256;&#36882;&#32452;&#20214;&#21644;&#20840;&#23616;&#20851;&#27880;&#65292;&#20197;&#21450;&#36807;&#21435;&#25991;&#29486;&#20013;&#30340;&#20854;&#20182;&#20851;&#38190;&#24605;&#24819;&#65292;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; GPS++&#65292;&#19968;&#31181;&#28151;&#21512;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;/&#22270;&#24418;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#38598;&#25104;&#20102;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#26412;&#22320;&#28040;&#24687;&#20256;&#36882;&#32452;&#20214;&#21644;&#26377;&#20559;&#21521;&#24615;&#30340;&#20840;&#23616;&#20851;&#27880;&#20197;&#21450;&#26469;&#33258;&#20808;&#21069;&#25991;&#29486;&#30340;&#20854;&#20182;&#20851;&#38190;&#24605;&#24819;&#65292;&#20197;&#22312;&#22823;&#35268;&#27169;&#20998;&#23376;&#25968;&#25454;&#38598; PCQM4Mv2 &#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20010;&#21035;&#32452;&#20214;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#22312;&#26368;&#36817;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#20027;&#23548;&#19979;&#65292;&#28040;&#24687;&#20256;&#36882;&#20173;&#28982;&#26159;&#36827;&#34892;&#19977;&#32500;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#31454;&#20105;&#26041;&#27861;&#65292;&#20960;&#20046;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20840;&#23616;&#33258;&#25105;&#20851;&#27880;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#12290; &#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#27809;&#26377;&#19977;&#32500;&#20301;&#32622;&#20449;&#24687;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#25216;&#26415;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present GPS++, a hybrid Message Passing Neural Network / Graph Transformer model for molecular property prediction. Our model integrates a well-tuned local message passing component and biased global attention with other key ideas from prior literature to achieve state-of-the-art results on large-scale molecular dataset PCQM4Mv2. Through a thorough ablation study we highlight the impact of individual components and find that nearly all of the model's performance can be maintained without any use of global self-attention, showing that message passing is still a competitive approach for 3D molecular property prediction despite the recent dominance of graph transformers. We also find that our approach is significantly more accurate than prior art when 3D positional information is not available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;LLM&#20195;&#29702;&#31243;&#24207;&#30340;&#24615;&#33021;&#26469;&#23454;&#29616;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.02662</link><description>&lt;p&gt;
&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;
&lt;/p&gt;
&lt;p&gt;
Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning. (arXiv:2302.02662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;LLM&#20195;&#29702;&#31243;&#24207;&#30340;&#24615;&#33021;&#26469;&#23454;&#29616;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25429;&#25417;&#19990;&#30028;&#29289;&#29702;&#30340;&#25277;&#35937;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#32780;&#38480;&#21046;&#20102;&#20854;&#21151;&#33021;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;GLAM&#65289;&#65306;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20351;&#29992;LLM&#20316;&#20026;&#31574;&#30053;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#38543;&#30528;&#20195;&#29702;&#31243;&#24207;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#32780;&#36880;&#27493;&#26356;&#26032;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#20854;&#35299;&#20915;&#30446;&#26631;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#25991;&#26412;&#29615;&#22659;&#35774;&#35745;&#26469;&#30740;&#31350;&#26356;&#39640;&#32423;&#24418;&#24335;&#30340;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#20197;&#21450;&#19968;&#32452;&#31354;&#38388;&#21644;&#23548;&#33322;&#20219;&#21153;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#31185;&#23398;&#38382;&#39064;&#65306;1&#65289;LLMs&#33021;&#21542;&#25552;&#39640;&#21508;&#31181;RL&#20219;&#21153;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65311;2&#65289;&#23427;&#22914;&#20309;&#25552;&#39640;&#19981;&#21516;&#24418;&#24335;&#30340;&#27867;&#21270;&#65311;3&#65289;&#22312;&#32447;&#23398;&#20064;&#30340;&#24433;&#21709;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#36890;&#36807;&#21151;&#33021;&#26041;&#24335;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AdaptDiffuser&#26041;&#27861;,&#36890;&#36807;&#20351;&#29992;&#22870;&#21169;&#26799;&#24230;&#30340;&#25351;&#23548;&#29983;&#25104;&#23500;&#26377;&#20016;&#23500;&#30340;&#30446;&#26631;&#26465;&#20214;&#20219;&#21153;&#30340;&#21512;&#25104;&#19987;&#23478;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37492;&#21035;&#22120;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#26469;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#26410;&#30693;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.01877</link><description>&lt;p&gt;
AdaptDiffuser: &#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#33258;&#36827;&#21270;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners. (arXiv:2302.01877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AdaptDiffuser&#26041;&#27861;,&#36890;&#36807;&#20351;&#29992;&#22870;&#21169;&#26799;&#24230;&#30340;&#25351;&#23548;&#29983;&#25104;&#23500;&#26377;&#20016;&#23500;&#30340;&#30446;&#26631;&#26465;&#20214;&#20219;&#21153;&#30340;&#21512;&#25104;&#19987;&#23478;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37492;&#21035;&#22120;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#26469;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#26410;&#30693;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20855;&#26377;&#20316;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33539;&#20363;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#36136;&#37327;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#36825;&#38459;&#30861;&#20102;&#35268;&#21010;&#30340;&#24615;&#33021;&#21644;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AdaptDiffuser&#65292;&#19968;&#31181;&#24102;&#26377;&#25193;&#25955;&#30340;&#36827;&#21270;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#25105;&#36827;&#21270;&#20197;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#35268;&#21010;&#22120;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#24050;&#30693;&#20219;&#21153;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#12290;AdaptDiffuser&#20351;&#29992;&#22870;&#21169;&#26799;&#24230;&#30340;&#25351;&#23548;&#29983;&#25104;&#23500;&#26377;&#20016;&#23500;&#30340;&#30446;&#26631;&#26465;&#20214;&#20219;&#21153;&#30340;&#21512;&#25104;&#19987;&#23478;&#25968;&#25454;&#12290;&#28982;&#21518;&#36890;&#36807;&#37492;&#21035;&#22120;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#26469;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#26410;&#30693;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#29615;&#22659;&#20197;&#21450; KUKA &#24037;&#19994;&#26426;&#22120;&#20154;&#33218;&#21644; Maze2D &#29615;&#22659;&#20013;&#23545;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26410;&#30693;&#20219;&#21153;&#36827;&#34892;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36890;&#36807;&#22870;&#21169;&#35774;&#35745;&#35753;&#26234;&#33021;&#20307;&#23398;&#20064;&#23631;&#34109;&#26080;&#20851;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;RL&#26041;&#27861;&#26356;&#22909;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.15589</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#8212;&#8212;&#19981;&#36866;&#29992;&#21160;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inapplicable Actions Learning for Knowledge Transfer in Reinforcement Learning. (arXiv:2211.15589v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36890;&#36807;&#22870;&#21169;&#35774;&#35745;&#35753;&#26234;&#33021;&#20307;&#23398;&#20064;&#23631;&#34109;&#26080;&#20851;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;RL&#26041;&#27861;&#26356;&#22909;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#35768;&#22810;&#21487;&#33021;&#21160;&#20316;&#30340;&#29615;&#22659;&#20013;&#24456;&#38590;&#25193;&#23637;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#26159;&#22312;&#27599;&#20010;&#21487;&#33021;&#30340;&#29366;&#24577;&#19979;&#32771;&#34385;&#30456;&#21516;&#30340;&#22266;&#23450;&#21160;&#20316;&#31354;&#38388;&#65292;&#36825;&#24847;&#21619;&#30528;&#26234;&#33021;&#20307;&#24517;&#39035;&#29702;&#35299;&#12289;&#21516;&#26102;&#23398;&#20064;&#22914;&#20309;&#26368;&#22823;&#21270;&#20854;&#22870;&#21169;&#65292;&#24573;&#30053;&#26080;&#20851;&#30340;&#21160;&#20316;&#65292;&#27604;&#22914;&#8220;&#19981;&#36866;&#29992;&#21160;&#20316;&#8221;&#65288;&#21363;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#25191;&#34892;&#26102;&#19981;&#20250;&#24433;&#21709;&#29615;&#22659;&#30340;&#21160;&#20316;&#65289;&#12290;&#20102;&#35299;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#23631;&#34109;&#19982;&#23547;&#25214;&#26368;&#20248;&#31574;&#30053;&#30456;&#20851;&#30340;&#21160;&#20316;&#65292;&#20174;&#31574;&#30053;&#20998;&#24067;&#20013;&#21435;&#25506;&#32034;&#26080;&#20851;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#24110;&#21161;&#38477;&#20302;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#38454;&#27573;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#35774;&#35745;&#26041;&#27861;&#23398;&#20064;&#23631;&#34109;&#19981;&#36866;&#29992;&#30340;&#21160;&#20316;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#19968;&#20010;&#21160;&#20316;&#26159;&#21542;&#36866;&#29992;&#65292;&#24182;&#26681;&#25454;&#20854;&#39044;&#27979;&#33719;&#24471;&#22870;&#21169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#26679;&#26412;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) algorithms are known to scale poorly to environments with many available actions, requiring numerous samples to learn an optimal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as $\textit{inapplicable actions}$ (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. While this technique has been formalized for quite some time within the Automated Planning community with the concept of precondition in the STRIPS language, RL algorithms have never formally taken advantage of this information to prune the search space to explore. This is typically done in an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#21327;&#21516;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#35782;&#21035;&#21327;&#21516;&#32676;&#32452;&#65292;&#26500;&#24314;&#20943;&#23569;&#32593;&#32476;&#65292;&#24182;&#32473;&#20986;&#36924;&#36817;&#35823;&#24046;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2211.15301</link><description>&lt;p&gt;
&#24369;&#36830;&#25509;&#32593;&#32476;&#31995;&#32479;&#20013;&#23398;&#20064;&#21327;&#21516;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Coherent Clusters in Weakly-Connected Network Systems. (arXiv:2211.15301v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#21327;&#21516;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#35782;&#21035;&#21327;&#21516;&#32676;&#32452;&#65292;&#26500;&#24314;&#20943;&#23569;&#32593;&#32476;&#65292;&#24182;&#32473;&#20986;&#36924;&#36817;&#35823;&#24046;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#32039;&#23494;&#36830;&#25509;&#32452;&#20214;&#30340;&#22823;&#35268;&#27169;&#21160;&#24577;&#32593;&#32476;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#35782;&#21035;&#21327;&#21516;&#32676;&#32452;&#12290;&#28982;&#21518;&#65292;&#26500;&#24314;&#20943;&#23569;&#32593;&#32476;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#27599;&#20010;&#21327;&#21516;&#32452;&#30340;&#32858;&#21512;&#21160;&#24577;&#65292;&#24182;&#19988;&#20943;&#23569;&#32593;&#32476;&#25429;&#25417;&#32452;&#20043;&#38388;&#30340;&#21160;&#24577;&#32806;&#21512;&#12290;&#24403;&#32593;&#32476;&#22270;&#20174;&#26435;&#37325;&#38543;&#26426;&#22359;&#27169;&#22411;&#38543;&#26426;&#29983;&#25104;&#26102;&#65292;&#32473;&#20986;&#20102;&#36924;&#36817;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a structure-preserving model-reduction methodology for large-scale dynamic networks with tightly-connected components. First, the coherent groups are identified by a spectral clustering algorithm on the graph Laplacian matrix that models the network feedback. Then, a reduced network is built, where each node represents the aggregate dynamics of each coherent group, and the reduced network captures the dynamic coupling between the groups. We provide an upper bound on the approximation error when the network graph is randomly generated from a weight stochastic block model. Finally, numerical experiments align with and validate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.08073</link><description>&lt;p&gt;
GLUE-X: &#20174;ODD&#26222;&#36866;&#24615;&#35282;&#24230;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24050;&#30693;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#30340;ODD&#26222;&#36866;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#24378;&#35843;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#22914;&#20309;&#34913;&#37327;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#65288;&#21253;&#25324;GPT-3&#21644;GPT-3.5&#65289;&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#30830;&#35748;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#19979;&#65292;&#19982;ID&#20934;&#30830;&#24230;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#38656;&#35201;&#25913;&#21892;NLP&#20219;&#21153;&#20013;&#30340;OOD&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
&lt;/p&gt;</description></item><item><title>&#22238;&#39038;&#20102;&#22825;&#25991;&#23398;&#20013;&#36830;&#25509;&#20027;&#20041;&#30340;&#19977;&#27425;&#28010;&#28526;&#65292;&#24182;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22825;&#25991;&#23398;&#20013;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;&#25552;&#20986;&#37319;&#29992;&#20026;&#22825;&#25991;&#24212;&#29992;&#24494;&#35843;&#30340;&#31867;GPT&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#22825;&#25991;&#25968;&#25454;&#35299;&#20915;&#38382;&#39064;&#21644;&#25552;&#20379;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.03796</link><description>&lt;p&gt;
&#22825;&#25991;&#23398;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#21382;&#21490;&#12289;&#20837;&#38376;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Astronomia ex machina: a history, primer, and outlook on neural networks in astronomy. (arXiv:2211.03796v2 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03796
&lt;/p&gt;
&lt;p&gt;
&#22238;&#39038;&#20102;&#22825;&#25991;&#23398;&#20013;&#36830;&#25509;&#20027;&#20041;&#30340;&#19977;&#27425;&#28010;&#28526;&#65292;&#24182;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22825;&#25991;&#23398;&#20013;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;&#25552;&#20986;&#37319;&#29992;&#20026;&#22825;&#25991;&#24212;&#29992;&#24494;&#35843;&#30340;&#31867;GPT&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#22825;&#25991;&#25968;&#25454;&#35299;&#20915;&#38382;&#39064;&#21644;&#25552;&#20379;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#21644;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#22825;&#25991;&#23398;&#20013;&#30340;&#21382;&#21490;&#21457;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;&#25105;&#20204;&#36319;&#36394;&#20102;&#22825;&#25991;&#23398;&#20013;&#36830;&#25509;&#20027;&#20041;&#30340;&#19977;&#27425;&#28010;&#28526;&#30340;&#28436;&#21464;&#65292;&#20174;&#26089;&#26399;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#21040;&#21367;&#31215;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#65292;&#26368;&#32456;&#21040;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#21644;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#38543;&#30528;&#22825;&#25991;&#25968;&#25454;&#30340;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20026;&#25581;&#31034;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#35299;&#20915;&#20197;&#21069;&#26840;&#25163;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#36827;&#20837;&#39044;&#26399;&#30340;&#22825;&#25991;&#36830;&#25509;&#20027;&#20041;&#30340;&#31532;&#22235;&#27425;&#28010;&#28526;&#26102;&#65292;&#37319;&#29992;&#20026;&#22825;&#25991;&#24212;&#29992;&#24494;&#35843;&#30340;&#31867;GPT&#22522;&#30784;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#39640;&#36136;&#37327;&#22825;&#25991;&#25968;&#25454;&#65292;&#20026;&#26368;&#20808;&#36827;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#12290;&#20026;&#20102;&#36319;&#19978;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#25512;&#21160;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#22825;&#25991;&#31038;&#21306;&#20869;&#37319;&#29992;&#21327;&#20316;&#24320;&#28304;&#26041;&#27861;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this review, we explore the historical development and future prospects of artificial intelligence (AI) and deep learning in astronomy. We trace the evolution of connectionism in astronomy through its three waves, from the early use of multilayer perceptrons, to the rise of convolutional and recurrent neural networks, and finally to the current era of unsupervised and generative deep learning methods. With the exponential growth of astronomical data, deep learning techniques offer an unprecedented opportunity to uncover valuable insights and tackle previously intractable problems. As we enter the anticipated fourth wave of astronomical connectionism, we argue for the adoption of GPT-like foundation models fine-tuned for astronomical applications. Such models could harness the wealth of high-quality, multimodal astronomical data to serve state-of-the-art downstream tasks. To keep pace with advancements driven by Big Tech, we propose a collaborative, open-source approach within the as
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22823;&#37327;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20174;&#32780;&#22686;&#24378;&#21407;&#22987;&#35757;&#32451;&#38598;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.03044</link><description>&lt;p&gt;
&#20197;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#20026;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#23398;&#20064;&#23569;&#26679;&#26412;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning. (arXiv:2211.03044v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22823;&#37327;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20174;&#32780;&#22686;&#24378;&#21407;&#22987;&#35757;&#32451;&#38598;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24778;&#20154;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65306;&#23427;&#20204;&#21487;&#20197;&#22312;&#20197;&#25552;&#31034;&#24418;&#24335;&#34920;&#36798;&#30340;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#24494;&#35843;&#21518;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20016;&#23500;&#30340;&#20219;&#21153;&#29305;&#23450;&#27880;&#37322;&#12290;&#23613;&#31649;&#26377;&#30528;&#24456;&#26377;&#21069;&#36884;&#30340;&#34920;&#29616;&#65292;&#20294;&#22823;&#22810;&#25968;&#20165;&#20174;&#23569;&#37327;&#35757;&#32451;&#38598;&#23398;&#20064;&#30340;&#29616;&#26377;&#23569;&#26679;&#26412;&#26041;&#27861;&#20173;&#28982;&#27604;&#38750;&#24179;&#20961;&#30340;&#20840;&#30417;&#30563;&#35757;&#32451;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20351;&#29992;PLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#25105;&#20204;&#39318;&#20808;&#35843;&#25972;&#33258;&#22238;&#24402;PLM&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#20316;&#20026;&#29983;&#25104;&#22120;&#65292;&#21512;&#25104;&#22823;&#37327;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20197;&#22686;&#24378;&#21407;&#22987;&#35757;&#32451;&#38598;&#12290;&#20026;&#20102;&#40723;&#21169;&#29983;&#25104;&#22120;&#20135;&#29983;&#20855;&#26377;&#26631;&#31614;&#21306;&#20998;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#24230;&#37327;&#35757;&#32451;&#23427;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;&#20196;&#29260;&#30340;&#26435;&#37325;&#22522;&#20110;&#19968;&#20010;&#21306;&#20998;&#24615;&#20803;&#23398;&#20064;&#30446;&#26631;&#33258;&#21160;&#35843;&#25972;&#12290;&#28982;&#21518;&#21487;&#20197;&#22312;&#22686;&#21152;&#21518;&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#20998;&#31867;PLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing few-shot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label-discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW&#65292;&#30456;&#36739;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05208</link><description>&lt;p&gt;
&#32593;&#32476;&#27969;&#30340;&#22270;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Modeling of Network Flows. (arXiv:2209.05208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW&#65292;&#30456;&#36739;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#38382;&#39064;&#28041;&#21450;&#23558;&#27969;&#37327;&#20998;&#24067;&#22312;&#32593;&#32476;&#20013;&#65292;&#20197;&#20351;&#22522;&#30784;&#35774;&#26045;&#24471;&#21040;&#26377;&#25928;&#21033;&#29992;&#65292;&#36825;&#22312;&#20132;&#36890;&#36816;&#36755;&#21644;&#29289;&#27969;&#20013;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#20854;&#20013;&#65292;&#22810;&#21830;&#21697;&#32593;&#32476;&#27969; (MCNF) &#38382;&#39064;&#26159;&#26222;&#36941;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#22312;&#22810;&#20010;&#28304;&#21644;&#27719;&#20043;&#38388;&#20998;&#37197;&#19981;&#21516;&#22823;&#23567;&#30340;&#22810;&#20010;&#27969;&#65292;&#21516;&#26102;&#23454;&#29616;&#38142;&#36335;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;&#30001;&#20110;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#30340;&#21560;&#24341;&#21147;&#65292;&#36825;&#20123;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW (Per-Edge Weights)&#12290;&#27492;&#26041;&#27861;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#24182;&#27839;&#30528;&#27599;&#20010;&#38142;&#25509;&#20351;&#29992;&#19981;&#21516;&#21442;&#25968;&#21270;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992; $17$ &#20010;&#26381;&#21153;&#25552;&#20379;&#21830;&#25299;&#25169;&#21644; $2$ &#20010;&#36335;&#30001;&#26041;&#26696;&#36827;&#34892;&#20114;&#32852;&#32593;&#27969;&#37327;&#36335;&#30001;&#26696;&#20363;&#30740;&#31350;&#65292;&#23545;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; PEW &#30456;&#23545;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network flow problems, which involve distributing traffic over a network such that the underlying infrastructure is used effectively, are ubiquitous in transportation and logistics. Among them, the Multi-Commodity Network Flow (MCNF) problem is of general interest, as it concerns the distribution of multiple flows of different sizes between several sources and sinks, while achieving effective utilization of the links. Due to the appeal of data-driven optimization, these problems have increasingly been approached using graph learning methods. In this paper, we propose a novel graph learning architecture for network flow problems called Per-Edge Weights (PEW). This method builds on a Graph Attention Network and uses distinctly parametrized message functions along each link. We extensively evaluate the proposed solution through an Internet flow routing case study using $17$ Service Provider topologies and $2$ routing schemes. We show that PEW yields substantial gains over architectures wh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23376;&#20108;&#27425;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Kronecker&#22238;&#24402;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#26464;&#26438;&#24471;&#20998;&#37319;&#26679;&#21644;&#36845;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#36816;&#31639;&#26102;&#38388;&#20013;&#30340;&#25351;&#25968;&#39033;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#21040; Kronecker&#23725;&#22238;&#24402;&#21644;&#26356;&#26032;&#24352;&#37327;Tucker&#20998;&#35299;&#20013;&#30340;&#22240;&#23376;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2209.04876</link><description>&lt;p&gt;
&#23376;&#20108;&#27425;&#32423;Kronecker&#22238;&#24402;&#21450;&#20854;&#22312;&#24352;&#37327;&#20998;&#35299;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Subquadratic Kronecker Regression with Applications to Tensor Decomposition. (arXiv:2209.04876v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23376;&#20108;&#27425;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Kronecker&#22238;&#24402;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#26464;&#26438;&#24471;&#20998;&#37319;&#26679;&#21644;&#36845;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#36816;&#31639;&#26102;&#38388;&#20013;&#30340;&#25351;&#25968;&#39033;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#21040; Kronecker&#23725;&#22238;&#24402;&#21644;&#26356;&#26032;&#24352;&#37327;Tucker&#20998;&#35299;&#20013;&#30340;&#22240;&#23376;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kronecker&#22238;&#24402;&#26159;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#21363; $\min_{\mathbf{x}} \lVert \mathbf{K}\mathbf{x} - \mathbf{b} \rVert_{2}^2$&#65292;&#20854;&#20013;&#35774;&#35745;&#30697;&#38453; $\mathbf{K} = \mathbf{A}^{(1)} \otimes \cdots \otimes \mathbf{A}^{(N)}$ &#26159;&#22240;&#23376;&#30697;&#38453;&#30340; Kronecker&#31215;&#12290;&#36825;&#20010;&#22238;&#24402;&#38382;&#39064;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#65288;ALS&#65289;&#31639;&#27861;&#30340;&#27599;&#19968;&#27493;&#20013;&#29992;&#20110;&#35745;&#31639;&#24352;&#37327;&#30340; Tucker&#20998;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35299;&#20915;Kronecker&#22238;&#24402;&#38382;&#39064;&#30340;&#23376;&#20108;&#27425;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#36798;&#21040; $(1+\varepsilon)$ &#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#32780;&#19988;&#36991;&#20813;&#20102;&#36816;&#34892;&#26102;&#38388;&#20013;&#30340;&#25351;&#25968;&#39033; $O(\varepsilon^{-N})$&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#32467;&#21512;&#20102;&#26464;&#26438;&#24471;&#20998;&#37319;&#26679;&#21644;&#36845;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20854;&#20013;&#19968;&#20010;&#22359;&#26159;&#19968;&#20010; Kronecker&#31215;&#30340;&#22359;&#35774;&#35745;&#30697;&#38453;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#29992;&#20110; (1) Kronecker&#23725;&#22238;&#24402; &#21644; (2) &#22312;ALS&#20013;&#26356;&#26032;&#24352;&#37327;&#30340; Tucker&#20998;&#35299;&#30340;&#22240;&#23376;&#30697;&#38453;&#30340;&#23376;&#20108;&#27425;&#26102;&#38388;&#31639;&#27861;&#65292;&#21363;&#19981;&#26159;&#19968;&#20010;&#32431;Krconecker&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kronecker regression is a highly-structured least squares problem $\min_{\mathbf{x}} \lVert \mathbf{K}\mathbf{x} - \mathbf{b} \rVert_{2}^2$, where the design matrix $\mathbf{K} = \mathbf{A}^{(1)} \otimes \cdots \otimes \mathbf{A}^{(N)}$ is a Kronecker product of factor matrices. This regression problem arises in each step of the widely-used alternating least squares (ALS) algorithm for computing the Tucker decomposition of a tensor. We present the first subquadratic-time algorithm for solving Kronecker regression to a $(1+\varepsilon)$-approximation that avoids the exponential term $O(\varepsilon^{-N})$ in the running time. Our techniques combine leverage score sampling and iterative methods. By extending our approach to block-design matrices where one block is a Kronecker product, we also achieve subquadratic-time algorithms for (1) Kronecker ridge regression and (2) updating the factor matrices of a Tucker decomposition in ALS, which is not a pure Kronecker regression problem, thereb
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#23610;&#24230;&#24314;&#27169;&#20013;&#37319;&#26679;&#21644;&#38598;&#21512;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#19982;&#20998;&#23376;&#27169;&#25311;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2208.10715</link><description>&lt;p&gt;
GAN&#21644;&#38381;&#21512;&#24615;: &#22810;&#23610;&#24230;&#24314;&#27169;&#20013;&#30340;&#24494;&#35266;-&#23439;&#35266;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GANs and Closures: Micro-Macro Consistency in Multiscale Modeling. (arXiv:2208.10715v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10715
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#23610;&#24230;&#24314;&#27169;&#20013;&#37319;&#26679;&#21644;&#38598;&#21512;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#19982;&#20998;&#23376;&#27169;&#25311;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#26679;&#20998;&#23376;&#31995;&#32479;&#30340;&#30456;&#31354;&#38388;&#65288;&#26356;&#24191;&#20041;&#30340;&#26159;&#65292;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#30456;&#31354;&#38388;&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#26159;&#37325;&#35201;&#30340;&#24314;&#27169;&#27493;&#39588;&#65292;&#20174;&#34507;&#30333;&#36136;&#25240;&#21472;&#21040;&#26448;&#26009;&#21457;&#29616;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#20855;&#26377;&#22810;&#23610;&#24230;&#24615;&#36136;&#65306;&#23427;&#20204;&#21487;&#20197;&#29992;&#23569;&#37327;&#8220;&#32531;&#24930;&#8221;&#21453;&#24212;&#22352;&#26631;&#21442;&#25968;&#21270;&#30340;&#20302;&#32500;&#26377;&#25928;&#33258;&#30001;&#33021;&#38754;&#26469;&#25551;&#36848;&#65307;&#21097;&#20313;&#30340;&#8220;&#24555;&#36895;&#8221;&#33258;&#30001;&#24230;&#22312;&#21453;&#24212;&#22352;&#26631;&#20540;&#19978;&#22635;&#20805;&#24179;&#34913;&#27979;&#24230;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#37319;&#26679;&#36807;&#31243;&#29992;&#20110;&#20272;&#35745;&#26377;&#25928;&#30340;&#33258;&#30001;&#33021;&#24046;&#24322;&#21644;&#19982;&#26465;&#20214;&#24179;&#34913;&#20998;&#24067;&#30456;&#20851;&#30340;&#38598;&#21512;&#24179;&#22343;&#25968;&#65307;&#36825;&#20123;&#21518;&#32773;&#30340;&#24179;&#22343;&#25968;&#23548;&#33268;&#26377;&#25928;&#30340;&#20943;&#23569;&#21160;&#24577;&#27169;&#22411;&#30340;&#38381;&#21512;&#12290;&#22810;&#24180;&#26469;&#65292;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#19982;&#20998;&#23376;&#27169;&#25311;&#30456;&#32467;&#21512;&#12290;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#23384;&#22312;&#19968;&#20010;&#26377;&#36259;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling the phase space of molecular systems -- and, more generally, of complex systems effectively modeled by stochastic differential equations -- is a crucial modeling step in many fields, from protein folding to materials discovery. These problems are often multiscale in nature: they can be described in terms of low-dimensional effective free energy surfaces parametrized by a small number of "slow" reaction coordinates; the remaining "fast" degrees of freedom populate an equilibrium measure on the reaction coordinate values. Sampling procedures for such problems are used to estimate effective free energy differences as well as ensemble averages with respect to the conditional equilibrium distributions; these latter averages lead to closures for effective reduced dynamic models. Over the years, enhanced sampling techniques coupled with molecular simulation have been developed. An intriguing analogy arises with the field of Machine Learning (ML), where Generative Adversarial Networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#38134;&#34892;&#21387;&#21147;&#27979;&#35797;&#20013;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#19981;&#21516;&#26426;&#26500;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#24322;&#24615;&#20197;&#21450;&#31616;&#21333;&#27719;&#38598;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22238;&#24402;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#20197;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2207.13319</link><description>&lt;p&gt;
&#38134;&#34892;&#21387;&#21147;&#27979;&#35797;&#26159;&#21542;&#24212;&#35813;&#20844;&#27491;?
&lt;/p&gt;
&lt;p&gt;
Should Bank Stress Tests Be Fair?. (arXiv:2207.13319v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#38134;&#34892;&#21387;&#21147;&#27979;&#35797;&#20013;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#19981;&#21516;&#26426;&#26500;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#24322;&#24615;&#20197;&#21450;&#31616;&#21333;&#27719;&#38598;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22238;&#24402;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#20197;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#31649;&#21387;&#21147;&#27979;&#35797;&#24050;&#25104;&#20026;&#35774;&#23450;&#32654;&#22269;&#26368;&#22823;&#38134;&#34892;&#36164;&#26412;&#35201;&#27714;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#32852;&#37030;&#20648;&#22791;&#31995;&#32479;&#20351;&#29992;&#20445;&#23494;&#27169;&#22411;&#65292;&#22312;&#20849;&#20139;&#21387;&#21147;&#24773;&#26223;&#20013;&#35780;&#20272;&#29305;&#23450;&#38134;&#34892;&#25237;&#36164;&#32452;&#21512;&#30340;&#38134;&#34892;&#29305;&#23450;&#32467;&#26524;&#12290;&#23613;&#31649;&#19981;&#21516;&#26426;&#26500;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24322;&#36136;&#24615;&#65292;&#20294;&#26681;&#25454;&#25919;&#31574;&#65292;&#25152;&#26377;&#38134;&#34892;&#22343;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#65292;&#32780;&#20010;&#21035;&#38134;&#34892;&#35748;&#20026;&#26576;&#20123;&#27169;&#22411;&#19981;&#36866;&#21512;&#20854;&#19994;&#21153;&#12290;&#22312;&#36825;&#22330;&#36777;&#35770;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#35201;&#38382;&#30340;&#26159;&#65292;&#22914;&#20309;&#23558;&#20010;&#24615;&#21270;&#23450;&#21046;&#27169;&#22411;&#32858;&#21512;&#25104;&#19968;&#20010;&#20849;&#21516;&#30340;&#27169;&#22411;&#26159;&#20844;&#24179;&#30340;&#65311;&#25105;&#20204;&#35748;&#20026;&#65292;&#20165;&#31616;&#21333;&#22320;&#23558;&#25968;&#25454;&#27719;&#38598;&#21040;&#38134;&#34892;&#24182;&#21015;&#22788;&#29702;&#34429;&#28982;&#23545;&#25152;&#26377;&#38134;&#34892;&#20844;&#24179;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#24330;&#31471;:&#23427;&#21487;&#33021;&#27498;&#26354;&#21512;&#27861;&#25237;&#36164;&#32452;&#21512;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#19988;&#23481;&#26131;&#23558;&#21512;&#27861;&#30340;&#20449;&#24687;&#24046;&#21521;&#38134;&#34892;&#36523;&#20221;&#30340;&#25512;&#26029;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181;&#22238;&#24402;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#32771;&#34385;&#21040;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#24179;&#31561;&#24453;&#36935;&#12290;&#22312;&#32447;&#24615;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;
&lt;/p&gt;
&lt;p&gt;
Regulatory stress tests have become one of the main tools for setting capital requirements at the largest U.S. banks. The Federal Reserve uses confidential models to evaluate bank-specific outcomes for bank-specific portfolios in shared stress scenarios. As a matter of policy, the same models are used for all banks, despite considerable heterogeneity across institutions; individual banks have contended that some models are not suited to their businesses. Motivated by this debate, we ask, what is a fair aggregation of individually tailored models into a common model? We argue that simply pooling data across banks treats banks equally but is subject to two deficiencies: it may distort the impact of legitimate portfolio features, and it is vulnerable to implicit misdirection of legitimate information to infer bank identity. We compare various notions of regression fairness to address these deficiencies, considering both forecast accuracy and equal treatment. In the setting of linear model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;RL&#22312;&#21435;&#30417;&#31649;&#30005;&#21147;&#24066;&#22330;&#20013;&#24212;&#29992;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;RL&#30456;&#36739;&#20110;&#20256;&#32479;&#20248;&#21270;&#24037;&#20855;&#20855;&#26377;&#20248;&#21183;&#65292;&#33021;&#22815;&#35299;&#20915;&#29305;&#24449;&#19981;&#30830;&#23450;&#12289;&#35745;&#31639;&#25928;&#29575;&#20302;&#31561;&#38590;&#39064;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.08369</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21435;&#30417;&#31649;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#21512;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Applications of Reinforcement Learning in Deregulated Power Market: A Comprehensive Review. (arXiv:2205.08369v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;RL&#22312;&#21435;&#30417;&#31649;&#30005;&#21147;&#24066;&#22330;&#20013;&#24212;&#29992;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;RL&#30456;&#36739;&#20110;&#20256;&#32479;&#20248;&#21270;&#24037;&#20855;&#20855;&#26377;&#20248;&#21183;&#65292;&#33021;&#22815;&#35299;&#20915;&#29305;&#24449;&#19981;&#30830;&#23450;&#12289;&#35745;&#31639;&#25928;&#29575;&#20302;&#31561;&#38590;&#39064;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#28183;&#36879;&#21644;&#30005;&#21147;&#34892;&#19994;&#30340;&#21435;&#30417;&#31649;&#19982;&#24066;&#22330;&#21270;&#65292;&#30005;&#21147;&#24066;&#22330;&#36816;&#33829;&#33539;&#24335;&#27491;&#22312;&#21457;&#29983;&#36716;&#21464;&#12290;&#22312;&#36825;&#20123;&#26032;&#33539;&#24335;&#19979;&#65292;&#20248;&#21270;&#25237;&#26631;&#31574;&#30053;&#21644;&#20998;&#37197;&#26041;&#27861;&#25104;&#20026;&#24066;&#22330;&#21442;&#19982;&#32773;&#21644;&#30005;&#21147;&#31995;&#32479;&#36816;&#33829;&#21830;&#30340;&#20248;&#20808;&#32771;&#34385;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#30528;&#29305;&#24449;&#19981;&#30830;&#23450;&#12289;&#35745;&#31639;&#25928;&#29575;&#20302;&#12289;&#36229;&#21069;&#20915;&#31574;&#35201;&#27714;&#31561;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#20248;&#21270;&#24037;&#20855;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;RL&#22312;&#21435;&#30417;&#31649;&#30005;&#21147;&#24066;&#22330;&#36816;&#33829;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25237;&#26631;&#21644;&#20998;&#37197;&#31574;&#30053;&#20248;&#21270;&#31561;&#26041;&#38754;&#65292;&#22522;&#20110;150&#20313;&#31687;&#31934;&#36873;&#30340;&#25991;&#29486;&#12290;&#23545;&#20110;&#27599;&#20010;&#24212;&#29992;&#65292;&#38500;&#20102;&#24635;&#32467;&#24191;&#20041;&#26041;&#27861;&#30340;&#33539;&#20363;&#20043;&#22806;&#65292;&#36824;&#37325;&#28857;&#38416;&#36848;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing penetration of renewable generations, along with the deregulation and marketization of power industry, promotes the transformation of power market operation paradigms. The optimal bidding strategy and dispatching methodology under these new paradigms are prioritized concerns for both market participants and power system operators, with obstacles of uncertain characteristics, computational efficiency, as well as requirements of hyperopic decision-making. To tackle these problems, the Reinforcement Learning (RL), as an emerging machine learning technique with advantages compared with conventional optimization tools, is playing an increasingly significant role in both academia and industry. This paper presents a comprehensive review of RL applications in deregulated power market operation including bidding and dispatching strategy optimization, based on more than 150 carefully selected literatures. For each application, apart from a paradigmatic summary of generalized metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#22411;&#37327;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20302;&#20301;&#23485;&#23384;&#20648;&#20840;&#31934;&#24230;&#20540;&#20197;&#23454;&#29616;&#33410;&#32422;&#20869;&#23384;&#21644;&#25805;&#20316;&#25104;&#26412;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#25991;&#31456;&#20998;&#31867;&#20171;&#32461;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#27169;&#22411;&#37327;&#21270;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#32570;&#28857;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2205.07877</link><description>&lt;p&gt;
&#27169;&#22411;&#37327;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Model Quantization for Deep Neural Networks. (arXiv:2205.07877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#22411;&#37327;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20302;&#20301;&#23485;&#23384;&#20648;&#20840;&#31934;&#24230;&#20540;&#20197;&#23454;&#29616;&#33410;&#32422;&#20869;&#23384;&#21644;&#25805;&#20316;&#25104;&#26412;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#25991;&#31456;&#20998;&#31867;&#20171;&#32461;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#27169;&#22411;&#37327;&#21270;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#32570;&#28857;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#23384;&#20648;&#21644;&#36816;&#31639;&#20250;&#24102;&#26469;&#30828;&#20214;&#25104;&#26412;&#30340;&#22686;&#21152;&#21644;&#25361;&#25112;&#12290;&#23545;&#27492;&#65292;&#25552;&#20986;&#20102;&#21387;&#32553;&#26041;&#27861;&#20197;&#35774;&#35745;&#39640;&#25928;&#30340;&#21152;&#36895;&#22120;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#27861;&#26159;&#25226;&#20840;&#31934;&#24230;&#30340;&#20540;&#23384;&#20648;&#22312;&#20302;&#20301;&#23485;&#20013;&#65292;&#36825;&#23601;&#21487;&#20197;&#33410;&#32422;&#20869;&#23384;&#21516;&#26102;&#29992;&#20302;&#25104;&#26412;&#30340;&#31616;&#21333;&#36816;&#31639;&#20195;&#26367;&#21407;&#26412;&#30340;&#25805;&#20316;&#12290;&#30001;&#20110;&#27169;&#22411;&#37327;&#21270;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#35774;&#35745;&#39640;&#25928;&#30828;&#20214;&#30340;&#24433;&#21709;&#65292;&#26368;&#36817;&#20960;&#24180;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#26041;&#27861;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#27604;&#36739;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#37327;&#21270;&#27010;&#24565;&#24182;&#20174;&#19981;&#21516;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#36991;&#20813;&#31934;&#24230;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#23545;&#27169;&#22411;&#37327;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning by deep neural networks are significant. But using these networks has been accompanied by a huge number of parameters for storage and computations that leads to an increase in the hardware cost and posing challenges. Therefore, compression approaches have been proposed to design efficient accelerators. One important approach for deep neural network compression is quantization that full-precision values are stored in low bit-width. In this way, in addition to memory saving, the operations will be replaced by simple ones with low cost. Many methods are suggested for DNNs Quantization in recent years, because of flexibility and influence in designing efficient hardware. Therefore, an integrated report is essential for better understanding, analysis, and comparison. In this paper, we provide a comprehensive survey. We describe the quantization concepts and categorize the methods from different perspectives. We discuss using the scale factor to match the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36125;&#21494;&#26031;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#25805;&#20316;&#26426;&#32676;&#20013;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#22312;&#19981;&#21516;&#30340;&#23376;&#32676;&#20043;&#38388;&#33258;&#21160;&#22320;&#20849;&#20139;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21345;&#36710;&#26426;&#32676;&#30340;&#29983;&#23384;&#20998;&#26512;&#21644;&#39118;&#30005;&#22330;&#30340;&#21151;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.12404</link><description>&lt;p&gt;
&#20998;&#23618;&#36125;&#21494;&#26031;&#24314;&#27169;&#22312;&#24037;&#31243;&#26426;&#32676;&#38388;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Bayesian Modelling for Knowledge Transfer Across Engineering Fleets via Multitask Learning. (arXiv:2204.12404v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36125;&#21494;&#26031;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#25805;&#20316;&#26426;&#32676;&#20013;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#22312;&#19981;&#21516;&#30340;&#23376;&#32676;&#20043;&#38388;&#33258;&#21160;&#22320;&#20849;&#20139;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21345;&#36710;&#26426;&#32676;&#30340;&#29983;&#23384;&#20998;&#26512;&#21644;&#39118;&#30005;&#22330;&#30340;&#21151;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#32423;&#21035;&#20998;&#26512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24037;&#31243;&#22522;&#30784;&#35774;&#26045;&#39044;&#27979;&#24314;&#27169;&#20013;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#25805;&#20316;&#26426;&#32676;&#25968;&#25454;&#65292;&#33258;&#28982;&#22320;&#23558;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#32534;&#30721;&#65288;&#21644;&#36866;&#24403;&#20849;&#20139;&#65289;&#21040;&#19981;&#21516;&#23376;&#32676;&#20043;&#38388;&#65292;&#20998;&#21035;&#20195;&#34920;&#65288;i&#65289;&#20351;&#29992;&#31867;&#22411;&#65292;&#65288;ii&#65289;&#37096;&#20214;&#25110;&#65288;iii&#65289;&#36816;&#34892;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20551;&#35774;&#65288;&#21644;&#20808;&#39564;&#20998;&#24067;&#65289;&#65292;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#38480;&#21046;&#27169;&#22411;&#65292;&#20351;&#24471;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#22312;&#31867;&#20284;&#36164;&#20135;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#21345;&#36710;&#26426;&#32676;&#30340;&#29983;&#23384;&#20998;&#26512;&#21644;&#39118;&#30005;&#22330;&#30340;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27599;&#20010;&#36164;&#20135;&#31649;&#29702;&#31034;&#20363;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#25512;&#29702;&#65292;&#22312;&#26426;&#32676;&#19978;&#23398;&#20064;&#19968;&#32452;&#30456;&#20851;&#20989;&#25968;&#65292;&#20197;&#23398;&#20064;&#32676;&#20307;&#27169;&#22411;&#12290;&#24403;&#23376;&#26426;&#32676;&#22312;&#23618;&#27425;&#32467;&#26500;&#30340;&#19981;&#21516;&#32423;&#21035;&#19978;&#20849;&#20139;&#30456;&#20851;&#20449;&#24687;&#26102;&#65292;&#21442;&#25968;&#20272;&#35745;&#24471;&#21040;&#25913;&#36827;&#12290;&#21453;&#36807;&#26469;&#65292;&#20855;&#26377;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#32676;&#20307;&#33258;&#21160;&#20511;&#29992;&#32479;&#35745;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A population-level analysis is proposed to address data sparsity when building predictive models for engineering infrastructure. Utilising an interpretable hierarchical Bayesian approach and operational fleet data, domain expertise is naturally encoded (and appropriately shared) between different sub-groups, representing (i) use-type, (ii) component, or (iii) operating condition. Specifically, domain expertise is exploited to constrain the model via assumptions (and prior distributions) allowing the methodology to automatically share information between similar assets, improving the survival analysis of a truck fleet and power prediction in a wind farm. In each asset management example, a set of correlated functions is learnt over the fleet, in a combined inference, to learn a population model. Parameter estimation is improved when sub-fleets share correlated information at different levels of the hierarchy. In turn, groups with incomplete data automatically borrow statistical strength
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.01937</link><description>&lt;p&gt;
BoMD&#65306;&#36866;&#29992;&#20110;&#22024;&#26434;X&#20809;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#21253;
&lt;/p&gt;
&lt;p&gt;
BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#38382;&#39064;&#30340;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20855;&#26377;&#28165;&#27905;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#36825;&#31181;&#25163;&#21160;&#27880;&#37322;&#30340;&#39640;&#25104;&#26412;&#65292;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#20381;&#36182;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#20107;&#23454;&#19978;&#65292;&#35768;&#22810;&#33016;&#37096;X&#20809;&#20998;&#31867;&#22120;&#24050;&#32463;&#20174;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#19981;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#23548;&#33268;&#27425;&#20248;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;CXR&#25968;&#25454;&#38598;&#22823;&#22810;&#26159;&#22810;&#26631;&#35760;&#30340;&#65292;&#22240;&#27492;&#24403;&#21069;&#35774;&#35745;&#29992;&#20110;&#22810;&#31867;&#38382;&#39064;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#19981;&#33021;&#36731;&#26494;&#22320;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22024;&#26434;&#22810;&#26631;&#31614;CXR&#23398;&#20064;&#65292;&#20854;&#20013;&#26816;&#27979;&#24182;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#24120;&#35265;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#20351;&#29992;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#20854;&#20248;&#28857;&#21450;&#23616;&#38480;&#24615;&#65292;&#20174;&#32593;&#32476;&#32467;&#26500;&#21644;&#24212;&#29992;&#20004;&#20010;&#35282;&#24230;&#23457;&#35270;&#20102;&#20854;&#36866;&#24212;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2202.07125</link><description>&lt;p&gt;
Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformers in Time Series: A Survey. (arXiv:2202.07125v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#20854;&#20248;&#28857;&#21450;&#23616;&#38480;&#24615;&#65292;&#20174;&#32593;&#32476;&#32467;&#26500;&#21644;&#24212;&#29992;&#20004;&#20010;&#35282;&#24230;&#23457;&#35270;&#20102;&#20854;&#36866;&#24212;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20063;&#24341;&#36215;&#20102;&#26102;&#38388;&#24207;&#21015;&#31038;&#21306;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;Transformer&#30340;&#22810;&#20010;&#20248;&#21183;&#20043;&#19968;&#26159;&#33021;&#22815;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#29305;&#21035;&#36866;&#21512;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#28857;&#21450;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#32593;&#32476;&#32467;&#26500;&#21644;&#24212;&#29992;&#20004;&#20010;&#23618;&#38754;&#23457;&#35270;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#36866;&#24212;&#21644;&#25913;&#36827;&#12290;&#20174;&#32593;&#32476;&#32467;&#26500;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20026;&#20102;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#32780;&#20570;&#20986;&#30340;&#25913;&#21464;&#21644;&#35843;&#25972;&#12290;&#20174;&#24212;&#29992;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#26681;&#25454;&#24120;&#35265;&#20219;&#21153;&#65288;&#21253;&#25324;&#39044;&#27979;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#31867;&#65289;&#23545;&#26102;&#38388;&#24207;&#21015;Transformer&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#21033;&#29992;&#27874;&#12289;&#36335;&#24452;&#34892;&#31243;&#26102;&#38388;&#21644;eikonal&#26041;&#31243;&#26469;&#25551;&#36848;&#20449;&#24687;&#30340;&#20256;&#25773;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#27874;&#21644;eikonal&#27169;&#22411;&#30340;&#32467;&#21512;&#12290;&#20316;&#32773;&#22312;&#38543;&#26426;&#22270;&#24418;&#12289;&#23567;&#19990;&#30028;&#22270;&#21644;&#23454;&#38469;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2201.07577</link><description>&lt;p&gt;
&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Models for information propagation on graphs. (arXiv:2201.07577v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#21033;&#29992;&#27874;&#12289;&#36335;&#24452;&#34892;&#31243;&#26102;&#38388;&#21644;eikonal&#26041;&#31243;&#26469;&#25551;&#36848;&#20449;&#24687;&#30340;&#20256;&#25773;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#27874;&#21644;eikonal&#27169;&#22411;&#30340;&#32467;&#21512;&#12290;&#20316;&#32773;&#22312;&#38543;&#26426;&#22270;&#24418;&#12289;&#23567;&#19990;&#30028;&#22270;&#21644;&#23454;&#38469;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#21644;&#32479;&#19968;&#20102;&#19981;&#21516;&#30340;&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#27169;&#22411;&#12290;&#31532;&#19968;&#31867;&#27169;&#22411;&#23558;&#20256;&#25773;&#24314;&#27169;&#20026;&#19968;&#31181;&#27874;&#65292;&#23427;&#20174;&#19968;&#32452;&#24050;&#30693;&#33410;&#28857;&#22312;&#21021;&#22987;&#26102;&#38388;&#24320;&#22987;&#21521;&#25152;&#26377;&#20854;&#20182;&#26410;&#30693;&#33410;&#28857;&#20256;&#25773;&#65292;&#20256;&#25773;&#30340;&#39034;&#24207;&#30001;&#20449;&#24687;&#27874;&#21069;&#30340;&#21040;&#36798;&#26102;&#38388;&#30830;&#23450;&#12290;&#31532;&#20108;&#31867;&#27169;&#22411;&#22522;&#20110;&#36335;&#24452;&#19978;&#30340;&#34892;&#31243;&#26102;&#38388;&#30340;&#27010;&#24565;&#12290;&#20174;&#19968;&#32452;&#21021;&#22987;&#24050;&#30693;&#33410;&#28857;&#21040;&#19968;&#20010;&#33410;&#28857;&#30340;&#20449;&#24687;&#20256;&#25773;&#26102;&#38388;&#34987;&#23450;&#20041;&#20026;&#25152;&#26377;&#21487;&#20197;&#21040;&#36798;&#35813;&#33410;&#28857;&#30340;&#36335;&#24452;&#30340;&#23376;&#38598;&#19978;&#30340;&#24191;&#20041;&#26053;&#34892;&#26102;&#38388;&#30340;&#26368;&#23567;&#20540;&#12290;&#26368;&#21518;&#19968;&#20010;&#27169;&#22411;&#31867;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#26410;&#30693;&#33410;&#28857;&#19978;&#26045;&#21152;&#19968;&#20010;eikonal&#24418;&#24335;&#30340;&#23616;&#37096;&#26041;&#31243;&#65292;&#24182;&#22312;&#24050;&#30693;&#33410;&#28857;&#22788;&#26045;&#21152;&#36793;&#30028;&#26465;&#20214;&#26469;&#32473;&#20986;&#30340;&#12290;&#22312;&#19968;&#20010;&#33410;&#28857;&#30340;&#35299;&#30340;&#20540;&#19982;&#20855;&#26377;&#36739;&#20302;&#20540;&#30340;&#30456;&#37051;&#33410;&#28857;&#30340;&#35299;&#30340;&#20540;&#32806;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#31867;&#30340;&#31934;&#30830;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#21463;&#21040;&#31532;&#19968;&#21040;&#36798;&#26102;&#38388;&#27169;&#22411;&#21644;eikonal&#26041;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24418;&#24335;&#65292;&#32467;&#21512;&#20102;&#27874;&#21644;eikonal&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22270;&#24418;&#19978;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25968;&#20540;&#27169;&#25311;&#65292;&#21253;&#25324;&#38543;&#26426;&#22270;&#24418;&#12289;&#23567;&#19990;&#30028;&#22270;&#21644;&#23454;&#38469;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and unify classes of different models for information propagation over graphs. In a first class, propagation is modelled as a wave which emanates from a set of known nodes at an initial time, to all other unknown nodes at later times with an ordering determined by the arrival time of the information wave front. A second class of models is based on the notion of a travel time along paths between nodes. The time of information propagation from an initial known set of nodes to a node is defined as the minimum of a generalised travel time over subsets of all admissible paths. A final class is given by imposing a local equation of an eikonal form at each unknown node, with boundary conditions at the known nodes. The solution value of the local equation at a node is coupled to those of neighbouring nodes with lower values. We provide precise formulations of the model classes and prove equivalences between them. Motivated by the connection between first arrival time model and the e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;ZEROS&#30340;&#36830;&#25509;&#27010;&#24565;&#26469;&#35780;&#20272;DARTS&#20013;&#30340;&#25805;&#20316;&#37325;&#35201;&#24615;&#65292;&#20351;&#24471;&#25972;&#20010;&#26550;&#26500;&#30340;&#25628;&#32034;&#36807;&#31243;&#26356;&#39640;&#25928;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;NTK&#29702;&#35770;&#30340;&#20840;&#26032;&#26694;&#26550;FreeDARTS&#12290;</title><link>http://arxiv.org/abs/2106.11542</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#25509;&#25935;&#24863;&#24615;&#30340;&#35757;&#32451;&#20813;&#36153;DARTS&#65306;&#20174;&#26550;&#26500;&#32423;&#35780;&#20998;&#21040;&#25805;&#20316;&#32423;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Connection Sensitivity Matters for Training-free DARTS: From Architecture-Level Scoring to Operation-Level Sensitivity Analysis. (arXiv:2106.11542v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;ZEROS&#30340;&#36830;&#25509;&#27010;&#24565;&#26469;&#35780;&#20272;DARTS&#20013;&#30340;&#25805;&#20316;&#37325;&#35201;&#24615;&#65292;&#20351;&#24471;&#25972;&#20010;&#26550;&#26500;&#30340;&#25628;&#32034;&#36807;&#31243;&#26356;&#39640;&#25928;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;NTK&#29702;&#35770;&#30340;&#20840;&#26032;&#26694;&#26550;FreeDARTS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#35757;&#32451;&#20813;&#36153;NAS&#26041;&#27861;&#25918;&#24323;&#20102;&#35757;&#32451;&#38454;&#27573;&#24182;&#35774;&#35745;&#20102;&#21508;&#31181;&#38646;&#25104;&#26412;&#20195;&#29702;&#20316;&#20026;&#35780;&#20998;&#65292;&#20197;&#35782;&#21035;&#20986;&#20248;&#31168;&#30340;&#26550;&#26500;&#65292;&#24341;&#36215;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#26497;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#35757;&#32451;&#20813;&#36153;&#30340;&#26041;&#24335;&#36866;&#24403;&#22320;&#27979;&#37327;DARTS&#20013;&#30340;&#25805;&#20316;&#37325;&#35201;&#24615;&#65292;&#36991;&#20813;&#21442;&#25968;&#23494;&#38598;&#30340;&#20559;&#24046;&#65311;&#25105;&#20204;&#36890;&#36807;&#36793;&#32536;&#36830;&#36890;&#24615;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32943;&#23450;&#30340;&#31572;&#26696;&#65292;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#36830;&#25509;&#27010;&#24565;&#8220;ZERo-cost Operation Sensitivity (ZEROS)&#8221;&#65292;&#26469;&#35780;&#20998;DARTS&#20013;&#20505;&#36873;&#25805;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36845;&#20195;&#21644;&#25968;&#25454;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;ZEROS&#36827;&#34892;NAS&#65292;&#25105;&#20204;&#30340;&#26032;&#23581;&#35797;&#23548;&#33268;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;training free differentiable architecture search (FreeDARTS)&#8221;&#30340;&#26694;&#26550;&#12290;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#36830;&#25509;&#35780;&#20998;&#19982;&#27867;&#21270;&#19979;&#38477;&#21576;&#36127;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed training-free NAS methods abandon the training phase and design various zero-cost proxies as scores to identify excellent architectures, arousing extreme computational efficiency for neural architecture search. In this paper, we raise an interesting problem: can we properly measure the operation importance in DARTS through a training-free way, with avoiding the parameter-intensive bias? We investigate this question through the lens of edge connectivity, and provide an affirmative answer by defining a connectivity concept, ZERo-cost Operation Sensitivity (ZEROS), to score the importance of candidate operations in DARTS at initialization. By devising an iterative and data-agnostic manner in utilizing ZEROS for NAS, our novel trial leads to a framework called training free differentiable architecture search (FreeDARTS). Based on the theory of Neural Tangent Kernel (NTK), we show the proposed connectivity score provably negatively correlated with the generalization bo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#26435;&#37325;&#20989;&#25968;&#25351;&#23450;&#30340;&#32463;&#39564;&#25439;&#22833;&#26368;&#23567;&#21270;&#31995;&#25968;&#12290;&#25130;&#36317;&#21457;&#25955;&#20294;&#20854;&#20313;&#31995;&#25968;&#21521;&#37327;&#26377;&#19968;&#20010;&#26377;&#38480;&#30340;&#20960;&#20046;&#32943;&#23450;&#30340;&#26497;&#38480;&#65292;&#26497;&#38480;&#20381;&#36182;&#20110;&#26435;&#37325;&#20989;&#25968;&#30340;&#24038;&#23614;&#22686;&#38271;&#36895;&#29575;&#12290;&#26497;&#38480;&#31995;&#25968;&#21521;&#37327;&#21453;&#26144;&#31283;&#20581;&#24615;&#25110;&#20445;&#23432;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#20122;&#25351;&#25968;&#24773;&#20917;&#19979;&#65292;&#26497;&#38480;&#31561;&#20215;&#20110;&#23569;&#25968;&#31867;&#30340;&#19978;&#37319;&#26679;&#20998;&#24067;&#30340;&#38544;&#24335;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2106.05797</link><description>&lt;p&gt;
&#26080;&#38480;&#19981;&#24179;&#34913;&#19979;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Linear Classifiers Under Infinite Imbalance. (arXiv:2106.05797v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05797
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#26435;&#37325;&#20989;&#25968;&#25351;&#23450;&#30340;&#32463;&#39564;&#25439;&#22833;&#26368;&#23567;&#21270;&#31995;&#25968;&#12290;&#25130;&#36317;&#21457;&#25955;&#20294;&#20854;&#20313;&#31995;&#25968;&#21521;&#37327;&#26377;&#19968;&#20010;&#26377;&#38480;&#30340;&#20960;&#20046;&#32943;&#23450;&#30340;&#26497;&#38480;&#65292;&#26497;&#38480;&#20381;&#36182;&#20110;&#26435;&#37325;&#20989;&#25968;&#30340;&#24038;&#23614;&#22686;&#38271;&#36895;&#29575;&#12290;&#26497;&#38480;&#31995;&#25968;&#21521;&#37327;&#21453;&#26144;&#31283;&#20581;&#24615;&#25110;&#20445;&#23432;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#20122;&#25351;&#25968;&#24773;&#20917;&#19979;&#65292;&#26497;&#38480;&#31561;&#20215;&#20110;&#23569;&#25968;&#31867;&#30340;&#19978;&#37319;&#26679;&#20998;&#24067;&#30340;&#38544;&#24335;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#22686;&#38271;&#21040;&#26080;&#31351;&#22823;&#32780;&#21478;&#19968;&#20010;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#20445;&#25345;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#32447;&#24615;&#21028;&#21035;&#20989;&#25968;&#30340;&#34892;&#20026;&#12290;&#20998;&#31867;&#22120;&#30340;&#31995;&#25968;&#36890;&#36807;&#19968;&#20010;&#26435;&#37325;&#20989;&#25968;&#25351;&#23450;&#30340;&#32463;&#39564;&#25439;&#22833;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#26435;&#37325;&#20989;&#25968;&#31867;&#65292;&#25130;&#36317;&#21457;&#25955;&#20294;&#20854;&#20313;&#31995;&#25968;&#21521;&#37327;&#22312;&#26080;&#31351;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#20855;&#26377;&#19968;&#20010;&#26377;&#38480;&#30340;&#20960;&#20046;&#32943;&#23450;&#30340;&#26497;&#38480;&#65292;&#36825;&#25193;&#23637;&#20102;&#20043;&#21069;&#23545;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;&#12290;&#26497;&#38480;&#20381;&#36182;&#20110;&#26435;&#37325;&#20989;&#25968;&#30340;&#24038;&#23614;&#22686;&#38271;&#36895;&#29575;&#65292;&#23545;&#27492;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#31181;&#24773;&#20917;&#65306;&#20122;&#25351;&#25968;&#21644;&#25351;&#25968;&#12290;&#26497;&#38480;&#31995;&#25968;&#21521;&#37327;&#21453;&#26144;&#20102;&#31283;&#20581;&#24615;&#25110;&#20445;&#23432;&#24615;&#23646;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20248;&#21270;&#20102;&#26576;&#20123;&#26368;&#22351;&#24773;&#20917;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#20122;&#25351;&#25968;&#24773;&#20917;&#19979;&#65292;&#26497;&#38480;&#31561;&#20215;&#20110;&#23569;&#25968;&#31867;&#30340;&#19978;&#37319;&#26679;&#20998;&#24067;&#30340;&#38544;&#24335;&#36873;&#25321;&#12290;&#25105;&#20204;&#22312;&#20449;&#29992;&#39118;&#38505;&#35774;&#32622;&#20013;&#24212;&#29992;&#20102;&#36825;&#20123;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the behavior of linear discriminant functions for binary classification in the infinite-imbalance limit, where the sample size of one class grows without bound while the sample size of the other remains fixed. The coefficients of the classifier minimize an empirical loss specified through a weight function. We show that for a broad class of weight functions, the intercept diverges but the rest of the coefficient vector has a finite almost sure limit under infinite imbalance, extending prior work on logistic regression. The limit depends on the left-tail growth rate of the weight function, for which we distinguish two cases: subexponential and exponential. The limiting coefficient vectors reflect robustness or conservatism properties in the sense that they optimize against certain worst-case alternatives. In the subexponential case, the limit is equivalent to an implicit choice of upsampling distribution for the minority class. We apply these ideas in a credit risk setting, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#32771;&#34385;&#25152;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#25152;&#20851;&#32852;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#38543;&#26426;&#21644;&#35748;&#35782;&#37096;&#20998;&#12290;&#30456;&#27604;&#20110;&#19981;&#20351;&#29992;&#35813;&#27169;&#22411;&#65292;&#20351;&#29992;&#38169;&#35823;&#21464;&#37327;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#23545;&#24050;&#30693;&#22238;&#24402;&#20989;&#25968;&#30340;&#35206;&#30422;&#29575;&#65292;&#19988;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.09095</link><description>&lt;p&gt;
&#28145;&#24230;&#22238;&#24402;&#20013;&#30340;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Aleatoric uncertainty for Errors-in-Variables models in deep regression. (arXiv:2105.09095v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.09095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#32771;&#34385;&#25152;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#25152;&#20851;&#32852;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#38543;&#26426;&#21644;&#35748;&#35782;&#37096;&#20998;&#12290;&#30456;&#27604;&#20110;&#19981;&#20351;&#29992;&#35813;&#27169;&#22411;&#65292;&#20351;&#29992;&#38169;&#35823;&#21464;&#37327;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#23545;&#24050;&#30693;&#22238;&#24402;&#20989;&#25968;&#30340;&#35206;&#30422;&#29575;&#65292;&#19988;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#22788;&#29702;&#21487;&#20197;&#35745;&#31639;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#22238;&#24402;&#20013;&#20351;&#29992;&#21464;&#37327;&#35823;&#24046;&#30340;&#27010;&#24565;&#65292;&#20197;&#32771;&#34385;&#25152;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#25152;&#20851;&#32852;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#30456;&#20851;&#20294;&#36890;&#24120;&#34987;&#24573;&#35270;&#30340;&#19981;&#30830;&#23450;&#24615;&#28304;&#65292;&#24182;&#23558;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#38543;&#26426;&#21644;&#35748;&#35782;&#37096;&#20998;&#65292;&#36825;&#22312;&#32479;&#35745;&#23398;&#35282;&#24230;&#26356;&#23436;&#25972;&#65292;&#32780;&#19988;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#26356;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#30340;&#20363;&#23376;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#20351;&#29992;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#20250;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#20351;&#29992;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#30340;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#23545;&#20110;&#24050;&#30693;&#22238;&#24402;&#20989;&#25968;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#22823;&#22823;&#25552;&#39640;&#20102;&#23545;&#22522;&#30784;&#20107;&#23454;&#30340;&#35206;&#30422;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Bayesian treatment of deep learning allows for the computation of uncertainties associated with the predictions of deep neural networks. We show how the concept of Errors-in-Variables can be used in Bayesian deep regression to also account for the uncertainty associated with the input of the employed neural network. The presented approach thereby exploits a relevant, but generally overlooked, source of uncertainty and yields a decomposition of the predictive uncertainty into an aleatoric and epistemic part that is more complete and, in many cases, more consistent from a statistical perspective. We discuss the approach along various simulated and real examples and observe that using an Errors-in-Variables model leads to an increase in the uncertainty while preserving the prediction performance of models without Errors-in-Variables. For examples with known regression function we observe that this ground truth is substantially better covered by the Errors-in-Variables model, indicating 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#19981;&#21516;&#20154;&#32676;&#30340;&#24433;&#21709;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2102.10019</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65306;&#24179;&#26435;&#34892;&#21160;&#19982;&#24179;&#26435;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#19981;&#21516;&#20154;&#32676;&#30340;&#24433;&#21709;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.
&lt;/p&gt;
&lt;p&gt;
&#20687;&#36151;&#27454;&#25209;&#20934;&#12289;&#21307;&#30103;&#24178;&#39044;&#21644;&#22823;&#23398;&#24405;&#21462;&#36825;&#26679;&#30340;&#20851;&#38190;&#20915;&#31574;&#26159;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#19981;&#24179;&#31561;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#65306;&#24179;&#22343;&#32467;&#26524;&#36739;&#39640;&#30340;&#32676;&#20307;&#36890;&#24120;&#34987;&#20998;&#37197;&#26356;&#39640;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#32780;&#24179;&#22343;&#32467;&#26524;&#36739;&#20302;&#30340;&#32676;&#20307;&#21017;&#34987;&#20998;&#37197;&#26356;&#39640;&#30340;&#20551;&#38452;&#24615;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39069;&#22806;&#30340;&#25968;&#25454;&#33719;&#21462;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#32447;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(LRNNs)&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#26102;&#21464;&#20989;&#25968;f(t)&#12290;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#36716;&#31227;&#30697;&#38453;&#30340;&#20027;&#35201;&#29305;&#24449;&#20540;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LRNN&#30340;&#35268;&#27169;&#12290;LRNNs&#20855;&#26377;&#20197;&#26925;&#22278;&#36712;&#36857;&#32467;&#26463;&#30340;&#26377;&#36259;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20540;&#21644;&#20989;&#25968;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/1802.03308</link><description>&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Power of Linear Recurrent Neural Networks. (arXiv:1802.03308v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1802.03308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#32447;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(LRNNs)&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#26102;&#21464;&#20989;&#25968;f(t)&#12290;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#36716;&#31227;&#30697;&#38453;&#30340;&#20027;&#35201;&#29305;&#24449;&#20540;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LRNN&#30340;&#35268;&#27169;&#12290;LRNNs&#20855;&#26377;&#20197;&#26925;&#22278;&#36712;&#36857;&#32467;&#26463;&#30340;&#26377;&#36259;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20540;&#21644;&#20989;&#25968;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26159;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;autoregressive linear,&#21363;&#32447;&#24615;&#28608;&#27963;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(LRNNs)&#21487;&#20197;&#36924;&#36817;&#30001;&#22810;&#20010;&#20989;&#25968;&#20540;&#32473;&#20986;&#30340;&#20219;&#20309;&#26102;&#21464;&#20989;&#25968;f(t)&#12290;&#36924;&#36817;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#35299;&#20915;&#19968;&#20010;&#32447;&#24615;&#26041;&#31243;&#32452;&#26469;&#26377;&#25928;&#23398;&#20064;&#65307;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#25110;&#31867;&#20284;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#21487;&#33021;&#26159;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#36716;&#31227;&#30697;&#38453;&#30340;&#39057;&#35889;&#65292;&#21363;&#23427;&#30340;&#29305;&#24449;&#20540;&#65292;&#21482;&#21462;&#26368;&#30456;&#20851;&#30340;&#32452;&#20214;&#65292;&#21487;&#20197;&#22312;&#19968;&#27493;&#20013;&#26174;&#33879;&#38477;&#20302;LRNN&#30340;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#32593;&#32476;&#26435;&#37325;&#65292;&#36824;&#21487;&#20197;&#23398;&#20064;&#32593;&#32476;&#26550;&#26500;&#12290;LRNNs&#20855;&#26377;&#26377;&#36259;&#30340;&#29305;&#24615;&#65306;&#23427;&#20204;&#26368;&#32456;&#20250;&#20197;&#26925;&#22278;&#36712;&#36857;&#32467;&#26463;&#65292;&#24182;&#20801;&#35768;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20540;&#21644;&#20989;&#25968;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#28436;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks are a powerful means to cope with time series. We show how autoregressive linear, i.e., linearly activated recurrent neural networks (LRNNs) can approximate any time-dependent function f(t) given by a number of function values. The approximation can effectively be learned by simply solving a linear equation system; no backpropagation or similar methods are needed. Furthermore, and this is probably the main contribution of this article, the size of an LRNN can be reduced significantly in one step after inspecting the spectrum of the network transition matrix, i.e., its eigenvalues, by taking only the most relevant components. Therefore, in contrast to other approaches, we do not only learn network weights but also the network architecture. LRNNs have interesting properties: They end up in ellipse trajectories in the long run and allow the prediction of further values and compact representations of functions. We demonstrate this by several experiments, among the
&lt;/p&gt;</description></item></channel></rss>