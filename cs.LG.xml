<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#31034;&#36394;&#21058;&#36716;&#21270;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;TC-INN&#65289;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;FDG&#22270;&#20687;&#26144;&#23556;&#21040;DOPA&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;DOPA&#22312;PET&#25104;&#20687;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00735</link><description>&lt;p&gt;
PET&#36890;&#36807;&#21487;&#21464;&#22686;&#24378;&#21487;&#36870;&#32593;&#32476;&#22312;&#33041;PET&#20043;&#38388;&#36827;&#34892;&#31034;&#36394;&#21058;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network. (arXiv:2311.00735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#31034;&#36394;&#21058;&#36716;&#21270;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;TC-INN&#65289;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;FDG&#22270;&#20687;&#26144;&#23556;&#21040;DOPA&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;DOPA&#22312;PET&#25104;&#20687;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25104;&#20687;&#65288;PET&#65289;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#39640;&#29983;&#21270;&#25935;&#24863;&#24615;&#30340;&#25104;&#20687;&#25216;&#26415;&#65292;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33041;&#30142;&#30149;&#35786;&#26029;&#21644;&#33041;&#31185;&#23398;&#30740;&#31350;&#12290;&#30001;&#20110;&#19981;&#21516;&#31034;&#36394;&#21058;&#22312;&#21516;&#19968;&#21306;&#22495;&#21576;&#29616;&#20986;&#19981;&#21516;&#25928;&#26524;&#65292;&#31034;&#36394;&#21058;&#30340;&#36873;&#25321;&#23545;PET&#25104;&#20687;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#22312;&#31070;&#32463;&#31934;&#31070;&#27835;&#30103;&#20013;&#24191;&#27867;&#24212;&#29992;PET&#25104;&#20687;&#65292;&#21457;&#29616;6-18F-&#27679;-3,4-&#20108;&#32671;&#22522;-L-&#33519;&#19993;&#27688;&#37240;&#65288;DOPA&#65289;&#22312;&#36825;&#19968;&#39046;&#22495;&#27604;18F&#26631;&#35760;&#30340;&#27679;&#20195;&#33073;&#27687;&#33889;&#33796;&#31958;&#65288;FDG&#65289;&#26356;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21046;&#22791;&#22797;&#26434;&#24615;&#20197;&#21450;&#20854;&#20182;&#38480;&#21046;&#65292;DOPA&#36828;&#19981;&#22914;FDG&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31034;&#36394;&#21058;&#36716;&#21270;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;TC-INN&#65289;&#29992;&#20110;&#22270;&#20687;&#25237;&#24433;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;FDG&#22270;&#20687;&#26144;&#23556;&#21040;DOPA&#22270;&#20687;&#12290;&#36890;&#36807;&#20174;FDG&#21040;DOPA&#29983;&#25104;PET&#22270;&#20687;&#65292;&#33719;&#24471;&#26356;&#22810;&#30340;&#35786;&#26029;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positron emission tomography (PET), as an imaging technique with high biochemical sensitivity, has been widely used in diagnosis of encephalopathy and brain science research used in brain disease diagnosis and brain science research. Since different tracers present different effects on the same focal area, the choice of tracers is getting more significant for PET imaging. Nowadays, with the wide application of PET imaging in neuropsychiatric treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine (DOPA) has been found to be more effective than 18F-labeled fluorine-2-deoxyglucose (FDG) in this field. However, due to the complexity of its preparation and other limitations, DOPA is far less widely used than FDG. To address this issue, a tracer conversion invertible neural network (TC-INN) for image projection is developed to map FDG images to DOPA images through deep learning. More diagnostic information is obtained by generating PET images from FDG to DOPA. Specifically, the proposed TC-I
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#25913;&#21892;T5&#22312;&#38271;&#24207;&#21015;&#22788;&#29702;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00684</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23545;&#40784;&#21644;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#25552;&#39640;&#20102;Transformer&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#25913;&#21892;T5&#22312;&#38271;&#24207;&#21015;&#22788;&#29702;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24819;&#30340;&#38271;&#24230;&#21487;&#22806;&#25512;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#27604;&#35757;&#32451;&#38271;&#24230;&#26356;&#38271;&#30340;&#24207;&#21015;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#38271;&#24207;&#21015;&#24494;&#35843;&#12290;&#36825;&#31181;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#39640;&#24230;&#20381;&#36182;&#20110;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#12290;&#22312;&#35843;&#26597;&#29616;&#26377;&#22823;&#22411;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;T5&#31995;&#21015;&#20540;&#24471;&#26356;&#20180;&#32454;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#30340;&#20301;&#32622;&#23884;&#20837;&#25429;&#25417;&#21040;&#20102;&#20016;&#23500;&#32780;&#28789;&#27963;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;T5&#23384;&#22312;&#30528;&#20998;&#25955;&#30340;&#27880;&#24847;&#21147;&#38382;&#39064;&#65306;&#36755;&#20837;&#24207;&#21015;&#36234;&#38271;&#65292;&#27880;&#24847;&#21147;&#20998;&#24067;&#23601;&#36234;&#24179;&#22374;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#39640;&#20102;T5&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#65292;&#36825;&#34920;&#26126;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#21644;&#27880;&#24847;&#21147;&#23545;&#40784;&#23545;&#20110;Transformer&#38271;&#24230;&#22806;&#25512;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\footnote{\url
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.18306</link><description>&lt;p&gt;
&#30417;&#30563;&#21644;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Supervised and Penalized Baseline Correction. (arXiv:2310.18306v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#27979;&#37327;&#21487;&#20197;&#26174;&#31034;&#30001;&#21560;&#25910;&#21644;&#25955;&#23556;&#25104;&#20998;&#28151;&#21512;&#24341;&#36215;&#30340;&#25197;&#26354;&#20809;&#35889;&#24418;&#29366;&#12290;&#36825;&#20123;&#25197;&#26354;&#65288;&#25110;&#22522;&#32447;&#65289;&#36890;&#24120;&#34920;&#29616;&#20026;&#38750;&#24658;&#23450;&#20559;&#31227;&#25110;&#20302;&#39057;&#25391;&#33633;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22522;&#32447;&#21487;&#33021;&#23545;&#20998;&#26512;&#21644;&#23450;&#37327;&#32467;&#26524;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22522;&#32447;&#26657;&#27491;&#26159;&#19968;&#20010;&#28085;&#30422;&#20102;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#24635;&#31216;&#65292;&#36890;&#36807;&#33719;&#21462;&#22522;&#32447;&#20809;&#35889;&#65288;&#19981;&#38656;&#35201;&#30340;&#25197;&#26354;&#65289;&#24182;&#36890;&#36807;&#24046;&#24322;&#21270;&#21435;&#38500;&#25197;&#26354;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#21363;&#20351;&#21487;&#29992;&#20998;&#26512;&#29289;&#27987;&#24230;&#25110;&#32773;&#23427;&#20204;&#23545;&#35266;&#23519;&#21040;&#30340;&#20809;&#35889;&#21464;&#24322;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#20063;&#27809;&#26377;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#65289;&#24182;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#23558;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#24615;&#33021;&#65292;&#21253;&#25324;&#32463;&#20856;&#21463;&#32602;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectroscopic measurements can show distorted spectra shapes arising from a mixture of absorbing and scattering contributions. These distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. As a result, these baselines can adversely affect analytical and quantitative results. Baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. However, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. We examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentration such that prediction can be enhanced. Performance will be access on two near infra-red data sets across both classical penal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.17658</link><description>&lt;p&gt;
&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#36890;&#36947;&#30456;&#20851;(CD)&#25110;&#36890;&#36947;&#29420;&#31435;(CI)&#24314;&#27169;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;CD&#21644;CI&#35270;&#20026;&#20004;&#31181;&#20114;&#34917;&#20294;&#20114;&#26021;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#36825;&#20004;&#20010;&#26497;&#31471;&#12290;&#32780;&#19988;&#65292;CD&#21644;CI&#37117;&#26159;&#38745;&#24577;&#31574;&#30053;&#65292;&#26080;&#27861;&#22312;&#27809;&#26377;&#22823;&#37327;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26159;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;CI&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;CSC&#65288;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#65289;&#65292;&#29992;&#20110;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#22686;&#24378;&#20102;CI&#31574;&#30053;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20943;&#23567;&#20102;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#39044;&#31639;&#19979;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#23454;&#25968;&#32452;&#21512;&#32431;&#25506;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CSA&#31639;&#27861;&#65292;&#21487;&#22312;&#33218;&#25968;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20339;&#21160;&#20316;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Minimax-CombSAR&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#21160;&#20316;&#31867;&#22823;&#23567;&#20026;&#22810;&#39033;&#24335;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30340;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#23454;&#39564;&#20013;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.15681</link><description>&lt;p&gt;
&#22266;&#23450;&#39044;&#31639;&#19979;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#23454;&#25968;&#32452;&#21512;&#32431;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit. (arXiv:2310.15681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#39044;&#31639;&#19979;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#23454;&#25968;&#32452;&#21512;&#32431;&#25506;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CSA&#31639;&#27861;&#65292;&#21487;&#22312;&#33218;&#25968;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20339;&#21160;&#20316;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Minimax-CombSAR&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#21160;&#20316;&#31867;&#22823;&#23567;&#20026;&#22810;&#39033;&#24335;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30340;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#23454;&#39564;&#20013;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22266;&#23450;&#39044;&#31639;&#35774;&#32622;&#19979;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#23454;&#25968;&#32452;&#21512;&#32431;&#25506;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#32452;&#21512;&#25104;&#21151;&#20998;&#37197;&#65288;CSA&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#21160;&#20316;&#31867;&#22823;&#23567;&#30456;&#23545;&#20110;&#33218;&#25968;&#21576;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26368;&#20339;&#21160;&#20316;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CSA&#31639;&#27861;&#30340;&#35823;&#24046;&#27010;&#29575;&#19978;&#30028;&#19982;&#19979;&#30028;&#22312;&#25351;&#25968;&#30340;&#23545;&#25968;&#22240;&#23376;&#20869;&#30456;&#21305;&#37197;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21478;&#19968;&#20010;&#31639;&#27861;&#65292;&#21517;&#20026;&#26497;&#23567;&#21270;&#32452;&#21512;&#36830;&#32493;&#25509;&#21463;&#19982;&#25298;&#32477;&#65288;Minimax-CombSAR&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21160;&#20316;&#31867;&#22823;&#23567;&#20026;&#22810;&#39033;&#24335;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#26368;&#20248;&#30340;&#65292;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#36825;&#20123;&#31639;&#27861;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the real-valued combinatorial pure exploration of the multi-armed bandit in the fixed-budget setting. We first introduce the Combinatorial Successive Asign (CSA) algorithm, which is the first algorithm that can identify the best action even when the size of the action class is exponentially large with respect to the number of arms. We show that the upper bound of the probability of error of the CSA algorithm matches a lower bound up to a logarithmic factor in the exponent. Then, we introduce another algorithm named the Minimax Combinatorial Successive Accepts and Rejects (Minimax-CombSAR) algorithm for the case where the size of the action class is polynomial, and show that it is optimal, which matches a lower bound. Finally, we experimentally compare the algorithms with previous methods and show that our algorithm performs better.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15612</link><description>&lt;p&gt;
Nko&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#65306;&#24037;&#20855;&#12289;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23612;&#31185;&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#27809;&#26377;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20294;&#23427;&#22312;&#25991;&#21270;&#21644;&#25945;&#32946;&#20215;&#20540;&#19978;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#26088;&#22312;&#24320;&#21457;&#21487;&#29992;&#30340;&#23612;&#31185;&#35821;&#21644;&#20854;&#20182;&#24403;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#24179;&#34892;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20855;&#20307;&#21253;&#25324;&#65306;(1) Friallel&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#65292;&#36890;&#36807;&#22522;&#20110;&#21103;&#26412;&#32534;&#36753;&#30340;&#24037;&#20316;&#27969;&#31243;&#23454;&#29616;&#36136;&#37327;&#25511;&#21046;&#12290;(2) &#25193;&#23637;&#20102;FLoRes-200&#21644;NLLB-Seed&#35821;&#26009;&#24211;&#65292;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#19982;&#23612;&#31185;&#35821;&#24179;&#34892;&#32763;&#35793;&#20102;2,009&#21644;6,193&#20010;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;(3) nicolingua-0005&#65306;&#21253;&#21547;130,850&#20010;&#24179;&#34892;&#29255;&#27573;&#30340;&#19977;&#35821;&#21644;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#36229;&#36807;3&#30334;&#19975;&#23612;&#31185;&#35821;&#21333;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;(4) &#22522;&#32447;&#21452;&#35821;&#21644;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#19982;b...
&lt;/p&gt;
&lt;p&gt;
Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.14421</link><description>&lt;p&gt;
&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#65292;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#38024;&#23545;&#65288;&#23616;&#37096;&#65289;&#21807;&#19968;&#21487;&#36870;&#20998;&#31867;&#22120;&#12289;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#21644;&#29109;AI&#65288;EAI&#65289;&#20855;&#26377;&#26368;&#23567;&#23545;&#25239;&#36335;&#24452;&#65288;MAP&#65289;&#21644;&#26368;&#23567;&#23545;&#25239;&#36317;&#31163;&#65288;MAD&#65289;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#26126;&#30830;&#30340;&#20998;&#26512;&#35745;&#31639;&#30340;&#31616;&#21333;&#21487;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#12290;&#22312;&#24120;&#35265;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#12289;&#25552;&#21319;&#38543;&#26426;&#26862;&#26519;&#12289;GLM&#21644;EAI&#31561;&#21508;&#31867;AI&#24037;&#20855;&#36827;&#34892;MAP&#21644;MAD&#30340;&#23454;&#38469;&#35745;&#31639;&#12289;&#27604;&#36739;&#21644;&#35299;&#37322;&#65292;&#21253;&#25324;&#21452;&#21367;&#29366;&#34746;&#26059;&#32447;&#21450;&#20854;&#25193;&#23637;&#20197;&#21450;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38382;&#39064;&#65288;&#29992;&#20110;&#20581;&#24247;&#20445;&#38505;&#29702;&#36180;&#39044;&#27979;&#21644;&#24515;&#33039;&#30149;&#21457;&#20316;&#33268;&#27515;&#29575;&#20998;&#31867;&#65289;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#23637;&#31034;&#20102;MAP&#22914;&#20309;&#22312;&#39044;&#23450;&#20041;&#30340;&#21487;&#35775;&#38382;&#25511;&#21046;&#21464;&#37327;&#23376;&#38598;&#20013;&#25552;&#20379;&#21807;&#19968;&#30340;&#26368;&#23567;&#24739;&#32773;&#29305;&#23450;&#39118;&#38505;&#32531;&#35299;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;OGD&#31639;&#27861;\textsf{AdaOGD}&#65292;&#22312;&#24378;&#20984;&#24615;&#19979;&#23454;&#29616;&#20102;$ O(\log^2(T)) $&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#20351;&#24471;&#32852;&#21512;&#34892;&#21160;&#26368;&#21518;&#19968;&#27425;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.14085</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#26799;&#24230;&#21453;&#39304;&#30340;&#24378;&#21333;&#35843;&#21644;&#25351;&#25968;&#20984;&#21338;&#24328;&#20013;&#30340;&#33258;&#36866;&#24212;&#12289;&#21452;&#37325;&#26368;&#20248;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback. (arXiv:2310.14085v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;OGD&#31639;&#27861;\textsf{AdaOGD}&#65292;&#22312;&#24378;&#20984;&#24615;&#19979;&#23454;&#29616;&#20102;$ O(\log^2(T)) $&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#20351;&#24471;&#32852;&#21512;&#34892;&#21160;&#26368;&#21518;&#19968;&#27425;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#20984;&#24615;&#25110;&#21333;&#35843;&#24615;&#20551;&#35774;&#19979;&#65292;&#32593;&#19978;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#21452;&#37325;&#26368;&#20248;&#30340;&#65306;&#65288;1&#65289;&#22312;&#21333;&#20010;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#24378;&#20984;&#25104;&#26412;&#20989;&#25968;&#65292;&#23427;&#23454;&#29616;&#20102;$ \Theta(\log T) $&#30340;&#26368;&#20248;&#21518;&#24724;&#65307;&#65288;2&#65289;&#22312;&#20855;&#26377;&#24378;&#21333;&#35843;&#24615;&#30340;&#22810;&#20195;&#29702;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#20195;&#29702;&#20351;&#29992;OGD&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20851;&#20110;&#32852;&#21512;&#34892;&#21160;&#30340;&#26368;&#21518;&#19968;&#27425;&#25910;&#25947;&#21040;&#21807;&#19968;&#32435;&#20160;&#22343;&#34913;&#30340;&#26368;&#20248;&#36895;&#29575;$ \Theta(\frac{1}{T}) $&#12290;&#23613;&#31649;&#36825;&#20123;&#26377;&#38480;&#26102;&#38388;&#30340;&#20445;&#35777;&#31361;&#20986;&#20102;&#20854;&#20248;&#28857;&#65292;&#20294;OGD&#30340;&#32570;&#28857;&#26159;&#38656;&#35201;&#30693;&#36947;&#24378;&#20984;&#24615;/&#21333;&#35843;&#24615;&#30340;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;OGD&#31639;&#27861;\textsf{AdaOGD}&#65292;&#23427;&#19981;&#38656;&#35201;&#20808;&#39564;&#30340;&#30693;&#35782;&#36825;&#20123;&#21442;&#25968;&#12290;&#22312;&#21333;&#20010;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24378;&#20984;&#24615;&#19979;&#23454;&#29616;&#20102;$ O(\log^2(T)) $&#30340;&#21518;&#24724;&#65292;&#36825;&#26159;&#26368;&#20248;&#30340;&#38500;&#20102;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#27599;&#20010;&#20195;&#29702;&#37117;&#20351;&#29992;\textsf{AdaOGD}&#65292;&#21017;&#32852;&#21512;&#34892;&#21160;&#25910;&#25947;&#21040;&#26368;&#21518;&#19968;&#20010;&#36845;&#20195;&#26102;&#30340;&#19968;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\Theta(\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate s
&lt;/p&gt;</description></item><item><title>&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#30740;&#31350;&#20102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#24433;&#21709;&#65292;&#19988;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.09336</link><description>&lt;p&gt;
&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. (arXiv:2310.09336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09336
&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#30740;&#31350;&#20102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#24433;&#21709;&#65292;&#19988;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#20135;&#29983;&#26497;&#20026;&#36924;&#30495;&#25968;&#25454;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#33258;&#28982;&#32452;&#21512;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#20351;&#29992;&#38656;&#35201;&#23637;&#31034;&#20986;&#33021;&#22815;&#32452;&#21512;&#26032;&#30340;&#27010;&#24565;&#38598;&#21512;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26410;&#35265;&#30340;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#30830;&#23454;&#34920;&#29616;&#20986;&#20102;&#26377;&#36259;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20063;&#20250;&#20986;&#29616;&#26080;&#27861;&#39044;&#27979;&#30340;&#22833;&#36133;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26377;&#25511;&#21046;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#21464;&#21270;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#21516;&#23646;&#24615;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#29983;&#25104;&#36234;&#30028;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;i&#65289;&#20174;&#19968;&#20010;&#27010;&#24565;&#29983;&#25104;&#26679;&#26412;&#30340;&#33021;&#21147;&#21644;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#30340;&#33021;&#21147;&#30340;&#20986;&#29616;&#39034;&#24207;&#21463;&#21040;&#20102;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#30340;&#24433;&#21709;&#65307;&#65288;ii&#65289;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#34920;&#26126;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#24050;&#26377;&#30340;&#27169;&#24577;&#29983;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#22312;BraTS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07250</link><description>&lt;p&gt;
&#22312;BraTS&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#24050;&#26377;&#30340;&#27169;&#24577;&#29983;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset. (arXiv:2310.07250v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#24050;&#26377;&#30340;&#27169;&#24577;&#29983;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#22312;BraTS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24230;&#20405;&#34989;&#24615;&#21644;&#33268;&#21629;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#33041;&#30284;&#12290;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#30001;&#20110;&#20854;&#26080;&#21019;&#21644;&#26080;&#36752;&#23556;&#24615;&#36136;&#65292;&#22312;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#24739;&#32773;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#38543;&#35775;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22269;&#38469;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#20026;&#21033;&#29992;&#22235;&#31181;&#32467;&#26500;&#24615;MRI&#25195;&#25551;&#65288;T1&#12289;T1Gd&#12289;T2&#12289;T2-FLAIR&#65289;&#20934;&#30830;&#39640;&#25928;&#22320;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#20122;&#21306;&#22495;&#25552;&#20379;&#20102;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#22235;&#20010;MRI&#24207;&#21015;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21512;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#21033;&#29992;&#24320;&#28304;&#30340;GAN&#26041;&#27861;&#65292;&#20197;&#20219;&#19977;&#20010;MRI&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#29983;&#25104;&#32570;&#22833;&#30340;&#31532;&#22235;&#20010;&#32467;&#26500;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36129;&#29486;&#32473;&#20102;&#31038;&#21306;&#39537;&#21160;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;GaNDLF&#65289;&#65292;&#24182;&#22312;&#21512;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing 
&lt;/p&gt;</description></item><item><title>AdaptNet&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.00239</link><description>&lt;p&gt;
AdaptNet: &#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
AdaptNet: Policy Adaptation for Physics-Based Character Control. (arXiv:2310.00239v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00239
&lt;/p&gt;
&lt;p&gt;
AdaptNet&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#22312;&#23398;&#20064;&#26032;&#25216;&#33021;&#26102;&#33021;&#22815;&#36866;&#24212;&#29616;&#26377;&#25216;&#33021;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaptNet&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#30456;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#12290;AdaptNet&#22312;&#32473;&#23450;&#30340;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#22686;&#21152;&#21407;&#22987;&#29366;&#24577;&#23884;&#20837;&#26469;&#25903;&#25345;&#34892;&#20026;&#30340;&#36866;&#24230;&#21464;&#21270;&#65292;&#24182;&#36827;&#19968;&#27493;&#20462;&#25913;&#31574;&#30053;&#32593;&#32476;&#23618;&#26469;&#23454;&#29616;&#26356;&#28145;&#36828;&#30340;&#21464;&#21270;&#12290;&#35813;&#25216;&#26415;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#29616;&#26377;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#25511;&#21046;&#22120;&#20197;&#36866;&#24212;&#24191;&#27867;&#30340;&#26032;&#30340;&#36816;&#21160;&#39118;&#26684;&#12289;&#26032;&#30340;&#20219;&#21153;&#30446;&#26631;&#12289;&#35282;&#33394;&#24418;&#24577;&#30340;&#21464;&#21270;&#20197;&#21450;&#29615;&#22659;&#30340;&#24191;&#27867;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#26174;&#31034;&#20986;&#26174;&#33879;&#25552;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#34920;&#29616;&#20026;&#22823;&#22823;&#32553;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20195;&#30721;&#21487;&#22312;https://motion-&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by humans' ability to adapt skills in the learning of new ones, this paper presents AdaptNet, an approach for modifying the latent space of existing policies to allow new behaviors to be quickly learned from like tasks in comparison to learning from scratch. Building on top of a given reinforcement learning controller, AdaptNet uses a two-tier hierarchy that augments the original state embedding to support modest changes in a behavior and further modifies the policy network layers to make more substantive changes. The technique is shown to be effective for adapting existing physics-based controllers to a wide range of new styles for locomotion, new task targets, changes in character morphology and extensive changes in environment. Furthermore, it exhibits significant increase in learning efficiency, as indicated by greatly reduced training times when compared to training from scratch or using other approaches that modify existing policies. Code is available at https://motion-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.17370</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Neural Weather Prediction for Limited Area Modeling. (arXiv:2309.17370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31934;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24212;&#29992;&#20026;&#27169;&#25311;&#22823;&#27668;&#30340;&#21487;&#33021;&#24615;&#24102;&#26469;&#20102;&#26032;&#30340;&#21464;&#38761;&#12290;&#22312;&#27668;&#20505;&#21464;&#21270;&#26102;&#20195;&#65292;&#33719;&#21462;&#20687;&#36825;&#26679;&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;&#27169;&#22411;&#30340;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#20840;&#29699;&#39044;&#27979;&#65292;&#20294;&#22914;&#20309;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#26377;&#38480;&#21306;&#22495;&#24314;&#27169;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340;&#31070;&#32463;&#32593;&#32476;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#24212;&#29992;&#20110;&#26377;&#38480;&#21306;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20998;&#23618;&#27169;&#22411;&#25193;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of accurate machine learning methods for weather forecasting is creating radical new possibilities for modeling the atmosphere. In the time of climate change, having access to high-resolution forecasts from models like these is also becoming increasingly vital. While most existing Neural Weather Prediction (NeurWP) methods focus on global forecasting, an important question is how these techniques can be applied to limited area modeling. In this work we adapt the graph-based NeurWP approach to the limited area setting and propose a multi-scale hierarchical model extension. Our approach is validated by experiments with a local model for the Nordic region.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#24448;&#24448;&#21482;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20844;&#27491;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#20449;&#21644;&#19981;&#36866;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.12632</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#26159;&#21542;&#20844;&#27491;&#21487;&#35299;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?. (arXiv:2309.12632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12632
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#24448;&#24448;&#21482;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20844;&#27491;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#20449;&#21644;&#19981;&#36866;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#29289;&#20307;&#20998;&#31867;&#20013;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20063;&#38754;&#20020;&#30528;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#21160;&#35786;&#26029;&#26696;&#20363;&#30340;&#21387;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#23581;&#35797;&#20165;&#20165;&#20851;&#27880;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#21487;&#35299;&#37322;&#24615;&#25110;&#32773;&#24739;&#32773;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#31163;&#12290;&#20363;&#22914;&#65292;&#22823;&#37096;&#20998;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#32954;&#32467;&#33410;&#20998;&#31867;&#35770;&#25991;&#20250;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#27927;&#29260;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#65292;&#23548;&#33268;&#19968;&#20010;&#20154;&#30340;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#26576;&#20123;&#22270;&#20687;&#20301;&#20110;&#35757;&#32451;&#38598;&#20013;&#65292;&#32780;&#20854;&#20182;&#22270;&#20687;&#21017;&#20301;&#20110;&#39564;&#35777;&#25110;&#27979;&#35797;&#22270;&#20687;&#38598;&#20013;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#20934;&#30830;&#29575;&#25253;&#21578;&#21644;&#23398;&#20064;&#21040;&#30340;&#26080;&#20851;&#29305;&#24449;&#65292;&#26368;&#32456;&#38477;&#20302;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the great success of various deep learning methods in image and object classification, the biomedical image processing society is also overwhelmed with their applications to various automatic diagnosis cases. Unfortunately, most of the deep learning-based classification attempts in the literature solely focus on the aim of extreme accuracy scores, without considering interpretability, or patient-wise separation of training and test data. For example, most lung nodule classification papers using deep learning randomly shuffle data and split it into training, validation, and test sets, causing certain images from the CT scan of a person to be in the training set, while other images of the exact same person to be in the validation or testing image sets. This can result in reporting misleading accuracy rates and the learning of irrelevant features, ultimately reducing the real-life usability of these models. When the deep neural networks trained on the traditional, unfair data sh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#32676;&#20307;&#26631;&#27880;&#21644;&#21482;&#26377;&#23569;&#37327;&#31867;&#21035;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20248;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#23454;&#29616;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08534</link><description>&lt;p&gt;
&#26397;&#30528;&#20351;&#29992;&#26356;&#23569;&#26631;&#27880;&#23454;&#29616;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Last-layer Retraining for Group Robustness with Fewer Annotations. (arXiv:2309.08534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#32676;&#20307;&#26631;&#27880;&#21644;&#21482;&#26377;&#23569;&#37327;&#31867;&#21035;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20248;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#23454;&#29616;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#23481;&#26131;&#36807;&#24230;&#20381;&#36182;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#23569;&#25968;&#32676;&#20307;&#19978;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#29305;&#24449;&#20877;&#36171;&#26435;(DFR)&#25216;&#26415;&#36890;&#36807;&#31616;&#21333;&#30340;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32676;&#20307;&#20445;&#25252;&#24615;&#33021;&#65292;&#20294;&#23427;&#38656;&#35201;&#20445;&#30041;&#32676;&#20307;&#21644;&#31867;&#21035;&#30340;&#26631;&#27880;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#32676;&#20307;&#24179;&#34913;&#30340;&#20877;&#36171;&#26435;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#35201;&#27714;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#27809;&#26377;&#32676;&#20307;&#26631;&#27880;&#65288;&#38500;&#20102;&#27169;&#22411;&#36873;&#25321;&#65289;&#65292;&#21482;&#26377;&#23569;&#37327;&#30340;&#31867;&#21035;&#26631;&#27880;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#20986;&#20154;&#24847;&#26009;&#22320;&#26377;&#25928;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21363;&#20351;&#20877;&#36171;&#26435;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#19968;&#23567;&#37096;&#20998;&#26368;&#24046;&#32676;&#20307;&#25968;&#25454;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#36890;&#36807;&#20445;&#30041;&#19968;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#26469;&#37325;&#26032;&#35757;&#32451;&#26368;&#21518;&#19968;&#23618;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#39069;&#22806;&#25968;&#25454;&#25110;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;ERM&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Empirical risk minimization (ERM) of neural networks is prone to over-reliance on spurious correlations and poor generalization on minority groups. The recent deep feature reweighting (DFR) technique achieves state-of-the-art group robustness via simple last-layer retraining, but it requires held-out group and class annotations to construct a group-balanced reweighting dataset. In this work, we examine this impractical requirement and find that last-layer retraining can be surprisingly effective with no group annotations (other than for model selection) and only a handful of class annotations. We first show that last-layer retraining can greatly improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data. This implies a "free lunch" where holding out a subset of training data to retrain the last layer can substantially outperform ERM on the entire dataset with no additional data or annotations. To further improve group robustness, we i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#27748;&#26222;&#26862;&#25277;&#26679;&#25506;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#21160;&#20316;&#38598;&#21512;&#22823;&#23567;&#20026;&#25351;&#25968;&#32423;&#21035;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.10238</link><description>&lt;p&gt;
Thompson Sampling&#29992;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit. (arXiv:2308.10238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10238
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#27748;&#26222;&#26862;&#25277;&#26679;&#25506;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#21160;&#20316;&#38598;&#21512;&#22823;&#23567;&#20026;&#25351;&#25968;&#32423;&#21035;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;&#65288;R-CPE-MAB&#65289;&#38382;&#39064;&#12290;&#22312;R-CPE-MAB&#20013;&#65292;&#29609;&#23478;&#20174;&#32473;&#23450;&#30340;d&#20010;&#38543;&#26426;&#33218;&#20013;&#36873;&#25321;&#19968;&#20010;&#65292;&#27599;&#20010;&#33218;s&#30340;&#22870;&#21169;&#36981;&#24490;&#26410;&#30693;&#20998;&#24067;&#65292;&#20854;&#24179;&#22343;&#20540;&#20026;&#956;s&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#65292;&#29609;&#23478;&#25289;&#21160;&#19968;&#20010;&#33218;&#24182;&#35266;&#23519;&#20854;&#22870;&#21169;&#12290;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#33218;&#25289;&#21160;&#27425;&#25968;&#26469;&#30830;&#23450;&#26368;&#20248;&#21160;&#20316;&#960;* = argmax&#960;&#8712;A &#956;T&#960;&#65292;&#20854;&#20013;A&#26159;&#26377;&#38480;&#22823;&#23567;&#30340;&#23454;&#20540;&#21160;&#20316;&#38598;&#21512;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20551;&#35774;&#21160;&#20316;&#38598;&#21512;A&#30340;&#22823;&#23567;&#22312;d&#30340;&#22810;&#39033;&#24335;&#32423;&#21035;&#19978;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#27748;&#26222;&#26862;&#25277;&#26679;&#25506;&#32034;&#65288;GenTS-Explore&#65289;&#31639;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#35299;&#20915;&#21160;&#20316;&#38598;&#21512;&#22823;&#23567;&#22312;d&#30340;&#25351;&#25968;&#32423;&#21035;&#19978;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#30456;&#20851;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem. In R-CPE-MAB, a player is given $d$ stochastic arms, and the reward of each arm $s\in\{1, \ldots, d\}$ follows an unknown distribution with mean $\mu_s$. In each time step, a player pulls a single arm and observes its reward. The player's goal is to identify the optimal \emph{action} $\boldsymbol{\pi}^{*} = \argmax_{\boldsymbol{\pi} \in \mathcal{A}} \boldsymbol{\mu}^{\top}\boldsymbol{\pi}$ from a finite-sized real-valued \emph{action set} $\mathcal{A}\subset \mathbb{R}^{d}$ with as few arm pulls as possible. Previous methods in the R-CPE-MAB assume that the size of the action set $\mathcal{A}$ is polynomial in $d$. We introduce an algorithm named the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large in $d$. We also introduce a novel problem-dependent sample complexity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19968;&#26086;&#32771;&#34385;&#21040;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.00755</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19968;&#26086;&#32771;&#34385;&#21040;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#25918;&#22823;&#26159;&#19968;&#31181;&#27169;&#22411;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#24179;&#34913;&#30340;&#29616;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#26469;&#27604;&#36739;&#35757;&#32451;&#25968;&#25454;&#19982;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#24615;&#21035;&#27604;&#20363;&#65292;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#20284;&#20046;&#25918;&#22823;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;-&#32844;&#19994;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#25918;&#22823;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#39064;&#36890;&#24120;&#21253;&#21547;&#26126;&#30830;&#30340;&#24615;&#21035;&#20449;&#24687;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#30340;&#25552;&#31034;&#21017;&#19981;&#21253;&#21547;&#65292;&#36825;&#23548;&#33268;&#20102;&#20998;&#24067;&#30340;&#20559;&#31227;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20559;&#35265;&#24230;&#37327;&#12290;&#19968;&#26086;&#25105;&#20204;&#32771;&#34385;&#21040;&#35757;&#32451;&#21644;&#29983;&#25104;&#26102;&#20351;&#29992;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25918;&#22823;&#29616;&#35937;&#22823;&#22823;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35828;&#26126;&#20102;&#27604;&#36739;&#27169;&#22411;&#21644;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#24378;&#35843;&#20102;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias amplification is a phenomenon in which models increase imbalances present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION). However, we discover that amplification can largely be attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while the prompts we use do not, which leads to a distribution shift and consequently impacts bias measures. Once we account for various distributional differences between texts used for training and generation, we observe that amplification decreases considerably. Our findings illustrate the challenges of comparing biases in models and the data they are trained on, and highlight confounding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#21028;&#24615;&#23457;&#35270;&#20102;(Normalised) Discounted Cumulative Gain&#20316;&#20026;Top-n&#25512;&#33616;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20309;&#26102;&#21487;&#20197;&#26399;&#26395;&#36825;&#20123;&#25351;&#26631;&#36924;&#36817;&#22312;&#32447;&#23454;&#39564;&#30340;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15053</link><description>&lt;p&gt;
&#20851;&#20110;(Normalised) Discounted Cumulative Gain&#20316;&#20026;Top-n&#25512;&#33616;&#30340;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation. (arXiv:2307.15053v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#21028;&#24615;&#23457;&#35270;&#20102;(Normalised) Discounted Cumulative Gain&#20316;&#20026;Top-n&#25512;&#33616;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20309;&#26102;&#21487;&#20197;&#26399;&#26395;&#36825;&#20123;&#25351;&#26631;&#36924;&#36817;&#22312;&#32447;&#23454;&#39564;&#30340;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65306;(1) &#36890;&#36807;(&#27169;&#25311;)&#22312;&#32447;&#23454;&#39564;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#37329;&#26631;&#20934;&#65292;&#25110;&#32773;(2) &#36890;&#36807;&#19968;&#20123;&#31163;&#32447;&#35780;&#20272;&#31243;&#24207;&#65292;&#30446;&#26631;&#26159;&#36817;&#20284;&#22312;&#32447;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;&#25991;&#29486;&#20013;&#37319;&#29992;&#20102;&#20960;&#31181;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#65292;&#21463;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#20013;&#24120;&#35265;&#30340;&#25490;&#21517;&#25351;&#26631;&#30340;&#21551;&#21457;&#12290;(Normalised) Discounted Cumulative Gain (nDCG)&#26159;&#20854;&#20013;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#24456;&#22810;&#24180;&#37324;&#65292;&#26356;&#39640;&#30340;(n)DCG&#20540;&#34987;&#29992;&#26469;&#23637;&#31034;&#26032;&#26041;&#27861;&#22312;Top-n&#25512;&#33616;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#23457;&#35270;&#65292;&#24182;&#30740;&#31350;&#20102;&#25105;&#20204;&#20309;&#26102;&#21487;&#20197;&#26399;&#26395;&#36825;&#20123;&#25351;&#26631;&#36924;&#36817;&#22312;&#32447;&#23454;&#39564;&#30340;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;&#25105;&#20204;&#20174;&#31532;&#19968;&#21407;&#29702;&#19978;&#27491;&#24335;&#25552;&#20986;&#20102;DCG&#34987;&#35748;&#20026;&#26159;&#22312;&#32447;&#22870;&#21169;&#30340;&#26080;&#20559;&#20272;&#35745;&#30340;&#20551;&#35774;&#65292;&#24182;&#32473;&#20986;&#20102;&#36825;&#20010;&#25351;&#26631;&#30340;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.  Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#25628;&#32034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13390</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#28508;&#31354;&#38388;&#30340;&#25628;&#32034;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space. (arXiv:2307.13390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#25628;&#32034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#26159;&#29992;&#20110;&#35299;&#20915;&#31639;&#27861;&#34917;&#25937;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24037;&#20855;&#65306;1. &#26159;&#20160;&#20040;&#20851;&#38190;&#22240;&#32032;&#23548;&#33268;&#20102;&#33258;&#21160;&#39044;&#27979;/&#20915;&#31574;&#65311;2. &#22914;&#20309;&#25913;&#21464;&#36825;&#20123;&#22240;&#32032;&#20197;&#20174;&#29992;&#25143;&#35282;&#24230;&#33719;&#24471;&#26356;&#26377;&#21033;&#30340;&#32467;&#26524;&#65311;&#22240;&#27492;&#65292;&#36890;&#36807;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#21644;&#26131;&#20110;&#23454;&#29616;&#30340;&#21487;&#34892;&#21464;&#21270;&#26469;&#24341;&#23548;&#29992;&#25143;&#19982;AI&#31995;&#32479;&#30340;&#20132;&#20114;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#37319;&#29992;&#21644;&#38271;&#26399;&#25509;&#21463;AI&#31995;&#32479;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;CEs&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#19981;&#21516;&#30340;&#36136;&#37327;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;CEs&#30340;&#29983;&#25104;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#24314;&#35758;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#27492;&#19981;&#21487;&#25805;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#23558;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#24418;&#25104;&#20026;&#39640;&#26031;&#20998;&#24067;&#30340;&#28151;&#21512;&#65292;&#20026;&#39044;&#20808;&#35757;&#32451;&#30340;&#20108;&#20998;&#31867;&#22120;&#29983;&#25104;CEs&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#27969;&#24335;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#65292;&#35745;&#31639;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#20197;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.08433</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#28216;&#36208;&#21040;&#22270;&#24418;&#24555;&#36305;&#65306;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#27969;&#24335;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#65292;&#35745;&#31639;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#20197;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#22522;&#30784;&#30340;&#21160;&#24577;&#22270;&#32467;&#26500;&#65292;&#20854;&#20013;&#23454;&#20307;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#32771;&#34385;&#36825;&#20123;&#21160;&#24577;&#22240;&#32032;&#65292;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#12290;&#20197;&#21069;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#35201;&#20040;&#20391;&#37325;&#20110;&#25277;&#26679;k-&#36339;&#37051;&#22495;&#65292;&#31867;&#20284;&#20110;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#38543;&#26426;&#28216;&#36208;&#65292;&#31867;&#20284;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#26102;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#20302;&#24310;&#36831;&#25512;&#26029;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#19988;&#19981;&#36866;&#29992;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#24418;&#24555;&#36305;&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#65288;CTDGs&#65289;&#30340;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#65292;&#24182;&#19988;&#19982;&#39640;&#24310;&#36831;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#12289;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#20165;&#21333;&#36339;&#25805;&#20316;&#35745;&#31639;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#30340;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop ope
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#20248;&#21270;&#38382;&#39064;&#30340;&#39118;&#26223;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#24046;&#24322;&#36739;&#22823;&#26102;&#65292;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#21487;&#20197;&#36991;&#20813;&#27745;&#26579;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17833</link><description>&lt;p&gt;
&#37325;&#32622;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#22120;&#65306;&#19968;&#20010;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Resetting the Optimizer in Deep RL: An Empirical Study. (arXiv:2306.17833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17833
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#20248;&#21270;&#38382;&#39064;&#30340;&#39118;&#26223;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#24046;&#24322;&#36739;&#22823;&#26102;&#65292;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#21487;&#20197;&#36991;&#20813;&#27745;&#26579;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36817;&#20284;&#35745;&#31639;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#21253;&#25324;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#35299;&#20915;&#19968;&#31995;&#21015;&#19981;&#21516;&#36845;&#20195;&#20013;&#30446;&#26631;&#20989;&#25968;&#21487;&#33021;&#25913;&#21464;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#29616;&#20195;&#21464;&#31181;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22914;Adam&#12290;&#36825;&#20123;&#20248;&#21270;&#22120;&#20445;&#25345;&#33258;&#24049;&#30340;&#20869;&#37096;&#21442;&#25968;&#65292;&#22914;&#26799;&#24230;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#65292;&#24182;&#38543;&#26102;&#38388;&#26356;&#26032;&#36825;&#20123;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#20043;&#21069;&#36845;&#20195;&#30340;&#20449;&#24687;&#34987;&#29992;&#26469;&#22312;&#24403;&#21069;&#36845;&#20195;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#20043;&#21069;&#36845;&#20195;&#30340;&#20248;&#21270;&#39118;&#26223;&#19982;&#24403;&#21069;&#36845;&#20195;&#30456;&#24046;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#21487;&#33021;&#20250;&#27745;&#26579;&#25152;&#20351;&#29992;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#24433;&#21709;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#26159;&#22312;&#24320;&#22987;&#26032;&#30340;&#36845;&#20195;&#26102;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of approximately solving a sequence of optimization problems where the objective function can change per iteration. The common approach to solving the problem is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first and the second moment of the gradient, and update these parameters over time. Therefore, information obtained in previous iterations is being used to solve the optimization problem in the current iteration. We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration. To hedge against this effect, a simple idea is to reset the internal parameters of the optimizer when starting a n
&lt;/p&gt;</description></item><item><title>ManimML&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#36890;&#36807;&#21160;&#30011;&#28436;&#31034;&#33258;&#21160;&#29983;&#25104;&#30340;ML&#31639;&#27861;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#29087;&#24713;&#30340;&#26041;&#24335;&#26469;&#27807;&#36890;&#21644;&#21487;&#35270;&#21270;ML&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.17108</link><description>&lt;p&gt;
ManimML&#65306;&#29992;&#21160;&#30011;&#28436;&#31034;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
ManimML: Communicating Machine Learning Architectures with Animation. (arXiv:2306.17108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17108
&lt;/p&gt;
&lt;p&gt;
ManimML&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#36890;&#36807;&#21160;&#30011;&#28436;&#31034;&#33258;&#21160;&#29983;&#25104;&#30340;ML&#31639;&#27861;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#29087;&#24713;&#30340;&#26041;&#24335;&#26469;&#27807;&#36890;&#21644;&#21487;&#35270;&#21270;ML&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#26032;&#39062;&#30340;ML&#31639;&#27861;&#30340;&#24037;&#20855;&#36824;&#36828;&#36828;&#33853;&#21518;&#12290;&#21160;&#30011;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#21046;&#20316;&#20986;&#38543;&#26102;&#38388;&#21160;&#24577;&#21464;&#21270;&#30340;&#31995;&#32479;&#30340;&#21560;&#24341;&#20154;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#27807;&#36890;ML&#31639;&#27861;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21160;&#30011;&#21270;ML&#31639;&#27861;&#30340;&#26041;&#27861;&#26159;&#25163;&#24037;&#21046;&#20316;&#31361;&#20986;&#29305;&#23450;&#31639;&#27861;&#25110;&#20351;&#29992;&#22797;&#26434;&#30340;&#36890;&#29992;&#21160;&#30011;&#36719;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ManimML&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20195;&#30721;&#20013;&#36731;&#26494;&#29983;&#25104;ML&#31639;&#27861;&#30340;&#21160;&#30011;&#12290;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;ML&#20174;&#19994;&#32773;&#23545;&#32534;&#31243;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#35201;&#27714;&#20182;&#20204;&#23398;&#20064;&#22797;&#26434;&#30340;&#21160;&#30011;&#36719;&#20214;&#12290;ManimML&#20855;&#26377;&#29087;&#24713;&#30340;&#35821;&#27861;&#65292;&#29992;&#20110;&#25351;&#23450;&#27169;&#20223;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;Pytorch&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an explosion in interest in machine learning (ML) in recent years due to its applications to science and engineering. However, as ML techniques have advanced, tools for explaining and visualizing novel ML algorithms have lagged behind. Animation has been shown to be a powerful tool for making engaging visualizations of systems that dynamically change over time, which makes it well suited to the task of communicating ML algorithms. However, the current approach to animating ML algorithms is to handcraft applications that highlight specific algorithms or use complex generalized animation software. We developed ManimML, an open-source Python library for easily generating animations of ML algorithms directly from code. We sought to leverage ML practitioners' preexisting knowledge of programming rather than requiring them to learn complex animation software. ManimML has a familiar syntax for specifying neural networks that mimics popular deep learning frameworks like Pytorch.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#28508;&#22312;SDE&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20301;&#29699;&#19978;&#30340;SDE&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#34920;&#36798;&#24335;&#26469;&#35745;&#31639;&#36817;&#20284;&#21518;&#39564;&#21644;&#20808;&#39564;&#36807;&#31243;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.16248</link><description>&lt;p&gt;
&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#28508;&#22312;SDE
&lt;/p&gt;
&lt;p&gt;
Latent SDEs on Homogeneous Spaces. (arXiv:2306.16248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#28508;&#22312;SDE&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20301;&#29699;&#19978;&#30340;SDE&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#34920;&#36798;&#24335;&#26469;&#35745;&#31639;&#36817;&#20284;&#21518;&#39564;&#21644;&#20808;&#39564;&#36807;&#31243;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#65288;&#21487;&#33021;&#26159;&#22797;&#26434;&#30340;&#65289;&#35266;&#27979;&#38543;&#26426;&#36807;&#31243;&#30001;&#28508;&#22312;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#25152;&#39537;&#21160;&#12290;&#21463;&#21040;&#23398;&#20064;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#65288;&#20960;&#20046;&#20219;&#24847;&#65289;&#28508;&#22312;&#31070;&#32463;SDE&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#20363;&#22914;&#25928;&#29575;&#26799;&#24230;&#35745;&#31639;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#24182;&#30740;&#31350;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#23376;&#31867;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;SDE&#22312;&#19968;&#20010;&#40784;&#27425;&#28508;&#22312;&#31354;&#38388;&#19978;&#28436;&#21464;&#65292;&#24182;&#30001;&#30456;&#24212;&#65288;&#30697;&#38453;&#65289;Lie&#32676;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#25152;&#35825;&#23548;&#12290;&#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21333;&#20301;$n$-&#29699;&#19978;&#30340;SDE&#21487;&#20197;&#35828;&#26159;&#36825;&#19968;&#35774;&#32622;&#20013;&#26368;&#30456;&#20851;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#65292;&#21333;&#20301;&#29699;&#19981;&#20165;&#26377;&#21161;&#20110;&#20351;&#29992;&#30495;&#27491;&#26080;&#20449;&#24687;&#30340;&#20808;&#39564;SDE&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#33719;&#24471;&#20102;&#20851;&#20110;&#36817;&#20284;&#21518;&#39564;&#21644;&#20808;&#39564;&#36807;&#31243;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#29305;&#21035;&#31616;&#21333;&#21644;&#30452;&#35266;&#30340;&#34920;&#36798;&#24335;&#65292;&#36825;&#22312;&#35777;&#25454;&#19979;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of variational Bayesian inference in a latent variable model where a (possibly complex) observed stochastic process is governed by the solution of a latent stochastic differential equation (SDE). Motivated by the challenges that arise when trying to learn an (almost arbitrary) latent neural SDE from large-scale data, such as efficient gradient computation, we take a step back and study a specific subclass instead. In our case, the SDE evolves on a homogeneous latent space and is induced by stochastic dynamics of the corresponding (matrix) Lie group. In learning problems, SDEs on the unit $n$-sphere are arguably the most relevant incarnation of this setup. Notably, for variational inference, the sphere not only facilitates using a truly uninformative prior SDE, but we also obtain a particularly simple and intuitive expression for the Kullback-Leibler divergence between the approximate posterior and prior process in the evidence lower bound. Experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;PriorBand&#65292;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#19987;&#23478;&#20449;&#24565;&#21644;&#24265;&#20215;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.12370</link><description>&lt;p&gt;
PriorBand: &#28145;&#24230;&#23398;&#20064;&#19979;&#30340;&#23454;&#29992;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning. (arXiv:2306.12370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12370
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;PriorBand&#65292;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#19987;&#23478;&#20449;&#24565;&#21644;&#24265;&#20215;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27969;&#31243;&#20013;&#30340;&#36229;&#21442;&#25968;&#23545;&#20854;&#19979;&#28216;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#20854;&#20195;&#20215;&#24448;&#24448;&#23545;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25163;&#21160;&#23454;&#39564;&#20173;&#26159;&#20248;&#21270;&#36229;&#21442;&#25968;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#30452;&#35273;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#24265;&#20215;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;HPO&#31639;&#27861;&#21644;DL&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#36825;&#31181;&#19981;&#21305;&#37197;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PriorBand&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;DL&#37327;&#36523;&#23450;&#21046;&#30340;HPO&#31639;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#19987;&#23478;&#20449;&#24565;&#21644;&#24265;&#20215;&#30340;&#20195;&#29702;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;PriorBand&#22312;&#19968;&#31995;&#21015;DL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#22312;&#25552;&#20379;&#26377;&#25928;&#19987;&#23478;&#36755;&#20837;&#21644;&#25239;&#20987;&#19981;&#33391;&#19987;&#23478;&#20449;&#24565;&#26041;&#38754;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters of Deep Learning (DL) pipelines are crucial for their downstream performance. While a large number of methods for Hyperparameter Optimization (HPO) have been developed, their incurred costs are often untenable for modern DL. Consequently, manual experimentation is still the most prevalent approach to optimize hyperparameters, relying on the researcher's intuition, domain knowledge, and cheap preliminary explorations. To resolve this misalignment between HPO algorithms and DL researchers, we propose PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency across a range of DL benchmarks and show its gains under informative expert input and robustness against poor expert beliefs
&lt;/p&gt;</description></item><item><title>AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11971</link><description>&lt;p&gt;
AdCraft&#65306;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20248;&#21270;&#30340;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11971
&lt;/p&gt;
&lt;p&gt;
AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#8212;&#8212; AdCraft&#65292;&#20854;&#20855;&#26377;&#38543;&#26426;&#21644;&#38750;&#38745;&#24577;&#29305;&#24615;&#12290;&#35813;&#29615;&#22659;&#27169;&#25311;&#20102;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20013;&#20986;&#20215;&#21644;&#39044;&#31639;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;SEM&#26159;&#19968;&#31181;&#21033;&#29992;&#20184;&#36153;&#24191;&#21578;&#26469;&#22686;&#21152;&#32593;&#31449;&#22312;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#19978;&#30340;&#21487;&#35265;&#24615;&#30340;&#25968;&#23383;&#33829;&#38144;&#25216;&#26415;&#12290;SEM&#24191;&#21578;&#27963;&#21160;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65292;&#21253;&#25324;&#20851;&#38190;&#23383;&#36873;&#25321;&#12289;&#24191;&#21578;&#35774;&#35745;&#12289;&#20986;&#20215;&#31649;&#29702;&#12289;&#39044;&#31639;&#35843;&#25972;&#21644;&#34920;&#29616;&#30417;&#25511;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20248;&#21270;SEM&#24191;&#21578;&#25237;&#25918;&#27963;&#21160;&#30340;&#28508;&#22312;&#31574;&#30053;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#25110;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#21487;&#23450;&#21046;&#29615;&#22659;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#35780;&#20272;&#21644;&#25552;&#39640;&#19982;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20184;&#20986;&#36825;&#20123;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;AdCraft&#29615;&#22659;&#19979;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$FPDM$&#65292;&#20351;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;$FPDM$&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#36755;&#20837;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#24494;&#35843;&#26102;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06190</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26723;&#32423;&#20803;&#25968;&#25454;&#30340;&#39046;&#22495;&#29305;&#23450;&#24555;&#36895;&#39044;&#35757;&#32451;&#25216;&#26415;$FPDM$
&lt;/p&gt;
&lt;p&gt;
$FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata. (arXiv:2306.06190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$FPDM$&#65292;&#20351;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;$FPDM$&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#36755;&#20837;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#24494;&#35843;&#26102;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#24050;&#26174;&#31034;&#20986;&#22312;&#24320;&#25918;&#39046;&#22495;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;transformers&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$FPDM$&#65288;Fast Pre-training Technique using Document Level Metadata&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#26368;&#20027;&#35201;&#30340;&#21019;&#26032;&#22312;&#20110;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#21477;&#23376;&#32423;&#21035;&#30340;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#65292;&#25345;&#32493;&#23545;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20197;&#36866;&#24212;&#38271;&#25991;&#26723;&#65289;&#65292;&#20294;&#22312;&#23545;&#35813;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#21017;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;$FPDM$&#22312;&#23458;&#25143;&#25903;&#25345;&#12289;&#31185;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#23383;&#31526;&#32423;F1&#20998;&#25968;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#22522;&#20110;transformer&#30340;&#22522;&#20934;&#65292;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#21518;&#24615;&#33021;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training Transformers has shown promising results on open-domain and domain-specific downstream tasks. However, state-of-the-art Transformers require an unreasonably large amount of pre-training data and compute. In this paper, we propose $FPDM$ (Fast Pre-training Technique using Document Level Metadata), a novel, compute-efficient framework that utilizes Document metadata and Domain-Specific Taxonomy as supervision signals to pre-train transformer encoder on a domain-specific corpus. The main innovation is that during domain-specific pretraining, an open-domain encoder is continually pre-trained using sentence-level embeddings as inputs (to accommodate long documents), however, fine-tuning is done with token-level embeddings as inputs to this encoder. We show that $FPDM$ outperforms several transformer-based baselines in terms of character-level F1 scores and other automated metrics in the Customer Support, Scientific, and Legal Domains, and shows a negligible drop in performance 
&lt;/p&gt;</description></item><item><title>BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.03530</link><description>&lt;p&gt;
BackpropTools: &#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03530
&lt;/p&gt;
&lt;p&gt;
BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20135;&#29983;&#20986;&#20855;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#23454;&#26102;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#23398;&#20064;&#31574;&#30053;&#22312;&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackpropTools&#65292;&#19968;&#31181;&#20381;&#36182;&#24615;-free&#12289;header-only&#12289;pure C++&#30340;&#28145;&#24230;&#30417;&#30563;&#21644;&#24378;&#21270;&#23398;&#20064;&#24211;&#12290;&#21033;&#29992;&#26368;&#36817;C++&#26631;&#20934;&#30340;&#27169;&#26495;&#20803;&#32534;&#31243;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20197;&#30001;&#32534;&#35793;&#22120;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#12290;&#20854;&#26032;&#39062;&#30340;&#26550;&#26500;&#20801;&#35768;BackpropTools&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#20174;HPC&#38598;&#32676;&#12289;&#24037;&#20316;&#31449;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#21040;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#25163;&#34920;&#21644;&#24494;&#25511;&#21046;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;RL&#31639;&#27861;&#19982;&#27169;&#25311;&#29615;&#22659;&#30340;&#32039;&#23494;&#38598;&#25104;&#65292;BackpropTools&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#20351;&#20854;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MiniSUPERB&#30340;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#27604;SUPERB&#26356;&#20302;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.19011</link><description>&lt;p&gt;
MiniSUPERB:&#36731;&#37327;&#32423;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models. (arXiv:2305.19011v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19011
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MiniSUPERB&#30340;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#27604;SUPERB&#26356;&#20302;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SUPERB&#34987;&#25552;&#20986;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35821;&#38899;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#20219;&#21153;&#65292;&#23427;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MiniSUPERB&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#20197;&#26126;&#26174;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#24182;&#19988;&#32467;&#26524;&#21487;&#19982;SUPERB&#30456;&#27604;&#12290;&#25105;&#20204;&#31934;&#36873;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#37319;&#26679;&#25968;&#25454;&#38598;&#65292;&#24182;&#31163;&#32447;&#25552;&#21462;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;SUPERB Paper&#21644;SUPERB Challenge&#20998;&#21035;&#36798;&#21040;0.954&#21644;0.982&#30340;&#26031;&#30382;&#23572;&#26364;&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20056;-&#32047;&#31215;&#25805;&#20316;&#65288;MACs&#65289;&#26041;&#38754;&#20943;&#23569;&#20102;97&#65285;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#24615;&#33021;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#26412;&#36523;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
SUPERB was proposed to evaluate the generalizability of self-supervised learning (SSL) speech models across various tasks. However, it incurs high computational costs due to the large datasets and diverse tasks. In this paper, we introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL speech models with comparable results to SUPERB but lower computational costs significantly. We carefully select representative tasks, sample datasets, and extract model representations offline. Our approach achieves a Spearman's rank correlation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge, respectively. Additionally, we reduce the computational cost by 97% in terms of Multiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech models in few-shot scenarios and observe significant variations in their performance. To our knowledge, this is the first study to examine both the computational cost of the model itself and the cost of evaluating it on a benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#23436;&#25972;&#35823;&#24046;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#19979;&#21521;&#37327;&#20540;RF&#20272;&#35745;&#22120;&#30340;&#24378;&#19968;&#33268;&#24615;&#21644;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#26497;&#23567;&#21270;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17170</link><description>&lt;p&gt;
&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#35823;&#24046;&#30028;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Error Bounds for Learning with Vector-Valued Random Features. (arXiv:2305.17170v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#23436;&#25972;&#35823;&#24046;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#19979;&#21521;&#37327;&#20540;RF&#20272;&#35745;&#22120;&#30340;&#24378;&#19968;&#33268;&#24615;&#21644;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#26497;&#23567;&#21270;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#23436;&#25972;&#35823;&#24046;&#20998;&#26512;&#12290;&#35813;&#29702;&#35770;&#26159;&#38024;&#23545;&#23436;&#20840;&#36890;&#29992;&#30340;&#26080;&#38480;&#32500;&#24230;&#36755;&#20837;-&#36755;&#20986;&#35774;&#23450;&#20013;&#30340;RF Ridge&#22238;&#24402;&#32780;&#24320;&#21457;&#30340;&#65292;&#20294;&#20173;&#36866;&#29992;&#20110;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26377;&#38480;&#32500;&#24230;&#20998;&#26512;&#12290;&#19982;&#25991;&#29486;&#20013;&#20854;&#20182;&#31867;&#20284;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24213;&#23618;&#39118;&#38505;&#20989;&#25968;&#30340;&#30452;&#25509;&#20998;&#26512;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#30340;&#26174;&#24335;RF Ridge&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#20844;&#24335;&#30340;&#20351;&#29992;&#12290;&#36825;&#28040;&#38500;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20013;&#30340;&#27987;&#24230;&#32467;&#26524;&#25110;&#20854;&#23545;&#38543;&#26426;&#31639;&#23376;&#30340;&#25512;&#24191;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#24314;&#31435;&#30340;&#20027;&#35201;&#32467;&#26524;&#21253;&#25324;&#22312;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#19979;&#21521;&#37327;&#20540;RF&#20272;&#35745;&#22120;&#30340;&#24378;&#19968;&#33268;&#24615;&#21644;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#26497;&#23567;&#21270;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#23454;&#29616;&#36825;&#20123;&#25910;&#25947;&#36895;&#29575;&#25152;&#38656;&#30340;&#21442;&#25968;&#22797;&#26434;&#24230;(&#38543;&#26426;&#29305;&#24449;&#25968;&#37327;)&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;(&#26631;&#35760;&#25968;&#25454;&#25968;&#37327;)&#19982;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive error analysis of learning with vector-valued random features (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses. In contrast to comparable work in the literature, the approach proposed here relies on a direct analysis of the underlying risk functional and completely avoids the explicit RF ridge regression solution formula in terms of random matrices. This removes the need for concentration results in random matrix theory or their generalizations to random operators. The main results established in this paper include strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting. The parameter complexity (number of random features) and sample complexity (number of labeled data) required to achieve such rates are comparable with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388; &#33021;&#22815;&#39640;&#25928;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#8220;Web-Available Image Search (WAIS)&#8221;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.12102</link><description>&lt;p&gt;
&#32479;&#19968;&#23884;&#20837;&#65306;&#38754;&#21521; Web &#35268;&#27169; ML &#31995;&#32479;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems. (arXiv:2305.12102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388; &#33021;&#22815;&#39640;&#25928;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#8220;Web-Available Image Search (WAIS)&#8221;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#23545;&#20110; Web &#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#27599;&#20010;&#29305;&#24449;&#20540;&#34920;&#31034;&#20026;&#19968;&#20010; d &#32500;&#23884;&#20837;&#65292;&#24341;&#20837;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#65292;&#32780;&#36825;&#20123;&#29305;&#24449;&#30340;&#22522;&#25968;&#38750;&#24120;&#39640;&#12290;&#36825;&#20010;&#29942;&#39048;&#23548;&#33268;&#20102;&#22791;&#36873;&#23884;&#20837;&#31639;&#27861;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#65292;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#20043;&#38388;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#22797;&#29992;&#30340;&#23884;&#20837;&#21487;&#20197;&#20998;&#35299;&#20026;&#27599;&#20010;&#32452;&#25104;&#29305;&#24449;&#30340;&#32452;&#20214;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#29992;&#30340;&#23884;&#20837;&#22312;&#20960;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Web-Available Image Search (WAIS)&#8221;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272; Web &#35268;&#27169;&#19979;&#30340;&#26032;&#23884;&#20837;&#31639;&#27861;&#12290;&#25105;&#20204;&#36992;&#35831;&#31038;&#21306;&#36890;&#36807;&#25552;&#20986;&#21487;&#20197;&#20934;&#30830;&#12289;&#39640;&#25928;&#22320;&#23558;&#25968;&#30334;&#19975;&#24352;&#22270;&#20687;&#23884;&#20837;&#21644;&#20998;&#31867;&#21040;&#25104;&#21315;&#19978;&#19975;&#20010;&#31867;&#21035;&#30340;&#26032;&#27169;&#22411;&#26469;&#36129;&#29486; WAIS &#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality feature embeddings efficiently and effectively is critical for the performance of web-scale machine learning systems. A typical model ingests hundreds of features with vocabularies on the order of millions to billions of tokens. The standard approach is to represent each feature value as a d-dimensional embedding, introducing hundreds of billions of parameters for extremely high-cardinality features. This bottleneck has led to substantial progress in alternative embedding algorithms. Many of these methods, however, make the assumption that each feature uses an independent embedding table. This work introduces a simple yet highly effective framework, Feature Multiplexing, where one single representation space is used across many different categorical features. Our theoretical and empirical analysis reveals that multiplexed embeddings can be decomposed into components from each constituent feature, allowing models to distinguish between features. We show that multip
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.11531</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#26032;&#22411;&#30005;&#30913;&#37327;&#35745;&#20960;&#20309;&#27169;&#25311;&#30340;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11531
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#29983;&#25104;&#23545;&#25758;&#20135;&#29289;&#30340;&#27169;&#25311;&#25506;&#27979;&#22120;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#12290;&#20854;&#20013;&#19968;&#20010;&#23376;&#25506;&#27979;&#22120;&#65292;&#30005;&#30913;&#37327;&#35745;&#30001;&#20110;&#20854;&#21333;&#20803;&#26684;&#30340;&#39640;&#31890;&#24230;&#21644;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#32780;&#21344;&#25454;&#20102;&#35745;&#31639;&#26102;&#38388;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#24555;&#30340;&#26679;&#26412;&#29983;&#25104;&#65292;&#20294;&#30446;&#21069;&#38656;&#35201;&#22823;&#37327;&#21162;&#21147;&#26469;&#20248;&#21270;&#29305;&#23450;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#32593;&#32476;&#26469;&#25551;&#36848;&#19981;&#21516;&#30340;&#21333;&#20803;&#26684;&#22823;&#23567;&#21644;&#25490;&#21015;&#26041;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#33021;&#25512;&#24191;&#21040;&#20854;&#20182;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#8220;&#20960;&#20309;&#24863;&#30693;&#8221;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#29983;&#25104;&#30475;&#19981;&#35265;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#27169;&#25311;&#21709;&#24212;&#32780;&#26080;&#38656;&#20854;&#20182;&#35757;&#32451;&#12290;&#35813;&#20960;&#20309;&#24863;&#30693;&#27169;&#22411;&#22312;&#28041;&#21450;&#20851;&#38190;&#21709;&#24212;&#30340;&#29983;&#25104;&#21644;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#31561;&#25351;&#26631;&#19978;&#27604;&#22522;&#32447;&#27169;&#22411;&#20248;&#36234;50&#65285;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#25193;&#23637;&#21040;&#38750;&#24179;&#38754;&#20960;&#20309;&#24418;&#29366;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of simulated detector response to collision products is crucial to data analysis in particle physics, but computationally very expensive. One subdetector, the calorimeter, dominates the computational time due to the high granularity of its cells and complexity of the interaction. Generative models can provide more rapid sample production, but currently require significant effort to optimize performance for specific detector geometries, often requiring many networks to describe the varying cell sizes and arrangements, which do not generalize to other geometries. We develop a {\it geometry-aware} autoregressive model, which learns how the calorimeter response varies with geometry, and is capable of generating simulated responses to unseen geometries without additional training. The geometry-aware model outperforms a baseline, unaware model by 50\% in metrics such as the Wasserstein distance between generated and true distributions of key quantities which summarize the simulate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.09863</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#19981;&#36879;&#26126;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#35810;&#38382;&#26159;&#21542;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#19968;&#20010;&#8220;&#25991;&#26412;&#27169;&#22359;&#8221;&#26159;&#23558;&#25991;&#26412;&#26144;&#23556;&#21040;&#26631;&#37327;&#36830;&#32493;&#20540;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#20363;&#22914;LLM&#20869;&#30340;&#23376;&#27169;&#22359;&#25110;&#22823;&#33041;&#21306;&#22495;&#30340;&#25311;&#21512;&#27169;&#22411;&#12290;&#8220;&#40657;&#30418;&#8221;&#34920;&#31034;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#27169;&#22359;&#30340;&#36755;&#20837;/&#36755;&#20986;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Summarize and Score&#65288;SASC&#65289;&#26041;&#27861;&#65292;&#23427;&#25509;&#21463;&#25991;&#26412;&#27169;&#22359;&#24182;&#36820;&#22238;&#27169;&#22359;&#36873;&#25321;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;SASC&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#27169;&#22359;&#19978;&#35780;&#20272;SASC&#65292;&#24182;&#21457;&#29616;&#23427;&#32463;&#24120;&#24674;&#22797;&#22522;&#26412;&#30495;&#30456;&#35828;&#26126;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;SASC&#26469;&#35299;&#37322;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#65292;&#20351;&#24471;&#26816;&#26597;BERT&#30340;&#27169;&#22359;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.06348</link><description>&lt;p&gt;
&#24102;&#27010;&#29575;&#24577;&#23556;&#21644;&#26680;&#24179;&#22343;&#23884;&#20837;&#30340;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised learning with probabilistic morphisms and kernel mean embeddings. (arXiv:2305.06348v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;X&#21644;&#26631;&#31614;&#31354;&#38388;Y&#12290; &#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#24517;&#39035;&#27491;&#30830;&#22320;&#24230;&#37327;&#21487;&#33021;&#39044;&#27979;&#22120;&#30340;&#20551;&#35774;&#31354;&#38388;H&#20013;&#30340;&#20803;&#32032;&#19982;&#30417;&#31649;&#36816;&#31639;&#31526;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#30417;&#31649;&#36816;&#31639;&#31526;&#21487;&#33021;&#19981;&#23646;&#20110;H&#12290; &#20026;&#20102;&#23450;&#20041;&#27491;&#30830;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#22312;&#25237;&#24433;&#928;X&#65306;X&#215;Y&#8594;X&#30456;&#23545;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#119883;&#215;&#119884;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20316;&#20026;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#22914;&#26524;Y&#26159;&#19968;&#20010;&#20855;&#26377;Borel &#963;-&#20195;&#25968; BY&#30340;&#21487;&#20998;&#30340;&#21487;&#24230;&#37327;&#21270;&#25299;&#25169;&#31354;&#38388;&#65292;&#21017;&#25552;&#20986;&#20102;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#30456;&#23545;&#20110;&#25237;&#24433;&#928;X&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#21478;&#19968;&#31181;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\mathcal{X}$ and a label space $\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operator equation. If $\mathcal{Y}$ is a separable metrizable topological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose another characterization of a regular conditional probability measure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#26469;&#23398;&#20064;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#25152;&#38656;&#30340;&#35266;&#23519;&#24207;&#21015;&#30340;&#26368;&#20248;&#39034;&#24207;&#12290;&#22240;&#20026;&#35786;&#26029;&#25351;&#21335;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#23545;&#32597;&#35265;&#30149;&#25110;&#24739;&#26377;&#22810;&#31181;&#30149;&#30340;&#24739;&#32773;&#65292;DRL&#31639;&#27861;&#20855;&#26377;&#37325;&#35201;&#29616;&#23454;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.06295</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20174;&#30005;&#23376;&#30149;&#21382;&#20013;&#25552;&#21462;&#35786;&#26029;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning. (arXiv:2305.06295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#26469;&#23398;&#20064;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#25152;&#38656;&#30340;&#35266;&#23519;&#24207;&#21015;&#30340;&#26368;&#20248;&#39034;&#24207;&#12290;&#22240;&#20026;&#35786;&#26029;&#25351;&#21335;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#23545;&#32597;&#35265;&#30149;&#25110;&#24739;&#26377;&#22810;&#31181;&#30149;&#30340;&#24739;&#32773;&#65292;DRL&#31639;&#27861;&#20855;&#26377;&#37325;&#35201;&#29616;&#23454;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35786;&#26029;&#25351;&#21335;&#26088;&#22312;&#35828;&#26126;&#21487;&#33021;&#23548;&#33268;&#35786;&#26029;&#30340;&#27493;&#39588;&#12290;&#25351;&#21335;&#33021;&#22815;&#29702;&#24615;&#22320;&#35268;&#33539;&#21270;&#20020;&#24202;&#20915;&#31574;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#30340;&#24314;&#31435;&#26159;&#20026;&#20102;&#35206;&#30422;&#22823;&#22810;&#25968;&#20154;&#32676;&#65292;&#22240;&#27492;&#22312;&#25351;&#23548;&#32597;&#35265;&#30149;&#25110;&#24739;&#26377;&#22810;&#31181;&#30149;&#30340;&#24739;&#32773;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#26041;&#38754;&#65292;&#23384;&#22312;&#32570;&#38519;&#12290;&#26412;&#25991;&#21463;&#25351;&#21335;&#21551;&#21457;&#65292;&#23558;&#35786;&#26029;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;(EHRs)&#35757;&#32451;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#25152;&#38656;&#30340;&#35266;&#23519;&#24207;&#21015;&#30340;&#26368;&#20248;&#39034;&#24207;&#12290;&#30001;&#20110;DRL&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#19978;&#19979;&#25991;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#26041;&#27861;&#21644;&#35774;&#32622;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#24444;&#27492;&#21644;&#32463;&#20856;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#20294;&#36924;&#30495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis. Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies. Moreover, their updates are long and expensive, making them unsuitable to emerging practices. Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis. Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers. We experimented on a synthetic but realistic dataset to differenti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;$\beta$-VAE&#21644;Transformer&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#36817;&#20284;&#27491;&#20132;&#30340;ROMs&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#28151;&#27788;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03571</link><description>&lt;p&gt;
$\beta$-&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;Transformer&#29992;&#20110;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
$\beta$-Variational autoencoders and transformers for reduced-order modelling of fluid flows. (arXiv:2304.03571v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;$\beta$-VAE&#21644;Transformer&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#36817;&#20284;&#27491;&#20132;&#30340;ROMs&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#28151;&#27788;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26550;&#26500;&#26377;&#28508;&#21147;&#24320;&#21457;&#28151;&#27788;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#27169;&#22411;&#65288;ROMs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;$\beta$-VAE&#21644;Transformer&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#36817;&#20284;&#27491;&#20132;&#30340;ROMs&#65292;&#21516;&#26102;&#22312;&#20108;&#32500;&#31896;&#24615;&#27969;&#20307;&#27969;&#21160;&#30340;&#21608;&#26399;&#21644;&#28151;&#27788;&#29366;&#24577;&#19979;&#36827;&#34892;&#25968;&#20540;&#27979;&#35797;&#12290;$\beta$-VAE&#34987;&#35757;&#32451;&#20026;&#23398;&#20064;&#27969;&#36895;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#65292;&#32780;Transformer&#21017;&#34987;&#35757;&#32451;&#20026;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#26102;&#38388;&#21160;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;$\beta$-VAE&#26469;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#24418;&#24335;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#21487;&#35299;&#37322;&#30340;&#27969;&#21160;&#27169;&#22411;&#65292;&#20854;&#29305;&#24449;&#31867;&#20284;&#20110;&#35266;&#23519;&#21040;&#30340;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#65292;&#20294;&#34920;&#31034;&#26356;&#39640;&#25928;&#12290;&#20351;&#29992;Poincar&#233;&#22270;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#27969;&#20307;&#27969;&#21160;&#30340;&#22522;&#26412;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#22825;&#27668;&#39044;&#25253;&#12289;&#22320;&#23618;&#27969;&#21644;&#27668;&#20505;&#27169;&#22411;&#31561;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoder (VAE) architectures have the potential to develop reduced-order models (ROMs) for chaotic fluid flows. We propose a method for learning compact and near-orthogonal ROMs using a combination of a $\beta$-VAE and a transformer, tested on numerical data from a two-dimensional viscous flow in both periodic and chaotic regimes. The $\beta$-VAE is trained to learn a compact latent representation of the flow velocity, and the transformer is trained to predict the temporal dynamics in latent space. Using the $\beta$-VAE to learn disentangled representations in latent-space, we obtain a more interpretable flow model with features that resemble those observed in the proper orthogonal decomposition, but with a more efficient representation. Using Poincar\'e maps, the results show that our method can capture the underlying dynamics of the flow outperforming other prediction models. The proposed method has potential applications in other fields such as weather forecasting, st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SS-shapelets&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#21644;&#20256;&#25773;&#30340;&#20266;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#26469;&#21457;&#29616;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03292</link><description>&lt;p&gt;
SS-shapelets: &#20195;&#34920;&#24418;&#29366;&#23376;&#24207;&#21015;&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SS-shapelets: Semi-supervised Clustering of Time Series Using Representative Shapelets. (arXiv:2304.03292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SS-shapelets&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#21644;&#20256;&#25773;&#30340;&#20266;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#26469;&#21457;&#29616;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#23376;&#24207;&#21015;&#26159;&#20351;&#29992;&#26412;&#22320;&#29305;&#24449;&#65288;&#23376;&#24207;&#21015;&#65289;&#37492;&#21035;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#21069;&#36884;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#22240;&#20026;&#23427;&#20204;&#20174;&#22823;&#37327;&#26080;&#20449;&#24687;&#30340;&#23376;&#24207;&#21015;&#20013;&#21457;&#29616;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#22240;&#27492;&#32858;&#31867;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#21644;&#20256;&#25773;&#30340;&#20266;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#26469;&#24110;&#21161;&#21457;&#29616;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24615;&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65288;SS-Shapelets&#65289;&#12290;&#22312;SS-Shapelets&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#21457;&#29616;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#12290;1&#65289;&#19968;&#20010;&#8220;&#26174;&#33879;&#23376;&#24207;&#21015;&#38142;&#8221;&#65288;$SSC$&#65289;&#65292;&#21487;&#20197;&#20174;&#26631;&#35760;/&#20266;&#26631;&#35760;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#26174;&#33879;&#23376;&#24207;&#21015;&#65288;&#20316;&#20026;&#20505;&#36873;&#24418;&#29366;&#23376;&#24207;&#21015;&#65289;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20174;&#27744;&#20013;&#21024;&#38500;&#22823;&#37327;&#26080;&#20449;&#24687;&#30340;&#23376;&#24207;&#21015;&#12290;2&#65289;&#19968;&#31181;&#8220;&#32447;&#24615;&#21028;&#21035;&#36873;&#25321;&#24418;&#29366;&#23376;&#24207;&#21015;&#8221;&#65288;$LDSS$&#65289;&#65292;&#23427;&#36873;&#25321;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#24418;&#29366;&#23376;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapelets that discriminate time series using local features (subsequences) are promising for time series clustering. Existing time series clustering methods may fail to capture representative shapelets because they discover shapelets from a large pool of uninformative subsequences, and thus result in low clustering accuracy. This paper proposes a Semi-supervised Clustering of Time Series Using Representative Shapelets (SS-Shapelets) method, which utilizes a small number of labeled and propagated pseudo-labeled time series to help discover representative shapelets, thereby improving the clustering accuracy. In SS-Shapelets, we propose two techniques to discover representative shapelets for the effective clustering of time series. 1) A \textit{salient subsequence chain} ($SSC$) that can extract salient subsequences (as candidate shapelets) of a labeled/pseudo-labeled time series, which helps remove massive uninformative subsequences from the pool. 2) A \textit{linear discriminant select
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21333;&#20010;&#29992;&#25143;&#25968;&#25454;&#24182;&#22312;&#21518;&#22788;&#29702;&#20013;&#21512;&#24182;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#29992;&#25143;&#27963;&#21160;&#30340;&#35782;&#21035;&#65292;&#26377;&#26395;&#29992;&#20110;&#20154;&#26426;&#21327;&#20316;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2302.05763</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36741;&#21161;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#23454;&#29616;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications. (arXiv:2302.05763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21333;&#20010;&#29992;&#25143;&#25968;&#25454;&#24182;&#22312;&#21518;&#22788;&#29702;&#20013;&#21512;&#24182;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#29992;&#25143;&#27963;&#21160;&#30340;&#35782;&#21035;&#65292;&#26377;&#26395;&#29992;&#20110;&#20154;&#26426;&#21327;&#20316;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#36880;&#28176;&#20851;&#27880;&#22810;&#26041;&#38754;&#22330;&#26223;&#65292;&#21363;&#26426;&#22120;&#20154;&#19982;&#22810;&#20010;&#20154;&#29992;&#25143;&#21516;&#26102;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290; &#28982;&#32780;&#65292;&#22312;&#20154;&#26426;&#21327;&#20316;&#26041;&#38754;&#65292;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#22788;&#29702;&#27492;&#31867;&#21512;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38656;&#35201;&#30340;&#25968;&#25454;&#27604;&#20856;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#35774;&#32622;&#20013;&#26356;&#19981;&#21487;&#34892;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#20108;&#20803;&#20154;&#26426;&#21327;&#20316;&#24212;&#29992;&#30340;&#24182;&#34892;&#20219;&#21153;&#22330;&#26223;&#65292;&#24182;&#25552;&#35758;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25910;&#38598;&#19982;&#22810;&#29992;&#25143;&#27963;&#21160;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#21363;&#25910;&#38598;&#19982;&#21333;&#20010;&#29992;&#25143;&#30456;&#20851;&#30340;&#25968;&#25454;&#24182;&#22312;&#21518;&#22788;&#29702;&#20013;&#21512;&#24182;&#23427;&#20204;&#65292;&#20197;&#20943;&#23569;&#20135;&#29983;&#25104;&#21452;&#35774;&#32622;&#24405;&#21046;&#30340;&#21162;&#21147;&#12290;&#25910;&#38598;&#20102;&#21333;&#20010;&#29992;&#25143;&#30340;&#27963;&#21160;&#19977;&#32500;&#39592;&#26550;&#23039;&#21183;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#25104;&#19968;&#23545;&#26469;&#39564;&#35777;&#35813;&#35821;&#21477;&#65292;&#38543;&#21518;&#65292;&#36825;&#20123;&#25968;&#25454;&#28857;&#34987;&#29992;&#20110;&#20998;&#21035;&#35757;&#32451;&#30001;LSTM&#32593;&#32476;&#21644;VAE &#28151;&#21512;&#32780;&#25104;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-robot interaction (HRI) research is progressively addressing multi-party scenarios, where a robot interacts with more than one human user at the same time. Conversely, research is still at an early stage for human-robot collaboration. The use of machine learning techniques to handle such type of collaboration requires data that are less feasible to produce than in a typical HRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRC applications. Based upon these concepts, this study also proposes an alternative way of gathering data regarding multi-user activity, by collecting data related to single users and merging them in post-processing, to reduce the effort involved in producing recordings of pair settings. To validate this statement, 3D skeleton poses of activity of single users were collected and merged in pairs. After this, such datapoints were used to separately train a long short-term memory (LSTM) network and a variational autoencoder (VAE) composed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;</title><link>http://arxiv.org/abs/2301.12554</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#28369;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing. (arXiv:2301.12554v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#37327;&#22686;&#24378;&#31070;&#32463;&#20998;&#31867;&#22120;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22312;&#28165;&#26224;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#21487;&#25509;&#21463;&#30340;&#20005;&#37325;&#24809;&#32602;&#65292;&#23454;&#36341;&#32773;&#20173;&#28982;&#19981;&#24895;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#24378;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#20854;&#20013;&#26631;&#20934;&#32593;&#32476;&#20248;&#21270;&#28165;&#26224;&#24230;&#32780;&#19981;&#26159;&#19968;&#33324;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#26174;&#30528;&#20943;&#36731;&#36825;&#31181;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#22522;&#20110;&#40065;&#26834;&#24615;&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#31034;&#20363;&#30340;&#32622;&#20449;&#24230;&#24046;&#24322;&#26159;&#36825;&#31181;&#25913;&#21892;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38500;&#25552;&#20379;&#30452;&#35266;&#21644;&#32463;&#39564;&#35777;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#19968;&#20010;&#23545;&#25239;&#24615;&#36755;&#20837;&#26816;&#27979;&#22120;&#36866;&#24212;&#20026;&#28151;&#21512;&#32593;&#32476;&#65292;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#28151;&#21512;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24615;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
While prior research has proposed a plethora of methods that enhance the adversarial robustness of neural classifiers, practitioners are still reluctant to adopt these techniques due to their unacceptably severe penalties in clean accuracy. This paper shows that by mixing the output probabilities of a standard classifier and a robust model, where the standard network is optimized for clean accuracy and is not robust in general, this accuracy-robustness trade-off can be significantly alleviated. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key ingredient of this improvement. In addition to providing intuitive and empirical evidence, we also theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The 
&lt;/p&gt;</description></item><item><title>&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11562</link><description>&lt;p&gt;
&#39044;&#27979;&#26159;&#21542;&#38543;&#24847;&#65311;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#35780;&#20272;&#33258;&#27965;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290; &#23454;&#35777;&#34920;&#26126;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30340;&#26041;&#24046;&#24046;&#24322;&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#20915;&#31574;&#23454;&#38469;&#19978;&#26159;&#38543;&#24847;&#30340;&#12290; &#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20570;&#20986;&#20102;&#22235;&#20010;&#24635;&#20307;&#36129;&#29486;&#65306;&#25105;&#20204;1&#65289;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#33258;&#27965;&#24615;&#65292;&#22312;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#26102;&#20351;&#29992;&#65307; 2&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#24403;&#39044;&#27979;&#26080;&#27861;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#21487;&#20197;&#25918;&#24323;&#20998;&#31867;&#65307; 3&#65289;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26377;&#20851;&#20844;&#24179;&#20998;&#31867;&#20013;&#26041;&#24046;&#65288;&#30456;&#23545;&#20110;&#33258;&#27965;&#24615;&#21644;&#38543;&#24847;&#24615;&#65289;&#20316;&#29992;&#30340;&#26368;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65307; 4&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#20351;&#32654;&#22269;&#20303;&#25151;&#25269;&#25276;&#36151;&#27454;&#25259;&#38706;&#27861;&#26696;&#65288;HMDA&#65289;&#25968;&#25454;&#38598;&#26131;&#20110;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#12290; &#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25581;&#31034;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#20196;&#20154;&#38663;&#24778;&#30340;&#35265;&#35299;&#12290;&#24403;&#32771;&#34385;&#21040;&#26041;&#24046;&#21644;&#38543;&#24847;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#22823;&#22810;&#25968;&#20844;&#24179;&#20998;&#31867;&#22522;&#20934;&#25509;&#36817;&#20844;&#24179;&#12290; &#20294;&#26159;&#65292;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#38543;&#24847;&#24615;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#24590;&#26679;&#30340;&#21442;&#25968;&#25928;&#29575;&#36924;&#36817;&#20989;&#25968;&#65292;&#21253;&#25324;$L_p(\Omega)$&#33539;&#25968;&#19979;&#30340;&#35823;&#24046;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q \leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26469;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2211.14400</link><description>&lt;p&gt;
&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#19978;&#65292;&#20851;&#20110;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20339;&#36924;&#36817;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#24590;&#26679;&#30340;&#21442;&#25968;&#25928;&#29575;&#36924;&#36817;&#20989;&#25968;&#65292;&#21253;&#25324;$L_p(\Omega)$&#33539;&#25968;&#19979;&#30340;&#35823;&#24046;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q \leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26469;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#31354;&#38388;$W^s(L_q(\Omega))$&#21644;Besov&#31354;&#38388;$B^s_r(L_q(\Omega))$&#20013;&#20197;$L_p(\Omega)$&#33539;&#25968;&#24230;&#37327;&#35823;&#24046;&#30340;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20110;&#22312;&#31185;&#23398;&#35745;&#31639;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#37325;&#35201;&#65292;&#22312;&#36807;&#21435;&#21482;&#26377;&#24403;$p=q=\infty$&#26102;&#25165;&#23436;&#20840;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q\leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#28176;&#36817;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#24037;&#20855;&#26159;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#31232;&#30095;&#21521;&#37327;&#30340;&#26368;&#20339;&#32534;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;$p&gt;q$&#30340;&#38750;&#32447;&#24615;&#21306;&#22495;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#30340;$L_p$&#36924;&#36817;&#19979;&#30028;&#25512;&#23548;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with error measured in the $L_p(\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been completely solved only when $p=q=\infty$. Our contribution is to provide a complete solution for all $1\leq p,q\leq \infty$ and $s &gt; 0$, including asymptotically matching upper and lower bounds. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p &gt; q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#32780;&#19981;&#20381;&#36182;&#20110;&#22870;&#21169;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2211.10851</link><description>&lt;p&gt;
&#22870;&#21169;&#24182;&#38750;&#24517;&#35201;&#65306;&#22914;&#20309;&#20026;&#32456;&#36523;&#23398;&#20064;&#21019;&#24314;&#19968;&#20010;&#32452;&#21512;&#24615;&#33258;&#25105;&#20445;&#25252;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10851
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#32780;&#19981;&#20381;&#36182;&#20110;&#22870;&#21169;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#35748;&#20026;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36991;&#20813;&#24809;&#32602;&#26159;&#35299;&#37322;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#29983;&#20013;&#65292;&#29983;&#29289;&#38656;&#35201;&#23398;&#20064;&#20851;&#20110;&#19990;&#30028;&#32467;&#26500;&#30340;&#35768;&#22810;&#19981;&#21516;&#26041;&#38754;&#65306;&#19990;&#30028;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#31227;&#21160;&#21147;&#23398;&#12290;&#38543;&#30528;&#26234;&#33021;&#20307;&#34701;&#20837;&#26032;&#30693;&#35782;&#65292;&#29366;&#24577;&#32452;&#21512;&#30340;&#25968;&#37327;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#19988;&#23545;&#20110;&#32473;&#23450;&#30340;&#29366;&#24577;&#32452;&#21512;&#65292;&#27809;&#26377;&#26126;&#26174;&#23450;&#20041;&#30340;&#39044;&#35774;&#22870;&#21169;&#25110;&#25104;&#26412;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#21152;&#26435;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#22312;&#19990;&#30028;&#20013;&#30340;&#32463;&#39564;&#20043;&#21069;&#23545;&#22909;&#30340;&#21644;&#22351;&#30340;&#32452;&#21512;&#36827;&#34892;&#32534;&#30721;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#20013;&#24320;&#21457;&#26356;&#33258;&#28982;&#30340;&#34892;&#20026;&#21644;&#21160;&#26426;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#65288;&#21363;&#36171;&#20104;&#33021;&#21147;&#65289;&#26159;&#21487;&#33021;&#30340;&#65292;&#35813;&#26631;&#20934;&#34913;&#37327;&#26234;&#33021;&#20307;&#22312;&#36716;&#31227;&#25805;&#20316;&#32773;&#19979;&#23454;&#29616;&#35768;&#22810;&#21487;&#33021;&#26410;&#26469;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36171;&#20104;&#33021;&#21147;&#25193;&#23637;&#21040;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning views the maximization of rewards and avoidance of punishments as central to explaining goal-directed behavior. However, over a life, organisms will need to learn about many different aspects of the world's structure: the states of the world and state-vector transition dynamics. The number of combinations of states grows exponentially as an agent incorporates new knowledge, and there is no obvious weighted combination of pre-existing rewards or costs defined for a given combination of states, as such a weighting would need to encode information about good and bad combinations prior to an agent's experience in the world. Therefore, we must develop more naturalistic accounts of behavior and motivation in large state-spaces. We show that it is possible to use only the intrinsic motivation metric of empowerment, which measures the agent's capacity to realize many possible futures under a transition operator. We propose to scale empowerment to hierarchical state-space
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#25277;&#35937;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#25277;&#35937;&#29366;&#24577;&#20250;&#24341;&#20837;&#26679;&#26412;&#30456;&#20851;&#24615;&#65292;&#32780;&#20351;&#29992;&#38789;&#19981;&#31561;&#24335;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#29616;&#26377;MBRL&#31639;&#27861;&#30340;&#20445;&#35777;&#25193;&#23637;&#21040;&#24102;&#26377;&#25277;&#35937;&#30340;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2208.14407</link><description>&lt;p&gt;
&#25277;&#35937;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Abstracted Model-Based Reinforcement Learning. (arXiv:2208.14407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#25277;&#35937;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#25277;&#35937;&#29366;&#24577;&#20250;&#24341;&#20837;&#26679;&#26412;&#30456;&#20851;&#24615;&#65292;&#32780;&#20351;&#29992;&#38789;&#19981;&#31561;&#24335;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#29616;&#26377;MBRL&#31639;&#27861;&#30340;&#20445;&#35777;&#25193;&#23637;&#21040;&#24102;&#26377;&#25277;&#35937;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26041;&#27861;&#37117;&#33021;&#22815;&#25552;&#20379;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#30340;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#29366;&#24577;&#25277;&#35937;&#25216;&#26415;&#21487;&#20197;&#22312;&#20445;&#25345;&#19982;&#21407;&#38382;&#39064;&#26377;&#30028;&#25439;&#22833;&#30340;&#21516;&#26102;&#20943;&#23569;MDP&#30340;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20004;&#31181;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#26102;&#65292;&#21363;MBRL&#20165;&#20165;&#35266;&#23519;&#25277;&#35937;&#29366;&#24577;&#26102;&#65292;&#21364;&#27809;&#26377;&#30456;&#24212;&#30340;&#20445;&#35777;&#21487;&#29992;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25277;&#35937;&#21487;&#20197;&#24341;&#20837;&#22312;&#32447;&#37319;&#38598;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65288;&#20363;&#22914;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#37319;&#38598;&#30340;&#26679;&#26412;&#65289;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#22914;&#26524;&#19981;&#32771;&#34385;&#36825;&#31181;&#30456;&#20851;&#24615;&#65292;MBRL&#30340;&#32467;&#26524;&#19981;&#33021;&#30452;&#25509;&#25512;&#24191;&#21040;&#36825;&#20010;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#38789;&#19981;&#31561;&#24335;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#32467;&#26524;&#20351;&#24471;&#23558;&#29616;&#26377;MBRL&#31639;&#27861;&#30340;&#20445;&#35777;&#25193;&#23637;&#21040;&#24102;&#26377;&#25277;&#35937;&#30340;&#35774;&#32622;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into account, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2207.04053</link><description>&lt;p&gt;
&#35770;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Need and Applicability of Causality for Fair Machine Learning. (arXiv:2207.04053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#25919;&#27835;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24120;&#35265;&#24212;&#29992;&#26696;&#20363;&#22806;&#65292;&#20107;&#23454;&#35777;&#26126;&#22240;&#26524;&#20851;&#31995;&#22312;&#35780;&#20272;&#33258;&#21160;&#20915;&#31574;&#30340;&#20844;&#27491;&#24615;&#26041;&#38754;&#21313;&#20998;&#37325;&#35201;&#65292;&#26080;&#35770;&#26159;&#22312;&#27861;&#24459;&#19978;&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#20026;&#20309;&#22240;&#26524;&#20851;&#31995;&#23545;&#20844;&#24179;&#24615;&#35780;&#20272;&#23588;&#20026;&#37325;&#35201;&#30340;&#35770;&#28857;&#21644;&#31034;&#20363;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#20197;&#21450;&#20381;&#36182;&#22240;&#26524;&#20027;&#24352;&#30340;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Besides its common use cases in epidemiology, political, and social sciences, causality turns out to be crucial in evaluating the fairness of automated decisions, both in a legal and everyday sense. We provide arguments and examples, of why causality is particularly important for fairness evaluation. In particular, we point out the social impact of non-causal predictions and the legal anti-discrimination process that relies on causal claims. We conclude with a discussion about the challenges and limitations of applying causality in practical scenarios as well as possible solutions.
&lt;/p&gt;</description></item></channel></rss>