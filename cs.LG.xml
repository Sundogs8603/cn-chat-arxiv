<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#25968;&#23398;&#38382;&#39064;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#40723;&#21169;&#31526;&#21495;&#21270;&#25512;&#29702;&#19982;&#25968;&#20540;&#31572;&#26696;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2308.01906</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#31526;&#21495;&#21270;&#25968;&#23398;&#38382;&#39064;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning in Large Language Models Through Symbolic Math Word Problems. (arXiv:2308.01906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#25968;&#23398;&#38382;&#39064;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#40723;&#21169;&#31526;&#21495;&#21270;&#25512;&#29702;&#19982;&#25968;&#20540;&#31572;&#26696;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#35299;&#20915;&#20960;&#20046;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#22810;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#25968;&#23398;&#38382;&#39064;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#22240;&#20026;&#31526;&#21495;&#34920;&#36798;&#26159;&#23545;&#25968;&#20540;&#31572;&#26696;&#30340;&#8220;&#31616;&#26126;&#35299;&#37322;&#8221;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20351;&#29992;&#20102;SVAMP&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#65292;&#24182;&#21457;&#29616;GPT-3&#30340;davinci-002&#27169;&#22411;&#22312;&#31526;&#21495;&#21270;&#25968;&#23398;&#38382;&#39064;&#19978;&#20063;&#20855;&#26377;&#33391;&#22909;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#19981;&#20165;&#32771;&#34385;&#20934;&#30830;&#29575;&#65292;&#36824;&#35780;&#20272;&#26368;&#32456;&#31572;&#26696;&#21644;&#25512;&#29702;&#32467;&#26524;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#25968;&#20540;&#21644;&#31526;&#21495;&#21270;&#31572;&#26696;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#40723;&#21169;&#31526;&#21495;&#21270;&#25512;&#29702;&#19982;&#25968;&#20540;&#31572;&#26696;&#20445;&#25345;&#19968;&#33268;&#65292;&#20174;&#32780;&#20351;LLM&#33021;&#22815;&#25552;&#20379;&#31616;&#26126;&#19988;&#21487;&#39564;&#35777;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21487;&#21464;&#24418;&#26680;&#21367;&#31215;&#20316;&#20026;&#19968;&#27425;&#36890;&#36807;&#30340;&#23436;&#21892;&#27169;&#22359;&#30340;&#26377;&#25928;&#26550;&#26500;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#21464;&#24418;&#21367;&#31215;&#22312;&#28145;&#24230;&#23436;&#25104;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01905</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#28145;&#24230;&#23436;&#25104;&#30340;&#21487;&#21464;&#24418;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Revisiting Deformable Convolution for Depth Completion. (arXiv:2308.01905v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21487;&#21464;&#24418;&#26680;&#21367;&#31215;&#20316;&#20026;&#19968;&#27425;&#36890;&#36807;&#30340;&#23436;&#21892;&#27169;&#22359;&#30340;&#26377;&#25928;&#26550;&#26500;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#21464;&#24418;&#21367;&#31215;&#22312;&#28145;&#24230;&#23436;&#25104;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23436;&#25104;&#26088;&#22312;&#20174;&#31232;&#30095;&#28145;&#24230;&#22270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23494;&#38598;&#28145;&#24230;&#22270;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;RGB&#22270;&#20687;&#20316;&#20026;&#24341;&#23548;&#65292;&#24182;&#24341;&#20837;&#36845;&#20195;&#30340;&#31354;&#38388;&#20256;&#25773;&#26469;&#23436;&#21892;&#20272;&#35745;&#30340;&#31895;&#31961;&#28145;&#24230;&#22270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#25773;&#23436;&#21892;&#26041;&#27861;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#65292;&#24182;&#19988;&#23384;&#22312;&#22266;&#23450;&#30340;&#24863;&#21463;&#37326;&#65292;&#21487;&#33021;&#21253;&#21547;&#19982;&#38750;&#24120;&#31232;&#30095;&#36755;&#20837;&#26080;&#20851;&#21644;&#26080;&#29992;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#24605;&#24819;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#21487;&#21464;&#24418;&#26680;&#21367;&#31215;&#20316;&#20026;&#19968;&#27425;&#36890;&#36807;&#30340;&#23436;&#21892;&#27169;&#22359;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#21151;&#33021;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#28145;&#24230;&#23436;&#25104;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21508;&#31181;&#20195;&#34920;&#24615;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19982;&#20197;&#24448;&#24037;&#20316;&#19981;&#21516;&#65292;&#21487;&#21464;&#24418;&#21367;&#31215;&#22312;&#28145;&#24230;&#23436;&#25104;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convol
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#22312;arXiv&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#37327;&#21270;&#20102;&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#26368;&#32456;&#34987;&#21360;&#21047;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#26399;&#21002;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#21360;&#26412;&#21644;&#26368;&#32456;&#21457;&#34920;&#29256;&#26412;&#30340;&#23545;&#24212;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#26144;&#23556;&#26041;&#27861;&#20351;&#29992;BERT&#12290;</title><link>http://arxiv.org/abs/2308.01899</link><description>&lt;p&gt;
&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#23454;&#38469;&#19978;&#34987;&#21360;&#21047;&#20102;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#65306;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#22312;arXiv&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How many preprints have actually been printed and why: a case study of computer science preprints on arXiv. (arXiv:2308.01899v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#22312;arXiv&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#37327;&#21270;&#20102;&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#26368;&#32456;&#34987;&#21360;&#21047;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#26399;&#21002;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#21360;&#26412;&#21644;&#26368;&#32456;&#21457;&#34920;&#29256;&#26412;&#30340;&#23545;&#24212;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#26144;&#23556;&#26041;&#27861;&#20351;&#29992;BERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#21360;&#26412;&#22312;&#23398;&#26415;&#30028;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#27491;&#24335;&#25552;&#20132;&#21040;&#26399;&#21002;&#25110;&#20250;&#35758;&#20043;&#21069;&#23558;&#20182;&#20204;&#30340;&#25163;&#31295;&#21457;&#24067;&#21040;&#39044;&#21360;&#26412;&#26381;&#21153;&#22120;&#19978;&#30340;&#21407;&#22240;&#26377;&#24456;&#22810;&#65292;&#20294;&#39044;&#21360;&#26412;&#30340;&#20351;&#29992;&#20063;&#24341;&#21457;&#20102;&#19981;&#23569;&#20105;&#35758;&#65292;&#23588;&#20854;&#26159;&#19982;&#20248;&#20808;&#26435;&#30340;&#22768;&#26126;&#26377;&#20851;&#12290;&#26412;&#25991;&#23545;2008&#24180;&#33267;2017&#24180;&#26399;&#38388;&#25552;&#20132;&#21040;arXiv&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#37327;&#21270;&#26368;&#32456;&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#22330;&#21512;&#20013;&#34987;&#21360;&#21047;&#12290;&#22312;&#36825;&#20123;&#24050;&#21457;&#34920;&#30340;&#25163;&#31295;&#20013;&#65292;&#26377;&#20123;&#20197;&#19981;&#21516;&#30340;&#26631;&#39064;&#21457;&#34920;&#65292;&#19988;&#26410;&#26356;&#26032;arXiv&#19978;&#30340;&#39044;&#21360;&#26412;&#12290;&#23545;&#20110;&#36825;&#20123;&#25163;&#31295;&#65292;&#20256;&#32479;&#30340;&#27169;&#31946;&#21305;&#37197;&#26041;&#27861;&#26080;&#27861;&#23558;&#39044;&#21360;&#26412;&#19982;&#26368;&#32456;&#21457;&#34920;&#29256;&#26412;&#23545;&#24212;&#36215;&#26469;&#12290;&#37492;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#26144;&#23556;&#26041;&#27861;&#65292;&#21033;&#29992;Transformers&#20013;&#30340;Bidirectional Encoder Representations (BERT)&#12290;&#21033;&#29992;&#36825;&#31181;&#26032;&#30340;&#26144;&#23556;&#26041;&#27861;&#21644;&#22810;&#31181;&#25968;&#25454;&#26469;&#28304;...
&lt;/p&gt;
&lt;p&gt;
Preprints play an increasingly critical role in academic communities. There are many reasons driving researchers to post their manuscripts to preprint servers before formal submission to journals or conferences, but the use of preprints has also sparked considerable controversy, especially surrounding the claim of priority. In this paper, a case study of computer science preprints submitted to arXiv from 2008 to 2017 is conducted to quantify how many preprints have eventually been printed in peer-reviewed venues. Among those published manuscripts, some are published under different titles and without an update to their preprints on arXiv. In the case of these manuscripts, the traditional fuzzy matching method is incapable of mapping the preprint to the final published version. In view of this issue, we introduce a semantics-based mapping method with the employment of Bidirectional Encoder Representations from Transformers (BERT). With this new mapping method and a plurality of data sou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#21644;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33988;&#27700;&#27744;&#25277;&#26679;&#21644;&#20854;&#20182;&#26367;&#20195;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#23547;&#25214;&#26368;&#20339;&#23384;&#20648;&#26679;&#26412;&#25968;&#37327;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.01895</link><description>&lt;p&gt;
&#25913;&#36827;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#21644;&#23384;&#20648;&#20197;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning. (arXiv:2308.01895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#21644;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33988;&#27700;&#27744;&#25277;&#26679;&#21644;&#20854;&#20182;&#26367;&#20195;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#23547;&#25214;&#26368;&#20339;&#23384;&#20648;&#26679;&#26412;&#25968;&#37327;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#20351;&#28145;&#24230;&#23398;&#20064;&#32773;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#26410;&#30693;&#38271;&#24230;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#20250;&#36951;&#24536;&#20197;&#21069;&#30340;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22238;&#25918;&#65292;&#21363;&#23558;&#23569;&#37327;&#20808;&#21069;&#30340;&#32463;&#39564;&#23384;&#20648;&#22312;&#20869;&#23384;&#20013;&#65292;&#24182;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#26102;&#37325;&#26032;&#25773;&#25918;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#22312;&#36873;&#25321;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#23384;&#20648;&#20197;&#21450;&#30830;&#23450;&#25152;&#38656;&#23384;&#20648;&#26679;&#26412;&#30340;&#26368;&#20339;&#25968;&#37327;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#24120;&#29992;&#30340;&#33988;&#27700;&#27744;&#25277;&#26679;&#19982;&#21508;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#26368;&#20339;&#30340;&#23384;&#20648;&#26679;&#26412;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning seeks to enable deep learners to train on a series of tasks of unknown length without suffering from the catastrophic forgetting of previous tasks. One effective solution is replay, which involves storing few previous experiences in memory and replaying them when learning the current task. However, there is still room for improvement when it comes to selecting the most informative samples for storage and determining the optimal number of samples to be stored. This study aims to address these issues with a novel comparison of the commonly used reservoir sampling to various alternative population strategies and providing a novel detailed analysis of how to find the optimal number of stored samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20462;&#25972;&#30340;Lasso&#65288;TRIM&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#12289;&#26377;&#38480;&#25968;&#25454;&#21644;&#22810;&#37325;&#20849;&#32447;&#24615;&#31561;&#38382;&#39064;&#30340;&#31283;&#20581;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01891</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#25972;&#30340;Lasso&#31934;&#30830;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Exact identification of nonlinear dynamical systems by Trimmed Lasso. (arXiv:2308.01891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20462;&#25972;&#30340;Lasso&#65288;TRIM&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#12289;&#26377;&#38480;&#25968;&#25454;&#21644;&#22810;&#37325;&#20849;&#32447;&#24615;&#31561;&#38382;&#39064;&#30340;&#31283;&#20581;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#35782;&#21035;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#36807;&#39034;&#24207;&#38408;&#20540;&#26368;&#23567;&#20108;&#20056;&#65288;STLS&#65289;&#31639;&#27861;&#36827;&#34892;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31232;&#30095;&#35782;&#21035;&#65288;SINDy&#65289;&#12290;&#35768;&#22810;SINDy&#30340;&#25193;&#23637;&#22312;&#25991;&#29486;&#20013;&#20986;&#29616;&#65292;&#20197;&#22788;&#29702;&#38271;&#24230;&#26377;&#38480;&#19988;&#22122;&#38899;&#36739;&#22823;&#30340;&#23454;&#39564;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#38598;&#25104;&#24341;&#23548;SINDy&#27169;&#22411;&#65288;E-SINDy&#65289;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#35782;&#21035;&#65292;&#22788;&#29702;&#26377;&#38480;&#19988;&#22122;&#38899;&#36739;&#22823;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;SINDy&#30340;&#25193;&#23637;&#24456;&#22810;&#65292;&#20294;&#23427;&#20204;&#20419;&#36827;&#31232;&#30095;&#20272;&#35745;&#30340;&#20272;&#35745;&#22120;&#26377;&#26102;&#20250;&#25552;&#20379;&#21160;&#24577;&#30340;&#31232;&#30095;&#36817;&#20284;&#65292;&#32780;&#19981;&#26159;&#31934;&#30830;&#24674;&#22797;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#22810;&#37325;&#20849;&#32447;&#24615;&#19979;&#23384;&#22312;&#38382;&#39064;&#65292;&#20363;&#22914;Lasso&#30340;&#19981;&#21487;&#34920;&#31034;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20462;&#21098;&#30340;Lasso&#29992;&#20110;&#23545;&#27169;&#22411;&#36827;&#34892;&#31283;&#20581;&#35782;&#21035;&#65288;TRIM&#65289;&#21487;&#20197;&#22312;&#36739;&#20005;&#37325;&#30340;&#22122;&#22768;&#12289;&#26377;&#38480;&#25968;&#25454;&#21644;&#22810;&#37325;&#20849;&#32447;&#24615;&#19979;&#25552;&#20379;&#31934;&#30830;&#24674;&#22797;&#65292;&#32780;&#19981;&#21516;&#20110;E-SINDy&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification of nonlinear dynamical systems has been popularized by sparse identification of the nonlinear dynamics (SINDy) via the sequentially thresholded least squares (STLS) algorithm. Many extensions SINDy have emerged in the literature to deal with experimental data which are finite in length and noisy. Recently, the computationally intensive method of ensembling bootstrapped SINDy models (E-SINDy) was proposed for model identification, handling finite, highly noisy data. While the extensions of SINDy are numerous, their sparsity-promoting estimators occasionally provide sparse approximations of the dynamics as opposed to exact recovery. Furthermore, these estimators suffer under multicollinearity, e.g. the irrepresentable condition for the Lasso. In this paper, we demonstrate that the Trimmed Lasso for robust identification of models (TRIM) can provide exact recovery under more severe noise, finite data, and multicollinearity as opposed to E-SINDy. Additionally, the computatio
&lt;/p&gt;</description></item><item><title>DualCoOp++&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#27880;&#37322;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#26631;&#31614;&#35782;&#21035;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#65292;&#37319;&#29992;Evidence-guided Dual Context Optimization&#26694;&#26550;&#26469;&#35299;&#20915;&#37096;&#20998;&#26631;&#31614;&#21644;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01890</link><description>&lt;p&gt;
DualCoOp++: &#38024;&#23545;&#26377;&#38480;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#35782;&#21035;&#36827;&#34892;&#24555;&#36895;&#26377;&#25928;&#30340;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations. (arXiv:2308.01890v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01890
&lt;/p&gt;
&lt;p&gt;
DualCoOp++&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#27880;&#37322;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#26631;&#31614;&#35782;&#21035;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#65292;&#37319;&#29992;Evidence-guided Dual Context Optimization&#26694;&#26550;&#26469;&#35299;&#20915;&#37096;&#20998;&#26631;&#31614;&#21644;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#26631;&#31614;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#26631;&#31614;&#22270;&#20687;&#35782;&#21035;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#21644;&#23454;&#38469;&#24847;&#20041;&#30340;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23398;&#20064;&#25991;&#26412;&#21644;&#35270;&#35273;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20197;&#24357;&#34917;&#22270;&#20687;&#26631;&#31614;&#26377;&#38480;&#25152;&#24102;&#26469;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#22810;&#26631;&#31614;&#27880;&#37322;&#31232;&#32570;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29992;&#25968;&#30334;&#19975;&#20010;&#36741;&#21161;&#22270;&#20687;-&#25991;&#26412;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#20043;&#38388;&#30340;&#24378;&#22823;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Evidence-guided Dual Context Optimization (DualCoOp++)&#65292;&#23427;&#20316;&#20026;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37096;&#20998;&#26631;&#31614;&#21644;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#22312;DualCoOp++&#20013;&#65292;&#25105;&#20204;&#21333;&#29420;&#23545;&#30446;&#26631;&#31867;&#21035;&#30340;&#35777;&#25454;&#12289;&#27491;&#38754;&#21644;&#36127;&#38754;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#20316;&#20026;&#35821;&#35328;&#36755;&#20837;&#65288;&#21363;&#25552;&#31034;&#65289;&#30340;&#21442;&#25968;&#32452;&#20214;&#12290;&#35777;&#25454;&#19978;&#19979;&#25991;&#26088;&#22312;&#21457;&#29616;&#19982;&#30446;&#26631;&#31867;&#21035;&#30456;&#20851;&#30340;&#25152;&#26377;&#35270;&#35273;&#20869;&#23481;&#65292;&#24182;&#20316;&#20026;&#32858;&#21512;&#27491;&#38754;&#21644;&#36127;&#38754;&#19978;&#19979;&#25991;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label image recognition in the low-label regime is a task of great challenge and practical significance. Previous works have focused on learning the alignment between textual and visual spaces to compensate for limited image labels, yet may suffer from reduced accuracy due to the scarcity of high-quality multi-label annotations. In this research, we leverage the powerful alignment between textual and visual features pretrained with millions of auxiliary image-text pairs. We introduce an efficient and effective framework called Evidence-guided Dual Context Optimization (DualCoOp++), which serves as a unified approach for addressing partial-label and zero-shot multi-label recognition. In DualCoOp++ we separately encode evidential, positive, and negative contexts for target classes as parametric components of the linguistic input (i.e., prompts). The evidential context aims to discover all the related visual content for the target class, and serves as guidance to aggregate positive 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#27668;&#20505;&#27169;&#24335;&#38477;&#23610;&#24230;&#20013;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#29289;&#29702;&#32422;&#26463;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#28201;&#24230;&#30340;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#20837;&#22810;&#21464;&#37327;&#30828;&#32422;&#26463;&#30340;&#26694;&#26550;&#65292;&#30830;&#20445;&#20102;&#38477;&#23610;&#24230;&#30340;&#27668;&#20505;&#21464;&#37327;&#32676;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.01868</link><description>&lt;p&gt;
&#27668;&#20505;&#27169;&#24335;&#38477;&#23610;&#24230;&#30340;&#22810;&#21464;&#37327;&#22256;&#38590;&#29289;&#29702;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Multi-variable Hard Physical Constraints for Climate Model Downscaling. (arXiv:2308.01868v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01868
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#27668;&#20505;&#27169;&#24335;&#38477;&#23610;&#24230;&#20013;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#29289;&#29702;&#32422;&#26463;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#28201;&#24230;&#30340;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#20837;&#22810;&#21464;&#37327;&#30828;&#32422;&#26463;&#30340;&#26694;&#26550;&#65292;&#30830;&#20445;&#20102;&#38477;&#23610;&#24230;&#30340;&#27668;&#20505;&#21464;&#37327;&#32676;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#27668;&#20505;&#27169;&#22411;&#65288;GCM&#65289;&#26159;&#27169;&#25311;&#27668;&#20505;&#28436;&#21464;&#21644;&#35780;&#20272;&#27668;&#20505;&#21464;&#21270;&#24433;&#21709;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#20197;&#31895;&#31961;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#36816;&#34892;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20877;&#29616;&#23616;&#37096;&#23610;&#24230;&#29616;&#35937;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#38477;&#23610;&#24230;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31895;&#31961;&#21464;&#37327;&#20013;&#36817;&#20284;&#23616;&#37096;&#23610;&#24230;&#27668;&#20505;&#22330;&#65292;&#20174;&#32780;&#23454;&#29616;&#21306;&#22495;GCM&#39044;&#27979;&#12290;&#36890;&#24120;&#65292;&#19981;&#21516;&#24863;&#20852;&#36259;&#30340;&#27668;&#20505;&#22330;&#34987;&#29420;&#31435;&#38477;&#23610;&#24230;&#65292;&#23548;&#33268;&#20114;&#36830;&#21464;&#37327;&#20043;&#38388;&#22522;&#26412;&#29289;&#29702;&#29305;&#24615;&#30340;&#36829;&#21453;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#23545;&#28201;&#24230;&#30340;&#24212;&#29992;&#65292;&#20026;&#24341;&#20837;&#22810;&#21464;&#37327;&#30828;&#32422;&#26463;&#30340;&#26694;&#26550;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#20197;&#30830;&#20445;&#38477;&#23610;&#24230;&#30340;&#27668;&#20505;&#21464;&#37327;&#32676;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global Climate Models (GCMs) are the primary tool to simulate climate evolution and assess the impacts of climate change. However, they often operate at a coarse spatial resolution that limits their accuracy in reproducing local-scale phenomena. Statistical downscaling methods leveraging deep learning offer a solution to this problem by approximating local-scale climate fields from coarse variables, thus enabling regional GCM projections. Typically, climate fields of different variables of interest are downscaled independently, resulting in violations of fundamental physical properties across interconnected variables. This study investigates the scope of this problem and, through an application on temperature, lays the foundation for a framework introducing multi-variable hard constraints that guarantees physical relationships between groups of downscaled climate variables.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MRQ&#30340;&#27169;&#22411;&#37325;&#26032;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#37327;&#21270;&#27169;&#22411;&#24555;&#36895;&#36716;&#25442;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#37327;&#21270;&#35201;&#27714;&#65292;&#35299;&#20915;&#20102;&#22312;&#22266;&#23450;&#28857;&#30828;&#20214;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.01867</link><description>&lt;p&gt;
MRQ:&#36890;&#36807;&#27169;&#22411;&#37325;&#26032;&#37327;&#21270;&#25903;&#25345;&#22810;&#31181;&#37327;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MRQ&#30340;&#27169;&#22411;&#37325;&#26032;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#37327;&#21270;&#27169;&#22411;&#24555;&#36895;&#36716;&#25442;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#37327;&#21270;&#35201;&#27714;&#65292;&#35299;&#20915;&#20102;&#22312;&#22266;&#23450;&#28857;&#30828;&#20214;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21508;&#31181;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;&#22914;NPU&#65292;TPU&#65292;DPU&#65289;&#30340;&#26222;&#21450;&#65292;&#20294;&#22312;&#22266;&#23450;&#28857;&#30828;&#20214;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#27169;&#22411;&#37327;&#21270;&#21644;&#36716;&#25442;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#37327;&#21270;&#26694;&#26550;&#65288;&#22914;Tensorflow QAT&#65292;TFLite PTQ&#21644;Qualcomm AIMET&#65289;&#21482;&#25903;&#25345;&#26377;&#38480;&#30340;&#37327;&#21270;&#26041;&#26696;&#65288;&#22914;&#20165;&#22312;TF1.x QAT&#20013;&#30340;&#38750;&#23545;&#31216;&#27599;&#24352;&#37327;&#37327;&#21270;&#65289;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#31245;&#24494;&#19981;&#21516;&#30340;&#37327;&#21270;&#35201;&#27714;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#33021;&#36731;&#26494;&#22320;&#20026;&#21508;&#31181;&#22266;&#23450;&#28857;&#30828;&#20214;&#36827;&#34892;&#37327;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27169;&#22411;&#37327;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;MRQ&#65288;&#27169;&#22411;&#37325;&#26032;&#37327;&#21270;&#65289;&#65292;&#23427;&#21487;&#20197;&#37319;&#29992;&#29616;&#26377;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#24182;&#24555;&#36895;&#23558;&#27169;&#22411;&#36716;&#25442;&#20026;&#28385;&#36275;&#19981;&#21516;&#37327;&#21270;&#35201;&#27714;&#65288;&#22914;&#38750;&#23545;&#31216;-&gt;&#23545;&#31216;&#65292;&#38750;2&#30340;&#24130;&#27425;-&gt;2&#30340;&#24130;&#27425;&#65289;&#12290;&#37325;&#26032;&#37327;&#21270;&#27604;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#37327;&#21270;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#22240;&#20026;&#23427;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU, DPU), deploying deep learning models on edge devices with fixed-point hardware is still challenging due to complex model quantization and conversion. Existing model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g., only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep learning models cannot be easily quantized for diverse fixed-point hardwares, mainly due to slightly different quantization requirements. In this paper, we envision a new type of model quantization approach called MRQ (model re-quantization), which takes existing quantized models and quickly transforms the models to meet different quantization requirements (e.g., asymmetric -&gt; symmetric, non-power-of-2 scale -&gt; power-of-2 scale). Re-quantization is much simpler than quantizing from scratch because it avoids costly re-training and
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32479;&#35745;&#20272;&#35745;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;Wasserstein&#20998;&#24067;&#20559;&#31227;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#20998;&#24067;&#20559;&#31227;&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#32479;&#35745;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#35770;&#25991;&#21457;&#29616;&#20102;&#26368;&#20248;&#30340;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#21644;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#65292;&#24182;&#35777;&#26126;&#20102;&#26679;&#26412;&#22343;&#20540;&#21644;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01853</link><description>&lt;p&gt;
&#32479;&#35745;&#20272;&#35745;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;: Wasserstein&#25200;&#21160;&#19982;&#26497;&#23567;&#26497;&#22823;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01853
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32479;&#35745;&#20272;&#35745;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;Wasserstein&#20998;&#24067;&#20559;&#31227;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#20998;&#24067;&#20559;&#31227;&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#32479;&#35745;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#35770;&#25991;&#21457;&#29616;&#20102;&#26368;&#20248;&#30340;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#21644;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#65292;&#24182;&#35777;&#26126;&#20102;&#26679;&#26412;&#22343;&#20540;&#21644;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#26159;&#29616;&#20195;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#23558;&#25968;&#25454;&#30340;&#29305;&#24615;&#20174;&#30495;&#23454;&#24773;&#20917;&#20013;&#31995;&#32479;&#22320;&#25913;&#21464;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;Wasserstein&#20998;&#24067;&#20559;&#31227;&#65292;&#20854;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#21487;&#33021;&#20250;&#21457;&#29983;&#36731;&#24494;&#25200;&#21160;&#65292;&#32780;&#19981;&#26159;Huber&#27745;&#26579;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#37096;&#20998;&#35266;&#27979;&#20540;&#26159;&#24322;&#24120;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36229;&#20986;&#29420;&#31435;&#25200;&#21160;&#30340;&#20559;&#31227;&#65292;&#25506;&#32034;&#20102;&#32852;&#21512;&#20998;&#24067;&#20559;&#31227;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#28857;&#30340;&#25200;&#21160;&#21487;&#20197;&#21327;&#35843;&#36827;&#34892;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#20010;&#37325;&#35201;&#30340;&#32479;&#35745;&#38382;&#39064;&#65292;&#21253;&#25324;&#20301;&#32622;&#20272;&#35745;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#22343;&#20540;&#20272;&#35745;&#21644;&#32447;&#24615;&#22238;&#24402;&#30340;&#39044;&#27979;&#35823;&#24046;&#26041;&#24046;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#31934;&#30830;&#30340;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#12289;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#65292;&#24182;&#35777;&#26126;&#20102;&#26679;&#26412;&#22343;&#20540;&#21644;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#20998;&#21035;&#26159;&#26368;&#20248;&#30340;&#12290;&#36825;&#36866;&#29992;&#20110;&#29420;&#31435;&#21644;&#32852;&#21512;&#20559;&#31227;&#65292;&#20294;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#21644;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#26159;&#19981;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#20837;&#20405;&#21644;&#35821;&#27861;&#20998;&#26512;&#24341;&#23548;&#65292;&#36880;&#27493;&#36866;&#24212;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#24182;&#22312;MultiWoZ&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.01849</link><description>&lt;p&gt;
&#21477;&#23376;&#32534;&#30721;&#20219;&#21153;&#30340;&#35838;&#31243;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curricular Transfer Learning for Sentence Encoded Tasks. (arXiv:2308.01849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#20837;&#20405;&#21644;&#35821;&#27861;&#20998;&#26512;&#24341;&#23548;&#65292;&#36880;&#27493;&#36866;&#24212;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#24182;&#22312;MultiWoZ&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#35768;&#22810;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#20998;&#24067;&#28418;&#31227;&#26102;&#65292;&#20363;&#22914;&#65292;&#23545;&#35805;&#29615;&#22659;&#19979;&#65292;&#36825;&#20123;&#25910;&#30410;&#24448;&#24448;&#20250;&#20943;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#8220;&#25968;&#25454;&#20837;&#20405;&#8221;&#21644;&#35821;&#27861;&#20998;&#26512;&#24341;&#23548;&#30340;&#39044;&#35757;&#32451;&#27493;&#39588;&#24207;&#21015;&#65288;&#35838;&#31243;&#65289;&#65292;&#20801;&#35768;&#22312;&#39044;&#35757;&#32451;&#20998;&#24067;&#20043;&#38388;&#36827;&#19968;&#27493;&#36880;&#28176;&#36866;&#24212;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19982;&#20854;&#20182;&#24050;&#30693;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MultiWoZ&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning language models in a downstream task is the standard approach for many state-of-the-art methodologies in the field of NLP. However, when the distribution between the source task and target task drifts, \textit{e.g.}, conversational environments, these gains tend to be diminished. This article proposes a sequence of pre-training steps (a curriculum) guided by "data hacking" and grammar analysis that allows further gradual adaptation between pre-training distributions. In our experiments, we acquire a considerable improvement from our method compared to other known pre-training approaches for the MultiWoZ task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;URET&#30340;&#36890;&#29992;&#40065;&#26834;&#24615;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#35813;&#24037;&#20855;&#21253;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#31867;&#22411;&#21644;&#20219;&#21153;&#39046;&#22495;&#19979;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#20197;&#30830;&#20445;&#20851;&#38190;AI&#20219;&#21153;&#30340;&#23433;&#20840;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01840</link><description>&lt;p&gt;
URET: &#36890;&#29992;&#40065;&#26834;&#24615;&#35780;&#20272;&#24037;&#20855;&#21253;&#65288;&#29992;&#20110;&#36867;&#36991;&#25915;&#20987;&#65289;
&lt;/p&gt;
&lt;p&gt;
URET: Universal Robustness Evaluation Toolkit (for Evasion). (arXiv:2308.01840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;URET&#30340;&#36890;&#29992;&#40065;&#26834;&#24615;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#35813;&#24037;&#20855;&#21253;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#31867;&#22411;&#21644;&#20219;&#21153;&#39046;&#22495;&#19979;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#20197;&#30830;&#20445;&#20851;&#38190;AI&#20219;&#21153;&#30340;&#23433;&#20840;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36867;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#25152;&#31034;&#12290;&#20805;&#20998;&#20102;&#35299;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#30830;&#20445;&#20851;&#38190;AI&#20219;&#21153;&#30340;&#23433;&#20840;&#21644;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36867;&#36991;&#25915;&#20987;&#24456;&#38590;&#23545;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#36827;&#34892;&#37096;&#32626;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#38598;&#20013;&#22312;&#22270;&#20687;&#39046;&#22495;&#24182;&#20855;&#26377;&#23569;&#25968;&#32422;&#26463;&#12290;&#19982;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#20854;&#20182;&#36755;&#20837;&#31867;&#22411;&#19981;&#21516;&#65292;&#22270;&#20687;&#30001;&#22343;&#21248;&#30340;&#12289;&#25968;&#20540;&#30340;&#12289;&#36830;&#32493;&#30340;&#21644;&#29420;&#31435;&#30340;&#29305;&#24449;&#32452;&#25104;&#12290;&#27492;&#22806;&#65292;&#26576;&#20123;&#36755;&#20837;&#31867;&#22411;&#21253;&#21547;&#39069;&#22806;&#30340;&#35821;&#20041;&#21644;&#21151;&#33021;&#32422;&#26463;&#65292;&#24517;&#39035;&#36981;&#23432;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#36755;&#20837;&#31867;&#22411;&#21644;&#20219;&#21153;&#39046;&#22495;&#26080;&#20851;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#21644;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#36755;&#20837;&#36716;&#25442;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#21457;&#29616;&#19968;&#31995;&#21015;&#36716;&#25442;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#31526;&#21512;&#35821;&#20041;&#21644;&#21151;&#33021;&#35201;&#27714;&#30340;&#27491;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25512;&#26029;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#22238;&#24402;&#20989;&#25968;&#65292;&#36890;&#36807;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#30456;&#20851;&#31639;&#27861;&#32463;&#36807;&#39564;&#35777;&#20855;&#26377;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01835</link><description>&lt;p&gt;
&#20998;&#24067;&#26080;&#20851;&#25512;&#26029;&#20108;&#20803;&#20998;&#31867;&#30340;&#22238;&#24402;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Inference for the Regression Function of Binary Classification. (arXiv:2308.01835v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25512;&#26029;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#22238;&#24402;&#20989;&#25968;&#65292;&#36890;&#36807;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#30456;&#20851;&#31639;&#27861;&#32463;&#36807;&#39564;&#35777;&#20855;&#26377;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20803;&#20998;&#31867;&#30340;&#19968;&#20010;&#20851;&#38190;&#23545;&#35937;&#26159;&#22238;&#24402;&#20989;&#25968;&#65292;&#21363;&#32473;&#23450;&#36755;&#20837;&#30340;&#31867;&#21035;&#26631;&#31614;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;&#36890;&#36807;&#22238;&#24402;&#20989;&#25968;&#65292;&#19981;&#20165;&#21487;&#20197;&#23450;&#20041;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#65292;&#36824;&#21487;&#20197;&#32534;&#30721;&#23545;&#24212;&#30340;&#38169;&#35823;&#20998;&#31867;&#27010;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#37319;&#26679;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#31934;&#30830;&#12289;&#20998;&#24067;&#26080;&#20851;&#19988;&#38750;&#28176;&#36817;&#20445;&#35777;&#30340;&#30495;&#23454;&#22238;&#24402;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#26681;&#25454;&#29992;&#25143;&#36873;&#25321;&#30340;&#32622;&#20449;&#27700;&#24179;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#29305;&#23450;&#30340;&#31639;&#27861;&#26469;&#28436;&#31034;&#35813;&#26694;&#26550;&#12290;&#35777;&#26126;&#20102;&#26500;&#24314;&#30340;&#32622;&#20449;&#21306;&#38388;&#26159;&#24378;&#19968;&#33268;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#20219;&#20309;&#38169;&#35823;&#30340;&#27169;&#22411;&#26368;&#32456;&#34987;&#25490;&#38500;&#30340;&#27010;&#29575;&#20026;1&#12290;&#25490;&#38500;&#30340;&#31243;&#24230;&#20063;&#36890;&#36807;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#31867;&#22411;&#30340;&#30028;&#38480;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#65292;&#24182;&#23558;&#26041;&#27861;&#19982;&#36817;&#20284;&#28176;&#36817;&#32622;&#20449;&#26925;&#22278;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key objects of binary classification is the regression function, i.e., the conditional expectation of the class labels given the inputs. With the regression function not only a Bayes optimal classifier can be defined, but it also encodes the corresponding misclassification probabilities. The paper presents a resampling framework to construct exact, distribution-free and non-asymptotically guaranteed confidence regions for the true regression function for any user-chosen confidence level. Then, specific algorithms are suggested to demonstrate the framework. It is proved that the constructed confidence regions are strongly consistent, that is, any false model is excluded in the long run with probability one. The exclusion is quantified with probably approximately correct type bounds, as well. Finally, the algorithms are validated via numerical experiments, and the methods are compared to approximate asymptotic confidence ellipsoids.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24739;&#32773;&#37319;&#35775;&#21644;&#20020;&#24202;&#25551;&#36848;&#39044;&#27979;&#31934;&#31070;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#35780;&#20998;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#19982;&#20154;&#31867;&#20020;&#24202;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#30456;&#36817;&#65292;&#25581;&#31034;&#20102;&#36890;&#29992;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01834</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Capability of Large Language Models to Measure Psychiatric Functioning. (arXiv:2308.01834v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24739;&#32773;&#37319;&#35775;&#21644;&#20020;&#24202;&#25551;&#36848;&#39044;&#27979;&#31934;&#31070;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#35780;&#20998;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#19982;&#20154;&#31867;&#20020;&#24202;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#30456;&#36817;&#65292;&#25581;&#31034;&#20102;&#36890;&#29992;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#21307;&#23398;&#30693;&#35782;&#65288;Med-PaLM 2&#65289;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27809;&#26377;&#32463;&#36807;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24739;&#32773;&#37319;&#35775;&#21644;&#20020;&#24202;&#25551;&#36848;&#26469;&#39044;&#27979;&#31934;&#31070;&#21151;&#33021;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#26469;&#25552;&#21462;&#20272;&#35745;&#30340;&#20020;&#24202;&#35780;&#20998;&#21644;&#35786;&#26029;&#65292;&#20998;&#26512;&#20102;145&#20363;&#25233;&#37057;&#30151;&#21644;115&#20363;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#35780;&#20272;&#20197;&#21450;46&#20363;&#20020;&#24202;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Med-PaLM 2&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#31934;&#31070;&#30142;&#30149;&#20013;&#35780;&#20272;&#31934;&#31070;&#21151;&#33021;&#65292;&#20854;&#20013;&#23545;&#22522;&#20110;&#26631;&#20934;&#35780;&#20272;&#30340;&#25233;&#37057;&#30151;&#35780;&#20998;&#30340;&#39044;&#27979;&#34920;&#29616;&#26368;&#20339;&#65288;&#20934;&#30830;&#29575;&#33539;&#22260;= 0.80-0.84&#65289;&#65292;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#22312;&#32479;&#35745;&#19978;&#19982;&#20154;&#31867;&#20020;&#24202;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#26080;&#27861;&#21306;&#20998;&#65288;t(1,144) = 1.20&#65292;p = 0.23&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#36890;&#29992;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current work investigates the capability of Large language models (LLMs) that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so. To assess this, n = 145 depression and n =115 PTSD assessments and n = 46 clinical case studies across high prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma and stress, Addictive disorders) were analyzed using prompts to extract estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which were statistically indistinguishable from human clinical raters t(1,144) = 1.20; p = 0.23. Results show the potential for general clinical language models to f
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#19981;&#20844;&#24179;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#22256;&#38590;&#23545;&#25239;&#26679;&#26412;&#25366;&#25496;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01823</link><description>&lt;p&gt;
&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#22256;&#38590;&#23545;&#25239;&#26679;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Hard Adversarial Example Mining for Improving Robust Fairness. (arXiv:2308.01823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#19981;&#20844;&#24179;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#22256;&#38590;&#23545;&#25239;&#26679;&#26412;&#25366;&#25496;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AE&#65289;&#40065;&#26834;&#24615;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#35266;&#23519;&#21457;&#29616;&#65292;&#36825;&#20010;&#38480;&#21046;&#21487;&#33021;&#26159;&#30001;&#20110;&#20005;&#37325;&#30340;&#23545;&#25239;&#32622;&#20449;&#36807;&#25311;&#21512;&#65292;&#21363;&#26576;&#20123;&#20855;&#26377;&#36807;&#24230;&#33258;&#20449;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HAM&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#22256;&#38590;&#23545;&#25239;&#26679;&#26412;&#25366;&#25496;&#12290;HAM&#38598;&#20013;&#20110;&#20197;&#36866;&#24212;&#24615;&#30340;&#26041;&#24335;&#25366;&#25496;&#22256;&#38590;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#21516;&#26102;&#20002;&#24323;&#23481;&#26131;&#30340;&#26679;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;HAM&#26681;&#25454;&#35745;&#31639;&#25439;&#22833;&#20540;&#26102;&#38656;&#35201;&#31359;&#36807;&#20915;&#31574;&#36793;&#30028;&#30340;&#27493;&#38271;&#26469;&#35782;&#21035;&#22256;&#38590;&#30340;AE&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#26089;&#26399;&#20002;&#24323;&#26426;&#21046;&#26469;&#22312;AE&#29983;&#25104;&#30340;&#21021;&#26399;&#20002;&#24323;&#23481;&#26131;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#20351;&#24471;&#32593;&#32476;&#26356;&#21152;&#40065;&#26834;&#21644;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AE). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems, restricting their applicability. In this paper, we empirically observe that this limitation may be attributed to serious adversarial confidence overfitting, i.e., certain adversarial examples with overconfidence. To alleviate this problem, we propose HAM, a straightforward yet effective framework via adaptive Hard Adversarial example Mining.HAM concentrates on mining hard adversarial examples while discarding the easy ones in an adaptive fashion. Specifically, HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value. Besides, an early-dropping mechanism is incorporated to discard the easy examples at the initial stages of AE generation, resulting
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#26032;&#29616;&#35937;&#65292;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;&#8220;&#31070;&#32463;&#20999;&#32447;&#8221;&#21644;&#8220;&#26368;&#22823;&#26356;&#26032;&#8221;&#26497;&#38480;&#65292;&#23637;&#31034;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644;&#26680;&#34892;&#20026;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#31616;&#21270;&#35745;&#31639;&#30340;bra-ket&#31526;&#21495;&#12290;</title><link>http://arxiv.org/abs/2308.01814</link><description>&lt;p&gt;
Tensor&#31243;&#24207;IVb&#65306;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#20013;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit. (arXiv:2308.01814v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#26032;&#29616;&#35937;&#65292;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;&#8220;&#31070;&#32463;&#20999;&#32447;&#8221;&#21644;&#8220;&#26368;&#22823;&#26356;&#26032;&#8221;&#26497;&#38480;&#65292;&#23637;&#31034;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644;&#26680;&#34892;&#20026;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#31616;&#21270;&#35745;&#31639;&#30340;bra-ket&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#36234;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#24403;&#20351;&#29992;Adam&#31561;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20250;&#20986;&#29616;&#26032;&#30340;&#29616;&#35937;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#19979;&#32467;&#26524;&#65306;&#19982;SGD&#19968;&#26679;&#65292;&#23545;&#20110;&#21253;&#25324;Adam&#22312;&#20869;&#30340;&#19968;&#33324;&#20248;&#21270;&#22120;&#65292;&#29305;&#24449;&#23398;&#20064;&#21644;&#26680;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#21516;&#30340;&#20108;&#20998;&#27861; - &#23613;&#31649;&#26377;&#19968;&#31181;&#38750;&#32447;&#24615;&#30340;&#8220;&#26680;&#8221;&#27010;&#24565;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#23545;&#20110;&#20219;&#20309;&#26550;&#26500;&#30340;&#30456;&#24212;&#30340;&#8220;&#31070;&#32463;&#20999;&#32447;&#8221;&#21644;&#8220;&#26368;&#22823;&#26356;&#26032;&#8221;&#26497;&#38480;&#12290;&#19978;&#36848;&#32467;&#26524;&#30340;&#20004;&#20010;&#22522;&#30784;&#24615;&#36827;&#23637;&#26159;&#65306;1&#65289;&#19968;&#31181;&#26032;&#30340;Tensor&#31243;&#24207;&#35821;&#35328;&#65292;NEXORT&#65292;&#21487;&#20197;&#34920;&#36798;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#22914;&#20309;&#23558;&#26799;&#24230;&#22788;&#29702;&#20026;&#26356;&#26032;&#12290;2&#65289;&#24341;&#20837;bra-ket&#31526;&#21495;&#26469;&#26497;&#22823;&#22320;&#31616;&#21270;Tensor&#31243;&#24207;&#20013;&#30340;&#34920;&#36798;&#24335;&#21644;&#35745;&#31639;&#12290;&#35813;&#24037;&#20316;&#24635;&#32467;&#24182;&#27010;&#25324;&#20102;Tensor&#31243;&#24207;&#31995;&#21015;&#35770;&#25991;&#20013;&#30340;&#25152;&#26377;&#20808;&#21069;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Going beyond stochastic gradient descent (SGD), what new phenomena emerge in wide neural networks trained by adaptive optimizers like Adam? Here we show: The same dichotomy between feature learning and kernel behaviors (as in SGD) holds for general optimizers as well, including Adam -- albeit with a nonlinear notion of "kernel." We derive the corresponding "neural tangent" and "maximal update" limits for any architecture. Two foundational advances underlie the above results: 1) A new Tensor Program language, NEXORT, that can express how adaptive optimizers process gradients into updates. 2) The introduction of bra-ket notation to drastically simplify expressions and calculations in Tensor Programs. This work summarizes and generalizes all previous results in the Tensor Programs series of papers.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21407;&#22987;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26041;&#27861;&#33258;&#21160;&#23398;&#20064;&#35843;&#24230;&#35268;&#21017;&#65292;&#36866;&#29992;&#20110;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#21644;&#20854;&#20182;&#26368;&#20248;&#20316;&#19994;&#35843;&#24230;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01797</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;: &#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach. (arXiv:2308.01797v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21407;&#22987;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26041;&#27861;&#33258;&#21160;&#23398;&#20064;&#35843;&#24230;&#35268;&#21017;&#65292;&#36866;&#29992;&#20110;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#21644;&#20854;&#20182;&#26368;&#20248;&#20316;&#19994;&#35843;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#19994;&#35843;&#24230;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#26080;&#23613;&#30340;&#24212;&#29992;&#12290;&#33391;&#22909;&#35268;&#21010;&#30340;&#35843;&#24230;&#22312;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#24102;&#26469;&#35768;&#22810;&#22909;&#22788;&#65306;&#23427;&#20204;&#38480;&#21046;&#29983;&#20135;&#25104;&#26412;&#21644;&#28010;&#36153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;NP&#38590;&#24230;&#20351;&#24471;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#35774;&#35745;&#22256;&#38590;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#20135;&#29983;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#35843;&#24230;&#65292;&#33258;&#21160;&#23398;&#20064;&#35843;&#24230;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22312;&#24207;&#21015;&#22788;&#29702;&#20013;&#30340;&#21551;&#21457;&#65292;&#24182;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20174;&#26410;&#29992;&#20110;&#35843;&#24230;&#30446;&#30340;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#21644;&#27979;&#35797;&#21040;&#20316;&#19994;&#36710;&#38388;&#38382;&#39064;&#30340;&#19968;&#20123;&#22522;&#20934;&#23454;&#20363;&#19978;&#65292;&#20294;&#35813;&#25216;&#26415;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#20197;&#26377;&#21487;&#33021;&#29992;&#20110;&#22788;&#29702;&#20854;&#20182;&#19981;&#21516;&#30340;&#26368;&#20248;&#20316;&#19994;&#35843;&#24230;&#20219;&#21153;&#65292;&#20943;&#23569;&#24178;&#39044;&#12290;&#32467;&#26524;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Job scheduling is a well-known Combinatorial Optimization problem with endless applications. Well planned schedules bring many benefits in the context of automated systems: among others, they limit production costs and waste. Nevertheless, the NP-hardness of this problem makes it essential to use heuristics whose design is difficult, requires specialized knowledge and often produces methods tailored to the specific task. This paper presents an original end-to-end Deep Reinforcement Learning approach to scheduling that automatically learns dispatching rules. Our technique is inspired by natural language encoder-decoder models for sequence processing and has never been used, to the best of our knowledge, for scheduling purposes. We applied and tested our method in particular to some benchmark instances of Job Shop Problem, but this technique is general enough to be potentially used to tackle other different optimal job scheduling tasks with minimal intervention. Results demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#19977;&#31181;&#33258;&#36866;&#24212;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;EVQE, VAns, RA-VQE&#65289;&#22635;&#34917;&#20102;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#31995;&#32479;&#27604;&#36739;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2308.01789</link><description>&lt;p&gt;
&#22522;&#20110;QUBO&#23454;&#20363;&#30340;&#33258;&#36866;&#24212;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances. (arXiv:2308.01789v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#19977;&#31181;&#33258;&#36866;&#24212;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;EVQE, VAns, RA-VQE&#65289;&#22635;&#34917;&#20102;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#31995;&#32479;&#27604;&#36739;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQAs&#65289;&#22312;NISQ&#26102;&#20195;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;VQAs&#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#22266;&#23450;&#32467;&#26500;&#30340;&#30005;&#36335;&#65292;&#36825;&#20123;&#30005;&#36335;&#21487;&#33021;&#26080;&#27861;&#38024;&#23545;&#29305;&#23450;&#30340;&#38382;&#39064;&#25110;&#30828;&#20214;&#37197;&#32622;&#36827;&#34892;&#23450;&#21046;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#20027;&#35201;&#31574;&#30053;&#26159;&#33258;&#36866;&#24212;VQAs&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#21644;&#21024;&#38500;&#38376;&#26469;&#21160;&#24577;&#20462;&#25913;&#30005;&#36335;&#32467;&#26500;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#21442;&#25968;&#12290;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#33258;&#36866;&#24212;VQAs&#65292;&#20363;&#22914;&#30005;&#36335;&#27973;&#24230;&#12289;&#32416;&#32544;&#33021;&#21147;&#21644;&#30828;&#20214;&#20860;&#23481;&#24615;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#23545;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#19977;&#31181;&#33258;&#36866;&#24212;VQAs&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#36827;&#21270;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;&#65288;EVQE&#65289;&#12289;&#21487;&#21464;&#27169;&#22411;&#65288;VAns&#65289;&#21644;&#38543;&#26426;&#33258;&#36866;&#24212;-VQE&#65288;RA-VQE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Variational Quantum Algorithms (VQAs) have emerged as a promising approach for solving optimization problems on quantum computers in the NISQ era. However, one limitation of VQAs is their reliance on fixed-structure circuits, which may not be taylored for specific problems or hardware configurations. A leading strategy to address this issue are Adaptative VQAs, which dynamically modify the circuit structure by adding and removing gates, and optimize their parameters during the training. Several Adaptative VQAs, based on heuristics such as circuit shallowness, entanglement capability and hardware compatibility, have already been proposed in the literature, but there is still lack of a systematic comparison between the different methods. In this paper, we aim to fill this gap by analyzing three Adaptative VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), Variable Ansatz (VAns), already proposed in the literature, and Random Adapt-VQE (RA-VQE), a random approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22522;&#20110;&#21160;&#33033;&#22721;&#27178;&#25130;&#38754;&#30340;&#31354;&#38388;&#25490;&#21015;&#21644;&#38041;&#21270;&#20449;&#24687;&#65292;&#20934;&#30830;&#39044;&#27979;&#20102;&#21160;&#33033;&#22721;&#30340;&#24212;&#21147;&#21644;&#24212;&#21464;&#20998;&#24067;&#65292;&#20197;&#25552;&#39640;&#24515;&#34880;&#31649;&#39118;&#38505;&#35780;&#20272;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01771</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21160;&#33033;&#22721;&#24212;&#21147;&#24212;&#21464;&#20998;&#24067;&#39044;&#27979;&#20197;&#25552;&#39640;&#24515;&#34880;&#31649;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment. (arXiv:2308.01771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22522;&#20110;&#21160;&#33033;&#22721;&#27178;&#25130;&#38754;&#30340;&#31354;&#38388;&#25490;&#21015;&#21644;&#38041;&#21270;&#20449;&#24687;&#65292;&#20934;&#30830;&#39044;&#27979;&#20102;&#21160;&#33033;&#22721;&#30340;&#24212;&#21147;&#21644;&#24212;&#21464;&#20998;&#24067;&#65292;&#20197;&#25552;&#39640;&#24515;&#34880;&#31649;&#39118;&#38505;&#35780;&#20272;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#20316;&#20026;&#26377;&#38480;&#20803;&#26041;&#27861;(FEM)&#30340;&#26356;&#26377;&#25928;&#26367;&#20195;&#21697;&#26469;&#39044;&#27979;&#21160;&#33033;&#22721;&#20108;&#32500;&#27178;&#25130;&#38754;&#20869;&#30340;&#24212;&#21147;&#21644;&#24212;&#21464;&#22330;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;U-Net&#30340;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#26681;&#25454;&#21160;&#33033;&#22721;&#27178;&#25130;&#38754;&#20013;&#38041;&#21270;&#30340;&#31354;&#38388;&#25490;&#21015;&#26469;&#39044;&#27979;von Mises&#24212;&#21147;&#21644;&#24212;&#21464;&#20998;&#24067;&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(cGAN)&#65292;&#20174;&#24863;&#30693;&#35282;&#24230;&#29305;&#21035;&#26159;&#22686;&#24378;&#20102;&#24212;&#21147;&#21644;&#24212;&#21464;&#22330;&#22320;&#22270;&#23545;&#20110;&#20855;&#26377;&#19981;&#21516;&#38041;&#21270;&#25968;&#37327;&#21644;&#31354;&#38388;&#37197;&#32622;&#30340;&#21160;&#33033;&#22721;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;U-Net&#21644;cGAN&#20043;&#19978;&#65292;&#25105;&#20204;&#36824;&#20998;&#21035;&#25552;&#20986;&#20102;&#23427;&#20204;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22330;&#22320;&#22270;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30001;&#23454;&#26045;&#36793;&#30028;&#26465;&#20214;&#24182;&#25552;&#21462;&#24212;&#21147;&#21644;&#24212;&#21464;&#22330;&#22320;&#22270;&#32780;&#29983;&#25104;&#30340;&#36755;&#20837;&#22270;&#20687;&#21644;&#36755;&#20986;&#22270;&#20687;&#32452;&#25104;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;U-Net&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#21160;&#33033;&#22721;&#30340;&#24212;&#21147;&#21644;&#24212;&#21464;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigated the potential of end-to-end deep learning tools as a more effective substitute for FEM in predicting stress-strain fields within 2D cross sections of arterial wall. We first proposed a U-Net based fully convolutional neural network (CNN) to predict the von Mises stress and strain distribution based on the spatial arrangement of calcification within arterial wall cross-sections. Further, we developed a conditional generative adversarial network (cGAN) to enhance, particularly from the perceptual perspective, the prediction accuracy of stress and strain field maps for arterial walls with various calcification quantities and spatial configurations. On top of U-Net and cGAN, we also proposed their ensemble approaches, respectively, to further improve the prediction accuracy of field maps. Our dataset, consisting of input and output images, was generated by implementing boundary conditions and extracting stress-strain field maps. The trained U-Net models can accurate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#34955;&#23376;&#31574;&#30053;"&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#21103;&#26412;&#30340;&#31181;&#32676;&#65292;&#21033;&#29992;&#29420;&#31435;&#26356;&#26032;&#30340;&#22836;&#37096;&#21644;&#31163;&#32447;&#26356;&#26032;&#31574;&#30053;&#65292;&#20026;&#27599;&#20010;&#22836;&#37096;&#25552;&#20379;&#19981;&#21516;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#30340;&#23398;&#20064;&#21644;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.01759</link><description>&lt;p&gt;
Bag of Policies for Distributional Deep Exploration&#65288;&#20998;&#24067;&#24335;&#28145;&#24230;&#25506;&#32034;&#20013;&#30340;&#19968;&#31181;&#31574;&#30053;&#32452;&#21512;&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
Bag of Policies for Distributional Deep Exploration. (arXiv:2308.01759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#34955;&#23376;&#31574;&#30053;"&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#21103;&#26412;&#30340;&#31181;&#32676;&#65292;&#21033;&#29992;&#29420;&#31435;&#26356;&#26032;&#30340;&#22836;&#37096;&#21644;&#31163;&#32447;&#26356;&#26032;&#31574;&#30053;&#65292;&#20026;&#27599;&#20010;&#22836;&#37096;&#25552;&#20379;&#19981;&#21516;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#30340;&#23398;&#20064;&#21644;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#22797;&#26434;&#29615;&#22659;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#19982;&#20197;&#24448;&#30340;&#21551;&#21457;&#24335;&#26426;&#21046;&#30456;&#27604;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#34955;&#23376;&#31574;&#30053;&#65288;Bag of Policies&#65292;BoP&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#20219;&#20309;&#36820;&#22238;&#20998;&#24067;&#20272;&#35745;&#22120;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#21103;&#26412;&#30340;&#31181;&#32676;&#12290;BoP&#30001;&#22810;&#20010;&#29420;&#31435;&#26356;&#26032;&#30340;&#22836;&#37096;&#32452;&#25104;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;episode&#30001;&#19968;&#20010;&#22836;&#37096;&#25511;&#21046;&#65292;&#25910;&#38598;&#21040;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#34987;&#29992;&#26469;&#31163;&#32447;&#26356;&#26032;&#25152;&#26377;&#22836;&#37096;&#65292;&#20026;&#27599;&#20010;&#22836;&#37096;&#25552;&#20379;&#19981;&#21516;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#20174;&#32780;&#22810;&#26679;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#20998;&#24067;&#36827;&#34892;&#20998;&#24067;&#24335;&#30340;actor-critic&#32676;&#20307;&#23454;&#29616;&#20102;BoP&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#20048;&#35266;&#38598;&#25104;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20687;Bootstrapped DQN&#22312;&#26631;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#26679;&#25552;&#39640;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient exploration in complex environments remains a major challenge for reinforcement learning (RL). Compared to previous Thompson sampling-inspired mechanisms that enable temporally extended exploration, i.e., deep exploration, we focus on deep exploration in distributional RL. We develop here a general purpose approach, Bag of Policies (BoP), that can be built on top of any return distribution estimator by maintaining a population of its copies. BoP consists of an ensemble of multiple heads that are updated independently. During training, each episode is controlled by only one of the heads and the collected state-action pairs are used to update all heads off-policy, leading to distinct learning signals for each head which diversify learning and behaviour. To test whether optimistic ensemble method can improve on distributional RL as did on scalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with a population of distributional actor-critics using Bayesian Distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#23849;&#28291;&#32456;&#27490;&#28857;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21450;&#20854;&#21464;&#20307;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#32467;&#26500;&#21644;&#21407;&#22411;&#28436;&#21270;&#26041;&#26696;&#26469;&#20445;&#25345;&#23545;&#26032;&#31867;&#21035;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01746</link><description>&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#32456;&#27490;&#28857;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21450;&#20854;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants. (arXiv:2308.01746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#23849;&#28291;&#32456;&#27490;&#28857;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21450;&#20854;&#21464;&#20307;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#32467;&#26500;&#21644;&#21407;&#22411;&#28436;&#21270;&#26041;&#26696;&#26469;&#20445;&#25345;&#23545;&#26032;&#31867;&#21035;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#20445;&#25345;&#23545;&#26087;&#31867;&#21035;&#30340;&#33021;&#21147;&#30340;&#21516;&#26102;&#20351;&#26032;&#31867;&#21035;&#20855;&#26377;&#21487;&#23398;&#20064;&#24615;&#65292;&#19968;&#30452;&#26159;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#12290;&#38500;&#20102;&#24120;&#35268;&#24773;&#20917;&#20043;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#38271;&#23614;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#20197;&#32771;&#34385;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;&#36825;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#36827;&#34892;&#25552;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#23849;&#28291;&#32456;&#27490;&#28857;&#65292;&#23427;&#26159;&#19968;&#20010;&#20855;&#26377;&#25972;&#20010;&#26631;&#31614;&#31354;&#38388;&#20013;&#26368;&#22823;&#31561;&#35282;&#20114;&#31867;&#38388;&#38548;&#30340;&#22266;&#23450;&#32467;&#26500;&#12290;&#23427;&#22312;&#25972;&#20010;&#22686;&#37327;&#35757;&#32451;&#36807;&#31243;&#20013;&#20805;&#24403;&#19968;&#33268;&#30340;&#30446;&#26631;&#65292;&#20197;&#36991;&#20813;&#36880;&#27493;&#21010;&#20998;&#29305;&#24449;&#31354;&#38388;&#12290;&#38024;&#23545;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21644;&#38271;&#23614;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#28436;&#21270;&#26041;&#26696;&#65292;&#23558;&#20027;&#24178;&#29305;&#24449;&#24341;&#20837;&#25105;&#20204;&#30340;&#31070;&#32463;&#23849;&#28291;&#32456;&#27490;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to enable learnability for new classes while keeping the capability well on old classes has been a crucial challenge for class incremental learning. Beyond the normal case, long-tail class incremental learning and few-shot class incremental learning are also proposed to consider the data imbalance and data scarcity, respectively, which are common in real-world implementations and further exacerbate the well-known problem of catastrophic forgetting. Existing methods are specifically proposed for one of the three tasks. In this paper, we offer a unified solution to the misalignment dilemma in the three tasks. Concretely, we propose neural collapse terminus that is a fixed structure with the maximal equiangular inter-class separation for the whole label space. It serves as a consistent target throughout the incremental training to avoid dividing the feature space incrementally. For CIL and LTCIL, we further propose a prototype evolving scheme to drive the backbone features into our ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22810;&#20219;&#21153;&#20449;&#24687;&#22686;&#30410;&#30340;&#20998;&#26512;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01744</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#24724;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65306;&#20174;&#25913;&#36827;&#30340;&#32622;&#20449;&#21306;&#38388;&#21040;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning. (arXiv:2308.01744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22810;&#20219;&#21153;&#20449;&#24687;&#22686;&#30410;&#30340;&#20998;&#26512;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#20449;&#24687;&#26469;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#30456;&#20851;&#20219;&#21153;&#12290;&#23545;&#20272;&#35745;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#23545;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#22312;&#32447;&#25110;&#20027;&#21160;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#21487;&#30693;&#35774;&#32622;&#19979;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#32622;&#20449;&#21306;&#38388;&#65292;&#21363;&#24403;&#23398;&#20064;&#32773;&#26080;&#27861;&#33719;&#24471;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#20219;&#21153;&#29305;&#24449;&#26102;&#12290;&#25152;&#24471;&#21040;&#30340;&#21306;&#38388;&#19981;&#38656;&#35201;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;&#36890;&#36807;&#23545;&#22810;&#20219;&#21153;&#20449;&#24687;&#22686;&#30410;&#30340;&#31934;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26032;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30456;&#20284;&#24615;&#21442;&#25968;&#26126;&#26174;&#25913;&#36827;&#29420;&#31435;&#22788;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20107;&#20808;&#30693;&#36947;&#36825;&#20010;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36825;&#31181;&#25913;&#36827;&#30340;&#36951;&#25022;&#65292;&#21363;&#33258;&#21160;&#36866;&#24212;&#20219;&#21153;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning is a powerful framework that enables one to simultaneously learn multiple related tasks by sharing information between them. Quantifying uncertainty in the estimated tasks is of pivotal importance for many downstream applications, such as online or active learning. In this work, we provide novel multitask confidence intervals in the challenging agnostic setting, i.e., when neither the similarity between tasks nor the tasks' features are available to the learner. The obtained intervals do not require i.i.d. data and can be directly applied to bound the regret in online learning. Through a refined analysis of the multitask information gain, we obtain new regret guarantees that, depending on a task similarity parameter, can significantly improve over treating tasks independently. We further propose a novel online learning algorithm that achieves such improved regret without knowing this parameter in advance, i.e., automatically adapting to task similarity. As a second k
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;CFD&#27169;&#25311;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#23547;&#25214;&#22823;&#22411;&#29123;&#27668;&#21457;&#21160;&#26426;&#39044;&#29123;&#23460;&#30340;&#26368;&#20339;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.01743</link><description>&lt;p&gt;
&#20351;&#29992;CFD&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#25214;&#21040;&#22823;&#22411;&#29123;&#27668;&#21457;&#21160;&#26426;&#39044;&#29123;&#23460;&#30340;&#26368;&#20339;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization. (arXiv:2308.01743v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;CFD&#27169;&#25311;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#23547;&#25214;&#22823;&#22411;&#29123;&#27668;&#21457;&#21160;&#26426;&#39044;&#29123;&#23460;&#30340;&#26368;&#20339;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39044;&#29123;&#23460;&#30340;&#28237;&#27969;&#23556;&#27969;&#28857;&#28779;&#27010;&#24565;&#26159;&#22312;&#22823;&#22411;&#29123;&#27668;&#21457;&#21160;&#26426;&#20013;&#23454;&#29616;&#31283;&#23450;&#29123;&#28903;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#36139;&#29123;&#28903;&#26465;&#20214;&#19979;&#23454;&#29616;&#39640;&#25928;&#29575;&#21644;&#20302;&#25490;&#25918;&#27700;&#24179;&#12290;&#30001;&#20110;&#22823;&#22411;&#29123;&#27668;&#21457;&#21160;&#26426;&#39044;&#29123;&#23460;&#30340;&#35774;&#35745;&#21644;&#24037;&#20316;&#21442;&#25968;&#33539;&#22260;&#24191;&#27867;&#65292;&#35780;&#20272;&#19981;&#21516;&#35774;&#35745;&#30340;&#39318;&#36873;&#26041;&#27861;&#26159;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#65292;&#22240;&#20026;&#22312;&#35797;&#39564;&#21488;&#27979;&#37327;&#27963;&#21160;&#20013;&#36827;&#34892;&#27979;&#35797;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35299;&#20915;&#24213;&#23618;&#29289;&#29702;&#23398;&#30340;&#22797;&#26434;&#24615;&#65292;&#35814;&#32454;CFD&#27169;&#25311;&#38656;&#35201;&#24456;&#38271;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#36825;&#20063;&#38480;&#21046;&#20102;&#23427;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#31867;&#20284;&#20110;&#26412;&#20363;&#30340;&#20248;&#21270;&#35774;&#32622;&#20013;&#65292;&#21363;&#22312;&#30446;&#26631;&#20989;&#25968;&#30340;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#30340;&#24773;&#20917;&#19979;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20195;&#20102;&#32463;&#20856;&#30340;&#35797;&#39564;&#35774;&#35745;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#28041;&#21450;&#20351;&#29992;CFD&#27169;&#25311;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#22823;&#22411;&#29123;&#27668;&#21457;&#21160;&#26426;&#39044;&#29123;&#23460;&#35774;&#35745;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The turbulent jet ignition concept using prechambers is a promising solution to achieve stable combustion at lean conditions in large gas engines, leading to high efficiency at low emission levels. Due to the wide range of design and operating parameters for large gas engine prechambers, the preferred method for evaluating different designs is computational fluid dynamics (CFD), as testing in test bed measurement campaigns is time-consuming and expensive. However, the significant computational time required for detailed CFD simulations due to the complexity of solving the underlying physics also limits its applicability. In optimization settings similar to the present case, i.e., where the evaluation of the objective function(s) is computationally costly, Bayesian optimization has largely replaced classical design-of-experiment. Thus, the present study deals with the computationally efficient Bayesian optimization of large gas engine prechambers design using CFD simulation. Reynolds-av
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26631;&#31614;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#36807;&#31243;&#20013;&#25429;&#25417;&#20302;&#31209;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01742</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#26631;&#31614;&#30456;&#20851;&#24615;&#22312;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploiting Multi-Label Correlation in Label Distribution Learning. (arXiv:2308.01742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26631;&#31614;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#36807;&#31243;&#20013;&#25429;&#25417;&#20302;&#31209;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#65288;LDL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20026;&#27599;&#20010;&#23454;&#20363;&#20998;&#37197;&#19968;&#20010;&#26631;&#31614;&#20998;&#24067;&#12290;&#35768;&#22810;LDL&#26041;&#27861;&#25552;&#20986;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#25351;&#25968;&#22823;&#23567;&#30340;&#36755;&#20986;&#31354;&#38388;&#65307;&#20854;&#20013;&#35768;&#22810;&#26041;&#27861;&#21033;&#29992;&#26631;&#31614;&#20998;&#24067;&#30340;&#20302;&#31209;&#32467;&#26500;&#26469;&#25429;&#25417;&#26631;&#31614;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26631;&#31614;&#20998;&#24067;&#30697;&#38453;&#36890;&#24120;&#26159;&#28385;&#31209;&#30340;&#65292;&#32473;&#21033;&#29992;&#20302;&#31209;&#26631;&#31614;&#30456;&#20851;&#24615;&#30340;&#24037;&#20316;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#22810;&#26631;&#31614;&#36890;&#24120;&#26159;&#20302;&#31209;&#30340;&#65307;&#20302;&#31209;&#26631;&#31614;&#30456;&#20851;&#24615;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#65288;MLL&#65289;&#25991;&#29486;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;LDL&#20013;&#24341;&#20837;&#20102;&#36741;&#21161;&#30340;MLL&#36807;&#31243;&#65292;&#24182;&#22312;MLL&#32780;&#19981;&#26159;LDL&#20013;&#25429;&#25417;&#20302;&#31209;&#26631;&#31614;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24688;&#24403;&#22320;&#21033;&#29992;&#20102;&#20302;&#31209;&#26631;&#31614;&#30456;&#20851;&#24615;&#22312;&#25105;&#20204;&#30340;LDL&#26041;&#27861;&#20013;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;LDL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label Distribution Learning (LDL) is a novel machine learning paradigm that assigns label distribution to each instance. Many LDL methods proposed to leverage label correlation in the learning process to solve the exponential-sized output space; among these, many exploited the low-rank structure of label distribution to capture label correlation. However, recent studies disclosed that label distribution matrices are typically full-rank, posing challenges to those works exploiting low-rank label correlation. Note that multi-label is generally low-rank; low-rank label correlation is widely adopted in multi-label learning (MLL) literature. Inspired by that, we introduce an auxiliary MLL process in LDL and capture low-rank label correlation on that MLL rather than LDL. In such a way, low-rank label correlation is appropriately exploited in our LDL methods. We conduct comprehensive experiments and demonstrate that our methods are superior to existing LDL methods. Besides, the ablation studi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#21644;&#22823;&#37327;&#29992;&#25143;&#28857;&#20987;&#26085;&#24535;&#65292;&#23398;&#20064;&#26356;&#24191;&#20041;&#21644;&#26377;&#25928;&#30340;&#29305;&#24449;&#21644;&#23454;&#20363;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01737</link><description>&lt;p&gt;
MAP: &#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. (arXiv:2308.01737v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#21644;&#22823;&#37327;&#29992;&#25143;&#28857;&#20987;&#26085;&#24535;&#65292;&#23398;&#20064;&#26356;&#24191;&#20041;&#21644;&#26377;&#25928;&#30340;&#29305;&#24449;&#21644;&#23454;&#20363;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20010;&#24615;&#21270;&#22312;&#32447;&#26381;&#21153;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#21644;&#30740;&#31350;&#12290;CTR&#39044;&#27979;&#30340;&#26368;&#31361;&#20986;&#29305;&#28857;&#26159;&#20854;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#26684;&#24335;&#21644;&#24222;&#22823;&#32780;&#26085;&#30410;&#22686;&#38271;&#30340;&#25968;&#25454;&#37327;&#12290;&#31070;&#32463;&#27169;&#22411;&#30340;&#22823;&#23481;&#37327;&#26377;&#21161;&#20110;&#22312;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19979;&#28040;&#21270;&#22914;&#27492;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#23427;&#20204;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;1&#27604;&#29305;&#30340;&#28857;&#20987;&#20449;&#21495;&#19981;&#36275;&#20197;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#21151;&#33021;&#24378;&#22823;&#30340;&#29305;&#24449;&#21644;&#23454;&#20363;&#34920;&#31034;&#12290;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#25552;&#20379;&#20102;&#26356;&#26377;&#21069;&#26223;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22823;&#37327;&#29992;&#25143;&#28857;&#20987;&#26085;&#24535;&#24182;&#23398;&#20064;&#26356;&#24191;&#20041;&#21644;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;CTR&#39044;&#27979;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#24403;&#21069;&#22312;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#20165;&#20165;&#26159;&#21021;&#27493;&#21644;&#22522;&#30784;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretrain
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#32493;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#20272;&#35745;&#32771;&#34385;&#25968;&#25454;&#27169;&#31946;&#24615;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#21512;&#29702;&#36755;&#20986;&#65292;&#24182;&#19988;&#19981;&#20551;&#35774;&#39044;&#27979;&#20998;&#24067;&#30340;&#21442;&#25968;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.01731</link><description>&lt;p&gt;
&#36890;&#36807;&#25512;&#26029;&#26102;&#38388;&#37319;&#26679;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantification of Predictive Uncertainty via Inference-Time Sampling. (arXiv:2308.01731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#32493;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#20272;&#35745;&#32771;&#34385;&#25968;&#25454;&#27169;&#31946;&#24615;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#21512;&#29702;&#36755;&#20986;&#65292;&#24182;&#19988;&#19981;&#20551;&#35774;&#39044;&#27979;&#20998;&#24067;&#30340;&#21442;&#25968;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#27169;&#31946;&#24615;&#32780;&#23548;&#33268;&#30340;&#39044;&#27979;&#21464;&#24322;&#24615;&#36890;&#24120;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#20869;&#32622;&#27010;&#29575;&#33021;&#21147;&#30340;&#19987;&#29992;&#27169;&#22411;&#26469;&#35299;&#20915;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#35757;&#32451;&#20197;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20316;&#20026;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19981;&#21516;&#30340;&#32467;&#26500;&#32452;&#20214;&#21644;&#35757;&#32451;&#26426;&#21046;&#65292;&#21487;&#33021;&#21253;&#21547;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#24182;&#34920;&#29616;&#20986;&#36807;&#20110;&#33258;&#20449;&#65292;&#21363;&#23545;&#19981;&#31934;&#30830;&#39044;&#27979;&#30340;&#39640;&#32622;&#20449;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#32493;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#20272;&#35745;&#32771;&#34385;&#25968;&#25454;&#27169;&#31946;&#24615;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20026;&#32473;&#23450;&#30340;&#36755;&#20837;&#29983;&#25104;&#19981;&#21516;&#30340;&#21512;&#29702;&#36755;&#20986;&#65292;&#24182;&#19988;&#19981;&#20551;&#35774;&#39044;&#27979;&#20998;&#24067;&#30340;&#21442;&#25968;&#24418;&#24335;&#12290;&#23427;&#26159;&#26550;&#26500;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#21069;&#39304;&#30830;&#23450;&#24615;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#23545;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#26356;&#25913;&#12290;&#22312;&#22270;&#20687;&#21644;&#38750;&#22270;&#20687;&#36755;&#20837;&#25968;&#25454;&#19978;&#30340;&#22238;&#24402;&#20219;&#21153;&#23454;&#39564;&#26174;&#31034;&#35813;&#26041;&#27861;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#22810;&#27169;&#24577;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;-
&lt;/p&gt;
&lt;p&gt;
Predictive variability due to data ambiguities has typically been addressed via construction of dedicated models with built-in probabilistic capabilities that are trained to predict uncertainty estimates as variables of interest. These approaches require distinct architectural components and training mechanisms, may include restrictive assumptions and exhibit overconfidence, i.e., high confidence in imprecise predictions. In this work, we propose a post-hoc sampling strategy for estimating predictive uncertainty accounting for data ambiguity. The method can generate different plausible outputs for a given input and does not assume parametric forms of predictive distributions. It is architecture agnostic and can be applied to any feed-forward deterministic network without changes to the architecture or training procedure. Experiments on regression tasks on imaging and non-imaging input data show the method's ability to generate diverse and multi-modal predictive distributions, and a des
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#30340;&#27178;&#26029;&#38754;&#21644;&#32437;&#21521;&#32034;&#36180;&#35745;&#25968;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#31934;&#31639;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01729</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#30340;&#27178;&#26029;&#38754;&#21644;&#32437;&#21521;&#32034;&#36180;&#35745;&#25968;&#25968;&#25454;&#30340;&#36710;&#36733;&#36890;&#20449;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data. (arXiv:2308.01729v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#30340;&#27178;&#26029;&#38754;&#21644;&#32437;&#21521;&#32034;&#36180;&#35745;&#25968;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#31934;&#31639;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mario W\"uthrich&#21644;Michael Merz&#25552;&#20986;&#30340;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#65288;CANN&#65289;&#26694;&#26550;&#30340;&#27178;&#26029;&#38754;&#21644;&#32437;&#21521;&#32034;&#36180;&#35745;&#25968;&#27169;&#22411;&#12290;CANN&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#31934;&#31639;&#27169;&#22411;&#65288;&#22914;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65289;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#32463;&#20856;&#22238;&#24402;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#37096;&#20998;&#30340;&#21452;&#32452;&#20214;&#27169;&#22411;&#12290;CANN&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#26082;&#21487;&#20197;&#25552;&#20379;&#32463;&#20856;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#21487;&#20197;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#22797;&#26434;&#20851;&#31995;&#21644;&#20132;&#20114;&#20316;&#29992;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24191;&#20026;&#20154;&#30693;&#30340;&#23545;&#25968;&#32447;&#24615;&#32034;&#36180;&#35745;&#25968;&#22238;&#24402;&#27169;&#22411;&#20316;&#20026;&#32463;&#20856;&#22238;&#24402;&#37096;&#20998;&#65292;&#20351;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#37096;&#20998;&#12290;MLP&#37096;&#20998;&#29992;&#20110;&#22788;&#29702;&#20197;&#21521;&#37327;&#24418;&#24335;&#34920;&#31034;&#30340;&#36710;&#36742;&#39550;&#39542;&#34892;&#20026;&#30340;&#36710;&#36733;&#36890;&#20449;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present novel cross-sectional and longitudinal claim count models for vehicle insurance built upon the Combined Actuarial Neural Network (CANN) framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach combines a classical actuarial model, such as a generalized linear model, with a neural network. This blending of models results in a two-component model comprising a classical regression model and a neural network part. The CANN model leverages the strengths of both components, providing a solid foundation and interpretability from the classical model while harnessing the flexibility and capacity to capture intricate relationships and interactions offered by the neural network. In our proposed models, we use well-known log-linear claim count regression models for the classical regression part and a multilayer perceptron (MLP) for the neural network part. The MLP part is used to process telematics car driving data given as a vector characterizing the driving behavior 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#36335;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#24213;&#23618;&#20551;&#35774;&#21644;&#25216;&#26415;&#32454;&#33410;&#23545;&#35299;&#37322;&#30340;&#36136;&#37327;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.01682</link><description>&lt;p&gt;
&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#36335;&#39044;&#27979;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Evaluating Link Prediction Explanations for Graph Neural Networks. (arXiv:2308.01682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#36335;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#24213;&#23618;&#20551;&#35774;&#21644;&#25216;&#26415;&#32454;&#33410;&#23545;&#35299;&#37322;&#30340;&#36136;&#37327;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#65288;GML&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#27604;&#22914;&#33410;&#28857;/&#22270;&#20998;&#31867;&#21644;&#38142;&#36335;&#39044;&#27979;&#12290;&#20026;GML&#27169;&#22411;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#22522;&#30784;&#30340;&#20219;&#21153;&#65292;&#20294;&#23545;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;&#30340;&#35299;&#37322;&#39564;&#35777;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#26469;&#35780;&#20272;&#38142;&#36335;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#26080;&#35770;&#26159;&#21542;&#26377;&#22522;&#20934;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20808;&#36827;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24213;&#23618;&#20551;&#35774;&#21644;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#29305;&#23450;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#27604;&#22914;&#33410;&#28857;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#36873;&#25321;&#65292;&#22914;&#20309;&#24433;&#21709;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Machine Learning (GML) has numerous applications, such as node/graph classification and link prediction, in real-world domains. Providing human-understandable explanations for GML models is a challenging yet fundamental task to foster their adoption, but validating explanations for link prediction models has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#32422;&#26463;&#26368;&#23567;&#21270;&#26041;&#27861;&#22312;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#36866;&#24403;&#30340;&#20005;&#26684;&#20114;&#34917;&#24615;&#26465;&#20214;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#22312;&#27492;&#26465;&#20214;&#19979;&#30340;&#20027;&#35201;&#32467;&#26524;&#65306;1.&#23545;&#20110;&#29305;&#23450;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26631;&#20934;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#23613;&#31649;&#30446;&#26631;&#20989;&#25968;&#19981;&#19968;&#23450;&#26159;&#24378;&#20984;&#30340;&#12290;2.&#23545;&#20110;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27599;&#27425;&#36845;&#20195;&#30340;&#36816;&#34892;&#26102;&#38388;&#21487;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2308.01677</link><description>&lt;p&gt;
&#21033;&#29992;&#24352;&#37327;&#26680;&#33539;&#25968;&#21644;&#20005;&#26684;&#20114;&#34917;&#24615;&#36827;&#34892;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#30340;&#19968;&#38454;&#26041;&#27861;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity. (arXiv:2308.01677v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#32422;&#26463;&#26368;&#23567;&#21270;&#26041;&#27861;&#22312;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#36866;&#24403;&#30340;&#20005;&#26684;&#20114;&#34917;&#24615;&#26465;&#20214;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#22312;&#27492;&#26465;&#20214;&#19979;&#30340;&#20027;&#35201;&#32467;&#26524;&#65306;1.&#23545;&#20110;&#29305;&#23450;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26631;&#20934;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#23613;&#31649;&#30446;&#26631;&#20989;&#25968;&#19981;&#19968;&#23450;&#26159;&#24378;&#20984;&#30340;&#12290;2.&#23545;&#20110;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27599;&#27425;&#36845;&#20195;&#30340;&#36816;&#34892;&#26102;&#38388;&#21487;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#24352;&#37327;&#26680;&#33539;&#25968;&#35825;&#23548;&#30340;&#29699;&#19978;&#30340;&#32422;&#26463;&#26368;&#23567;&#21270;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#32771;&#34385;&#20102;&#29992;&#20110;&#24674;&#22797;&#20302;&#31209;&#30697;&#38453;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#24182;&#19988;&#24050;&#32463;&#24314;&#31435;&#20102;&#22312;&#20005;&#26684;&#20114;&#34917;&#24615;&#26465;&#20214;&#19979;&#65292;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27599;&#27425;&#36845;&#20195;&#30340;&#36816;&#34892;&#26102;&#38388;&#21487;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#38024;&#23545;&#24352;&#37327;&#26680;&#33539;&#25968;&#29699;&#20307;&#24320;&#21457;&#20102;&#36866;&#24403;&#30340;&#20005;&#26684;&#20114;&#34917;&#24615;&#26465;&#20214;&#65292;&#24182;&#33719;&#24471;&#20102;&#20197;&#19979;&#20027;&#35201;&#32467;&#26524;&#65306;1. &#24403;&#35201;&#26368;&#23567;&#21270;&#30340;&#30446;&#26631;&#20855;&#26377;&#24418;&#24335;$f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$&#65292;&#20854;&#20013;$g$&#26159;&#24378;&#20984;&#20989;&#25968;&#65292;$\mA$&#26159;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#65288;&#20363;&#22914;&#26368;&#23567;&#20108;&#20056;&#27861;&#65289;&#65292;&#23384;&#22312;&#20108;&#27425;&#22686;&#38271;&#30028;&#65292;&#36825;&#24847;&#21619;&#30528;&#26631;&#20934;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#23613;&#31649;$f$&#19981;&#19968;&#23450;&#26159;&#24378;&#20984;&#30340;&#12290;2.&#23545;&#20110;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
We consider convex relaxations for recovering low-rank tensors based on constrained minimization over a ball induced by the tensor nuclear norm, recently introduced in \cite{tensor_tSVD}. We build on a recent line of results that considered convex relaxations for the recovery of low-rank matrices and established that under a strict complementarity condition (SC), both the convergence rate and per-iteration runtime of standard gradient methods may improve dramatically. We develop the appropriate strict complementarity condition for the tensor nuclear norm ball and obtain the following main results under this condition: 1. When the objective to minimize is of the form $f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$ , where $g$ is strongly convex and $\mA$ is a linear map (e.g., least squares), a quadratic growth bound holds, which implies linear convergence rates for standard projected gradient methods, despite the fact that $f$ need not be strongly convex. 2. For a smooth objective function,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.01674</link><description>&lt;p&gt;
&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC. (arXiv:2308.01674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#65288;&#32463;&#27982;&#65289;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;&#65288;e&#65289;NMPC&#65289;&#38656;&#35201;&#22312;&#25152;&#26377;&#30456;&#20851;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#37117;&#20855;&#26377;&#36275;&#22815;&#20934;&#30830;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#24517;&#39035;&#35745;&#31639;&#25104;&#26412;&#36275;&#22815;&#20302;&#20197;&#30830;&#20445;&#23454;&#26102;&#21487;&#34892;&#24615;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#26426;&#21046;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#65288;e&#65289;NMPC&#30340;&#35745;&#31639;&#36127;&#25285;&#65307;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#31995;&#32479;&#36776;&#35782;&#20197;&#22312;&#27169;&#25311;&#26679;&#26412;&#19978;&#33719;&#24471;&#26368;&#22823;&#24179;&#22343;&#39044;&#27979;&#20934;&#30830;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20316;&#20026;&#23454;&#38469;&#65288;e&#65289;NMPC&#30340;&#19968;&#37096;&#20998;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#20339;&#65288;e&#65289;NMPC&#24615;&#33021;&#30340;&#21160;&#24577;&#26367;&#20195;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#33391;&#22909;&#24179;&#34913;&#30340;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;&#24050;&#24314;&#31435;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#25605;&#25292;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#24212;&#29992;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the 
&lt;/p&gt;</description></item><item><title>UniG-Encoder&#26159;&#19968;&#31181;&#36890;&#29992;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22270;&#21644;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#23558;&#36830;&#25509;&#33410;&#28857;&#30340;&#25299;&#25169;&#20851;&#31995;&#36716;&#25442;&#20026;&#36793;&#25110;&#36229;&#36793;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#21407;&#22987;&#33410;&#28857;&#29305;&#24449;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#32534;&#30721;&#21518;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.01650</link><description>&lt;p&gt;
UniG-Encoder: &#19968;&#31181;&#29992;&#20110;&#22270;&#19982;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#30340;&#36890;&#29992;&#29305;&#24449;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification. (arXiv:2308.01650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01650
&lt;/p&gt;
&lt;p&gt;
UniG-Encoder&#26159;&#19968;&#31181;&#36890;&#29992;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22270;&#21644;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#23558;&#36830;&#25509;&#33410;&#28857;&#30340;&#25299;&#25169;&#20851;&#31995;&#36716;&#25442;&#20026;&#36793;&#25110;&#36229;&#36793;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#21407;&#22987;&#33410;&#28857;&#29305;&#24449;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#32534;&#30721;&#21518;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21644;&#36229;&#22270;&#34920;&#31034;&#23398;&#20064;&#21463;&#21040;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12289;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#21450;&#20854;&#31934;&#24515;&#35774;&#35745;&#30340;&#21464;&#31181;&#22312;&#19968;&#20123;&#24120;&#29992;&#22522;&#20934;&#22270;&#21644;&#36229;&#22270;&#19978;&#26377;&#19981;&#38169;&#30340;&#24615;&#33021;&#21644;&#20016;&#23500;&#30340;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21364;&#19981;&#22914;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;Multi-Layer Perceptron&#65289;&#34920;&#29616;&#22909;&#12290;&#36825;&#19968;&#35266;&#23519;&#20419;&#20351;&#20154;&#20204;&#37325;&#26032;&#23457;&#35270;&#24403;&#21069;GNNs&#21644;HGNNs&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#25552;&#21462;&#22270;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#21644;&#36229;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;UniG-Encoder&#12290;&#35813;&#26550;&#26500;&#20174;&#36830;&#25509;&#33410;&#28857;&#30340;&#25299;&#25169;&#20851;&#31995;&#30340;&#21069;&#21521;&#36716;&#25442;&#24320;&#22987;&#65292;&#36890;&#36807;&#19968;&#20010;&#24402;&#19968;&#21270;&#25237;&#24433;&#30697;&#38453;&#23558;&#20854;&#36716;&#25442;&#20026;&#36793;&#25110;&#36229;&#36793;&#29305;&#24449;&#12290;&#24471;&#21040;&#30340;&#36793;/&#36229;&#36793;&#29305;&#24449;&#20197;&#21450;&#21407;&#22987;&#33410;&#28857;&#29305;&#24449;&#19968;&#36215;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#12290;&#32534;&#30721;&#21518;&#30340;&#33410;&#28857;&#23884;&#20837;&#28982;&#21518;&#34987;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph and hypergraph representation learning has attracted increasing attention from various research fields. Despite the decent performance and fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural Networks (HGNNs), and their well-designed variants, on some commonly used benchmark graphs and hypergraphs, they are outperformed by even a simple Multi-Layer Perceptron. This observation motivates a reexamination of the design paradigm of the current GNNs and HGNNs and poses challenges of extracting graph features effectively. In this work, a universal feature encoder for both graph and hypergraph representation learning is designed, called UniG-Encoder. The architecture starts with a forward transformation of the topological relationships of connected nodes into edge or hyperedge features via a normalized projection matrix. The resulting edge/hyperedge features, together with the original node features, are fed into a neural network. The encoded node embeddings are then
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MARLIM&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38543;&#26426;&#38656;&#27714;&#21644;&#20132;&#36135;&#26102;&#38388;&#30340;&#21333;&#23618;&#22810;&#20135;&#21697;&#20379;&#24212;&#38142;&#30340;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01649</link><description>&lt;p&gt;
MARLIM: &#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MARLIM: Multi-Agent Reinforcement Learning for Inventory Management. (arXiv:2308.01649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MARLIM&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38543;&#26426;&#38656;&#27714;&#21644;&#20132;&#36135;&#26102;&#38388;&#30340;&#21333;&#23618;&#22810;&#20135;&#21697;&#20379;&#24212;&#38142;&#30340;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20379;&#24212;&#38142;&#34892;&#19994;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#34917;&#36135;&#20915;&#31574;&#26469;&#32500;&#25345;&#20135;&#21697;&#20379;&#38656;&#24179;&#34913;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARLIM&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38543;&#26426;&#38656;&#27714;&#21644;&#20132;&#36135;&#26102;&#38388;&#30340;&#21333;&#23618;&#22810;&#20135;&#21697;&#20379;&#24212;&#38142;&#30340;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#36890;&#36807;&#21333;&#20010;&#25110;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#29615;&#22659;&#20013;&#24320;&#21457;&#25511;&#21046;&#22120;&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining a balance between the supply and demand of products by optimizing replenishment decisions is one of the most important challenges in the supply chain industry. This paper presents a novel reinforcement learning framework called MARLIM, to address the inventory management problem for a single-echelon multi-products supply chain with stochastic demands and lead-times. Within this context, controllers are developed through single or multiple agents in a cooperative setting. Numerical experiments on real data demonstrate the benefits of reinforcement learning methods over traditional baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#20114;&#20132;&#38169;&#24212;&#29992;&#20110;&#20070;&#31821;&#23553;&#38754;&#35774;&#35745;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#26631;&#39064;&#33719;&#24471;&#22810;&#20010;&#36873;&#39033;&#24182;&#21033;&#29992;&#37492;&#21035;&#22120;&#36873;&#25321;&#26368;&#20339;&#22270;&#20687;&#65292;&#34920;&#29616;&#20986;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#21644;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.01626</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#20114;&#20132;&#38169;&#24212;&#29992;&#20110;&#20070;&#31821;&#23553;&#38754;&#35774;&#35745;&#30340;&#21019;&#24847;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Interleaving GANs with knowledge graphs to support design creativity for book covers. (arXiv:2308.01626v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#20114;&#20132;&#38169;&#24212;&#29992;&#20110;&#20070;&#31821;&#23553;&#38754;&#35774;&#35745;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#26631;&#39064;&#33719;&#24471;&#22810;&#20010;&#36873;&#39033;&#24182;&#21033;&#29992;&#37492;&#21035;&#22120;&#36873;&#25321;&#26368;&#20339;&#22270;&#20687;&#65292;&#34920;&#29616;&#20986;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#21644;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20070;&#31821;&#23553;&#38754;&#30340;&#21560;&#24341;&#21147;&#23545;&#20110;&#19968;&#26412;&#20070;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24212;&#29992;&#20110;&#20070;&#31821;&#23553;&#38754;&#39046;&#22495;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;GANs&#30456;&#20114;&#20132;&#38169;&#65292;&#25913;&#21464;&#36755;&#20837;&#26631;&#39064;&#20197;&#33719;&#24471;&#32473;&#23450;&#26631;&#39064;&#30340;&#22810;&#20010;&#21487;&#33021;&#36873;&#39033;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#29983;&#25104;&#22120;&#30340;&#22686;&#24378;&#36755;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#37492;&#21035;&#22120;&#36873;&#25321;&#20351;&#29992;&#26032;&#26631;&#39064;&#29983;&#25104;&#30340;&#26368;&#20339;&#22270;&#20687;&#12290;&#30456;&#27604;&#20165;&#20351;&#29992;GANs&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#20070;&#31821;&#23553;&#38754;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#30693;&#35782;&#22270;&#35889;&#19982;&#20070;&#31821;&#20316;&#32773;&#25110;&#32534;&#36753;&#30456;&#27604;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
An attractive book cover is important for the success of a book. In this paper, we apply Generative Adversarial Networks (GANs) to the book covers domain, using different methods for training in order to obtain better generated images. We interleave GANs with knowledge graphs to alter the input title to obtain multiple possible options for any given title, which are then used as an augmented input to the generator. Finally, we use the discriminator obtained during the training phase to select the best images generated with new titles. Our method performed better at generating book covers than previous attempts, and the knowledge graph gives better options to the book author or editor compared to using GANs alone.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#30340;&#29305;&#28857;&#65292;&#24076;&#26395;&#23558;&#23545;&#31216;&#24615;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#25512;&#24191;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#24212;&#29992;&#20110;ConvNet&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.01621</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#30340;&#29305;&#28857;&#65292;&#24076;&#26395;&#23558;&#23545;&#31216;&#24615;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#25512;&#24191;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#24212;&#29992;&#20110;ConvNet&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNet)&#26550;&#26500;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#19968;&#31867;&#31216;&#20026;&#25311;&#32447;&#24615;&#21452;&#26354;&#31995;&#32479;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23427;&#20801;&#35768;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#12290;&#36825;&#26159;&#19982;&#20256;&#32479;&#27169;&#22411;&#20013;&#22522;&#26412;&#22266;&#23450;&#30340;&#26550;&#26500;&#21644;&#26435;&#37325;&#30456;&#27604;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;&#25105;&#20204;&#24076;&#26395;&#23558;(&#20869;&#37096;)&#23545;&#31216;&#24615;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#21560;&#24341;&#23545;PDE&#35270;&#35282;&#20998;&#26512;&#21644;&#35299;&#37322;ConvNet&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#20107;&#23454;&#39564;&#35777;&#35780;&#20272;DNNs&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2308.01614</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#39564;&#35777;&#35780;&#20272;DNNs&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Assessing Systematic Weaknesses of DNNs using Counterfactuals. (arXiv:2308.01614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01614
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#39564;&#35777;&#35780;&#20272;DNNs&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;DNNs&#36827;&#20837;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#23545;&#36825;&#31181;&#27169;&#22411;&#30340;&#27979;&#35797;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#19968;&#20010;&#26041;&#21521;&#26159;&#23547;&#25214;&#21644;&#35782;&#21035;&#31995;&#32479;&#24615;&#24369;&#28857;&#65292;&#36825;&#20123;&#24369;&#28857;&#20351;&#22522;&#20110;&#24179;&#22343;&#24615;&#33021;&#20540;&#30340;&#23433;&#20840;&#20551;&#35774;&#22788;&#20110;&#21361;&#38505;&#20043;&#20013;&#12290;&#36825;&#20123;&#24369;&#28857;&#21487;&#20197;&#34920;&#29616;&#20026;&#65288;&#35821;&#20041;&#19978;&#36830;&#36143;&#30340;&#65289;&#23376;&#38598;&#25110;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#21306;&#22495;&#65292;&#22312;&#36825;&#20123;&#21306;&#22495;&#20013;&#65292;DNN&#30340;&#24615;&#33021;&#27604;&#39044;&#26399;&#30340;&#24179;&#22343;&#24615;&#33021;&#35201;&#24046;&#12290;&#28982;&#32780;&#65292;&#23558;&#35266;&#23519;&#21040;&#30340;&#20302;&#24615;&#33021;&#24402;&#22240;&#20110;&#25551;&#36848;&#35813;&#23376;&#38598;&#30340;&#29305;&#23450;&#35821;&#20041;&#29305;&#24449;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#20363;&#22914;&#65292;&#19982;&#20854;&#20182;&#65288;&#26410;&#32771;&#34385;&#65289;&#23646;&#24615;&#30456;&#20851;&#30340;&#25968;&#25454;&#19981;&#22343;&#21248;&#24615;&#21487;&#33021;&#20250;&#25197;&#26354;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#25152;&#26377;&#65288;&#21487;&#29992;&#65289;&#23646;&#24615;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#21463;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#35745;&#31639;&#25104;&#26412;&#20302;&#30340;&#31639;&#27861;&#26469;&#39564;&#35777;&#29616;&#26377;&#23376;&#38598;&#30340;&#35821;&#20041;&#24402;&#22240;&#65292;&#21363;&#26816;&#26597;...
&lt;/p&gt;
&lt;p&gt;
With the advancement of DNNs into safety-critical applications, testing approaches for such models have gained more attention. A current direction is the search for and identification of systematic weaknesses that put safety assumptions based on average performance values at risk. Such weaknesses can take on the form of (semantically coherent) subsets or areas in the input space where a DNN performs systematically worse than its expected average. However, it is non-trivial to attribute the reason for such observed low performances to the specific semantic features that describe the subset. For instance, inhomogeneities within the data w.r.t. other (non-considered) attributes might distort results. However, taking into account all (available) attributes and their interaction is often computationally highly expensive. Inspired by counterfactual explanations, we propose an effective and computationally cheap algorithm to validate the semantic attribution of existing subsets, i.e., to chec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#29305;&#24449;&#22122;&#22768;&#26041;&#27861;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#26631;&#31614;&#22122;&#22768;&#20250;&#21066;&#24369;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#29305;&#24449;&#22122;&#22768;&#36890;&#36807;&#32422;&#26463;&#27169;&#22411;&#26435;&#37325;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#30830;&#20445;&#26377;&#25928;&#27867;&#21270;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24212;&#29992;&#20998;&#26512;&#24182;&#30830;&#23450;&#20102;&#21512;&#36866;&#30340;&#29305;&#24449;&#22122;&#22768;&#31867;&#22411;&#21644;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.01609</link><description>&lt;p&gt;
&#29305;&#24449;&#22122;&#22768;&#25552;&#21319;&#24102;&#26377;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Feature Noise Boosts DNN Generalization under Label Noise. (arXiv:2308.01609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#29305;&#24449;&#22122;&#22768;&#26041;&#27861;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#26631;&#31614;&#22122;&#22768;&#20250;&#21066;&#24369;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#29305;&#24449;&#22122;&#22768;&#36890;&#36807;&#32422;&#26463;&#27169;&#22411;&#26435;&#37325;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#30830;&#20445;&#26377;&#25928;&#27867;&#21270;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24212;&#29992;&#20998;&#26512;&#24182;&#30830;&#23450;&#20102;&#21512;&#36866;&#30340;&#29305;&#24449;&#22122;&#22768;&#31867;&#22411;&#21644;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#29305;&#24449;&#22122;&#22768;&#26041;&#27861;&#65292;&#30452;&#25509;&#32473;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24449;&#28155;&#21152;&#22122;&#22768;&#65292;&#21487;&#20197;&#25552;&#21319;&#24102;&#26377;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;DNNs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#22122;&#22768;&#36890;&#36807;&#25918;&#23485;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#32780;&#23548;&#33268;DNN&#27867;&#21270;&#33021;&#21147;&#20943;&#24369;&#65292;&#32780;&#29305;&#24449;&#22122;&#22768;&#36890;&#36807;&#23545;&#27169;&#22411;&#26435;&#37325;&#19982;&#29305;&#24449;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#35774;&#32622;&#19978;&#30028;&#26469;&#32422;&#26463;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;DNN&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30830;&#20445;DNN&#30340;&#26377;&#25928;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24212;&#29992;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#28155;&#21152;&#21512;&#36866;&#30340;&#29305;&#24449;&#22122;&#22768;&#31867;&#22411;&#21644;&#27700;&#24179;&#20197;&#33719;&#24471;&#29702;&#24819;&#30340;&#26631;&#31614;&#22122;&#22768;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The presence of label noise in the training data has a profound impact on the generalization of deep neural networks (DNNs). In this study, we introduce and theoretically demonstrate a simple feature noise method, which directly adds noise to the features of training data, can enhance the generalization of DNNs under label noise. Specifically, we conduct theoretical analyses to reveal that label noise leads to weakened DNN generalization by loosening the PAC-Bayes generalization bound, and feature noise results in better DNN generalization by imposing an upper bound on the mutual information between the model weights and the features, which constrains the PAC-Bayes generalization bound. Furthermore, to ensure effective generalization of DNNs in the presence of label noise, we conduct application analyses to identify the optimal types and levels of feature noise to add for obtaining desirable label noise generalization. Finally, extensive experimental results on several popular datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#37325;&#22797;&#21512;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#20010;&#22270;&#20043;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#21644;&#19968;&#33268;&#20449;&#24687;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#26679;&#26412;&#22806;&#38382;&#39064;&#21644;&#22122;&#22768;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01606</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22810;&#37325;&#22797;&#21512;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20114;&#34917;&#21644;&#19968;&#33268;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multiplex Graph Learning with Complementary and Consistent Information. (arXiv:2308.01606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#37325;&#22797;&#21512;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#20010;&#22270;&#20043;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#21644;&#19968;&#33268;&#20449;&#24687;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#26679;&#26412;&#22806;&#38382;&#39064;&#21644;&#22122;&#22768;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#22810;&#20010;&#22270;&#20043;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#21644;&#19968;&#33268;&#20449;&#24687;&#65292;&#26080;&#30417;&#30563;&#30340;&#22810;&#37325;&#22797;&#21512;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;UMGL&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38382;&#39064;&#65292;&#21363;&#26679;&#26412;&#22806;&#38382;&#39064;&#21644;&#22122;&#22768;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;UMGL&#26041;&#27861;&#26469;&#25506;&#32034;&#20114;&#34917;&#21644;&#19968;&#33268;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#22810;&#20010;MLP&#32534;&#30721;&#22120;&#32780;&#19981;&#26159;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#21516;&#26102;&#20855;&#26377;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#65306;&#20445;&#30041;&#33410;&#28857;&#20043;&#38388;&#30340;&#23616;&#37096;&#22270;&#32467;&#26500;&#20197;&#22788;&#29702;&#26679;&#26412;&#22806;&#38382;&#39064;&#65292;&#24182;&#26368;&#22823;&#21270;&#22810;&#20010;&#33410;&#28857;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#22788;&#29702;&#22122;&#22768;&#38382;&#39064;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#27604;&#36739;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised multiplex graph learning (UMGL) has been shown to achieve significant effectiveness for different downstream tasks by exploring both complementary information and consistent information among multiple graphs. However, previous methods usually overlook the issues in practical applications, i.e., the out-of-sample issue and the noise issue. To address the above issues, in this paper, we propose an effective and efficient UMGL method to explore both complementary and consistent information. To do this, our method employs multiple MLP encoders rather than graph convolutional network (GCN) to conduct representation learning with two constraints, i.e., preserving the local graph structure among nodes to handle the out-of-sample issue, and maximizing the correlation of multiple node representations to handle the noise issue. Comprehensive experiments demonstrate that our proposed method achieves superior effectiveness and efficiency over the comparison methods and effectively tac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#22788;&#29702;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20960;&#20309;&#21464;&#24322;&#30340;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39640;&#25928;&#28436;&#21270;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.01602</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26367;&#20195;&#27169;&#22411;&#65306;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20960;&#20309;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks. (arXiv:2308.01602v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#22788;&#29702;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20960;&#20309;&#21464;&#24322;&#30340;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39640;&#25928;&#28436;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24314;&#27169;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#26102;&#65292;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#25311;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#35768;&#22810;&#23398;&#31185;&#20013;&#37117;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#27714;&#35299;&#21442;&#25968;&#21270;&#30340;&#26102;&#38388;&#30456;&#20851;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20840;&#38454;&#27169;&#22411;&#65288;FOMs&#65289;&#65292;&#27604;&#22914;&#22522;&#20110;&#26377;&#38480;&#20803;&#27861;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#24456;&#39640;&#30340;&#31934;&#24230;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#20043;&#38388;&#25214;&#21040;&#26377;&#21033;&#30340;&#24179;&#34913;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#26469;&#21462;&#20195;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#30340;&#27714;&#35299;&#22120;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20960;&#20309;&#21464;&#24322;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#26102;&#38388;&#30456;&#20851;PDEs&#20223;&#30495;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26102;&#38388;&#27493;&#36827;&#26041;&#26696;&#30340;&#31995;&#32479;&#31574;&#30053;&#65292;&#22312;&#20854;&#20013;&#20351;&#29992;GNN&#26550;&#26500;&#26469;&#39640;&#25928;&#28436;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mesh-based simulations play a key role when modeling complex physical systems that, in many disciplines across science and engineering, require the solution of parametrized time-dependent nonlinear partial differential equations (PDEs). In this context, full order models (FOMs), such as those relying on the finite element method, can reach high levels of accuracy, however often yielding intensive simulations to run. For this reason, surrogate models are developed to replace computationally expensive solvers with more efficient ones, which can strike favorable trade-offs between accuracy and efficiency. This work explores the potential usage of graph neural networks (GNNs) for the simulation of time-dependent PDEs in the presence of geometrical variability. In particular, we propose a systematic strategy to build surrogate models based on a data-driven time-stepping scheme where a GNN architecture is used to efficiently evolve the system. With respect to the majority of surrogate models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ULTS&#24211;&#30340;&#24555;&#36895;&#23454;&#29616;&#19982;&#32479;&#19968;&#35780;&#20272;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.01578</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Representation Learning for Time Series: A Review. (arXiv:2308.01578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ULTS&#24211;&#30340;&#24555;&#36895;&#23454;&#29616;&#19982;&#32479;&#19968;&#35780;&#20272;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#23545;&#27599;&#20010;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#12290;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#35828;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#38750;&#24120;&#20851;&#38190;&#65292;&#22240;&#20026;&#20854;&#22797;&#26434;&#29305;&#24615;&#21644;&#32570;&#20047;&#19982;&#20854;&#20182;&#25968;&#25454;&#24418;&#24577;&#30456;&#27604;&#30340;&#35270;&#35273;&#25552;&#31034;&#23548;&#33268;&#20102;&#26631;&#27880;&#22256;&#22659;&#12290;&#36817;&#24180;&#26469;&#65292;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#20998;&#26512;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#29616;&#26377;&#24555;&#36895;&#21457;&#23637;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#25991;&#29486;&#32508;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#21644;&#26631;&#20934;&#21270;&#30340;&#24211;&#65292;&#21517;&#20026;ULTS&#65288;&#21363;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#65289;&#65292;&#20197;&#20415;&#23545;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#23454;&#29616;&#21644;&#32479;&#19968;&#35780;&#20272;&#12290;&#36890;&#36807;ULTS&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning approaches aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#37492;&#21035;&#22120;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319; DiffGAN-TTS &#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#25193;&#25955;&#37492;&#21035;&#22120;&#23398;&#20064;&#36870;&#36807;&#31243;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#22768;&#35889;&#22270;&#37492;&#21035;&#22120;&#23398;&#20064;&#22768;&#35889;&#22270;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2308.01573</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#23545;&#25239;&#22120;&#23545;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#20197;&#23454;&#29616;&#39640;&#20445;&#30495;&#22810;&#20154;&#35828;&#35805;&#20154; TTS
&lt;/p&gt;
&lt;p&gt;
Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS. (arXiv:2308.01573v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#37492;&#21035;&#22120;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319; DiffGAN-TTS &#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#25193;&#25955;&#37492;&#21035;&#22120;&#23398;&#20064;&#36870;&#36807;&#31243;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#22768;&#35889;&#22270;&#37492;&#21035;&#22120;&#23398;&#20064;&#22768;&#35889;&#22270;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#27010;&#29575;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#27493;&#39588;&#65292;&#20854;&#29983;&#25104;&#36895;&#24230;&#32531;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#65292;&#22914;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411; (DDIM)&#65292;&#19987;&#27880;&#20110;&#22312;&#19981;&#30452;&#25509;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26679;&#26412;&#65292;&#32780;&#20687;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#36825;&#26679;&#30340;&#27169;&#22411;&#23558;&#25193;&#25955;&#36807;&#31243;&#19982; GAN &#32467;&#21512;&#12290;&#22312;&#35821;&#38899;&#21512;&#25104;&#39046;&#22495;&#65292;&#19968;&#31181;&#26368;&#36817;&#30340;&#25193;&#25955;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#21483;&#20570; DiffGAN-TTS&#65292;&#21033;&#29992;&#20102; GAN &#30340;&#32467;&#26500;&#65292;&#23637;&#29616;&#20102;&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#29983;&#25104;&#36895;&#24230;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#21319; DiffGAN-TTS &#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#37492;&#21035;&#22120;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65306;&#19968;&#20010;&#36127;&#36131;&#23398;&#20064;&#36870;&#36807;&#31243;&#20998;&#24067;&#30340;&#25193;&#25955;&#37492;&#21035;&#22120;&#21644;&#19968;&#20010;&#36127;&#36131;&#23398;&#20064;&#22768;&#35889;&#22270;&#20998;&#24067;&#30340;&#22768;&#35889;&#22270;&#37492;&#21035;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diffusion model is capable of generating high-quality data through a probabilistic approach. However, it suffers from the drawback of slow generation speed due to the requirement of a large number of time steps. To address this limitation, recent models such as denoising diffusion implicit models (DDIM) focus on generating samples without directly modeling the probability distribution, while models like denoising diffusion generative adversarial networks (GAN) combine diffusion processes with GANs. In the field of speech synthesis, a recent diffusion speech synthesis model called DiffGAN-TTS, utilizing the structure of GANs, has been introduced and demonstrates superior performance in both speech quality and generation speed. In this paper, to further enhance the performance of DiffGAN-TTS, we propose a speech synthesis model with two discriminators: a diffusion discriminator for learning the distribution of the reverse process and a spectrogram discriminator for learning the distr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24555;&#36895;Slate&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#20915;&#31574;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#20248;&#21270;&#20219;&#24847;&#22870;&#21169;&#20989;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30334;&#19975;&#32423;&#21035;&#21160;&#20316;&#31354;&#38388;&#38382;&#39064;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01566</link><description>&lt;p&gt;
&#24555;&#36895;Slate&#31574;&#30053;&#20248;&#21270;&#65306;&#36229;&#36234;Plackett-Luce
&lt;/p&gt;
&lt;p&gt;
Fast Slate Policy Optimization: Going Beyond Plackett-Luce. (arXiv:2308.01566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24555;&#36895;Slate&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#20915;&#31574;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#20248;&#21270;&#20219;&#24847;&#22870;&#21169;&#20989;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30334;&#19975;&#32423;&#21035;&#21160;&#20316;&#31354;&#38388;&#38382;&#39064;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#26500;&#24314;&#27169;&#22359;&#26159;&#36820;&#22238;Slate&#65292;&#21363;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#36820;&#22238;&#26377;&#24207;&#30340;&#39033;&#30446;&#21015;&#34920;&#12290;&#35813;&#25216;&#26415;&#30340;&#24212;&#29992;&#21253;&#25324;&#25628;&#32034;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#24403;&#34892;&#21160;&#31354;&#38388;&#24456;&#22823;&#26102;&#65292;&#20915;&#31574;&#31995;&#32479;&#20250;&#38480;&#21046;&#22312;&#29305;&#23450;&#32467;&#26500;&#20013;&#20197;&#24555;&#36895;&#23436;&#25104;&#22312;&#32447;&#26597;&#35810;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#22823;&#35268;&#27169;&#20915;&#31574;&#31995;&#32479;&#22312;&#32473;&#23450;&#20219;&#24847;&#22870;&#21169;&#20989;&#25968;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#65292;&#23427;&#28304;&#20110;&#20915;&#31574;&#20989;&#25968;&#30340;&#19968;&#31181;&#26032;&#39062;&#25918;&#26494;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#21160;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24120;&#29992;&#30340;Plackett-Luce&#31574;&#30053;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#36798;&#21040;&#30334;&#19975;&#32423;&#21035;&#30340;&#38382;&#39064;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. Applications of this technology include: search, information retrieval and recommender systems. When the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. This paper addresses the optimization of these large scale decision systems given an arbitrary reward function. We cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. This results in a simple, yet efficient learning algorithm that scales to massive action spaces. We compare our method to the commonly adopted Plackett-Luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#22686;&#24378;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;PHFL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24102;&#23485;&#31232;&#32570;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#26080;&#32447;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#21644;&#33021;&#32791;&#32422;&#26463;&#19979;&#25910;&#25947;&#36895;&#24230;&#30340;&#26368;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.01562</link><description>&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65306;&#21098;&#26525;&#35299;&#20915;&#24102;&#23485;&#31232;&#32570;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity. (arXiv:2308.01562v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#22686;&#24378;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;PHFL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24102;&#23485;&#31232;&#32570;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#26080;&#32447;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#21644;&#33021;&#32791;&#32422;&#26463;&#19979;&#25910;&#25947;&#36895;&#24230;&#30340;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#26080;&#32447;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#32456;&#31471;&#29992;&#25143;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20043;&#38388;&#23384;&#22312;&#22810;&#20010;&#23618;&#32423;&#65292;&#29992;&#25143;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#30005;&#27744;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;&#26381;&#21153;&#22522;&#31449;&#20855;&#26377;&#22266;&#23450;&#30340;&#24102;&#23485;&#12290;&#37492;&#20110;&#36825;&#20123;&#23454;&#38469;&#32422;&#26463;&#21644;&#31995;&#32479;&#27169;&#22411;&#65292;&#26412;&#25991;&#21033;&#29992;&#27169;&#22411;&#21098;&#26525;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#32593;&#32476;&#30340;&#21098;&#26525;&#22686;&#24378;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;PHFL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#28165;&#26224;&#22320;&#23637;&#31034;&#20102;&#27169;&#22411;&#21098;&#26525;&#21644;&#23458;&#25143;&#31471;&#19982;&#20851;&#32852;&#22522;&#31449;&#20043;&#38388;&#30340;&#26080;&#32447;&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#21098;&#26525;&#27604;&#29575;&#12289;&#23458;&#25143;&#31471;&#30340;&#20013;&#22830;&#22788;&#29702;&#22120;&#65288;CPU&#65289;&#39057;&#29575;&#21644;&#20256;&#36755;&#21151;&#29575;&#65292;&#20197;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#21644;&#33021;&#32791;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#25910;&#25947;&#30028;&#30340;&#21487;&#25511;&#39033;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21407;&#22987;&#38382;&#39064;&#19981;&#26159;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#36880;&#27493;&#20984;&#36924;&#36817;&#65288;SCA&#65289;&#26041;&#27861;&#65292;&#32852;&#21512;&#20248;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a practical wireless network has many tiers where end users do not directly communicate with the central server, the users' devices have limited computation and battery powers, and the serving base station (BS) has a fixed bandwidth. Owing to these practical constraints and system models, this paper leverages model pruning and proposes a pruning-enabled hierarchical federated learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper bound of the convergence rate that clearly demonstrates the impact of the model pruning and wireless communications between the clients and the associated BS. Then we jointly optimize the model pruning ratio, central processing unit (CPU) frequency and transmission power of the clients in order to minimize the controllable terms of the convergence bound under strict delay and energy constraints. However, since the original problem is not convex, we perform successive convex approximation (SCA) and jointly optimize the parameters fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#21152;&#36895;&#36816;&#21160;&#35268;&#21010;&#20248;&#21270;&#36807;&#31243;&#12290;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2308.01557</link><description>&lt;p&gt;
&#36816;&#21160;&#35268;&#21010;&#25193;&#25955;&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#21152;&#36895;&#36816;&#21160;&#35268;&#21010;&#20248;&#21270;&#36807;&#31243;&#12290;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36712;&#36857;&#20998;&#24067;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#21152;&#24555;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#20248;&#21270;&#12290;&#22312;&#32473;&#23450;&#20808;&#21069;&#25104;&#21151;&#30340;&#35268;&#21010;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#26032;&#35268;&#21010;&#38382;&#39064;&#30340;&#20808;&#39564;&#30693;&#35782;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#36825;&#31181;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#24341;&#23548;&#30340;&#26041;&#27861;&#12290;&#21487;&#20197;&#36890;&#36807;&#20174;&#20808;&#39564;&#30693;&#35782;&#20013;&#37319;&#26679;&#21021;&#22987;&#21270;&#65292;&#25110;&#32773;&#22312;&#26368;&#22823;&#21518;&#39564;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#20808;&#39564;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#21435;&#22122;&#36807;&#31243;&#65292;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#36825;&#23545;&#20110;&#22823;&#37327;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#38750;&#24120;&#36866;&#29992;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;-&#36816;&#21160;&#35268;&#21010;&#25193;&#25955;&#19982;&#20960;&#31181;&#22522;&#20934;&#26041;&#21457;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several ba
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;OpenAI&#30340;ChatGPT&#38598;&#25104;&#21040;&#20855;&#36523;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23558;ChatGPT&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#24182;&#19982;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;98%&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20102;ChatGPT&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01552</link><description>&lt;p&gt;
InterAct: &#25506;&#32034;&#23558;ChatGPT&#20316;&#20026;&#21512;&#20316;&#20195;&#29702;&#20154;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent. (arXiv:2308.01552v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;OpenAI&#30340;ChatGPT&#38598;&#25104;&#21040;&#20855;&#36523;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23558;ChatGPT&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#24182;&#19982;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;98%&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20102;ChatGPT&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;OpenAI&#30340;ChatGPT&#38598;&#25104;&#21040;&#20855;&#36523;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#35780;&#20272;&#20854;&#23545;&#20132;&#20114;&#20915;&#31574;&#22522;&#20934;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;InterAct&#36825;&#19968;&#27010;&#24565;&#65292;&#23558;&#20854;&#31867;&#27604;&#20110;&#20154;&#20204;&#26681;&#25454;&#33258;&#24049;&#29420;&#29305;&#30340;&#20248;&#21183;&#25198;&#28436;&#35282;&#33394;&#30340;&#27010;&#24565;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#32473;ChatGPT&#25552;&#20379;&#21508;&#31181;&#25552;&#31034;&#65292;&#23558;&#20854;&#20998;&#37197;&#20026;&#20687;&#26816;&#26597;&#21592;&#21644;&#20998;&#31867;&#21592;&#36825;&#26679;&#30340;&#22810;&#31181;&#35282;&#33394;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#19982;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;AlfWorld&#20013;&#23637;&#31034;&#20102;98%&#30340;&#26174;&#33879;&#25104;&#21151;&#29575;&#65292;AlfWorld&#26159;&#19968;&#20010;&#27169;&#25311;&#23478;&#24237;&#29615;&#22659;&#20013;&#21253;&#21547;6&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#33391;&#22909;&#30340;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;ChatGPT&#22312;&#29702;&#35299;&#21644;&#39640;&#25928;&#22320;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#20219;&#21153;&#35268;&#21010;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper delves into the integration of OpenAI's ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.
&lt;/p&gt;</description></item><item><title>MusicLDM&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#21644;AudioLDM&#26550;&#26500;&#65292;&#32467;&#21512;&#37325;&#26032;&#35757;&#32451;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;Hifi-GAN&#22768;&#30721;&#22120;&#65292;&#20197;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33410;&#22863;&#21516;&#27493;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#22686;&#24191;&#65292;&#25552;&#39640;&#26032;&#39062;&#24615;&#24182;&#36991;&#20813;&#25220;&#34989;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01546</link><description>&lt;p&gt;
MusicLDM&#65306;&#20351;&#29992;&#33410;&#22863;&#21516;&#27493;&#30340;&#28151;&#21512;&#31574;&#30053;&#22686;&#24378;&#25991;&#26412;&#36716;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#26032;&#39062;&#24615;
&lt;/p&gt;
&lt;p&gt;
MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. (arXiv:2308.01546v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01546
&lt;/p&gt;
&lt;p&gt;
MusicLDM&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#21644;AudioLDM&#26550;&#26500;&#65292;&#32467;&#21512;&#37325;&#26032;&#35757;&#32451;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;Hifi-GAN&#22768;&#30721;&#22120;&#65292;&#20197;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33410;&#22863;&#21516;&#27493;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#22686;&#24191;&#65292;&#25552;&#39640;&#26032;&#39062;&#24615;&#24182;&#36991;&#20813;&#25220;&#34989;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#36328;&#27169;&#24577;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38899;&#20048;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#20197;&#21450;&#19982;&#29256;&#26435;&#21644;&#25220;&#34989;&#30456;&#20851;&#30340;&#25935;&#24863;&#38382;&#39064;&#65292;&#29983;&#25104;&#38899;&#20048;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#38899;&#39057;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#27169;&#22411;MusicLDM&#65292;&#23558;&#31283;&#23450;&#25193;&#25955;&#21644;AudioLDM&#26550;&#26500;&#36866;&#24212;&#21040;&#38899;&#20048;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#38899;&#20048;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;(CLAP)&#21644;Hifi-GAN&#22768;&#30721;&#22120;&#36825;&#20123;MusicLDM&#30340;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#24182;&#36991;&#20813;&#25220;&#34989;&#65292;&#25105;&#20204;&#21033;&#29992;&#33410;&#25293;&#36861;&#36394;&#27169;&#22411;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24191;&#28151;&#21512;&#31574;&#30053;&#65306;&#33410;&#22863;&#21516;&#27493;&#38899;&#39057;&#28151;&#21512;&#21644;&#33410;&#22863;&#21516;&#27493;&#28508;&#22312;&#28151;&#21512;&#65292;&#20998;&#21035;&#36890;&#36807;&#30452;&#25509;&#37325;&#26032;&#32452;&#21512;&#35757;&#32451;&#38899;&#39057;&#25110;&#36890;&#36807;&#28508;&#22312;&#23884;&#20837;&#31354;&#38388;&#37325;&#26032;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;AI&#22686;&#24378;&#30340;&#19978;&#37319;&#26679;&#20316;&#20026;&#35774;&#35745;&#36741;&#21161;&#24037;&#20855;&#65292;&#22312;2D&#28216;&#25103;&#20851;&#21345;&#35774;&#35745;&#20013;&#23454;&#29616;&#21327;&#21516;&#21019;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23558;&#20154;&#24037;&#38477;&#20302;&#20998;&#36776;&#29575;&#30340;&#20851;&#21345;&#29255;&#27573;&#19978;&#37319;&#26679;&#65292;&#24182;&#20026;&#27492;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#19978;&#37319;&#26679;&#24182;&#22788;&#29702;&#19981;&#22826;&#24120;&#35265;&#22270;&#22359;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#32463;&#36807;&#19982;&#35774;&#35745;&#24072;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#35774;&#35745;&#24072;&#20204;&#21916;&#27426;&#36825;&#20010;&#24037;&#20855;&#30340;&#20849;&#21516;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#35748;&#20026;&#23427;&#20855;&#26377;&#28508;&#21147;&#25512;&#21160;&#26356;&#22810;&#30340;&#24320;&#21457;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.01543</link><description>&lt;p&gt;
Lode Enhancer: &#36890;&#36807;&#25193;&#23637;&#20419;&#36827;&#20851;&#21345;&#30340;&#21327;&#21516;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Lode Enhancer: Level Co-creation Through Scaling. (arXiv:2308.01543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;AI&#22686;&#24378;&#30340;&#19978;&#37319;&#26679;&#20316;&#20026;&#35774;&#35745;&#36741;&#21161;&#24037;&#20855;&#65292;&#22312;2D&#28216;&#25103;&#20851;&#21345;&#35774;&#35745;&#20013;&#23454;&#29616;&#21327;&#21516;&#21019;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23558;&#20154;&#24037;&#38477;&#20302;&#20998;&#36776;&#29575;&#30340;&#20851;&#21345;&#29255;&#27573;&#19978;&#37319;&#26679;&#65292;&#24182;&#20026;&#27492;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#19978;&#37319;&#26679;&#24182;&#22788;&#29702;&#19981;&#22826;&#24120;&#35265;&#22270;&#22359;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#32463;&#36807;&#19982;&#35774;&#35745;&#24072;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#35774;&#35745;&#24072;&#20204;&#21916;&#27426;&#36825;&#20010;&#24037;&#20855;&#30340;&#20849;&#21516;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#35748;&#20026;&#23427;&#20855;&#26377;&#28508;&#21147;&#25512;&#21160;&#26356;&#22810;&#30340;&#24320;&#21457;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#21019;&#24314;2D&#28216;&#25103;&#20851;&#21345;&#26102;&#65292;&#20351;&#29992;AI&#22686;&#24378;&#30340;&#19978;&#37319;&#26679;&#20316;&#20026;&#35774;&#35745;&#36741;&#21161;&#24037;&#20855;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#26469;&#33258;&#35868;&#39064;&#24179;&#21488;&#28216;&#25103;Lode Runner&#30340;&#20154;&#24037;&#38477;&#20302;&#20998;&#36776;&#29575;&#30340;&#20851;&#21345;&#29255;&#27573;&#36827;&#34892;&#19978;&#37319;&#26679;&#12290;&#35757;&#32451;&#24471;&#21040;&#30340;&#32593;&#32476;&#34987;&#25972;&#21512;&#21040;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#32534;&#36753;&#22120;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#20197;3&#20010;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#65288;4x4&#12289;8x8&#21644;16x16&#65289;&#21019;&#24314;&#21644;&#32534;&#36753;&#20851;&#21345;&#12290;&#22312;&#20219;&#20309;&#20998;&#36776;&#29575;&#19978;&#30340;&#32534;&#36753;&#37117;&#20250;&#31435;&#21363;&#20256;&#36755;&#21040;&#20854;&#20182;&#20998;&#36776;&#29575;&#19978;&#12290;&#30001;&#20110;&#19978;&#37319;&#26679;&#38656;&#35201;&#21457;&#26126;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#19979;&#21487;&#33021;&#19981;&#23384;&#22312;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#22797;&#21046;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#23398;&#20064;&#19978;&#37319;&#26679;&#65292;&#36824;&#33021;&#22815;&#26356;&#39640;&#20248;&#20808;&#32423;&#22320;&#22788;&#29702;&#19981;&#22826;&#24120;&#35265;&#30340;&#22270;&#22359;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#24037;&#20855;&#30340;&#28508;&#21147;&#24182;&#25351;&#23548;&#36827;&#19968;&#27493;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23450;&#24615;&#30740;&#31350;&#65292;&#19982;3&#20301;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#20102;&#35299;&#20182;&#20204;&#22914;&#20309;&#20351;&#29992;&#23427;&#12290;&#35774;&#35745;&#24072;&#20139;&#21463;&#19982;&#36825;&#20010;&#24037;&#20855;&#30340;&#20849;&#21516;&#35774;&#35745;&#36807;&#31243;&#65292;&#21916;&#27426;&#23427;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore AI-powered upscaling as a design assistance tool in the context of creating 2D game levels. Deep neural networks are used to upscale artificially downscaled patches of levels from the puzzle platformer game Lode Runner. The trained networks are incorporated into a web-based editor, where the user can create and edit levels at three different levels of resolution: 4x4, 8x8, and 16x16. An edit at any resolution instantly transfers to the other resolutions. As upscaling requires inventing features that might not be present at lower resolutions, we train neural networks to reproduce these features. We introduce a neural network architecture that is capable of not only learning upscaling but also giving higher priority to less frequent tiles. To investigate the potential of this tool and guide further development, we conduct a qualitative study with 3 designers to understand how they use it. Designers enjoyed co-designing with the tool, liked its underlying concept, and provided 
&lt;/p&gt;</description></item><item><title>MFIM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#33080;&#20132;&#25442;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#30334;&#19975;&#20687;&#32032;&#22270;&#20687;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#32473;&#23450;&#22270;&#20687;&#30340;&#36523;&#20221;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#20154;&#30340;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#30041;&#36523;&#20221;&#26080;&#20851;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01536</link><description>&lt;p&gt;
MFIM: &#30334;&#19975;&#20687;&#32032;&#20154;&#33080;&#36523;&#20221;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
MFIM: Megapixel Facial Identity Manipulation. (arXiv:2308.01536v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01536
&lt;/p&gt;
&lt;p&gt;
MFIM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#33080;&#20132;&#25442;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#30334;&#19975;&#20687;&#32032;&#22270;&#20687;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#32473;&#23450;&#22270;&#20687;&#30340;&#36523;&#20221;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#20154;&#30340;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#30041;&#36523;&#20221;&#26080;&#20851;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#20132;&#25442;&#26159;&#19968;&#39033;&#23558;&#32473;&#23450;&#22270;&#20687;&#30340;&#20154;&#33080;&#36523;&#20221;&#26356;&#25913;&#20026;&#21478;&#19968;&#20010;&#20154;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30334;&#19975;&#20687;&#32032;&#20154;&#33080;&#36523;&#20221;&#25805;&#32437;&#65288;MFIM&#65289;&#30340;&#26032;&#39062;&#20154;&#33080;&#20132;&#25442;&#26694;&#26550;&#12290;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#23454;&#29616;&#20004;&#20010;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25797;&#38271;&#29983;&#25104;&#30334;&#19975;&#20687;&#32032;&#22270;&#20687;&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#35201;&#24819;&#29983;&#25104;&#30334;&#19975;&#20687;&#32032;&#22270;&#20687;&#36890;&#24120;&#20250;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#20180;&#32454;&#35774;&#35745;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;StyleGAN&#20197;GAN-&#36870;&#21521;&#30340;&#26041;&#24335;&#26469;&#26377;&#25928;&#22320;&#29983;&#25104;&#30334;&#19975;&#20687;&#32032;&#22270;&#20687;&#12290;&#20854;&#27425;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#26377;&#25928;&#22320;&#36716;&#25442;&#32473;&#23450;&#22270;&#20687;&#30340;&#36523;&#20221;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#23558;&#32473;&#23450;&#22270;&#20687;&#30340;&#36523;&#20221;&#23646;&#24615;&#65288;&#22914;&#33080;&#22411;&#21644;&#30524;&#30555;&#65289;&#31215;&#26497;&#22320;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#20154;&#30340;&#23646;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#36523;&#20221;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#23039;&#21183;&#21644;&#34920;&#24773;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#38754;&#37096;&#29305;&#24449;&#30340;3DMM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face swapping is a task that changes a facial identity of a given image to that of another person. In this work, we propose a novel face-swapping framework called Megapixel Facial Identity Manipulation (MFIM). The face-swapping model should achieve two goals. First, it should be able to generate a high-quality image. We argue that a model which is proficient in generating a megapixel image can achieve this goal. However, generating a megapixel image is generally difficult without careful model design. Therefore, our model exploits pretrained StyleGAN in the manner of GAN-inversion to effectively generate a megapixel image. Second, it should be able to effectively transform the identity of a given image. Specifically, it should be able to actively transform ID attributes (e.g., face shape and eyes) of a given image into those of another person, while preserving ID-irrelevant attributes (e.g., pose and expression). To achieve this goal, we exploit 3DMM that can capture various facial att
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#21024;&#38500;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20351;&#29992;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>http://arxiv.org/abs/2308.01508</link><description>&lt;p&gt;
&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Circumventing Concept Erasure Methods For Text-to-Image Generative Models. (arXiv:2308.01508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#21024;&#38500;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20351;&#29992;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20026;&#26497;&#20854;&#24191;&#27867;&#30340;&#27010;&#24565;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20351;&#29992;&#22312;&#26222;&#36890;&#20844;&#20247;&#20013;&#24191;&#27867;&#26222;&#21450;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#35768;&#22810;&#32570;&#28857;&#65292;&#21253;&#25324;&#21487;&#33021;&#29983;&#25104;&#20855;&#26377;&#24615;&#21035;&#26292;&#38706;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#26410;&#32463;&#35768;&#21487;&#22320;&#27169;&#20223;&#33402;&#26415;&#39118;&#26684;&#65292;&#29978;&#33267;&#26159;&#28145;&#24230;&#20266;&#36896;&#21517;&#20154;&#30340;&#26679;&#35980;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#8220;&#25830;&#38500;&#8221;&#25935;&#24863;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20116;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#37117;&#26080;&#27861;&#23436;&#20840;&#21024;&#38500;&#30446;&#26631;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29305;&#27530;&#30340;&#23398;&#20064;&#35789;&#23884;&#20837;&#30340;&#23384;&#22312;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#23545;&#20854;&#26435;&#37325;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#32463;&#36807;&#22788;&#29702;&#30340;&#27169;&#22411;&#20013;&#26816;&#32034;&#8220;&#21024;&#38500;&#8221;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#20107;&#21518;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#31639;&#27861;&#24037;&#20855;&#21253;&#20013;&#30340;&#20351;&#29992;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to "erase" sensitive concepts from text-to-image models. In this work, we examine five recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve "erased" concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20855;&#26377;&#26368;&#36817;&#37051;&#30340;$Q$&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#19979;&#25910;&#25947;&#36895;&#24230;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01490</link><description>&lt;p&gt;
&#20855;&#26377;&#26368;&#36817;&#37051;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;$Q$&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimal $Q$ Learning with Nearest Neighbors. (arXiv:2308.01490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20855;&#26377;&#26368;&#36817;&#37051;&#30340;$Q$&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#19979;&#25910;&#25947;&#36895;&#24230;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$Q$&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20998;&#26512;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;$Q$&#23398;&#20064;&#12290;&#22914;&#26524;&#29366;&#24577;&#31354;&#38388;&#26159;&#36830;&#32493;&#30340;&#65292;&#37027;&#20040;&#21407;&#22987;&#30340;$Q$&#23398;&#20064;&#26041;&#27861;&#23601;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#12290;(Shah and Xie, 2018) &#25552;&#20986;&#20102;&#21407;&#22987;$Q$&#23398;&#20064;&#26041;&#27861;&#30340;&#20462;&#25913;&#29256;&#65292;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#20272;&#35745;$Q$&#20540;&#12290;&#36825;&#31181;&#20462;&#25913;&#20351;&#24471;$Q$&#23398;&#20064;&#36866;&#29992;&#20110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;&#35813;&#35770;&#25991;&#25351;&#20986;&#20272;&#35745;$Q$&#20989;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#20026;$\tilde{O}(T^{-1/(d+3)})$&#65292;&#27604;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;$\tilde{\Omega}(T^{-1/(d+2)})$&#24930;&#65292;&#35828;&#26126;&#35813;&#26041;&#27861;&#25928;&#29575;&#19981;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;$Q$&#23398;&#20064;&#26041;&#27861;&#65292;&#26469;&#24357;&#21512;(Shah and Xie, 2018)&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#24046;&#36317;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#31163;&#32447;&#30340;&#65292;&#21478;&#19968;&#31181;&#26159;&#22312;&#32447;&#30340;&#12290;&#23613;&#31649;&#25105;&#20204;&#20173;&#28982;&#20351;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#26469;&#20272;&#35745;$Q$&#20989;&#25968;&#65292;&#20294;&#31639;&#27861;&#19982;(Shah and Xie, 2018)&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
$Q$ learning is a popular model free reinforcement learning method. Most of existing works focus on analyzing $Q$ learning for finite state and action spaces. If the state space is continuous, then the original $Q$ learning method can not be directly used. A modification of the original $Q$ learning method was proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearest neighbors. Such modification makes $Q$ learning suitable for continuous state space. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$ function is $\tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lower bound $\tilde{\Omega}(T^{-1/(d+2)})$, indicating that this method is not efficient. This paper proposes two new $Q$ learning methods to bridge the gap of convergence rates in (Shah and Xie, 2018), with one of them being offline, while the other is online. Despite that we still use nearest neighbor approach to estimate $Q$ function, the algorithms are crucially different from (Sh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#28216;&#25103;&#20869;&#23481;&#19978;&#36827;&#34892;&#36229;&#37319;&#26679;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;&#25928;4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25512;&#21160;&#28216;&#25103;&#20869;&#23481;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.01483</link><description>&lt;p&gt;
&#22312;&#26032;&#22411;&#28216;&#25103;&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#25928;&#31070;&#32463;&#36229;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Efficient neural supersampling on a novel gaming dataset. (arXiv:2308.01483v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#28216;&#25103;&#20869;&#23481;&#19978;&#36827;&#34892;&#36229;&#37319;&#26679;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;&#25928;4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25512;&#21160;&#28216;&#25103;&#20869;&#23481;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38656;&#35201;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#12289;&#24103;&#29575;&#21644;&#36924;&#30495;&#24230;&#65292;&#23454;&#26102;&#28210;&#26579;&#35270;&#39057;&#28216;&#25103;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36229;&#37319;&#26679;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#36229;&#37319;&#26679;&#28210;&#26579;&#20869;&#23481;&#65292;&#20854;&#25928;&#29575;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#25552;&#20379;&#20102;&#36741;&#21161;&#27169;&#24577;&#65292;&#20363;&#22914;&#20351;&#29992;&#22270;&#24418;&#28210;&#26579;&#21151;&#33021;&#29983;&#25104;&#30340;&#36816;&#21160;&#30690;&#37327;&#21644;&#28145;&#24230;&#65292;&#22914;&#35270;&#21475;&#25238;&#21160;&#21644;&#22810;&#32423;&#32441;&#29702;&#20559;&#24046;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19979;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#22635;&#34917;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#24110;&#21161;&#34913;&#37327;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#24182;&#25512;&#21160;&#28216;&#25103;&#20869;&#23481;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time rendering for video games has become increasingly challenging due to the need for higher resolutions, framerates and photorealism. Supersampling has emerged as an effective solution to address this challenge. Our work introduces a novel neural algorithm for supersampling rendered content that is 4 times more efficient than existing methods while maintaining the same level of accuracy. Additionally, we introduce a new dataset which provides auxiliary modalities such as motion vectors and depth generated using graphics rendering features like viewport jittering and mipmap biasing at different resolutions. We believe that this dataset fills a gap in the current dataset landscape and can serve as a valuable resource to help measure progress in the field and advance the state-of-the-art in super-resolution techniques for gaming content.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#22312;&#32447;&#37325;&#21472;&#25209;&#27425;&#22343;&#20540;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#36895;&#29575;&#20026;$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$&#21644;$O\big(\sqrt{d}\,n^{-1/8}\big)$&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#29366;&#24577;&#30456;&#20851;&#21644;&#29366;&#24577;&#26080;&#20851;&#30340;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#12290;&#36825;&#20123;&#36895;&#29575;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#20811;&#26381;&#20102;&#30001;&#20110;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.01481</link><description>&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#65292;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#22312;&#32447;&#21327;&#26041;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#22312;&#32447;&#37325;&#21472;&#25209;&#27425;&#22343;&#20540;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#36895;&#29575;&#20026;$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$&#21644;$O\big(\sqrt{d}\,n^{-1/8}\big)$&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#29366;&#24577;&#30456;&#20851;&#21644;&#29366;&#24577;&#26080;&#20851;&#30340;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#12290;&#36825;&#20123;&#36895;&#29575;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#20811;&#26381;&#20102;&#30001;&#20110;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#22312;&#32447;&#37325;&#21472;&#25209;&#27425;&#22343;&#20540;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$&#21644;$O\big(\sqrt{d}\,n^{-1/8}\big)$&#65292;&#20854;&#20013;$d$&#20195;&#34920;&#32500;&#24230;&#65292;$n$&#34920;&#31034;&#35266;&#27979;&#25968;&#37327;&#25110;SGD&#36845;&#20195;&#27425;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#36895;&#29575;&#19982;&#20808;&#21069;&#30001;\cite{zhu2021online}&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;($\iid$)&#24773;&#20917;&#19979;&#24314;&#31435;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#65292;&#38500;&#20102;&#23545;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20811;&#26381;&#20102;&#30001;&#20110;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#32780;&#20135;&#29983;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#35823;&#24046;&#39033;&#21644;&#25209;&#27425;&#22343;&#20540;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;SGD&#21160;&#24577;&#35823;&#24046;$\ell_2$&#33539;&#25968;&#30340;&#21069;&#22235;&#38454;&#30697;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and $O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\iid$) case by \cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\ell_2$ norm of the error of SGD dynamics u
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34987;&#24191;&#27867;&#29992;&#20110;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#12289;&#21487;&#35270;&#21270;&#39044;&#27979;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#21457;&#29616;&#65292;&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#36825;&#19968;&#39046;&#22495;&#24182;&#25506;&#35752;&#20102;&#39564;&#35777;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.01475</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21457;&#29616;&#65306;&#32479;&#35745;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for Discovery: Statistical Challenges \&amp; Opportunities. (arXiv:2308.01475v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01475
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34987;&#24191;&#27867;&#29992;&#20110;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#12289;&#21487;&#35270;&#21270;&#39044;&#27979;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#21457;&#29616;&#65292;&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#36825;&#19968;&#39046;&#22495;&#24182;&#25506;&#35752;&#20102;&#39564;&#35777;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#25216;&#26415;&#23548;&#33268;&#20102;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#21644;&#34892;&#19994;&#30340;&#24222;&#22823;&#12289;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#22788;&#29702;&#12289;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#36825;&#20123;&#22823;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25216;&#26415;&#26469;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#21457;&#29616;&#12290;&#26412;&#25991;&#35752;&#35770;&#21644;&#22238;&#39038;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#29305;&#21035;&#20851;&#27880;&#36825;&#20123;&#25216;&#26415;&#22312;&#20174;&#22823;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#26032;&#30693;&#35782;&#25110;&#36827;&#34892;&#21457;&#29616;&#26102;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#22330;&#26223;&#19979;&#21487;&#20197;&#23454;&#29616;&#30340;&#21457;&#29616;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#39564;&#35777;&#36825;&#20123;&#21457;&#29616;&#65292;&#20197;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20449;&#20219;&#21644;&#31185;&#23398;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39564;&#35777;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
New technologies have led to vast troves of large and complex datasets across many scientific domains and industries. People routinely use machine learning techniques to not only process, visualize, and make predictions from this big data, but also to make data-driven discoveries. These discoveries are often made using Interpretable Machine Learning, or machine learning models and techniques that yield human understandable insights. In this paper, we discuss and review the field of interpretable machine learning, focusing especially on the techniques as they are often employed to generate new knowledge or make discoveries from large data sets. We outline the types of discoveries that can be made using Interpretable Machine Learning in both supervised and unsupervised settings. Additionally, we focus on the grand challenge of how to validate these discoveries in a data-driven manner, which promotes trust in machine learning systems and reproducibility in science. We discuss validation f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01472</link><description>&lt;p&gt;
&#21453;&#21521;&#31283;&#23450;&#25193;&#25955;&#65306;&#29983;&#25104;&#35813;&#22270;&#20687;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#26368;&#36817;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#29983;&#25104;&#36807;&#31243;&#21644;&#22914;&#20309;&#35774;&#35745;&#25552;&#31034;&#20197;&#33719;&#24471;&#25152;&#38656;&#22270;&#20687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65288;&#26377;&#21644;&#26080;&#23545;&#25193;&#25955;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#35775;&#38382;&#65289;&#26469;&#22788;&#29702;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#32852;&#21512;&#25552;&#31034;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#35789;&#27719;&#20998;&#31867;&#30446;&#26631;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#20855;&#26377;&#26356;&#20302;&#26631;&#27880;&#22122;&#22768;&#65288;&#21363;&#26356;&#22909;&#23545;&#40784;&#65289;&#30340;&#22270;&#20687;&#25552;&#31034;&#23545;&#30340;&#23398;&#20064;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#20284;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24863;&#30693;&#21644;&#26410;&#26469;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38544;&#24335;&#21344;&#29992;&#27969;&#22330;&#26469;&#34920;&#31034;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#24863;&#30693;&#24182;&#39044;&#27979;&#20854;&#20182;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#21644;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01471</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#38544;&#24335;&#21344;&#29992;&#27969;&#22330;&#24212;&#29992;&#20110;&#24863;&#30693;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving. (arXiv:2308.01471v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24863;&#30693;&#21644;&#26410;&#26469;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38544;&#24335;&#21344;&#29992;&#27969;&#22330;&#26469;&#34920;&#31034;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#24863;&#30693;&#24182;&#39044;&#27979;&#20854;&#20182;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#21644;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24517;&#39035;&#33021;&#22815;&#24863;&#30693;&#21608;&#22260;&#29615;&#22659;&#24182;&#39044;&#27979;&#20854;&#20182;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#65292;&#28982;&#21518;&#23545;&#26816;&#27979;&#21040;&#30340;&#30446;&#26631;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#65292;&#35201;&#20040;&#39044;&#27979;&#25972;&#20010;&#22330;&#26223;&#30340;&#23494;&#38598;&#21344;&#29992;&#21644;&#27969;&#26684;&#12290;&#21069;&#32773;&#30001;&#20110;&#25928;&#29575;&#21407;&#22240;&#38656;&#35201;&#20445;&#25345;&#26816;&#27979;&#25968;&#37327;&#36739;&#23569;&#65292;&#20174;&#32780;&#29306;&#29298;&#20102;&#23545;&#35937;&#30340;&#22238;&#24518;&#29575;&#65292;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#12290;&#21518;&#32773;&#30001;&#20110;&#36755;&#20986;&#26684;&#30340;&#39640;&#32500;&#24615;&#21644;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#22266;&#26377;&#30340;&#26377;&#38480;&#24863;&#21463;&#37326;&#32780;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#39044;&#27979;&#21487;&#33021;&#27704;&#36828;&#19981;&#20250;&#34987;&#36816;&#21160;&#35268;&#21010;&#22120;&#26597;&#35810;&#30340;&#21306;&#22495;&#25110;&#23545;&#35937;&#12290;&#37492;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24863;&#30693;&#21644;&#26410;&#26469;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#34920;&#31034;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#21344;&#29992;&#21644;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#30452;&#25509;&#34987;&#36816;&#21160;&#35268;&#21010;&#22120;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
A self-driving vehicle (SDV) must be able to perceive its surroundings and predict the future behavior of other traffic participants. Existing works either perform object detection followed by trajectory forecasting of the detected objects, or predict dense occupancy and flow grids for the whole scene. The former poses a safety concern as the number of detections needs to be kept low for efficiency reasons, sacrificing object recall. The latter is computationally expensive due to the high-dimensionality of the output grid, and suffers from the limited receptive field inherent to fully convolutional networks. Furthermore, both approaches employ many computational resources predicting areas or objects that might never be queried by the motion planner. This motivates our unified approach to perception and future prediction that implicitly represents occupancy and flow over time with a single neural network. Our method avoids unnecessary computation, as it can be directly queried by the mo
&lt;/p&gt;</description></item><item><title>VertexSerum&#26159;&#19968;&#31181;&#38024;&#23545;&#38142;&#36335;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27602;&#21270;&#25915;&#20987;&#65292;&#36890;&#36807;&#25918;&#22823;&#38142;&#25509;&#36830;&#25509;&#24615;&#27844;&#28431;&#26469;&#22686;&#21152;&#22270;&#38142;&#25509;&#31363;&#21462;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23884;&#20837;&#21040;&#38142;&#25509;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;VertexSerum&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;GNN&#32467;&#26500;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;9.8&#65285;&#30340;AUC&#20998;&#25968;&#65292;&#19988;&#22312;&#40657;&#30418;&#21644;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#22343;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01469</link><description>&lt;p&gt;
VertexSerum: &#38024;&#23545;&#38142;&#36335;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
VertexSerum: Poisoning Graph Neural Networks for Link Inference. (arXiv:2308.01469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01469
&lt;/p&gt;
&lt;p&gt;
VertexSerum&#26159;&#19968;&#31181;&#38024;&#23545;&#38142;&#36335;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27602;&#21270;&#25915;&#20987;&#65292;&#36890;&#36807;&#25918;&#22823;&#38142;&#25509;&#36830;&#25509;&#24615;&#27844;&#28431;&#26469;&#22686;&#21152;&#22270;&#38142;&#25509;&#31363;&#21462;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23884;&#20837;&#21040;&#38142;&#25509;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;VertexSerum&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;GNN&#32467;&#26500;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;9.8&#65285;&#30340;AUC&#20998;&#25968;&#65292;&#19988;&#22312;&#40657;&#30418;&#21644;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#22343;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21033;&#29992;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#65288;&#22914;&#31038;&#20132;&#20998;&#26512;&#21644;&#27450;&#35784;&#26816;&#27979;&#65289;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22270;&#30340;&#38142;&#25509;&#65288;&#20363;&#22914;&#31038;&#20132;&#20851;&#31995;&#21644;&#20132;&#26131;&#21382;&#21490;&#65289;&#26159;&#25935;&#24863;&#21644;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;GNNs&#26102;&#20250;&#24341;&#36215;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VertexSerum&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27602;&#21270;&#25915;&#20987;&#65292;&#36890;&#36807;&#25918;&#22823;&#38142;&#25509;&#36830;&#25509;&#24615;&#27844;&#28431;&#26469;&#22686;&#21152;&#22270;&#38142;&#25509;&#31363;&#21462;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#33410;&#28857;&#37051;&#25509;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23884;&#20837;&#21040;&#38142;&#25509;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;GNN&#32467;&#26500;&#19978;&#65292;VertexSerum&#30340;AUC&#20998;&#25968;&#24179;&#22343;&#25552;&#39640;&#20102;9.8&#65285;&#65292;&#26174;&#33879;&#20248;&#20110;SOTA&#38142;&#36335;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36824;&#39564;&#35777;&#20102;VertexSerum&#22312;&#40657;&#30418;&#21644;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have brought superb performance to various applications utilizing graph structural data, such as social analysis and fraud detection. The graph links, e.g., social relationships and transaction history, are sensitive and valuable information, which raises privacy concerns when using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel graph poisoning attack that increases the effectiveness of graph link stealing by amplifying the link connectivity leakage. To infer node adjacency more accurately, we propose an attention mechanism that can be embedded into the link detection network. Our experiments demonstrate that VertexSerum significantly outperforms the SOTA link inference attack, improving the AUC scores by an average of $9.8\%$ across four real-world datasets and three different GNN structures. Furthermore, our experiments reveal the effectiveness of VertexSerum in both black-box and online learning settings, further validating its a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22303;&#26408;&#24037;&#31243;&#32467;&#26500;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#32534;&#30721;&#36164;&#20135;&#23402;&#29983;&#32806;&#21512;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21516;&#21270;&#24863;&#27979;&#25968;&#25454;&#26469;&#25552;&#20379;&#23454;&#26102;&#30340;&#32467;&#26500;&#20581;&#24247;&#35786;&#26029;&#65292;&#19981;&#26029;&#26356;&#26032;&#25968;&#23383;&#23402;&#29983;&#29366;&#24577;&#24182;&#29992;&#20110;&#20248;&#21270;&#32500;&#25252;&#21644;&#31649;&#29702;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2308.01445</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#22303;&#26408;&#24037;&#31243;&#32467;&#26500;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A digital twin framework for civil engineering structures. (arXiv:2308.01445v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22303;&#26408;&#24037;&#31243;&#32467;&#26500;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#32534;&#30721;&#36164;&#20135;&#23402;&#29983;&#32806;&#21512;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21516;&#21270;&#24863;&#27979;&#25968;&#25454;&#26469;&#25552;&#20379;&#23454;&#26102;&#30340;&#32467;&#26500;&#20581;&#24247;&#35786;&#26029;&#65292;&#19981;&#26029;&#26356;&#26032;&#25968;&#23383;&#23402;&#29983;&#29366;&#24577;&#24182;&#29992;&#20110;&#20248;&#21270;&#32500;&#25252;&#21644;&#31649;&#29702;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#27010;&#24565;&#20026;&#22303;&#26408;&#24037;&#31243;&#31995;&#32479;&#30340;&#22522;&#20110;&#26465;&#20214;&#21644;&#39044;&#27979;&#32500;&#25252;&#33539;&#24335;&#30340;&#25512;&#36827;&#25552;&#20379;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#38477;&#20302;&#29983;&#21629;&#21608;&#26399;&#25104;&#26412;&#12289;&#22686;&#21152;&#31995;&#32479;&#23433;&#20840;&#24615;&#21644;&#25552;&#39640;&#31995;&#32479;&#21487;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#24615;&#30340;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#29992;&#20110;&#22303;&#26408;&#24037;&#31243;&#32467;&#26500;&#30340;&#20581;&#24247;&#30417;&#27979;&#12289;&#32500;&#25252;&#21644;&#31649;&#29702;&#35268;&#21010;&#12290;&#36164;&#20135;&#23402;&#29983;&#32806;&#21512;&#21160;&#24577;&#31995;&#32479;&#37319;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#21487;&#20197;&#32771;&#34385;&#21040;&#25152;&#26377;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#29305;&#21035;&#26159;&#65292;&#37319;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#23545;&#26102;&#38388;&#37325;&#22797;&#30340;&#35266;&#27979;-&#20915;&#31574;&#27969;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23558;&#24863;&#27979;&#25968;&#25454;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21516;&#21270;&#65292;&#23454;&#26102;&#30340;&#32467;&#26500;&#20581;&#24247;&#35786;&#26029;&#24471;&#20197;&#25552;&#20379;&#12290;&#25968;&#23383;&#23402;&#29983;&#29366;&#24577;&#20197;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#26041;&#24335;&#19981;&#26029;&#26356;&#26032;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#21160;&#24577;&#20915;&#31574;&#19979;&#30340;&#32500;&#25252;&#21644;&#31649;&#29702;&#34892;&#21160;&#30340;&#26368;&#20248;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digital twin concept represents an appealing opportunity to advance condition-based and predictive maintenance paradigms for civil engineering systems, thus allowing reduced lifecycle costs, increased system safety, and increased system availability. This work proposes a predictive digital twin approach to the health monitoring, maintenance, and management planning of civil engineering structures. The asset-twin coupled dynamical system is encoded employing a probabilistic graphical model, which allows all relevant sources of uncertainty to be taken into account. In particular, the time-repeating observations-to-decisions flow is modeled using a dynamic Bayesian network. Real-time structural health diagnostics are provided by assimilating sensed data with deep learning models. The digital twin state is continually updated in a sequential Bayesian inference fashion. This is then exploited to inform the optimal planning of maintenance and management actions within a dynamic decision-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20845;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#36817;&#20284;&#23460;&#20869;&#27745;&#26579;&#29289;&#27987;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#29366;&#24577;&#31354;&#38388;&#27010;&#24565;&#12289;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#20998;&#35299;&#25216;&#26415;&#12290;&#36890;&#36807;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#19968;&#26635;&#21830;&#19994;&#24314;&#31569;&#20013;&#20116;&#20010;&#21150;&#20844;&#23460;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.01438</link><description>&lt;p&gt;
&#23460;&#20869;&#31354;&#27668;&#36136;&#37327;&#36817;&#20284;&#30340;&#26032;&#39062;&#22522;&#20110;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations. (arXiv:2308.01438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20845;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#36817;&#20284;&#23460;&#20869;&#27745;&#26579;&#29289;&#27987;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#29366;&#24577;&#31354;&#38388;&#27010;&#24565;&#12289;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#20998;&#35299;&#25216;&#26415;&#12290;&#36890;&#36807;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#19968;&#26635;&#21830;&#19994;&#24314;&#31569;&#20013;&#20116;&#20010;&#21150;&#20844;&#23460;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#26412;&#20302;&#24265;&#30340;&#20256;&#24863;&#22120;&#33021;&#22815;&#23454;&#26102;&#25429;&#25417;&#21040;&#19981;&#21516;&#27745;&#26579;&#29289;&#27987;&#24230;&#12289;&#23460;&#20869;/&#23460;&#22806;&#28287;&#24230;&#21644;&#28201;&#24230;&#31561;&#19982;&#31354;&#27668;&#36136;&#37327;&#30456;&#20851;&#30340;&#22810;&#31181;&#27169;&#24577;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#25552;&#21069;&#36817;&#20284;&#23460;&#20869;&#31354;&#27668;&#36136;&#37327;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;&#20934;&#30830;&#30340;&#23460;&#20869;&#31354;&#27668;&#36136;&#37327;&#36817;&#20284;&#26377;&#21161;&#20110;&#25552;&#20379;&#20581;&#24247;&#30340;&#23460;&#20869;&#29615;&#22659;&#65292;&#20248;&#21270;&#30456;&#20851;&#33021;&#32791;&#65292;&#24182;&#25552;&#20379;&#20154;&#20307;&#33298;&#36866;&#24230;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#25152;&#35859;&#30340;&#38382;&#39064;&#29289;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#36817;&#20284;&#23460;&#20869;&#27745;&#26579;&#29289;&#27987;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21253;&#25324;&#29289;&#29702;&#23398;&#20013;&#30340;&#29366;&#24577;&#31354;&#38388;&#27010;&#24565;&#12289;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#20998;&#35299;&#25216;&#26415;&#30340;&#24039;&#22937;&#32452;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#20174;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#19968;&#26635;&#21830;&#19994;&#24314;&#31569;&#20013;&#20116;&#20010;&#21150;&#20844;&#23460;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36739;&#20026;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cost-effective sensors are capable of real-time capturing a variety of air quality-related modalities from different pollutant concentrations to indoor/outdoor humidity and temperature. Machine learning (ML) models are capable of performing air-quality "ahead-of-time" approximations. Undoubtedly, accurate indoor air quality approximation significantly helps provide a healthy indoor environment, optimize associated energy consumption, and offer human comfort. However, it is crucial to design an ML architecture to capture the domain knowledge, so-called problem physics. In this study, we propose six novel physics-based ML models for accurate indoor pollutant concentration approximations. The proposed models include an adroit combination of state-space concepts in physics, Gated Recurrent Units, and Decomposition techniques. The proposed models were illustrated using data collected from five offices in a commercial building in California. The proposed models are shown to be less complex, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30005;&#21147;&#24066;&#22330;&#28165;&#31639;&#20248;&#21270;&#23884;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#23618;&#20013;&#65292;&#24179;&#34913;&#39044;&#27979;&#35823;&#24046;&#21644;&#23450;&#20215;&#35823;&#24046;&#65292;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#24182;&#25511;&#21046;&#20215;&#26684;&#35823;&#24046;&#30340;&#31354;&#38388;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2308.01436</link><description>&lt;p&gt;
&#30005;&#21147;&#24066;&#22330;&#30340;&#20215;&#26684;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Price-Aware Deep Learning for Electricity Markets. (arXiv:2308.01436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30005;&#21147;&#24066;&#22330;&#28165;&#31639;&#20248;&#21270;&#23884;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#23618;&#20013;&#65292;&#24179;&#34913;&#39044;&#27979;&#35823;&#24046;&#21644;&#23450;&#20215;&#35823;&#24046;&#65292;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#24182;&#25511;&#21046;&#20215;&#26684;&#35823;&#24046;&#30340;&#31354;&#38388;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#36880;&#28176;&#28183;&#36879;&#21040;&#36816;&#33829;&#35268;&#21010;&#20013;&#65292;&#20294;&#20854;&#22266;&#26377;&#30340;&#39044;&#27979;&#35823;&#24046;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#30005;&#21147;&#20215;&#26684;&#12290;&#26412;&#25991;&#32771;&#23519;&#20102;&#39044;&#27979;&#35823;&#24046;&#22914;&#20309;&#20256;&#25773;&#21040;&#30005;&#21147;&#20215;&#26684;&#20013;&#65292;&#25581;&#31034;&#20102;&#25317;&#25380;&#30005;&#21147;&#31995;&#32479;&#20013;&#26174;&#33879;&#30340;&#23450;&#20215;&#35823;&#24046;&#21450;&#20854;&#31354;&#38388;&#24046;&#24322;&#12290;&#20026;&#20102;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#30005;&#21147;&#24066;&#22330;&#28165;&#31639;&#20248;&#21270;&#23884;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#23618;&#20013;&#12290;&#36890;&#36807;&#27492;&#23618;&#30340;&#21306;&#20998;&#65292;&#21487;&#20197;&#22312;&#39044;&#27979;&#35823;&#24046;&#21644;&#23450;&#20215;&#35823;&#24046;&#20043;&#38388;&#24179;&#34913;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#12290;&#35813;&#23618;&#38544;&#24335;&#22320;&#20248;&#21270;&#20844;&#24179;&#24615;&#65292;&#24182;&#25511;&#21046;&#31995;&#32479;&#20013;&#20215;&#26684;&#35823;&#24046;&#30340;&#31354;&#38388;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#21644;&#30701;&#26399;&#30005;&#21147;&#24066;&#22330;&#28165;&#31639;&#20013;&#30340;&#20215;&#26684;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning gradually penetrates operational planning, its inherent prediction errors may significantly affect electricity prices. This letter examines how prediction errors propagate into electricity prices, revealing notable pricing errors and their spatial disparity in congested power systems. To improve fairness, we propose to embed electricity market-clearing optimization as a deep learning layer. Differentiating through this layer allows for balancing between prediction and pricing errors, as oppose to minimizing prediction errors alone. This layer implicitly optimizes fairness and controls the spatial distribution of price errors across the system. We showcase the price-aware deep learning in the nexus of wind power forecasting and short-term electricity market clearing.
&lt;/p&gt;</description></item><item><title>COVID-VR&#26159;&#19968;&#31181;&#20351;&#29992;&#20307;&#31215;&#28210;&#26579;&#22270;&#20687;&#35782;&#21035;&#32954;&#37096;&#30142;&#30149;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;COVID-VR&#22312;&#25972;&#20010;&#32954;&#37096;&#25552;&#20379;&#20102;&#32508;&#21512;&#35270;&#22270;&#65292;&#24182;&#33021;&#26377;&#25928;&#35782;&#21035;&#32954;&#37096;&#30149;&#21464;&#12290;</title><link>http://arxiv.org/abs/2308.01433</link><description>&lt;p&gt;
COVID-VR:&#19968;&#31181;&#20351;&#29992;&#20307;&#31215;&#28210;&#26579;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#28145;&#24230;&#23398;&#20064;COVID-19&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography. (arXiv:2308.01433v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01433
&lt;/p&gt;
&lt;p&gt;
COVID-VR&#26159;&#19968;&#31181;&#20351;&#29992;&#20307;&#31215;&#28210;&#26579;&#22270;&#20687;&#35782;&#21035;&#32954;&#37096;&#30142;&#30149;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;COVID-VR&#22312;&#25972;&#20010;&#32954;&#37096;&#25552;&#20379;&#20102;&#32508;&#21512;&#35270;&#22270;&#65292;&#24182;&#33021;&#26377;&#25928;&#35782;&#21035;&#32954;&#37096;&#30149;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#27969;&#34892;&#23545;&#20840;&#29699;&#30340;&#21307;&#30103;&#31995;&#32479;&#25552;&#20986;&#20102;&#20247;&#22810;&#25361;&#25112;&#12290;&#37492;&#20110;&#32954;&#37096;&#24863;&#26579;&#22312;COVID-19&#24739;&#32773;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#32463;&#24120;&#34987;&#29992;&#20316;&#35782;&#21035;COVID-19&#30149;&#24773;&#21644;&#20854;&#20182;&#32954;&#37096;&#30142;&#30149;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#21033;&#29992;CT&#25195;&#25551;&#20999;&#29255;&#20316;&#20026;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#33258;&#21160;&#35782;&#21035;&#32954;&#37096;&#30142;&#30149;&#31867;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;COVID-VR&#65292;&#19968;&#31181;&#22522;&#20110;&#20174;&#22810;&#20010;&#35282;&#24230;&#25429;&#33719;&#30340;&#32954;&#37096;&#20307;&#31215;&#28210;&#26579;&#22270;&#20687;&#20998;&#31867;&#32954;&#37096;&#30142;&#30149;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#22270;&#20687;&#20013;&#25552;&#20379;&#23545;&#25972;&#20010;&#32954;&#37096;&#30340;&#32508;&#21512;&#35270;&#22270;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20351;&#29992;&#21512;&#20316;&#21307;&#38498;&#33719;&#21462;&#30340;&#31169;&#26377;&#25968;&#25454;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#31454;&#20105;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#32954;&#37096;&#30149;&#21464;&#24182;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic presented numerous challenges to healthcare systems worldwide. Given that lung infections are prevalent among COVID-19 patients, chest Computer Tomography (CT) scans have frequently been utilized as an alternative method for identifying COVID-19 conditions and various other types of pulmonary diseases. Deep learning architectures have emerged to automate the identification of pulmonary disease types by leveraging CT scan slices as inputs for classification models. This paper introduces COVID-VR, a novel approach for classifying pulmonary diseases based on volume rendering images of the lungs captured from multiple angles, thereby providing a comprehensive view of the entire lung in each image. To assess the effectiveness of our proposal, we compared it against competing strategies utilizing both private data obtained from partner hospitals and a publicly available dataset. The results demonstrate that our approach effectively identifies pulmonary lesions and perfo
&lt;/p&gt;</description></item><item><title>ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01423</link><description>&lt;p&gt;
ChatMOF: &#19968;&#31181;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;
&lt;/p&gt;
&lt;p&gt;
ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01423
&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#20010;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#65288;MOFs&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;gpt-3.5-turbo&#65289;&#65292;ChatMOF&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#21018;&#24615;&#32467;&#26500;&#21270;&#26597;&#35810;&#30340;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#30001;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#65288;&#21363;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#65289;&#32452;&#25104;&#65292;&#24418;&#25104;&#19968;&#20010;&#24378;&#22823;&#30340;&#27969;&#27700;&#32447;&#65292;&#31649;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#12290;&#35813;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#21644;&#36807;&#25311;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#21644;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01421</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#12289;&#25552;&#21069;&#20572;&#27490;&#21644;&#26790;&#24819;&#65306;&#19968;&#31181;&#22788;&#29702;&#27867;&#21270;&#21644;&#36807;&#25311;&#21512;&#30340;&#31867;Hopfield&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting. (arXiv:2308.01421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01421
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#21644;&#36807;&#25311;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#21644;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#21560;&#24341;&#23376;&#31070;&#32463;&#32593;&#32476;&#65306;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#26469;&#23547;&#25214;&#26368;&#20248;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#26368;&#20248;&#30340;&#31070;&#32463;&#20803;&#20132;&#20114;&#30697;&#38453;&#34987;&#35777;&#26126;&#26159;&#19968;&#31867;&#36890;&#36807;&#36845;&#20195;&#24212;&#29992;&#26576;&#20123;&#21462;&#28040;&#23398;&#20064;&#21327;&#35758;&#20462;&#35746;&#30340;Hebbian&#26680;&#30697;&#38453;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21462;&#28040;&#23398;&#20064;&#27493;&#39588;&#30340;&#25968;&#37327;&#34987;&#35777;&#26126;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#26377;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#35774;&#35745;&#36991;&#20813;&#36807;&#25311;&#21512;&#30340;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29992;&#20132;&#20114;&#30697;&#38453;&#30340;&#20195;&#25968;&#24615;&#36136;&#26469;&#25551;&#36848;&#65292;&#25110;&#32773;&#31561;&#20215;&#22320;&#29992;&#27491;&#21017;&#21270;&#35843;&#25972;&#21644;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#26469;&#25551;&#36848;&#12290;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#21560;&#24341;&#23376;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#38024;&#23545;&#38543;&#26426;&#21512;&#25104;&#25968;&#25454;&#38598;&#33719;&#24471;&#20102;&#20998;&#26512;&#32467;&#26524;&#65292;&#38543;&#21518;&#29992;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#20102;&#25152;&#24471;&#21040;&#30340;&#25972;&#20307;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we approach attractor neural networks from a machine learning perspective: we look for optimal network parameters by applying a gradient descent over a regularized loss function. Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which correspond to Hebbian kernels revised by iteratively applying some unlearning protocols. Remarkably, the number of unlearning steps is proved to be related to the regularization hyperparameters of the loss function and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in terms of the algebraic properties of the interaction matrix, or, equivalently, in terms of regularization tuning and early-stopping strategies. The generalization capabilities of these attractor networks are also investigated: analytical results are obtained for random synthetic datasets, next, the emerging picture is corroborated by numerical experiments that highlight the existence o
&lt;/p&gt;</description></item><item><title>SAP-sLDA&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20154;&#31867;&#21442;&#19982;&#30340;LDA&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#32500;&#25237;&#24433;&#20013;&#23398;&#20064;&#20027;&#39064;&#24182;&#20445;&#30041;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.01420</link><description>&lt;p&gt;
SAP-sLDA: &#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#30028;&#38754;&#29992;&#20110;&#25506;&#32034;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text. (arXiv:2308.01420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01420
&lt;/p&gt;
&lt;p&gt;
SAP-sLDA&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20154;&#31867;&#21442;&#19982;&#30340;LDA&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#32500;&#25237;&#24433;&#20013;&#23398;&#20064;&#20027;&#39064;&#24182;&#20445;&#30041;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#25506;&#32034;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#25991;&#26723;&#30340;&#20302;&#32500;&#25237;&#24433;&#65292;&#24076;&#26395;&#20027;&#39064;&#30456;&#20284;&#30340;&#25991;&#26723;&#33021;&#22815;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#32858;&#31867;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#29992;&#20110;&#38477;&#32500;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#31639;&#27861;&#65292;&#22914;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#65292;&#24448;&#24448;&#20250;&#20135;&#29983;&#26080;&#27861;&#25429;&#25417;&#25991;&#26723;&#30456;&#20284;&#24615;&#30340;&#20154;&#31867;&#27010;&#24565;&#30340;&#25237;&#24433;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20154;&#31867;&#21442;&#19982;&#30340;&#22522;&#20110;LDA&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20027;&#39064;&#65292;&#22312;&#20302;&#32500;&#25237;&#24433;&#20013;&#20445;&#30041;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#25991;&#26723;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#21512;&#25104;&#35821;&#26009;&#24211;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20165;&#25552;&#20379;&#23569;&#37327;&#26631;&#31614;&#30340;&#22522;&#32447;&#26041;&#27861;&#20135;&#29983;&#26356;&#26131;&#35299;&#37322;&#30340;&#25237;&#24433;&#12290;&#22312;&#23454;&#38469;&#35821;&#26009;&#24211;&#19978;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common way to explore text corpora is through low-dimensional projections of the documents, where one hopes that thematically similar documents will be clustered together in the projected space. However, popular algorithms for dimensionality reduction of text corpora, like Latent Dirichlet Allocation (LDA), often produce projections that do not capture human notions of document similarity. We propose a semi-supervised human-in-the-loop LDA-based method for learning topics that preserve semantically meaningful relationships between documents in low-dimensional projections. On synthetic corpora, our method yields more interpretable projections than baseline methods with only a fraction of labels provided. On a real corpus, we obtain qualitatively similar results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#21464;&#37327;&#23454;&#29616;&#27874;&#21160;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#22810;&#36339;&#37051;&#23621;&#30340;&#28322;&#20986;&#25928;&#24212;&#25552;&#39640;&#23454;&#29616;&#27874;&#21160;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25311;&#20284;&#20284;&#28982;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01419</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20855;&#26377;&#28322;&#20986;&#25928;&#24212;&#30340;&#22810;&#21464;&#37327;&#23454;&#29616;&#27874;&#21160;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects. (arXiv:2308.01419v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#21464;&#37327;&#23454;&#29616;&#27874;&#21160;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#22810;&#36339;&#37051;&#23621;&#30340;&#28322;&#20986;&#25928;&#24212;&#25552;&#39640;&#23454;&#29616;&#27874;&#21160;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25311;&#20284;&#20284;&#28982;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23450;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#21464;&#37327;&#30340;&#23454;&#38469;&#27874;&#21160;&#65292;&#20197;&#25972;&#21512;&#32929;&#31080;&#38388;&#30340;&#28322;&#20986;&#25928;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#23558;&#22810;&#36339;&#37051;&#23621;&#30340;&#28322;&#20986;&#25928;&#24212;&#32435;&#20837;&#32771;&#34385;&#12289;&#25429;&#25417;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#20351;&#29992;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#28789;&#27963;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20165;&#32771;&#34385;&#22810;&#36339;&#37051;&#23621;&#30340;&#28322;&#20986;&#25928;&#24212;&#24182;&#19981;&#33021;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#24314;&#27169;&#38750;&#32447;&#24615;&#28322;&#20986;&#25928;&#24212;&#21487;&#20197;&#25552;&#39640;&#23454;&#29616;&#27874;&#21160;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26368;&#38271;&#19968;&#21608;&#30340;&#30701;&#26399;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#26029;&#34920;&#26126;&#65292;&#20351;&#29992;&#25311;&#20284;&#20284;&#28982;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#27604;&#24120;&#29992;&#30340;&#22343;&#26041;&#35823;&#24046;&#20855;&#26377;&#26126;&#26174;&#30340;&#27169;&#22411;&#24615;&#33021;&#25913;&#21892;&#12290;&#22312;&#20854;&#20182;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#20840;&#38754;&#19968;&#31995;&#21015;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel methodology for modeling and forecasting multivariate realized volatilities using customized graph neural networks to incorporate spillover effects across stocks. The proposed model offers the benefits of incorporating spillover effects from multi-hop neighbors, capturing nonlinear relationships, and flexible training with different loss functions. Our empirical findings provide compelling evidence that incorporating spillover effects from multi-hop neighbors alone does not yield a clear advantage in terms of predictive accuracy. However, modeling nonlinear spillover effects enhances the forecasting accuracy of realized volatilities, particularly for short-term horizons of up to one week. Moreover, our results consistently indicate that training with the Quasi-likelihood loss leads to substantial improvements in model performance compared to the commonly-used mean squared error. A comprehensive series of empirical evaluations in alternative settings confirm the robus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#19987;&#23478;&#30340;&#23545;&#35805;&#21644;&#21453;&#39304;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#37329;&#34701;&#39118;&#26684;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.01415</link><description>&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#29992;&#20110;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25351;&#20196;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model. (arXiv:2308.01415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#19987;&#23478;&#30340;&#23545;&#35805;&#21644;&#21453;&#39304;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#37329;&#34701;&#39118;&#26684;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21021;&#26399;&#38454;&#27573;&#65292;&#20026;&#37329;&#34701;&#30456;&#20851;&#20219;&#21153;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#38750;&#24120;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#24341;&#21457;&#20102;&#19968;&#20010;AI&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#19987;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#24182;&#34701;&#20837;&#20102;&#20154;&#24037;&#37329;&#34701;&#19987;&#23478;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#25968;&#25454;&#38598;&#12290;&#35813;&#27969;&#27700;&#32447;&#20135;&#29983;&#20102;&#19968;&#20010;&#30001;103k&#20010;&#22810;&#36718;&#23545;&#35805;&#32452;&#25104;&#30340;&#31283;&#23450;&#30340;&#25351;&#20196;&#31934;&#35843;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#37319;&#29992;&#22806;&#37096;&#30340;GPT-4&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26377;&#24076;&#26395;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#37329;&#34701;&#39118;&#26684;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32780;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;"Hoodwinked"&#30340;&#25991;&#26412;&#28216;&#25103;&#65292;&#30740;&#31350;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#26432;&#25163;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.01404</link><description>&lt;p&gt;
&#33945;&#39575;&#65306;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#20013;&#30340;&#27450;&#39575;&#19982;&#21512;&#20316;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;"Hoodwinked"&#30340;&#25991;&#26412;&#28216;&#25103;&#65292;&#30740;&#31350;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#26432;&#25163;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#65311;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;&#8220;Hoodwinked&#8221;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#65292;&#21463;&#21040;&#8220;&#40657;&#24110;&#8221;&#21644;&#8220;&#35841;&#26159;&#21351;&#24213;&#8221;&#28216;&#25103;&#30340;&#21551;&#21457;&#65292;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#29609;&#23478;&#20204;&#34987;&#38145;&#22312;&#19968;&#20010;&#25151;&#23376;&#37324;&#65292;&#24517;&#39035;&#25214;&#21040;&#19968;&#25226;&#38053;&#21273;&#25165;&#33021;&#36867;&#33073;&#65292;&#20294;&#20854;&#20013;&#19968;&#20010;&#29609;&#23478;&#34987;&#27966;&#20219;&#21153;&#26432;&#27515;&#20854;&#20182;&#20154;&#12290;&#27599;&#27425;&#21457;&#29983;&#35851;&#26432;&#65292;&#24184;&#23384;&#30340;&#29609;&#23478;&#20204;&#20250;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#35752;&#35770;&#65292;&#28982;&#21518;&#25237;&#31080;&#23558;&#19968;&#21517;&#29609;&#23478;&#25918;&#36880;&#20986;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#25805;&#25511;&#20195;&#29702;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#20102;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#35777;&#25454;&#12290;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#33258;&#24049;&#30340;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#21487;&#27979;&#37327;&#30340;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;24&#20010;&#20004;&#20004;&#27604;&#36739;&#20013;&#30340;18&#20010;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26432;&#25163;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#27425;&#35201;&#25351;&#26631;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#31181;&#25913;&#36827;&#24182;&#19981;&#26159;&#36890;&#36807;&#19981;&#21516;&#30340;&#34892;&#21160;&#23454;&#29616;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called $\textit{Hoodwinked}$, inspired by $\textit{Mafia}$ and $\textit{Among Us}$. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger deception capabilities during discussions. Overall, we find substantial
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.01399</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#19982;&#20154;&#31867;&#22312;&#19990;&#30028;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#20195;&#29702;&#22120;&#38656;&#35201;&#29702;&#35299;&#20154;&#20204;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#19990;&#30028;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#22522;&#20110;&#35821;&#35328;&#34892;&#21160;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#20195;&#29702;&#22120;&#21487;&#20197;&#36890;&#36807;&#20219;&#21153;&#22870;&#21169;&#23398;&#20064;&#25191;&#34892;&#31616;&#21333;&#30340;&#35821;&#35328;&#25351;&#20196;&#65292;&#20294;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#21487;&#20197;&#21033;&#29992;&#20256;&#36798;&#19968;&#33324;&#30693;&#35782;&#12289;&#25551;&#36848;&#19990;&#30028;&#29366;&#24577;&#12289;&#25552;&#20379;&#20114;&#21160;&#21453;&#39304;&#31561;&#22810;&#26679;&#21270;&#35821;&#35328;&#30340;&#20195;&#29702;&#22120;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#65306;&#23558;&#20250;&#34987;&#35266;&#23519;&#21040;&#20160;&#20040;&#12289;&#19990;&#30028;&#23558;&#22914;&#20309;&#36816;&#34892;&#20197;&#21450;&#21738;&#20123;&#24773;&#20917;&#23558;&#33719;&#24471;&#22870;&#21169;&#12290;&#36825;&#20010;&#35266;&#28857;&#23558;&#35821;&#35328;&#29702;&#35299;&#19982;&#26410;&#26469;&#39044;&#27979;&#32479;&#19968;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Dynalang&#65292;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#30340;&#20195;&#29702;&#22120;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#24819;&#20687;&#30340;&#27169;&#22411;&#22238;&#28378;&#20013;&#23398;&#20064;&#34892;&#21160;&#12290;&#19982;&#21482;&#20351;&#29992;&#35821;&#35328;&#39044;&#27979;&#21160;&#20316;&#30340;&#20256;&#32479;&#20195;&#29702;&#22120;&#19981;&#21516;&#65292;Dynalang&#36890;&#36807;&#36807;&#21435;&#30340;&#35821;&#35328;&#36824;&#21487;&#20197;&#33719;&#21462;&#20016;&#23500;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
&lt;/p&gt;</description></item><item><title>OpenFlamingo&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;&#23545;&#24212;&#27169;&#22411;&#24615;&#33021;&#30340;80%&#33267;89%&#12290;</title><link>http://arxiv.org/abs/2308.01390</link><description>&lt;p&gt;
OpenFlamingo: &#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01390
&lt;/p&gt;
&lt;p&gt;
OpenFlamingo&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;&#23545;&#24212;&#27169;&#22411;&#24615;&#33021;&#30340;80%&#33267;89%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;OpenFlamingo&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#33258;&#22238;&#24402;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;3B&#21040;9B&#12290; OpenFlamingo&#26159;&#19968;&#20010;&#25345;&#32493;&#21162;&#21147;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#22797;&#21046;DeepMind&#30340;Flamingo&#27169;&#22411;&#30340;&#24320;&#28304;&#29256;&#26412;&#12290;&#22312;&#19971;&#20010;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#65292;OpenFlamingo&#27169;&#22411;&#30340;&#24615;&#33021;&#20171;&#20110;&#23545;&#24212;&#30340;Flamingo&#24615;&#33021;&#30340;80%&#33267;89%&#20043;&#38388;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21644;&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#22312;https://github.com/mlfoundations/open_flamingo&#19978;&#20998;&#20139;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open_flamingo.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#20248;&#21270;&#30340;&#21333;&#27425;&#22810;&#26694;&#26816;&#27979;&#21644;&#24378;&#21270;&#23398;&#20064;&#36861;&#36394;&#22763;&#20853;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;DeepRacer&#26500;&#24314;&#19968;&#20010;&#33258;&#20027;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;SSD Lite&#26367;&#20195;SSD&#65292;&#25105;&#20204;&#22312;&#25512;&#29702;&#36895;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#32780;&#20934;&#30830;&#24615;&#24182;&#26410;&#21463;&#21040;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.01389</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#30340;&#21333;&#27425;&#22810;&#26694;&#26816;&#27979;&#21644;&#24378;&#21270;&#23398;&#20064;&#36861;&#36394;&#22763;&#20853;
&lt;/p&gt;
&lt;p&gt;
Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning. (arXiv:2308.01389v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#20248;&#21270;&#30340;&#21333;&#27425;&#22810;&#26694;&#26816;&#27979;&#21644;&#24378;&#21270;&#23398;&#20064;&#36861;&#36394;&#22763;&#20853;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;DeepRacer&#26500;&#24314;&#19968;&#20010;&#33258;&#20027;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;SSD Lite&#26367;&#20195;SSD&#65292;&#25105;&#20204;&#22312;&#25512;&#29702;&#36895;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#32780;&#20934;&#30830;&#24615;&#24182;&#26410;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#30001;&#20110;&#22312;&#25112;&#22330;&#19978;&#21644;&#35299;&#20915;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#37117;&#26377;&#35768;&#22810;&#28508;&#22312;&#24212;&#29992;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#25105;&#20204;&#39033;&#30446;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;DeepRacer&#26500;&#24314;&#19968;&#20010;&#33258;&#20027;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#22312;&#22763;&#20853;&#65288;&#39033;&#30446;&#20013;&#30340;&#29305;&#23450;&#20154;&#29289;&#65289;&#22312;&#20219;&#20309;&#26041;&#21521;&#31227;&#21160;&#26102;&#36861;&#36394;&#20182;&#20204;&#12290;&#23454;&#29616;&#27492;&#39033;&#30446;&#30340;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#20248;&#21270;&#30340;&#21333;&#27425;&#22810;&#26694;&#26816;&#27979;&#65288;SSD&#65289;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;SSD Lite&#32780;&#19981;&#26159;SSD&#23436;&#25104;&#20102;&#20219;&#21153;&#65292;&#24182;&#26368;&#32456;&#27604;&#36739;&#20102;SSD&#12289;&#20855;&#26377;&#31070;&#32463;&#35745;&#31639;&#26834;&#65288;NCS&#65289;&#30340;SSD&#21644;SSD Lite&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SSD Lite&#22312;&#36825;&#19977;&#31181;&#25216;&#26415;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;2-3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, autonomous cars are gaining traction due to their numerous potential applications on battlefields and in resolving a variety of other real-world challenges. The main goal of our project is to build an autonomous system using DeepRacer which will follow a specific person (for our project, a soldier) when they will be moving in any direction. Two main components to accomplish this project is an optimized Single-Shot Multibox Detection (SSD) object detection model and a Reinforcement Learning (RL) model. We accomplished the task using SSD Lite instead of SSD and at the end, compared the results among SSD, SSD with Neural Computing Stick (NCS), and SSD Lite. Experimental results show that SSD Lite gives better performance among these three techniques and exhibits a considerable boost in inference speed (~2-3 times) without compromising accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#29190;&#21457;&#25668;&#24433;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#25163;&#25345;&#26234;&#33021;&#25163;&#26426;&#30456;&#26426;&#24212;&#29992;&#20013;&#23454;&#29616;&#38271;&#26333;&#20809;&#25668;&#24433;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#21069;&#26223;&#27169;&#31946;&#21644;&#32972;&#26223;&#27169;&#31946;&#12290;&#36890;&#36807;&#26816;&#27979;&#21644;&#20998;&#21106;&#26174;&#33879;&#20027;&#20307;&#65292;&#24182;&#36319;&#36394;&#22330;&#26223;&#20013;&#30340;&#21160;&#24577;&#20803;&#32032;&#65292;&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#21512;&#25104;&#20986;&#20196;&#20154;&#24778;&#21497;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.01379</link><description>&lt;p&gt;
&#35745;&#31639;&#38271;&#26333;&#20809;&#31227;&#21160;&#25668;&#24433;
&lt;/p&gt;
&lt;p&gt;
Computational Long Exposure Mobile Photography. (arXiv:2308.01379v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#29190;&#21457;&#25668;&#24433;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#25163;&#25345;&#26234;&#33021;&#25163;&#26426;&#30456;&#26426;&#24212;&#29992;&#20013;&#23454;&#29616;&#38271;&#26333;&#20809;&#25668;&#24433;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#21069;&#26223;&#27169;&#31946;&#21644;&#32972;&#26223;&#27169;&#31946;&#12290;&#36890;&#36807;&#26816;&#27979;&#21644;&#20998;&#21106;&#26174;&#33879;&#20027;&#20307;&#65292;&#24182;&#36319;&#36394;&#22330;&#26223;&#20013;&#30340;&#21160;&#24577;&#20803;&#32032;&#65292;&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#21512;&#25104;&#20986;&#20196;&#20154;&#24778;&#21497;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26333;&#20809;&#25668;&#24433;&#20135;&#29983;&#20196;&#20154;&#24778;&#21497;&#30340;&#22270;&#20687;&#65292;&#29992;&#36816;&#21160;&#27169;&#31946;&#26469;&#34920;&#29616;&#22330;&#26223;&#20013;&#30340;&#31227;&#21160;&#20803;&#32032;&#12290;&#23427;&#36890;&#24120;&#20998;&#20026;&#20004;&#31181;&#27169;&#24335;&#65292;&#19968;&#31181;&#26159;&#20135;&#29983;&#21069;&#26223;&#27169;&#31946;&#25928;&#26524;&#65292;&#19968;&#31181;&#26159;&#20135;&#29983;&#32972;&#26223;&#27169;&#31946;&#25928;&#26524;&#12290;&#20256;&#32479;&#19978;&#65292;&#21069;&#26223;&#27169;&#31946;&#22270;&#20687;&#26159;&#22312;&#19977;&#33050;&#26550;&#19978;&#30340;&#30456;&#26426;&#19978;&#25293;&#25668;&#30340;&#65292;&#25551;&#32472;&#20986;&#27169;&#31946;&#30340;&#31227;&#21160;&#21069;&#26223;&#20803;&#32032;&#65288;&#22914;&#19997;&#32504;&#33324;&#30340;&#27700;&#27969;&#25110;&#20809;&#36857;&#65289;&#65292;&#32780;&#32972;&#26223;&#21017;&#21576;&#29616;&#20986;&#23436;&#20840;&#28165;&#26224;&#30340;&#32972;&#26223;&#26223;&#35266;&#12290;&#32972;&#26223;&#27169;&#31946;&#22270;&#20687;&#65292;&#20063;&#31216;&#20026;&#36319;&#25293;&#25668;&#24433;&#65292;&#26159;&#22312;&#30456;&#26426;&#36861;&#36394;&#31227;&#21160;&#23545;&#35937;&#26102;&#25293;&#25668;&#30340;&#65292;&#20197;&#20135;&#29983;&#19968;&#20010;&#28165;&#26224;&#30340;&#20027;&#20307;&#22270;&#20687;&#21644;&#19968;&#20010;&#22240;&#30456;&#23545;&#36816;&#21160;&#32780;&#27169;&#31946;&#30340;&#32972;&#26223;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#37117;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#21644;&#39640;&#32423;&#25216;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#25163;&#25345;&#26234;&#33021;&#25163;&#26426;&#30456;&#26426;&#24212;&#29992;&#20013;&#36816;&#34892;&#30340;&#35745;&#31639;&#29190;&#21457;&#25668;&#24433;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#25353;&#19979;&#24555;&#38376;&#25353;&#38062;&#26102;&#23436;&#20840;&#33258;&#21160;&#22320;&#23454;&#29616;&#36825;&#20123;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#26816;&#27979;&#21644;&#20998;&#21106;&#26174;&#33879;&#20027;&#20307;&#12290;&#25105;&#20204;&#36319;&#36394;&#22330;&#26223;&#20013;&#30340;&#25152;&#26377;&#21160;&#24577;&#20803;&#32032;&#65292;&#20197;&#20272;&#35745;&#23427;&#20204;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#23545;&#22270;&#20687;&#24207;&#21015;&#30340;&#24103;&#36827;&#34892;&#34701;&#21512;&#65292;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long exposure photography produces stunning imagery, representing moving elements in a scene with motion-blur. It is generally employed in two modalities, producing either a foreground or a background blur effect. Foreground blur images are traditionally captured on a tripod-mounted camera and portray blurred moving foreground elements, such as silky water or light trails, over a perfectly sharp background landscape. Background blur images, also called panning photography, are captured while the camera is tracking a moving subject, to produce an image of a sharp subject over a background blurred by relative motion. Both techniques are notoriously challenging and require additional equipment and advanced skills. In this paper, we describe a computational burst photography system that operates in a hand-held smartphone camera app, and achieves these effects fully automatically, at the tap of the shutter button. Our approach first detects and segments the salient subject. We track the sce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;-ODE&#36827;&#34892;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#21644;&#25972;&#20307;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#25130;&#26029;&#25968;&#25454;&#20013;&#36827;&#34892;&#26080;&#20559;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.01362</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#21644;&#20351;&#29992;&#31070;&#32463;-ODE&#36827;&#34892;&#25972;&#20307;&#29983;&#23384;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE. (arXiv:2308.01362v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;-ODE&#36827;&#34892;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#21644;&#25972;&#20307;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#25130;&#26029;&#25968;&#25454;&#20013;&#36827;&#34892;&#26080;&#20559;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25903;&#25345;&#32959;&#30244;&#33647;&#29289;&#30340;&#24320;&#21457;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#22686;&#21152;&#39044;&#27979;&#33021;&#21147;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#27835;&#30103;&#24182;&#25913;&#21892;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32959;&#30244;&#21160;&#21147;&#31070;&#32463;-ODE&#65288;TDNODE&#65289;&#20316;&#20026;&#19968;&#31181;&#33647;&#29702;&#23398;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#32437;&#21521;&#32959;&#30244;&#22823;&#23567;&#25968;&#25454;&#20013;&#23454;&#29616;&#27169;&#22411;&#21457;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TDNODE&#22312;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#19978;&#30340;&#33021;&#21147;&#65292;&#21363;&#33021;&#22815;&#20174;&#25130;&#26029;&#25968;&#25454;&#20013;&#36827;&#34892;&#26080;&#20559;&#39044;&#27979;&#12290;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#35774;&#35745;&#29992;&#20110;&#34920;&#36798;&#20855;&#26377;&#26102;&#38388;&#30340;&#24191;&#20041;&#40784;&#27425;&#24615;&#36825;&#19968;&#22522;&#26412;&#29305;&#24615;&#30340;&#22522;&#30784;&#21160;&#21147;&#23398;&#23450;&#24459;&#12290;&#22240;&#27492;&#65292;&#24314;&#27169;&#24418;&#24335;&#20351;&#24471;&#32534;&#30721;&#22120;&#36755;&#20986;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21160;&#21147;&#23398;&#36895;&#29575;&#25351;&#26631;&#65292;&#20854;&#20013;&#20498;&#25968;&#26102;&#38388;&#20316;&#20026;&#29289;&#29702;&#21333;&#20301;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#25351;&#26631;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#29992;&#20110;&#39044;&#27979;&#24739;&#32773;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;&#25152;&#25552;&#20986;&#30340;&#24314;&#27169;&#24418;&#24335;&#20026;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
While tumor dynamic modeling has been widely applied to support the development of oncology drugs, there remains a need to increase predictivity, enable personalized therapy, and improve decision-making. We propose the use of Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to enable model discovery from longitudinal tumor size data. We show that TDNODE overcomes a key limitation of existing models in its ability to make unbiased predictions from truncated data. The encoder-decoder architecture is designed to express an underlying dynamical law which possesses the fundamental property of generalized homogeneity with respect to time. Thus, the modeling formalism enables the encoder output to be interpreted as kinetic rate metrics, with inverse time as the physical unit. We show that the generated metrics can be used to predict patients' overall survival (OS) with high accuracy. The proposed modeling formalism provides a principled way to integrate multimodal d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#23545;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26080;&#20559;&#21387;&#32553;&#25805;&#20316;&#31526;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36229;&#36234;&#20102;&#32463;&#20856;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#12290;&#38024;&#23545;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#38543;&#26426;&#22330;&#30340;&#19968;&#33324;&#20551;&#35774;&#21644;&#22122;&#22768;&#21327;&#26041;&#24046;&#30340;&#38480;&#21046;&#65292;&#20197;&#20998;&#26512;&#21508;&#31181;&#38543;&#26426;&#21270;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.01358</link><description>&lt;p&gt;
&#21387;&#32553;&#21644;&#20998;&#24067;&#24335;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65306;&#25910;&#25947;&#36895;&#24230;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#23545;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26080;&#20559;&#21387;&#32553;&#25805;&#20316;&#31526;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36229;&#36234;&#20102;&#32463;&#20856;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#12290;&#38024;&#23545;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#38543;&#26426;&#22330;&#30340;&#19968;&#33324;&#20551;&#35774;&#21644;&#22122;&#22768;&#21327;&#26041;&#24046;&#30340;&#38480;&#21046;&#65292;&#20197;&#20998;&#26512;&#21508;&#31181;&#38543;&#26426;&#21270;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#21387;&#32553;&#23545;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20960;&#31181;&#26080;&#20559;&#21387;&#32553;&#25805;&#20316;&#31526;&#20043;&#38388;&#30340;&#25910;&#25947;&#36895;&#24230;&#24046;&#24322;&#65292;&#36825;&#20123;&#25805;&#20316;&#31526;&#37117;&#28385;&#36275;&#30456;&#21516;&#30340;&#26041;&#24046;&#26465;&#20214;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;&#32463;&#20856;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65288;LSR&#65289;&#30340;&#24773;&#20917;&#65292;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#38543;&#26426;&#22330;&#30340;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#12290;&#25105;&#20204;&#23545;&#38543;&#26426;&#22330;&#30340;&#19968;&#33324;&#24615;&#20551;&#35774;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65288;&#29305;&#21035;&#26159;&#26399;&#26395;&#30340;H&#246;lder&#27491;&#21017;&#24615;&#65289;&#24182;&#23545;&#22122;&#22768;&#21327;&#26041;&#24046;&#36827;&#34892;&#20102;&#38480;&#21046;&#65292;&#20197;&#20415;&#20998;&#26512;&#21508;&#31181;&#38543;&#26426;&#21270;&#26426;&#21046;&#65292;&#21253;&#25324;&#21387;&#32553;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#21152;&#24615;&#22122;&#22768;&#30340;&#21327;&#26041;&#24046;&#120226;&#120224;&#120237;&#120232;&#120224;&#23545;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H\"older regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning.  More formally, we highlight the impact on the convergence of the covariance $\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;&#23398;&#20064;&#30340;&#23618;&#27425;&#25506;&#32034;&#31639;&#27861;EmbeddingTree&#21644;&#30456;&#24212;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#33021;&#22815;&#35299;&#37322;&#20855;&#26377;&#35821;&#20041;&#30340;&#23454;&#20307;&#29305;&#24449;&#21644;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01329</link><description>&lt;p&gt;
EmbeddingTree&#65306;&#23884;&#20837;&#24335;&#20013;&#23454;&#20307;&#29305;&#24449;&#30340;&#23618;&#27425;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding. (arXiv:2308.01329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;&#23398;&#20064;&#30340;&#23618;&#27425;&#25506;&#32034;&#31639;&#27861;EmbeddingTree&#21644;&#30456;&#24212;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#33021;&#22815;&#35299;&#37322;&#20855;&#26377;&#35821;&#20041;&#30340;&#23454;&#20307;&#29305;&#24449;&#21644;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#23398;&#20064;&#23558;&#31163;&#25955;&#30340;&#25968;&#25454;&#23454;&#20307;&#36716;&#25442;&#20026;&#36830;&#32493;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#32534;&#30721;&#20102;&#23454;&#20307;&#30340;&#29305;&#24449;/&#23646;&#24615;&#12290;&#23613;&#31649;&#26377;&#22810;&#31181;&#23884;&#20837;&#23398;&#20064;&#31639;&#27861;&#25253;&#21578;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#25237;&#20837;&#21040;&#23545;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#22914;&#20309;&#32534;&#30721;&#30340;&#32467;&#26500;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EmbeddingTree&#65292;&#19968;&#31181;&#23618;&#27425;&#23884;&#20837;&#25506;&#32034;&#31639;&#27861;&#65292;&#23558;&#23454;&#20307;&#29305;&#24449;&#30340;&#35821;&#20041;&#19982;&#36739;&#38590;&#35299;&#37322;&#30340;&#23884;&#20837;&#21521;&#37327;&#30456;&#20851;&#32852;&#12290;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;EmbeddingTree&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#25506;&#32034;&#39640;&#32500;&#23884;&#20837;&#12290;&#35813;&#24037;&#20855;&#21487;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#25968;&#25454;&#23454;&#20307;&#30340;&#24494;&#22937;&#29305;&#24449;&#65292;&#22312;&#23884;&#20837;&#35757;&#32451;&#20013;&#25191;&#34892;&#29305;&#24449;&#21435;&#22122;/&#27880;&#20837;&#65292;&#24182;&#20026;&#26410;&#35265;&#23454;&#20307;&#29983;&#25104;&#23884;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#24037;&#19994;&#35268;&#27169;&#30340;&#21830;&#25143;&#25968;&#25454;&#21644;&#20844;&#20849;&#30340;30Music&#21548;&#27468;/&#25773;&#25918;&#21015;&#34920;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#23884;&#20837;&#26469;&#35777;&#26126;EmbeddingTree&#21644;&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding learning transforms discrete data entities into continuous numerical representations, encoding features/properties of the entities. Despite the outstanding performance reported from different embedding learning algorithms, few efforts were devoted to structurally interpreting how features are encoded in the learned embedding space. This work proposes EmbeddingTree, a hierarchical embedding exploration algorithm that relates the semantics of entity features with the less-interpretable embedding vectors. An interactive visualization tool is also developed based on EmbeddingTree to explore high-dimensional embeddings. The tool helps users discover nuance features of data entities, perform feature denoising/injecting in embedding training, and generate embeddings for unseen entities. We demonstrate the efficacy of EmbeddingTree and our visualization tool through embeddings generated for industry-scale merchant data and the public 30Music listening/playlists dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#23545;&#22833;&#35821;&#30151;&#20122;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#21033;&#29992;&#22768;&#38899;&#24405;&#38899;&#33258;&#21160;&#35782;&#21035;&#24182;&#35780;&#20272;&#35328;&#35821;&#38556;&#30861;&#12290;&#35813;&#26041;&#27861;&#22312;&#21306;&#20998;&#22833;&#35821;&#30151;&#24739;&#32773;&#21644;&#20581;&#24247;&#23545;&#29031;&#32452;&#30340;&#24405;&#38899;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#36817;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;&#26368;&#24120;&#35265;&#30340;&#22833;&#35821;&#30151;&#31867;&#22411;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30142;&#30149;&#21644;&#35821;&#35328;&#65292;&#24182;&#26377;&#26395;&#31283;&#20581;&#22320;&#25552;&#21462;&#35786;&#26029;&#24615;&#30340;&#35328;&#35821;&#29983;&#29289;&#26631;&#24535;&#12290;</title><link>http://arxiv.org/abs/2308.01327</link><description>&lt;p&gt;
Careful Whisper - &#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#36827;&#23637;&#26469;&#36827;&#34892;&#20581;&#22766;&#19988;&#26131;&#35299;&#37322;&#30340;&#22833;&#35821;&#30151;&#20122;&#22411;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Careful Whisper -- leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification. (arXiv:2308.01327v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#23545;&#22833;&#35821;&#30151;&#20122;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#21033;&#29992;&#22768;&#38899;&#24405;&#38899;&#33258;&#21160;&#35782;&#21035;&#24182;&#35780;&#20272;&#35328;&#35821;&#38556;&#30861;&#12290;&#35813;&#26041;&#27861;&#22312;&#21306;&#20998;&#22833;&#35821;&#30151;&#24739;&#32773;&#21644;&#20581;&#24247;&#23545;&#29031;&#32452;&#30340;&#24405;&#38899;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#36817;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;&#26368;&#24120;&#35265;&#30340;&#22833;&#35821;&#30151;&#31867;&#22411;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30142;&#30149;&#21644;&#35821;&#35328;&#65292;&#24182;&#26377;&#26395;&#31283;&#20581;&#22320;&#25552;&#21462;&#35786;&#26029;&#24615;&#30340;&#35328;&#35821;&#29983;&#29289;&#26631;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22768;&#38899;&#24405;&#38899;&#35782;&#21035;&#26469;&#36741;&#21161;&#35780;&#20272;&#35328;&#35821;&#38556;&#30861;&#12290;&#36890;&#36807;&#32467;&#21512;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#21644;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#20016;&#23500;&#30340;&#22768;&#23398;&#21644;&#28165;&#26224;&#30340;&#36716;&#24405;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20174;&#36825;&#20123;&#36716;&#24405;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#29983;&#25104;&#20581;&#24247;&#35821;&#38899;&#30340;&#21407;&#22411;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#22411;&#30340;&#22522;&#26412;&#36317;&#31163;&#24230;&#37327;&#20316;&#20026;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#20154;&#31867;&#27700;&#24179;&#19978;&#21306;&#20998;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#24405;&#38899;&#21644;&#20581;&#24247;&#23545;&#29031;&#32452;&#12290;&#27492;&#22806;&#65292;&#26368;&#24120;&#35265;&#30340;&#22833;&#35821;&#30151;&#31867;&#22411;&#21487;&#20197;&#20197;90%&#30340;&#20934;&#30830;&#29575;&#36827;&#34892;&#21306;&#20998;&#12290;&#35813;&#27969;&#31243;&#21487;&#30452;&#25509;&#36866;&#29992;&#20110;&#20854;&#20182;&#30142;&#30149;&#21644;&#35821;&#35328;&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#26469;&#31283;&#20581;&#22320;&#25552;&#21462;&#35786;&#26029;&#24615;&#35328;&#35821;&#29983;&#29289;&#26631;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a fully automated approach for identifying speech anomalies from voice recordings to aid in the assessment of speech impairments. By combining Connectionist Temporal Classification (CTC) and encoder-decoder-based automatic speech recognition models, we generate rich acoustic and clean transcripts. We then apply several natural language processing methods to extract features from these transcripts to produce prototypes of healthy speech. Basic distance measures from these prototypes serve as input features for standard machine learning classifiers, yielding human-level accuracy for the distinction between recordings of people with aphasia and a healthy control group. Furthermore, the most frequently occurring aphasia types can be distinguished with 90% accuracy. The pipeline is directly applicable to other diseases and languages, showing promise for robustly extracting diagnostic speech biomarkers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#32593;&#32476;&#24341;&#23548;&#30340;&#38543;&#26426;&#26862;&#26519;&#22312;&#30142;&#30149;&#22522;&#22240;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#30142;&#30149;&#39044;&#27979;&#26041;&#38754;&#19982;&#26631;&#20934;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#24182;&#26080;&#20248;&#21183;&#65292;&#20294;&#22312;&#30142;&#30149;&#22522;&#22240;&#21457;&#29616;&#26041;&#38754;&#65292;&#32593;&#32476;&#24341;&#23548;&#30340;&#38543;&#26426;&#26862;&#26519;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22522;&#22240;&#27169;&#22359;&#65292;&#20294;&#23545;&#20110;&#20013;&#24515;&#22522;&#22240;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#36873;&#25321;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.01323</link><description>&lt;p&gt;
&#35780;&#20272;&#32593;&#32476;&#24341;&#23548;&#30340;&#38543;&#26426;&#26862;&#26519;&#22312;&#30142;&#30149;&#22522;&#22240;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of network-guided random forest for disease gene discovery. (arXiv:2308.01323v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#32593;&#32476;&#24341;&#23548;&#30340;&#38543;&#26426;&#26862;&#26519;&#22312;&#30142;&#30149;&#22522;&#22240;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#30142;&#30149;&#39044;&#27979;&#26041;&#38754;&#19982;&#26631;&#20934;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#24182;&#26080;&#20248;&#21183;&#65292;&#20294;&#22312;&#30142;&#30149;&#22522;&#22240;&#21457;&#29616;&#26041;&#38754;&#65292;&#32593;&#32476;&#24341;&#23548;&#30340;&#38543;&#26426;&#26862;&#26519;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22522;&#22240;&#27169;&#22359;&#65292;&#20294;&#23545;&#20110;&#20013;&#24515;&#22522;&#22240;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#36873;&#25321;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32593;&#32476;&#20449;&#24687;&#34987;&#35748;&#20026;&#23545;&#20110;&#30142;&#30149;&#27169;&#22359;&#21644;&#36890;&#36335;&#30340;&#35782;&#21035;&#26377;&#30410;&#65292;&#20294;&#22312;&#26631;&#20934;&#30340;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#20013;&#23578;&#26410;&#26126;&#30830;&#21033;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32593;&#32476;&#24341;&#23548;&#30340;RF&#30340;&#24615;&#33021;&#65292;&#22312;&#26500;&#24314;RF&#26102;&#65292;&#32593;&#32476;&#20449;&#24687;&#34987;&#24635;&#32467;&#20026;&#39044;&#27979;&#21464;&#37327;&#30340;&#37319;&#26679;&#27010;&#29575;&#65292;&#24182;&#22312;&#20854;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32593;&#32476;&#24341;&#23548;&#30340;RF&#22312;&#30142;&#30149;&#39044;&#27979;&#26041;&#38754;&#24182;&#19981;&#27604;&#26631;&#20934;RF&#26356;&#22909;&#12290;&#22312;&#30142;&#30149;&#22522;&#22240;&#21457;&#29616;&#26041;&#38754;&#65292;&#22914;&#26524;&#30142;&#30149;&#22522;&#22240;&#24418;&#25104;&#27169;&#22359;&#65292;&#21017;&#32593;&#32476;&#24341;&#23548;&#30340;RF&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#24403;&#32473;&#23450;&#32593;&#32476;&#20013;&#30340;&#22522;&#22240;&#19982;&#30142;&#30149;&#29366;&#24577;&#26080;&#20851;&#26102;&#65292;&#20351;&#29992;&#32593;&#32476;&#20449;&#24687;&#21487;&#33021;&#20250;&#20135;&#29983;&#34394;&#20551;&#30340;&#22522;&#22240;&#36873;&#25321;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20013;&#24515;&#22522;&#22240;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;The Cancer Genome Atlas (TCGA)&#30340;&#20004;&#20010;&#24179;&#34913;&#30340;&#24494;&#38453;&#21015;&#21644;RNA-Seq&#20083;&#33146;&#30284;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#29992;&#20110;&#20998;&#31867;&#23381;&#28608;&#32032;&#21463;&#20307;&#65288;PR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gene network information is believed to be beneficial for disease module and pathway identification, but has not been explicitly utilized in the standard random forest (RF) algorithm for gene expression data analysis. We investigate the performance of a network-guided RF where the network information is summarized into a sampling probability of predictor variables which is further used in the construction of the RF. Our results suggest that network-guided RF does not provide better disease prediction than the standard RF. In terms of disease gene discovery, if disease genes form module(s), network-guided RF identifies them more accurately. In addition, when disease status is independent from genes in the given network, spurious gene selection results can occur when using network information, especially on hub genes. Our empirical analysis on two balanced microarray and RNA-Seq breast cancer datasets from The Cancer Genome Atlas (TCGA) for classification of progesterone receptor (PR) st
&lt;/p&gt;</description></item><item><title>DeepSpeed-Chat&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20351;&#24471;ChatGPT-like&#27169;&#22411;&#30340;RLHF&#22521;&#35757;&#26131;&#20110;&#35775;&#38382;&#65292;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#12290;&#23427;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20307;&#39564;&#65292;&#22797;&#21046;&#20102;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#21508;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01320</link><description>&lt;p&gt;
DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (arXiv:2308.01320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01320
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-Chat&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20351;&#24471;ChatGPT-like&#27169;&#22411;&#30340;RLHF&#22521;&#35757;&#26131;&#20110;&#35775;&#38382;&#65292;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#12290;&#23427;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20307;&#39564;&#65292;&#22797;&#21046;&#20102;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#21508;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT-like&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#65292;&#20174;&#25688;&#35201;&#21644;&#32534;&#30721;&#21040;&#32763;&#35793;&#65292;&#29978;&#33267;&#36229;&#36234;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29615;&#22659;&#36824;&#32570;&#20047;&#19968;&#31181;&#26131;&#20110;&#35775;&#38382;&#12289;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#30340;&#31471;&#21040;&#31471;RLHF&#65288;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65289;&#35757;&#32451;&#27969;&#31243;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#35268;&#27169;&#36798;&#21040;&#25968;&#21313;&#20159;&#21442;&#25968;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Chat&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20351;RLHF&#22521;&#35757;&#23545;AI&#31038;&#21306;&#21487;&#29992;&#12290;DeepSpeed-Chat&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;ChatGPT-like&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20307;&#39564;&#65292;&#19968;&#20010;&#22797;&#21046;InstructGPT&#35757;&#32451;&#27969;&#31243;&#30340;DeepSpeed-RLHF&#27969;&#27700;&#32447;&#65292;&#20197;&#21450;&#19968;&#20010;&#38598;&#25104;&#20102;&#21508;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20248;&#21270;&#30340;&#24378;&#22823;DeepSpeed-RLHF&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#20197;&#35757;&#32451;&#20855;&#26377;&#25968;&#21315;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#39046;&#22495;&#22312;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#30142;&#30149;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#20854;&#22312;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.01319</link><description>&lt;p&gt;
&#36817;&#24180;&#26469;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#30142;&#30149;&#35786;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;: &#31995;&#32479;&#24615;&#35843;&#26597;&#12289;&#27604;&#36739;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges. (arXiv:2308.01319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#39046;&#22495;&#22312;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#30142;&#30149;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#20854;&#22312;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;(CAD)&#20316;&#20026;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#39046;&#22495;&#65292;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#30001;&#20110;&#21307;&#23398;&#35786;&#26029;&#31995;&#32479;&#30340;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#35823;&#23548;&#30340;&#21307;&#30103;&#27835;&#30103;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#20184;&#20986;&#20102;&#37325;&#35201;&#21162;&#21147;&#26469;&#25913;&#36827;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#24212;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#31561;&#24335;&#21487;&#33021;&#20250;&#23548;&#33268;&#20851;&#20110;&#22120;&#23448;&#31561;&#39033;&#30446;&#30340;&#38169;&#35823;&#25351;&#31034;&#12290;&#22240;&#27492;&#65292;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#26159;&#27169;&#24335;&#35782;&#21035;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#26426;&#22120;&#23398;&#20064;&#26377;&#26395;&#25552;&#39640;&#30142;&#30149;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#31934;&#30830;&#24615;&#65292;&#21516;&#26102;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#30340;&#23458;&#35266;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#20026;&#21019;&#24314;&#20248;&#38597;&#19988;&#33258;&#20027;&#30340;&#31639;&#27861;&#26469;&#20998;&#26512;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#38469;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#29992;&#20110;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#32925;&#28814;&#12289;&#31958;&#23615;&#30149;&#12289;&#32925;&#33039;&#30142;&#30149;&#12289;&#30331;&#38761;&#28909;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer-aided diagnosis (CAD), a vibrant medical imaging research field, is expanding quickly. Because errors in medical diagnostic systems might lead to seriously misleading medical treatments, major efforts have been made in recent years to improve computer-aided diagnostics applications. The use of machine learning in computer-aided diagnosis is crucial. A simple equation may result in a false indication of items like organs. Therefore, learning from examples is a vital component of pattern recognition. Pattern recognition and machine learning in the biomedical area promise to increase the precision of disease detection and diagnosis. They also support the decision-making process's objectivity. Machine learning provides a practical method for creating elegant and autonomous algorithms to analyze high-dimensional and multimodal bio-medical data. This review article examines machine-learning algorithms for detecting diseases, including hepatitis, diabetes, liver disease, dengue fever
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#23618;&#20248;&#21270;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#21452;&#23618;&#20248;&#21270;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20004;&#20010;&#23618;&#27425;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24314;&#27169;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#23427;&#22312;&#26080;&#32447;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#21644;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00788</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#20171;&#32461;&#65306;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning. (arXiv:2308.00788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#23618;&#20248;&#21270;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#21452;&#23618;&#20248;&#21270;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20004;&#20010;&#23618;&#27425;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24314;&#27169;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#23427;&#22312;&#26080;&#32447;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#21644;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21452;&#23618;&#20248;&#21270;&#65288;BLO&#65289;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20123;&#28608;&#21160;&#20154;&#24515;&#30340;&#21457;&#23637;&#20013;&#21344;&#25454;&#20102;&#20013;&#24515;&#33310;&#21488;&#12290;&#31895;&#30053;&#22320;&#35828;&#65292;BLO&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20004;&#20010;&#23618;&#27425;&#65288;&#21363;&#19978;&#23618;&#21644;&#19979;&#23618;&#65289;&#65292;&#20854;&#20013;&#35299;&#20915;&#19978;&#23618;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#19979;&#23618;&#38382;&#39064;&#12290;BLO&#20043;&#25152;&#20197;&#21463;&#21040;&#27426;&#36814;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#22240;&#20026;&#23427;&#22312;&#24314;&#27169;&#28041;&#21450;&#20248;&#21270;&#23884;&#22871;&#30446;&#26631;&#20989;&#25968;&#30340;SP&#21644;ML&#31561;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#24378;&#22823;&#12290;BLO&#30340;&#26174;&#33879;&#24212;&#29992;&#33539;&#22260;&#20174;&#26080;&#32447;&#31995;&#32479;&#30340;&#36164;&#28304;&#20998;&#37197;&#21040;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#19968;&#31867;&#22312;SP&#21644;ML&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#21487;&#35299;BLO&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31867;BLO&#38382;&#39064;&#30340;&#19968;&#20123;&#22522;&#26412;&#27010;&#24565;&#30340;&#27010;&#36848;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#12289;&#26631;&#20934;&#31639;&#27861;&#65288;&#21253;&#25324;&#23427;&#20204;&#30340;&#20248;&#21270;&#21407;&#29702;&#21644;&#23454;&#38469;&#23454;&#29616;&#26041;&#27861;&#65289;&#65292;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#33021;&#22815;&#34987;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, bi-level optimization (BLO) has taken center stage in some very exciting developments in the area of signal processing (SP) and machine learning (ML). Roughly speaking, BLO is a classical optimization problem that involves two levels of hierarchy (i.e., upper and lower levels), wherein obtaining the solution to the upper-level problem requires solving the lower-level one. BLO has become popular largely because it is powerful in modeling problems in SP and ML, among others, that involve optimizing nested objective functions. Prominent applications of BLO range from resource allocation for wireless systems to adversarial machine learning. In this work, we focus on a class of tractable BLO problems that often appear in SP and ML applications. We provide an overview of some basic concepts of this class of BLO problems, such as their optimality conditions, standard algorithms (including their optimization principles and practical implementations), as well as how they can be levera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.16149</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;LSTM-DDPM&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#65288;ETD&#65289;&#21644;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#65288;ECF&#65289;&#26159;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#31995;&#32479;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;ETD&#21644;ECF&#30340;&#30456;&#20114;&#20851;&#32852;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#36755;&#20837;&#37325;&#26500;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#21644;&#39044;&#27979;&#35823;&#24046;&#30340;&#26041;&#27861;&#30456;&#20114;&#34917;&#20805;&#65292;&#21487;&#20197;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;ETD&#21644;ECF&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;ETD&#24615;&#33021;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21040;&#22522;&#20934;&#26041;&#27861;&#26410;&#33021;&#26816;&#27979;&#21040;&#30340;&#33021;&#37327;&#30423;&#31363;&#25915;&#20987;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;ETD&#21644;ECF&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20013;&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.13831</link><description>&lt;p&gt;
&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#19982;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13831
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20013;&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;&#34429;&#28982;SGD&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#23398;&#20064;&#29575;&#65292;&#22914;&#24120;&#25968;&#25110;&#36882;&#20943;&#30340;&#23398;&#20064;&#29575;&#65292;&#20294;&#20043;&#21069;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;SGD&#20351;&#29992;&#32447;&#25628;&#32034;&#26041;&#27861;&#32473;&#20986;&#30340;&#23398;&#20064;&#29575;&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#32473;&#20986;&#23398;&#20064;&#29575;&#30340;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;SGD&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#27493;&#25968;&#21644;&#25209;&#22823;&#23567;&#37117;&#24456;&#22823;&#26102;&#65292;&#20840;&#26799;&#24230;&#30340;&#24179;&#26041;&#33539;&#25968;&#30340;&#26399;&#26395;&#19978;&#30028;&#21464;&#23567;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#23398;&#20064;&#29575;&#30340;SGD&#26469;&#35828;&#65292;&#38750;&#20984;&#20248;&#21270;&#25152;&#38656;&#30340;&#27493;&#25968;&#26159;&#25209;&#22823;&#23567;&#30340;&#21333;&#35843;&#36882;&#20943;&#20984;&#20989;&#25968;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#38750;&#20984;&#20248;&#21270;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38543;&#26426;&#28779;&#28798;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26631;&#31614;&#25200;&#21160;&#30340;&#27169;&#22411;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#26657;&#20934;&#20108;&#36827;&#21046;&#20132;&#21449;&#29109;&#25439;&#22833;&#26469;&#32479;&#19968;&#19981;&#21516;&#24418;&#24335;&#30340;&#26631;&#31614;&#25200;&#21160;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#39044;&#27979;&#29109;&#26469;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#65292;&#24182;&#22312;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#32416;&#27491;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13539</link><description>&lt;p&gt;
&#22312;&#23494;&#38598;&#20998;&#31867;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#26631;&#31614;&#25200;&#21160;&#36827;&#34892;&#27169;&#22411;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Model Calibration in Dense Classification with Adaptive Label Perturbation. (arXiv:2307.13539v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26631;&#31614;&#25200;&#21160;&#30340;&#27169;&#22411;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#26657;&#20934;&#20108;&#36827;&#21046;&#20132;&#21449;&#29109;&#25439;&#22833;&#26469;&#32479;&#19968;&#19981;&#21516;&#24418;&#24335;&#30340;&#26631;&#31614;&#25200;&#21160;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#39044;&#27979;&#29109;&#26469;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#65292;&#24182;&#22312;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#32416;&#27491;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23433;&#20840;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#65292;&#20135;&#29983;&#21487;&#20449;&#36182;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20854;&#39044;&#27979;&#19982;&#32622;&#20449;&#24230;&#30456;&#20851;&#65292;&#21487;&#20197;&#20195;&#34920;&#27491;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#20379;&#21518;&#32493;&#20915;&#31574;&#20351;&#29992;&#12290;&#29616;&#26377;&#30340;&#23494;&#38598;&#20108;&#20998;&#31867;&#27169;&#22411;&#23481;&#26131;&#36807;&#20110;&#33258;&#20449;&#12290;&#20026;&#20102;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#38543;&#26426;&#26631;&#31614;&#25200;&#21160;&#65288;ASLP&#65289;&#65292;&#23427;&#20026;&#27599;&#20010;&#35757;&#32451;&#22270;&#20687;&#23398;&#20064;&#19968;&#20010;&#29420;&#29305;&#30340;&#26631;&#31614;&#25200;&#21160;&#32423;&#21035;&#12290;ASLP&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#26657;&#20934;&#20108;&#36827;&#21046;&#20132;&#21449;&#29109;&#65288;SC-BCE&#65289;&#25439;&#22833;&#65292;&#23558;&#21253;&#25324;&#38543;&#26426;&#26041;&#27861;&#65288;&#22914;&#25200;&#21160;&#26631;&#31614;&#65289;&#21644;&#26631;&#31614;&#24179;&#28369;&#22312;&#20869;&#30340;&#26631;&#31614;&#25200;&#21160;&#36807;&#31243;&#32479;&#19968;&#36215;&#26469;&#65292;&#20197;&#22312;&#20445;&#25345;&#20998;&#31867;&#29575;&#30340;&#21516;&#26102;&#32416;&#27491;&#26657;&#20934;&#38382;&#39064;&#12290;ASLP&#37319;&#29992;&#32463;&#20856;&#32479;&#35745;&#21147;&#23398;&#30340;&#26368;&#22823;&#29109;&#25512;&#26029;&#65292;&#20197;&#26368;&#22823;&#21270;&#30456;&#23545;&#20110;&#32570;&#22833;&#20449;&#24687;&#30340;&#39044;&#27979;&#29109;&#12290;&#23427;&#21487;&#20197;&#22312;&#20197;&#19979;&#24773;&#20917;&#19979;&#25191;&#34892;&#65306;&#65288;1&#65289;&#22312;&#24050;&#30693;&#25968;&#25454;&#19978;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#20316;&#20026;&#20445;&#23432;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#65288;2&#65289;&#19987;&#38376;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model cali
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24635;&#32467;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#30340;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30740;&#31350;&#65292;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;ST&#28023;&#27915;&#25968;&#25454;&#38598;&#21644;&#20854;&#29305;&#28857;&#65292;&#20197;&#21450;ST&#28023;&#27915;&#25968;&#25454;&#36136;&#37327;&#22686;&#24378;&#25216;&#26415;&#21644;&#29616;&#26377;STDM&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.10803</link><description>&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#30340;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#65306;&#25968;&#25454;&#12289;&#26041;&#27861;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities. (arXiv:2307.10803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24635;&#32467;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#30340;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30740;&#31350;&#65292;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;ST&#28023;&#27915;&#25968;&#25454;&#38598;&#21644;&#20854;&#29305;&#28857;&#65292;&#20197;&#21450;ST&#28023;&#27915;&#25968;&#25454;&#36136;&#37327;&#22686;&#24378;&#25216;&#26415;&#21644;&#29616;&#26377;STDM&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28023;&#27915;&#26102;&#31354;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#25366;&#25496;&#65288;STDM&#65289;&#30740;&#31350;&#26469;&#35299;&#20915;&#21508;&#31181;&#28023;&#27915;&#38382;&#39064;&#65292;&#22914;&#27668;&#20505;&#39044;&#27979;&#21644;&#28798;&#23475;&#35686;&#31034;&#12290;&#19982;&#20856;&#22411;&#30340;ST&#25968;&#25454;&#65288;&#22914;&#20132;&#36890;&#25968;&#25454;&#65289;&#30456;&#27604;&#65292;ST&#28023;&#27915;&#25968;&#25454;&#26356;&#21152;&#22797;&#26434;&#65292;&#20855;&#26377;&#19968;&#20123;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#22810;&#26679;&#21270;&#30340;&#21306;&#22495;&#24615;&#21644;&#39640;&#31232;&#30095;&#24615;&#12290;&#36825;&#20123;&#29305;&#28857;&#20351;&#24471;&#35774;&#35745;&#21644;&#35757;&#32451;STDM&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36824;&#32570;&#20047;&#23545;&#36825;&#20123;&#30740;&#31350;&#30340;&#27010;&#36848;&#65292;&#36825;&#38459;&#30861;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#22312;&#28023;&#27915;&#39046;&#22495;&#35782;&#21035;&#30740;&#31350;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#20351;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#19981;&#24895;&#24212;&#29992;&#20808;&#36827;&#30340;STDM&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#28023;&#27915;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;STDM&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;ST&#28023;&#27915;&#25968;&#25454;&#38598;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#29305;&#28857;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20856;&#22411;&#30340;ST&#28023;&#27915;&#25968;&#25454;&#36136;&#37327;&#22686;&#24378;&#25216;&#26415;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;STDM&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, e.g., climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models. Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research issues in ocean while discouraging researchers in ocean science from applying advanced STDM techniques. To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean. Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics. Then, typical ST ocean data quality enhancement techniques are discussed. Next, we classify existing STDM st
&lt;/p&gt;</description></item><item><title>AnyTeleop&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#35273;&#23548;&#21521;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#25903;&#25345;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#26800;&#33218;&#12289;&#25163;&#37096;&#12289;&#29615;&#22659;&#21644;&#25668;&#20687;&#22836;&#37197;&#32622;&#65292;&#22312;&#23454;&#38469;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04577</link><description>&lt;p&gt;
AnyTeleop: &#36890;&#29992;&#35270;&#35273;&#23548;&#21521;&#30340;&#28789;&#24039;&#26426;&#26800;&#33218;&#25163;&#25805;&#20316;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System. (arXiv:2307.04577v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04577
&lt;/p&gt;
&lt;p&gt;
AnyTeleop&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#35273;&#23548;&#21521;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#25903;&#25345;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#26800;&#33218;&#12289;&#25163;&#37096;&#12289;&#29615;&#22659;&#21644;&#25668;&#20687;&#22836;&#37197;&#32622;&#65292;&#22312;&#23454;&#38469;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#36828;&#31243;&#25805;&#20316;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19982;&#29615;&#22659;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#20302;&#25104;&#26412;&#30340;&#25668;&#20687;&#22836;&#20256;&#24863;&#22120;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#26159;&#38024;&#23545;&#29305;&#23450;&#26426;&#22120;&#20154;&#27169;&#22411;&#21644;&#37096;&#32626;&#29615;&#22659;&#36827;&#34892;&#35774;&#35745;&#21644;&#24037;&#31243;&#21270;&#30340;&#65292;&#38543;&#30528;&#26426;&#22120;&#20154;&#27169;&#22411;&#30340;&#22686;&#21152;&#21644;&#25805;&#20316;&#29615;&#22659;&#30340;&#22810;&#26679;&#21270;&#65292;&#20854;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AnyTeleop&#65292;&#19968;&#20010;&#32479;&#19968;&#21644;&#36890;&#29992;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#25903;&#25345;&#22312;&#21333;&#20010;&#31995;&#32479;&#20013;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#26800;&#33218;&#12289;&#25163;&#37096;&#12289;&#29616;&#23454;&#29615;&#22659;&#21644;&#25668;&#20687;&#22836;&#37197;&#32622;&#12290;&#23613;&#31649;&#35774;&#35745;&#20855;&#26377;&#36873;&#25321;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#30828;&#20214;&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22312;&#23454;&#38469;&#23454;&#39564;&#20013;&#65292;AnyTeleop&#21487;&#20197;&#20197;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#20987;&#36133;&#20026;&#29305;&#23450;&#26426;&#22120;&#20154;&#30828;&#20214;&#35774;&#35745;&#30340;&#20197;&#21069;&#31995;&#32479;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#20154;&#12290;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#36828;&#31243;&#25805;&#20316;&#26102;&#65292;AnyTeleop&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#24182;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#30340;&#35774;&#32622;&#65292;&#20855;&#26377;&#39640;&#25928;&#36816;&#34892;&#12289;&#24555;&#36895;&#25628;&#32034;&#21644;&#20934;&#32447;&#24615;&#31354;&#38388;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.13773</link><description>&lt;p&gt;
&#24102;&#26377;Bandit&#21453;&#39304;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbour with Bandit Feedback. (arXiv:2306.13773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#24182;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#30340;&#35774;&#32622;&#65292;&#20855;&#26377;&#39640;&#25928;&#36816;&#34892;&#12289;&#24555;&#36895;&#25628;&#32034;&#21644;&#20934;&#32447;&#24615;&#31354;&#38388;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26368;&#36817;&#37051;&#31639;&#27861;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#20013;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22788;&#29702;&#20102;&#23436;&#20840;&#23545;&#25239;&#30340;&#35774;&#32622;&#65292;&#21363;&#19981;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#24403;&#19982;&#24555;&#36895;&#25968;&#25454;&#32467;&#26500;&#65288;&#21487;&#33021;&#26159;&#36817;&#20284;&#30340;&#33258;&#36866;&#24212;&#26368;&#36817;&#37051;&#25628;&#32034;&#65292;&#22914;&#23548;&#33322;&#32593;&#32476;&#65289;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#38750;&#24120;&#39640;&#25928;-&#27599;&#27425;&#35797;&#39564;&#30340;&#36816;&#34892;&#26102;&#38388;&#23545;&#21160;&#20316;&#25968;&#21644;&#35797;&#39564;&#25968;&#21576;&#23545;&#25968;&#22810;&#39033;&#24335;&#22686;&#38271;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#20934;&#32447;&#24615;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we adapt the nearest neighbour rule to the contextual bandit problem. Our algorithm handles the fully adversarial setting in which no assumptions at all are made about the data-generation process. When combined with a sufficiently fast data-structure for (perhaps approximate) adaptive nearest neighbour search, such as a navigating net, our algorithm is extremely efficient - having a per trial running time polylogarithmic in both the number of trials and actions, and taking only quasi-linear space.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12344</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#35777;&#26126;&#31934;&#30830;&#30340;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An efficient, provably exact algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#20855;&#26377;&#24736;&#20037;&#30340;&#21382;&#21490;&#65292;&#33267;&#23569;&#21487;&#20197;&#36861;&#28335;&#21040;1936&#24180;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#12290;&#23545;&#20110;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#35768;&#22810;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24471;&#21040;&#30456;&#24212;&#30340;0-1&#25439;&#22833;&#20998;&#31867;&#38382;&#39064;&#30340;&#31934;&#30830;&#35299;&#65292;&#20294;&#23545;&#20110;&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#24050;&#32463;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#22312;&#23436;&#20840;&#33539;&#22260;&#20869;&#26159;NP&#38590;&#30340;&#12290;&#25152;&#26377;&#26367;&#20195;&#26041;&#27861;&#37117;&#28041;&#21450;&#26576;&#31181;&#24418;&#24335;&#30340;&#36817;&#20284;&#65292;&#21253;&#25324;&#20351;&#29992;0-1&#25439;&#22833;&#30340;&#20195;&#29702;&#65288;&#20363;&#22914;hinge&#25110;logistic&#25439;&#22833;&#65289;&#25110;&#36817;&#20284;&#30340;&#32452;&#21512;&#25628;&#32034;&#65292;&#36825;&#20123;&#37117;&#19981;&#33021;&#20445;&#35777;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#12290;&#25214;&#21040;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#26377;&#25928;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#30340;&#26500;&#24314;&#36807;&#31243;&#65292;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#65292;&#23427;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;0-1&#25439;&#22833;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for solving the linear classification problem have a long history, dating back at least to 1936 with linear discriminant analysis. For linearly separable data, many algorithms can obtain the exact solution to the corresponding 0-1 loss classification problem efficiently, but for data which is not linearly separable, it has been shown that this problem, in full generality, is NP-hard. Alternative approaches all involve approximations of some kind, including the use of surrogates for the 0-1 loss (for example, the hinge or logistic loss) or approximate combinatorial search, none of which can be guaranteed to solve the problem exactly. Finding efficient algorithms to obtain an exact i.e. globally optimal solution for the 0-1 loss linear classification problem with fixed dimension, remains an open problem. In research we report here, we detail the construction of a new algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss classification problem exactly in po
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.11363</link><description>&lt;p&gt;
&#21463;&#36974;&#34109;&#25193;&#25955;&#27169;&#22411;&#26159;&#24555;&#36895;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#20107;&#23454;&#19978;&#25216;&#26415;&#65292;&#28982;&#32780;&#23427;&#20204;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#35813;&#25216;&#26415;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36890;&#36807;&#36974;&#34109;&#36755;&#20837;&#22270;&#20687;&#30340;&#39640;&#27604;&#20363;&#65288;&#20363;&#22914;&#39640;&#36798;90&#65285;&#65289;&#65292;&#24182;&#21033;&#29992;&#36974;&#34109;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#26469;&#21435;&#22122;&#21487;&#35265;&#21306;&#22495;&#65292;&#20174;&#32780;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#26174;&#33879;&#30340;&#29305;&#24449;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#36974;&#34109;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;CelebA-HQ $256 \times 256$&#20687;&#32032;&#31354;&#38388;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#20102;&#22522;&#20110;ViT&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;4&#20493;&#21152;&#36895;&#65292;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#19982;&#21435;&#22122;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as the \emph{de-facto} technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90\%) of the input image and employing masked denoising score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing masked learning in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to denoisin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.10045</link><description>&lt;p&gt;
&#39044;&#27979;&#26230;&#20307;&#24615;&#36136;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#39640;&#25928;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#39044;&#27979;&#12290;&#26230;&#20307;&#32467;&#26500;&#30001;&#19968;&#20010;&#26368;&#23567;&#30340;&#21333;&#20803;&#26684;&#32452;&#25104;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#37325;&#22797;&#12290;&#22914;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20934;&#30830;&#34920;&#31034;&#36825;&#31181;&#37325;&#22797;&#32467;&#26500;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#22312;&#38468;&#36817;&#30340;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36793;&#32536;&#26469;&#26500;&#24314;&#22270;&#24418;&#65292;&#22240;&#27492;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#30340;&#27169;&#24335;&#21644;&#36828;&#36317;&#31163;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#24314;&#27169;&#29289;&#29702;&#21407;&#29702;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#20351;&#29992;&#36317;&#31163;&#65292;&#22914;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25152;&#20570;&#30340;&#12290;&#36825;&#20123;&#21183;&#21253;&#25324;&#24211;&#20177;&#21183;&#65292;&#20262;&#25958;&#20998;&#25955;&#21183;&#21644;Pauli&#26021;&#21147;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#27169;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#23436;&#25972;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38468;&#36817;&#21407;&#23376;&#20043;&#38388;&#30340;&#21183;&#12290;&#36825;&#24471;&#30410;&#20110;&#25105;&#20204;&#29992;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#36924;&#36817;&#26080;&#38480;&#21183;&#21644;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#22320;&#21457;&#29616;&#19981;&#21516;&#30340;&#29983;&#25104;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#20123;&#29983;&#25104;&#27010;&#24565;&#21487;&#20197;&#34987;&#29992;&#20110;&#37325;&#26032;&#32452;&#21512;&#21644;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#29992;&#20110;&#19979;&#28216;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.05357</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#19979;&#30340;&#32452;&#21512;&#24335;&#27010;&#24565;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#22320;&#21457;&#29616;&#19981;&#21516;&#30340;&#29983;&#25104;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#20123;&#29983;&#25104;&#27010;&#24565;&#21487;&#20197;&#34987;&#29992;&#20110;&#37325;&#26032;&#32452;&#21512;&#21644;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#29992;&#20110;&#19979;&#28216;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21512;&#25104;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#38656;&#35201;&#29992;&#25143;&#25351;&#23450;&#20182;&#20204;&#24819;&#35201;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#30456;&#21453;&#30340;&#38382;&#39064;&#8212;&#8212;&#22312;&#32473;&#20986;&#30340;&#19981;&#21516;&#22270;&#20687;&#38598;&#21512;&#20013;&#65292;&#25105;&#20204;&#33021;&#21542;&#21457;&#29616;&#20195;&#34920;&#27599;&#20010;&#22270;&#20687;&#30340;&#29983;&#25104;&#27010;&#24565;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20174;&#19968;&#32452;&#22270;&#20687;&#20013;&#21457;&#29616;&#29983;&#25104;&#30340;&#27010;&#24565;&#65292;&#23558;&#32472;&#30011;&#20013;&#19981;&#21516;&#30340;&#33402;&#26415;&#39118;&#26684;&#65292;&#23545;&#35937;&#21644;&#29031;&#26126;&#20174;&#21416;&#25151;&#22330;&#26223;&#20013;&#20998;&#35299;&#20986;&#26469;&#65292;&#24182;&#36890;&#36807;&#32473;&#23450;&#30340;ImageNet&#22270;&#20687;&#21457;&#29616;&#22270;&#20687;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#29983;&#25104;&#27010;&#24565;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#22270;&#20687;&#30340;&#20869;&#23481;&#65292;&#33021;&#22815;&#37325;&#26032;&#32452;&#21512;&#21644;&#32452;&#25104;&#20197;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#31181;&#34920;&#31034;&#26469;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem -- given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#28508;&#22312;&#22270;&#21040;&#28508;&#22312;&#25299;&#25169;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#20998;&#30340;&#21333;&#22797;&#24418;&#27169;&#22359;&#65288;DCM&#65289;&#65292;&#23398;&#20064;&#25551;&#36848;&#25968;&#25454;&#28857;&#20043;&#38388;&#22810;&#21521;&#20132;&#20114;&#30340;&#39640;&#38454;&#21333;&#22797;&#24418;&#30340;&#31232;&#30095;&#19988;&#19981;&#35268;&#21017;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#21333;&#22797;&#24418;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#23618;&#38598;&#25104;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16174</link><description>&lt;p&gt;
&#20174;&#28508;&#22312;&#22270;&#21040;&#28508;&#22312;&#25299;&#25169;&#25512;&#26029;&#65306;&#21487;&#24494;&#20998;&#30340;&#21333;&#22797;&#24418;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module. (arXiv:2305.16174v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16174
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#28508;&#22312;&#22270;&#21040;&#28508;&#22312;&#25299;&#25169;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#20998;&#30340;&#21333;&#22797;&#24418;&#27169;&#22359;&#65288;DCM&#65289;&#65292;&#23398;&#20064;&#25551;&#36848;&#25968;&#25454;&#28857;&#20043;&#38388;&#22810;&#21521;&#20132;&#20114;&#30340;&#39640;&#38454;&#21333;&#22797;&#24418;&#30340;&#31232;&#30095;&#19988;&#19981;&#35268;&#21017;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#21333;&#22797;&#24418;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#23618;&#38598;&#25104;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#22270;&#25512;&#26029;&#65288;LGI&#65289;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#26469;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23545;&#32473;&#23450;&#22270;&#25299;&#25169;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LGI&#26041;&#27861;&#20551;&#35774;&#23384;&#22312;&#65288;&#22122;&#22768;&#12289;&#19981;&#23436;&#25972;&#12289;&#21487;&#25913;&#36827;&#30340;...&#65289;&#36755;&#20837;&#22270;&#26469;&#37325;&#26032;&#36830;&#25509;&#65292;&#24182;&#19988;&#21482;&#33021;&#23398;&#20064;&#24120;&#35268;&#30340;&#22270;&#25299;&#25169;&#12290;&#22312;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65288;TDL&#65289;&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#23398;&#20064;&#25551;&#36848;&#25968;&#25454;&#28857;&#20043;&#38388;&#22810;&#21521;&#20132;&#20114;&#30340;&#39640;&#38454;&#21333;&#22797;&#24418;&#65288;&#20855;&#26377;&#31232;&#30095;&#19988;&#19981;&#35268;&#21017;&#30340;&#25299;&#25169;&#32467;&#26500;&#65289;&#30340;&#28508;&#22312;&#25299;&#25169;&#25512;&#26029;&#65288;LTI&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#24494;&#20998;&#30340;&#21333;&#22797;&#24418;&#27169;&#22359;&#65288;DCM&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#22797;&#26434;&#20013;&#21333;&#20803;&#27010;&#29575;&#30340;&#26032;&#22411;&#21487;&#23398;&#20064;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;DCM&#19982;&#21333;&#22797;&#24418;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#23618;&#38598;&#25104;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#20004;&#27493;&#25512;&#26029;&#36807;&#31243;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#23545;&#36755;&#20837;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#21333;&#20803;&#36827;&#34892;&#35814;&#23613;&#25628;&#32034;&#65292;&#20174;&#32780;&#20445;&#25345;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#39640;&#38454;&#25299;&#25169;&#25512;&#26029;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message passing networks layers and train it in a end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several ho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Mlinear&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#36890;&#36947;&#29420;&#31435;&#24615;&#21644;&#36890;&#36947;&#20381;&#36182;&#24615;&#23646;&#24615;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04800</link><description>&lt;p&gt;
Mlinear:&#37325;&#26032;&#24605;&#32771;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mlinear: Rethink the Linear Model for Time-series Forecasting. (arXiv:2305.04800v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Mlinear&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#36890;&#36947;&#29420;&#31435;&#24615;&#21644;&#36890;&#36947;&#20381;&#36182;&#24615;&#23646;&#24615;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#20851;&#27880;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24615;&#36136;&#65292;&#20363;&#22914;&#36890;&#36947;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#21644;&#36890;&#36947;&#20381;&#36182;&#24615;&#65288;CD&#65289;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#35774;&#35745;&#22797;&#26434;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#29420;&#30340;CI&#25110;CD&#19978;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#20004;&#20010;&#30456;&#21453;&#30340;&#23646;&#24615;&#20197;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;CI&#21644;CD&#30340;&#30456;&#21453;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36804;&#20170;&#26410;&#33021;&#26377;&#25928;&#22238;&#31572;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#21363;&#8220;&#22914;&#20309;&#26377;&#25928;&#22320;&#28151;&#21512;&#26102;&#38388;&#24207;&#21015;&#30340;CI&#21644;CD&#23646;&#24615;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mlinear&#65288;MIX-Linear&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#32447;&#24615;&#23618;&#30340;&#26041;&#27861;&#12290;Mlinear&#30340;&#35774;&#35745;&#29702;&#24565;&#20027;&#35201;&#21253;&#25324;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#22522;&#20110;&#21160;&#24577;&#35843;&#33410;CI&#21644;CD&#23646;&#24615;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recently, significant advancements have been made in time-series forecasting research, with an increasing focus on analyzing the nature of time-series data, e.g, channel-independence (CI) and channel-dependence (CD), rather than solely focusing on designing sophisticated forecasting models. However, current research has primarily focused on either CI or CD in isolation, and the challenge of effectively combining these two opposing properties to achieve a synergistic effect remains an unresolved issue. In this paper, we carefully examine the opposing properties of CI and CD, and raise a practical question that has not been effectively answered, e.g.,"How to effectively mix the CI and CD properties of time series to achieve better predictive performance?" To answer this question, we propose Mlinear (MIX-Linear), a simple yet effective method based mainly on linear layers. The design philosophy of Mlinear mainly includes two aspects:(1) dynamically tuning the CI and CD properties based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26085;&#24694;&#24847;&#36719;&#20214;&#30340;&#20998;&#31867;&#21644;&#22312;&#32447;&#32858;&#31867;&#12290;&#23454;&#39564;&#20351;&#29992; EMBER &#25968;&#25454;&#38598;&#65292;&#23545;&#26377;&#27969;&#20837;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24471;&#21040;&#20102; 95.33% &#30340;&#24179;&#34913;&#20934;&#30830;&#24230;&#12290;&#22312;&#21097;&#19979;&#30340;&#25968;&#25454;&#20013;&#65292;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#23454;&#29616;&#20102;&#32431;&#24230;&#20174; 47.61% &#21040; 77.68% &#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.00605</link><description>&lt;p&gt;
&#38646;&#26085;&#24694;&#24847;&#36719;&#20214;&#30340;&#20998;&#31867;&#21644;&#22312;&#32447;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification and Online Clustering of Zero-Day Malware. (arXiv:2305.00605v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26085;&#24694;&#24847;&#36719;&#20214;&#30340;&#20998;&#31867;&#21644;&#22312;&#32447;&#32858;&#31867;&#12290;&#23454;&#39564;&#20351;&#29992; EMBER &#25968;&#25454;&#38598;&#65292;&#23545;&#26377;&#27969;&#20837;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24471;&#21040;&#20102; 95.33% &#30340;&#24179;&#34913;&#20934;&#30830;&#24230;&#12290;&#22312;&#21097;&#19979;&#30340;&#25968;&#25454;&#20013;&#65292;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#23454;&#29616;&#20102;&#32431;&#24230;&#20174; 47.61% &#21040; 77.68% &#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#20135;&#29983;&#22823;&#37327;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#65292;&#25105;&#20204;&#38656;&#35201;&#23558;&#20854;&#19982;&#33391;&#24615;&#26679;&#26412;&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#30740;&#31350;&#29616;&#26377;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#26159;&#22914;&#20309;&#21457;&#23637;&#65292;&#20197;&#21450;&#22914;&#20309;&#26816;&#26597;&#26032;&#20986;&#29616;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#23545;&#20837;&#20405;&#26679;&#26412;&#36827;&#34892;&#22312;&#32447;&#22788;&#29702;&#65292;&#23558;&#20854;&#20998;&#37197;&#32473;&#29616;&#26377;&#23478;&#26063;&#65292;&#25110;&#22312;&#26032;&#23478;&#26063;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992; EMBER &#25968;&#25454;&#38598;&#20013;&#30340;&#19971;&#20010;&#27969;&#34892;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#65292;&#20854;&#20013;&#22235;&#20010;&#22312;&#35757;&#32451;&#38598;&#20013;&#65292;&#21478;&#22806;&#19977;&#20010;&#22312;&#27979;&#35797;&#38598;&#20013;&#12290;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#20998;&#31867;&#24471;&#20998;&#65292;&#25105;&#20204;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#23558;&#34987;&#20998;&#31867;&#65292;&#21738;&#20123;&#23558;&#34987;&#32858;&#31867;&#21040;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#20013;&#12290;&#25105;&#20204;&#20197;&#24179;&#34913;&#20934;&#30830;&#24230;&#20026; 95.33% &#23545; 97.21% &#30340;&#27969;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#28982;&#21518;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#23545;&#21097;&#20313;&#25968;&#25454;&#36827;&#34892;&#20102;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#32431;&#24230;&#20174;&#22235;&#20010;&#32858;&#31867;&#30340; 47.61% &#21040;&#21313;&#20010;&#32858;&#31867;&#30340; 77.68%&#12290;
&lt;/p&gt;
&lt;p&gt;
A large amount of new malware is constantly being generated, which must not only be distinguished from benign samples, but also classified into malware families. For this purpose, investigating how existing malware families are developed and examining emerging families need to be explored. This paper focuses on the online processing of incoming malicious samples to assign them to existing families or, in the case of samples from new families, to cluster them. We experimented with seven prevalent malware families from the EMBER dataset, with four in the training set and three additional new families in the test set. Based on the classification score of the multilayer perceptron, we determined which samples would be classified and which would be clustered into new malware families. We classified 97.21% of streaming data with a balanced accuracy of 95.33%. Then, we clustered the remaining data using a self-organizing map, achieving a purity from 47.61% for four clusters to 77.68% for ten 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#33258;&#21160;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#24418;&#24577;&#20998;&#31867;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.12729</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#23545;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#36827;&#34892;&#24418;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Morphological Classification of Extragalactic Radio Sources Using Gradient Boosting Methods. (arXiv:2304.12729v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#33258;&#21160;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#24418;&#24577;&#20998;&#31867;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#30340;&#23556;&#30005;&#26395;&#36828;&#38236;&#30340;&#24314;&#25104;&#65292;&#26080;&#32447;&#30005;&#22825;&#25991;&#23398;&#30340;&#25968;&#25454;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#24418;&#24577;&#20998;&#31867;&#33073;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#30340;&#33258;&#21160;&#20998;&#31867;&#26159;&#35813;&#39046;&#22495;&#20013;&#26368;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22823;&#37096;&#20998;&#20851;&#20110;&#35813;&#39046;&#22495;&#30340;&#26368;&#36817;&#30340;&#25104;&#26524;&#37117;&#26159;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#26412;&#25991;&#21017;&#25552;&#20986;&#20102;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#20316;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;XGBoost&#65292;LightGBM&#21644;CatBoost&#23454;&#29616;&#30340;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#23545;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#35813;&#20998;&#31867;&#22120;&#23558;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of radio astronomy is witnessing a boom in the amount of data produced per day due to newly commissioned radio telescopes. One of the most crucial problems in this field is the automatic classification of extragalactic radio sources based on their morphologies. Most recent contributions in the field of morphological classification of extragalactic radio sources have proposed classifiers based on convolutional neural networks. Alternatively, this work proposes gradient boosting machine learning methods accompanied by principal component analysis as data-efficient alternatives to convolutional neural networks. Recent findings have shown the efficacy of gradient boosting methods in outperforming deep learning methods for classification problems with tabular data. The gradient boosting methods considered in this work are based on the XGBoost, LightGBM, and CatBoost implementations. This work also studies the effect of dataset size on classifier performance. A three-class classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20302;&#20998;&#36776;&#29575;LES&#25968;&#25454;&#37325;&#24314;&#36830;&#32493;&#30340;DNS&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#28237;&#27969;&#27969;&#22330;&#30340;&#26102;&#31354;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12130</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#24863;&#30693;&#30340;&#26102;&#31354;&#21160;&#21147;&#23398;&#21644;&#27979;&#35797;&#26102;&#38388;&#32454;&#21270;&#37325;&#24314;&#28237;&#27969;&#27969;&#22330;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics and Test-Time Refinement. (arXiv:2304.12130v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20302;&#20998;&#36776;&#29575;LES&#25968;&#25454;&#37325;&#24314;&#36830;&#32493;&#30340;DNS&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#28237;&#27969;&#27969;&#22330;&#30340;&#26102;&#31354;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#28237;&#27969;&#23545;&#20110;&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#12289;&#29615;&#22659;&#31185;&#23398;&#12289;&#33021;&#28304;&#34892;&#19994;&#21644;&#29983;&#29289;&#21307;&#23398;&#31561;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#30001;&#20110;&#20854;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#34987;&#24191;&#27867;&#29992;&#20316;&#27169;&#25311;&#28237;&#27969;&#27969;&#22330;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#65288;DNS&#65289;&#12290;&#28982;&#32780;&#65292;LES&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#28237;&#27969;&#36816;&#36755;&#30340;&#25152;&#26377;&#23610;&#24230;&#12290;&#20174;&#20302;&#20998;&#36776;&#29575;LES&#37325;&#24314;DNS&#23545;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#23398;&#31185;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#28237;&#27969;&#27969;&#22330;&#30340;&#26102;&#31354;&#22797;&#26434;&#24615;&#65292;&#36825;&#32473;&#29616;&#26377;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#25552;&#20986;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#20302;&#20998;&#36776;&#29575;LES&#25968;&#25454;&#37325;&#24314;&#36830;&#32493;&#30340;DNS&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#27969;&#21160;&#21160;&#21147;&#23398;&#24213;&#23618;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#26102;&#31354;&#27169;&#22411;&#26550;&#26500;&#30340;&#35774;&#35745;&#20013;&#36827;&#34892;&#24314;&#27169;&#12290;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38477;&#32423;&#30340;&#32454;&#21270;&#26041;&#27861;&#65292;&#20197;&#24378;&#21046;&#23454;&#26045;p
&lt;/p&gt;
&lt;p&gt;
Simulating turbulence is critical for many societally important applications in aerospace engineering, environmental science, the energy industry, and biomedicine. Large eddy simulation (LES) has been widely used as an alternative to direct numerical simulation (DNS) for simulating turbulent flows due to its reduced computational cost. However, LES is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the spatio-temporal complexity of turbulent flows. In this work, we propose a new physics-guided neural network for reconstructing the sequential DNS from low-resolution LES data. The proposed method leverages the partial differential equation that underlies the flow dynamics in the design of spatio-temporal model architecture. A degradation-based refinement method is also developed to enforce p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#20272;&#35745;&#24773;&#32490;&#21453;&#24212;&#24378;&#24230;&#12290;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10741</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20272;&#35745;&#24773;&#32490;&#21453;&#24212;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Computer Vision Estimation of Emotion Reaction Intensity in the Wild. (arXiv:2303.10741v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#20272;&#35745;&#24773;&#32490;&#21453;&#24212;&#24378;&#24230;&#12290;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#22312;&#20154;&#31867;&#27807;&#36890;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#24320;&#21457;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#24773;&#32490;&#34920;&#36798;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#25552;&#20379;&#24110;&#21161;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#25968;&#23383;&#21270;&#34892;&#20026;&#21307;&#30103;&#21644;&#23186;&#20307;&#20998;&#26512;&#12290;&#22312;&#24773;&#24863;&#35745;&#31639;&#30740;&#31350;&#20013;&#65292;&#20256;&#32479;&#19978;&#26377;&#19977;&#31181;&#24773;&#32490;&#34920;&#36798;&#30340;&#24314;&#27169;&#26041;&#24335;&#65306;&#34892;&#21160;&#21333;&#20803;&#12289;&#24773;&#24863;&#20215;&#20540;&#19982;&#21796;&#36215;&#24230;&#65288;VA&#65289;&#21644;&#20998;&#31867;&#24773;&#32490;&#12290;&#20316;&#20026;&#36229;&#36234;&#36825;&#20123;&#34920;&#36798;&#26041;&#24335;&#65292;&#26397;&#30528;&#26356;&#31934;&#32454;&#26631;&#31614;&#30340;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#37326;&#22806;&#24773;&#32490;&#34892;&#20026;&#20998;&#26512;&#65288;ABAW&#65289;&#31532;&#20116;&#27425;&#27604;&#36187;&#20013;&#25552;&#20132;&#30340;&#27169;&#22411;&#65292;&#35813;&#27604;&#36187;&#24341;&#20837;&#20102;&#24773;&#32490;&#21453;&#24212;&#24378;&#24230;&#65288;ERI&#65289;&#20272;&#35745;&#25361;&#25112;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22235;&#20010;&#22312;&#35270;&#35273;&#39046;&#22495;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#38899;&#39057;&#29305;&#24449;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26469;&#39044;&#27979;&#24773;&#32490;&#21453;&#24212;&#24378;&#24230;&#12290;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#26368;&#20339;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;0.4080&#30340;&#24179;&#22343;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotions play an essential role in human communication. Developing computer vision models for automatic recognition of emotion expression can aid in a variety of domains, including robotics, digital behavioral healthcare, and media analytics. There are three types of emotional representations which are traditionally modeled in affective computing research: Action Units, Valence Arousal (VA), and Categorical Emotions. As part of an effort to move beyond these representations towards more fine-grained labels, we describe our submission to the newly introduced Emotional Reaction Intensity (ERI) Estimation challenge in the 5th competition for Affective Behavior Analysis in-the-Wild (ABAW). We developed four deep neural networks trained in the visual domain and a multimodal model trained with both visual and audio features to predict emotion reaction intensity. Our best performing model on the Hume-Reaction dataset achieved an average Pearson correlation coefficient of 0.4080 on the test se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#21363;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#21644;&#20107;&#20214;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#32771;&#34385;&#36825;&#20004;&#20010;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.10112</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;:&#32508;&#36848;&#21644;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Temporal Data: An Overview and New Perspectives. (arXiv:2303.10112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#21363;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#21644;&#20107;&#20214;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#32771;&#34385;&#36825;&#20004;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25968;&#25454;&#20195;&#34920;&#30528;&#22797;&#26434;&#31995;&#32479;&#30340;&#26102;&#38388;&#39034;&#24207;&#35266;&#27979;&#65292;&#21487;&#20197;&#34987;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#29983;&#25104;&#65292;&#20363;&#22914;&#24037;&#19994;&#12289;&#21307;&#30103;&#21644;&#37329;&#34701;&#12290;&#20998;&#26512;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#39044;&#27979;&#12290;&#20854;&#20013;&#65292;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#30340;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#24037;&#20316;&#21487;&#20197;&#26681;&#25454;&#26102;&#38388;&#25968;&#25454;&#26159;&#21542;&#34987;&#26657;&#20934;&#26469;&#20998;&#20026;&#20004;&#20010;&#39640;&#24230;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#21363;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#21644;&#20107;&#20214;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65292;&#24573;&#30053;&#20102;&#31532;&#20108;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#36825;&#20004;&#20010;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#32771;&#34385;&#36825;&#20004;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal data, representing chronological observations of complex systems, has always been a typical data structure that can be widely generated by many domains, such as industry, medicine and finance. Analyzing this type of data is extremely valuable for various applications. Thus, different temporal data analysis tasks, eg, classification, clustering and prediction, have been proposed in the past decades. Among them, causal discovery, learning the causal relations from temporal data, is considered an interesting yet critical task and has attracted much research attention. Existing casual discovery works can be divided into two highly correlated categories according to whether the temporal data is calibrated, ie, multivariate time series casual discovery, and event sequence casual discovery. However, most previous surveys are only focused on the time series casual discovery and ignore the second category. In this paper, we specify the correlation between the two categories and provide
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22235;&#32500;CTP&#20840;&#38754;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#65292;&#20197;&#20998;&#21106;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.08757</link><description>&lt;p&gt;
&#21033;&#29992;&#22235;&#32500;CT&#28748;&#27880;&#25104;&#20687;&#23545;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke. (arXiv:2303.08757v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22235;&#32500;CTP&#20840;&#38754;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#65292;&#20197;&#20998;&#21106;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#12289;&#24555;&#36895;&#30340;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#65288;AIS&#65289;&#24739;&#32773;&#32570;&#34880;&#21306;&#65288;&#26680;&#24515;&#21644;&#21322;&#24433;&#21306;&#65289;&#39044;&#27979;&#26041;&#27861;&#23545;&#20110;&#25913;&#36827;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#30097;&#20284;AIS&#24739;&#32773;&#26089;&#26399;&#35780;&#20272;&#30340;&#20027;&#35201;&#27169;&#24335;&#20043;&#19968;&#12290;CT&#28748;&#27880;&#25104;&#20687;&#65288;CTP&#65289;&#36890;&#24120;&#29992;&#20316;&#20027;&#35201;&#35780;&#20272;&#25163;&#27573;&#65292;&#20197;&#30830;&#23450;&#21330;&#20013;&#20301;&#32622;&#12289;&#20005;&#37325;&#31243;&#24230;&#21644;&#32570;&#34880;&#24615;&#30149;&#28790;&#20307;&#31215;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;CTP&#33258;&#21160;&#20998;&#21106;&#26041;&#27861;&#37117;&#20351;&#29992;&#24050;&#32463;&#22788;&#29702;&#36807;&#30340;&#19977;&#32500;&#24425;&#33394;&#22320;&#22270;&#20316;&#20026;&#25918;&#23556;&#31185;&#21307;&#24072;&#24120;&#35268;&#35270;&#35273;&#35780;&#20272;&#30340;&#36755;&#20837;&#12290;&#25110;&#32773;&#65292;&#22522;&#20110;&#20999;&#29255;&#30340;&#20108;&#32500;+&#26102;&#38388;&#36755;&#20837;&#20351;&#29992;&#21407;&#22987;CTP&#25968;&#25454;&#65292;&#20854;&#20013;&#24573;&#30053;&#20102;&#22312;&#20307;&#31215;&#19978;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#26041;&#27861;&#26469;&#21033;&#29992;&#25972;&#20010;&#22235;&#32500;CTP&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;4D&#21367;&#31215;&#23618;&#12290;&#25105;&#20204;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise and fast prediction methods for ischemic areas (core and penumbra) in acute ischemic stroke (AIS) patients are of significant clinical interest: they play an essential role in improving diagnosis and treatment planning. Computed Tomography (CT) scan is one of the primary modalities for early assessment in patients with suspected AIS. CT Perfusion (CTP) is often used as a primary assessment to determine stroke location, severity, and volume of ischemic lesions. Current automatic segmentation methods for CTP mostly use already processed 3D color maps conventionally used for visual assessment by radiologists as input. Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time input, where the spatial information over the volume is ignored. In this paper, we investigate different methods to utilize the entire 4D CTP as input to fully exploit the spatio-temporal information. This leads us to propose a novel 4D convolution layer. Our comprehensive experiments on a l
&lt;/p&gt;</description></item><item><title>SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05118</link><description>&lt;p&gt;
SLCA: &#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05118
&lt;/p&gt;
&lt;p&gt;
SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#22312;&#23398;&#20064;&#39034;&#24207;&#21040;&#36798;&#30340;&#25968;&#25454;&#20013;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#29616;&#26377;&#24037;&#20316;&#37117;&#24314;&#31435;&#22312;&#20174;&#22836;&#23398;&#20064;&#30340;&#21069;&#25552;&#19979;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#24050;&#32463;&#33268;&#21147;&#20110;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#27599;&#20010;&#22686;&#37327;&#20219;&#21153;&#20013;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;&#20851;&#38190;&#25361;&#25112;&#24402;&#22240;&#20110;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#35266;&#23519;&#21040;&#22312;&#34920;&#24449;&#23618;&#27425;&#19978;&#36873;&#25321;&#24615;&#38477;&#20302;&#23398;&#20064;&#29575;&#20960;&#20046;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;&#65288;SLCA&#65289;&#65292;&#36890;&#36807;&#24314;&#27169;&#31867;&#21035;&#20998;&#24067;&#24182;&#22312;&#20107;&#21518;&#23545;&#40784;&#20998;&#31867;&#23618;&#27425;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20998;&#31867;&#23618;&#27425;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SLCA&#22312;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22343;&#26041;&#24046;&#20272;&#35745;&#32593;&#32476;&#30340;&#26368;&#20248;&#23454;&#29616;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#39044;&#28909;&#26399;&#21487;&#20197;&#36991;&#20813;&#25910;&#25947;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2302.08875</link><description>&lt;p&gt;
&#26368;&#20248;&#35757;&#32451;&#22343;&#26041;&#24046;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Optimal Training of Mean Variance Estimation Neural Networks. (arXiv:2302.08875v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22343;&#26041;&#24046;&#20272;&#35745;&#32593;&#32476;&#30340;&#26368;&#20248;&#23454;&#29616;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#39044;&#28909;&#26399;&#21487;&#20197;&#36991;&#20813;&#25910;&#25947;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22343;&#26041;&#24046;&#20272;&#35745;&#32593;&#32476;&#65288;MVE&#32593;&#32476;&#65289;&#30340;&#26368;&#20248;&#23454;&#29616;&#12290;&#36825;&#31181;&#32593;&#32476;&#32463;&#24120;&#34987;&#29992;&#20316;&#22238;&#24402;&#35774;&#32622;&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#20363;&#22914;Concrete dropout&#21644;Deep Ensembles&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MVE&#32593;&#32476;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#19968;&#20010;&#20855;&#26377;&#22343;&#20540;&#20989;&#25968;&#21644;&#26041;&#24046;&#20989;&#25968;&#30340;&#27491;&#24577;&#20998;&#24067;&#20135;&#29983;&#30340;&#12290;MVE&#32593;&#32476;&#36755;&#20986;&#22343;&#20540;&#21644;&#26041;&#24046;&#30340;&#20272;&#35745;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#25253;&#21578;&#30340;&#25910;&#25947;&#22256;&#38590;&#21487;&#20197;&#36890;&#36807;&#36981;&#24490;&#21407;&#22987;&#20316;&#32773;&#30340;&#31616;&#21333;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#24314;&#35758;&#26469;&#30456;&#23545;&#23481;&#26131;&#22320;&#36991;&#20813;&#65292;&#21363;&#20351;&#29992;&#19968;&#20010;&#39044;&#28909;&#26399;&#12290;&#22312;&#36825;&#20010;&#26399;&#38388;&#65292;&#21482;&#20248;&#21270;&#22343;&#20540;&#65292;&#26041;&#24046;&#20445;&#25345;&#22266;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#27493;&#39588;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focusses on the optimal implementation of a Mean Variance Estimation network (MVE network) (Nix and Weigend, 1994). This type of network is often used as a building block for uncertainty estimation methods in a regression setting, for instance Concrete dropout (Gal et al., 2017) and Deep Ensembles (Lakshminarayanan et al., 2017). Specifically, an MVE network assumes that the data is produced from a normal distribution with a mean function and variance function. The MVE network outputs a mean and variance estimate and optimizes the network parameters by minimizing the negative loglikelihood. In our paper, we present two significant insights. Firstly, the convergence difficulties reported in recent work can be relatively easily prevented by following the simple yet often overlooked recommendation from the original authors that a warm-up period should be used. During this period, only the mean is optimized with a fixed variance. We demonstrate the effectiveness of this step thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#21355;&#26143;&#21644;&#27979;&#31449;&#38477;&#27700;&#25968;&#25454;&#21512;&#24182;&#65292;&#37325;&#28857;&#20851;&#27880;&#26497;&#20540;&#37327;&#21270;&#12290;&#36890;&#36807;&#23558;&#35266;&#27979;&#38477;&#27700;&#20316;&#20026;&#22240;&#21464;&#37327;&#65292;&#21355;&#26143;&#25968;&#25454;&#20316;&#20026;&#39044;&#27979;&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#38477;&#27700;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.03606</link><description>&lt;p&gt;
&#20351;&#29992;LightGBM&#21512;&#24182;&#21355;&#26143;&#21644;&#27979;&#31449;&#38477;&#27700;&#25968;&#25454;&#65292;&#24182;&#24378;&#35843;&#26497;&#20540;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Merging satellite and gauge-measured precipitation using LightGBM with an emphasis on extreme quantiles. (arXiv:2302.03606v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#21355;&#26143;&#21644;&#27979;&#31449;&#38477;&#27700;&#25968;&#25454;&#21512;&#24182;&#65292;&#37325;&#28857;&#20851;&#27880;&#26497;&#20540;&#37327;&#21270;&#12290;&#36890;&#36807;&#23558;&#35266;&#27979;&#38477;&#27700;&#20316;&#20026;&#22240;&#21464;&#37327;&#65292;&#21355;&#26143;&#25968;&#25454;&#20316;&#20026;&#39044;&#27979;&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#38477;&#27700;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27700;&#25991;&#27169;&#22411;&#24212;&#29992;&#20013;&#20102;&#35299;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#38477;&#27700;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#30001;&#20110;&#32463;&#27982;&#38480;&#21046;&#65292;&#38632;&#37327;&#35266;&#27979;&#31449;&#30340;&#31354;&#38388;&#35206;&#30422;&#26377;&#38480;&#12290;&#32593;&#26684;&#21270;&#21355;&#26143;&#38477;&#27700;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#36873;&#25321;&#65292;&#21487;&#20197;&#36890;&#36807;&#35206;&#30422;&#22823;&#38754;&#31215;&#30340;&#22343;&#21248;&#22320;&#21306;&#26469;&#20272;&#35745;&#23454;&#38469;&#38477;&#27700;&#65292;&#23613;&#31649;&#30456;&#20851;&#20272;&#35745;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#25552;&#39640;&#38477;&#27700;&#20272;&#35745;&#31934;&#24230;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21512;&#24182;&#22522;&#20110;&#27979;&#31449;&#35266;&#27979;&#21644;&#32593;&#26684;&#21270;&#21355;&#26143;&#38477;&#27700;&#20135;&#21697;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35266;&#27979;&#38477;&#27700;&#20805;&#24403;&#22240;&#21464;&#37327;&#65292;&#21355;&#26143;&#25968;&#25454;&#20805;&#24403;&#39044;&#27979;&#21464;&#37327;&#12290;&#22312;&#30456;&#20851;&#24212;&#29992;&#20013;&#65292;&#38543;&#26426;&#26862;&#26519;&#26159;&#20027;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#36825;&#20123;&#31354;&#38388;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#36890;&#24120;&#21457;&#24067;&#22240;&#21464;&#37327;&#30340;&#28857;&#39044;&#27979;&#65288;&#20027;&#35201;&#26159;&#26465;&#20214;&#20998;&#24067;&#30340;&#22343;&#20540;&#25110;&#20013;&#20301;&#25968;&#65289;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#38477;&#27700;&#27010;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing the actual precipitation in space and time is critical in hydrological modelling applications, yet the spatial coverage with rain gauge stations is limited due to economic constraints. Gridded satellite precipitation datasets offer an alternative option for estimating the actual precipitation by covering uniformly large areas, albeit related estimates are not accurate. To improve precipitation estimates, machine learning is applied to merge rain gauge-based measurements and gridded satellite precipitation products. In this context, observed precipitation plays the role of the dependent variable, while satellite data play the role of predictor variables. Random forests is the dominant machine learning algorithm in relevant applications. In those spatial predictions settings, point predictions (mostly the mean or the median of the conditional distribution) of the dependent variable are issued. The aim of the manuscript is to solve the problem of probabilistic prediction of precip
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#20307;&#20844;&#24179;&#24615;(IF)&#21644;&#30697;&#38453;&#20272;&#35745;(ME)&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#20351;&#29992;ME&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#21487;&#20197;&#25913;&#21892;&#31639;&#27861;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02096</link><description>&lt;p&gt;
&#20010;&#20307;&#20844;&#24179;&#30340;&#30697;&#38453;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Matrix Estimation for Individual Fairness. (arXiv:2302.02096v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#20307;&#20844;&#24179;&#24615;(IF)&#21644;&#30697;&#38453;&#20272;&#35745;(ME)&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#20351;&#29992;ME&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#21487;&#20197;&#25913;&#21892;&#31639;&#27861;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22810;&#31181;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#12290;&#20854;&#20013;&#19968;&#31181;&#27010;&#24565;&#26159;&#20010;&#20307;&#20844;&#24179;&#24615;(IF)&#65292;&#35201;&#27714;&#30456;&#20284;&#30340;&#20010;&#20307;&#25509;&#21463;&#30456;&#20284;&#30340;&#23545;&#24453;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30697;&#38453;&#20272;&#35745;(ME)&#20316;&#20026;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#22122;&#22768;&#25968;&#25454;&#30340;&#19968;&#31181;&#33258;&#28982;&#33539;&#24335;&#20063;&#20986;&#29616;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#27010;&#24565;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;ME&#26041;&#27861;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#31639;&#27861;&#30340;IF&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#22855;&#24322;&#20540;&#38408;&#20540;(SVT)&#30340;&#27969;&#34892;ME&#26041;&#27861;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21487;&#20197;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;IF&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#31867;&#20284;&#30340;&#26465;&#20214;&#19979;&#65292;SVT&#39044;&#22788;&#29702;&#36824;&#20135;&#29983;&#20102;&#19968;&#33268;&#19988;&#36817;&#20284;&#26368;&#23567;&#21270;&#25932;&#23545;&#39118;&#38505;&#30340;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#22312;&#25152;&#36848;&#26465;&#20214;&#19979;&#65292;ME&#39044;&#22788;&#29702;&#27493;&#39588;&#19981;&#20250;&#22686;&#21152;&#22522;&#26412;&#31639;&#27861;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#21363;&#19981;&#20250;&#32473;&#20844;&#24179;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#24102;&#26469;&#26435;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, multiple notions of algorithmic fairness have arisen. One such notion is individual fairness (IF), which requires that individuals who are similar receive similar treatment. In parallel, matrix estimation (ME) has emerged as a natural paradigm for handling noisy data with missing values. In this work, we connect the two concepts. We show that pre-processing data using ME can improve an algorithm's IF without sacrificing performance. Specifically, we show that using a popular ME method known as singular value thresholding (SVT) to pre-process the data provides a strong IF guarantee under appropriate conditions. We then show that, under analogous conditions, SVT pre-processing also yields estimates that are consistent and approximately minimax optimal. As such, the ME pre-processing step does not, under the stated conditions, increase the prediction error of the base algorithm, i.e., does not impose a fairness-performance trade-off. We verify these results on synthetic a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.06267</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26377;&#21161;&#20110;&#21333;&#27169;&#24577;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#30340;&#20132;&#21449;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#20195;&#29702;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#20063;&#34987;&#31216;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#20351;&#29992;&#26469;&#33258;&#21333;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#26679;&#26412;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#19981;&#36275;&#20197;&#25551;&#36848;&#25972;&#20010;&#27010;&#24565;&#31867;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20351;&#29992;&#36328;&#27169;&#24577;&#20449;&#24687;&#39640;&#25928;&#22320;&#23398;&#20064;&#26032;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38405;&#35835;&#20851;&#20110;&#29399;&#24182;&#21548;&#23427;&#20204;&#21536;&#21483;&#30340;&#22768;&#38899;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;&#35270;&#35273;&#29399;&#20998;&#31867;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26159;&#22266;&#26377;&#30340;&#36328;&#27169;&#24577;&#30340;&#29305;&#24615;&#65292;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#26144;&#23556;&#21040;&#30456;&#21516;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#36328;&#36234;&#19981;&#21516;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#31867;&#21517;&#37325;&#26032;&#29992;&#20316;&#39069;&#22806;&#30340;&#19968;&#27425;&#24615;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26497;&#20854;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for visi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#27969;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#23450;&#20041;&#21644;&#35774;&#32622;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#37325;&#26032;&#26500;&#24605;&#30417;&#30563;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#22522;&#26412;&#23450;&#20041;&#21644;&#35774;&#32622;&#65292;&#24182;&#37325;&#26032;&#32771;&#34385;&#20102;&#19968;&#20123;&#20851;&#20110;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2212.14720</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#65306;&#27010;&#36848;&#19982;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Learning from Data Streams: An Overview and Update. (arXiv:2212.14720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14720
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#27969;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#23450;&#20041;&#21644;&#35774;&#32622;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#37325;&#26032;&#26500;&#24605;&#30417;&#30563;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#22522;&#26412;&#23450;&#20041;&#21644;&#35774;&#32622;&#65292;&#24182;&#37325;&#26032;&#32771;&#34385;&#20102;&#19968;&#20123;&#20851;&#20110;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#27969;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#38750;&#24120;&#24191;&#27867;&#19988;&#19981;&#26029;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#25968;&#25454;&#27969;&#23398;&#20064;&#20219;&#21153;&#30340;&#23450;&#20041;&#20551;&#35774;&#36890;&#24120;&#22826;&#24378;&#65292;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#28385;&#36275;&#25110;&#32773;&#29978;&#33267;&#30456;&#20114;&#30683;&#30462;&#65292;&#23588;&#20854;&#22312;&#30417;&#30563;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#12290;&#31639;&#27861;&#30340;&#36873;&#25321;&#21644;&#35774;&#35745;&#22522;&#20110;&#36890;&#24120;&#19981;&#26126;&#30830;&#35828;&#26126;&#30340;&#26631;&#20934;&#65292;&#38024;&#23545;&#19981;&#26126;&#30830;&#23450;&#20041;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#22312;&#19981;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#19988;&#19982;&#26356;&#24191;&#27867;&#30340;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#26041;&#27861;&#23396;&#31435;&#22320;&#36827;&#34892;&#12290;&#36825;&#23545;&#20110;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#26500;&#24605;&#30340;&#35768;&#22810;&#26041;&#27861;&#20135;&#29983;&#20102;&#30495;&#23454;&#19990;&#30028;&#24433;&#21709;&#30340;&#21487;&#33021;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#19988;&#23384;&#22312;&#20256;&#25773;&#35823;&#23548;&#24615;&#30740;&#31350;&#28966;&#28857;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#37325;&#26032;&#26500;&#24605;&#30417;&#30563;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#22522;&#26412;&#23450;&#20041;&#21644;&#35774;&#32622;&#65292;&#20197;&#32771;&#34385;&#30456;&#20851;&#27010;&#24565;&#28418;&#31227;&#21644;&#26102;&#38388;&#20381;&#36182;&#30340;&#29616;&#20195;&#24605;&#32771;&#26041;&#24335;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65307;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20160;&#20040;&#26500;&#25104;&#20102;&#30417;&#30563;&#25968;&#25454;&#27969;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#21450;&#37325;&#26032;&#32771;&#34385;&#19968;&#20123;&#20851;&#20110;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
The literature on machine learning in the context of data streams is vast and growing. However, many of the defining assumptions regarding data-stream learning tasks are too strong to hold in practice, or are even contradictory such that they cannot be met in the contexts of supervised learning. Algorithms are chosen and designed based on criteria which are often not clearly stated, for problem settings not clearly defined, tested in unrealistic settings, and/or in isolation from related approaches in the wider literature. This puts into question the potential for real-world impact of many approaches conceived in such contexts, and risks propagating a misguided research focus. We propose to tackle these issues by reformulating the fundamental definitions and settings of supervised data-stream learning with regard to contemporary considerations of concept drift and temporal dependence; and we take a fresh look at what constitutes a supervised data-stream learning task, and a reconsidera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#65292;&#24182;&#24212;&#29992;&#20110;&#21306;&#38388;&#35745;&#31639;&#12289;&#20248;&#21270;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2212.11429</link><description>&lt;p&gt;
&#33258;&#21160;&#35745;&#31639;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#65306;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#21644;&#26032;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications. (arXiv:2212.11429v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#65292;&#24182;&#24212;&#29992;&#20110;&#21306;&#38388;&#35745;&#31639;&#12289;&#20248;&#21270;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#30340;&#31639;&#27861;&#12290;&#22312;&#26631;&#37327;&#20989;&#25968; $f:\mathbb{R}\to\mathbb{R}$ &#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#21442;&#32771;&#28857; $x_0$&#12289;&#20449;&#20219;&#22495; $[a,b]$ &#21644;&#25972;&#25968; $k\geq1$ &#20026;&#36755;&#20837;&#65292;&#24182;&#36820;&#22238;&#19968;&#20010;&#21306;&#38388; $I$&#65292;&#20351;&#24471;&#23545;&#20110;&#25152;&#26377; $x\in[a,b]$, $f(x)\sum_{i=0}^{k-1}\frac{1}{i!}f^{(i)}(x_0)(x-x_0)^i \in I(x-x_0)^k$&#12290;&#19982;&#33258;&#21160;&#24494;&#20998;&#31867;&#20284;&#65292;&#20989;&#25968; $f$ &#30340;&#36755;&#20837;&#24517;&#39035;&#20026;&#24050;&#30693;&#30340;&#21407;&#23376;&#20989;&#25968;&#12290;&#22312;&#31639;&#27861;&#30340;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#22810;&#31181;&#24120;&#29992;&#30340;&#21021;&#31561;&#20989;&#25968;&#65288;&#22914; $\exp$&#65292;$\log$&#65289;&#23548;&#20986;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#30340;&#23574;&#38160;&#22810;&#39033;&#24335;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21306;&#38388;&#31639;&#26415;&#30340;&#27888;&#21202;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#36882;&#24402;&#22320;&#32452;&#21512;&#20803;&#20989;&#25968;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithm for automatically bounding the Taylor remainder series. In the special case of a scalar function $f: \mathbb{R} \to \mathbb{R}$, our algorithm takes as input a reference point $x_0$, trust region $[a, b]$, and integer $k \ge 1$, and returns an interval $I$ such that $f(x) \sum_{i=0}^{k-1} \frac {1} {i!} f^{(i)}(x_0) (x - x_0)^i \in I (x - x_0)^k$ for all $x \in [a, b]$. As in automatic differentiation, the function $f$ is provided to the algorithm in symbolic form, and must be composed of known atomic functions.  At a high level, our algorithm has two steps. First, for a variety of commonly-used elementary functions (e.g., $\exp$, $\log$), we derive sharp polynomial upper and lower bounds on the Taylor remainder series. We then recursively combine the bounds for the elementary functions using an interval arithmetic variant of Taylor-mode automatic differentiation. Our algorithm can make efficient use of machine learning hardware accelerators, and we provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#28909;&#21551;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22768;&#23398;&#38556;&#30861;&#29289;&#25955;&#23556;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35745;&#31639;&#25955;&#23556;&#22330;&#21644;&#32473;&#23450;&#27979;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;$L^2$&#36317;&#31163;&#30340;&#21306;&#22495;&#36793;&#30028;&#20013;&#25214;&#21040;&#33391;&#22909;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#20197;&#20811;&#26381;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2212.08736</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28909;&#21551;&#21160;&#26041;&#27861;&#29992;&#20110;&#22768;&#23398;&#38556;&#30861;&#29289;&#25955;&#23556;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle Scattering Problem. (arXiv:2212.08736v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#28909;&#21551;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22768;&#23398;&#38556;&#30861;&#29289;&#25955;&#23556;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35745;&#31639;&#25955;&#23556;&#22330;&#21644;&#32473;&#23450;&#27979;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;$L^2$&#36317;&#31163;&#30340;&#21306;&#22495;&#36793;&#30028;&#20013;&#25214;&#21040;&#33391;&#22909;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#20197;&#20811;&#26381;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20108;&#32500;&#20013;&#30340;&#22768;&#23398;&#38556;&#30861;&#29289;&#25955;&#23556;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#29289;&#20307;&#22806;&#37096;&#30340;&#19968;&#31995;&#21015;&#25509;&#25910;&#22120;&#25509;&#25910;&#21040;&#30340;&#25955;&#23556;&#22330;&#30340;&#27979;&#37327;&#26469;&#30830;&#23450;&#38556;&#30861;&#29289;&#30340;&#36793;&#30028;&#12290;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#20043;&#19968;&#26159;&#23558;&#20854;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#25214;&#21040;&#26368;&#23567;&#21270;&#35745;&#31639;&#30340;&#25955;&#23556;&#22330;&#20540;&#19982;&#32473;&#23450;&#27979;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;$L^2$&#36317;&#31163;&#30340;&#21306;&#22495;&#36793;&#30028;&#12290;&#30001;&#20110;&#23616;&#37096;&#20984;&#24615;&#38543;&#30528;&#39057;&#29575;&#22686;&#21152;&#32780;&#25910;&#32553;&#65292;&#24182;&#22312;&#30495;&#35299;&#38468;&#36817;&#20135;&#29983;&#36234;&#26469;&#36234;&#22810;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#23454;&#39564;&#35013;&#32622;&#25110;&#27979;&#37327;&#20256;&#24863;&#22120;&#30340;&#38480;&#21046;&#65292;&#20302;&#39057;&#27979;&#37327;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#33719;&#24471;&#19968;&#20010;&#33391;&#22909;&#30340;&#20248;&#21270;&#38382;&#39064;&#21021;&#22987;&#29468;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the inverse acoustic obstacle problem for sound-soft star-shaped obstacles in two dimensions wherein the boundary of the obstacle is determined from measurements of the scattered field at a collection of receivers outside the object. One of the standard approaches for solving this problem is to reformulate it as an optimization problem: finding the boundary of the domain that minimizes the $L^2$ distance between computed values of the scattered field and the given measurement data. The optimization problem is computationally challenging since the local set of convexity shrinks with increasing frequency and results in an increasing number of local minima in the vicinity of the true solution. In many practical experimental settings, low frequency measurements are unavailable due to limitations of the experimental setup or the sensors used for measurement. Thus, obtaining a good initial guess for the optimization problem plays a vital role in this environment.  We present a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22320;&#38663;&#21160;&#35889;&#30340;&#23450;&#20041;&#29305;&#24449;&#65292;&#20197;&#36741;&#21161;&#22320;&#38663;&#21160;&#36873;&#25321;&#12290;&#23427;&#32467;&#21512;&#20102;&#26426;&#22120;&#21457;&#29616;&#30340;&#28508;&#22312;&#29305;&#24449;&#21644;&#20256;&#32479;&#24378;&#24230;&#27979;&#37327;&#65292;&#36890;&#36807;&#32858;&#31867;&#20998;&#26512;&#36873;&#25321;&#20195;&#34920;&#24615;&#30340;&#22320;&#38663;&#21160;&#35760;&#24405;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.03188</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#22320;&#38663;&#21160;&#35889;&#30340;&#32858;&#31867;&#21644;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection. (arXiv:2212.03188v2 [physics.geo-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22320;&#38663;&#21160;&#35889;&#30340;&#23450;&#20041;&#29305;&#24449;&#65292;&#20197;&#36741;&#21161;&#22320;&#38663;&#21160;&#36873;&#25321;&#12290;&#23427;&#32467;&#21512;&#20102;&#26426;&#22120;&#21457;&#29616;&#30340;&#28508;&#22312;&#29305;&#24449;&#21644;&#20256;&#32479;&#24378;&#24230;&#27979;&#37327;&#65292;&#36890;&#36807;&#32858;&#31867;&#20998;&#26512;&#36873;&#25321;&#20195;&#34920;&#24615;&#30340;&#22320;&#38663;&#21160;&#35760;&#24405;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#24212;&#29992;&#31185;&#23398;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24207;&#21015;&#25968;&#25454;&#30340;&#32858;&#31867;&#20998;&#26512;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#20173;&#28982;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22320;&#38663;&#21160;&#35889;&#30340;&#23450;&#20041;&#29305;&#24449;&#65292;&#20063;&#31216;&#20026;&#28508;&#22312;&#29305;&#24449;&#65292;&#20197;&#36741;&#21161;&#22320;&#38663;&#21160;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#28508;&#22312;&#29305;&#24449;&#26159;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#20302;&#32500;&#24230;&#26426;&#22120;&#21457;&#29616;&#30340;&#35889;&#29305;&#24449;&#12290;&#26426;&#22120;&#21457;&#29616;&#30340;&#28508;&#22312;&#29305;&#24449;&#21487;&#20197;&#19982;&#20256;&#32479;&#23450;&#20041;&#30340;&#24378;&#24230;&#27979;&#37327;&#30456;&#32467;&#21512;&#65292;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#65292;&#20174;&#22823;&#37327;&#30340;&#22320;&#38663;&#21160;&#26679;&#26412;&#20013;&#36873;&#25321;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#23376;&#38598;&#12290;&#39640;&#25928;&#30340;&#22320;&#38663;&#21160;&#36873;&#25321;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#20195;&#34920;&#24615;&#30340;&#35760;&#24405;&#65292;&#20197;&#27010;&#29575;&#24615;&#22320;&#21453;&#26144;&#32467;&#26500;&#22312;&#20854;&#23551;&#21629;&#21608;&#26399;&#20869;&#23558;&#32463;&#21382;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19977;&#20010;&#31034;&#20363;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#21512;&#25104;&#21644;&#29616;&#22330;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering analysis of sequence data continues to address many applications in engineering design, aided with the rapid growth of machine learning in applied science. This paper presents an unsupervised machine learning algorithm to extract defining characteristics of earthquake ground-motion spectra, also called latent features, to aid in ground-motion selection (GMS). In this context, a latent feature is a low-dimensional machine-discovered spectral characteristic learned through nonlinear relationships of a neural network autoencoder. Machine discovered latent features can be combined with traditionally defined intensity measures and clustering can be performed to select a representative subgroup from a large ground-motion suite. The objective of efficient GMS is to choose characteristic records representative of what the structure will probabilistically experience in its lifetime. Three examples are presented to validate this approach, including the use of synthetic and field recor
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;Nothigattu&#12289;Shah&#21644;Procaccia&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#32858;&#21512;&#35780;&#23457;&#20154;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#36127;&#38754;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.02144</link><description>&lt;p&gt;
&#26080;&#25439;&#22833;&#21363;&#26080;&#21327;&#35758;&#65306;&#23398;&#20064;&#19982;&#21516;&#34892;&#35780;&#23457;&#30340;&#31038;&#20250;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
No Agreement Without Loss: Learning and Social Choice in Peer Review. (arXiv:2211.02144v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02144
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;Nothigattu&#12289;Shah&#21644;Procaccia&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#32858;&#21512;&#35780;&#23457;&#20154;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#36127;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#20013;&#65292;&#35780;&#23457;&#20154;&#32463;&#24120;&#34987;&#35201;&#27714;&#35780;&#20272;&#25552;&#20132;&#31295;&#20214;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#25216;&#26415;&#36136;&#37327;&#25110;&#26032;&#39062;&#24615;&#12290;&#32473;&#23450;&#27599;&#20010;&#39044;&#23450;&#20041;&#29305;&#24449;&#30340;&#35780;&#20998;&#65292;&#35780;&#23457;&#20154;&#24517;&#39035;&#25552;&#20379;&#19968;&#20010;&#25972;&#20307;&#30340;&#23450;&#37327;&#24314;&#35758;&#12290;&#21487;&#20197;&#20551;&#35774;&#27599;&#20010;&#35780;&#23457;&#20154;&#23545;&#29305;&#24449;&#38598;&#21512;&#21040;&#24314;&#35758;&#30340;&#26144;&#23556;&#37117;&#26377;&#33258;&#24049;&#30340;&#30475;&#27861;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#35780;&#23457;&#20154;&#24515;&#20013;&#26377;&#30528;&#19981;&#21516;&#30340;&#26144;&#23556;&#12290;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#27604;&#20363;&#20559;&#24046;&#30340;&#20219;&#36873;&#24615;&#22240;&#32032;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#30001;Noothigattu&#12289;Shah&#21644;Procaccia&#24341;&#20837;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#22312;AAAI2022&#20250;&#35758;&#30340;&#32452;&#32455;&#32773;&#20013;&#24212;&#29992;&#12290;Noothigattu&#12289;Shah&#21644;Procaccia&#25552;&#20986;&#36890;&#36807;&#26368;&#23567;&#21270;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#26469;&#32858;&#21512;&#35780;&#23457;&#20154;&#30340;&#26144;&#23556;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#30340;&#20844;&#29702;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#20182;&#20204;&#30340;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#32467;&#26524;&#21644;&#20551;&#35774;&#36827;&#34892;&#20102;&#25361;&#25112;&#65292;&#24182;&#25253;&#21578;&#20102;&#19968;&#20123;&#36127;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In peer review systems, reviewers are often asked to evaluate various features of submissions, such as technical quality or novelty. A score is given to each of the predefined features and based on these the reviewer has to provide an overall quantitative recommendation. It may be assumed that each reviewer has her own mapping from the set of features to a recommendation, and that different reviewers have different mappings in mind. This introduces an element of arbitrariness known as commensuration bias. In this paper we discuss a framework, introduced by Noothigattu, Shah and Procaccia, and then applied by the organizers of the AAAI 2022 conference. Noothigattu, Shah and Procaccia proposed to aggregate reviewer's mapping by minimizing certain loss functions, and studied axiomatic properties of this approach, in the sense of social choice theory. We challenge several of the results and assumptions used in their work and report a number of negative results. On the one hand, we study a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#21453;&#39304;&#30340;Hebbian&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#21453;&#39304;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#24615;&#12289;&#29983;&#29289;&#20860;&#23481;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2209.11883</link><description>&lt;p&gt;
&#26080;&#21453;&#39304;&#30340;Hebbian&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hebbian Deep Learning Without Feedback. (arXiv:2209.11883v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#21453;&#39304;&#30340;Hebbian&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#21453;&#39304;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#24615;&#12289;&#29983;&#29289;&#20860;&#23481;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#36817;&#20284;&#26041;&#27861;&#22312;&#20943;&#23569;&#35745;&#31639;&#25928;&#29575;&#21644;&#19982;&#29983;&#29289;&#23398;&#30340;&#19981;&#20860;&#23481;&#24615;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#20173;&#23384;&#22312;&#37325;&#35201;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36817;&#20284;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#34920;&#26126;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#27861;&#21487;&#33021;&#26356;&#26377;&#25104;&#25928;&#12290;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#36719;&#36194;&#32773;&#20840;&#25299;&#25169;&#32593;&#32476;&#20013;Hebbian&#23398;&#20064;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23618;SoftHebb&#31639;&#27861;&#65292;&#21363;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#26080;&#38656;&#20219;&#20309;&#21453;&#39304;&#12289;&#30446;&#26631;&#25110;&#38169;&#35823;&#20449;&#21495;&#12290;&#22240;&#27492;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#36890;&#36807;&#36991;&#20813;&#26435;&#37325;&#20256;&#36755;&#12289;&#38750;&#23616;&#37096;&#21487;&#22609;&#24615;&#12289;&#23618;&#26356;&#26032;&#30340;&#26102;&#38388;&#38145;&#23450;&#12289;&#36845;&#20195;&#24179;&#34913;&#20197;&#21450;&#65288;&#33258;&#36523;&#65289;&#30417;&#30563;&#25110;&#20854;&#20182;&#21453;&#39304;&#20449;&#21495;&#26469;&#23454;&#29616;&#25928;&#29575;&#25552;&#21319;&#21644;&#29983;&#29289;&#20860;&#23481;&#24615;&#65292;&#32780;&#19981;&#26159;&#20197;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#12290;&#28155;&#21152;&#32447;&#24615;&#20998;&#31867;&#22120;&#21518;&#65292;&#23427;&#22312;&#26368;&#22810;&#20116;&#20010;&#38544;&#34255;&#23618;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#21487;&#29983;&#29289;&#23398;&#20064;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals -- which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#34701;&#21512;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#30340;&#32570;&#22833;&#20540;&#22635;&#20805;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#36712;&#36857;&#31070;&#32463;&#20803;&#21644;&#24452;&#21521;&#22522;&#31070;&#32463;&#20803;&#30340;&#38544;&#34255;&#23618;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#23398;&#20064;&#30456;&#20851;&#29305;&#24449;&#21644;&#20849;&#21516;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.13495</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#34701;&#21512;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#30340;&#32570;&#22833;&#20540;&#22635;&#20805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Missing Value Filling Model Based on Feature Fusion Enhanced Autoencoder. (arXiv:2208.13495v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#34701;&#21512;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#30340;&#32570;&#22833;&#20540;&#22635;&#20805;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#36712;&#36857;&#31070;&#32463;&#20803;&#21644;&#24452;&#21521;&#22522;&#31070;&#32463;&#20803;&#30340;&#38544;&#34255;&#23618;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#23398;&#20064;&#30456;&#20851;&#29305;&#24449;&#21644;&#20849;&#21516;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20851;&#38190;&#12290;&#22312;&#20247;&#22810;&#22240;&#32032;&#20013;&#65292;&#32570;&#22833;&#20540;&#25968;&#25454;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#22240;&#27492;&#24320;&#21457;&#26377;&#25928;&#30340;&#22635;&#20805;&#27169;&#22411;&#25104;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#19968;&#20010;&#20851;&#38190;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#19968;&#39033;&#20027;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#33258;&#32452;&#32455;&#26144;&#23556;&#25110;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#22635;&#20805;&#32570;&#22833;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20256;&#32479;&#26041;&#27861;&#24456;&#38590;&#21516;&#26102;&#21457;&#29616;&#25968;&#25454;&#23646;&#24615;&#38388;&#30340;&#30456;&#20851;&#29305;&#24449;&#21644;&#20849;&#21516;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#32463;&#20856;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#35828;&#65292;&#20182;&#20204;&#24120;&#24120;&#23398;&#20064;&#21040;&#26080;&#25928;&#30340;&#24120;&#37327;&#26144;&#23556;&#65292;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#22635;&#20805;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#34701;&#21512;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#30340;&#32570;&#22833;&#20540;&#22635;&#20805;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#33258;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#21435;&#36712;&#36857;&#31070;&#32463;&#20803;&#21644;&#24452;&#21521;&#22522;&#31070;&#32463;&#20803;&#30340;&#38544;&#34255;&#23618;&#65292;&#36825;&#21487;&#20197;&#22686;&#24378;&#23398;&#20064;&#30456;&#20851;&#29305;&#24449;&#21644;&#20849;&#21516;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of the big data era, the data quality problem is becoming more critical. Among many factors, data with missing values is one primary issue, and thus developing effective imputation models is a key topic in the research community. Recently, a major research direction is to employ neural network models such as self-organizing mappings or automatic encoders for filling missing values. However, these classical methods can hardly discover interrelated features and common features simultaneously among data attributes. Especially, it is a very typical problem for classical autoencoders that they often learn invalid constant mappings, which dramatically hurts the filling performance. To solve the above-mentioned problems, we propose a missing-value-filling model based on a feature-fusion-enhanced autoencoder. We first incorporate into an autoencoder a hidden layer that consists of de-tracking neurons and radial basis function neurons, which can enhance the ability of learning i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniCon&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23458;&#25143;VQA&#20219;&#21153;&#30340;&#20445;&#23494;&#24615;&#32422;&#26463;&#21644;&#23458;&#25143;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#22411;&#20849;&#20139;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#65292;&#37319;&#29992;&#20998;&#35010;&#23398;&#20064;&#26550;&#26500;&#30830;&#20445;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2208.11435</link><description>&lt;p&gt;
UniCon: &#24102;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#21333;&#21521;&#20998;&#27495;&#23398;&#20064;&#29992;&#20110;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering. (arXiv:2208.11435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniCon&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23458;&#25143;VQA&#20219;&#21153;&#30340;&#20445;&#23494;&#24615;&#32422;&#26463;&#21644;&#23458;&#25143;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#22411;&#20849;&#20139;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#65292;&#37319;&#29992;&#20998;&#35010;&#23398;&#20064;&#26550;&#26500;&#30830;&#20445;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26377;&#21161;&#20110;&#29616;&#23454;&#24212;&#29992;&#65292;&#22914;&#23478;&#24237;&#26426;&#22120;&#20154;&#21644;&#21307;&#23398;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#20026;&#21508;&#31181;&#23458;&#25143;&#20219;&#21153;&#35774;&#35745;&#24378;&#22823;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#30830;&#20445;&#38544;&#31169;&#65292;&#22240;&#20026;&#30001;&#20110;&#20445;&#23494;&#38382;&#39064;&#65292;&#23458;&#25143;&#25968;&#25454;&#20849;&#20139;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#22810;&#23458;&#25143;VQA&#20219;&#21153;&#30340;&#20445;&#23494;&#24615;&#32422;&#26463;&#21644;&#23458;&#25143;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#21333;&#21521;&#20998;&#27495;&#23398;&#20064;&#65288;UniCon&#65289;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#23458;&#25143;&#30340;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#19978;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#22411;&#20849;&#20139;&#23398;&#20064;&#31934;&#32454;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#35777;&#65292;&#21033;&#29992;&#20998;&#35010;&#23398;&#20064;&#26550;&#26500;&#30830;&#20445;&#38544;&#31169;&#65292;&#20854;&#20013;&#23436;&#25972;&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#32452;&#20214;&#36827;&#34892;&#29420;&#31435;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#19982;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#24230;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) using multi-modal data facilitates real-life applications, such as home robots and medical diagnoses. However, one significant challenge is to design a robust learning method for various client tasks. One critical aspect is to ensure privacy, as client data sharing is limited due to confidentiality concerns. This work focuses on addressing the issue of confidentiality constraints in multi-client VQA tasks and limited labeled training data of clients. We propose the Unidirectional Split Learning with Contrastive Loss (UniCon) method to overcome these limitations. The proposed method trains a global model on the entire data distribution of different clients, learning refined cross-modal representations through model sharing. Privacy is ensured by utilizing a split learning architecture in which a complete model is partitioned into two components for independent training. Moreover, recent self-supervised learning techniques were found to be highly compatibl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#36890;&#36807;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#21644;&#22788;&#29702;&#25928;&#24212;&#65292;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.07898</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#21327;&#21516;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Collaborative causal inference on distributed data. (arXiv:2208.07898v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#36890;&#36807;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#21644;&#22788;&#29702;&#25928;&#24212;&#65292;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#39318;&#20808;&#65292;&#26412;&#22320;&#21508;&#26041;&#20174;&#31169;&#26377;&#25968;&#25454;&#20013;&#26500;&#24314;&#38477;&#32500;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#20182;&#20204;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#21518;&#65292;&#20174;&#20849;&#20139;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#12290;&#26368;&#21518;&#65292;&#20174;&#20542;&#21521;&#20998;&#25968;&#20013;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#20943;&#23569;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#38543;&#26426;&#35823;&#24046;&#12290;&#36890;&#36807;&#22312;&#20154;&#24037;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#27604;&#21333;&#29420;&#20998;&#26512;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of technologies for causal inference with the privacy preservation of distributed data has attracted considerable attention in recent years. To address this issue, we propose a data collaboration quasi-experiment (DC-QE) that enables causal inference from distributed data with privacy preservation. In our method, first, local parties construct dimensionality-reduced intermediate representations from the private data. Second, they share intermediate representations, instead of private data for privacy preservation. Third, propensity scores were estimated from the shared intermediate representations. Finally, the treatment effects were estimated from propensity scores. Our method can reduce both random errors and biases, whereas existing methods can only reduce random errors in the estimation of treatment effects. Through numerical experiments on both artificial and real-world data, we confirmed that our method can lead to better estimation results than individual analyse
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProMix&#30340;&#26032;&#39062;LNL&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24178;&#20928;&#26679;&#26412;&#30340;&#25928;&#29992;&#26469;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;&#12290;&#37319;&#29992;&#19968;&#31181;&#21305;&#37197;&#39640;&#32622;&#20449;&#24230;&#36873;&#25321;&#25216;&#26415;&#26469;&#21160;&#24577;&#25193;&#23637;&#22522;&#30784;&#24178;&#20928;&#26679;&#26412;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#24179;&#34913;&#21644;&#26080;&#20559;&#30340;SSL&#26694;&#26550;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.10276</link><description>&lt;p&gt;
ProMix&#65306;&#36890;&#36807;&#26368;&#22823;&#21270;&#24178;&#20928;&#26679;&#26412;&#25928;&#29992;&#26469;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10276
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProMix&#30340;&#26032;&#39062;LNL&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24178;&#20928;&#26679;&#26412;&#30340;&#25928;&#29992;&#26469;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;&#12290;&#37319;&#29992;&#19968;&#31181;&#21305;&#37197;&#39640;&#32622;&#20449;&#24230;&#36873;&#25321;&#25216;&#26415;&#26469;&#21160;&#24577;&#25193;&#23637;&#22522;&#30784;&#24178;&#20928;&#26679;&#26412;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#24179;&#34913;&#21644;&#26080;&#20559;&#30340;SSL&#26694;&#26550;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#65288;LNL&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#35805;&#39064;&#65292;&#22240;&#20026;&#24102;&#26377;&#19981;&#23436;&#25972;&#26631;&#27880;&#30340;&#25968;&#25454;&#30456;&#23545;&#36739;&#20415;&#23452;&#26131;&#24471;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#37319;&#29992;&#29305;&#23450;&#30340;&#36873;&#25321;&#26426;&#21046;&#26469;&#21306;&#20998;&#24178;&#20928;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#65292;&#28982;&#21518;&#24212;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25216;&#26415;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#27493;&#39588;&#20027;&#35201;&#25552;&#20379;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#21644;&#36275;&#22815;&#22909;&#30340;&#28165;&#27905;&#23376;&#38598;&#65292;&#24573;&#35270;&#20102;&#20016;&#23500;&#30340;&#24178;&#20928;&#26679;&#26412;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LNL&#26694;&#26550;ProMix&#65292;&#35797;&#22270;&#36890;&#36807;&#26368;&#22823;&#21270;&#24178;&#20928;&#26679;&#26412;&#30340;&#25928;&#29992;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#37197;&#39640;&#32622;&#20449;&#24230;&#36873;&#25321;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36873;&#25321;&#37027;&#20123;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#24471;&#20998;&#21644;&#19982;&#32473;&#23450;&#26631;&#31614;&#21305;&#37197;&#39044;&#27979;&#30340;&#31034;&#20363;&#65292;&#20197;&#21160;&#24577;&#25193;&#23637;&#22522;&#30784;&#24178;&#20928;&#26679;&#26412;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#36807;&#24230;&#36873;&#25321;&#28165;&#27905;&#26679;&#26412;&#38598;&#31243;&#24207;&#30340;&#28508;&#22312;&#21103;&#20316;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SSL&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#26102;&#24471;&#21040;&#24179;&#34913;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels (LNL) has become an appealing topic, as imperfectly annotated data are relatively cheaper to obtain. Recent state-of-the-art approaches employ specific selection mechanisms to separate clean and noisy samples and then apply Semi-Supervised Learning (SSL) techniques for improved performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. To fulfill this, we propose a novel LNL framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high confidence selection technique that selects those examples with high confidence scores and matched predictions with given labels to dynamically expand a base clean sample set. To overcome the potential side effect of excessive clean set selection procedure, we further devise a novel SSL framework that is able to train balanced and unbiased classifiers on the se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22312;&#32447;&#38544;&#31169;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;DPSDA&#65292;&#36890;&#36807;&#24046;&#20998;&#31169;&#26377;&#26041;&#24335;&#21644;&#23545;&#20598;&#24179;&#22343;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#38544;&#31169;&#21644;&#26102;&#38388;&#21464;&#21270;&#29305;&#24615;&#30340;&#32422;&#26463;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;DPSDA-C&#21644;DPSDA-PS&#65292;&#20854;&#20998;&#21035;&#37319;&#29992;&#20102;&#24490;&#29615;&#24335;&#36890;&#20449;&#21644;&#20855;&#26377;&#22122;&#22768;&#24178;&#25200;&#26799;&#24230;&#30340;&#23545;&#20598;&#26356;&#26032;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.07944</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22312;&#32447;&#38544;&#31169;&#23398;&#20064;&#30340;&#20984;&#38750;&#21487;&#20998;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Distributed Online Private Learning of Convex Nondecomposable Objectives. (arXiv:2206.07944v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22312;&#32447;&#38544;&#31169;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;DPSDA&#65292;&#36890;&#36807;&#24046;&#20998;&#31169;&#26377;&#26041;&#24335;&#21644;&#23545;&#20598;&#24179;&#22343;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#38544;&#31169;&#21644;&#26102;&#38388;&#21464;&#21270;&#29305;&#24615;&#30340;&#32422;&#26463;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;DPSDA-C&#21644;DPSDA-PS&#65292;&#20854;&#20998;&#21035;&#37319;&#29992;&#20102;&#24490;&#29615;&#24335;&#36890;&#20449;&#21644;&#20855;&#26377;&#22122;&#22768;&#24178;&#25200;&#26799;&#24230;&#30340;&#23545;&#20598;&#26356;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22788;&#29702;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#32593;&#32476;&#38544;&#31169;&#65292;&#32771;&#34385;&#20102;&#19968;&#31867;&#38750;&#21487;&#20998;&#30446;&#26631;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#27599;&#20010;&#33410;&#28857;&#21482;&#25511;&#21046;&#20840;&#23616;&#20915;&#31574;&#30340;&#19968;&#37096;&#20998;&#65292;&#25152;&#26377;&#33410;&#28857;&#30340;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#33539;&#22260;T&#20869;&#21327;&#21516;&#26368;&#23567;&#21270;&#20840;&#23616;&#25104;&#26412;&#65292;&#24182;&#20445;&#35777;&#20256;&#36755;&#20449;&#24687;&#30340;&#23433;&#20840;&#24615;&#12290;&#38024;&#23545;&#36825;&#31181;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#31639;&#27861;&#26694;&#26550;&#65292;&#31216;&#20026;DPSDA&#65292;&#20351;&#29992;Laplace&#26426;&#21046;&#21644;&#38543;&#26426;&#21464;&#37327;&#30340;&#23545;&#20598;&#24179;&#22343;&#27861;&#36827;&#34892;&#24046;&#20998;&#31169;&#26377;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#12290;&#27880;&#24847;&#65292;&#22312;&#23545;&#20598;&#26356;&#26032;&#20013;&#65292;DPSDA&#30340;&#25152;&#26377;&#33410;&#28857;&#37117;&#20351;&#29992;&#21463;&#22122;&#22768;&#24178;&#25200;&#30340;&#26799;&#24230;&#65292;&#20197;&#25552;&#39640;&#36890;&#29992;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#19979;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#31216;&#20026;DPSDA-C&#21644;DPSDA-PS&#12290;&#22312;DPSDA-C&#20013;&#65292;&#33410;&#28857;&#22312;&#21407;&#22987;&#26356;&#26032;&#20013;&#23454;&#29616;&#22522;&#20110;&#24490;&#29615;&#30340;&#36890;&#20449;&#65292;&#20197;&#20943;&#36731;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26080;&#21521;&#32593;&#32476;&#19978;&#30340;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We deal with a general distributed constrained online learning problem with privacy over time-varying networks, where a class of nondecomposable objectives are considered. Under this setting, each node only controls a part of the global decision, and the goal of all nodes is to collaboratively minimize the global cost over a time horizon $T$ while guarantees the security of the transmitted information. For such problems, we first design a novel generic algorithm framework, named as DPSDA, of differentially private distributed online learning using the Laplace mechanism and the stochastic variants of dual averaging method. Note that in the dual updates, all nodes of DPSDA employ the noise-corrupted gradients for more generality. Then, we propose two algorithms, named as DPSDA-C and DPSDA-PS, under this framework. In DPSDA-C, the nodes implement a circulation-based communication in the primal updates so as to alleviate the disagreements over time-varying undirected networks. In addition,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#38024;&#23545;&#25512;&#33616;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24212;&#29992;&#26469;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13619</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#22522;&#30784;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#38024;&#23545;&#25512;&#33616;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24212;&#29992;&#26469;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26368;&#26222;&#36941;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#25512;&#33616;&#31995;&#32479;&#22312;&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#21644;&#24179;&#21488;&#30340;&#21033;&#30410;&#19982;&#29983;&#25104;&#30340;&#25512;&#33616;&#32467;&#26524;&#30340;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#39640;&#24230;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#21463;&#21040;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#21066;&#24369;&#31995;&#32479;&#30340;&#21487;&#20449;&#36182;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#33616;&#35774;&#32622;&#20013;&#35299;&#20915;&#28508;&#22312;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#32771;&#34385;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#28041;&#21450;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30456;&#23545;&#38646;&#25955;&#19988;&#32570;&#20047;&#31995;&#32479;&#21270;&#25972;&#29702;&#65292;&#22240;&#27492;&#23545;&#20110;&#26032;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#38590;&#20197;&#28145;&#20837;&#39046;&#22495;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23545;&#25512;&#33616;&#20013;&#29616;&#26377;&#20844;&#24179;&#24615;&#20316;&#21697;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Bootstrapped Deep Ensembles&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32463;&#20856;&#30340;&#26377;&#38480;&#25968;&#25454;&#25928;&#24212;&#65292;&#26126;&#30830;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2202.10903</link><description>&lt;p&gt;
&#33258;&#20449;&#30340;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#19982;&#24341;&#23548;&#28145;&#24230;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Confident Neural Network Regression with Bootstrapped Deep Ensembles. (arXiv:2202.10903v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Bootstrapped Deep Ensembles&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32463;&#20856;&#30340;&#26377;&#38480;&#25968;&#25454;&#25928;&#24212;&#65292;&#26126;&#30830;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#21644;&#20351;&#29992;&#22686;&#21152;&#65292;&#21487;&#20449;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#26368;&#31361;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#26159;Deep Ensembles&#65288;Lakshminarayanan&#31561;&#20154;&#65292;2017&#65289;&#12290;&#19968;&#20010;&#32463;&#20856;&#30340;&#21442;&#25968;&#27169;&#22411;&#30001;&#20110;&#24314;&#27169;&#25968;&#25454;&#26159;&#38543;&#26426;&#26679;&#26412;&#65292;&#22240;&#27492;&#20854;&#21442;&#25968;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#32593;&#32476;&#20248;&#21270;&#30340;&#38543;&#26426;&#24615;&#20063;&#20855;&#26377;&#39069;&#22806;&#30340;&#19981;&#30830;&#23450;&#24615;&#25104;&#20998;&#12290;Lakshminarayanan&#31561;&#20154;&#65288;2017&#65289;&#25351;&#20986;&#65292;Deep Ensembles&#26410;&#32771;&#34385;&#21040;&#30001;&#26377;&#38480;&#25968;&#25454;&#25928;&#24212;&#24341;&#36215;&#30340;&#32463;&#20856;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#35774;&#32622;&#30340;&#35745;&#31639;&#24265;&#20215;&#24615;&#25193;&#23637;Deep Ensembles&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Bootstrapped Deep Ensembles&#65292;&#23427;&#20351;&#29992;&#25913;&#36827;&#29256;&#30340;&#21442;&#25968;&#33258;&#21161;&#27861;&#26126;&#30830;&#32771;&#34385;&#20102;&#26377;&#38480;&#25968;&#25454;&#30340;&#32463;&#20856;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of the popularity and usage of neural networks, trustworthy uncertainty estimation is becoming increasingly essential. One of the most prominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan et al., 2017) . A classical parametric model has uncertainty in the parameters due to the fact that the data on which the model is build is a random sample. A modern neural network has an additional uncertainty component since the optimization of the network is random. Lakshminarayanan et al. (2017) noted that Deep Ensembles do not incorporate the classical uncertainty induced by the effect of finite data. In this paper, we present a computationally cheap extension of Deep Ensembles for the regression setting, called Bootstrapped Deep Ensembles, that explicitly takes this classical effect of finite data into account using a modified version of the parametric bootstrap. We demonstrate through an experimental study that our method significantly improves upon standar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;&#20013;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#36741;&#21161;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#24577;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21033;&#29992;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.07901</link><description>&lt;p&gt;
&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;&#20013;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#36741;&#21161;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition. (arXiv:2202.07901v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;&#20013;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#36741;&#21161;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#24577;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21033;&#29992;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#20004;&#31181;&#25110;&#22810;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#23884;&#20837;&#65292;&#27604;&#20165;&#20351;&#29992;&#20854;&#20013;&#19968;&#31181;&#27169;&#24577;&#33021;&#22815;&#25552;&#39640;&#32473;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#22914;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65288;&#20363;&#22914;&#38899;&#39057;&#25110;&#25991;&#26412;&#25968;&#25454;&#65289;&#65292;&#38656;&#35201;&#26368;&#23567;&#21270;&#27169;&#24577;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#25110;&#19977;&#20803;&#32452;&#25439;&#22833;&#65292;&#21033;&#29992;&#27491;&#36127;&#26631;&#31614;&#21019;&#24314;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65292;&#36827;&#34892;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#24577;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65288;CMR-IS&#65289;&#12290;&#36890;&#36807;&#35843;&#25972;&#19977;&#20803;&#32452;&#25439;&#22833;&#36827;&#34892;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21033;&#29992;&#36741;&#21161;&#65288;&#22270;&#20687;&#20998;&#31867;&#65289;&#20219;&#21153;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#20027;&#35201;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21333;&#26631;&#31614;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#30340;&#21160;&#24577;&#36793;&#30028;&#19977;&#20803;&#32452;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal representation learning learns a shared embedding between two or more modalities to improve performance in a given task compared to using only one of the modalities. Cross-modal representation learning from different data types -- such as images and time-series data (e.g., audio or text data) -requires a deep metric learning loss that minimizes the distance between the modality embeddings. In this paper, we propose to use the contrastive or triplet loss, which uses positive and negative identities to create sample pairs with different labels, for cross-modal representation learning between image and time-series modalities (CMR-IS). By adapting the triplet loss for cross-modal representation learning, higher accuracy in the main (time-series classification) task can be achieved by exploiting additional information of the auxiliary (image classification) task. We present a triplet loss with a dynamic margin for single label and sequence-to-sequence classification tasks. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#32463;&#39564;&#22238;&#25918;&#65288;RER&#65289;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#26679;&#26412;&#37325;&#35201;&#24615;&#65292;&#20197;&#24179;&#34913;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#35201;&#27714;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#24182;&#31215;&#32047;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2112.15402</link><description>&lt;p&gt;
&#20851;&#31995;&#32463;&#39564;&#22238;&#25918;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.15402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#32463;&#39564;&#22238;&#25918;&#65288;RER&#65289;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#26679;&#26412;&#37325;&#35201;&#24615;&#65292;&#20197;&#24179;&#34913;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#35201;&#27714;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#24182;&#31215;&#32047;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#21487;&#20197;&#22312;&#27969;&#24335;&#35757;&#32451;&#25968;&#25454;&#19978;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#36890;&#36807;&#20445;&#30041;&#26469;&#33258;&#26087;&#20219;&#21153;&#30340;&#19968;&#23567;&#37096;&#20998;&#25968;&#25454;&#20316;&#20026;&#20869;&#23384;&#32531;&#20914;&#21306;&#65292;&#22312;&#20943;&#36731;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#27599;&#20010;&#26032;&#20219;&#21153;&#35270;&#20026;&#24179;&#31561;&#30340;&#65292;&#36825;&#21487;&#33021;&#19981;&#20805;&#20998;&#32771;&#34385;&#26087;&#20219;&#21153;&#21644;&#26032;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#25110;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#25345;&#32493;&#35757;&#32451;&#36807;&#31243;&#20013;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#32463;&#39564;&#22238;&#25918;&#65288;RER&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21452;&#23618;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#27599;&#20010;&#20219;&#21153;&#20869;&#30340;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#26679;&#26412;&#37325;&#35201;&#24615;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#8220;&#31283;&#23450;&#24615;&#8221;&#21644;&#8220;&#21487;&#22609;&#24615;&#8221;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#31215;&#32047;&#26032;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is a promising machine learning paradigm to learn new tasks while retaining previously learned knowledge over streaming training data. Till now, rehearsal-based methods, keeping a small part of data from old tasks as a memory buffer, have shown good performance in mitigating catastrophic forgetting for previously learned knowledge. However, most of these methods typically treat each new task equally, which may not adequately consider the relationship or similarity between old and new tasks. Furthermore, these methods commonly neglect sample importance in the continual training process and result in sub-optimal performance on certain tasks. To address this challenging problem, we propose Relational Experience Replay (RER), a bi-level learning framework, to adaptively tune task-wise relationships and sample importance within each task to achieve a better `stability' and `plasticity' trade-off. As such, the proposed method is capable of accumulating new knowledge while 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#23545;Meta-Learning&#65292;Multi-Task Learning&#21644;Transfer Learning&#30340;&#20803;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#23398;&#20064;&#33539;&#20363;&#30340;&#29305;&#28857;&#21450;&#20854;&#27604;&#36739;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.12146</link><description>&lt;p&gt;
&#20998;&#20139;&#23398;&#20064;&#19982;&#23398;&#20064;&#20998;&#20139; - Meta-Learning&#65292;Multi-Task Learning&#21644;Transfer Learning&#30340;&#26377;&#26426;&#32467;&#21512;: &#19968;&#31687;&#20803;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Sharing to learn and learning to share -- Fitting together Meta-Learning, Multi-Task Learning, and Transfer Learning: A meta review. (arXiv:2111.12146v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.12146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#23545;Meta-Learning&#65292;Multi-Task Learning&#21644;Transfer Learning&#30340;&#20803;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#23398;&#20064;&#33539;&#20363;&#30340;&#29305;&#28857;&#21450;&#20854;&#27604;&#36739;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#19981;&#21516;&#39046;&#22495;&#25972;&#21512;&#30693;&#35782;&#26159;&#20154;&#31867;&#23398;&#20064;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#36716;&#31227;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#31561;&#23398;&#20064;&#33539;&#20363;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30693;&#35782;&#26469;&#20026;&#26032;&#20219;&#21153;&#25552;&#20379;&#26356;&#24555;&#30340;&#23398;&#20064;&#21644;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21453;&#26144;&#20102;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20123;&#23398;&#20064;&#33539;&#20363;&#21450;&#20854;&#27604;&#36739;&#20998;&#26512;&#12290;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#30340;&#24369;&#28857;&#24448;&#24448;&#26159;&#21478;&#19968;&#20010;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#22240;&#27492;&#23558;&#23427;&#20204;&#21512;&#24182;&#26159;&#25991;&#29486;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29305;&#24449;&#12290;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#35770;&#25991;&#29420;&#31435;&#20851;&#27880;&#20110;&#36825;&#20123;&#23398;&#20064;&#33539;&#20363;&#65292;&#24182;&#23545;&#23427;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#23558;&#65288;&#20004;&#20010;&#65289;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#12290;&#26412;&#35843;&#30740;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#26469;&#35299;&#20915;&#21508;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#39640;&#20809;&#35889;&#25104;&#20687;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating knowledge across different domains is an essential feature of human learning. Learning paradigms such as transfer learning, meta learning, and multi-task learning reflect the human learning process by exploiting the prior knowledge for new tasks, encouraging faster learning and good generalization for new tasks. This article gives a detailed view of these learning paradigms and their comparative analysis. The weakness of one learning algorithm turns out to be a strength of another, and thus merging them is a prevalent trait in the literature. There are numerous research papers that focus on each of these learning paradigms separately and provide a comprehensive overview of them. However, this article provides a review of research studies that combine (two of) these learning algorithms. This survey describes how these techniques are combined to solve problems in many different fields of study, including computer vision, natural language processing, hyperspectral imaging, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35760;&#24518;&#25511;&#21046;&#21644;&#21518;&#32487;&#29305;&#24449;&#20004;&#20010;&#26694;&#26550;&#30340;&#25972;&#21512;&#65292;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#37325;&#29992;&#30340;&#20248;&#38597;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.03110</link><description>&lt;p&gt;
&#21518;&#32487;&#29305;&#24449;&#31070;&#32463;&#35760;&#24518;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Successor Feature Neural Episodic Control. (arXiv:2111.03110v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35760;&#24518;&#25511;&#21046;&#21644;&#21518;&#32487;&#29305;&#24449;&#20004;&#20010;&#26694;&#26550;&#30340;&#25972;&#21512;&#65292;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#37325;&#29992;&#30340;&#20248;&#38597;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#26159;&#26500;&#24314;&#26234;&#33021;&#20195;&#29702;&#65292;&#23637;&#31034;&#31867;&#20284;&#20110;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#24555;&#36895;&#23398;&#20064;&#21644;&#28789;&#27963;&#30340;&#25216;&#33021;&#36716;&#31227;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#20915;&#36825;&#20123;&#30446;&#26631;&#30340;&#20004;&#20010;&#26694;&#26550;&#30340;&#25972;&#21512;&#65306;&#35760;&#24518;&#25511;&#21046;&#21644;&#21518;&#32487;&#29305;&#24449;&#12290;&#35760;&#24518;&#25511;&#21046;&#26159;&#19968;&#31181;&#21463;&#35748;&#30693;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#24773;&#33410;&#35760;&#24518;&#65292;&#21363;&#20195;&#29702;&#30340;&#32463;&#39564;&#30340;&#22522;&#20110;&#23454;&#20363;&#30340;&#20869;&#23384;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#21518;&#32487;&#29305;&#24449;&#21644;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827;&#65288;SF&amp;amp;GPI&#65289;&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#20855;&#26377;&#19981;&#21516;&#22870;&#21169;&#20989;&#25968;&#30340;&#21518;&#32493;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#22320;&#37325;&#29992;&#20808;&#21069;&#23398;&#21040;&#30340;&#31574;&#30053;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#20998;&#21035;&#22312;&#22823;&#22823;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#20248;&#38597;&#22320;&#37325;&#29992;&#20808;&#21069;&#23398;&#21040;&#30340;&#31574;&#30053;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20854;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
A longstanding goal in reinforcement learning is to build intelligent agents that show fast learning and a flexible transfer of skills akin to humans and animals. This paper investigates the integration of two frameworks for tackling those goals: episodic control and successor features. Episodic control is a cognitively inspired approach relying on episodic memory, an instance-based memory model of an agent's experiences. Meanwhile, successor features and generalized policy improvement (SF&amp;GPI) is a meta and transfer learning framework allowing to learn policies for tasks that can be efficiently reused for later tasks which have a different reward function. Individually, these two techniques have shown impressive results in vastly improving sample efficiency and the elegant reuse of previously learned policies. Thus, we outline a combination of both approaches in a single reinforcement learning framework and empirically illustrate its benefits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#20272;&#35745;&#36136;&#37327;&#21644;&#39044;&#27979;&#21306;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2106.03395</link><description>&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Evaluate Uncertainty Estimates in Machine Learning for Regression?. (arXiv:2106.03395v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#20272;&#35745;&#36136;&#37327;&#21644;&#39044;&#27979;&#21306;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#21450;&#65292;&#23545;&#20110;&#30456;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22823;&#12290;&#30446;&#21069;&#26377;&#20004;&#31181;&#20027;&#35201;&#30340;&#26041;&#27861;&#26469;&#27979;&#35797;&#36825;&#20123;&#20272;&#35745;&#30340;&#36136;&#37327;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#36755;&#20986;&#19968;&#20010;&#27010;&#29575;&#23494;&#24230;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20854;&#23545;&#25968;&#20284;&#28982;&#26469;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20182;&#26041;&#27861;&#30452;&#25509;&#36755;&#20986;&#19968;&#20010;&#39044;&#27979;&#21306;&#38388;&#65292;&#36890;&#24120;&#36890;&#36807;&#26816;&#26597;&#33853;&#20837;&#30456;&#24212;&#39044;&#27979;&#21306;&#38388;&#30340;&#27979;&#35797;&#28857;&#30340;&#27604;&#20363;&#26469;&#36827;&#34892;&#27979;&#35797;&#12290;&#30452;&#35266;&#19978;&#30475;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26159;&#21512;&#29702;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35770;&#35777;&#21644;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#36136;&#37327;&#30340;&#36825;&#20004;&#31181;&#26041;&#24335;&#37117;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#39318;&#20808;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#26080;&#27861;&#20998;&#31163;&#20849;&#21516;&#20135;&#29983;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#32780;&#38590;&#20197;&#35780;&#20272;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#30340;&#20272;&#35745;&#36136;&#37327;&#12290;&#20854;&#27425;&#65292;&#26356;&#22909;&#30340;&#23545;&#25968;&#20284;&#28982;&#24182;&#19981;&#20445;&#35777;&#26356;&#22909;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#25152;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As neural networks become more popular, the need for accompanying uncertainty estimates increases. There are currently two main approaches to test the quality of these estimates. Most methods output a density. They can be compared by evaluating their loglikelihood on a test set. Other methods output a prediction interval directly. These methods are often tested by examining the fraction of test points that fall inside the corresponding prediction intervals. Intuitively both approaches seem logical. However, we demonstrate through both theoretical arguments and simulations that both ways of evaluating the quality of uncertainty estimates have serious flaws. Firstly, both approaches cannot disentangle the separate components that jointly create the predictive uncertainty, making it difficult to evaluate the quality of the estimates of these components. Secondly, a better loglikelihood does not guarantee better prediction intervals, which is what the methods are often used for in practice
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#38543;&#26426;&#31181;&#26893;&#26862;&#26519;"&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#20445;&#30041;&#20999;&#20998;&#21518;&#30340;&#26576;&#20123;&#21494;&#23376;&#65292;&#24418;&#25104;&#38750;&#20108;&#36827;&#21046;&#26641;&#65292;&#23454;&#29616;&#30452;&#25509;&#21487;&#35299;&#37322;&#30340;&#26641;&#38598;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#21644;&#21487;&#35270;&#21270;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2012.14563</link><description>&lt;p&gt;
&#38543;&#26426;&#31181;&#26893;&#26862;&#26519;&#65306;&#19968;&#31181;&#30452;&#25509;&#21487;&#35299;&#37322;&#30340;&#26641;&#38598;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random Planted Forest: a directly interpretable tree ensemble. (arXiv:2012.14563v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.14563
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#38543;&#26426;&#31181;&#26893;&#26862;&#26519;"&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#20445;&#30041;&#20999;&#20998;&#21518;&#30340;&#26576;&#20123;&#21494;&#23376;&#65292;&#24418;&#25104;&#38750;&#20108;&#36827;&#21046;&#26641;&#65292;&#23454;&#29616;&#30452;&#25509;&#21487;&#35299;&#37322;&#30340;&#26641;&#38598;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#21644;&#21487;&#35270;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26641;&#30340;&#22238;&#24402;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#20174;&#21151;&#33021;&#20998;&#35299;&#30340;&#35282;&#24230;&#20272;&#35745;&#26410;&#30693;&#30340;&#22238;&#24402;&#20989;&#25968;&#65292;&#20854;&#20013;&#21151;&#33021;&#32452;&#20214;&#23545;&#24212;&#20110;&#20302;&#38454;&#20132;&#20114;&#39033;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#20462;&#25913;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20999;&#20998;&#21518;&#20445;&#30041;&#26576;&#20123;&#21494;&#23376;&#32780;&#19981;&#26159;&#21024;&#38500;&#23427;&#20204;&#12290;&#36825;&#23548;&#33268;&#38750;&#20108;&#36827;&#21046;&#26641;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31181;&#26893;&#26641;&#12290;&#23558;&#20854;&#25193;&#23637;&#20026;&#19968;&#20010;&#26862;&#26519;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#38543;&#26426;&#31181;&#26893;&#26862;&#26519;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#23545;&#21494;&#23376;&#20869;&#21487;&#20197;&#30456;&#20114;&#20316;&#29992;&#30340;&#21327;&#21464;&#37327;&#25968;&#37327;&#36827;&#34892;&#38480;&#21046;&#12290;&#22914;&#26524;&#25105;&#20204;&#23558;&#20132;&#20114;&#38480;&#21046;&#35774;&#32622;&#20026;1&#65292;&#24471;&#21040;&#30340;&#20272;&#35745;&#37327;&#26159;&#19968;&#32500;&#20989;&#25968;&#30340;&#21644;&#12290;&#22312;&#21478;&#19968;&#20010;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#25105;&#20204;&#19981;&#35774;&#32622;&#38480;&#21046;&#65292;&#24471;&#21040;&#30340;&#20272;&#35745;&#37327;&#21644;&#30456;&#24212;&#30340;&#27169;&#22411;&#23545;&#22238;&#24402;&#20989;&#25968;&#30340;&#24418;&#24335;&#19981;&#21152;&#38480;&#21046;&#12290;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#38543;&#26426;&#31181;&#26893;&#26862;&#26519;&#31639;&#27861;&#20855;&#26377;&#40723;&#21169;&#20154;&#30340;&#39044;&#27979;&#21644;&#21487;&#35270;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel interpretable tree based algorithm for prediction in a regression setting. Our motivation is to estimate the unknown regression function from a functional decomposition perspective in which the functional components correspond to lower order interaction terms. The idea is to modify the random forest algorithm by keeping certain leaves after they are split instead of deleting them. This leads to non-binary trees which we refer to as planted trees. An extension to a forest leads to our random planted forest algorithm. Additionally, the maximum number of covariates which can interact within a leaf can be bounded. If we set this interaction bound to one, the resulting estimator is a sum of one-dimensional functions. In the other extreme case, if we do not set a limit, the resulting estimator and corresponding model place no restrictions on the form of the regression function. In a simulation study we find encouraging prediction and visualisation properties of our rando
&lt;/p&gt;</description></item><item><title>ROME&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;NAS&#26041;&#27861;&#65292;&#36890;&#36807;&#25299;&#25169;&#35299;&#32806;&#21644;&#26799;&#24230;&#32047;&#31215;&#35299;&#20915;&#20102;&#21333;&#36335;&#24452;DARTS&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.11233</link><description>&lt;p&gt;
ROME: &#36890;&#36807;&#25299;&#25169;&#35299;&#32806;&#21644;&#26799;&#24230;&#32047;&#31215;&#23454;&#29616;&#40065;&#26834;&#30340;&#20869;&#23384;&#39640;&#25928;NAS
&lt;/p&gt;
&lt;p&gt;
ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation. (arXiv:2011.11233v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.11233
&lt;/p&gt;
&lt;p&gt;
ROME&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;NAS&#26041;&#27861;&#65292;&#36890;&#36807;&#25299;&#25169;&#35299;&#32806;&#21644;&#26799;&#24230;&#32047;&#31215;&#35299;&#20915;&#20102;&#21333;&#36335;&#24452;DARTS&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#25972;&#20010;&#36229;&#32593;&#32476;&#23384;&#25918;&#22312;&#20869;&#23384;&#20013;&#65292;&#23427;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#20869;&#23384;&#24320;&#38144;&#30340;&#38480;&#21046;&#12290;&#36825;&#23601;&#26159;&#21333;&#36335;&#24452;DARTS&#30340;&#20248;&#21183;&#25152;&#22312;&#65292;&#23427;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#21482;&#36873;&#25321;&#19968;&#20010;&#21333;&#36335;&#24452;&#23376;&#27169;&#22411;&#12290;&#34429;&#28982;&#23427;&#23545;&#20869;&#23384;&#21451;&#22909;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#36335;&#24452;DARTS&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#36825;&#19968;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20063;&#20250;&#20986;&#29616;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#20687;DARTS&#19968;&#26679;&#65292;&#23427;&#20250;&#23548;&#20986;&#22826;&#22810;&#26080;&#21442;&#25968;&#25805;&#20316;&#65292;&#20363;&#22914;&#36339;&#36291;&#36830;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25299;&#25169;&#25628;&#32034;&#21644;&#25805;&#20316;&#25628;&#32034;&#35299;&#32806;&#65292;&#20351;&#25628;&#32034;&#21644;&#35780;&#20272;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;Gumbel-Top2&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#26799;&#24230;&#32047;&#31215;&#26469;&#22686;&#24378;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21452;&#23618;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;ROME&#36827;&#34892;&#20102;&#22823;&#37327;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Albeit being a prevalent architecture searching approach, differentiable architecture search (DARTS) is largely hindered by its substantial memory cost since the entire supernet resides in the memory. This is where the single-path DARTS comes in, which only chooses a single-path submodel at each step. While being memory-friendly, it also comes with low computational costs. Nonetheless, we discover a critical issue of single-path DARTS that has not been primarily noticed. Namely, it also suffers from severe performance collapse since too many parameter-free operations like skip connections are derived, just like DARTS does. In this paper, we propose a new algorithm called RObustifying Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology search from the operation search to make searching and evaluation consistent. We then adopt Gumbel-Top2 reparameterization and gradient accumulation to robustify the unwieldy bi-level optimization. We verify ROME extensively acr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#24230;&#37327;&#23618;&#27425;&#32858;&#31867;&#30340;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#31283;&#23450;&#19968;&#33268;&#30340;&#23494;&#24230;-based&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#20013;&#25552;&#21462;&#21333;&#20010;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2005.09048</link><description>&lt;p&gt;
&#31283;&#23450;&#19968;&#33268;&#30340;&#23494;&#24230;-based&#32858;&#31867;&#31639;&#27861;&#36890;&#36807;&#22810;&#21442;&#25968;&#25345;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stable and consistent density-based clustering via multiparameter persistence. (arXiv:2005.09048v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.09048
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#24230;&#37327;&#23618;&#27425;&#32858;&#31867;&#30340;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#31283;&#23450;&#19968;&#33268;&#30340;&#23494;&#24230;-based&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#20013;&#25552;&#21462;&#21333;&#20010;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24230;-Rips&#26500;&#36896;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#23494;&#24230;&#25935;&#24863;&#30340;&#22810;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#24230;&#37327;&#23618;&#27425;&#32858;&#31867;&#30340;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#65292;&#20998;&#26512;&#20102;&#23427;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#20174;&#24230;-Rips&#20013;&#21462;&#26576;&#20123;&#19968;&#21442;&#25968;&#20999;&#29255;&#21487;&#20197;&#24674;&#22797;&#20986;&#24050;&#30693;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20316;&#20026;&#22810;&#21442;&#25968;&#23545;&#35937;&#30340;&#24230;-Rips&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24230;-Rips&#20013;&#21462;&#20999;&#29255;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#31283;&#23450;&#24615;&#23646;&#24615;&#30340;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20174;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#20013;&#25552;&#21462;&#21333;&#20010;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#26041;&#38754;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the degree-Rips construction from topological data analysis, which provides a density-sensitive, multiparameter hierarchical clustering algorithm. We analyze its stability to perturbations of the input data using the correspondence-interleaving distance, a metric for hierarchical clusterings that we introduce. Taking certain one-parameter slices of degree-Rips recovers well-known methods for density-based clustering, but we show that these methods are unstable. However, we prove that degree-Rips, as a multiparameter object, is stable, and we propose an alternative approach for taking slices of degree-Rips, which yields a one-parameter hierarchical clustering algorithm with better stability properties. We prove that this algorithm is consistent, using the correspondence-interleaving distance. We provide an algorithm for extracting a single clustering from one-parameter hierarchical clusterings, which is stable with respect to the correspondence-interleaving distance. And, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#23454;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#23454;&#29616;&#23545;&#35268;&#36991;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#35757;&#32451;&#36807;&#31243;RAB&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#32039;&#23494;&#24615;&#12290;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23545;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#40065;&#26834;&#24615;&#20445;&#25252;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2003.08904</link><description>&lt;p&gt;
RAB: &#21487;&#35777;&#23454;&#25269;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RAB: Provable Robustness Against Backdoor Attacks. (arXiv:2003.08904v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.08904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#23454;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#23454;&#29616;&#23545;&#35268;&#36991;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#35757;&#32451;&#36807;&#31243;RAB&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#32039;&#23494;&#24615;&#12290;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23545;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#40065;&#26834;&#24615;&#20445;&#25252;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21253;&#25324;&#35268;&#36991;&#25915;&#20987;&#21644;&#21518;&#38376;&#65288;&#27602;&#21270;&#65289;&#25915;&#20987;&#12290;&#22312;&#38450;&#24481;&#26041;&#38754;&#65292;&#23545;&#20110;&#35268;&#36991;&#25915;&#20987;&#24050;&#32463;&#36827;&#34892;&#20102;&#23494;&#38598;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#32463;&#39564;&#21644;&#21487;&#35777;&#23454;&#30340;&#40065;&#26834;&#24615;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#21487;&#35777;&#23454;&#40065;&#26834;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#38024;&#23545;&#36890;&#29992;&#23041;&#32961;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#23454;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#26469;&#20445;&#35777;&#23545;&#35268;&#36991;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#39318;&#20010;&#40065;&#26834;&#35757;&#32451;&#36807;&#31243;RAB&#65292;&#20351;&#35757;&#32451;&#27169;&#22411;&#24179;&#28369;&#24182;&#20445;&#35777;&#20854;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;RAB&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#40065;&#26834;&#24615;&#30028;&#38480;&#26159;&#32039;&#23494;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#23454;&#29616;&#23545;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#40065;&#26834;&#24615;&#20445;&#25252;&#26159;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible
&lt;/p&gt;</description></item></channel></rss>