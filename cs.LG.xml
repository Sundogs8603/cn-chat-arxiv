<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#21069;&#21521;&#36866;&#24212;&#65288;FOA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#23548;&#25968;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#20165;&#23398;&#20064;&#26032;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;&#27169;&#22411;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2404.01650</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#25773;&#30340;&#27979;&#35797;&#26102;&#38388;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Model Adaptation with Only Forward Passes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#21069;&#21521;&#36866;&#24212;&#65288;FOA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#23548;&#25968;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#20165;&#23398;&#20064;&#26032;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;&#27169;&#22411;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24050;&#34987;&#35777;&#26126;&#22312;&#36866;&#24212;&#32473;&#23450;&#35757;&#32451;&#27169;&#22411;&#21040;&#20855;&#26377;&#28508;&#22312;&#20998;&#24067;&#36716;&#31227;&#30340;&#26410;&#35265;&#27979;&#35797;&#26679;&#26412;&#26102;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#27169;&#22411;&#36890;&#24120;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#65292;&#20363;&#22914;FPGA&#65292;&#24182;&#19988;&#36890;&#24120;&#34987;&#37327;&#21270;&#21644;&#30828;&#32534;&#30721;&#20026;&#19981;&#21487;&#20462;&#25913;&#30340;&#21442;&#25968;&#20197;&#21152;&#36895;&#12290;&#37492;&#20110;&#27492;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#27169;&#22411;&#26356;&#26032;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#21069;&#21521;&#36866;&#24212;&#65288;FOA&#65289;&#26041;&#27861;&#12290; &#22312;FOA&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#19968;&#20010;&#26080;&#23548;&#25968;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#26469;&#20165;&#23398;&#20064;&#26032;&#28155;&#21152;&#30340;&#25552;&#31034;&#65288;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65289;&#12290; &#20026;&#20102;&#20351;&#36825;&#31181;&#31574;&#30053;&#22312;&#25105;&#20204;&#30340;&#22312;&#32447;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#31283;&#23450;&#24037;&#20316;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36890;&#36807;&#34913;&#37327;&#27979;&#35797;&#35757;&#32451;&#32479;&#35745;&#24046;&#24322;&#21644;&#27169;&#22411;&#39044;&#27979;&#29109;&#30340;&#26032;&#39062;&#36866;&#24212;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28608;&#27963;&#31227;&#20301;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01650v1 Announce Type: new  Abstract: Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts. However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Only Adaptation (FOA) method. In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting sche
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#29702;&#35299;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#24182;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.18517</link><description>&lt;p&gt;
&#38024;&#23545;&#27491;&#21017;&#21270;&#38750;&#36127;&#23610;&#24230;&#19981;&#21464;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for Regularized Nonnegative Scale-invariant Low-rank Approximation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18517
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#29702;&#35299;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#24182;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#38750;&#36127;&#20302;&#31209;&#36924;&#36817;&#65292;&#22914;&#31232;&#30095;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#25110;&#31232;&#30095;&#30340;&#38750;&#36127;Tucker&#20998;&#35299;&#65292;&#26159;&#20855;&#26377;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#38477;&#32500;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#12290;&#28982;&#32780;&#65292;&#20174;&#23454;&#36341;&#35282;&#24230;&#26469;&#30475;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#22240;&#32032;&#29305;&#24615;&#20197;&#21450;&#32570;&#20047;&#25903;&#25345;&#36825;&#20123;&#36873;&#25321;&#30340;&#29702;&#35770;&#65292;&#27491;&#21017;&#21270;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#39640;&#25928;&#31639;&#27861;&#30340;&#35774;&#35745;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#20855;&#26377;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#30410;&#21644;&#26377;&#23475;&#25928;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18517v1 Announce Type: new  Abstract: Regularized nonnegative low-rank approximations such as sparse Nonnegative Matrix Factorization or sparse Nonnegative Tucker Decomposition are an important branch of dimensionality reduction models with enhanced interpretability. However, from a practical perspective, the choice of regularizers and regularization coefficients, as well as the design of efficient algorithms, is challenging because of the multifactor nature of these models and the lack of theory to back these choices. This paper aims at improving upon these issues. By studying a more general model called the Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance inherent to low-rank approximation models causes an implicit regularization with both unexpected beneficial and detrimental effects. This observation allows to better understand the effect of regularization functions in low-rank approximation models, to guide the choice of the regularization hyp
&lt;/p&gt;</description></item><item><title>&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36328;&#36234;PDEs&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#29992;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.17728</link><description>&lt;p&gt;
&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#26159;PDE&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders are PDE Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17728
&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36328;&#36234;PDEs&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#29992;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27714;&#35299;&#22120;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23454;&#29992;&#24615;&#30446;&#21069;&#21463;&#21040;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290; PDE&#22312;&#24191;&#27867;&#30340;&#23610;&#24230;&#19978;&#28436;&#21464;&#24182;&#23637;&#31034;&#20986;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#65307;&#39044;&#27979;&#36825;&#20123;&#29616;&#35937;&#23558;&#38656;&#35201;&#23398;&#20064;&#36328;&#36234;&#21508;&#31181;&#36755;&#20837;&#30340;&#34920;&#31034;&#65292;&#36825;&#20123;&#36755;&#20837;&#21487;&#33021;&#28085;&#30422;&#19981;&#21516;&#30340;&#31995;&#25968;&#12289;&#20960;&#20309;&#22270;&#24418;&#25110;&#26041;&#31243;&#12290;&#20316;&#20026;&#36890;&#21521;&#21487;&#27867;&#21270;PDE&#24314;&#27169;&#30340;&#19968;&#27493;&#65292;&#25105;&#20204;&#20026;PDEs&#35843;&#25972;&#20102;&#25513;&#30721;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36328;&#36234;PDEs&#65292;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#23398;&#20064;&#26377;&#29992;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#25513;&#30721;&#39044;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#27714;&#35299;&#22120;&#23545;&#26410;&#35265;&#26041;&#31243;&#30340;&#31995;&#25968;&#22238;&#24402;&#21644;&#26102;&#38388;&#27493;&#39588;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25513;&#30721;&#39044;&#35757;&#32451;&#33021;&#25104;&#20026;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#12289;&#26410;&#26631;&#35760;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#35268;&#27169;&#21270;&#30340;&#28508;&#22312;&#29289;&#29702;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17728v1 Announce Type: new  Abstract: Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#29702;&#35299;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#30340;&#25805;&#20316;&#26426;&#21046;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#35813;&#26041;&#27861;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20854;&#26356;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26799;&#24230;&#24433;&#21709;&#21644;&#36739;&#24930;&#25910;&#25947;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.12404</link><description>&lt;p&gt;
&#29702;&#35299;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#65306;&#26426;&#21046;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Training-free Diffusion Guidance: Mechanisms and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#29702;&#35299;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#30340;&#25805;&#20316;&#26426;&#21046;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#35813;&#26041;&#27861;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20854;&#26356;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26799;&#24230;&#24433;&#21709;&#21644;&#36739;&#24930;&#25910;&#25947;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#28155;&#21152;&#39069;&#22806;&#25511;&#21046;&#24050;&#25104;&#20026;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22312;&#24178;&#20928;&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;&#32593;&#32476;&#36827;&#34892;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#26465;&#20214;&#29983;&#25104;&#65292;&#36866;&#29992;&#20110;&#36890;&#29992;&#25511;&#21046;&#26684;&#24335;&#65292;&#30475;&#36215;&#26469;&#25552;&#20379;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#20013;&#30340;&#20813;&#36153;&#21320;&#39184;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#26080;&#38656;&#35757;&#32451;&#30340;&#24341;&#23548;&#30340;&#36816;&#34892;&#26426;&#21046;&#21644;&#22522;&#26412;&#38480;&#21046;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#39033;&#25903;&#25345;&#26080;&#38656;&#35757;&#32451;&#24341;&#23548;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#21306;&#20998;&#20102;&#23427;&#19982;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#65288;&#25110;&#32773;&#26080;&#20998;&#31867;&#22120;&#30340;&#65289;&#24341;&#23548;&#12290;&#20026;&#20102;&#38416;&#26126;&#23427;&#20204;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26080;&#38656;&#35757;&#32451;&#26041;&#27861;&#26356;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26799;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26356;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12404v1 Announce Type: new  Abstract: Adding additional control to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science. Recently, several studies have proposed training-free diffusion guidance by using off-the-shelf networks pretrained on clean images. This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance. In this paper, we aim to develop a deeper understanding of the operational mechanisms and fundamental limitations of training-free guidance. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free methods are more susceptible to adversarial gradients and exhibit slower conv
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#36830;&#32493;&#23545;&#25239;&#35757;&#32451;&#65288;ACAT&#65289;&#26469;&#25345;&#32493;&#38598;&#25104;&#23545;&#25239;&#35757;&#32451;&#26679;&#26412;&#21040;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#23454;&#38469;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#25968;&#25454;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#23545;&#25239;&#23041;&#32961;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10461</link><description>&lt;p&gt;
&#24341;&#20837;&#33258;&#36866;&#24212;&#36830;&#32493;&#23545;&#25239;&#35757;&#32451;&#65288;ACAT&#65289;&#20197;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10461
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#36830;&#32493;&#23545;&#25239;&#35757;&#32451;&#65288;ACAT&#65289;&#26469;&#25345;&#32493;&#38598;&#25104;&#23545;&#25239;&#35757;&#32451;&#26679;&#26412;&#21040;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#23454;&#38469;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#25968;&#25454;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#23545;&#25239;&#23041;&#32961;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26131;&#21463;&#38024;&#23545;ML&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#27450;&#39575;ML&#27169;&#22411;&#65292;&#20351;&#20854;&#20135;&#29983;&#38169;&#35823;&#39044;&#27979;&#12290; &#23545;&#25239;&#35757;&#32451;&#34987;&#21457;&#29616;&#33021;&#25552;&#39640;ML&#27169;&#22411;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#32593;&#32476;&#21644;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#27010;&#24565;&#28418;&#31227;&#21152;&#28145;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#35832;&#22914;&#32593;&#32476;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21160;&#24577;&#39046;&#22495;&#20013;&#65292;&#38656;&#35201;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#23450;&#26399;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#36830;&#32493;&#23545;&#25239;&#35757;&#32451;&#65288;ACAT&#65289;&#65292;&#20197;&#22312;&#25345;&#32493;&#30340;&#23398;&#20064;&#20250;&#35805;&#26399;&#38388;&#25345;&#32493;&#23558;&#23545;&#25239;&#35757;&#32451;&#26679;&#26412;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#23454;&#38469;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#23545;&#25239;&#23041;&#32961;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290; ACAT&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#21033;&#29992;&#23450;&#26399;&#37325;&#26032;&#35757;&#32451;&#26469;&#26377;&#25928;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#65292;&#21516;&#26102;&#20943;&#36731;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10461v1 Announce Type: new  Abstract: Machine Learning (ML) is susceptible to adversarial attacks that aim to trick ML models, making them produce faulty predictions. Adversarial training was found to increase the robustness of ML models against these attacks. However, in network and cybersecurity, obtaining labeled training and adversarial training data is challenging and costly. Furthermore, concept drift deepens the challenge, particularly in dynamic domains like network and cybersecurity, and requires various models to conduct periodic retraining. This letter introduces Adaptive Continuous Adversarial Training (ACAT) to continuously integrate adversarial training samples into the model during ongoing learning sessions, using real-world detected adversarial data, to enhance model resilience against evolving adversarial threats. ACAT is an adaptive defense mechanism that utilizes periodic retraining to effectively counter adversarial attacks while mitigating catastrophic f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#20445;&#30041;&#20102;&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#36991;&#20813;&#20102;&#32047;&#31215;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06328</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#23454;&#29616;&#21487;&#36801;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferable Reinforcement Learning via Generalized Occupancy Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#20445;&#30041;&#20102;&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#36991;&#20813;&#20102;&#32047;&#31215;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20195;&#29702;&#24517;&#39035;&#26159;&#36890;&#29992;&#30340; - &#20855;&#26377;&#24555;&#36895;&#36866;&#24212;&#21644;&#27010;&#25324;&#21040;&#19981;&#21516;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#20869;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#31639;&#27861;&#23398;&#20064;&#19990;&#30028;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21160;&#24577;&#27169;&#22411;&#65292;&#21407;&#21017;&#19978;&#20351;&#23427;&#20204;&#33021;&#22815;&#27010;&#25324;&#21040;&#20219;&#24847;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#19968;&#27493;&#27169;&#22411;&#33258;&#28982;&#20250;&#21463;&#21040;&#32047;&#31215;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#20351;&#23427;&#20204;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#22823;&#29366;&#24577;&#31354;&#38388;&#30340;&#38382;&#39064;&#19978;&#22833;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#22411;&#27169;&#22411; - &#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65288;GOMs&#65289;&#65292;&#20445;&#30041;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32047;&#31215;&#24615;&#38169;&#35823;&#12290;GOMs&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#35206;&#30422;&#19979;&#65292;&#24314;&#27169;&#32473;&#23450;&#29366;&#24577;&#30340;&#25152;&#26377;&#21487;&#33021;&#38271;&#26399;&#32467;&#26524;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#23454;&#29616;&#32473;&#23450;&#29366;&#24577;&#30340;&#29305;&#23450;&#32467;&#26524;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36805;&#36895;&#29992;&#20110;&#20026;&#20219;&#24847;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20248;&#25805;&#20316;&#65292;&#32780;&#26080;&#38656;&#25285;&#24515;&#32047;&#31215;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06328v1 Announce Type: new  Abstract: Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without hav
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06054</link><description>&lt;p&gt;
&#20855;&#26377;&#25193;&#25955;&#20928;&#21270;&#30340;&#20998;&#31163;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Decoupled Data Consistency with Diffusion Purification for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#31867;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#22320;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#65292;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#20284;&#28982;&#26799;&#24230;&#27493;&#39588;&#32435;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#26799;&#24230;&#27493;&#39588;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#24403;&#20351;&#29992;&#21152;&#36895;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#22120;&#26102;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#27493;&#39588;&#36824;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#30340;&#25968;&#37327;&#21463;&#38480;&#20110;&#21453;&#21521;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#20998;&#31163;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#24773;&#32490;&#25512;&#29702;&#65288;CLEF&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#20559;&#24046;&#24178;&#25200;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.05963</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#21435;&#20559;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robust Emotion Recognition in Context Debiasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#24773;&#32490;&#25512;&#29702;&#65288;CLEF&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#20559;&#24046;&#24178;&#25200;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#24773;&#32490;&#35782;&#21035;&#65288;CAER&#65289;&#26368;&#36817;&#22312;&#26080;&#32422;&#26463;&#29615;&#22659;&#20013;&#25512;&#21160;&#20102;&#24773;&#24863;&#35745;&#31639;&#25216;&#26415;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290; &#20027;&#27969;&#30340;CAER&#26041;&#27861;&#24635;&#26159;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#21644;&#20197;&#20027;&#20307;&#20026;&#20013;&#24515;&#30340;&#29305;&#24449;&#20013;&#25552;&#21462;&#38598;&#25104;&#34920;&#31034;&#65292;&#20197;&#24863;&#30693;&#30446;&#26631;&#20154;&#29289;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290; &#23613;&#31649;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#26368;&#22823;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#30001;&#20110;&#19978;&#19979;&#25991;&#20559;&#24046;&#30340;&#24178;&#25200;&#12290; &#26377;&#23475;&#30340;&#20559;&#35265;&#36843;&#20351;&#27169;&#22411;&#20381;&#36182;&#20110;&#32972;&#26223;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#21487;&#33021;&#24615;&#20272;&#35745;&#20013;&#36896;&#25104;&#20005;&#37325;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#20351;&#26377;&#20215;&#20540;&#30340;&#19978;&#19979;&#25991;&#20808;&#39564;&#28151;&#28102;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#24773;&#32490;&#25512;&#29702;&#65288;CLEF&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#24191;&#20041;&#22240;&#26524;&#22270;&#65292;&#20197;&#35299;&#32806;CAER&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290; &#36981;&#24490;&#22240;&#26524;&#22270;&#65292;CLEF&#24341;&#20837;&#20102;&#19968;&#20010;&#38750;&#20405;&#20837;&#24335;&#30340;&#19978;&#19979;&#25991;&#20998;&#25903;&#26469;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05963v1 Announce Type: cross  Abstract: Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#24494;&#35843;&#31034;&#20363;&#30340;&#30417;&#30563;&#26469;&#25511;&#21046;&#20854;&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#20102;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.05612</link><description>&lt;p&gt;
&#19981;&#29087;&#24713;&#30340;&#24494;&#35843;&#31034;&#20363;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Unfamiliar Finetuning Examples Control How Language Models Hallucinate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#24494;&#35843;&#31034;&#20363;&#30340;&#30417;&#30563;&#26469;&#25511;&#21046;&#20854;&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#20102;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20542;&#21521;&#20110;&#29983;&#25104;&#21548;&#36215;&#26469;&#20196;&#20154;&#20449;&#26381;&#20294;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#29305;&#21035;&#26159;&#24403;&#22312;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#19978;&#36827;&#34892;&#26597;&#35810;&#26102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35843;&#25972;&#21518;&#30340;LLMs&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#27169;&#24335;&#65306;&#38543;&#30528;&#36755;&#20837;&#21464;&#24471;&#26356;&#19981;&#29087;&#24713;&#65292;LLMs&#30340;&#36755;&#20986;&#20542;&#21521;&#20110;&#40664;&#35748;&#20026;"&#21547;&#31946;&#20854;&#35789;"&#30340;&#39044;&#27979;&#65292;&#20854;&#24418;&#24335;&#21463;&#24494;&#35843;&#25968;&#25454;&#20013;&#19981;&#29087;&#24713;&#31034;&#20363;&#30417;&#30563;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20462;&#25913;&#36825;&#20123;&#31034;&#20363;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;LLM&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#65288;&#20363;&#22914;&#65292;&#25945;&#20250;&#23427;&#20204;&#35828;&#8220;&#25105;&#19981;&#30693;&#36947;&#8221;&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;RL&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22870;&#21169;&#27169;&#22411;&#24187;&#35273;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MMLU&#19978;&#30340;&#22810;&#36873;QA&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05612v1 Announce Type: cross  Abstract: Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts. In this work, we explore the underlying mechanisms that govern how finetuned LLMs hallucinate. Our investigation reveals an interesting pattern: as inputs become more unfamiliar, LLM outputs tend to default towards a ``hedged'' prediction, whose form is determined by how the unfamiliar examples in the finetuning data are supervised. Thus, by strategically modifying these examples' supervision, we can control LLM predictions for unfamiliar inputs (e.g., teach them to say ``I don't know''). Based on these principles, we develop an RL approach that more reliably mitigates hallucinations for long-form generation tasks, by tackling the challenges presented by reward model hallucinations. We validate our findings with a series of controlled experiments in multiple-choice QA on MMLU, as
&lt;/p&gt;</description></item><item><title>&#20027;&#21160;&#25512;&#26029;&#26159;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30830;&#23450;&#26368;&#26377;&#21033;&#20110;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#26469;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#65292;&#23454;&#29616;&#27604;&#29616;&#26377;&#22522;&#32447;&#26356;&#23569;&#26679;&#26412;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03208</link><description>&lt;p&gt;
&#20027;&#21160;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Active Statistical Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03208
&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#26029;&#26159;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30830;&#23450;&#26368;&#26377;&#21033;&#20110;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#26469;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#65292;&#23454;&#29616;&#27604;&#29616;&#26377;&#22522;&#32447;&#26356;&#23569;&#26679;&#26412;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20027;&#21160;&#23398;&#20064;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20027;&#21160;&#25512;&#26029;&#8212;&#8212;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#25968;&#25454;&#25910;&#38598;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#20551;&#35774;&#23545;&#21487;&#25910;&#38598;&#30340;&#26631;&#31614;&#25968;&#37327;&#26377;&#39044;&#31639;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30830;&#23450;&#21738;&#20123;&#25968;&#25454;&#28857;&#26368;&#26377;&#21033;&#20110;&#26631;&#35760;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#12290;&#20854;&#36816;&#20316;&#26041;&#24335;&#22522;&#20110;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#30452;&#35273;&#65306;&#20248;&#20808;&#25910;&#38598;&#27169;&#22411;&#34920;&#29616;&#20986;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#28857;&#30340;&#26631;&#31614;&#65292;&#24182;&#22312;&#27169;&#22411;&#34920;&#29616;&#20986;&#33258;&#20449;&#26102;&#20381;&#36182;&#20110;&#20854;&#39044;&#27979;&#12290;&#20027;&#21160;&#25512;&#26029;&#26500;&#24314;&#20102;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#20551;&#35774;&#26816;&#39564;&#65292;&#21516;&#26102;&#21033;&#29992;&#20219;&#20309;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#22788;&#29702;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#12290;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#23427;&#33021;&#20197;&#27604;&#20381;&#36182;&#20110;&#38750;&#33258;&#36866;&#24212;&#25910;&#38598;&#25968;&#25454;&#30340;&#29616;&#26377;&#22522;&#32447;&#26356;&#23569;&#30340;&#26679;&#26412;&#36798;&#21040;&#30456;&#21516;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#20110;&#30456;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03208v1 Announce Type: cross  Abstract: Inspired by the concept of active learning, we propose active inference$\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02354</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Field Neural Networks for Air Quality Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#38382;&#39064;&#26088;&#22312;&#21033;&#29992;&#26469;&#33258;&#26377;&#38480;&#35266;&#27979;&#31449;&#30340;&#21382;&#21490;&#25968;&#25454;&#25512;&#26029;&#26410;&#30693;&#20301;&#32622;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#12290;&#32771;&#34385;&#21040;&#35266;&#27979;&#31449;&#39640;&#26114;&#30340;&#32500;&#25252;&#25104;&#26412;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#33391;&#22909;&#30340;&#25512;&#26029;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#33410;&#32422;&#25104;&#26412;&#24182;&#32454;&#21270;&#25968;&#25454;&#31890;&#24230;&#12290;&#23613;&#31649;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#29616;&#23454;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#21644;&#31163;&#25955;&#25968;&#25454;&#32467;&#26500;&#24314;&#27169;&#38480;&#21046;&#20102;&#28508;&#21147;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#21363;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#65292;&#21450;&#20854;&#23545;&#24212;&#30340;&#26032;&#26694;&#26550;&#65292;&#37329;&#23383;&#22612;&#25512;&#26029;&#65292;&#23558;&#20004;&#31181;&#19981;&#21516;&#30340;&#26102;&#31354;&#35266;&#28857;&#65292;&#22330;&#21644;&#22270;&#65292;&#30456;&#32467;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20013;&#22269;&#22823;&#38470;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02354v1 Announce Type: cross  Abstract: The air quality inference problem aims to utilize historical data from a limited number of observation sites to infer the air quality index at an unknown location. Considering the sparsity of data due to the high maintenance cost of the stations, good inference algorithms can effectively save the cost and refine the data granularity. While spatio-temporal graph neural networks have made excellent progress on this problem, their non-Euclidean and discrete data structure modeling of reality limits its potential. In this work, we make the first attempt to combine two different spatio-temporal perspectives, fields and graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and its corresponding new framework, Pyramidal Inference. Extensive experiments validate that our model achieves state-of-the-art performance in nationwide air quality inference in the Chinese Mainland, demonstrating the superiority of our proposed model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17606</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#25299;&#25169;&#34920;&#31034;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38024;&#23545;&#26080;&#21521;&#22270;&#30340;&#29616;&#25104;GNN&#27169;&#22411;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#65292;&#24182;&#24573;&#30053;&#20102;&#24182;&#21457;&#22270;&#65288;DGs&#65289;&#30340;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#39062;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#26412;&#22320;&#25628;&#32034;&#26694;&#26550;&#20013;&#23884;&#20837;DG&#20197;&#35299;&#20915;JSSP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TBGAT&#20998;&#21035;&#20174;&#27491;&#21521;&#21644;&#21453;&#21521;&#35270;&#22270;&#23884;&#20837;DG&#65292;&#28040;&#24687;&#36890;&#36807;&#36981;&#24490;&#19981;&#21516;&#35270;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#36827;&#34892;&#27719;&#24635;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#26032;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#35745;&#31639;DG&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#25299;&#25169;&#25490;&#24207;&#65292;&#36825;&#20123;&#29305;&#24449;&#29992;&#20110;&#34920;&#24449;&#25299;&#25169;&#32467;&#26500;&#24182;&#34987;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;TBGAT&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17606v1 Announce Type: cross  Abstract: Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;</title><link>https://arxiv.org/abs/2402.17106</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20844;&#24179;&#24615;&#65306;&#22312;&#24744;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#20013;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#23567;&#21270;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20043;&#38388;&#24046;&#24322;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#30340;&#20005;&#37325;&#31243;&#24230;&#22522;&#26412;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#22914;&#25968;&#25454;&#38598;&#30340;&#19981;&#22343;&#34913;&#25110;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#20351;&#29992;&#32479;&#19968;&#30340;&#20844;&#24179;&#24615;&#35201;&#27714;&#20173;&#28982;&#20540;&#24471;&#24576;&#30097;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#23548;&#33268;&#25928;&#29992;&#26497;&#20302;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#38598;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25903;&#25345;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;You-Only-Train-Once&#65288;YOTO&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#22312;&#36924;&#36817;&#26435;&#34913;&#26354;&#32447;&#26102;&#38656;&#35201;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#35813;&#26354;&#32447;&#21608;&#22260;&#24341;&#20837;&#32622;&#20449;&#21306;&#38388;&#26469;&#37327;&#21270;&#25105;&#20204;&#36817;&#20284;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17106v1 Announce Type: cross  Abstract: In machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. Therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17012</link><description>&lt;p&gt;
Pandora's White-Box&#65306;&#24320;&#25918;LLMs&#20013;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#30340;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Pandora's White-Box: Increased Training Data Leakage in Open LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36973;&#21463;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20854;&#20013;&#23545;&#25163;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#12289;&#26799;&#24230;&#25110;&#25439;&#22833;&#65292;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#26469;&#20102;&#35299;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;&#31532;&#19968;&#20010;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#39640;TPR&#21644;&#20302;FPR&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24213;&#23618;&#27169;&#22411;&#30340;&#19981;&#21516;&#35775;&#38382;&#31243;&#24230;&#12289;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#20197;&#21450;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#22312;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#30333;&#30418;MIAs&#65306;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#30340;&#25915;&#20987;&#12289;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#21644;&#21333;&#27493;&#25439;&#22833;&#27604;&#25915;&#20987;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#40657;&#30418;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;.....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.14883</link><description>&lt;p&gt;
&#21452;I&#27700;&#21360;&#65306;&#20445;&#25252;LLM&#24494;&#35843;&#27169;&#22411;&#29256;&#26435;
&lt;/p&gt;
&lt;p&gt;
Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14883
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#65292;&#19994;&#20027;&#32463;&#24120;&#36890;&#36807;LLM&#25152;&#26377;&#32773;&#25110;&#20113;&#26381;&#21153;&#22120;&#25552;&#20379;&#30340;API&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#21462;&#23450;&#21046;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#23384;&#22312;&#30528;&#27169;&#22411;&#34987;&#28389;&#29992;&#30340;&#39118;&#38505;&#65292;&#21487;&#33021;&#20250;&#32473;&#19994;&#20027;&#24102;&#26469;&#20005;&#37325;&#30340;&#32463;&#27982;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#20445;&#25252;&#36825;&#20123;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#24050;&#25104;&#20026;&#32039;&#36843;&#30340;&#23454;&#38469;&#38656;&#27714;&#65292;&#20294;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32039;&#36843;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#26032;&#22411;&#27700;&#21360;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#22522;&#20110;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#65292;&#20998;&#21035;&#22312;&#25351;&#20196;&#21644;&#36755;&#20837;&#20013;&#35302;&#21457;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#23558;&#23450;&#21046;&#30340;&#21518;&#38376;&#26679;&#26412;&#32435;&#20837;&#25968;&#25454;&#38598;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#27880;&#20837;&#20102;&#29305;&#23450;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14883v1 Announce Type: cross  Abstract: To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named "Double-I watermark". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermar
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.14648</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#37325;&#26032;&#24605;&#32771;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#20197;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14648
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#19968;&#30452;&#26159;&#25269;&#25239;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AEs&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#65292;&#23398;&#20064;&#20855;&#26377;&#36776;&#21035;&#24615;&#21364;&#23545;&#25239;&#24615;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#30830;&#23450;&#20102;&#22952;&#30861;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19981;&#21464;&#24615;&#25439;&#22833;&#21644;&#20998;&#31867;&#30446;&#26631;&#20043;&#38388;&#30340;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#65292;&#34920;&#26126;&#23384;&#22312;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30001;&#20110;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#20998;&#24067;&#21457;&#25955;&#32780;&#20986;&#29616;&#30340;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AR-AT&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#20572;&#27490;&#26799;&#24230;&#25805;&#20316;&#21644;&#19968;&#20010;&#39044;&#27979;&#22120;&#26469;&#36991;&#20813;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#28789;&#24863;&#26469;&#33258;&#26368;&#36817;&#30340;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14648v1 Announce Type: cross  Abstract: Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a "gradient conflict" between invariance loss and classification objectives, indicating the existence of "collapsing solutions," and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid "collapsing solutions," inspired by a recent non-contrastive self-supervised learning a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#26497;&#23567;&#36951;&#25022;&#30340;&#22810;&#23545;&#25968;&#32553;&#25918;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#23454;&#29616;&#23545;&#35774;&#35745;&#30697;&#38453;&#29305;&#24449;&#20540;&#20851;&#31995;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#23545;&#25968;&#32553;&#25918;&#12290;</title><link>https://arxiv.org/abs/2402.12042</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#23545;&#25968;&#26497;&#23567;&#26497;&#23567;&#36951;&#25022;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Linear bandits with polylogarithmic minimax regret
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#26497;&#23567;&#36951;&#25022;&#30340;&#22810;&#23545;&#25968;&#32553;&#25918;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#23454;&#29616;&#23545;&#35774;&#35745;&#30697;&#38453;&#29305;&#24449;&#20540;&#20851;&#31995;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#23545;&#25968;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#26426;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#23545;&#20110;&#35813;&#27169;&#22411;&#65292;&#24403;&#25105;&#20204;&#36873;&#25321;&#36234;&#26469;&#36234;&#25509;&#36817;&#26410;&#30693;&#21521;&#37327;&#30340;&#21333;&#20301;&#29699;&#19978;&#30340;&#21160;&#20316;&#26102;&#65292;&#20122;&#39640;&#26031;&#22122;&#22768;&#21442;&#25968;&#20197;&#32447;&#24615;&#26041;&#24335;&#28040;&#22833;&#12290;&#25105;&#20204;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#22312;&#26102;&#38388;&#38271;&#24230;$T$&#30340;&#24773;&#20917;&#19979;&#21576;&#23545;&#25968;$^3&#65288;T&#65289;$&#30340;&#26368;&#23567;&#36951;&#25022;&#32553;&#25918;&#65292;&#19982;&#20856;&#22411;&#36172;&#21338;&#26426;&#31639;&#27861;&#30340;&#24179;&#26041;&#26681;&#36951;&#25022;&#32553;&#25918;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22522;&#20110;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#36890;&#36807;&#20960;&#20309;&#35770;&#35777;&#23454;&#29616;&#20102;&#35774;&#35745;&#30697;&#38453;$V_t$&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;$t$&#22788;&#30340;&#29305;&#24449;&#20540;&#20851;&#31995;$\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$&#65292;&#36825;&#20123;&#20960;&#20309;&#35770;&#35777;&#19982;&#22122;&#22768;&#27169;&#22411;&#26080;&#20851;&#65292;&#24182;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#25511;&#21046;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#26399;&#26395;&#36951;&#25022;&#20026;$O(\frac1{t})$&#30340;&#25968;&#37327;&#32423;&#65292;&#20174;&#32780;&#23548;&#33268;&#32047;&#31215;&#36951;&#25022;&#30340;&#23545;&#25968;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12042v1 Announce Type: cross  Abstract: We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms. Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#22312; Variational Autoencoder (VAE) &#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.11124</link><description>&lt;p&gt;
&#36890;&#36807;&#24320;&#20851;&#21464;&#37327;&#22312;&#38544;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#35299;&#24320;&#32416;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentanglement in Implicit Causal Models via Switch Variable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11124
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#22312; Variational Autoencoder (VAE) &#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#24449;&#65292;&#22312;&#27809;&#26377;&#24050;&#30693;&#30340;&#22320;&#38754;&#30495;&#23454;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#12290;&#38544;&#24335;&#23398;&#20064;&#22240;&#26524;&#26426;&#21046;&#36890;&#24120;&#28041;&#21450;&#20004;&#31867;&#24178;&#39044;&#25968;&#25454;&#65306;&#30828;&#24178;&#39044;&#21644;&#36719;&#24178;&#39044;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#65292;&#36719;&#24178;&#39044;&#36890;&#24120;&#27604;&#30828;&#24178;&#39044;&#26356;&#29616;&#23454;&#65292;&#22240;&#20026;&#21518;&#32773;&#38656;&#35201;&#23436;&#20840;&#21463;&#25511;&#30340;&#29615;&#22659;&#12290;&#19982;&#30452;&#25509;&#24378;&#21046;&#25913;&#21464;&#22240;&#26524;&#21464;&#37327;&#30340;&#30828;&#24178;&#39044;&#19981;&#21516;&#65292;&#36719;&#24178;&#39044;&#36890;&#36807;&#24433;&#21709;&#22240;&#26524;&#26426;&#21046;&#38388;&#25509;&#22320;&#20135;&#29983;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#20013;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26088;&#22312;&#22312;&#19981;&#21516;&#22240;&#26524;&#26426;&#21046;&#20043;&#38388;&#20999;&#25442;&#30340;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#26469;&#24314;&#27169;&#36719;&#24178;&#39044;&#25928;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22987;&#32456;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11124v1 Announce Type: new  Abstract: Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning. Implicitly learning causal mechanisms typically involves two categories of interventional data: hard and soft interventions. In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments. Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism. In this paper, we tackle implicit latent causal representation learning in a Variational Autoencoder (VAE) framework through soft interventions. Our approach models soft interventions effects by employing a causal mechanism switch variable designed to toggle between different causal mechanisms. In our experiments, we consistentl
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22810;&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;</title><link>https://arxiv.org/abs/2402.11101</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22522;&#20110;&#29289;&#29702;&#30340;&#26448;&#26009;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Physics-based material parameters extraction from perovskite experiments via Bayesian optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11101
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22810;&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23450;&#37327;&#23454;&#39564;&#20998;&#26512;&#20013;&#25552;&#21462;&#26448;&#26009;&#21442;&#25968;&#30340;&#33021;&#21147;&#23545;&#20110;&#21512;&#29702;&#35774;&#35745;&#21644;&#29702;&#35770;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29702;&#35770;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#26448;&#26009;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#20998;&#26512;&#30340;&#38590;&#24230;&#26174;&#30528;&#22686;&#21152;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#30636;&#24577;&#20809;&#33268;&#21457;&#20809;&#23454;&#39564;&#20013;&#25552;&#21462;&#19968;&#20010;&#26377;&#26426;&#37329;&#23646;&#38041;&#38043;&#30719;&#21322;&#23548;&#20307;&#30340;8&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#22522;&#20110;&#19968;&#20010;&#21253;&#25324;&#36733;&#27969;&#23376;&#28418;&#31227;&#25193;&#25955;&#21644;&#21160;&#24577;&#32570;&#38519;&#21344;&#25454;&#30340;&#22797;&#26434;&#20840;&#29289;&#29702;&#27169;&#22411;&#12290;&#28909;&#38477;&#35299;&#30340;&#19968;&#20010;&#31034;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#25530;&#26434;&#27987;&#24230;&#21644;&#36733;&#27969;&#23376;&#36801;&#31227;&#29575;&#30340;&#21464;&#21270;&#20027;&#23548;&#65292;&#32780;&#32570;&#38519;&#33021;&#32423;&#20960;&#20046;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#20010;&#24179;&#21488;&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;&#20854;&#20182;&#23454;&#39564;&#25110;&#23454;&#39564;&#32452;&#21512;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11101v1 Announce Type: cross  Abstract: The ability to extract material parameters from quantitative experimental analysis is essential for rational design and theory advancement. However, the difficulty of this analysis increases significantly with the complexity of the theoretical model and the number of material parameters. Here we use Bayesian optimization to develop an analysis platform that can extract up to 8 fundamental material parameters of an organometallic perovskite semiconductor from a transient photoluminescence experiment, based on a complex full physics model that includes drift-diffusion of carriers and dynamic defect occupation. An example study of thermal degradation reveals that changes in doping concentration and carrier mobility dominate, while the defect energy level remains nearly unchanged. This platform can be conveniently applied to other experiments or to combinations of experiments, accelerating materials discovery and optimization of semiconduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38899;&#39057;&#20449;&#21495;&#30340;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#36753;&#21644;&#26080;&#30417;&#30563;&#21457;&#29616;&#32534;&#36753;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38899;&#20048;&#20449;&#21495;&#20013;&#23637;&#29616;&#20102;&#22810;&#26679;&#30340;&#38899;&#20048;&#20852;&#36259;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2402.10009</link><description>&lt;p&gt;
&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38646;&#26679;&#26412;&#26080;&#30417;&#30563;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38899;&#39057;&#20449;&#21495;&#30340;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#36753;&#21644;&#26080;&#30417;&#30563;&#21457;&#29616;&#32534;&#36753;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38899;&#20048;&#20449;&#21495;&#20013;&#23637;&#29616;&#20102;&#22810;&#26679;&#30340;&#38899;&#20048;&#20852;&#36259;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#32534;&#36753;&#24050;&#32463;&#22312;&#22270;&#20687;&#39046;&#22495;&#21462;&#24471;&#20102;&#36805;&#29467;&#30340;&#21457;&#23637;&#65292;&#20294;&#22312;&#38899;&#39057;&#39046;&#22495;&#23578;&#26410;&#20986;&#29616;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#22522;&#20110;DDPM&#21453;&#36716;&#30340;&#38899;&#39057;&#20449;&#21495;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#12290;&#31532;&#19968;&#31181;&#26159;&#20174;&#22270;&#20687;&#39046;&#22495;&#37319;&#29992;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#22522;&#20110;&#25991;&#26412;&#36827;&#34892;&#32534;&#36753;&#12290;&#31532;&#20108;&#31181;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#21457;&#29616;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#32534;&#36753;&#26041;&#21521;&#12290;&#24403;&#24212;&#29992;&#20110;&#38899;&#20048;&#20449;&#21495;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23637;&#29616;&#20986;&#19968;&#31995;&#21015;&#20855;&#26377;&#38899;&#20048;&#20852;&#36259;&#30340;&#20462;&#25913;&#65292;&#20174;&#25511;&#21046;&#29305;&#23450;&#20048;&#22120;&#30340;&#21442;&#19982;&#21040;&#23545;&#26059;&#24459;&#36827;&#34892;&#21363;&#20852;&#28436;&#22863;&#12290;&#31034;&#20363;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#20363;&#23376;&#39029;&#38754;&#20013;&#25214;&#21040;&#65306;https://hilamanor.github.io/AudioEditing/ &#65292;&#20195;&#30721;&#21487;&#20197;&#22312; https://github.com/hilamanor/AudioEditing/ &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10009v1 Announce Type: cross  Abstract: Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody. Samples can be found on our examples page in https://hilamanor.github.io/AudioEditing/ and code can be found in https://github.com/hilamanor/AudioEditing/ .
&lt;/p&gt;</description></item><item><title>SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09025</link><description>&lt;p&gt;
SLEB: &#36890;&#36807;&#20887;&#20313;&#39564;&#35777;&#21644;&#28040;&#38500;Transformer&#22359;&#20248;&#21270;LLM&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09025
&lt;/p&gt;
&lt;p&gt;
SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#31934;&#31616;&#65292;&#19968;&#31181;&#26088;&#22312;&#20943;&#23567;LLM&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#20887;&#20313;&#32452;&#20214;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#31934;&#31616;&#26377;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31471;&#21040;&#31471;LLM&#25512;&#29702;&#21152;&#36895;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLEB&#65292;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;Transformer&#22359;&#20316;&#20026;&#31934;&#31616;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#22240;&#20026;LLM&#22312;&#30456;&#37051;&#22359;&#30340;&#36755;&#20986;&#20043;&#38388;&#20855;&#26377;&#22359;&#32423;&#21035;&#30340;&#20887;&#20313;&#21644;&#39640;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;LLM&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLEB&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#28193;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#30001;&#20110;&#36716;&#21464;&#32422;&#26463;&#23548;&#33268;&#30340;&#25628;&#32034;&#31354;&#38388;&#20381;&#36182;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21270;&#23398;&#21453;&#24212;&#22120;&#20248;&#21270;&#12289;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#12289;&#26426;&#22120;&#26657;&#20934;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.08406</link><description>&lt;p&gt;
&#36807;&#28193;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transition Constrained Bayesian Optimization via Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#28193;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#30001;&#20110;&#36716;&#21464;&#32422;&#26463;&#23548;&#33268;&#30340;&#25628;&#32034;&#31354;&#38388;&#20381;&#36182;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21270;&#23398;&#21453;&#24212;&#22120;&#20248;&#21270;&#12289;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#12289;&#26426;&#22120;&#26657;&#20934;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#20851;&#27880;&#30340;&#26159;&#21487;&#20197;&#20219;&#24847;&#26597;&#35810;&#25628;&#32034;&#31354;&#38388;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38382;&#39064;&#24182;&#19981;&#20855;&#22791;&#36825;&#31181;&#28789;&#27963;&#24615;&#65307;&#29305;&#21035;&#26159;&#65292;&#19979;&#19968;&#20010;&#26597;&#35810;&#30340;&#25628;&#32034;&#31354;&#38388;&#21487;&#33021;&#21462;&#20915;&#20110;&#20808;&#21069;&#30340;&#26597;&#35810;&#12290;&#29289;&#29702;&#31185;&#23398;&#39046;&#22495;&#30340;&#20363;&#23376;&#20013;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#23616;&#37096;&#31227;&#21160;&#38480;&#21046;&#12289;&#29305;&#23450;&#21464;&#37327;&#30340;&#21333;&#35843;&#24615;&#35201;&#27714;&#20197;&#21450;&#36716;&#21464;&#24433;&#21709;&#27979;&#37327;&#31934;&#24230;&#12290;&#24635;&#20043;&#65292;&#36825;&#20123;&#36807;&#28193;&#32422;&#26463;&#38656;&#35201;&#19968;&#31181;&#35268;&#21010;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#25193;&#23637;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36845;&#20195;&#22320;&#35299;&#20915;&#25105;&#20204;&#30446;&#26631;&#30340;&#19968;&#20010;&#21487;&#34892;&#32447;&#24615;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#33021;&#22815;&#25552;&#21069;&#35268;&#21010;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#31574;&#30053;&#12290;&#24471;&#21040;&#30340;&#31574;&#30053;&#21487;&#33021;&#26159;&#20381;&#36182;&#21382;&#21490;&#30340;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21270;&#23398;&#21453;&#24212;&#22120;&#20248;&#21270;&#12289;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#12289;&#26426;&#22120;&#26657;&#20934;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a methodology to optimize black-box functions. Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such transition constraints necessitate a form of planning. This work extends Bayesian optimization via the framework of Markov Decision Processes, iteratively solving a tractable linearization of our objective using reinforcement learning to obtain a policy that plans ahead over long horizons. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other s
&lt;/p&gt;</description></item><item><title>NeuRes&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#35777;&#26126;&#20026;&#22522;&#30784;&#30340;SAT&#35299;&#26512;&#22120;&#65292;&#33021;&#22815;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#24182;&#21152;&#36895;&#25214;&#21040;&#21487;&#28385;&#36275;&#30495;&#20540;&#20998;&#37197;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.08365</link><description>&lt;p&gt;
NeuRes: &#23398;&#20064;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
NeuRes: Learning Proofs of Propositional Satisfiability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08365
&lt;/p&gt;
&lt;p&gt;
NeuRes&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#35777;&#26126;&#20026;&#22522;&#30784;&#30340;SAT&#35299;&#26512;&#22120;&#65292;&#33021;&#22815;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#24182;&#21152;&#36895;&#25214;&#21040;&#21487;&#28385;&#36275;&#30495;&#20540;&#20998;&#37197;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#35777;&#26126;&#20026;&#22522;&#30784;&#30340;SAT&#35299;&#26512;&#22120;NeuRes&#12290;&#19982;&#20854;&#20182;&#31070;&#32463;SAT&#35299;&#31639;&#27861;&#19981;&#21516;&#65292;NeuRes&#33021;&#22815;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#23427;&#12290;NeuRes&#36890;&#36807;&#37319;&#29992;&#21629;&#39064;&#25512;&#29702;&#26469;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#24182;&#21152;&#36895;&#22312;&#19981;&#21487;&#28385;&#36275;&#21644;&#21487;&#28385;&#36275;&#20844;&#24335;&#20013;&#25214;&#21040;&#28385;&#36275;&#30495;&#20540;&#20998;&#37197;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25351;&#38024;&#32593;&#32476;&#30340;&#20803;&#32032;&#65292;&#20174;&#21160;&#24577;&#22270;&#32467;&#26500;&#20013;&#33258;&#21160;&#36873;&#25321;&#33410;&#28857;&#23545;&#65292;&#36825;&#23545;&#20110;&#29983;&#25104;&#35299;&#26512;&#35777;&#26126;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;NeuroSAT&#30456;&#21516;&#30340;&#38543;&#26426;&#20844;&#24335;&#20998;&#24067;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#25945;&#24072;&#35777;&#26126;&#21644;&#30495;&#20540;&#20998;&#37197;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NeuRes&#22312;&#19981;&#21516;&#20998;&#24067;&#19978;&#27604;NeuroSAT&#35299;&#20915;&#26356;&#22810;&#30340;&#27979;&#35797;&#20844;&#24335;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other neural SAT solving methods, NeuRes is capable of proving unsatisfiability as opposed to merely predicting it. By design, NeuRes operates in a certificate-driven fashion by employing propositional resolution to prove unsatisfiability and to accelerate the process of finding satisfying truth assignments in case of unsat and sat formulas, respectively. To realize this, we propose a novel architecture that adapts elements from Graph Neural Networks and Pointer Networks to autoregressively select pairs of nodes from a dynamic graph structure, which is essential to the generation of resolution proofs. Our model is trained and evaluated on a dataset of teacher proofs and truth assignments that we compiled with the same random formula distribution used by NeuroSAT. In our experiments, we show that NeuRes solves more test formulas than NeuroSAT by a rather wide margin on different distributions while being much more data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#21033;&#29992;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#31435;&#20307;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07868</link><description>&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#30340;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nesting Particle Filters for Experimental Design in Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#21033;&#29992;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#31435;&#20307;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#20132;&#25442;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20869;&#22806;SMC^2&#31639;&#27861;&#65292;&#20351;&#29992;&#23884;&#22871;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;SMC&#65289;&#20272;&#35745;&#22120;&#26469;&#39044;&#27979;&#26399;&#26395;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#31890;&#23376;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;pMCMC&#65289;&#26694;&#26550;&#20013;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#19982;&#26368;&#36817;&#20381;&#36182;&#20110;&#20559;&#20272;&#35745;&#22120;&#26469;&#25674;&#38144;&#20808;&#21069;&#23398;&#20064;&#35774;&#35745;&#31574;&#30053;&#30340;&#25104;&#26412;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#32452;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#20540;&#39564;&#35777;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization. This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#36229;&#36807;$\mathcal{O}(\sqrt{T})$&#36951;&#25022;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;$\mathcal{O}(T^{1/3})$&#30340;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2402.07108</link><description>&lt;p&gt;
&#35299;&#32806;&#23398;&#20064;&#21644;&#20915;&#31574;&#65306;&#29992;&#19968;&#38454;&#26041;&#27861;&#31361;&#30772;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;$\mathcal{O}(\sqrt{T})$&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Decoupling Learning and Decision-Making: Breaking the $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#36229;&#36807;$\mathcal{O}(\sqrt{T})$&#36951;&#25022;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;$\mathcal{O}(T^{1/3})$&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#22312;&#25910;&#30410;&#31649;&#29702;&#21644;&#36164;&#28304;&#20998;&#37197;&#20043;&#38388;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#19968;&#38454;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#23613;&#31649;&#19968;&#38454;&#26041;&#27861;&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21482;&#33021;&#23454;&#29616;$\mathcal{O}(\sqrt{T})$&#30340;&#36951;&#25022;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;(LP)&#30340;&#22312;&#32447;&#31639;&#27861;&#25152;&#20445;&#35777;&#30340;$\mathcal{O}(\log T)$&#30028;&#38480;&#30456;&#27604;&#26159;&#27425;&#20248;&#30340;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#20851;&#20110;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#30340;&#20960;&#20010;&#37325;&#35201;&#20107;&#23454;&#65292;&#25581;&#31034;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#36229;&#36807;$\mathcal{O}(\sqrt{T})$&#36951;&#25022;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#19982;&#20915;&#31574;&#20998;&#31163;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#36825;&#20010;&#26032;&#26694;&#26550;&#19979;&#21487;&#20197;&#36798;&#21040;$\mathcal{O}(T^{1/3})$&#30340;&#36951;&#25022;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online linear programming plays an important role in both revenue management and resource allocation, and recent research has focused on developing efficient first-order online learning algorithms. Despite the empirical success of first-order methods, they typically achieve a regret no better than $\mathcal{O}(\sqrt{T})$, which is suboptimal compared to the $\mathcal{O}(\log T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based online algorithms. This paper establishes several important facts about online linear programming, which unveils the challenge for first-order-method-based online algorithms to achieve beyond $\mathcal{O}(\sqrt{T})$ regret. To address the challenge, we introduce a new algorithmic framework that decouples learning from decision-making. More importantly, for the first time, we show that first-order methods can attain regret $\mathcal{O}(T^{1/3})$ with this new framework. Lastly, we conduct numerical experiments to validate our theoretical find
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06700</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Entropy-Regularized Token-Level Policy Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#12289;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25110;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#25110;RLHF&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#20811;&#26381;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#37325;&#22256;&#38590;&#65306;1&#65289;&#30001;&#20110;&#24040;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#38656;&#35201;&#25506;&#32034;&#32780;&#20135;&#29983;&#30340;&#19981;&#31283;&#23450;&#24615;&#65307;2&#65289;&#22522;&#20110;&#21160;&#20316;&#32423;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#20934;&#30830;&#24314;&#27169;&#35821;&#26009;&#24211;&#25968;&#25454;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#65288;ETPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22312;&#20196;&#29260;&#32423;&#20248;&#21270;LLMs&#32780;&#35774;&#35745;&#30340;&#29109;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;ETPO&#30340;&#26680;&#24515;&#26159;&#25105;&#20204;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#20196;&#29260;&#36719;Bellman&#26356;&#26032;&#31639;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#26159;&#19968;&#31181;&#31163;&#32447;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#31867;&#20984;&#20989;&#25968;&#26469;&#23454;&#29616;&#32479;&#19968;&#30340;&#20559;&#22909;&#20248;&#21270;&#35270;&#35282;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#24037;&#20855;&#21644;&#23454;&#35777;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.05749</link><description>&lt;p&gt;
&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65306;&#31163;&#32447;&#23545;&#40784;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Preference Optimization: A Unified Approach to Offline Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05749
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#26159;&#19968;&#31181;&#31163;&#32447;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#31867;&#20984;&#20989;&#25968;&#26469;&#23454;&#29616;&#32479;&#19968;&#30340;&#20559;&#22909;&#20248;&#21270;&#35270;&#35282;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#24037;&#20855;&#21644;&#23454;&#35777;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20559;&#22909;&#20248;&#21270;&#20801;&#35768;&#30452;&#25509;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#26368;&#36817;&#30340;&#23545;&#40784;&#23454;&#36341;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31867;&#36890;&#36807;&#19968;&#33324;&#30340;&#20984;&#20989;&#25968;&#21442;&#25968;&#21270;&#30340;&#31163;&#32447;&#25439;&#22833;&#20989;&#25968;&#12290;GPO&#25552;&#20379;&#20102;&#23545;&#20559;&#22909;&#20248;&#21270;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#28085;&#30422;&#20102;&#29616;&#26377;&#31639;&#27861;&#65288;DPO&#12289;IPO&#21644;SLiC&#65289;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#21516;&#26102;&#33258;&#28982;&#24341;&#20837;&#20102;&#26032;&#30340;&#21464;&#20307;&#12290;GPO&#26694;&#26550;&#36824;&#25581;&#31034;&#20102;&#31163;&#32447;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#23450;&#20041;&#25439;&#22833;&#30340;&#20984;&#20989;&#25968;&#26469;&#23454;&#26045;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#25581;&#31034;&#20102;&#31163;&#32447;&#27491;&#21017;&#21270;&#21644;&#35268;&#33539;&#30340;RLHF&#20844;&#24335;&#25152;&#24847;&#22270;&#30340;KL&#25955;&#24230;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#24494;&#22937;&#24046;&#24322;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#23545;&#40784;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#24037;&#20855;&#21644;&#23454;&#35777;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05643</link><description>&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Token-Based World Models with Parallel Observation Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;Transformer&#24212;&#29992;&#20110;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;TBWMs&#65289;&#20316;&#20026;&#39640;&#25928;&#26679;&#26412;&#26041;&#27861;&#12290;&#22312;TBWMs&#20013;&#65292;&#19990;&#30028;&#27169;&#22411;&#23558;&#20195;&#29702;&#32463;&#39564;&#20316;&#20026;&#19968;&#31181;&#31867;&#20284;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#28040;&#32791;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#26500;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20196;&#29260;&#36880;&#20010;&#29983;&#25104;&#19979;&#19968;&#20010;&#35266;&#27979;&#30340;&#20018;&#34892;&#26041;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;GPU&#21033;&#29992;&#29575;&#20302;&#21644;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#65288;POP&#65289;&#26426;&#21046;&#12290;POP&#36890;&#36807;&#19968;&#31181;&#38024;&#23545;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#35774;&#35745;&#30340;&#26032;&#22411;&#21069;&#21521;&#27169;&#24335;&#26469;&#25193;&#20805;&#20102;&#20445;&#25345;&#32593;&#32476;&#65288;RetNet&#65289;&#12290;&#25105;&#20204;&#23558;POP&#38598;&#25104;&#21040;&#19968;&#31181;&#21517;&#20026;REM&#65288;&#20445;&#25345;&#29615;&#22659;&#27169;&#22411;&#65289;&#30340;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;TBWMs&#24555;15.4&#20493;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;REM&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#28216;&#25103;&#20013;&#30340;12&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20248;&#20808;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20559;&#22909;&#21453;&#39304;&#26500;&#24314;&#40657;&#30418;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20048;&#35266;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#21644;&#25910;&#25947;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05367</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#21017;&#30340;&#20248;&#20808;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Principled Preferential Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20248;&#20808;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20559;&#22909;&#21453;&#39304;&#26500;&#24314;&#40657;&#30418;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20048;&#35266;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#21644;&#25910;&#25947;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#20808;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#24076;&#26395;&#20165;&#20973;&#20559;&#22909;&#21453;&#39304;&#26469;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#19968;&#23545;&#20505;&#36873;&#35299;&#12290;&#21463;&#21040;&#20284;&#28982;&#27604;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20165;&#20973;&#20559;&#22909;&#21453;&#39304;&#26500;&#24314;&#40657;&#30418;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20048;&#35266;&#31639;&#27861;&#21644;&#39640;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#22312;&#32047;&#31215;&#36951;&#25022;&#19978;&#20855;&#26377;&#20449;&#24687;&#35770;&#30340;&#30028;&#38480;&#65292;&#36825;&#22312;&#20248;&#20808;BO&#20013;&#26159;&#39318;&#27425;&#12290;&#36825;&#20010;&#30028;&#38480;&#36827;&#19968;&#27493;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#26041;&#26696;&#26469;&#25253;&#21578;&#26368;&#20339;&#35299;&#30340;&#20272;&#35745;&#20540;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#36895;&#29575;&#12290;&#20174;&#39640;&#26031;&#36807;&#31243;&#12289;&#26631;&#20934;&#27979;&#35797;&#20989;&#25968;&#21644;&#19968;&#20010;&#28909;&#33298;&#36866;&#24230;&#20248;&#21270;&#38382;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35828;&#65292;&#31283;&#23450;&#22320;&#36798;&#21040;&#26356;&#22909;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#32780;&#29616;&#26377;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#25110;&#25910;&#25947;&#24615;&#19978;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of preferential Bayesian optimization (BO), where we aim to optimize a black-box function with only preference feedback over a pair of candidate solutions. Inspired by the likelihood ratio idea, we construct a confidence set of the black-box function using only the preference feedback. An optimistic algorithm with an efficient computational method is then developed to solve the problem, which enjoys an information-theoretic bound on the cumulative regret, a first-of-its-kind for preferential BO. This bound further allows us to design a scheme to report an estimated best solution, with a guaranteed convergence rate. Experimental results on sampled instances from Gaussian processes, standard test functions, and a thermal comfort optimization problem all show that our method stably achieves better or competitive performance as compared to the existing state-of-the-art heuristics, which, however, do not have theoretical guarantees on regret bounds or convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#30495;&#30340;&#26377;&#21161;&#20110;&#21152;&#36895;&#22312;&#20998;&#23376;&#31354;&#38388;&#20013;&#30340;&#27491;&#35268;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#26631;&#20934;&#20294;&#27491;&#35268;&#30340;BO&#26367;&#20195;&#27169;&#22411;&#30340;&#22266;&#23450;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#21033;&#29992;&#21442;&#25968;&#25928;&#33021;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05015</link><description>&lt;p&gt;
&#23545;&#20110;&#26448;&#26009;&#21457;&#29616;&#26469;&#35828;&#65292;&#23545;LLM&#30340;&#25308;&#21344;&#24237;&#20248;&#21270;&#26159;&#21542;&#30495;&#30340;&#26377;&#21033;&#65311;&#19968;&#20010;&#20919;&#38745;&#30340;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#30495;&#30340;&#26377;&#21161;&#20110;&#21152;&#36895;&#22312;&#20998;&#23376;&#31354;&#38388;&#20013;&#30340;&#27491;&#35268;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#26631;&#20934;&#20294;&#27491;&#35268;&#30340;BO&#26367;&#20195;&#27169;&#22411;&#30340;&#22266;&#23450;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#21033;&#29992;&#21442;&#25968;&#25928;&#33021;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26159;&#24403;&#20195;&#26448;&#26009;&#21457;&#29616;&#30340;&#37325;&#35201;&#22522;&#30707;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#36825;&#31181;&#24037;&#20316;&#27969;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#20351;&#31185;&#23398;&#23478;&#33021;&#22815;&#23558;&#20808;&#21069;&#30340;&#39046;&#22495;&#30693;&#35782;&#24212;&#29992;&#21040;&#23545;&#22823;&#35268;&#27169;&#20998;&#23376;&#31354;&#38388;&#30340;&#39640;&#25928;&#25506;&#32034;&#20013;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#20808;&#21069;&#30693;&#35782;&#21487;&#20197;&#37319;&#29992;&#22810;&#31181;&#24418;&#24335;&#65292;&#20294;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25152;&#21253;&#21547;&#30340;&#36741;&#21161;&#31185;&#23398;&#30693;&#35782;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#36720;&#21160;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#30740;&#31350;&#20165;&#25506;&#32034;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#26448;&#26009;&#25628;&#32034;&#30340;LLMs&#12290;&#23454;&#38469;&#19978;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#20174;&#28857;&#20272;&#35745;&#30340;&#38750;&#36125;&#21494;&#26031;LLMs&#20013;&#33719;&#24471;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#26159;BO&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#30495;&#30340;&#26377;&#21161;&#20110;&#21152;&#36895;&#22312;&#20998;&#23376;&#31354;&#38388;&#20013;&#30340;&#27491;&#35268;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#37319;&#21462;&#20102;&#20919;&#38745;&#12289;&#23458;&#35266;&#30340;&#31435;&#22330;&#12290;&#36825;&#26159;&#36890;&#36807;&#20180;&#32454;&#22320;&#65288;i&#65289;&#23558;LLMs&#35270;&#20026;&#26631;&#20934;&#20294;&#27491;&#35268;&#30340;BO&#26367;&#20195;&#27169;&#22411;&#30340;&#22266;&#23450;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#21033;&#29992;&#21442;&#25968;&#25928;&#33021;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-effic
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#22312;&#22270;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#20960;&#20309;&#32467;&#26500;&#30340;&#22270;&#19978;&#27010;&#29575;&#27979;&#24230;&#20256;&#36755;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36229;&#21147; Wassestein&#65288;OW&#65289;&#30340;&#27010;&#24565;&#65292;&#20026;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2402.04516</link><description>&lt;p&gt;
&#22270;&#19978;&#27010;&#29575;&#27979;&#24230;&#30340;&#24191;&#20041; Sobolev &#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Generalized Sobolev Transport for Probability Measures on a Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04516
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#22312;&#22270;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#20960;&#20309;&#32467;&#26500;&#30340;&#22270;&#19978;&#27010;&#29575;&#27979;&#24230;&#20256;&#36755;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36229;&#21147; Wassestein&#65288;OW&#65289;&#30340;&#27010;&#24565;&#65292;&#20026;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#22312;&#22270;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#30340;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;Le &#31561;&#20154;&#65288;2022&#65289;&#21033;&#29992;&#22270;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181; OT &#30340;&#21464;&#20307;&#65292;&#31216;&#20026; Sobolev &#20256;&#36755;&#65288;ST&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#38381;&#24335;&#34920;&#36798;&#24335;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;ST &#30340;&#23450;&#20041;&#20013;&#23454;&#36136;&#19978;&#19982; $L^p$ &#20960;&#20309;&#32467;&#26500;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#36825;&#20351;&#24471;&#22312;&#20854;&#20182;&#20808;&#39564;&#32467;&#26500;&#20013;&#21033;&#29992; ST &#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#32463;&#20856;&#30340; OT &#36890;&#36807;&#20462;&#25913;&#24213;&#23618;&#25104;&#26412;&#20989;&#25968;&#20855;&#26377;&#36866;&#24212;&#21508;&#31181;&#20960;&#20309;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#20363;&#23376;&#26159;&#36229;&#21147; Wassestein&#65288;OW&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;\emph{Orlicz &#20960;&#20309;&#32467;&#26500;}&#36229;&#36234;&#20102; $L^p$ &#32467;&#26500;&#12290;&#19982;&#20351;&#29992;&#26631;&#20934; $p$-&#38454; Wassestein &#30456;&#27604;&#65292;OW &#26174;&#33879;&#25552;&#39640;&#20102;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20004;&#23618;&#20248;&#21270; formulation&#65292;OW &#22312;&#20854;&#35745;&#31639;&#19978;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31867;&#29305;&#23450;&#30340;&#20984;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the optimal transport (OT) problem for measures supported on a graph metric space. Recently, Le et al. (2022) leverage the graph structure and propose a variant of OT, namely Sobolev transport (ST), which yields a closed-form expression for a fast computation. However, ST is essentially coupled with the $L^p$ geometric structure within its definition which makes it nontrivial to utilize ST for other prior structures. In contrast, the classic OT has the flexibility to adapt to various geometric structures by modifying the underlying cost function. An important instance is the Orlicz-Wasserstein (OW) which moves beyond the $L^p$ structure by leveraging the \emph{Orlicz geometric structure}. Comparing to the usage of standard $p$-order Wasserstein, OW remarkably helps to advance certain machine learning approaches. Nevertheless, OW brings up a new challenge on its computation due to its two-level optimization formulation. In this work, we leverage a specific class of convex funct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#22312;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20016;&#23500;&#30340;&#28304;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04416</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#22312;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20016;&#23500;&#30340;&#28304;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#25110;&#22810;&#20010;&#28304;&#39046;&#22495;&#22312;&#20849;&#20139;&#26631;&#31614;&#31354;&#38388;&#30340;&#20551;&#35774;&#19979;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#27979;&#35797;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DG&#26041;&#27861;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20016;&#23500;&#30340;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#20013;&#30340;&#28304;&#25968;&#25454;&#65292;&#36825;&#20010;&#35201;&#27714;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#22826;&#36807;&#20005;&#26684;&#65292;&#22240;&#20026;&#33719;&#21462;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#21516;&#30340;&#26631;&#31614;&#31354;&#38388;&#36153;&#29992;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22788;&#29702;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;(UDG)&#38382;&#39064;&#30340;&#22810;&#27169;&#24577;&#29256;&#26412;&#65292;&#35813;&#38382;&#39064;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#26410;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;LAION-2B&#22312;&#24494;&#35843;&#26399;&#38388;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#26174;&#24335;&#22320;&#20551;&#35774;&#28304;&#25968;&#25454;&#38598;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20219;&#20309;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#23427;&#21482;&#20381;&#36182;&#20110;&#28304;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#32852;&#21512;&#35270;&#35273;-&#35821;&#35328;&#31354;&#38388;&#20013;&#39640;&#25928;&#25628;&#32034;&#30340;&#21069;&#25552;&#12290;&#38024;&#23545;&#36825;&#31181;&#22810;&#27169;&#24577;UDG&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#65288;&#23567;&#20110;100K&#65289;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) is an important problem that learns a model that can generalize to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (UDG) problem, which uses a large task-agnostic unlabeled source dataset, such as LAION-2B during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be efficiently searched in a joint vision-language space. For this multimodal UDG setting, we propose a novel method to build a small ($&lt;$100K) subset of the source data in th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;PAC-Bayesian&#20803;&#23398;&#20064;&#65292;&#20801;&#35768;&#26356;&#28789;&#27963;&#30340;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#20379;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21487;&#36866;&#29992;&#20110;&#20998;&#26512;&#21644;&#35774;&#35745;&#21508;&#31181;&#20803;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25913;&#21892;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04054</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;PAC-Bayesian&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;PAC-Bayesian&#20803;&#23398;&#20064;&#65292;&#20801;&#35768;&#26356;&#28789;&#27963;&#30340;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#20379;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21487;&#36866;&#29992;&#20110;&#20998;&#26512;&#21644;&#35774;&#35745;&#21508;&#31181;&#20803;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25913;&#21892;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#30740;&#31350;&#20803;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#20854;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#23427;&#20801;&#35768;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#20013;&#26356;&#20855;&#28789;&#27963;&#24615;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#21482;&#33021;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#30340;&#20808;&#39564;&#20998;&#24067;&#38388;&#25509;&#21457;&#29983;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#26356;&#30452;&#25509;&#22320;&#34920;&#36798;&#20102;&#20803;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#21363;&#23398;&#20064;&#36866;&#29992;&#20110;&#23558;&#26469;&#20219;&#21153;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#20351;&#20854;&#36866;&#29992;&#20110;&#20998;&#26512;&#21508;&#31181;&#20803;&#23398;&#20064;&#26426;&#21046;&#29978;&#33267;&#35774;&#35745;&#26032;&#30340;&#26426;&#21046;&#12290;&#38500;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23454;&#38469;&#20803;&#23398;&#20064;&#26426;&#21046;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#39640;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new framework for studying meta-learning methods using PAC-Bayesian theory. Its main advantage over previous work is that it allows for more flexibility in how the transfer of knowledge between tasks is realized. For previous approaches, this could only happen indirectly, by means of learning prior distributions over models. In contrast, the new generalization bounds that we prove express the process of meta-learning much more directly as learning the learning algorithm that should be used for future tasks. The flexibility of our framework makes it suitable to analyze a wide range of meta-learning mechanisms and even design new mechanisms. Other than our theoretical contributions we also show empirically that our framework improves the prediction quality in practical meta-learning mechanisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03664</link><description>&lt;p&gt;
&#39640;&#25928;&#27714;&#35299;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficient Solvers for Partial Gromov-Wasserstein
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24046;Gromov-Wasserstein&#65288;PGW&#65289;&#38382;&#39064;&#21487;&#20197;&#27604;&#36739;&#20855;&#26377;&#19981;&#22343;&#21248;&#36136;&#37327;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#21644;&#37096;&#20998;&#21305;&#37197;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;Gromov-Wasserstein&#38382;&#39064;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31867;&#20284;&#20110;&#25226;&#20559;&#24046;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#12290;&#36825;&#20010;&#36716;&#21270;&#23548;&#33268;&#20102;&#20004;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#65292;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#65292;&#25968;&#23398;&#21644;&#35745;&#31639;&#19978;&#31561;&#20215;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;PGW&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#22312;&#24418;&#29366;&#21305;&#37197;&#21644;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces. In this paper, we demonstrate that the PGW problem can be transformed into a variant of the Gromov-Wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem. This transformation leads to two new solvers, mathematically and computationally equivalent, based on the Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We further establish that the PGW problem constitutes a metric for metric measure spaces. Finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines.
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03008</link><description>&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Diffusive Gibbs Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03008
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#28151;&#21512;&#19981;&#36275;&#26041;&#38754;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#65288;DiGS&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#37319;&#26679;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;DiGS&#38598;&#25104;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21033;&#29992;&#39640;&#26031;&#21367;&#31215;&#21019;&#24314;&#19968;&#20010;&#36741;&#21161;&#22122;&#22768;&#20998;&#24067;&#65292;&#20197;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#36830;&#25509;&#23396;&#31435;&#30340;&#27169;&#24577;&#65292;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#20174;&#20004;&#20010;&#31354;&#38388;&#20013;&#20132;&#26367;&#25277;&#21462;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37319;&#26679;&#22810;&#27169;&#24577;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#24182;&#34892;&#28201;&#24230;&#27861;&#31561;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.
&lt;/p&gt;</description></item><item><title>InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.02933</link><description>&lt;p&gt;
InterpretCC: &#36866;&#20110;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02933
&lt;/p&gt;
&lt;p&gt;
InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#30495;&#23454;&#19990;&#30028;&#35299;&#37322;&#24615;&#22312;&#19977;&#20010;&#26041;&#38754;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65306;1&#65289;&#38656;&#35201;&#20154;&#31867;&#20449;&#20219;&#35299;&#37322;&#30340;&#36817;&#20284;&#65288;&#20363;&#22914;&#20107;&#21518;&#26041;&#27861;&#65289;&#65307;2&#65289;&#21066;&#24369;&#20102;&#35299;&#37322;&#30340;&#21487;&#29702;&#35299;&#24615;&#65288;&#20363;&#22914;&#33258;&#21160;&#35782;&#21035;&#30340;&#29305;&#24449;&#25513;&#30721;&#65289;&#65307;3&#65289;&#21066;&#24369;&#20102;&#27169;&#22411;&#24615;&#33021;&#65288;&#20363;&#22914;&#20915;&#31574;&#26641;&#65289;&#12290;&#36825;&#20123;&#32570;&#28857;&#23545;&#20110;&#38754;&#21521;&#20154;&#31867;&#30340;&#39046;&#22495;&#65288;&#22914;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#25110;&#33258;&#28982;&#35821;&#35328;&#65289;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#36825;&#20123;&#39046;&#22495;&#38656;&#35201;&#21487;&#20449;&#30340;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InterpretCC&#65288;&#21487;&#35299;&#37322;&#30340;&#26465;&#20214;&#35745;&#31639;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20043;&#21069;&#33258;&#36866;&#24212;&#21644;&#31232;&#30095;&#22320;&#28608;&#27963;&#29305;&#24449;&#65292;&#30830;&#20445;&#20154;&#31867;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24605;&#24819;&#25193;&#23637;&#20026;&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#20801;&#35768;&#20154;&#20204;&#31163;&#25955;&#22320;&#25351;&#23450;&#20852;&#36259;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees). These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions. In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction. We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separate
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02464</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20540;&#21315;&#35328;&#65306;&#20351;&#29992;&#32431;Transformer&#23558;&#22270;&#24418;&#27431;&#25289;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#24314;&#27169;&#20026;&#32431;&#35821;&#35328;&#29978;&#33267;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#22266;&#26377;&#20449;&#24687;&#65311;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#19968;&#30452;&#26159;&#22270;&#24418;&#24314;&#27169;&#20013;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;GNN&#21644;Graphformer&#21162;&#21147;&#23558;&#22270;&#24418;&#32534;&#30721;&#20026;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#20294;&#20174;&#21521;&#37327;&#20013;&#24674;&#22797;&#20986;&#21407;&#22987;&#22270;&#24418;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GraphsGPT&#65292;&#23427;&#20855;&#26377;&#19968;&#20010;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#22270;&#24418;&#21333;&#35789;&#30340;Graph2Seq&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#20174;&#22270;&#24418;&#21333;&#35789;&#37325;&#26500;&#21407;&#22987;&#22270;&#24418;&#20197;&#30830;&#20445;&#20449;&#24687;&#31561;&#20215;&#24615;&#30340;GraphGPT&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;100M&#20010;&#20998;&#23376;&#19978;&#39044;&#35757;&#32451;&#20102;GraphsGPT&#65292;&#24182;&#24471;&#21040;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;(1) &#39044;&#35757;&#32451;&#30340;Graph2Seq&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;8/9&#20010;&#22270;&#24418;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;(2) &#39044;&#35757;&#32451;&#30340;GraphGPT&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#29983;&#25104;&#22120;&#65292;&#20854;&#33021;&#22815;&#36827;&#34892;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#30340;&#22270;&#24418;&#29983;&#25104;&#12290;(3) Graph2Seq+Gr
&lt;/p&gt;
&lt;p&gt;
Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+Gr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.02244</link><description>&lt;p&gt;
&#36229;&#36234;&#26497;&#38480;&#65306;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#24322;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#29702;&#35299;&#19978;&#19979;&#25991;&#12289;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#21644;&#29983;&#25104;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#20005;&#26684;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#20026;&#20195;&#20215;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#25903;&#25345;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26368;&#36817;&#20026;&#25193;&#23637;LLMs&#24207;&#21015;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#23545;&#38271;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22238;&#39038;&#21644;&#20998;&#31867;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#20462;&#25913;&#20301;&#32622;&#32534;&#30721;&#21644;&#20462;&#25913;&#27880;&#24847;&#26426;&#21046;&#31561;&#26550;&#26500;&#20462;&#25913;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#26356;&#38271;&#24207;&#21015;&#30340;&#22788;&#29702;&#65292;&#21516;&#26102;&#36991;&#20813;&#35745;&#31639;&#38656;&#27714;&#30340;&#25104;&#27604;&#20363;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#30340;&#22810;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;LLMs&#30340;&#19981;&#21516;&#38454;&#27573;&#65288;&#21363;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#65289;&#20013;&#21033;&#29992;&#12290;&#36825;&#20351;&#24471;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#24182;&#25552;&#21319;&#23545;&#38271;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to effic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32763;&#35793;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#35299;&#37322;&#23376;&#22270;&#30340;&#20195;&#29702;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19982;&#21487;&#35299;&#37322;&#23376;&#22270;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02036</link><description>&lt;p&gt;
&#29992;&#20998;&#24067;&#24335;&#20195;&#29702;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Interpreting Graph Neural Networks with In-Distributed Proxies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32763;&#35793;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#35299;&#37322;&#23376;&#22270;&#30340;&#20195;&#29702;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19982;&#21487;&#35299;&#37322;&#23376;&#22270;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#22270;&#25968;&#25454;&#22788;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#20851;&#38190;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#37096;&#32626;GNN&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#38656;&#35201;&#29992;&#25143;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#33021;&#22815;&#35299;&#37322;&#20854;&#21407;&#22240;&#12290;&#35299;&#37322;GNN&#30340;&#27969;&#34892;&#33539;&#24335;&#26159;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#19982;&#21407;&#22987;&#22270;&#30340;&#26631;&#31614;&#26469;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#23376;&#22270;&#12290;&#30001;&#20110;&#35757;&#32451;&#38598;&#20013;&#21407;&#22987;&#22270;&#19982;&#21487;&#35299;&#37322;&#23376;&#22270;&#38598;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23548;&#33268;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#23376;&#22270;&#30340;&#26631;&#31614;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30456;&#31526;&#30340;&#21487;&#35299;&#37322;&#23376;&#22270;&#30340;&#20195;&#29702;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20351;&#29992;&#22270;&#29983;&#25104;&#22120;&#29983;&#25104;&#20195;&#29702;&#22270;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#22522;&#20110;&#20449;&#24687;&#35770;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#22270;&#19981;&#20165;&#36981;&#24490;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#32780;&#19988;&#20415;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become a building block in graph data processing, with wide applications in critical domains. The growing needs to deploy GNNs in high-stakes applications necessitate explainability for users in the decision-making processes. A popular paradigm for the explainability of GNNs is to identify explainable subgraphs by comparing their labels with the ones of original graphs. This task is challenging due to the substantial distributional shift from the original graphs in the training set to the set of explainable subgraphs, which prevents accurate prediction of labels with the subgraphs. To address it, in this paper, we propose a novel method that generates proxy graphs for explainable subgraphs that are in the distribution of training data. We introduce a parametric method that employs graph generators to produce proxy graphs. A new training objective based on information theory is designed to ensure that proxy graphs not only adhere to the distribution of 
&lt;/p&gt;</description></item><item><title>&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00809</link><description>&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#30340;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00809
&lt;/p&gt;
&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#20154;&#20204;&#20027;&#35201;&#20851;&#27880;&#22312;&#28041;&#21450;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#25581;&#31034;&#20102;&#35768;&#22810;&#34987;&#24573;&#35270;&#30340;&#24230;&#37327;&#26631;&#20934;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#12289;&#20027;&#21160;&#21644;&#25345;&#32493;&#23398;&#20064;&#20197;&#21450;&#31185;&#23398;&#25968;&#25454;&#65292;&#36825;&#20123;&#26041;&#38754;&#38656;&#35201;&#20851;&#27880;&#12290;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#26159;&#19968;&#26465;&#26377;&#21069;&#26223;&#30340;&#36947;&#36335;&#65292;&#21487;&#20197;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#20248;&#21183;&#12290;&#26412;&#25991;&#35748;&#20026;BDL&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#23427;&#37325;&#26032;&#23457;&#35270;&#20102;BDL&#30340;&#20248;&#21183;&#12289;&#25215;&#35748;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#19968;&#20123;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#30340;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#35752;&#35770;&#38598;&#20013;&#22312;&#21487;&#33021;&#30340;&#26041;&#24335;&#19978;&#65292;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;BDL&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
&lt;/p&gt;</description></item><item><title>InstructRetro&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#27169;&#22411;Retro 48B&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2310.07713</link><description>&lt;p&gt;
InstructRetro: &#26816;&#32034;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07713
&lt;/p&gt;
&lt;p&gt;
InstructRetro&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#27169;&#22411;Retro 48B&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#23545;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#22256;&#24785;&#24230;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26816;&#32034;&#22686;&#24378;LLM&#30340;&#35268;&#27169;&#20173;&#28982;&#26377;&#38480;&#65288;&#22914;Retro&#20855;&#26377;75&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#25351;&#20196;&#35843;&#20248;&#21644;&#38646;&#26679;&#20363;&#27867;&#21270;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Retro 48B&#65292;&#36825;&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#25216;&#26415;&#20174;1.2&#19975;&#20159;&#20010;&#26631;&#35760;&#20013;&#32487;&#32493;&#39044;&#35757;&#32451;&#19968;&#20010;43B&#30340;GPT&#27169;&#22411;&#65292;&#24182;&#20511;&#21161;Retro&#26041;&#27861;&#23558;&#20854;&#25193;&#23637;&#21040;4800&#20159;&#20010;&#21442;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#24471;&#21040;&#30340;&#22522;&#30784;&#27169;&#22411;Retro 48B&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20165;&#20351;&#29992;1.2&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#30340;43B GPT&#27169;&#22411;&#65292;&#19988;&#21482;&#22686;&#21152;&#20102;2.58%&#30340;GPU&#20351;&#29992;&#26102;&#38388;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#25193;&#23637;&#28508;&#21147;&#12290;&#22312;&#23545;Retro&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#21518;&#65292;InstructRetro&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Spe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22343;&#21248;&#21270;&#26041;&#27861;&#30340;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28385;&#36275;&#26799;&#24230;&#20027;&#23548;&#24615;&#36136;&#30340;&#38543;&#26426;&#20989;&#25968;&#65292;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#25552;&#20379;&#20102;&#22686;&#24378;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26080;&#38656;&#31435;&#26041;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2308.10630</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#22343;&#21248;&#21270;&#26041;&#27861;&#30340;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Homogenization Approach for Gradient-Dominated Stochastic Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22343;&#21248;&#21270;&#26041;&#27861;&#30340;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28385;&#36275;&#26799;&#24230;&#20027;&#23548;&#24615;&#36136;&#30340;&#38543;&#26426;&#20989;&#25968;&#65292;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#25552;&#20379;&#20102;&#22686;&#24378;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26080;&#38656;&#31435;&#26041;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#20027;&#23548;&#24615;&#36136;&#26159;&#19968;&#31181;&#27604;&#24378;&#20984;&#24615;&#26465;&#20214;&#26356;&#24369;&#20294;&#36275;&#20197;&#30830;&#20445;&#20840;&#23616;&#25910;&#25947;&#30340;&#26465;&#20214;&#65292;&#21363;&#20351;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#20063;&#21487;&#20197;&#24212;&#29992;&#24191;&#27867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#22343;&#21248;&#21270;&#26041;&#27861;&#30340;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20108;&#38454;&#19979;&#38477;&#26041;&#27861;&#65288;SHSODM&#65289;&#65292;&#29992;&#20110;&#28385;&#36275;&#26799;&#24230;&#20027;&#23548;&#24615;&#36136;&#30340;&#38543;&#26426;&#20989;&#25968;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#22686;&#24378;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;SHSODM&#19982;&#20854;&#20182;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20248;&#21270;&#30340;&#20108;&#38454;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#36798;&#21040;&#24050;&#30693;&#30340;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32780;&#26080;&#38656;&#31435;&#26041;&#27491;&#21017;&#21270;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#30001;&#20110;&#22343;&#21248;&#21270;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#27599;&#27425;&#36845;&#20195;&#20013;&#35299;&#26497;&#20540;&#29305;&#24449;&#21521;&#37327;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#29275;&#39039;&#31867;&#22411;&#30340;&#31995;&#32479;&#65292;&#25152;&#20197;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient dominance property is a condition weaker than strong convexity, yet sufficiently ensures global convergence even in non-convex optimization. This property finds wide applications in machine learning, reinforcement learning (RL), and operations management. In this paper, we propose the stochastic homogeneous second-order descent method (SHSODM) for stochastic functions enjoying gradient dominance property based on a recently proposed homogenization approach. Theoretically, we provide its sample complexity analysis, and further present an enhanced result by incorporating variance reduction techniques. Our findings show that SHSODM matches the best-known sample complexity achieved by other second-order methods for gradient-dominated stochastic optimization but without cubic regularization. Empirically, since the homogenization approach only relies on solving extremal eigenvector problem at each iteration instead of Newton-type system, our methods gain the advantage of cheaper com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15113</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#23454;&#29616;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data. (arXiv:2401.15113v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#23545;&#20110;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#36807;&#31243;&#21463;&#21040;&#20912;&#24029;&#22810;&#26679;&#24615;&#12289;&#38590;&#20197;&#20998;&#31867;&#30340;&#30862;&#30707;&#21644;&#22823;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Glacier-VisionTransformer-U-Net (GlaViTU)&#65292;&#19968;&#20010;&#21367;&#31215;-Transformer&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#31181;&#21033;&#29992;&#24320;&#25918;&#21355;&#26143;&#24433;&#20687;&#36827;&#34892;&#22810;&#26102;&#30456;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#31574;&#30053;&#12290;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#36328;&#20256;&#24863;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;IoU&#65288;&#20132;&#24182;&#27604;&#65289;&gt; 0.85&#65292;&#24182;&#19988;&#22312;&#20197;&#20912;&#38634;&#20026;&#20027;&#30340;&#22320;&#21306;&#22686;&#21152;&#21040;&#20102;&gt; 0.90&#65292;&#32780;&#22312;&#39640;&#23665;&#20122;&#27954;&#31561;&#30862;&#30707;&#20016;&#23500;&#30340;&#21306;&#22495;&#21017;&#38477;&#33267;&gt; 0.75&#12290;&#27492;&#22806;&#65292;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#21363;&#22238;&#27874;&#21644;&#24178;&#28041;&#30456;&#24178;&#24230;&#65292;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#21487;&#29992;&#22320;&#21306;&#30340;&#20934;&#30830;&#24615;&#12290;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#20351;&#39044;&#27979;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate global glacier mapping is critical for understanding climate change impacts. It is challenged by glacier diversity, difficult-to-classify debris and big data processing. Here we propose Glacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep learning model, and five strategies for multitemporal global-scale glacier mapping using open satellite imagery. Assessing the spatial, temporal and cross-sensor generalisation shows that our best strategy achieves intersection over union &gt;0.85 on previously unobserved images in most cases, which drops to &gt;0.75 for debris-rich areas such as High-Mountain Asia and increases to &gt;0.90 for regions dominated by clean ice. Additionally, adding synthetic aperture radar data, namely, backscatter and interferometric coherence, increases the accuracy in all regions where available. The calibrated confidence for glacier extents is reported making the predictions more reliable and interpretable. We also release a benchmark dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39044;&#23450;&#20041;&#22270;&#21644;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10155</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A novel hybrid time-varying graph neural network for traffic flow forecasting. (arXiv:2401.10155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39044;&#23450;&#20041;&#22270;&#21644;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20934;&#30830;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#30830;&#20445;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39640;&#25928;&#36816;&#34892;&#30340;&#22522;&#30784;&#12290;&#22312;&#29616;&#26377;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#22270;&#26469;&#25551;&#36848;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#19981;&#21516;&#20132;&#36890;&#33410;&#28857;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#39044;&#23450;&#20041;&#22270;&#25551;&#36848;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#21463;&#38480;&#20110;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#22270;&#29983;&#25104;&#26041;&#27861;&#12290;&#23613;&#31649;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30340;&#26102;&#21464;&#22270;&#33021;&#37096;&#20998;&#20811;&#26381;&#39044;&#23450;&#20041;&#22270;&#30340;&#32570;&#28857;&#65292;&#20294;&#29616;&#26377;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#26377;&#38480;&#12290;&#20363;&#22914;&#65292;&#26102;&#21464;&#22270;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time and accurate traffic flow prediction is the foundation for ensuring the efficient operation of intelligent transportation systems.In existing traffic flow prediction methods based on graph neural networks (GNNs), pre-defined graphs were usually used to describe the spatial correlations of different traffic nodes in urban road networks. However, the ability of pre-defined graphs used to describe spatial correlation was limited by prior knowledge and graph generation methods. Although time-varying graphs based on data-driven learning can partially overcome the drawbacks of pre-defined graphs, the learning ability of existing adaptive graphs was limited. For example, time-varying graphs cannot adequately capture the inherent spatial correlations in traffic flow data.In order to solve these problems, we have proposed a hybrid time-varying graph neural network (HTVGNN) for traffic flow prediction.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#65292;&#26500;&#24314;&#21512;&#25104;&#32593;&#32476;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25509;&#36817;&#12290;&#36825;&#25552;&#20379;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#21512;&#25104;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.04133</link><description>&lt;p&gt;
SynHIN: &#29983;&#25104;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI. (arXiv:2401.04133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#65292;&#26500;&#24314;&#21512;&#25104;&#32593;&#32476;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25509;&#36817;&#12290;&#36825;&#25552;&#20379;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#21512;&#25104;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#30528;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#20174;&#26816;&#27979;&#30005;&#23376;&#21830;&#21153;&#22403;&#22334;&#37038;&#20214;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#20849;&#22270;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#30740;&#31350;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#26041;&#38754;&#12290;&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#23545;&#20110;&#20844;&#24179;&#30340;HIN&#27604;&#36739;&#32780;&#35328;&#65292;&#38656;&#35201;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SynHIN&#65292;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;SynHIN&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#65292;&#24635;&#32467;&#22270;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#21512;&#25104;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;In-Cluster&#21644;Out-Cluster Merge&#27169;&#22359;&#20174;&#20027;&#35201;&#30340;&#27169;&#24335;&#38598;&#32676;&#26500;&#24314;&#21512;&#25104;HIN&#12290;&#22312;In/Out-Cluster&#21512;&#24182;&#21644;&#31526;&#21512;&#30495;&#23454;&#25968;&#25454;&#38598;&#32422;&#26463;&#30340;&#21518;&#20462;&#21098;&#36807;&#31243;&#21518;&#65292;&#25105;&#20204;&#30830;&#20445;&#21512;&#25104;&#30340;&#22270;&#32479;&#35745;&#25968;&#25454;&#19982;&#21442;&#32771;&#25968;&#25454;&#25509;&#36817;&#12290;SynHIN&#29983;&#25104;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#21512;&#25104;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20027;&#35201;&#30340;&#27169;&#24335;&#20316;&#20026;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) excel in various domains, from detecting e-commerce spam to social network classification problems. However, the lack of public graph datasets hampers research progress, particularly in heterogeneous information networks (HIN). The demand for datasets for fair HIN comparisons is growing due to advancements in GNN interpretation models. In response, we propose SynHIN, a unique method for generating synthetic heterogeneous information networks. SynHIN identifies motifs in real-world datasets, summarizes graph statistics, and constructs a synthetic network. Our approach utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN from primary motif clusters. After In/Our-Cluster mergers and a post-pruning process fitting the real dataset constraints, we ensure the synthetic graph statistics align closely with the reference one. SynHIN generates a synthetic heterogeneous graph dataset for node classification tasks, using the primary motif as the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.01259</link><description>&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#23616;&#37096;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22522;&#30784;&#23398;&#20064;&#36890;&#36807;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#20351;&#29992;&#20154;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#29420;&#31435;&#20110;&#20854;&#20182;&#27010;&#24565;&#30340;&#32473;&#23450;&#27010;&#24565;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#28872;&#26263;&#31034;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#22312;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#20013;&#19981;&#33021;&#25104;&#31435;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#27010;&#24565;&#26082;&#22312;&#31354;&#38388;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#23436;&#20840;&#30001;&#22266;&#23450;&#23376;&#38598;&#30340;&#29305;&#24449;&#23450;&#20041;&#65289;&#21448;&#22312;&#35821;&#20041;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#20165;&#19982;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#23376;&#38598;&#30340;&#27010;&#24565;&#30456;&#20851;&#32852;&#65289;&#23450;&#20301;&#26102;&#65292;CBMs&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#23616;&#37096;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27010;&#24565;&#20043;&#22806;&#30340;&#29305;&#24449;&#21464;&#21270;&#23545;&#27010;&#24565;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#21253;&#21547;&#20102;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38543;&#26426;&#32593;&#26684;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.00961</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Automated Model Selection for Tabular Data. (arXiv:2401.00961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#21253;&#21547;&#20102;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38543;&#26426;&#32593;&#26684;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#21253;&#21547;&#29420;&#29305;&#19988;&#31163;&#25955;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#23545;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;&#21508;&#19981;&#30456;&#21516;&#12290;&#21333;&#20010;&#29305;&#24449;&#30340;&#32452;&#21512;&#21487;&#33021;&#27604;&#31616;&#21333;&#30340;&#21333;&#20010;&#29305;&#24449;&#36129;&#29486;&#26356;&#20855;&#39044;&#27979;&#24615;&#21644;&#24847;&#20041;&#12290;R&#30340;&#28151;&#21512;&#25928;&#24212;&#32447;&#24615;&#27169;&#22411;&#24211;&#20801;&#35768;&#29992;&#25143;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#25552;&#20379;&#36825;&#31181;&#20132;&#20114;&#24335;&#29305;&#24449;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#26377;&#35768;&#22810;&#29305;&#24449;&#21644;&#21487;&#33021;&#30340;&#20132;&#20114;&#36873;&#25321;&#65292;&#27169;&#22411;&#36873;&#25321;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#36739;&#23567;&#65292;&#33258;&#21160;&#21270;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#21516;&#26102;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65306;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38543;&#26426;&#32593;&#26684;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#26041;&#27861;&#12290;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#39564;&#27010;&#29575;&#26469;&#24341;&#23548;&#25628;&#32034;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#29305;&#24449;&#32452;&#21512;&#12290;&#36138;&#23146;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#22320;&#28155;&#21152;&#29305;&#24449;&#26500;&#24314;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured data in the form of tabular datasets contain features that are distinct and discrete, with varying individual and relative importances to the target. Combinations of one or more features may be more predictive and meaningful than simple individual feature contributions. R's mixed effect linear models library allows users to provide such interactive feature combinations in the model design. However, given many features and possible interactions to select from, model selection becomes an exponentially difficult task. We aim to automate the model selection process for predictions on tabular datasets incorporating feature interactions while keeping computational costs small. The framework includes two distinct approaches for feature selection: a Priority-based Random Grid Search and a Greedy Search method. The Priority-based approach efficiently explores feature combinations using prior probabilities to guide the search. The Greedy method builds the solution iteratively by addin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#24615;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#26631;&#35760;&#20998;&#21106;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#24494;&#35843;&#26102;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.02380</link><description>&lt;p&gt;
FaultFormer: &#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#36866;&#24212;&#24615;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
FaultFormer: Pretraining Transformers for Adaptable Bearing Fault Classification. (arXiv:2312.02380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#24615;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#26631;&#35760;&#20998;&#21106;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#24494;&#35843;&#26102;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#28040;&#36153;&#30340;&#22686;&#38271;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#26234;&#33021;&#21046;&#36896;&#21644;&#26426;&#22120;&#20581;&#24247;&#30417;&#27979;&#26041;&#38754;&#30340;&#37325;&#35201;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25391;&#21160;&#25968;&#25454;&#25552;&#20379;&#20102;&#20016;&#23500;&#21487;&#38752;&#30340;&#20449;&#24687;&#65292;&#33021;&#22815;&#23545;&#26426;&#22120;&#20581;&#24247;&#21644;&#39044;&#27979;&#24615;&#32500;&#25252;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#36724;&#25215;&#25925;&#38556;&#35782;&#21035;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26694;&#26550;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26631;&#35760;&#20998;&#21106;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#25391;&#21160;&#20449;&#21495;&#30340;&#25513;&#30721;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21450;&#20854;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#12289;&#20219;&#21153;&#36866;&#24212;&#21644;&#25968;&#25454;&#38598;&#36866;&#24212;&#20013;&#30340;&#24212;&#29992;&#12290;&#39044;&#35757;&#32451;&#33021;&#22815;&#25552;&#21319;&#22312;&#31232;&#32570;&#26410;&#35265;&#35757;&#32451;&#26679;&#26412;&#19978;&#30340;10&#31867;&#36724;&#25215;&#20998;&#31867;&#24615;&#33021;&#12290;&#24403;&#22312;&#39044;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#25925;&#38556;&#31867;&#21035;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;Transformer&#27169;&#22411;&#20063;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of global consumption has motivated important applications of deep learning to smart manufacturing and machine health monitoring. In particular, vibration data offers a rich and reliable source to provide meaningful insights into machine health and predictive maintenance. In this work, we present pretraining and fine-tuning frameworks for identifying bearing faults based on transformer models. In particular, we investigate different tokenization and data augmentation strategies to improve performance and reach state of the art accuracies. Furthermore, we demonstrate masked self-supervised pretraining for vibration signals and its application to low-data regimes, task adaptation, and dataset adaptation. Pretraining is able to improve performance on 10-way bearing classification on scarce, unseen training samples. Transformer models also benefit from pretraining when fine-tuning on fault classes outside of the pretraining distribution. Lastly, pretrained transformers are shown
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24102;&#26377;&#22823;&#23398;&#20064;&#29575;&#21644;&#23398;&#20064;&#29575;&#39044;&#28909;&#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#26174;&#31034;&#20986;&#22823;&#22411;&#24377;&#23556;&#25928;&#24212;&#65292;&#23558;&#36845;&#20195;&#26397;&#30528;&#27604;&#26799;&#24230;&#19979;&#38477;&#21457;&#29616;&#30340;&#26356;&#24179;&#32531;&#30340;&#26497;&#23567;&#20540;&#26041;&#21521;&#25512;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.15051</link><description>&lt;p&gt;
&#24102;&#39044;&#28909;&#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#22823;&#22411;&#24377;&#23556;&#27010;&#24565;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Catapults in Momentum Gradient Descent with Warmup: An Empirical Study. (arXiv:2311.15051v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24102;&#26377;&#22823;&#23398;&#20064;&#29575;&#21644;&#23398;&#20064;&#29575;&#39044;&#28909;&#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#26174;&#31034;&#20986;&#22823;&#22411;&#24377;&#23556;&#25928;&#24212;&#65292;&#23558;&#36845;&#20195;&#26397;&#30528;&#27604;&#26799;&#24230;&#19979;&#38477;&#21457;&#29616;&#30340;&#26356;&#24179;&#32531;&#30340;&#26497;&#23567;&#20540;&#26041;&#21521;&#25512;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20854;&#23545;&#35757;&#32451;&#36712;&#36857;&#30340;&#24433;&#21709;&#30340;&#20855;&#20307;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24102;&#26377;&#22823;&#23398;&#20064;&#29575;&#21644;&#23398;&#20064;&#29575;&#39044;&#28909;&#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#26174;&#31034;&#20986;&#22823;&#22411;&#24377;&#23556;&#25928;&#24212;&#65292;&#23558;&#36845;&#20195;&#26397;&#30528;&#27604;&#26799;&#24230;&#19979;&#38477;&#21457;&#29616;&#30340;&#26356;&#24179;&#32531;&#30340;&#26497;&#23567;&#20540;&#26041;&#21521;&#25512;&#36827;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#21644;&#29702;&#35770;&#30452;&#35273;&#65292;&#34920;&#26126;&#22823;&#22411;&#24377;&#23556;&#25928;&#24212;&#26159;&#30001;&#20110;&#21160;&#37327;&#8220;&#25918;&#22823;&#8221;&#20102;&#33258;&#31283;&#23450;&#25928;&#24212;&#65288;Damian&#31561;&#65292;2023&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although gradient descent with momentum is widely used in modern deep learning, a concrete understanding of its effects on the training trajectory still remains elusive. In this work, we empirically show that momentum gradient descent with a large learning rate and learning rate warmup displays large catapults, driving the iterates towards flatter minima than those found by gradient descent. We then provide empirical evidence and theoretical intuition that the large catapult is caused by momentum "amplifying" the self-stabilization effect (Damian et al., 2023).B.1
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25104;&#21592;&#19981;&#21464;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;MIST&#31639;&#27861;&#26377;&#25928;&#38450;&#24481;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#33021;&#22815;&#35782;&#21035;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#23454;&#20363;&#24182;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2311.00919</link><description>&lt;p&gt;
MIST: &#36890;&#36807;&#25104;&#21592;&#19981;&#21464;&#23376;&#31354;&#38388;&#35757;&#32451;&#23545;&#25239;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training. (arXiv:2311.00919v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00919
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25104;&#21592;&#19981;&#21464;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;MIST&#31639;&#27861;&#26377;&#25928;&#38450;&#24481;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#33021;&#22815;&#35782;&#21035;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#23454;&#20363;&#24182;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#65288;MI&#65289;&#25915;&#20987;&#20013;&#65292;&#23545;&#25163;&#35797;&#22270;&#30830;&#23450;&#19968;&#20010;&#23454;&#20363;&#26159;&#21542;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;MI&#25915;&#20987;&#26159;&#22312;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#35757;&#32451;ML&#27169;&#22411;&#26102;&#30340;&#19968;&#20010;&#20027;&#35201;&#38544;&#31169;&#38382;&#39064;&#12290;&#25991;&#29486;&#20013;&#30340;&#22823;&#22810;&#25968;MI&#25915;&#20987;&#21033;&#29992;&#20102;ML&#27169;&#22411;&#34987;&#35757;&#32451;&#24471;&#24456;&#22909;&#20197;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#28857;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#23454;&#20363;&#19978;&#20855;&#26377;&#38750;&#24120;&#20302;&#30340;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#23545;&#25239;MI&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#35797;&#22270;&#20351;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#25311;&#21512;&#31243;&#24230;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#36890;&#24120;&#20250;&#23548;&#33268;&#36739;&#20302;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#35757;&#32451;&#23454;&#20363;&#23545;MI&#25915;&#20987;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#33030;&#24369;&#24615;&#12290;&#22823;&#22810;&#25968;&#23454;&#20363;&#21363;&#20351;&#19981;&#21253;&#21547;&#22312;&#35757;&#32451;&#20013;&#20063;&#20250;&#26377;&#20302;&#30340;&#25439;&#22833;&#12290;&#23545;&#20110;&#36825;&#20123;&#23454;&#20363;&#65292;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#36866;&#24212;&#23427;&#20204;&#32780;&#19981;&#29992;&#25285;&#24515;MI&#25915;&#20987;&#12290;&#26377;&#25928;&#30340;&#38450;&#24481;&#21482;&#38656;&#35201;&#65288;&#21487;&#33021;&#26159;&#38544;&#24335;&#22320;&#65289;&#35782;&#21035;&#20986;&#23481;&#26131;&#21463;&#21040;MI&#25915;&#20987;&#30340;&#23454;&#20363;&#65292;&#24182;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#36825;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Member Inference (MI) attacks, the adversary try to determine whether an instance is used to train a machine learning (ML) model. MI attacks are a major privacy concern when using private data to train ML models. Most MI attacks in the literature take advantage of the fact that ML models are trained to fit the training data well, and thus have very low loss on training instances. Most defenses against MI attacks therefore try to make the model fit the training data less well. Doing so, however, generally results in lower accuracy. We observe that training instances have different degrees of vulnerability to MI attacks. Most instances will have low loss even when not included in training. For these instances, the model can fit them well without concerns of MI attacks. An effective defense only needs to (possibly implicitly) identify instances that are vulnerable to MI attacks and avoids overfitting them. A major challenge is how to achieve such an effect in an efficient training proc
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;AIGC&#26381;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17705</link><description>&lt;p&gt;
&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication. (arXiv:2310.17705v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17705
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;AIGC&#26381;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#36890;&#36807;&#21019;&#24314;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26469;&#28385;&#36275;&#24191;&#22823;&#29992;&#25143;&#32676;&#20307;&#30340;&#38656;&#27714;&#12290;&#38543;&#30528;&#31227;&#21160;&#35774;&#22791;&#30340;&#26222;&#21450;&#21644;&#31227;&#21160;&#27969;&#37327;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#36890;&#36807;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#25552;&#20379;&#23545;&#39640;&#36136;&#37327;AIGC&#26381;&#21153;&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#35775;&#38382;&#24050;&#25104;&#20026;AIGC&#20135;&#21697;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#31283;&#23450;&#30340;&#20449;&#36947;&#12289;&#26377;&#38480;&#30340;&#24102;&#23485;&#36164;&#28304;&#21644;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;&#30340;AIGC&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#35821;&#20041;&#36890;&#20449;&#65288;SemCom&#65289;&#22686;&#24378;&#30340;AIGC&#65288;SemAIGC&#65289;&#29983;&#25104;&#21644;&#20256;&#36755;&#26694;&#26550;&#65292;&#20854;&#20013;&#21482;&#38656;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#30340;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SemAIGC&#22312;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#38598;&#25104;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20869;&#23481;&#29983;&#25104;&#21644;&#28789;&#27963;&#35843;&#25972;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI applications are recently catering to a vast user base by creating diverse and high-quality AI-generated content (AIGC). With the proliferation of mobile devices and rapid growth of mobile traffic, providing ubiquitous access to high-quality AIGC services via wireless communication networks is becoming the future direction for AIGC products. However, it is challenging to provide optimal AIGC services in wireless networks with unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To tackle these challenges, we propose a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be extracted and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion-based models within the semantic encoder and decoder for efficient content generation and flexible adjustment of the computing workload of both tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#33021;&#25214;&#21040;&#24615;&#33021;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.16363</link><description>&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms. (arXiv:2310.16363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#33021;&#25214;&#21040;&#24615;&#33021;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Actor Critic&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#25214;&#21040;&#20102;&#24040;&#22823;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#24456;&#22823;&#30340;&#26102;&#20505;&#12290;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;actor critic&#21644;natural actor critic&#31639;&#27861;&#26469;&#22788;&#29702;&#28041;&#21450;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;C-MDP&#65289;&#65292;&#24182;&#22312;&#38750; i.i.d&#65288;&#39532;&#23572;&#21487;&#22827;&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#38750;&#28176;&#36817;&#20998;&#26512;&#12290;&#25105;&#20204;&#32771;&#34385;&#38271;&#26399;&#24179;&#22343;&#25104;&#26412;&#20934;&#21017;&#65292;&#20854;&#20013;&#30446;&#26631;&#21644;&#32422;&#26463;&#20989;&#25968;&#37117;&#26159;&#26576;&#20123;&#35268;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#36866;&#24403;&#31574;&#30053;&#20381;&#36182;&#30340;&#38271;&#26399;&#24179;&#22343;&#12290;&#25105;&#20204;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#27861;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#20445;&#35777;&#33021;&#25214;&#21040;&#24615;&#33021;&#65288;&#25289;&#26684;&#26391;&#26085;&#65289;&#20989;&#25968;$L(\theta,\gamma)$&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;&#21363;$\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$&#65289;&#65292;&#24182;&#19988;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.5})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost functions. We handle the inequality constraints using the Lagrange multiplier method. We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$) of the performance (Lagrange) function $L(\theta,\gamma)$, with a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ in the case of both C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#26469;&#28304;&#21644;&#32622;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#25913;&#32534;&#21518;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#20010;&#20307;&#26465;&#20214;&#26399;&#26395;&#22270;&#31561;&#26041;&#27861;&#26469;&#27979;&#37327;&#29305;&#24449;&#23545;&#39044;&#27979;&#20998;&#24067;&#30340;&#29109;&#21644;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12842</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#26080;&#20851;&#21464;&#37327;&#37325;&#35201;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic variable importance for predictive uncertainty: an entropy-based approach. (arXiv:2310.12842v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#26469;&#28304;&#21644;&#32622;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#25913;&#32534;&#21518;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#20010;&#20307;&#26465;&#20214;&#26399;&#26395;&#22270;&#31561;&#26041;&#27861;&#26469;&#27979;&#37327;&#29305;&#24449;&#23545;&#39044;&#27979;&#20998;&#24067;&#30340;&#29109;&#21644;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30456;&#20449;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24517;&#39035;&#29702;&#35299;&#23548;&#33268;&#36825;&#20123;&#39044;&#27979;&#30340;&#22240;&#32032;&#12290;&#23545;&#20110;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#19981;&#20165;&#38656;&#35201;&#29702;&#35299;&#39044;&#27979;&#26412;&#36523;&#30340;&#21407;&#22240;&#65292;&#36824;&#35201;&#29702;&#35299;&#27169;&#22411;&#23545;&#36825;&#20123;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#25193;&#23637;&#21040;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27169;&#22411;&#65292;&#24182;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#25193;&#23637;&#26469;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#29305;&#21035;&#26159;&#36890;&#36807;&#25913;&#32534;&#25490;&#21015;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#20010;&#20307;&#26465;&#20214;&#26399;&#26395;&#22270;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#33719;&#24471;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#34913;&#37327;&#29305;&#24449;&#23545;&#39044;&#27979;&#20998;&#24067;&#30340;&#29109;&#21644;&#22522;&#20110;&#35813;&#20998;&#24067;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to trust the predictions of a machine learning algorithm, it is necessary to understand the factors that contribute to those predictions. In the case of probabilistic and uncertainty-aware models, it is necessary to understand not only the reasons for the predictions themselves, but also the model's level of confidence in those predictions. In this paper, we show how existing methods in explainability can be extended to uncertainty-aware models and how such extensions can be used to understand the sources of uncertainty in a model's predictive distribution. In particular, by adapting permutation feature importance, partial dependence plots, and individual conditional expectation plots, we demonstrate that novel insights into model behaviour may be obtained and that these methods can be used to measure the impact of features on both the entropy of the predictive distribution and the log-likelihood of the ground truth labels under that distribution. With experiments using both s
&lt;/p&gt;</description></item><item><title>ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.10692</link><description>&lt;p&gt;
ACES: &#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#30340;&#32534;&#31243;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10692
&lt;/p&gt;
&lt;p&gt;
ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#21644;&#36873;&#25321;&#26032;&#39062;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#22909;&#22855;&#24515;&#12289;&#31185;&#23398;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#12290;&#22312;Python&#32534;&#31243;&#38590;&#39064;&#30340;&#26080;&#38480;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26088;&#22312;&#24314;&#27169;&#21442;&#32771;&#20998;&#24067;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;&#20854;&#20182;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#25163;&#24037;&#32534;&#30721;&#34920;&#31034;&#31354;&#38388;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#30830;&#20248;&#21270;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#23884;&#20837;&#31354;&#38388;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#26377;&#36259;&#21464;&#21270;&#30340;&#24863;&#30693;&#19981;&#31526;&#12290;&#36890;&#36807;ACES&#65288;&#33258;&#25105;&#30446;&#26631;&#20195;&#30721;&#25506;&#32034;&#19982;&#35821;&#20041;&#25551;&#36848;&#31526;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#30452;&#25509;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;&#27599;&#20010;&#38590;&#39064;&#37117;&#26631;&#35760;&#26377;10&#20010;&#32500;&#24230;&#65292;&#27599;&#20010;&#32500;&#24230;&#25429;&#25417;&#20102;&#35299;&#20915;&#23427;&#25152;&#38656;&#30340;&#32534;&#31243;&#25216;&#33021;&#12290;ACES&#29983;&#25104;&#24182;&#36861;&#27714;&#26032;&#39062;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28857;&#38388;&#20114;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#32454;&#20998;&#24067;&#23478;&#26063;&#26469;&#35299;&#20915;&#29616;&#26377;&#20114;&#20449;&#24687;&#20272;&#35745;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#31070;&#32463;&#25209;&#35780;&#23478;&#22312;&#21464;&#20998;&#20272;&#35745;&#22120;&#20013;&#30340;&#34892;&#20026;&#65292;&#20197;&#21450;&#23454;&#39564;&#24322;&#24120;&#20540;&#23545;&#20114;&#20449;&#24687;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#19988;&#38656;&#35201;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10240</link><description>&lt;p&gt;
&#28151;&#21512;&#29289;&#19982;&#31070;&#32463;&#25209;&#35780;&#23478;&#65306;&#20851;&#20110;&#31934;&#32454;&#20998;&#24067;&#30340;&#28857;&#38388;&#20114;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Mixtures and the Neural Critics: On the Pointwise Mutual Information Profiles of Fine Distributions. (arXiv:2310.10240v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28857;&#38388;&#20114;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#32454;&#20998;&#24067;&#23478;&#26063;&#26469;&#35299;&#20915;&#29616;&#26377;&#20114;&#20449;&#24687;&#20272;&#35745;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#31070;&#32463;&#25209;&#35780;&#23478;&#22312;&#21464;&#20998;&#20272;&#35745;&#22120;&#20013;&#30340;&#34892;&#20026;&#65292;&#20197;&#21450;&#23454;&#39564;&#24322;&#24120;&#20540;&#23545;&#20114;&#20449;&#24687;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#19988;&#38656;&#35201;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#20449;&#24687;&#37327;&#21270;&#20102;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#24494;&#20998;&#21516;&#32986;&#19979;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#28857;&#38388;&#20114;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#36825;&#26159;&#20114;&#20449;&#24687;&#30340;&#25512;&#24191;&#24418;&#24335;&#65292;&#20445;&#25345;&#20102;&#36825;&#31181;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#22312;&#35299;&#26512;&#19978;&#25551;&#36848;&#20102;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#20102;&#32454;&#20998;&#24067;&#23478;&#26063;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#36924;&#36817;&#36825;&#31181;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#32454;&#20998;&#24067;&#26469;&#30740;&#31350;&#29616;&#26377;&#20114;&#20449;&#24687;&#20272;&#35745;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#35843;&#26597;&#22312;&#21464;&#20998;&#20272;&#35745;&#22120;&#20013;&#20351;&#29992;&#30340;&#31070;&#32463;&#25209;&#35780;&#23478;&#30340;&#34892;&#20026;&#65292;&#24182;&#20102;&#35299;&#23454;&#39564;&#24322;&#24120;&#20540;&#23545;&#20114;&#20449;&#24687;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#32454;&#20998;&#24067;&#26469;&#33719;&#24471;&#22522;&#20110;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#30340;&#20114;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#21487;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#19988;&#38656;&#35201;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mutual information quantifies the dependence between two random variables and remains invariant under diffeomorphisms. In this paper, we explore the pointwise mutual information profile, an extension of mutual information that maintains this invariance. We analytically describe the profiles of multivariate normal distributions and introduce the family of fine distributions, for which the profile can be accurately approximated using Monte Carlo methods. We then show how fine distributions can be used to study the limitations of existing mutual information estimators, investigate the behavior of neural critics used in variational estimators, and understand the effect of experimental outliers on mutual information estimation. Finally, we show how fine distributions can be used to obtain model-based Bayesian estimates of mutual information, suitable for problems with available domain expertise in which uncertainty quantification is necessary.
&lt;/p&gt;</description></item><item><title>ParFam&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#31526;&#21495;&#20989;&#25968;&#26063;&#23558;&#31163;&#25955;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05537</link><description>&lt;p&gt;
ParFam - &#22522;&#20110;&#36830;&#32493;&#20840;&#23616;&#20248;&#21270;&#30340;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
ParFam -- Symbolic Regression Based on Continuous Global Optimization. (arXiv:2310.05537v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05537
&lt;/p&gt;
&lt;p&gt;
ParFam&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#31526;&#21495;&#20989;&#25968;&#26063;&#23558;&#31163;&#25955;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#38382;&#39064;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#27604;&#22914;&#20174;&#32473;&#23450;&#25968;&#25454;&#20013;&#35782;&#21035;&#29289;&#29702;&#23450;&#24459;&#25110;&#25512;&#23548;&#25551;&#36848;&#37329;&#34701;&#24066;&#22330;&#34892;&#20026;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#30446;&#21069;&#23384;&#22312;&#22810;&#31181;&#35299;&#20915;SR&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38750;&#24120;&#22797;&#26434;&#65292;&#38656;&#35201;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;ParFam&#65292;&#23427;&#21033;&#29992;&#36866;&#21512;&#30340;&#31526;&#21495;&#20989;&#25968;&#30340;&#21442;&#25968;&#21270;&#26063;&#23558;&#31163;&#25955;&#30340;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#35774;&#32622;&#26356;&#21152;&#30452;&#35266;&#12290;&#32467;&#21512;&#24378;&#22823;&#30340;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;SR&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#39640;&#32423;&#30340;&#31639;&#27861;&#65292;&#20363;&#22914;&#28155;&#21152;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#25214;&#21040;&#36866;&#21512;&#30340;&#21442;&#25968;&#21270;&#26063;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually quite complicated and require a lot of hyperparameter tuning and computational resources. In this paper, we present our new method ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a powerful global optimizer, this approach results in an effective method to tackle the problem of SR. Furthermore, it can be easily extended to more advanced algorithms, e.g., by adding a deep neural network to find good-fitting parametric families. We prove the performance of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#19981;&#31934;&#30830;&#28040;&#38500;&#27861;&#30340;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#32473;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;</title><link>http://arxiv.org/abs/2310.04283</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#31934;&#30830;&#28040;&#38500;&#27861;&#22312;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
On the Error-Propagation of Inexact Deflation for Principal Component Analysis. (arXiv:2310.04283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04283
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#19981;&#31934;&#30830;&#28040;&#38500;&#27861;&#30340;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#32473;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#24120;&#29992;&#30340;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;PCA&#26088;&#22312;&#25214;&#21040;&#30001;&#25152;&#35859;&#8220;&#20027;&#25104;&#20998;&#8221;&#25152;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#65292;&#36825;&#20123;&#20027;&#25104;&#20998;&#26368;&#33021;&#35299;&#37322;&#25968;&#25454;&#38598;&#30340;&#26041;&#24046;&#12290;&#28040;&#38500;&#27861;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#36825;&#26679;&#30340;&#23376;&#31354;&#38388;&#65292;&#23427;&#20174;&#26368;&#37325;&#35201;&#30340;&#20027;&#25104;&#20998;&#24320;&#22987;&#39034;&#24207;&#22320;&#25214;&#21040;&#27599;&#20010;&#20027;&#25104;&#20998;&#65292;&#30452;&#21040;&#25214;&#21040;&#36739;&#19981;&#37325;&#35201;&#30340;&#20027;&#25104;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39034;&#24207;&#24615;&#36136;&#65292;&#30001;&#20110;&#19981;&#23436;&#20840;&#20272;&#35745;&#20027;&#25104;&#20998;&#24341;&#20837;&#30340;&#25968;&#20540;&#35823;&#24046; - &#20363;&#22914;&#65292;&#30001;&#20110;&#27492;&#36807;&#31243;&#20013;&#30340;&#25968;&#20540;&#36817;&#20284; - &#20250;&#38543;&#30528;&#28040;&#38500;&#30340;&#36827;&#34892;&#32780;&#20256;&#25773;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#25968;&#23398;&#19978;&#23545;&#19981;&#31934;&#30830;&#28040;&#38500;&#27861;&#30340;&#35823;&#24046;&#20256;&#25773;&#36827;&#34892;&#20102;&#29305;&#24615;&#21270;&#30340;&#24037;&#20316;&#65292;&#36825;&#26159;&#26412;&#25991;&#30340;&#20851;&#38190;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#65306;$ i&#65289;$&#24403;&#29992;&#20110;&#26597;&#25214;&#20027;&#35201;&#29305;&#24449;&#21521;&#37327;&#30340;&#23376;&#20363;&#31243;&#26159;&#27867;&#22411;&#30340;&#26102;&#20505;&#65292;&#20197;&#21450;$ ii&#65289;$
&lt;/p&gt;
&lt;p&gt;
Principal Component Analysis (PCA) is a popular tool in data analysis, especially when the data is high-dimensional. PCA aims to find subspaces, spanned by the so-called \textit{principal components}, that best explain the variance in the dataset. The deflation method is a popular meta-algorithm -used to discover such subspaces -- that sequentially finds individual principal components, starting from the most important one and working its way towards the less important ones. However, due to its sequential nature, the numerical error introduced by not estimating principal components exactly -- e.g., due to numerical approximations through this process -- propagates, as deflation proceeds. To the best of our knowledge, this is the first work that mathematically characterizes the error propagation of the inexact deflation method, and this is the key contribution of this paper. We provide two main results: $i)$ when the sub-routine for finding the leading eigenvector is generic, and $ii)
&lt;/p&gt;</description></item><item><title>DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;</title><link>http://arxiv.org/abs/2310.02027</link><description>&lt;p&gt;
DeepHGCN&#65306;&#26397;&#30528;&#26356;&#28145;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks. (arXiv:2310.02027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02027
&lt;/p&gt;
&lt;p&gt;
DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HGCN&#65289;&#22312;&#25552;&#21462;&#20998;&#23618;&#22270;&#20449;&#24687;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#21452;&#26354;&#25805;&#20316;&#21644;&#38543;&#30528;&#28145;&#24230;&#22686;&#21152;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;HGCN&#21463;&#38480;&#20110;&#27973;&#23618;&#26550;&#26500;&#12290;&#23613;&#31649;&#22312;GCNs&#20013;&#24050;&#32463;&#24212;&#29992;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#20294;&#26159;&#24320;&#21457;&#21452;&#26354;&#27835;&#30103;&#26041;&#27861;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25805;&#20316;&#24517;&#39035;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#20197;&#36866;&#24212;&#21452;&#26354;&#24615;&#36136;&#12290;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DeepHGCN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22823;&#22823;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#25928;&#26524;&#30340;&#28145;&#23618;&#22810;&#23618;HGCN&#26550;&#26500;&#12290;DeepHGCN&#20855;&#26377;&#20004;&#20010;&#28145;&#23618;HGCN&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#65292;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#32447;&#24615;&#26144;&#23556;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26377;&#25928;&#30340;&#21452;&#26354;&#27531;&#24046;&#36830;&#25509;&#21644;&#26435;&#37325;&#21644;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#20419;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic graph convolutional networks (HGCN) have demonstrated significant potential in extracting information from hierarchical graphs. However, existing HGCNs are limited to shallow architectures, due to the expensive hyperbolic operations and the over-smoothing issue as depth increases. Although in GCNs, treatments have been applied to alleviate over-smoothing, developing a hyperbolic therapy presents distinct challenges since operations should be carefully designed to fit the hyperbolic nature. Addressing the above challenges, in this work, we propose DeepHGCN, the first deep multi-layer HGCN architecture with dramatically improved computational efficiency and substantially alleviated over-smoothing effect. DeepHGCN presents two key enablers of deep HGCNs: (1) a novel hyperbolic feature transformation layer that enables fast and accurate linear maps; and (2) Techniques such as hyperbolic residual connections and regularization for both weights and features facilitated by an effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#19982;&#26680;&#26041;&#27861;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#34429;&#28982;&#22312;&#21512;&#25104;&#26550;&#26500;&#20013;&#23637;&#31034;&#20102;&#19968;&#20123;&#20248;&#21183;&#65292;&#22914;&#26356;&#24555;&#30340;&#20248;&#21270;&#21644;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#23454;&#38469;&#30456;&#20851;&#30340;&#26550;&#26500;&#38656;&#35201;&#27604;&#28145;&#24230;&#22823;&#24456;&#22810;&#20493;&#30340;&#23485;&#24230;&#25165;&#33021;&#23454;&#29616;&#36825;&#20123;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00137</link><description>&lt;p&gt;
&#20851;&#20110;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#19982;&#23454;&#36341;&#30340;&#33073;&#33410;
&lt;/p&gt;
&lt;p&gt;
On the Disconnect Between Theory and Practice of Overparametrized Neural Networks. (arXiv:2310.00137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#19982;&#26680;&#26041;&#27861;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#34429;&#28982;&#22312;&#21512;&#25104;&#26550;&#26500;&#20013;&#23637;&#31034;&#20102;&#19968;&#20123;&#20248;&#21183;&#65292;&#22914;&#26356;&#24555;&#30340;&#20248;&#21270;&#21644;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#23454;&#38469;&#30456;&#20851;&#30340;&#26550;&#26500;&#38656;&#35201;&#27604;&#28145;&#24230;&#22823;&#24456;&#22810;&#20493;&#30340;&#23485;&#24230;&#25165;&#33021;&#23454;&#29616;&#36825;&#20123;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#20316;&#20026;&#20998;&#26512;&#22823;&#35268;&#27169;&#12289;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#34892;&#20026;&#30340;&#29702;&#35770;&#26694;&#26550;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#36890;&#36807;&#25509;&#36817;&#26080;&#38480;&#23485;&#24230;&#65292;NNs&#21487;&#20197;&#26377;&#25928;&#22320;&#25910;&#25947;&#21040;&#19968;&#20010;&#20855;&#26377;&#30001;&#31070;&#32463;&#20999;&#32447;&#26680;(NTK)&#29305;&#24449;&#21270;&#30340;&#32447;&#24615;&#27169;&#22411;&#12290;&#36825;&#24314;&#31435;&#20102;NNs&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21518;&#32773;&#26159;&#34987;&#20805;&#20998;&#29702;&#35299;&#30340;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#24050;&#32463;&#20551;&#35774;&#24182;&#22312;&#21512;&#25104;&#26550;&#26500;&#20013;&#20174;&#29702;&#35770;&#19978;&#21644;&#31639;&#27861;&#19978;&#39564;&#35777;&#20102;&#19968;&#20123;&#20248;&#21183;&#12290;&#36825;&#20123;&#20248;&#21183;&#21253;&#25324;&#26356;&#24555;&#30340;&#20248;&#21270;&#12289;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#25913;&#36827;&#30340;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#37327;&#21270;&#21521;&#26680;&#24515;&#39046;&#22495;&#25910;&#25947;&#36895;&#24230;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36825;&#20123;&#20248;&#21183;&#38656;&#35201;&#27604;&#28145;&#24230;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26550;&#26500;&#12290;&#36825;&#20010;&#20551;&#35774;&#24341;&#21457;&#20102;&#23545;&#23454;&#38469;&#30456;&#20851;&#26550;&#26500;&#26159;&#21542;&#34920;&#29616;&#22914;&#39044;&#27979;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
The infinite-width limit of neural networks (NNs) has garnered significant attention as a theoretical framework for analyzing the behavior of large-scale, overparametrized networks. By approaching infinite width, NNs effectively converge to a linear model with features characterized by the neural tangent kernel (NTK). This establishes a connection between NNs and kernel methods, the latter of which are well understood. Based on this link, theoretical benefits and algorithmic improvements have been hypothesized and empirically demonstrated in synthetic architectures. These advantages include faster optimization, reliable uncertainty quantification and improved continual learning. However, current results quantifying the rate of convergence to the kernel regime suggest that exploiting these benefits requires architectures that are orders of magnitude wider than they are deep. This assumption raises concerns that practically relevant architectures do not exhibit behavior as predicted via 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#32593;&#32476;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#31283;&#20581;&#24615;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12289;&#25429;&#25417;&#31283;&#20581;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.08012</link><description>&lt;p&gt;
&#22522;&#20110;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#31283;&#20581;&#24615;&#35780;&#20272;&#30340;&#32508;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling. (arXiv:2308.08012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#32593;&#32476;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#31283;&#20581;&#24615;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12289;&#25429;&#25417;&#31283;&#20581;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#36890;&#24615;&#31283;&#20581;&#24615;&#26159;&#29702;&#35299;&#12289;&#20248;&#21270;&#21644;&#20462;&#22797;&#22797;&#26434;&#32593;&#32476;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#32791;&#26102;&#19988;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#27169;&#25311;&#26469;&#35780;&#20272;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#65292;&#21253;&#25324;&#22312;&#26356;&#19968;&#33324;&#30340;&#36793;&#32536;&#21024;&#38500;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25915;&#20987;&#26354;&#32447;&#25429;&#25417;&#31283;&#20581;&#24615;&#32780;&#19981;&#26159;&#30452;&#25509;&#35757;&#32451;&#31283;&#20581;&#24615;&#65292;&#39044;&#27979;&#20219;&#21153;&#30340;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#39044;&#27979;&#33021;&#21147;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#32593;&#32476;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;(CNN)&#65292;&#35843;&#25972;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#37325;&#26032;&#35774;&#35745;&#25915;&#20987;&#27169;&#24335;&#65292;&#24341;&#20837;&#36866;&#24403;&#30340;&#36807;&#28388;&#35268;&#21017;&#65292;&#24182;&#23558;&#31283;&#20581;&#24615;&#30340;&#20215;&#20540;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#21152;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CNN&#26694;&#26550;&#22312;&#35299;&#20915;&#39640;&#35745;&#31639;&#25361;&#25112;&#26041;&#38754;&#20855;&#26377;&#20840;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectivity robustness, a crucial aspect for understanding, optimizing, and repairing complex networks, has traditionally been evaluated through time-consuming and often impractical simulations. Fortunately, machine learning provides a new avenue for addressing this challenge. However, several key issues remain unresolved, including the performance in more general edge removal scenarios, capturing robustness through attack curves instead of directly training for robustness, scalability of predictive tasks, and transferability of predictive capabilities. In this paper, we address these challenges by designing a convolutional neural networks (CNN) model with spatial pyramid pooling networks (SPP-net), adapting existing evaluation metrics, redesigning the attack modes, introducing appropriate filtering rules, and incorporating the value of robustness as training data. The results demonstrate the thoroughness of the proposed CNN framework in addressing the challenges of high computational
&lt;/p&gt;</description></item><item><title>GPLaSDI&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#23436;&#20840;&#38454;&#25968;&#30340;PDE&#35299;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#25554;&#20540;&#21644;&#35299;&#20915;ODE&#31995;&#32479;&#36827;&#34892;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;ROM&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05882</link><description>&lt;p&gt;
GPLaSDI: &#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder. (arXiv:2308.05882v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05882
&lt;/p&gt;
&lt;p&gt;
GPLaSDI&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#23436;&#20840;&#38454;&#25968;&#30340;PDE&#35299;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#25554;&#20540;&#21644;&#35299;&#20915;ODE&#31995;&#32479;&#36827;&#34892;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;ROM&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#36825;&#23548;&#33268;&#20102;&#20943;&#23569;&#38454;&#25968;&#27169;&#22411;(ROMs)&#30340;&#21457;&#23637;&#65292;&#20854;&#31934;&#30830;&#24615;&#39640;&#20110;&#23436;&#20840;&#38454;&#25968;&#27169;&#22411;(FOMs)&#20294;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#23454;&#29616;&#20102;&#38750;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#30340;&#21019;&#24314;&#65292;&#20363;&#22914;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;(LaSDI)&#12290;LaSDI&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#23436;&#20840;&#38454;&#25968;&#30340;PDE&#35299;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#23398;&#20064;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;ODE&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#20943;&#23569;&#30340;&#28508;&#31354;&#38388;&#20013;&#25554;&#20540;&#21644;&#35299;&#20915;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#36755;&#20837;&#35299;&#30721;&#22120;&#26469;&#36827;&#34892;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;ROM&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;(GP)&#30340;&#26032;&#22411;LaSDI&#26694;&#26550;&#65292;&#29992;&#20110;&#28508;&#31354;&#38388;ODE&#25554;&#20540;&#12290;&#20351;&#29992;GP&#24102;&#26469;&#20004;&#20010;&#37325;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#33021;&#22815;&#37327;&#21270;ROM&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#36825;&#20010;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerically solving partial differential equations (PDEs) can be challenging and computationally expensive. This has led to the development of reduced-order models (ROMs) that are accurate but faster than full order models (FOMs). Recently, machine learning advances have enabled the creation of non-linear projection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI maps full-order PDE solutions to a latent space using autoencoders and learns the system of ODEs governing the latent space dynamics. By interpolating and solving the ODE system in the reduced latent space, fast and accurate ROM predictions can be made by feeding the predicted latent space dynamics into the decoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework that relies on Gaussian process (GP) for latent space ODE interpolations. Using GPs offers two significant advantages. First, it enables the quantification of uncertainty over the ROM predictions. Second, leveraging this predict
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04417</link><description>&lt;p&gt;
&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#20844;&#27491;&#24863;&#30693;&#32852;&#37030;&#26497;&#23567;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Federated Minimax Optimization with Convergence Guarantee. (arXiv:2307.04417v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064; (FL) &#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31649;&#29702;&#29992;&#25143;&#25968;&#25454;&#30340;&#33258;&#30001;&#24230;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#20559;&#21521;&#20110;&#25935;&#24863;&#22240;&#32032;&#35832;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#24102;&#26377;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#20844;&#24179;&#32852;&#37030;&#24179;&#22343;&#27861; (FFALM)&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;FL&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#35757;&#32451;&#30446;&#26631;&#26045;&#21152;&#20102;&#20844;&#24179;&#32422;&#26463;&#65292;&#24182;&#35299;&#20915;&#20102;&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;FFALM&#30340;&#25910;&#25947;&#36895;&#29575;&#30340;&#29702;&#35770;&#19978;&#30028;&#12290;&#36890;&#36807;&#22312;CelebA&#21644;UTKFace&#25968;&#25454;&#38598;&#20013;&#20805;&#20998;&#32771;&#34385;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;FFALM &#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models are biased towards sensitive factors such as race or gender. To tackle this issue, this paper proposes a novel algorithm, fair federated averaging with augmented Lagrangian method (FFALM), designed explicitly to address group fairness issues in FL. Specifically, we impose a fairness constraint on the training objective and solve the minimax reformulation of the constrained optimization problem. Then, we derive the theoretical upper bound for the convergence rate of FFALM. The effectiveness of FFALM in improving fairness is shown empirically on CelebA and UTKFace datasets in the presence of severe statistical heterogeneity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01379</link><description>&lt;p&gt;
&#23558;&#20851;&#27880;&#28857;&#36716;&#31227;&#21040;&#30456;&#20851;&#24615;&#19978;: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#23545;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#29992;&#25143;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20123;&#21551;&#21457;&#24615;&#30340;&#20107;&#23454;&#65292;&#21363;&#22312;&#33258;&#22238;&#24402;&#30340;LLMs&#20013;&#65292;&#20196;&#29260;&#22312;&#21453;&#26144;&#29983;&#25104;&#30340;&#21547;&#20041;&#26041;&#38754;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#21363;&#19968;&#20123;&#20196;&#29260;&#27604;&#20854;&#20182;&#20196;&#29260;&#26356;&#30456;&#20851;&#65288;&#25110;&#26356;&#20855;&#20195;&#34920;&#24615;&#65289;&#65292;&#28982;&#32780;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#25152;&#26377;&#30340;&#20196;&#29260;&#34987;&#31561;&#20540;&#23545;&#24453;&#12290;&#36825;&#26159;&#30001;&#20110;&#35821;&#35328;&#20887;&#20313;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#20851;&#38190;&#35789;&#23601;&#36275;&#20197;&#20256;&#36798;&#19968;&#20010;&#38271;&#21477;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#19981;&#24179;&#31561;&#31216;&#20026;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#20196;&#29260;&#21644;&#21253;&#21547;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#65292;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#34987;&#21516;&#31561;&#25110;&#29978;&#33267;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#24341;&#36215;&#30340;&#36825;&#20123;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;NeuGraspNet&#33021;&#22815;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#20174;&#20219;&#24847;&#35270;&#35282;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2306.07392</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#23398;&#20064;&#20219;&#24847;&#35270;&#35282;&#30340;6DoF&#26426;&#22120;&#20154;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering. (arXiv:2306.07392v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07392
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;NeuGraspNet&#33021;&#22815;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#20174;&#20219;&#24847;&#35270;&#35282;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#22312;&#26234;&#33021;&#36741;&#21161;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#26434;&#20081;&#30340;&#29615;&#22659;&#20013;&#20174;&#20219;&#20309;&#35270;&#35282;&#26377;&#25928;&#22320;&#25235;&#21462;&#23545;&#35937;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#22330;&#26223;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NeuGraspNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;6DoF&#25235;&#21462;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#31070;&#32463;&#20307;&#31215;&#34920;&#31034;&#21644;&#34920;&#38754;&#28210;&#26579;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#20840;&#23616;&#65288;&#22330;&#26223;&#32423;&#21035;&#65289;&#21644;&#23616;&#37096;&#65288;&#25235;&#21462;&#32423;&#21035;&#65289;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#22330;&#26223;&#30340;&#26410;&#35265;&#37096;&#20998;&#65292;&#20063;&#33021;&#26377;&#25928;&#22320;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25235;&#21462;&#37325;&#26032;&#35299;&#37322;&#20026;&#19968;&#20010;&#23616;&#37096;&#30340;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#38382;&#39064;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#21644;&#23545;&#35937;&#34920;&#38754;&#20960;&#20309;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;NeuGraspNet&#22312;&#21333;&#20010;&#35270;&#35282;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#38544;&#24335;&#21644;&#21322;&#38544;&#24335;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation is critical for admitting robotic agents to various application domains, like intelligent assistance. A major challenge therein is the effective 6DoF grasping of objects in cluttered environments from any viewpoint without requiring additional scene exploration. We introduce $\textit{NeuGraspNet}$, a novel method for 6DoF grasp detection that leverages recent advances in neural volumetric representations and surface rendering. Our approach learns both global (scene-level) and local (grasp-level) neural surface representations, enabling effective and fully implicit 6DoF grasp quality prediction, even in unseen parts of the scene. Further, we reinterpret grasping as a local neural surface rendering problem, allowing the model to encode the interaction between the robot's end-effector and the object's surface geometry. NeuGraspNet operates on single viewpoints and can sample grasp candidates in occluded scenes, outperforming existing implicit and semi-implicit baselin
&lt;/p&gt;</description></item><item><title>SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04251</link><description>&lt;p&gt;
&#38543;&#26426;&#22349;&#32553;&#65306;&#22914;&#20309;&#21033;&#29992;&#26799;&#24230;&#22122;&#22768;&#20351;SGD&#21160;&#24577;&#36235;&#21521;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04251
&lt;/p&gt;
&lt;p&gt;
SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#19968;&#20010;&#24378;&#28872;&#38544;&#24335;&#20559;&#22909;&#65292;&#23427;&#23558;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#39537;&#21160;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#29420;&#31435;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#20010;&#20559;&#22909;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#19981;&#21464;&#38598;&#65292;&#25110;&#32773;&#35828;&#26159;SGD&#26410;&#20462;&#25913;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31867;&#19981;&#21464;&#38598;&#65292;&#23427;&#20204;&#23545;&#24212;&#20110;&#29616;&#20195;&#26550;&#26500;&#20013;&#24120;&#35265;&#30340;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SGD&#22312;&#36825;&#20123;&#31616;&#21333;&#19981;&#21464;&#38598;&#26041;&#38754;&#20855;&#26377;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#25439;&#22833;&#26223;&#35266;&#22312;&#19981;&#21464;&#38598;&#21608;&#22260;&#30340;&#26354;&#29575;&#21644;&#38543;&#26426;&#26799;&#24230;&#24341;&#20837;&#30340;&#22122;&#22768;&#20043;&#38388;&#30340;&#31454;&#20105;&#24314;&#31435;&#20102;&#19968;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#22122;&#22768;&#27700;&#24179;&#20250;&#22686;&#24378;&#21560;&#24341;&#21147;&#65292;&#23548;&#33268;&#19982;&#38797;&#28857;&#25110;&#35757;&#32451;&#25439;&#22833;&#30340;&#23616;&#37096;&#26497;&#22823;&#20540;&#30456;&#20851;&#30340;&#21560;&#24341;&#19981;&#21464;&#38598;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
&lt;/p&gt;</description></item><item><title>Vocos&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20613;&#37324;&#21494;&#35889;&#31995;&#25968;&#65292;&#28040;&#38500;&#20102;&#26102;&#22495;&#21644;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#21270;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#22312;&#39640;&#36136;&#37327;&#38899;&#39057;&#21512;&#25104;&#20013;&#30340;&#24046;&#36317;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.00814</link><description>&lt;p&gt;
Vocos&#65306;&#28040;&#38500;&#26102;&#22495;&#21644;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#21270;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#22312;&#39640;&#36136;&#37327;&#38899;&#39057;&#21512;&#25104;&#20013;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis. (arXiv:2306.00814v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00814
&lt;/p&gt;
&lt;p&gt;
Vocos&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20613;&#37324;&#21494;&#35889;&#31995;&#25968;&#65292;&#28040;&#38500;&#20102;&#26102;&#22495;&#21644;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#21270;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#22312;&#39640;&#36136;&#37327;&#38899;&#39057;&#21512;&#25104;&#20013;&#30340;&#24046;&#36317;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#21457;&#23637;&#20027;&#35201;&#30001;&#22312;&#26102;&#22495;&#20013;&#25805;&#20316;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#25512;&#21160;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#26102;&#39057;&#34920;&#31034;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268;&#20887;&#20313;&#21644;&#35745;&#31639;&#23494;&#38598;&#30340;&#19978;&#37319;&#26679;&#25805;&#20316;&#12290;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#26102;&#39057;&#34920;&#31034;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#19982;&#20154;&#30340;&#21548;&#35273;&#24863;&#30693;&#26356;&#21152;&#20934;&#30830;&#65292;&#24182;&#19988;&#36890;&#36807;&#20854;&#35745;&#31639;&#24471;&#21040;&#20102;&#32463;&#36807;&#20805;&#20998;&#39564;&#35777;&#30340;&#24555;&#36895;&#31639;&#27861;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#37325;&#24314;&#22797;&#20540;&#35889;&#22270;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#23384;&#22312;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;Vocos&#65292;&#19968;&#20010;&#30452;&#25509;&#29983;&#25104;&#20613;&#37324;&#21494;&#35889;&#31995;&#25968;&#30340;&#26032;&#27169;&#22411;&#65292;&#26469;&#28040;&#38500;&#36825;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Vocos&#19981;&#20165;&#19982;&#38899;&#39057;&#36136;&#37327;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#36895;&#24230;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22522;&#20110;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#32771;&#34385;&#20102;&#33021;&#28304;&#21644;&#20449;&#21495;&#34928;&#20943;&#31561;&#22240;&#32032;&#65292;&#20026;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.18493</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Insights from the Design Space Exploration of Flow-Guided Nanoscale Localization. (arXiv:2305.18493v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18493
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22522;&#20110;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#32771;&#34385;&#20102;&#33021;&#28304;&#21644;&#20449;&#21495;&#34928;&#20943;&#31561;&#22240;&#32032;&#65292;&#20026;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22826;&#36203;&#20857;&#26080;&#32447;&#36890;&#20449;&#33021;&#21147;&#30340;&#32435;&#31859;&#35774;&#22791;&#20026;&#22312;&#20154;&#31867;&#34880;&#28082;&#20013;&#36827;&#34892;&#27969;&#23548;&#21521;&#23450;&#20301;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#27492;&#31867;&#23450;&#20301;&#20351;&#24471;&#23558;&#25152;&#24863;&#21463;&#21040;&#30340;&#20107;&#20214;&#30340;&#20301;&#32622;&#19982;&#20107;&#20214;&#26412;&#36523;&#36827;&#34892;&#21305;&#37197;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31934;&#20934;&#21307;&#30103;&#26041;&#38754;&#30340;&#26089;&#26399;&#21644;&#31934;&#20934;&#35786;&#26029;&#12289;&#38477;&#20302;&#25104;&#26412;&#21644;&#20405;&#20837;&#24615;&#12290;&#27969;&#23548;&#21521;&#23450;&#20301;&#20173;&#22788;&#20110;&#21407;&#22987;&#38454;&#27573;&#65292;&#21482;&#26377;&#23569;&#25968;&#35770;&#25991;&#28041;&#21450;&#27492;&#38382;&#39064;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#35780;&#20272;&#20173;&#28982;&#20197;&#38750;&#26631;&#20934;&#21270;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#36890;&#24120;&#21482;&#32771;&#34385;&#21333;&#19968;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24573;&#30053;&#20102;&#22312;&#36825;&#31181;&#35268;&#27169;&#65288;&#20363;&#22914;&#65292;&#32435;&#31859;&#22120;&#20214;&#30340;&#33021;&#37327;&#21463;&#38480;&#65289;&#21644;&#23545;&#20110;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#20307;&#20869;&#22826;&#36203;&#20857;&#20256;&#25773;&#30340;&#20005;&#37325;&#34928;&#20943;&#65289;&#19979;&#30456;&#20851;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#35780;&#20272;&#20855;&#26377;&#20302;&#27700;&#24179;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#20197;&#23458;&#35266;&#30340;&#26041;&#24335;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20256;&#36755;&#33021;&#37327;&#28040;&#32791;&#21644;&#20449;&#21495;&#34928;&#20943;&#65292;&#23545;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32771;&#34385;&#20102;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#65288;&#20363;&#22914;&#33021;&#37327;&#28040;&#32791;&#21644;&#23450;&#20301;&#31934;&#24230;&#65289;&#21644;&#25361;&#25112;&#65288;&#20363;&#22914;&#36523;&#20307;&#36816;&#21160;&#21644;&#34880;&#21387;&#65289;&#65292;&#23548;&#33268;&#25105;&#20204;&#21487;&#20197;&#20026;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#25552;&#20379;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nanodevices with Terahertz (THz)-based wireless communication capabilities are providing a primer for flow-guided localization within the human bloodstreams. Such localization is allowing for assigning the locations of sensed events with the events themselves, providing benefits in precision medicine along the lines of early and precise diagnostics, and reduced costs and invasiveness. Flow-guided localization is still in a rudimentary phase, with only a handful of works targeting the problem. Nonetheless, the performance assessments of the proposed solutions are already carried out in a non-standardized way, usually along a single performance metric, and ignoring various aspects that are relevant at such a scale (e.g., nanodevices' limited energy) and for such a challenging environment (e.g., extreme attenuation of in-body THz propagation). As such, these assessments feature low levels of realism and cannot be compared in an objective way. Toward addressing this issue, we account for t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#22312;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#20013;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65292;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#38544;&#24335;&#29305;&#24449;&#36873;&#25321;&#24182;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.16905</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65306;&#36125;&#21494;&#26031;&#25512;&#29702;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference. (arXiv:2305.16905v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#22312;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#20013;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65292;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#38544;&#24335;&#29305;&#24449;&#36873;&#25321;&#24182;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#24615;&#36136;&#38459;&#30861;&#20102;&#35299;&#37322;&#24615;&#12290;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#20998;&#20026;&#21152;&#24615;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#20351;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20132;&#20114;&#21464;&#24471;&#26126;&#26174;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65306;a&#65289;&#23427;&#36890;&#36807;&#20272;&#35745;&#23376;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#19981;&#30830;&#23450;&#24615;&#20026;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65307;b&#65289;&#23427;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#32463;&#39564;&#36125;&#21494;&#26031;&#36807;&#31243;&#25191;&#34892;&#29305;&#24449;&#30340;&#38544;&#24335;&#36873;&#25321;&#65307;c&#65289;&#23427;&#21487;&#29992;&#20110;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#65292;&#20316;&#20026;&#31934;&#32454;&#35843;&#25972;&#30340;&#20132;&#20114;&#27169;&#22411;&#20505;&#36873;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;LA-NAM&#65289;&#25552;&#39640;&#20102;NAM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#23376;&#32593;&#32476;&#30340;&#20132;&#20114;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have found successful applications in many fields, but their black-box nature hinders interpretability. This is addressed by the neural additive model (NAM), in which the network is divided into additive sub-networks, thus making apparent the interaction between input features and predictions. In this paper, we approach the additive structure from a Bayesian perspective and develop a practical Laplace approximation. This enhances interpretability in three primary ways: a) It provides credible intervals for the recovered feature interactions by estimating function-space uncertainty of the sub-networks; b) it yields a tractable estimate of the marginal likelihood, which can be used to perform an implicit selection of features through an empirical Bayes procedure; and c) it can be used to rank feature pairs as candidates for second-order interactions in fine-tuned interaction models. We show empirically that our proposed Laplace-approximated NAM (LA-NAM) improv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16338</link><description>&lt;p&gt;
&#28145;&#24605;&#29087;&#34385;&#65306;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#24050;&#32463;&#23637;&#31034;&#20102;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#20302;&#25928;&#24615;&#28304;&#20110;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#35760;&#24518;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#22240;&#27492;&#65292;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#20808;&#21069;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;LLM&#30340;&#38544;&#24335;&#35760;&#24518;&#26426;&#21046;&#19981;&#21516;&#65292;&#20154;&#33041;&#21033;&#29992;&#20998;&#24067;&#24335;&#23384;&#20648;&#22120;&#23384;&#20648;&#35760;&#24518;&#65292;&#20197;&#26377;&#25928;&#22320;&#31649;&#29702;&#21644;&#32452;&#32455;&#22810;&#31181;&#25216;&#33021;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#26469;&#23384;&#20648;&#12289;&#34701;&#21512;&#21644;&#26816;&#32034;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;Atari&#28216;&#25103;&#21644;&#20803;&#19990;&#30028;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#24494;&#35843;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#38750;&#23545;&#25968;&#20984;&#20998;&#24067;&#36827;&#34892;&#36817;&#20284;&#25277;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807; Langevin Monte Carlo &#31639;&#27861;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;&#20004;&#31181;&#38750;&#20809;&#28369;&#24773;&#20917;&#65292;&#36825;&#20123;&#20219;&#21153;&#28304;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#22270;&#20687;&#21453;&#38382;&#39064;&#12290;&#25968;&#20540;&#27169;&#25311;&#27604;&#36739;&#20102;&#26368;&#24120;&#29992;&#30340; Langevin Monte Carlo &#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15988</link><description>&lt;p&gt;
&#38750;&#23545;&#25968;&#20984;&#21644;&#38750;&#20809;&#28369;&#37319;&#26679;&#30340; Langevin Monte Carlo &#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-Log-Concave and Nonsmooth Sampling via Langevin Monte Carlo Algorithms. (arXiv:2305.15988v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#38750;&#23545;&#25968;&#20984;&#20998;&#24067;&#36827;&#34892;&#36817;&#20284;&#25277;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807; Langevin Monte Carlo &#31639;&#27861;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;&#20004;&#31181;&#38750;&#20809;&#28369;&#24773;&#20917;&#65292;&#36825;&#20123;&#20219;&#21153;&#28304;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#22270;&#20687;&#21453;&#38382;&#39064;&#12290;&#25968;&#20540;&#27169;&#25311;&#27604;&#36739;&#20102;&#26368;&#24120;&#29992;&#30340; Langevin Monte Carlo &#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#38750;&#23545;&#25968;&#20984;&#20998;&#24067;&#65288;&#20363;&#22914;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#65289;&#36827;&#34892;&#36817;&#20284;&#25277;&#26679;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#31163;&#25955;&#36807;&#24230;&#38459;&#23612; Langevin &#25193;&#25955;&#25152;&#23548;&#20986;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#31216;&#20026; Langevin Monte Carlo &#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#38750;&#20809;&#28369;&#24773;&#20917;&#65292;&#20854;&#20013;&#24050;&#32463;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#36817;&#31471; MCMC &#26041;&#27861;&#65306;(i) &#32771;&#34385;&#21040;&#38750;&#20809;&#28369;&#30340;&#20808;&#39564;&#21644;&#39640;&#26031;&#28151;&#21512;&#20284;&#28982;&#65307;(ii) &#25289;&#26222;&#25289;&#26031;&#28151;&#21512;&#20998;&#24067;&#12290;&#36825;&#26679;&#30340;&#38750;&#20809;&#28369;&#21644;&#38750;&#23545;&#25968;&#20984;&#37319;&#26679;&#20219;&#21153;&#28304;&#20110;&#24191;&#27867;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#22270;&#20687;&#21453;&#38382;&#39064;&#65292;&#22914;&#22270;&#20687;&#21453;&#35126;&#31215;&#20013;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#20197;&#27604;&#36739;&#26368;&#24120;&#29992;&#30340; Langevin Monte Carlo &#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of approximate sampling from non-log-concave distributions, e.g., Gaussian mixtures, which is often challenging even in low dimensions due to their multimodality. We focus on performing this task via Markov chain Monte Carlo (MCMC) methods derived from discretizations of the overdamped Langevin diffusions, which are commonly known as Langevin Monte Carlo algorithms. Furthermore, we are also interested in two nonsmooth cases for which a large class of proximal MCMC methods have been developed: (i) a nonsmooth prior is considered with a Gaussian mixture likelihood; (ii) a Laplacian mixture distribution. Such nonsmooth and non-log-concave sampling tasks arise from a wide range of applications to Bayesian inference and imaging inverse problems such as image deconvolution. We perform numerical simulations to compare the performance of most commonly used Langevin Monte Carlo algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;UP5&#65292;&#23427;&#37319;&#29992;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#26469;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.12090</link><description>&lt;p&gt;
UP5: &#38754;&#21521;&#20844;&#24179;&#24615;&#25512;&#33616;&#30340;&#26080;&#20559;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UP5: Unbiased Foundation Model for Fairness-aware Recommendation. (arXiv:2305.12090v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;UP5&#65292;&#23427;&#37319;&#29992;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#26469;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31561;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24050;&#23558;&#23427;&#20204;&#25512;&#21040;&#20102;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#30340;&#21069;&#27839;&#12290;&#27492;&#22806;&#65292;RS&#20013;&#30340;&#20844;&#24179;&#24615;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#35768;&#22810;&#29992;&#25143;&#23558;&#20854;&#29992;&#20110;&#20915;&#31574;&#21644;&#38656;&#27714;&#23653;&#34892;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20844;&#24179;&#24615;&#27700;&#24179;&#21644;&#20844;&#24179;&#22788;&#29702;&#19981;&#21516;&#29992;&#25143;&#32676;&#32452;&#30340;&#36866;&#24403;&#26041;&#27861;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#29992;&#25143;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24443;&#24213;&#26816;&#26597;&#34920;&#26126;&#65292;LLMs&#20013;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#65292;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;&#20026;&#20102;&#28040;&#38500;LLM&#20013;&#30340;&#20559;&#24046;&#20197;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#30340;&#26032;&#22411;&#26080;&#20559;P5&#65288;UP5&#65289;&#22522;&#30784;&#27169;&#22411;&#12290;CFP&#21253;&#25324;&#20004;&#20010;&#23376;&#27169;&#22359;&#65306;&#20010;&#24615;&#21270;&#21069;&#32512;&#25552;&#31034;&#21644;Prompt&#28151;&#21512;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20010;&#20307;&#25935;&#24863;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in foundation models such as large language models (LLM) have propelled them to the forefront of recommender systems (RS). Moreover, fairness in RS is critical since many users apply it for decision-making and demand fulfillment. However, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. In this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in LLMs that lead to unfair recommendation results. To eliminate bias from LLM for fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5) foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a Prompt Mixture that int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\partial$-CROWN&#30340;&#26694;&#26550;&#65292;&#20197;&#20445;&#35777;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20855;&#26377;&#20840;&#23616;&#27491;&#30830;&#24615;&#30340;&#26368;&#22351;&#21097;&#20313;&#35823;&#24046;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#33719;&#24471;&#26377;&#25928;&#35777;&#20070;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10157</link><description>&lt;p&gt;
&#35777;&#26126;&#27491;&#30830;&#24615;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Provably Correct Physics-Informed Neural Networks. (arXiv:2305.10157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\partial$-CROWN&#30340;&#26694;&#26550;&#65292;&#20197;&#20445;&#35777;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20855;&#26377;&#20840;&#23616;&#27491;&#30830;&#24615;&#30340;&#26368;&#22351;&#21097;&#20313;&#35823;&#24046;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#33719;&#24471;&#26377;&#25928;&#35777;&#20070;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#26410;&#33021;&#20445;&#35777;PINN&#22312;&#25972;&#20010;&#26102;&#31354;&#22495;&#20869;&#30340;&#26368;&#22351;&#21097;&#20313;&#35823;&#24046;&#65292;&#36825;&#26159;&#31867;&#20284;&#20110;&#25968;&#23383;&#27714;&#35299;&#22120;&#30340;&#20844;&#24046;&#30340;&#19968;&#31181;&#24230;&#37327;&#65292;&#32780;&#26159;&#38598;&#20013;&#20110;&#22312;&#19968;&#32452;&#36755;&#20837;&#19978;&#36890;&#36807;&#28857;&#23545;&#28857;&#27604;&#36739;&#26469;&#24471;&#21040;&#35299;&#20915;&#26041;&#26696;&#21644;&#27714;&#35299;&#22120;&#24471;&#21040;&#35299;&#20915;&#26041;&#26696;&#30340;&#32467;&#26524;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19981;&#33021;&#35748;&#20026;&#22312;&#19968;&#32452;&#26377;&#38480;&#28857;&#19978;&#30340;&#27979;&#35797;&#23601;&#36275;&#20197;&#20351;&#24471;&#37096;&#32626;&#25104;&#31435;&#65292;&#22240;&#20026;&#22312;&#21478;&#19968;&#32452;&#28857;&#19978;&#24615;&#33021;&#21487;&#33021;&#22823;&#19981;&#30456;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#25972;&#20010;&#36755;&#20837;&#22495;&#30340;PINN&#22522;&#20110;&#20844;&#24046;&#30340;&#27491;&#30830;&#24615;&#26465;&#20214;&#12290;&#20026;&#20102;&#39564;&#35777;&#23427;&#20204;&#30340;&#26377;&#25928;&#31243;&#24230;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;$\partial$-CROWN&#65306;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#39640;&#25928;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#12289;&#21518;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#38480;&#21046;PINN&#30340;&#21097;&#20313;&#35823;&#24046;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23427;&#22312;&#33719;&#24471;&#32039;&#23494;&#35777;&#20070;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work provides promising evidence that Physics-informed neural networks (PINN) can efficiently solve partial differential equations (PDE). However, previous works have failed to provide guarantees on the worst-case residual error of a PINN across the spatio-temporal domain - a measure akin to the tolerance of numerical solvers - focusing instead on point-wise comparisons between their solution and the ones obtained by a solver on a set of inputs. In real-world applications, one cannot consider tests on a finite set of points to be sufficient grounds for deployment, as the performance could be substantially worse on a different set. To alleviate this issue, we establish tolerance-based correctness conditions for PINNs over the entire input domain. To verify the extent to which they hold, we introduce $\partial$-CROWN: a general, efficient and scalable post-training framework to bound PINN residual errors. We demonstrate its effectiveness in obtaining tight certificates by applying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2304.06720</link><description>&lt;p&gt;
&#23500;&#25991;&#26412;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Expressive Text-to-Image Generation with Rich Text. (arXiv:2304.06720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#25991;&#23383;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#27969;&#34892;&#30028;&#38754;&#12290;&#20294;&#26159;&#65292;&#23427;&#30340;&#23450;&#21046;&#36873;&#39033;&#26377;&#38480;&#65292;&#38459;&#30861;&#20102;&#29992;&#25143;&#31934;&#30830;&#25551;&#36848;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25903;&#25345;&#23383;&#20307;&#26679;&#24335;&#12289;&#22823;&#23567;&#12289;&#39068;&#33394;&#21644;&#33050;&#27880;&#31561;&#26684;&#24335;&#30340;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#12290;&#25105;&#20204;&#20174;&#23500;&#25991;&#26412;&#20013;&#25552;&#21462;&#27599;&#20010;&#23383;&#30340;&#23646;&#24615;&#65292;&#20197;&#21551;&#29992;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#36825;&#20123;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#26041;&#27861;&#26356;&#22909;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plain text has become a prevalent interface for text-to-image synthesis. However, its limited customization options hinder users from accurately describing desired outputs. For example, plain text makes it hard to specify continuous quantities, such as the precise RGB color value or importance of each word. Furthermore, creating detailed text prompts for complex scenes is tedious for humans to write and challenging for text encoders to interpret. To address these challenges, we propose using a rich-text editor supporting formats such as font style, size, color, and footnote. We extract each word's attributes from rich text to enable local style control, explicit token reweighting, precise color rendering, and detailed region synthesis. We achieve these capabilities through a region-based diffusion process. We first obtain each word's region based on cross-attention maps of a vanilla diffusion process using plain text. For each region, we enforce its text attributes by creating region-s
&lt;/p&gt;</description></item><item><title>SoftED metrics &#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20107;&#20214;&#26816;&#27979;&#30340;&#26032;&#25351;&#26631;&#65292;&#26082;&#21253;&#25324;&#26102;&#38388;&#30340;&#27010;&#24565;&#65292;&#21448;&#21253;&#25324;&#23545;&#30456;&#37051;&#26816;&#27979;&#30340;&#26102;&#38388;&#23481;&#24525;&#24230;&#65292;&#23427;&#20204;&#33021;&#22815;&#21516;&#26102;&#35780;&#20272;&#20107;&#20214;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#20854;&#26816;&#27979;&#26159;&#21542;&#20195;&#34920;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.00439</link><description>&lt;p&gt;
SoftED: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20107;&#20214;&#26816;&#27979;&#30340;&#36719;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
SoftED: Metrics for Soft Evaluation of Time Series Event Detection. (arXiv:2304.00439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00439
&lt;/p&gt;
&lt;p&gt;
SoftED metrics &#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20107;&#20214;&#26816;&#27979;&#30340;&#26032;&#25351;&#26631;&#65292;&#26082;&#21253;&#25324;&#26102;&#38388;&#30340;&#27010;&#24565;&#65292;&#21448;&#21253;&#25324;&#23545;&#30456;&#37051;&#26816;&#27979;&#30340;&#26102;&#38388;&#23481;&#24525;&#24230;&#65292;&#23427;&#20204;&#33021;&#22815;&#21516;&#26102;&#35780;&#20272;&#20107;&#20214;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#20854;&#26816;&#27979;&#26159;&#21542;&#20195;&#34920;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#26631;&#20934;&#30340;&#20998;&#31867;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#25351;&#26631;&#20165;&#20851;&#27880;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20107;&#20214;&#26816;&#27979;&#30340;&#19981;&#20934;&#30830;&#24448;&#24448;&#26159;&#30001;&#20110;&#21069;&#21518;&#30456;&#20851;&#20107;&#20214;&#22312;&#30456;&#37051;&#26816;&#27979;&#20013;&#30340;&#21453;&#24212;&#20135;&#29983;&#30340;&#12290;&#36825;&#20123;&#26816;&#27979;&#23545;&#20110;&#35302;&#21457;&#24517;&#35201;&#30340;&#34892;&#21160;&#25110;&#24110;&#21161;&#20943;&#36731;&#19981;&#33391;&#21518;&#26524;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#23545;&#20110;&#20107;&#20214;&#26816;&#27979;&#26469;&#35828;&#26159;&#19981;&#20805;&#20998;&#21644;&#19981;&#36866;&#24403;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#25351;&#26631;&#65292;&#26082;&#21253;&#25324;&#26102;&#38388;&#30340;&#27010;&#24565;&#65292;&#21448;&#21253;&#25324;&#23545;&#30456;&#37051;&#26816;&#27979;&#30340;&#26102;&#38388;&#23481;&#24525;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#38598;&#21512;&#8220;SoftED metrics&#8221;&#65292;&#26088;&#22312;&#36719;&#35780;&#20272;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#35780;&#20272;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#20197;&#21450;&#20854;&#26816;&#27979;&#26159;&#21542;&#20195;&#34920;&#20107;&#20214;&#12290;&#36890;&#36807;&#23558;&#20107;&#20214;&#21644;&#20195;&#34920;&#24615;&#26816;&#27979;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;36\%&#20197;&#19978;&#30340;&#23454;&#39564;&#20013;&#21152;&#20837;&#26102;&#38388;&#23481;&#24525;&#24230;&#65292;&#25552;&#39640;&#20102;&#20107;&#20214;&#26816;&#27979;&#30340;&#35780;&#20272;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series event detection methods are evaluated mainly by standard classification metrics that focus solely on detection accuracy. However, inaccuracy in detecting an event can often result from its preceding or delayed effects reflected in neighboring detections. These detections are valuable to trigger necessary actions or help mitigate unwelcome consequences. In this context, current metrics are insufficient and inadequate for the context of event detection. There is a demand for metrics that incorporate both the concept of time and temporal tolerance for neighboring detections. This paper introduces SoftED metrics, a new set of metrics designed for soft evaluating event detection methods. They enable the evaluation of both detection accuracy and the degree to which their detections represent events. They improved event detection evaluation by associating events and their representative detections, incorporating temporal tolerance in over 36\% of experiments compared to the usual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.11789</link><description>&lt;p&gt;
&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65306;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#38543;&#26426;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#35813;&#38382;&#39064;&#20855;&#26377;&#23454;&#26102;&#30340;&#22270;&#19978;&#35266;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25910;&#25947;&#24615;&#36716;&#21270;&#20026;&#24102;&#26377;L2&#26377;&#30028;&#38789;&#24046;&#20998;&#39033;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#38543;&#26426;&#26102;&#21464;&#24046;&#20998;&#26041;&#31243;&#30340;&#28176;&#36817;&#31283;&#23450;&#24615;&#65292;&#24182;&#21457;&#23637;&#20102;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#32593;&#32476;&#22270;&#26159;&#36830;&#36890;&#30340;&#65292;&#24182;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#65292;&#21017;&#25152;&#26377;&#33410;&#28857;&#30340;&#20272;&#35745;&#22343;&#20026;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#30340;&#12290;&#36890;&#36807;&#23558;RKHS&#20013;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#31561;&#25928;&#22320;&#36716;&#21270;&#20026;&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#20013;&#24515;&#33410;&#28857;&#30340;RKHS&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with L2-bounded martingale difference terms and develop the L2 -asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the infinitedimensional spatio-temporal persistence of excitation condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on no
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20041;&#26469;&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#31639;&#27861;&#30340;&#23454;&#20363;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#35201;&#27714;&#26368;&#20248;&#31639;&#27861;&#19982;&#19968;&#20010;&#26368;&#20339;&#30340;&#24050;&#30693;&#25968;&#25454;&#38598;&#24182;&#22312;&#20854;&#36739;&#22823;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30456;&#31454;&#20105;&#65292;&#20174;&#32780;&#20351;&#22522;&#20934;&#31639;&#27861;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#23454;&#20540;&#25968;&#25454;&#38598;&#19978;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#23454;&#29616;&#23454;&#20363;&#20248;&#21270;&#30340;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#23545;&#22343;&#20540;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20272;&#35745;&#19968;&#31867;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#23646;&#24615;&#26102;&#33021;&#36798;&#21040;&#25110;&#36229;&#36807;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01262</link><description>&lt;p&gt;
&#38544;&#31169;&#20272;&#35745;&#20013;&#22522;&#20110;&#23376;&#38598;&#30340;&#23454;&#20363;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subset-Based Instance Optimality in Private Estimation. (arXiv:2303.01262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20041;&#26469;&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#31639;&#27861;&#30340;&#23454;&#20363;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#35201;&#27714;&#26368;&#20248;&#31639;&#27861;&#19982;&#19968;&#20010;&#26368;&#20339;&#30340;&#24050;&#30693;&#25968;&#25454;&#38598;&#24182;&#22312;&#20854;&#36739;&#22823;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30456;&#31454;&#20105;&#65292;&#20174;&#32780;&#20351;&#22522;&#20934;&#31639;&#27861;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#23454;&#20540;&#25968;&#25454;&#38598;&#19978;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#23454;&#29616;&#23454;&#20363;&#20248;&#21270;&#30340;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#23545;&#22343;&#20540;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20272;&#35745;&#19968;&#31867;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#23646;&#24615;&#26102;&#33021;&#36798;&#21040;&#25110;&#36229;&#36807;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#31639;&#27861;&#30340;&#23454;&#20363;&#20248;&#21270;&#30340;&#26032;&#23450;&#20041;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#35201;&#27714;&#26368;&#20248;&#31639;&#27861;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;$D$&#19978;&#37117;&#19982;&#26368;&#20339;&#30340;&#24050;&#30693;$D$&#24182;&#20197;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#23545;$D$&#30340;&#22823;&#23376;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#38544;&#31169;&#22522;&#20934;&#31639;&#27861;&#21516;&#26102;&#31454;&#20105;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22522;&#20934;&#31639;&#27861;&#22312;&#28508;&#22312;&#30340;&#26497;&#31471;&#28857;&#34987;&#28155;&#21152;&#21040;$D$&#26102;&#21487;&#33021;&#34920;&#29616;&#19981;&#22909;&#65307;&#23427;&#21482;&#38656;&#35201;&#22788;&#29702;&#21024;&#38500;&#24050;&#32463;&#23384;&#22312;&#30340;&#19968;&#23567;&#37096;&#20998;&#30495;&#23454;&#25968;&#25454;&#28857;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#22522;&#20934;&#31639;&#27861;&#27604;&#20043;&#21069;&#25552;&#20986;&#30340;&#22522;&#20934;&#31639;&#27861;&#26174;&#33879;&#26356;&#24378;&#22823;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#20173;&#28982;&#23637;&#31034;&#20102;&#23545;&#20110;&#23454;&#20540;&#25968;&#25454;&#38598;&#65292;&#22914;&#20309;&#26500;&#36896;&#33021;&#22815;&#23454;&#29616;&#25105;&#20204;&#30340;&#23454;&#20363;&#20248;&#21270;&#27010;&#24565;&#30340;&#38544;&#31169;&#31639;&#27861;&#65292;&#20197;&#20272;&#35745;&#21253;&#25324;&#22343;&#20540;&#12289;&#20998;&#20301;&#25968;&#21644;$\ell_p$-&#33539;&#25968;&#26368;&#23567;&#21270;&#22120;&#22312;&#20869;&#30340;&#24191;&#27867;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#23646;&#24615;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#22343;&#20540;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21516;&#26102;&#21305;&#37197;&#25110;&#36229;&#36807;&#20102;&#28176;&#36817;&#30340;p
&lt;/p&gt;
&lt;p&gt;
We propose a new definition of instance optimality for differentially private estimation algorithms. Our definition requires an optimal algorithm to compete, simultaneously for every dataset $D$, with the best private benchmark algorithm that (a) knows $D$ in advance and (b) is evaluated by its worst-case performance on large subsets of $D$. That is, the benchmark algorithm need not perform well when potentially extreme points are added to $D$; it only has to handle the removal of a small number of real data points that already exist. This makes our benchmark significantly stronger than those proposed in prior work. We nevertheless show, for real-valued datasets, how to construct private algorithms that achieve our notion of instance optimality when estimating a broad class of dataset properties, including means, quantiles, and $\ell_p$-norm minimizers. For means in particular, we provide a detailed analysis and show that our algorithm simultaneously matches or exceeds the asymptotic p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;&#21487;&#31359;&#25140;&#35774;&#22791;&#30417;&#27979;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25214;&#21040;&#19982;&#25351;&#23450;&#20027;&#39064;&#30456;&#20851;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#30561;&#30496;&#36136;&#37327;&#27963;&#21160;&#24314;&#35758;&#65292;&#20026;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2110.13745</link><description>&lt;p&gt;
PARIS&#65306;&#29992;&#20110;&#25913;&#21892;&#30561;&#30496;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PARIS: Personalized Activity Recommendation for Improving Sleep Quality. (arXiv:2110.13745v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;&#21487;&#31359;&#25140;&#35774;&#22791;&#30417;&#27979;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25214;&#21040;&#19982;&#25351;&#23450;&#20027;&#39064;&#30456;&#20851;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#30561;&#30496;&#36136;&#37327;&#27963;&#21160;&#24314;&#35758;&#65292;&#20026;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#36136;&#37327;&#23545;&#20154;&#20204;&#30340;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#30561;&#30496;&#19981;&#36275;&#30340;&#20154;&#26356;&#23481;&#26131;&#25253;&#21578;&#36523;&#20307;&#21644;&#24515;&#29702;&#22256;&#25200;&#12289;&#27963;&#21160;&#21463;&#38480;&#12289;&#28966;&#34385;&#21644;&#30140;&#30171;&#12290;&#27492;&#22806;&#65292;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#27963;&#21160;&#30417;&#27979;&#21644;&#20581;&#24247;&#36319;&#36394;&#30340;&#24212;&#29992;&#21644;&#35774;&#22791;&#26041;&#20852;&#26410;&#33406;&#12290;&#20174;&#36825;&#20123;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#21040;&#30340;&#20449;&#21495;&#21487;&#29992;&#20110;&#30740;&#31350;&#21644;&#25913;&#21892;&#30561;&#30496;&#36136;&#37327;&#12290;&#26412;&#25991;&#21033;&#29992;&#23454;&#20307;&#27963;&#21160;&#21644;&#30561;&#30496;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25214;&#21040;&#21327;&#21161;&#20154;&#20204;&#25913;&#21892;&#30561;&#30496;&#30340;&#26041;&#27861;&#12290;&#23545;&#27963;&#21160;&#25968;&#25454;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#65292;&#25105;&#20204;&#25214;&#21040;&#19982;&#29305;&#23450;&#20027;&#39064;&#26368;&#26174;&#30528;&#30340;&#34892;&#20026;&#27169;&#24335;&#30456;&#20851;&#30340;&#31751;&#20013;&#24515;&#12290;&#28982;&#21518;&#20026;&#27599;&#20010;&#34892;&#20026;&#27169;&#24335;&#20013;&#30340;&#27599;&#20010;&#31751;&#29983;&#25104;&#26377;&#21161;&#20110;&#33391;&#22909;&#30561;&#30496;&#36136;&#37327;&#30340;&#27963;&#21160;&#24314;&#35758;&#12290;&#36825;&#20123;&#27963;&#21160;&#24314;&#35758;&#20379;&#24212;&#32473;&#27599;&#20301;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of sleep has a deep impact on people's physical and mental health. People with insufficient sleep are more likely to report physical and mental distress, activity limitation, anxiety, and pain. Moreover, in the past few years, there has been an explosion of applications and devices for activity monitoring and health tracking. Signals collected from these wearable devices can be used to study and improve sleep quality. In this paper, we utilize the relationship between physical activity and sleep quality to find ways of assisting people improve their sleep using machine learning techniques. People usually have several behavior modes that their bio-functions can be divided into. Performing time series clustering on activity data, we find cluster centers that would correlate to the most evident behavior modes for a specific subject. Activity recipes are then generated for good sleep quality for each behavior mode within each cluster. These activity recipes are supplied to an a
&lt;/p&gt;</description></item><item><title>MRCpy&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#40065;&#26834;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#21033;&#29992;0-1&#25439;&#22833;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#25552;&#20379;&#20102;&#32039;&#23494;&#30340;&#26399;&#26395;&#25439;&#22833;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2108.01952</link><description>&lt;p&gt;
MRCpy&#65306;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
MRCpy: A Library for Minimax Risk Classifiers. (arXiv:2108.01952v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.01952
&lt;/p&gt;
&lt;p&gt;
MRCpy&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#40065;&#26834;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#21033;&#29992;0-1&#25439;&#22833;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#25552;&#20379;&#20102;&#32039;&#23494;&#30340;&#26399;&#26395;&#25439;&#22833;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29616;&#26377;&#30340;&#30417;&#30563;&#20998;&#31867;&#24211;&#37117;&#26159;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20351;&#29992;&#20195;&#29702;&#25439;&#22833;&#25216;&#26415;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;MRCpy&#24211;&#65292;&#35813;&#24211;&#23454;&#29616;&#20102;&#22522;&#20110;&#40065;&#26834;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#65288;MRC&#65289;&#65292;&#24182;&#21487;&#21033;&#29992;0-1&#25439;&#22833;&#12290;&#36825;&#31181;&#25216;&#26415;&#20135;&#29983;&#20102;&#35768;&#22810;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#32039;&#23494;&#30340;&#26399;&#26395;&#25439;&#22833;&#30028;&#38480;&#12290;MRCpy&#20026;&#19981;&#21516;&#21464;&#37327;&#30340;MRC&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#24182;&#36981;&#24490;&#27969;&#34892;Python&#24211;&#30340;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;MRCpy&#36824;&#25552;&#20379;&#20102;&#23454;&#29616;&#19968;&#20123;&#27969;&#34892;&#25216;&#26415;&#30340;&#21151;&#33021;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#30475;&#20316;&#26159;MRC&#65292;&#20363;&#22914;L1&#27491;&#21017;&#21270;&#36923;&#36753;&#22238;&#24402;&#65292;0-1&#23545;&#25239;&#24615;&#21644;&#26368;&#22823;&#29109;&#26426;&#12290;&#27492;&#22806;&#65292;MRCpy&#36824;&#23454;&#29616;&#20102;&#26368;&#36817;&#30340;&#29305;&#24449;&#26144;&#23556;&#65292;&#22914;&#20613;&#37324;&#21494;&#65292;ReLU&#21644;&#38408;&#20540;&#29305;&#24449;&#12290;&#35813;&#24211;&#37319;&#29992;&#38754;&#21521;&#23545;&#35937;&#30340;&#26041;&#27861;&#35774;&#35745;&#65292;&#26041;&#20415;&#21327;&#20316;&#32773;&#21644;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing libraries for supervised classification implement techniques that are based on empirical risk minimization and utilize surrogate losses. We present MRCpy library that implements minimax risk classifiers (MRCs) that are based on robust risk minimization and can utilize 0-1-loss. Such techniques give rise to a manifold of classification methods that can provide tight bounds on the expected loss. MRCpy provides a unified interface for different variants of MRCs and follows the standards of popular Python libraries. The presented library also provides implementation for popular techniques that can be seen as MRCs such as L1-regularized logistic regression, zero-one adversarial, and maximum entropy machines. In addition, MRCpy implements recent feature mappings such as Fourier, ReLU, and threshold features. The library is designed with an object-oriented approach that facilitates collaborators and users.
&lt;/p&gt;</description></item></channel></rss>