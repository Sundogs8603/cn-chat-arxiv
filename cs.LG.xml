<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#23556;&#27874;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#27425;&#25195;&#25551;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;MRI&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#34394;&#25311;&#32447;&#22280;&#30340;&#24212;&#29992;&#65292;&#33021;&#22815;&#20811;&#26381;&#22810;&#27425;&#25195;&#25551;&#20013;&#30340;&#30456;&#20301;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.05103</link><description>&lt;p&gt;
&#21033;&#29992;&#38646;&#23556;&#27874;&#33258;&#30417;&#30563;&#23398;&#20064;&#37325;&#24314;&#25913;&#36827;&#30340;&#22810;&#27425;&#25195;&#25551;&#25193;&#25955;&#21152;&#26435;MRI
&lt;/p&gt;
&lt;p&gt;
Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction. (arXiv:2308.05103v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#23556;&#27874;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#27425;&#25195;&#25551;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;MRI&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#34394;&#25311;&#32447;&#22280;&#30340;&#24212;&#29992;&#65292;&#33021;&#22815;&#20811;&#26381;&#22810;&#27425;&#25195;&#25551;&#20013;&#30340;&#30456;&#20301;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;MRI&#36890;&#24120;&#20351;&#29992;&#22238;&#27874;&#24179;&#38754;&#25104;&#20687;&#65288;EPI&#65289;&#36827;&#34892;&#65292;&#22240;&#20026;&#20854;&#37319;&#38598;&#26102;&#38388;&#24555;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#21152;&#26435;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#24120;&#24120;&#21463;&#21040;&#30913;&#22330;&#19981;&#22343;&#21248;&#24615;&#30456;&#20851;&#20266;&#24433;&#20197;&#21450;T2&#21644;T2*&#24347;&#35947;&#25928;&#24212;&#24341;&#36215;&#30340;&#27169;&#31946;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#65292;&#24120;&#24120;&#37319;&#29992;&#22810;&#27425;&#25195;&#25551;EPI&#65288;msEPI&#65289;&#19982;&#24182;&#34892;&#25104;&#20687;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#20010;&#25195;&#25551;&#20043;&#38388;&#30340;&#30456;&#20301;&#21464;&#21270;&#65292;&#37325;&#24314;msEPI&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;msEPI&#37325;&#24314;&#26041;&#27861;&#65292;&#31216;&#20026;zero-MIRID&#65288;&#38646;&#23556;&#27874;&#33258;&#30417;&#30563;&#23398;&#20064;&#22810;&#27425;&#25195;&#25551;&#22270;&#20687;&#37325;&#24314;&#25913;&#36827;&#25193;&#25955;MRI&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#32852;&#21512;&#37325;&#24314;msEPI&#25968;&#25454;&#12290;&#35813;&#32593;&#32476;&#22312;k&#31354;&#38388;&#21644;&#22270;&#20687;&#31354;&#38388;&#20013;&#37117;&#20351;&#29992;CNN&#21435;&#22122;&#22120;&#65292;&#24182;&#21033;&#29992;&#34394;&#25311;&#32447;&#22280;&#26469;&#22686;&#24378;&#22270;&#20687;&#37325;&#24314;&#26465;&#20214;&#12290;&#36890;&#36807;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Diffusion MRI is commonly performed using echo-planar imaging (EPI) due to its rapid acquisition time. However, the resolution of diffusion-weighted images is often limited by magnetic field inhomogeneity-related artifacts and blurring induced by T2- and T2*-relaxation effects. To address these limitations, multi-shot EPI (msEPI) combined with parallel imaging techniques is frequently employed. Nevertheless, reconstructing msEPI can be challenging due to phase variation between multiple shots. In this study, we introduce a novel msEPI reconstruction approach called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). This method jointly reconstructs msEPI data by incorporating deep learning-based image regularization techniques. The network incorporates CNN denoisers in both k- and image-spaces, while leveraging virtual coils to enhance image reconstruction conditioning. By employing a self-supervised learning technique and divi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"DOST"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#35268;&#21017;&#30340;&#32435;&#20837;&#65292;&#21033;&#29992;&#39046;&#22495;&#36981;&#20174;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20851;&#38190;&#25351;&#26631;&#65292;&#20943;&#23567;&#20102;&#27880;&#37322;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.05101</link><description>&lt;p&gt;
DOST - &#26080;&#22122;&#22768;&#26631;&#31614;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#39046;&#22495;&#36981;&#20174;&#33258;&#30417;&#30563;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DOST -- Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels. (arXiv:2308.05101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"DOST"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#35268;&#21017;&#30340;&#32435;&#20837;&#65292;&#21033;&#29992;&#39046;&#22495;&#36981;&#20174;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20851;&#38190;&#25351;&#26631;&#65292;&#20943;&#23567;&#20102;&#27880;&#37322;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#24040;&#22823;&#38656;&#27714;&#24341;&#21457;&#20102;&#27880;&#37322;&#22122;&#22768;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24191;&#27867;&#35752;&#35770;&#65292;&#20294;&#22312;"&#22810;&#26631;&#31614;&#20998;&#31867;"&#65288;MLC&#65289;&#20219;&#21153;&#30340;&#22797;&#26434;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#21364;&#30456;&#23545;&#26410;&#32463;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#24403;&#25152;&#28041;&#21450;&#30340;&#39046;&#22495;&#20855;&#26377;&#26576;&#20123;&#36923;&#36753;&#32422;&#26463;&#26102;&#65292;&#22122;&#22768;&#26631;&#27880;&#24120;&#24120;&#21152;&#21095;&#36829;&#35268;&#24773;&#20917;&#65292;&#20351;&#24471;&#35813;&#31995;&#32479;&#34987;&#19987;&#23478;&#35748;&#20026;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#31614;&#22122;&#22768;&#23545;MLC&#20219;&#21153;&#20013;&#39046;&#22495;&#35268;&#21017;&#36829;&#35268;&#20107;&#20214;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#39046;&#22495;&#35268;&#21017;&#32435;&#20837;&#25105;&#20204;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"Domain Obedient Self-supervised Training"&#65288;DOST&#65289;&#33539;&#24335;&#65292;&#19981;&#20165;&#21487;&#20197;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26356;&#31526;&#21512;&#39046;&#22495;&#35268;&#21017;&#65292;&#36824;&#21487;&#20197;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#26368;&#23567;&#21270;&#27880;&#37322;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20351;&#29992;&#39046;&#22495;&#25351;&#23548;&#35757;&#32451;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enormous demand for annotated data brought forth by deep learning techniques has been accompanied by the problem of annotation noise. Although this issue has been widely discussed in machine learning literature, it has been relatively unexplored in the context of "multi-label classification" (MLC) tasks which feature more complicated kinds of noise. Additionally, when the domain in question has certain logical constraints, noisy annotations often exacerbate their violations, making such a system unacceptable to an expert. This paper studies the effect of label noise on domain rule violation incidents in the MLC task, and incorporates domain rules into our learning algorithm to mitigate the effect of noise. We propose the Domain Obedient Self-supervised Training (DOST) paradigm which not only makes deep learning models more aligned to domain rules, but also improves learning performance in key metrics and minimizes the effect of annotation noise. This novel approach uses domain guid
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25193;&#23637;&#25968;&#25454;&#37327;&#21644;&#22270;&#20687;&#36136;&#37327;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#26356;&#20808;&#36827;&#30340;&#38598;&#32676;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#20122;&#20154;&#31867;&#23610;&#24230;&#19979;&#30340;&#20154;&#31867;&#32423;&#21035;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05092</link><description>&lt;p&gt;
&#22312;&#26356;&#20808;&#36827;&#30340;&#38598;&#32676;&#26465;&#20214;&#19979;&#65292;&#21487;&#33021;&#23454;&#29616;&#20122;&#20154;&#31867;&#23610;&#24230;&#19979;&#30340;&#22270;&#20687;&#35782;&#21035;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
A degree of image identification at sub-human scales could be possible with more advanced clusters. (arXiv:2308.05092v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25193;&#23637;&#25968;&#25454;&#37327;&#21644;&#22270;&#20687;&#36136;&#37327;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#26356;&#20808;&#36827;&#30340;&#38598;&#32676;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#20122;&#20154;&#31867;&#23610;&#24230;&#19979;&#30340;&#20154;&#31867;&#32423;&#21035;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#30830;&#23450;&#24403;&#21069;&#21487;&#29992;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#19982;&#20154;&#31867;&#33719;&#21462;&#24863;&#23448;&#36755;&#20837;&#30456;&#21516;&#31243;&#24230;&#21644;&#25968;&#37327;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#35270;&#35273;&#22270;&#20687;&#30340;&#29702;&#35299;&#12290;&#19968;&#24320;&#22987;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#25968;&#25454;&#37327;&#30340;&#25193;&#22823;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21516;&#26102;&#25193;&#23637;&#20102;&#25968;&#25454;&#37327;&#21644;&#22270;&#20687;&#36136;&#37327;&#12290;&#36825;&#20010;&#25193;&#23637;&#23454;&#39564;&#26159;&#19968;&#31181;&#26080;&#38656;&#20219;&#20309;&#22806;&#37096;&#34701;&#36164;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21516;&#26102;&#25193;&#23637;&#25968;&#25454;&#37327;&#21644;&#22270;&#29255;&#20998;&#36776;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#20122;&#20154;&#31867;&#23610;&#24230;&#19979;&#30340;&#20154;&#31867;&#32423;&#21035;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#20197;&#26368;&#39640;256&#20687;&#32032;&#27599;&#33521;&#23544;&#20998;&#36776;&#29575;&#35757;&#32451;&#30340;Vision Transformers&#36827;&#34892;&#20102;&#21487;&#25193;&#23637;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#36798;200,000&#24352;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from. Initial research on this topic solely considered data volume scaling. Here, we scale both the volume of data and the quality of the image. This scaling experiment is a self-supervised learning method that may be done without any outside financing. We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#36870;&#36716;&#25442;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19987;&#23478;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#23398;&#20064;&#36807;&#28193;&#21160;&#24577;T&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20943;&#23569;&#25919;&#31574;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.05075</link><description>&lt;p&gt;
&#20855;&#26377;&#36125;&#21494;&#26031;&#36870;&#36716;&#25442;&#23398;&#20064;&#30340;&#31163;&#32447;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Bayesian Inverse Transition Learning for Offline Settings. (arXiv:2308.05075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#36870;&#36716;&#25442;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19987;&#23478;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#23398;&#20064;&#36807;&#28193;&#21160;&#24577;T&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20943;&#23569;&#25919;&#31574;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#29992;&#20110;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#65292;&#20854;&#20013;&#22870;&#21169;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#19988;&#24517;&#39035;&#22522;&#20110;&#25209;&#37327;&#25968;&#25454;&#20272;&#35745;&#36807;&#28193;&#21160;&#24577;T&#12290;&#23545;&#20110;&#25152;&#26377;&#20219;&#21153;&#32780;&#35328;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#23398;&#20064;&#19968;&#20010;&#21487;&#38752;&#30340;&#36807;&#28193;&#21160;&#24577;T&#20272;&#35745;&#65292;&#36825;&#20123;&#20272;&#35745;&#33021;&#22815;&#20135;&#29983;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#36275;&#22815;&#23433;&#20840;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#20174;&#26410;&#37319;&#21462;&#36828;&#31163;&#26368;&#20339;&#21160;&#20316;&#30340;&#34892;&#21160;&#65292;&#24182;&#19988;&#36275;&#22815;&#20449;&#24687;&#20016;&#23500;&#65292;&#20197;&#20256;&#36798;&#20854;&#25152;&#20855;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#25105;&#20204;&#21487;&#38752;&#22320;&#23398;&#20064;&#36807;&#28193;&#21160;&#24577;T&#21518;&#39564;&#20998;&#24067;&#30340;&#38656;&#35201;&#65292;&#32780;&#36825;&#20123;&#20998;&#24067;&#21448;&#19981;&#28041;&#21450;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#39640;&#24615;&#33021;&#25919;&#31574;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20943;&#23569;&#25919;&#31574;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#8230;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement learning is commonly used for sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data. A key challenge for all tasks is how to learn a reliable estimate of the transition dynamics $T$ that produce near-optimal policies that are safe enough so that they never take actions that are far away from the best action with respect to their value functions and informative enough so that they communicate the uncertainties they have. Using data from an expert, we propose a new constraint-based approach that captures our desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients. Our results demonstrate that by using our constraints, we learn a high-performing policy, while considerably reducing the policy's variance over different datasets. We also explain how combining uncertainty estimation with th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05061</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#22312;&#25512;&#29702;&#38454;&#27573;&#20174;&#25552;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#36816;&#31639;&#31526;&#30340;&#26174;&#33879;&#28508;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#36816;&#31639;&#31526;&#30340;&#23453;&#36149;&#30340;&#20154;&#31867;&#27934;&#23519;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#36716;&#21270;&#20026;&#19968;&#31181;&#22810;&#27169;&#24335;&#33539;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#8220;&#26631;&#39064;&#8221;&#26469;&#25972;&#21512;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#26041;&#31243;&#24335;&#34920;&#36798;&#30340;&#36816;&#31639;&#31526;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36941;&#24615;&#65292;&#32780;&#19988;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22810;&#27169;&#24335;&#19978;&#19979;&#25991;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#8220;ICON-LM&#8221;&#65292;&#22522;&#20110;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#26102;&#21442;&#25968;&#26356;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05059</link><description>&lt;p&gt;
&#36890;&#36807;&#24674;&#22797;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#25216;&#26415;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique. (arXiv:2308.05059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#26102;&#21442;&#25968;&#26356;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#39046;&#22495;&#24443;&#24213;&#25913;&#21464;&#20102;&#24037;&#19994;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#35201;&#26041;&#27861;&#8212;&#8212;&#21453;&#21521;&#20256;&#25773;&#65292;&#38754;&#20020;&#30528;&#35745;&#31639;&#24320;&#38144;&#22823;&#21644;&#26799;&#24230;&#28040;&#22833;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#26102;&#21442;&#25968;&#26356;&#26032;&#26041;&#27861;&#65292;&#23427;&#28040;&#38500;&#20102;&#22312;&#27599;&#20010;&#23618;&#35745;&#31639;&#26799;&#24230;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#39640;&#25928;&#26377;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has revolutionized industries like computer vision, natural language processing, and speech recognition. However, back propagation, the main method for training deep neural networks, faces challenges like computational overhead and vanishing gradients. In this paper, we propose a novel instant parameter update methodology that eliminates the need for computing gradients at each layer. Our approach accelerates learning, avoids the vanishing gradient problem, and outperforms state-of-the-art methods on benchmark data sets. This research presents a promising direction for efficient and effective deep neural network training.
&lt;/p&gt;</description></item><item><title>RadGraph2&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20197;&#20998;&#23618;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;HGIE&#20026;&#22522;&#30784;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#30142;&#30149;&#36827;&#23637;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.05046</link><description>&lt;p&gt;
RadGraph2&#65306;&#36890;&#36807;&#20998;&#23618;&#20449;&#24687;&#25552;&#21462;&#24314;&#27169;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#30142;&#30149;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction. (arXiv:2308.05046v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05046
&lt;/p&gt;
&lt;p&gt;
RadGraph2&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20197;&#20998;&#23618;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;HGIE&#20026;&#22522;&#30784;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#30142;&#30149;&#36827;&#23637;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RadGraph2&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#30142;&#30149;&#29366;&#24577;&#21644;&#35774;&#22791;&#25918;&#32622;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#20851;&#31995;&#32452;&#32455;&#23454;&#20307;&#30340;&#20998;&#23618;&#27169;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25913;&#21892;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23545;DyGIE++&#26694;&#26550;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;HGIE&#65292;&#35813;&#27169;&#22411;&#22312;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RadGraph2&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#21457;&#29616;&#65292;&#24182;&#22312;&#20851;&#31995;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#37027;&#20123;&#22312;&#21407;&#22987;RadGraph&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22880;&#23450;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#24320;&#21457;&#21487;&#20197;&#36319;&#36394;&#30142;&#30149;&#36827;&#23637;&#24182;&#21033;&#29992;&#26631;&#31614;&#30340;&#33258;&#28982;&#20998;&#23618;&#30340;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RadGraph2, a novel dataset for extracting information from radiology reports that focuses on capturing changes in disease state and device placement over time. We introduce a hierarchical schema that organizes entities based on their relationships and show that using this hierarchy during training improves the performance of an information extraction model. Specifically, we propose a modification to the DyGIE++ framework, resulting in our model HGIE, which outperforms previous models in entity and relation extraction tasks. We demonstrate that RadGraph2 enables models to capture a wider variety of findings and perform better at relation extraction compared to those trained on the original RadGraph dataset. Our work provides the foundation for developing automated systems that can track disease progression over time and develop information extraction models that leverage the natural hierarchy of labels in the medical domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24335;&#32593;&#32476;&#21270;&#26080;&#20154;&#26426;&#22312;UTM&#31995;&#32479;&#20013;&#36827;&#34892;&#23485;&#24102;&#39057;&#35889;&#24863;&#30693;&#21644;&#35843;&#24230;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#20998;&#31867;&#36755;&#20986;&#26469;&#22686;&#24378;&#39057;&#35889;&#24863;&#30693;&#27169;&#22359;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23558;&#26816;&#27979;&#21040;&#30340;&#39057;&#35889;&#31354;&#27934;&#20998;&#37197;&#32473;&#27425;&#35201;&#29992;&#25143;&#36827;&#34892;&#21160;&#24577;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05036</link><description>&lt;p&gt;
&#21327;&#20316;&#24335;&#32593;&#32476;&#21270;&#26080;&#20154;&#26426;&#22312;UTM&#31995;&#32479;&#20013;&#30340;&#23485;&#24102;&#39057;&#35889;&#24863;&#30693;&#21644;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems. (arXiv:2308.05036v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24335;&#32593;&#32476;&#21270;&#26080;&#20154;&#26426;&#22312;UTM&#31995;&#32479;&#20013;&#36827;&#34892;&#23485;&#24102;&#39057;&#35889;&#24863;&#30693;&#21644;&#35843;&#24230;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#20998;&#31867;&#36755;&#20986;&#26469;&#22686;&#24378;&#39057;&#35889;&#24863;&#30693;&#27169;&#22359;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23558;&#26816;&#27979;&#21040;&#30340;&#39057;&#35889;&#31354;&#27934;&#20998;&#37197;&#32473;&#27425;&#35201;&#29992;&#25143;&#36827;&#34892;&#21160;&#24577;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#20316;&#24335;&#32593;&#32476;&#21270;&#26080;&#20154;&#26426;&#22312;UTM&#31995;&#32479;&#20013;&#30340;&#23485;&#24102;&#39057;&#35889;&#24863;&#30693;&#21644;&#35843;&#24230;&#65292;&#36825;&#20123;&#26080;&#20154;&#26426;&#20316;&#20026;&#27425;&#35201;&#29992;&#25143;&#21487;&#20197;&#26426;&#20250;&#24615;&#22320;&#21033;&#29992;&#26816;&#27979;&#21040;&#30340;&#39057;&#35889;&#31354;&#27934;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#65292;&#29992;&#20110;&#22522;&#20110;&#37319;&#38598;&#21040;&#30340;I/Q&#26679;&#26412;&#26816;&#27979;&#31354;&#38386;&#39057;&#35889;&#20301;&#32622;&#12290;&#20026;&#20102;&#25552;&#39640;&#39057;&#35889;&#24863;&#30693;&#27169;&#22359;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26080;&#20154;&#26426;&#31995;&#32479;&#20132;&#36890;&#31649;&#29702;&#65288;UTM&#65289;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#21333;&#20010;&#26080;&#20154;&#26426;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#36755;&#20986;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#34701;&#21512;&#12290;&#22312;&#39057;&#35889;&#35843;&#24230;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21160;&#24577;&#22320;&#23558;&#26816;&#27979;&#21040;&#30340;&#39057;&#35889;&#31354;&#27934;&#20998;&#37197;&#32473;&#27425;&#35201;&#29992;&#25143;&#65288;&#21363;&#26080;&#20154;&#26426;&#65289;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#20223;&#30495;&#26694;&#26550;&#65292;&#20351;&#29992;MATLAB LTE&#24037;&#20855;&#31665;&#29983;&#25104;&#36817;&#30495;&#23454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#22522;&#31449;&#65288;BS&#65289;&#20301;&#32622;&#32435;&#20837;&#21040;&#20223;&#30495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a data-driven framework for collaborative wideband spectrum sensing and scheduling for networked unmanned aerial vehicles (UAVs), which act as the secondary users to opportunistically utilize detected spectrum holes. To this end, we propose a multi-class classification problem for wideband spectrum sensing to detect vacant spectrum spots based on collected I/Q samples. To enhance the accuracy of the spectrum sensing module, the outputs from the multi-class classification by each individual UAV are fused at a server in the unmanned aircraft system traffic management (UTM) ecosystem. In the spectrum scheduling phase, we leverage reinforcement learning (RL) solutions to dynamically allocate the detected spectrum holes to the secondary users (i.e., UAVs). To evaluate the proposed methods, we establish a comprehensive simulation framework that generates a near-realistic synthetic dataset using MATLAB LTE toolbox by incorporating base-station~(BS) locations in a cho
&lt;/p&gt;</description></item><item><title>Kairos&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#28385;&#36275;&#33539;&#22260;&#12289;&#25915;&#20987;&#19981;&#21487;&#30693;&#24615;&#12289;&#26102;&#25928;&#24615;&#21644;&#25915;&#20987;&#37325;&#24314;&#32500;&#24230;&#35201;&#27714;&#30340;&#28335;&#28304;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#35843;&#26597;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.05034</link><description>&lt;p&gt;
Kairos: &#20351;&#29992;&#25972;&#20307;&#31995;&#32479;&#28335;&#28304;&#36827;&#34892;&#23454;&#29992;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance. (arXiv:2308.05034v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05034
&lt;/p&gt;
&lt;p&gt;
Kairos&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#28385;&#36275;&#33539;&#22260;&#12289;&#25915;&#20987;&#19981;&#21487;&#30693;&#24615;&#12289;&#26102;&#25928;&#24615;&#21644;&#25915;&#20987;&#37325;&#24314;&#32500;&#24230;&#35201;&#27714;&#30340;&#28335;&#28304;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#35843;&#26597;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28335;&#28304;&#22270;&#26159;&#25551;&#36848;&#31995;&#32479;&#25191;&#34892;&#21382;&#21490;&#30340;&#32467;&#26500;&#21270;&#23457;&#35745;&#26085;&#24535;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#20998;&#26512;&#28335;&#28304;&#22270;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#20027;&#26426;&#20837;&#20405;&#26816;&#27979;&#65292;&#29305;&#21035;&#20851;&#27880;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#12290;&#36890;&#36807;&#30740;&#31350;&#20854;&#35774;&#35745;&#25991;&#26723;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#24120;&#35265;&#32500;&#24230;&#65292;&#25512;&#21160;&#28335;&#28304;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;PIDS&#65289;&#30340;&#21457;&#23637;&#65306;&#33539;&#22260;&#65288;PIDS&#33021;&#21542;&#26816;&#27979;&#36328;&#24212;&#29992;&#36793;&#30028;&#28183;&#36879;&#30340;&#29616;&#20195;&#25915;&#20987;&#65311;&#65289;&#12289;&#25915;&#20987;&#19981;&#21487;&#30693;&#24615;&#65288;PIDS&#33021;&#21542;&#22312;&#27809;&#26377;&#25915;&#20987;&#29305;&#24449;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#26032;&#22411;&#25915;&#20987;&#65311;&#65289;&#12289;&#26102;&#25928;&#24615;&#65288;PIDS&#33021;&#21542;&#39640;&#25928;&#30417;&#35270;&#20027;&#26426;&#31995;&#32479;&#36816;&#34892;&#65311;&#65289;&#21644;&#25915;&#20987;&#37325;&#24314;&#65288;PIDS&#33021;&#21542;&#20174;&#22823;&#22411;&#28335;&#28304;&#22270;&#20013;&#25552;&#28860;&#25915;&#20987;&#27963;&#21160;&#65292;&#20197;&#20415;&#31995;&#32479;&#31649;&#29702;&#21592;&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#24182;&#36805;&#36895;&#24212;&#23545;&#31995;&#32479;&#20837;&#20405;&#65311;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KAIROS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#28385;&#36275;&#25152;&#26377;&#22235;&#20010;&#32500;&#24230;&#35201;&#27714;&#30340;PIDS&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#33021;&#20570;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approac
&lt;/p&gt;</description></item><item><title>&#23494;&#24230;&#20316;&#29289;&#24341;&#23548;&#30340;&#21322;&#30417;&#30563;&#26816;&#27979;&#22120;&#36890;&#36807;&#35782;&#21035;&#23567;&#29289;&#20307;&#30340;&#32858;&#31867;&#26469;&#25552;&#39640;&#22312;&#33322;&#25293;&#22270;&#20687;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05032</link><description>&lt;p&gt;
&#23494;&#24230;&#20316;&#29289;&#24341;&#23548;&#30340;&#21322;&#30417;&#30563;&#33322;&#31354;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Density Crop-guided Semi-supervised Object Detection in Aerial Images. (arXiv:2308.05032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05032
&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#20316;&#29289;&#24341;&#23548;&#30340;&#21322;&#30417;&#30563;&#26816;&#27979;&#22120;&#36890;&#36807;&#35782;&#21035;&#23567;&#29289;&#20307;&#30340;&#32858;&#31867;&#26469;&#25552;&#39640;&#22312;&#33322;&#25293;&#22270;&#20687;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#29616;&#20195;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#29942;&#39048;&#26159;&#38656;&#35201;&#26631;&#35760;&#30340;&#22270;&#20687;&#65292;&#20854;&#20013;&#38656;&#35201;&#20026;&#22270;&#20687;&#20013;&#30340;&#27599;&#20010;&#23545;&#35937;&#20135;&#29983;&#36793;&#30028;&#26694;&#27880;&#37322;&#12290;&#22312;&#33322;&#25293;&#22270;&#20687;&#20013;&#65292;&#36825;&#20010;&#29942;&#39048;&#26356;&#21152;&#20005;&#37325;&#65292;&#26631;&#27880;&#21592;&#24517;&#39035;&#23545;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#19978;&#20998;&#24067;&#30340;&#23567;&#29289;&#20307;&#36827;&#34892;&#26631;&#27880;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#21644;&#24369;&#24378;&#22686;&#24378;&#19968;&#33268;&#24615;&#35757;&#32451;&#30340;&#24179;&#22343;&#25945;&#24072;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#36825;&#31181;&#21322;&#30417;&#30563;&#26816;&#27979;&#22120;&#24212;&#29992;&#20110;&#33322;&#31354;&#22270;&#20687;&#65292;&#20854;&#20013;&#32463;&#24120;&#23384;&#22312;&#23567;&#30340;&#32858;&#31867;&#23545;&#35937;&#65292;&#21487;&#33021;&#19981;&#20250;&#24471;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#24230;&#20316;&#29289;&#24341;&#23548;&#30340;&#21322;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#23427;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35782;&#21035;&#20986;&#23567;&#29289;&#20307;&#30340;&#32858;&#31867;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the important bottlenecks in training modern object detectors is the need for labeled images where bounding box annotations have to be produced for each object present in the image. This bottleneck is further exacerbated in aerial images where the annotators have to label small objects often distributed in clusters on high-resolution images. In recent days, the mean-teacher approach trained with pseudo-labels and weak-strong augmentation consistency is gaining popularity for semi-supervised object detection. However, a direct adaptation of such semi-supervised detectors for aerial images where small clustered objects are often present, might not lead to optimal results. In this paper, we propose a density crop-guided semi-supervised detector that identifies the cluster of small objects during training and also exploits them to improve performance at inference. During training, image crops of clusters identified from labeled and unlabeled images are used to augment the training s
&lt;/p&gt;</description></item><item><title>AbDiffuser&#26159;&#19968;&#20010;&#29289;&#29702;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#25239;&#20307;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#32422;&#26463;&#25913;&#21892;&#34507;&#30333;&#36136;&#25193;&#25955;&#65292;&#22788;&#29702;&#24207;&#21015;&#38271;&#24230;&#21464;&#21270;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#19982;&#21442;&#32771;&#38598;&#21512;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#29305;&#24615;&#23494;&#20999;&#21305;&#37197;&#30340;&#25239;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AbDiffuser&#33021;&#22815;&#29983;&#25104;&#39640;&#27700;&#24179;&#34920;&#36798;&#30340;&#25239;&#20307;&#65292;&#20854;&#20013;57.1%&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#32039;&#23494;&#32467;&#21512;&#21058;&#12290;</title><link>http://arxiv.org/abs/2308.05027</link><description>&lt;p&gt;
AbDiffuser&#65306;&#20307;&#22806;&#21151;&#33021;&#25239;&#20307;&#30340;&#20840;&#21407;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies. (arXiv:2308.05027v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05027
&lt;/p&gt;
&lt;p&gt;
AbDiffuser&#26159;&#19968;&#20010;&#29289;&#29702;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#25239;&#20307;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#32422;&#26463;&#25913;&#21892;&#34507;&#30333;&#36136;&#25193;&#25955;&#65292;&#22788;&#29702;&#24207;&#21015;&#38271;&#24230;&#21464;&#21270;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#19982;&#21442;&#32771;&#38598;&#21512;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#29305;&#24615;&#23494;&#20999;&#21305;&#37197;&#30340;&#25239;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AbDiffuser&#33021;&#22815;&#29983;&#25104;&#39640;&#27700;&#24179;&#34920;&#36798;&#30340;&#25239;&#20307;&#65292;&#20854;&#20013;57.1%&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#32039;&#23494;&#32467;&#21512;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;AbDiffuser&#30340;&#31561;&#21464;&#29289;&#29702;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#25239;&#20307;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#24207;&#21015;&#12290;AbDiffuser&#24314;&#31435;&#22312;&#19968;&#31181;&#26032;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#19978;&#65292;&#20381;&#36182;&#20110;&#19968;&#31181;&#38024;&#23545;&#40784;&#20301;&#34507;&#30333;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#24378;&#25193;&#25955;&#20808;&#39564;&#25913;&#21892;&#21435;&#22122;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#32422;&#26463;&#25913;&#21892;&#20102;&#34507;&#30333;&#36136;&#25193;&#25955;&#65307;&#22788;&#29702;&#24207;&#21015;&#38271;&#24230;&#21464;&#21270;&#65307;&#24182;&#23558;&#20869;&#23384;&#22797;&#26434;&#24615;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#23454;&#29616;&#20102;&#39592;&#26550;&#21644;&#20391;&#38142;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#20307;&#20869;&#21644;&#20307;&#22806;&#39564;&#35777;&#20102;AbDiffuser&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;AbDiffuser&#29983;&#25104;&#19982;&#21442;&#32771;&#38598;&#21512;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#29305;&#24615;&#23494;&#20999;&#21305;&#37197;&#30340;&#25239;&#20307;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#23460;&#23454;&#39564;&#35777;&#23454;&#65292;&#21457;&#29616;&#30340;16&#31181;HER2&#25239;&#20307;&#22343;&#20197;&#39640;&#27700;&#24179;&#34920;&#36798;&#65292;&#24182;&#19988;57.1%&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#32039;&#23494;&#32467;&#21512;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of selected designs were tight binders.
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#25454;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#21435;&#22122;&#27169;&#22359;&#26159;&#21542;&#20855;&#26377;&#23481;&#38169;&#24615;&#20197;&#21450;&#22914;&#20309;&#20943;&#23569;&#20998;&#24067;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.05021</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#20250;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65311;&#29702;&#35770;&#20998;&#26512;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization. (arXiv:2308.05021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05021
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#25454;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#21435;&#22122;&#27169;&#22359;&#26159;&#21542;&#20855;&#26377;&#23481;&#38169;&#24615;&#20197;&#21450;&#22914;&#20309;&#20943;&#23569;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#65292;&#20294;&#30001;&#20110;&#20854;&#32423;&#32852;&#32467;&#26500;&#65292;&#21363;&#21435;&#22122;&#27169;&#22359;&#38142;&#24335;&#20256;&#25773;&#21644;&#25918;&#22823;&#20102;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#38169;&#35823;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26399;&#26395;&#36827;&#34892;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#22240;&#20026;&#35768;&#22810;&#39034;&#24207;&#27169;&#22411;&#65292;&#22914;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRF&#65289;&#65292;&#26159;&#19981;&#20250;&#20986;&#29616;&#38169;&#35823;&#20256;&#25773;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#39564;&#35777;&#20102;&#25193;&#25955;&#27169;&#22411;&#30830;&#23454;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#24402;&#32467;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#27599;&#20010;&#21435;&#22122;&#27169;&#22359;&#26159;&#21542;&#20855;&#26377;&#23481;&#38169;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26377;&#28145;&#21051;&#35265;&#35299;&#30340;&#36716;&#31227;&#26041;&#31243;&#65292;&#34920;&#26126;&#27169;&#22359;&#26080;&#27861;&#20174;&#36755;&#20837;&#38169;&#35823;&#20013;&#24674;&#22797;&#65292;&#29978;&#33267;&#20250;&#23558;&#39069;&#22806;&#30340;&#38169;&#35823;&#20256;&#25773;&#21040;&#19979;&#19968;&#20010;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30452;&#25509;&#23548;&#33268;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#26126;&#30830;&#20943;&#23569;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
While diffusion models have achieved promising performances in data synthesis, they might suffer error propagation because of their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules. However, a strict analysis is expected since many sequential models such as Conditional Random Field (CRF) are free from error propagation. In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation and we then propose a regularization to address this problem. Our theoretical analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant. We derive insightful transition equations, indicating that the module can't recover from input errors and even propagates additional errors to the next module. Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#30740;&#31350;&#20309;&#26102;&#21644;&#22914;&#20309;&#21033;&#29992;&#24050;&#30693;&#31867;&#21035;&#24110;&#21161;&#21457;&#29616;&#26032;&#39062;&#31867;&#21035;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;NCD&#35889;&#23545;&#27604;&#25439;&#22833;&#65288;NSCL&#65289;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#38480;&#21644;NCD&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;NSCL&#21487;&#20197;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#25110;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#26041;&#27861;&#65292;&#20855;&#26377;&#23454;&#38469;&#20351;&#29992;&#20215;&#20540;&#19988;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.05017</link><description>&lt;p&gt;
&#20309;&#26102;&#21644;&#22914;&#20309;&#36890;&#36807;&#35889;&#20998;&#26512;&#21033;&#29992;&#24050;&#30693;&#31867;&#21035;&#24110;&#21161;&#21457;&#29616;&#26410;&#30693;&#31867;&#21035;&#65311;&#36890;&#36807;&#21487;&#35777;&#26126;&#30340;&#35889;&#20998;&#26512;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis. (arXiv:2308.05017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#30740;&#31350;&#20309;&#26102;&#21644;&#22914;&#20309;&#21033;&#29992;&#24050;&#30693;&#31867;&#21035;&#24110;&#21161;&#21457;&#29616;&#26032;&#39062;&#31867;&#21035;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;NCD&#35889;&#23545;&#27604;&#25439;&#22833;&#65288;NSCL&#65289;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#38480;&#21644;NCD&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;NSCL&#21487;&#20197;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#25110;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#26041;&#27861;&#65292;&#20855;&#26377;&#23454;&#38469;&#20351;&#29992;&#20215;&#20540;&#19988;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#39062;&#31867;&#21035;&#21457;&#29616;&#65288;NCD&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#24050;&#30693;&#31867;&#21035;&#30340;&#26631;&#35760;&#38598;&#21512;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#26410;&#26631;&#35760;&#30340;&#38598;&#21512;&#20013;&#25512;&#26029;&#20986;&#26032;&#39062;&#31867;&#21035;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;NCD&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#20197;&#24418;&#24335;&#21270;&#21644;&#30740;&#31350;&#20309;&#26102;&#21644;&#22914;&#20309;&#24050;&#30693;&#31867;&#21035;&#33021;&#22815;&#24110;&#21161;&#21457;&#29616;&#26032;&#39062;&#31867;&#21035;&#12290;&#38024;&#23545;NCD&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#35770;&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;NCD&#35889;&#23545;&#27604;&#25439;&#22833;&#65288;NSCL&#65289;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#23567;&#21270;&#36825;&#20010;&#30446;&#26631;&#31561;&#21516;&#20110;&#20998;&#35299;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;NCD&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;NSCL&#21487;&#20197;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#25110;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#26041;&#27861;&#65292;&#36825;&#23545;&#23454;&#38469;&#20351;&#29992;&#26159;&#26377;&#21560;&#24341;&#21147;&#30340;&#65292;&#21516;&#26102;&#20139;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel Class Discovery (NCD) aims at inferring novel classes in an unlabeled set by leveraging prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for NCD. This paper bridges the gap by providing an analytical framework to formalize and investigate when and how known classes can help discover novel classes. Tailored to the NCD problem, we introduce a graph-theoretic representation that can be learned by a novel NCD Spectral Contrastive Loss (NSCL). Minimizing this objective is equivalent to factorizing the graph's adjacency matrix, which allows us to derive a provable error bound and provide the sufficient and necessary condition for NCD. Empirically, NSCL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#65292;&#24182;&#23545;&#36825;&#20123;&#38169;&#35823;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#20026;&#25552;&#39640;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.05014</link><description>&lt;p&gt;
&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#38169;&#35823;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Bugs in Open-Source Federated Learning Framework. (arXiv:2308.05014v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#65292;&#24182;&#23545;&#36825;&#20123;&#38169;&#35823;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#20026;&#25552;&#39640;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#20445;&#25252;&#29992;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#65292;&#22312;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#23398;&#20064;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22810;&#25968;&#22269;&#23478;&#23454;&#26045;&#26356;&#20005;&#26684;&#30340;&#27861;&#24459;&#27861;&#35268;&#20043;&#21518;&#12290;&#22240;&#27492;&#65292;&#21457;&#24067;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;FL&#27169;&#22411;&#21644;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;FL&#26694;&#26550;&#30340;&#23433;&#20840;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;1,112&#20010;FL&#26694;&#26550;&#38169;&#35823;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#38169;&#35823;&#26159;&#36890;&#36807;&#25163;&#21160;&#20174;GitHub&#19978;&#25910;&#38598;&#12289;&#20998;&#31867;&#21644;&#26631;&#35760;&#30340;12&#20010;&#24320;&#28304;FL&#26694;&#26550;&#24471;&#26469;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36825;&#20123;&#38169;&#35823;&#30340;15&#20010;&#30151;&#29366;&#12289;12&#20010;&#26681;&#26412;&#21407;&#22240;&#21644;20&#20010;&#20462;&#22797;&#27169;&#24335;&#30340;&#20998;&#31867;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;23&#20010;&#36923;&#36753;&#32452;&#20214;&#21644;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#22330;&#26223;&#19978;&#30340;&#30456;&#20851;&#24615;&#21644;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), as a decentralized machine learning solution to the protection of users' private data, has become an important learning paradigm in recent years, especially since the enforcement of stricter laws and regulations in most countries. Therefore, a variety of FL frameworks are released to facilitate the development and application of federated learning. Despite the considerable amount of research on the security and privacy of FL models and systems, the security issues in FL frameworks have not been systematically studied yet. In this paper, we conduct the first empirical study on 1,112 FL framework bugs to investigate their characteristics. These bugs are manually collected, classified, and labeled from 12 open-source FL frameworks on GitHub. In detail, we construct taxonomies of 15 symptoms, 12 root causes, and 20 fix patterns of these bugs and investigate their correlations and distributions on 23 logical components and two main application scenarios. From the re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#20869;&#37096;&#31867;&#21035;&#30340;&#22825;&#25991;&#23398;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#22810;&#31867;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;(MCDSVDD)&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#36229;&#29699;&#20307;&#20013;&#65292;&#27599;&#20010;&#36229;&#29699;&#20307;&#20195;&#34920;&#19968;&#20010;&#29305;&#23450;&#30340;&#20869;&#37096;&#31867;&#21035;&#65292;&#24182;&#35745;&#31639;&#26679;&#26412;&#19982;&#36229;&#29699;&#20307;&#20013;&#24515;&#30340;&#36317;&#31163;&#20197;&#30830;&#23450;&#24322;&#24120;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.05011</link><description>&lt;p&gt;
&#22810;&#31867;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#65306;&#19968;&#31181;&#29992;&#20110;&#22825;&#25991;&#23398;&#20013;&#20855;&#26377;&#19981;&#21516;&#20869;&#37096;&#31867;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories. (arXiv:2308.05011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05011
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#20869;&#37096;&#31867;&#21035;&#30340;&#22825;&#25991;&#23398;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#22810;&#31867;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;(MCDSVDD)&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#36229;&#29699;&#20307;&#20013;&#65292;&#27599;&#20010;&#36229;&#29699;&#20307;&#20195;&#34920;&#19968;&#20010;&#29305;&#23450;&#30340;&#20869;&#37096;&#31867;&#21035;&#65292;&#24182;&#35745;&#31639;&#26679;&#26412;&#19982;&#36229;&#29699;&#20307;&#20013;&#24515;&#30340;&#36317;&#31163;&#20197;&#30830;&#23450;&#24322;&#24120;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#21208;&#27979;&#26395;&#36828;&#38236;&#20135;&#29983;&#30340;&#22825;&#25991;&#25968;&#25454;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#33258;&#21160;&#21270;&#27969;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#20026;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21644;&#25552;&#21462;&#30693;&#35782;&#30340;&#20851;&#38190;&#12290;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#25968;&#25454;&#20013;&#35782;&#21035;&#19981;&#35268;&#21017;&#25110;&#24847;&#22806;&#27169;&#24335;&#30340;&#20219;&#21153;&#65292;&#26159;&#22825;&#25991;&#23398;&#20013;&#30340;&#19968;&#20010;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31867;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;(MCDSVDD)&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#23545;&#29616;&#26377;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;One-Class Deep SVDD&#36827;&#34892;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#21516;&#20869;&#37096;&#31867;&#21035;&#12290;MCDSVDD&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#36229;&#29699;&#20307;&#20013;&#65292;&#20854;&#20013;&#27599;&#20010;&#36229;&#29699;&#20307;&#20195;&#34920;&#19968;&#20010;&#29305;&#23450;&#30340;&#20869;&#37096;&#31867;&#21035;&#12290;&#26679;&#26412;&#36317;&#31163;&#36825;&#20123;&#36229;&#29699;&#20307;&#20013;&#24515;&#30340;&#36317;&#31163;&#20915;&#23450;&#20102;&#24322;&#24120;&#20998;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MCDSVDD&#30340;&#24615;&#33021;&#19982;&#22810;&#20010;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#22825;&#25991;&#20809;&#21464;&#26354;&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;MCDSVDD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing volume of astronomical data generated by modern survey telescopes, automated pipelines and machine learning techniques have become crucial for analyzing and extracting knowledge from these datasets. Anomaly detection, i.e. the task of identifying irregular or unexpected patterns in the data, is a complex challenge in astronomy. In this paper, we propose Multi-Class Deep Support Vector Data Description (MCDSVDD), an extension of the state-of-the-art anomaly detection algorithm One-Class Deep SVDD, specifically designed to handle different inlier categories with distinct data distributions. MCDSVDD uses a neural network to map the data into hyperspheres, where each hypersphere represents a specific inlier category. The distance of each sample from the centers of these hyperspheres determines the anomaly score. We evaluate the effectiveness of MCDSVDD by comparing its performance with several anomaly detection algorithms on a large dataset of astronomical light-curves 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#22768;&#23398;&#27169;&#22411;BioLingual&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#36830;&#25509;&#35821;&#35328;&#21644;&#38899;&#39057;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#36328;&#29289;&#31181;&#35782;&#21035;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#21160;&#29289;&#22768;&#38899;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20061;&#39033;&#20219;&#21153;&#30340;&#26368;&#26032;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04978</link><description>&lt;p&gt;
&#20855;&#26377;&#20154;&#31867;&#35821;&#35328;&#30417;&#30563;&#30340;&#21487;&#36716;&#31227;&#29983;&#29289;&#22768;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transferable Models for Bioacoustics with Human Language Supervision. (arXiv:2308.04978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#22768;&#23398;&#27169;&#22411;BioLingual&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#36830;&#25509;&#35821;&#35328;&#21644;&#38899;&#39057;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#36328;&#29289;&#31181;&#35782;&#21035;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#21160;&#29289;&#22768;&#38899;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20061;&#39033;&#20219;&#21153;&#30340;&#26368;&#26032;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28304;&#22768;&#23398;&#30417;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#26469;&#36861;&#36394;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#21644;&#20154;&#20026;&#27963;&#21160;&#23545;&#29289;&#31181;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20294;&#24403;&#21069;&#30340;&#27169;&#22411;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#36890;&#24120;&#21482;&#28085;&#30422;&#23569;&#25968;&#29289;&#31181;&#65292;&#24182;&#21463;&#21040;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#22768;&#23398;&#27169;&#22411;BioLingual&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#29983;&#29289;&#22768;&#23398;&#26723;&#26696;&#32858;&#21512;&#25104;&#19968;&#20010;&#21517;&#20026;AnimalSpeak&#30340;&#35821;&#38899;-&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21547;&#26377;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#21253;&#21547;&#29289;&#31181;&#12289;&#21457;&#22768;&#19978;&#19979;&#25991;&#21644;&#21160;&#29289;&#34892;&#20026;&#30340;&#20449;&#24687;&#12290;&#22312;&#23545;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#35328;&#21644;&#38899;&#39057;&#34920;&#31034;&#30340;&#36830;&#25509;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36328;&#29289;&#31181;&#35782;&#21035;&#36229;&#36807;&#19968;&#21315;&#31181;&#29289;&#31181;&#30340;&#21483;&#22768;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;&#23436;&#25104;&#29983;&#29289;&#22768;&#23398;&#20219;&#21153;&#65292;&#24182;&#33021;&#20174;&#33258;&#28982;&#25991;&#26412;&#26597;&#35810;&#20013;&#26816;&#32034;&#21160;&#29289;&#30340;&#21457;&#22768;&#24405;&#38899;&#12290;&#22312;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;BioLingual&#22312;&#21160;&#29289;&#22768;&#38899;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20061;&#39033;&#20219;&#21153;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passive acoustic monitoring offers a scalable, non-invasive method for tracking global biodiversity and anthropogenic impacts on species. Although deep learning has become a vital tool for processing this data, current models are inflexible, typically cover only a handful of species, and are limited by data scarcity. In this work, we propose BioLingual, a new model for bioacoustics based on contrastive language-audio pretraining. We first aggregate bioacoustic archives into a language-audio dataset, called AnimalSpeak, with over a million audio-caption pairs holding information on species, vocalization context, and animal behavior. After training on this dataset to connect language and audio representations, our model can identify over a thousand species' calls across taxa, complete bioacoustic tasks zero-shot, and retrieve animal vocalization recordings from natural text queries. When fine-tuned, BioLingual sets a new state-of-the-art on nine tasks in the Benchmark of Animal Sounds. G
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Adversarial ModSecurity&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#23545;&#25239;SQL&#27880;&#20837;&#25915;&#20987;&#30340;&#38450;&#28779;&#22681;&#12290;&#36890;&#36807;&#23558;&#26680;&#24515;&#35268;&#21017;&#38598;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;&#24182;&#38450;&#24481;&#23545;&#25239;&#24615;SQL&#27880;&#20837;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvModSec&#22312;&#35757;&#32451;&#21518;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#36825;&#31867;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2308.04964</link><description>&lt;p&gt;
Adversarial ModSecurity: &#20351;&#29992;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;SQL&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning. (arXiv:2308.04964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04964
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Adversarial ModSecurity&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#23545;&#25239;SQL&#27880;&#20837;&#25915;&#20987;&#30340;&#38450;&#28779;&#22681;&#12290;&#36890;&#36807;&#23558;&#26680;&#24515;&#35268;&#21017;&#38598;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;&#24182;&#38450;&#24481;&#23545;&#25239;&#24615;SQL&#27880;&#20837;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvModSec&#22312;&#35757;&#32451;&#21518;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#36825;&#31867;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ModSecurity&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#26631;&#20934;&#30340;&#24320;&#28304;Web&#24212;&#29992;&#38450;&#28779;&#22681;(WAF)&#65292;&#30001;OWASP&#22522;&#37329;&#20250;&#32500;&#25252;&#12290;&#23427;&#36890;&#36807;&#19982;&#26680;&#24515;&#35268;&#21017;&#38598;&#36827;&#34892;&#21305;&#37197;&#26469;&#26816;&#27979;&#24694;&#24847;&#35831;&#27714;&#65292;&#35782;&#21035;&#20986;&#24120;&#35265;&#30340;&#25915;&#20987;&#27169;&#24335;&#12290;&#27599;&#20010;&#35268;&#21017;&#22312;CRS&#20013;&#37117;&#34987;&#25163;&#21160;&#20998;&#37197;&#19968;&#20010;&#26435;&#37325;&#65292;&#22522;&#20110;&#30456;&#24212;&#25915;&#20987;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#22914;&#26524;&#35302;&#21457;&#35268;&#21017;&#30340;&#26435;&#37325;&#20043;&#21644;&#36229;&#36807;&#32473;&#23450;&#30340;&#38408;&#20540;&#65292;&#23601;&#20250;&#34987;&#26816;&#27979;&#20026;&#24694;&#24847;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#22312;&#26816;&#27979;SQL&#27880;&#20837;&#25915;&#20987;&#26041;&#38754;&#24456;&#19981;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#24448;&#24448;&#20250;&#38459;&#27490;&#35768;&#22810;&#21512;&#27861;&#35831;&#27714;&#65292;&#21516;&#26102;&#36824;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;SQL&#27880;&#20837;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#25925;&#24847;&#25805;&#32437;&#20197;&#36867;&#36991;&#26816;&#27979;&#30340;&#25915;&#20987;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;AdvModSec&#30340;&#24378;&#22823;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#23558;CRS&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#32463;&#36807;&#35757;&#32451;&#20197;&#26816;&#27979;&#23545;&#25239;&#24615;SQL&#27880;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AdvModSec&#22312;&#38024;&#23545;&#35813;&#25915;&#20987;&#30340;&#27969;&#37327;&#19978;&#36827;&#34892;&#35757;&#32451;&#21518;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
ModSecurity is widely recognized as the standard open-source Web Application Firewall (WAF), maintained by the OWASP Foundation. It detects malicious requests by matching them against the Core Rule Set, identifying well-known attack patterns. Each rule in the CRS is manually assigned a weight, based on the severity of the corresponding attack, and a request is detected as malicious if the sum of the weights of the firing rules exceeds a given threshold. In this work, we show that this simple strategy is largely ineffective for detecting SQL injection (SQLi) attacks, as it tends to block many legitimate requests, while also being vulnerable to adversarial SQLi attacks, i.e., attacks intentionally manipulated to evade detection. To overcome these issues, we design a robust machine learning model, named AdvModSec, which uses the CRS rules as input features, and it is trained to detect adversarial SQLi attacks. Our experiments show that AdvModSec, being trained on the traffic directed towa
&lt;/p&gt;</description></item><item><title>CasCIFF&#26159;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#32593;&#32476;&#20013;&#32423;&#32852;&#39044;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#36328;&#39046;&#22495;&#20449;&#24687;&#34701;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#29992;&#25143;&#23646;&#24615;&#34920;&#31034;&#19981;&#20934;&#30830;&#12289;&#24573;&#35270;&#28608;&#27963;&#26102;&#38388;&#21644;&#26080;&#27861;&#25972;&#21512;&#26102;&#38388;&#21644;&#32467;&#26500;&#22240;&#32032;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04961</link><description>&lt;p&gt;
CasCIFF: &#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#32593;&#32476;&#20013;&#32423;&#32852;&#39044;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#36328;&#39046;&#22495;&#20449;&#24687;&#34701;&#21512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
CasCIFF: A Cross-Domain Information Fusion Framework Tailored for Cascade Prediction in Social Networks. (arXiv:2308.04961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04961
&lt;/p&gt;
&lt;p&gt;
CasCIFF&#26159;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#32593;&#32476;&#20013;&#32423;&#32852;&#39044;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#36328;&#39046;&#22495;&#20449;&#24687;&#34701;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#29992;&#25143;&#23646;&#24615;&#34920;&#31034;&#19981;&#20934;&#30830;&#12289;&#24573;&#35270;&#28608;&#27963;&#26102;&#38388;&#21644;&#26080;&#27861;&#25972;&#21512;&#26102;&#38388;&#21644;&#32467;&#26500;&#22240;&#32032;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#19977;&#31867;&#65306;&#29305;&#24449;&#39537;&#21160;&#26041;&#27861;&#12289;&#28857;&#36807;&#31243;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20197;&#20854;&#21331;&#36234;&#30340;&#23398;&#20064;&#21644;&#34920;&#31034;&#33021;&#21147;&#26469;&#32531;&#35299;&#20854;&#20182;&#26041;&#27861;&#30340;&#22266;&#26377;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20173;&#38754;&#20020;&#19968;&#20123;&#25345;&#20037;&#24615;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#23384;&#22312;&#20266;&#31881;&#19997;&#21644;&#22797;&#26434;&#30340;&#32593;&#32476;&#37197;&#32622;&#31561;&#22240;&#32032;&#65292;&#20934;&#30830;&#34920;&#31034;&#29992;&#25143;&#23646;&#24615;&#20173;&#28982;&#26377;&#38382;&#39064;&#12290;&#20043;&#21069;&#19987;&#27880;&#20110;&#29992;&#25143;&#28608;&#27963;&#39034;&#24207;&#30340;&#31639;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#28608;&#27963;&#26102;&#38388;&#25552;&#20379;&#30340;&#20016;&#23500;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#26080;&#27861;&#20840;&#38754;&#22320;&#25972;&#21512;&#26102;&#38388;&#21644;&#32467;&#26500;&#26041;&#38754;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#38169;&#36807;&#20102;&#20449;&#24687;&#32423;&#32852;&#20013;&#22266;&#26377;&#30340;&#24494;&#22937;&#20256;&#25773;&#36235;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#20449;&#24687;&#34701;&#21512;&#26694;&#26550;&#65288;CasCIFF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for information cascade prediction fall into three main categories: feature-driven methods, point process-based methods, and deep learning-based methods. Among them, deep learning-based methods, characterized by its superior learning and representation capabilities, mitigates the shortcomings inherent of the other methods. However, current deep learning methods still face several persistent challenges. In particular, accurate representation of user attributes remains problematic due to factors such as fake followers and complex network configurations. Previous algorithms that focus on the sequential order of user activations often neglect the rich insights offered by activation timing. Furthermore, these techniques often fail to holistically integrate temporal and structural aspects, thus missing the nuanced propagation trends inherent in information cascades.To address these issues, we propose the Cross-Domain Information Fusion Framework (CasCIFF), which is tailor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28304;&#20998;&#31163;&#21644;&#23545;&#25239;&#23398;&#20064;&#30340;&#38899;&#39057;&#38544;&#31169;&#20445;&#25252;&#30340;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#65292;&#25552;&#39640;&#35821;&#38899;&#38544;&#31169;&#20445;&#25252;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04960</link><description>&lt;p&gt;
&#20351;&#29992;&#28304;&#20998;&#31163;&#21644;&#40065;&#26834;&#23545;&#25239;&#23398;&#20064;&#30340;&#38899;&#39057;&#38544;&#31169;&#20445;&#25252;&#30340;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Audio Privacy Preservation using Source Separation and Robust Adversarial Learning. (arXiv:2308.04960v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28304;&#20998;&#31163;&#21644;&#23545;&#25239;&#23398;&#20064;&#30340;&#38899;&#39057;&#38544;&#31169;&#20445;&#25252;&#30340;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#65292;&#25552;&#39640;&#35821;&#38899;&#38544;&#31169;&#20445;&#25252;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#38544;&#31169;&#20445;&#25252;&#19968;&#30452;&#26159;&#26234;&#33021;&#22768;&#23398;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#27880;&#28857;&#65292;&#20854;&#20013;&#35821;&#38899;&#21487;&#20197;&#34987;&#34987;&#21160;&#22320;&#35760;&#24405;&#19979;&#26469;&#65292;&#21516;&#26102;&#20063;&#35760;&#24405;&#19979;&#20102;&#31995;&#32479;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#30446;&#26631;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#38544;&#31169;&#20445;&#25252;&#20013;&#24120;&#29992;&#30340;&#20004;&#31181;&#26041;&#27861;&#30340;&#25972;&#21512;&#65306;&#28304;&#20998;&#31163;&#21644;&#23545;&#25239;&#24615;&#34920;&#24449;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#23398;&#20064;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#65292;&#20197;&#38450;&#27490;&#21306;&#20998;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#35760;&#24405;&#12290;&#26368;&#21021;&#65292;&#28304;&#20998;&#31163;&#32593;&#32476;&#36807;&#28388;&#25481;&#20102;&#19968;&#20123;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#65292;&#32780;&#22312;&#23545;&#25239;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#31995;&#32479;&#20250;&#22312;&#36807;&#28388;&#21518;&#30340;&#20449;&#21495;&#19978;&#23398;&#20064;&#38544;&#31169;&#20445;&#25252;&#34920;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27809;&#26377;&#28304;&#20998;&#31163;&#12289;&#27809;&#26377;&#23545;&#25239;&#23398;&#20064;&#20197;&#21450;&#20004;&#32773;&#37117;&#27809;&#26377;&#30340;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#31995;&#32479;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy preservation has long been a concern in smart acoustic monitoring systems, where speech can be passively recorded along with a target signal in the system's operating environment. In this study, we propose the integration of two commonly used approaches in privacy preservation: source separation and adversarial representation learning. The proposed system learns the latent representation of audio recordings such that it prevents differentiating between speech and non-speech recordings. Initially, the source separation network filters out some of the privacy-sensitive data, and during the adversarial learning process, the system will learn privacy-preserving representation on the filtered signal. We demonstrate the effectiveness of our proposed method by comparing our method against systems without source separation, without adversarial learning, and without both. Overall, our results suggest that the proposed system can significantly improve speech privacy preservation compared
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36895;&#24230;&#21644;&#22402;&#30452;&#26426;&#21160;&#25163;&#27573;&#65292;&#22312;&#39640;&#23494;&#24230;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#33258;&#20027;&#30340;&#33258;&#25105;&#20998;&#31163;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#20027;&#20998;&#31163;&#20445;&#38556;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.04958</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#33258;&#20027;&#20998;&#31163;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks. (arXiv:2308.04958v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36895;&#24230;&#21644;&#22402;&#30452;&#26426;&#21160;&#25163;&#27573;&#65292;&#22312;&#39640;&#23494;&#24230;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#33258;&#20027;&#30340;&#33258;&#25105;&#20998;&#31163;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#20027;&#20998;&#31163;&#20445;&#38556;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#31354;&#20013;&#31227;&#21160;&#65288;AAM&#65289;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#39640;&#25928;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#21033;&#29992;&#36710;&#36742;&#33258;&#20027;&#24615;&#21644;&#30005;&#21160;&#39134;&#26426;&#65292;&#22312;&#20043;&#21069;&#26410;&#24471;&#21040;&#20805;&#20998;&#26381;&#21153;&#30340;&#24066;&#22330;&#20043;&#38388;&#25552;&#20379;&#36234;&#26469;&#36234;&#33258;&#20027;&#30340;&#20132;&#36890;&#12290;&#36890;&#36807;&#39640;&#23494;&#24230;&#29615;&#22659;&#20013;&#20302;&#31354;&#39134;&#34892;&#22120;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#23548;&#33322;&#65292;&#38656;&#35201;&#25972;&#21512;&#22823;&#37327;&#22797;&#26434;&#35266;&#27979;&#25968;&#25454;&#65292;&#22914;&#30417;&#35270;&#65292;&#36710;&#36742;&#21160;&#21147;&#23398;&#30693;&#35782;&#21644;&#22825;&#27668;&#12290;&#22312;&#22788;&#29702;&#21644;&#25512;&#29702;&#36825;&#20123;&#35266;&#27979;&#25968;&#25454;&#26102;&#65292;&#38754;&#20020;&#30528;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#30340;&#22810;&#20010;&#26469;&#28304;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#35201;&#30830;&#20445;&#19982;&#31354;&#22495;&#20013;&#21487;&#21464;&#25968;&#37327;&#30340;&#39134;&#26426;&#21512;&#20316;&#12290;&#36825;&#20123;&#25361;&#25112;&#21152;&#19978;&#38656;&#35201;&#23454;&#26102;&#20570;&#20986;&#23433;&#20840;&#20851;&#38190;&#20915;&#31574;&#30340;&#35201;&#27714;&#65292;&#20351;&#24471;&#20256;&#32479;&#30340;&#20998;&#31163;&#20445;&#38556;&#25216;&#26415;&#26080;&#27861;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AAM&#36208;&#24266;&#20869;&#20351;&#29992;&#36895;&#24230;&#21644;&#22402;&#30452;&#26426;&#21160;&#25163;&#27573;&#25552;&#20379;&#33258;&#20027;&#30340;&#33258;&#25105;&#20998;&#31163;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced Air Mobility (AAM) introduces a new, efficient mode of transportation with the use of vehicle autonomy and electrified aircraft to provide increasingly autonomous transportation between previously underserved markets. Safe and efficient navigation of low altitude aircraft through highly dense environments requires the integration of a multitude of complex observations, such as surveillance, knowledge of vehicle dynamics, and weather. The processing and reasoning on these observations pose challenges due to the various sources of uncertainty in the information while ensuring cooperation with a variable number of aircraft in the airspace. These challenges coupled with the requirement to make safety-critical decisions in real-time rule out the use of conventional separation assurance techniques. We present a decentralized reinforcement learning framework to provide autonomous self-separation capabilities within AAM corridors with the use of speed and vertical maneuvers. The probl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;BERT&#12289;ALBERT&#21644;RoBERTa&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;BERT&#65289;&#22312;&#22788;&#29702;&#25991;&#26412;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#19981;&#21516;&#30740;&#31350;&#23545;&#24615;&#33021;&#35780;&#20272;&#21644;&#32467;&#35770;&#30340;&#23454;&#29616;&#26041;&#27861;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.04950</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#65288;BERT&#65292;ALBERT&#21644;RoBERTa&#65289;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection. (arXiv:2308.04950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;BERT&#12289;ALBERT&#21644;RoBERTa&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;BERT&#65289;&#22312;&#22788;&#29702;&#25991;&#26412;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#19981;&#21516;&#30740;&#31350;&#23545;&#24615;&#33021;&#35780;&#20272;&#21644;&#32467;&#35770;&#30340;&#23454;&#29616;&#26041;&#27861;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26159;&#19968;&#31181;&#23186;&#20307;&#26684;&#24335;&#30340;&#34394;&#20551;&#26448;&#26009;&#65292;&#20294;&#27809;&#26377;&#32463;&#36807;&#26032;&#38395;&#26426;&#26500;&#30340;&#36866;&#24403;&#22788;&#29702;&#12290;&#36825;&#20123;&#34394;&#20551;&#26448;&#26009;&#21487;&#33021;&#20250;&#28608;&#36215;&#25110;&#35837;&#35876;&#37325;&#35201;&#23454;&#20307;&#25110;&#20010;&#20154;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#21019;&#20316;&#32773;&#30340;&#20010;&#20154;&#21033;&#30410;&#65292;&#32473;&#31038;&#20250;&#24102;&#26469;&#38382;&#39064;&#12290;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#26377;&#38480;&#21644;&#26102;&#38388;&#38480;&#21046;&#65292;&#21306;&#20998;&#20551;&#26032;&#38395;&#21644;&#30495;&#26032;&#38395;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26681;&#25454;&#35843;&#26597;&#65292;&#21463;&#21040;&#35875;&#35328;&#21644;&#20449;&#24687;&#35823;&#23548;&#30340;&#19977;&#20010;&#22320;&#21306;&#26368;&#22810;&#30340;&#26159;&#19975;&#20025;&#29305;&#21306;&#12289;&#38597;&#21152;&#36798;&#29305;&#21306;&#21644;&#35199;&#29226;&#21703;&#12290;Transformer&#27169;&#22411;&#26159;&#25351;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#12290;Transformer&#27169;&#22411;&#36890;&#36807;&#24378;&#22823;&#30340;&#27880;&#24847;&#26426;&#21046;&#24182;&#34892;&#22788;&#29702;&#25991;&#26412;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#35789;&#34920;&#31034;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#31181;&#21517;&#20026;BERT&#30340;Transformer&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#38750;Transformer&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#24615;&#33021;&#35780;&#20272;&#21644;&#32467;bonclusion&#30340;&#23454;&#29616;&#26041;&#27861;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake news is fake material in a news media format but is not processed properly by news agencies. The fake material can provoke or defame significant entities or individuals or potentially even for the personal interests of the creators, causing problems for society. Distinguishing fake news and real news is challenging due to limited of domain knowledge and time constraints. According to the survey, the top three areas most exposed to hoaxes and misinformation by residents are in Banten, DKI Jakarta and West Java. The model of transformers is referring to an approach in the field of artificial intelligence (AI) in natural language processing utilizing the deep learning architectures. Transformers exercise a powerful attention mechanism to process text in parallel and produce rich and contextual word representations. A previous study indicates a superior performance of a transformer model known as BERT over and above non transformer approach. However, some studies suggest the performan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#32780;&#20840;&#38754;&#22320;&#25551;&#36848;&#20102;&#20174;&#19981;&#21516;&#26469;&#28304;&#33719;&#24471;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#24110;&#21161;&#29702;&#35299;&#32929;&#31080;&#24066;&#22330;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04947</link><description>&lt;p&gt;
&#33719;&#24471;&#21644;&#25972;&#21512;&#30693;&#35782;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Methods for Acquiring and Incorporating Knowledge into Stock Price Prediction: A Survey. (arXiv:2308.04947v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04947
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#32780;&#20840;&#38754;&#22320;&#25551;&#36848;&#20102;&#20174;&#19981;&#21516;&#26469;&#28304;&#33719;&#24471;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#24110;&#21161;&#29702;&#35299;&#32929;&#31080;&#24066;&#22330;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#22240;&#20026;&#32929;&#31080;&#24066;&#22330;&#30340;&#22266;&#26377;&#27874;&#21160;&#24615;&#21644;&#38750;&#32447;&#24615;&#24615;&#36136;&#12290;&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#29702;&#35299;&#32929;&#31080;&#24066;&#22330;&#30340;&#30693;&#35782;&#22686;&#24378;&#22411;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#22312;&#20174;&#22806;&#37096;&#30693;&#35782;&#31867;&#22411;&#30340;&#35282;&#24230;&#31995;&#32479;&#22320;&#32508;&#21512;&#20197;&#24448;&#30740;&#31350;&#26041;&#38754;&#65292;&#23398;&#26415;&#20316;&#21697;&#30340;&#31232;&#32570;&#24615;&#23384;&#22312;&#19968;&#23450;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22806;&#37096;&#30693;&#35782;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#25968;&#25454;&#32467;&#26500;&#24314;&#27169;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#38750;&#22270;&#24418;&#21270;&#26684;&#24335;&#21644;&#22270;&#24418;&#21270;&#26684;&#24335;&#20004;&#31867;&#65306;1) &#38750;&#22270;&#24418;&#21270;&#30693;&#35782;&#25429;&#33719;&#19982;&#20010;&#21035;&#32929;&#31080;&#23494;&#20999;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#22810;&#23186;&#20307;&#25551;&#36848;&#65307;2) &#22270;&#24418;&#21270;&#30693;&#35782;&#25429;&#33719;&#32929;&#31080;&#24066;&#22330;&#20013;&#30456;&#20114;&#20851;&#32852;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20449;&#24687;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#26088;&#22312;&#31995;&#32479;&#32780;&#20840;&#38754;&#22320;&#25551;&#36848;&#20174;&#19981;&#21516;&#26469;&#28304;&#33719;&#21462;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting stock prices presents a challenging research problem due to the inherent volatility and non-linear nature of the stock market. In recent years, knowledge-enhanced stock price prediction methods have shown groundbreaking results by utilizing external knowledge to understand the stock market. Despite the importance of these methods, there is a scarcity of scholarly works that systematically synthesize previous studies from the perspective of external knowledge types. Specifically, the external knowledge can be modeled in different data structures, which we group into non-graph-based formats and graph-based formats: 1) non-graph-based knowledge captures contextual information and multimedia descriptions specifically associated with an individual stock; 2) graph-based knowledge captures interconnected and interdependent information in the stock market. This survey paper aims to provide a systematic and comprehensive description of methods for acquiring external knowledge from va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#20445;&#25252;&#33410;&#28857;&#20449;&#24687;&#30340;&#38544;&#31169;&#12290;&#20854;&#20013;&#65292;&#26681;&#25454;&#25299;&#25169;&#32467;&#26500;&#21644;&#20013;&#24515;&#24615;&#20449;&#24687;&#25512;&#26029;&#33410;&#28857;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#33410;&#28857;&#37325;&#35201;&#24615;&#32423;&#21035;&#23545;&#37051;&#22495;&#32858;&#21512;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2308.04943</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#35201;&#24615;&#20998;&#32423;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#24046;&#20998;&#38544;&#31169;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Graph Neural Network with Importance-Grained Noise Adaption. (arXiv:2308.04943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#20445;&#25252;&#33410;&#28857;&#20449;&#24687;&#30340;&#38544;&#31169;&#12290;&#20854;&#20013;&#65292;&#26681;&#25454;&#25299;&#25169;&#32467;&#26500;&#21644;&#20013;&#24515;&#24615;&#20449;&#24687;&#25512;&#26029;&#33410;&#28857;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#33410;&#28857;&#37325;&#35201;&#24615;&#32423;&#21035;&#23545;&#37051;&#22495;&#32858;&#21512;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#23545;&#38544;&#31169;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#20445;&#25252;&#33410;&#28857;&#20449;&#24687;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#37051;&#22495;&#21644;&#20013;&#24515;&#24615;&#20449;&#24687;&#26469;&#25512;&#26029;&#26410;&#30693;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#38544;&#31169;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#33410;&#28857;&#37325;&#35201;&#24615;&#32423;&#21035;&#26469;&#23545;&#37051;&#22495;&#32858;&#21512;&#36827;&#34892;&#25200;&#21160;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#26377;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#25252;&#33410;&#28857;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) with differential privacy have been proposed to preserve graph privacy when nodes represent personal and sensitive information. However, the existing methods ignore that nodes with different importance may yield diverse privacy demands, which may lead to over-protect some nodes and decrease model utility. In this paper, we study the problem of importance-grained privacy, where nodes contain personal data that need to be kept private but are critical for training a GNN. We propose NAP-GNN, a node-importance-grained privacy-preserving GNN algorithm with privacy guarantees based on adaptive differential privacy to safeguard node information. First, we propose a Topology-based Node Importance Estimation (TNIE) method to infer unknown node importance with neighborhood and centrality awareness. Second, an adaptive private aggregation method is proposed to perturb neighborhood aggregation from node-importance-grain. Third, we propose to privately train a graph lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#20351;&#29992;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#36827;&#34892;&#36890;&#20449;&#23398;&#20064;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04938</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#36827;&#34892;&#36890;&#20449;&#23398;&#20064;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An In-Depth Analysis of Discretization Methods for Communication Learning using Backpropagation with Multi-Agent Reinforcement Learning. (arXiv:2308.04938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#20351;&#29992;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#36827;&#34892;&#36890;&#20449;&#23398;&#20064;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26234;&#33021;&#20307;&#26080;&#27861;&#35266;&#23519;&#21040;&#23436;&#25972;&#30340;&#29615;&#22659;&#29366;&#24577;&#26102;&#65292;&#36890;&#20449;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20801;&#35768;&#26234;&#33021;&#20307;&#20043;&#38388;&#23398;&#20064;&#36890;&#20449;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36890;&#20449;&#36890;&#36947;&#65292;&#20197;&#20415;&#26799;&#24230;&#33021;&#22815;&#20316;&#20026;&#21453;&#39304;&#27969;&#21160;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#24819;&#35201;&#20351;&#29992;&#31163;&#25955;&#28040;&#24687;&#26469;&#20943;&#23567;&#28040;&#24687;&#30340;&#22823;&#23567;&#26102;&#65292;&#36825;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#26799;&#24230;&#19981;&#33021;&#36890;&#36807;&#31163;&#25955;&#30340;&#36890;&#20449;&#36890;&#36947;&#20256;&#25773;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#36890;&#20449;&#23398;&#20064;&#26550;&#26500;&#21644;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20351;&#24471;&#24456;&#38590;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#26799;&#24230;&#36827;&#34892;&#36890;&#20449;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;COMA-DIAL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is crucial in multi-agent reinforcement learning when agents are not able to observe the full state of the environment. The most common approach to allow learned communication between agents is the use of a differentiable communication channel that allows gradients to flow between agents as a form of feedback. However, this is challenging when we want to use discrete messages to reduce the message size, since gradients cannot flow through a discrete communication channel. Previous work proposed methods to deal with this problem. However, these methods are tested in different communication learning architectures and environments, making it hard to compare them. In this paper, we compare several state-of-the-art discretization methods as well as a novel approach. We do this comparison in the context of communication learning using gradients from other agents and perform tests on several environments. In addition, we present COMA-DIAL, a communication learning approach based
&lt;/p&gt;</description></item><item><title>JEDI&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#19987;&#23478;&#30693;&#35782;&#36827;&#34892;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#36328;&#25968;&#25454;&#38598;&#27867;&#21270;&#21644;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04934</link><description>&lt;p&gt;
JEDI&#65306;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#21322;&#30417;&#30563;&#22810;&#25968;&#25454;&#38598;&#23398;&#29983;-&#25945;&#24072;&#32852;&#21512;&#19987;&#23478;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition. (arXiv:2308.04934v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04934
&lt;/p&gt;
&lt;p&gt;
JEDI&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#19987;&#23478;&#30693;&#35782;&#36827;&#34892;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#36328;&#25968;&#25454;&#38598;&#27867;&#21270;&#21644;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;JEDI&#65292;&#19968;&#31181;&#22810;&#25968;&#25454;&#38598;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#32467;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#20197;&#35757;&#32451;&#21644;&#25552;&#39640;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#23548;&#33268;&#30340;&#30417;&#30563;&#35757;&#32451;&#38480;&#21046;&#12290;&#25105;&#20204;&#20174;&#20219;&#24847;&#25968;&#37327;&#30340;&#19987;&#23478;&#24320;&#22987;&#65292;&#20182;&#20204;&#22312;&#21508;&#33258;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24418;&#25104;&#21021;&#22987;&#30340;&#23398;&#29983;&#27169;&#22411;&#38598;&#21512;&#12290;&#25945;&#24072;&#31435;&#21363;&#36890;&#36807;&#36830;&#25509;&#23398;&#29983;&#20498;&#25968;&#31532;&#20108;&#23618;&#30340;&#29305;&#24449;&#34920;&#31034;&#27966;&#29983;&#20986;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23398;&#29983;-&#25945;&#24072;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#20013;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#27169;&#22411;&#65292;&#30452;&#33267;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#39640;&#25928;&#26041;&#27861;&#34920;&#26126;&#65292;&#23398;&#29983;&#21644;&#25945;&#24072;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose JEDI, a multi-dataset semi-supervised learning method, which efficiently combines knowledge from multiple experts, learned on different datasets, to train and improve the performance of individual, per dataset, student models. Our approach achieves this by addressing two important problems in current machine learning research: generalization across datasets and limitations of supervised training due to scarcity of labeled data. We start with an arbitrary number of experts, pretrained on their own specific dataset, which form the initial set of student models. The teachers are immediately derived by concatenating the feature representations from the penultimate layers of the students. We then train all models in a student-teacher semi-supervised learning scenario until convergence. In our efficient approach, student-teacher training is carried out jointly and end-to-end, showing that both students and teachers improve their generalization capacity during training. We validate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#20896;&#29366;CT&#34880;&#31649;&#36896;&#24433;&#65288;CCTA&#65289;&#25195;&#25551;&#20013;&#39044;&#27979;&#20896;&#29366;&#21160;&#33033;&#30340;&#20998;&#25968;&#27969;&#39044;&#27979;&#65288;FFR&#65289;&#65292;&#36991;&#20813;&#20102;&#20405;&#20837;&#24615;&#27979;&#37327;&#65292;&#24182;&#19988;&#19981;&#20165;&#39044;&#27979;&#20102;&#27599;&#20010;&#20896;&#29366;&#21160;&#33033;&#30340;FFR&#20540;&#65292;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#29421;&#31364;&#20301;&#32622;&#21644;&#27835;&#30103;&#31574;&#30053;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.04923</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20896;&#29366;&#21160;&#33033;&#20998;&#25968;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Prediction of Fractional Flow Reserve along the Coronary Artery. (arXiv:2308.04923v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#20896;&#29366;CT&#34880;&#31649;&#36896;&#24433;&#65288;CCTA&#65289;&#25195;&#25551;&#20013;&#39044;&#27979;&#20896;&#29366;&#21160;&#33033;&#30340;&#20998;&#25968;&#27969;&#39044;&#27979;&#65288;FFR&#65289;&#65292;&#36991;&#20813;&#20102;&#20405;&#20837;&#24615;&#27979;&#37327;&#65292;&#24182;&#19988;&#19981;&#20165;&#39044;&#27979;&#20102;&#27599;&#20010;&#20896;&#29366;&#21160;&#33033;&#30340;FFR&#20540;&#65292;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#29421;&#31364;&#20301;&#32622;&#21644;&#27835;&#30103;&#31574;&#30053;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20896;&#29366;&#21160;&#33033;&#20869;&#26001;&#22359;&#30340;&#22534;&#31215;&#24341;&#36215;&#30340;&#21151;&#33021;&#24615;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#21487;&#33021;&#23548;&#33268;&#20896;&#29366;&#21160;&#33033;&#31649;&#33108;&#29421;&#31364;&#65292;&#21363;&#20896;&#29366;&#21160;&#33033;&#29421;&#31364;&#65292;&#20174;&#32780;&#26174;&#33879;&#38459;&#30861;&#34880;&#28082;&#27969;&#21521;&#24515;&#32908;&#12290;&#30446;&#21069;&#29992;&#20110;&#30830;&#23450;&#21151;&#33021;&#24615;&#29421;&#31364;&#23384;&#22312;&#30340;&#21442;&#32771;&#26159;&#20405;&#20837;&#24615;&#30340;&#20998;&#25968;&#27969;&#39044;&#27979;&#65288;FFR&#65289;&#27979;&#37327;&#12290;&#20026;&#20102;&#36991;&#20813;&#20405;&#20837;&#24615;&#27979;&#37327;&#65292;&#38750;&#20405;&#20837;&#24615;&#30340;&#20174;&#20896;&#29366;CT&#34880;&#31649;&#36896;&#24433;&#65288;CCTA&#65289;&#39044;&#27979;FFR&#30340;&#26041;&#27861;&#36880;&#28176;&#20986;&#29616;&#12290;&#20026;&#27492;&#65292;&#29305;&#24449;&#20026;&#24555;&#36895;&#25512;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26085;&#36235;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#39044;&#27979;&#27599;&#20010;&#20896;&#29366;&#21160;&#33033;&#30340;&#21333;&#20010;FFR&#20540;&#65292;&#21363;&#23427;&#20204;&#19981;&#25552;&#20379;&#20851;&#20110;&#29421;&#31364;&#20301;&#32622;&#25110;&#27835;&#30103;&#31574;&#30053;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;CCTA&#25195;&#25551;&#20013;&#39044;&#27979;&#27839;&#20896;&#29366;&#21160;&#33033;&#30340;FFR&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;110&#21517;&#24739;&#32773;&#36827;&#34892;&#30340;112&#26465;&#20896;&#29366;&#21160;&#33033;&#20405;&#20837;&#24615;FFR&#22238;&#25289;&#27979;&#37327;&#30340;CCTA&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functionally significant coronary artery disease (CAD) is caused by plaque buildup in the coronary arteries, potentially leading to narrowing of the arterial lumen, i.e. coronary stenosis, that significantly obstructs blood flow to the myocardium. The current reference for establishing the presence of a functionally significant stenosis is invasive fractional flow reserve (FFR) measurement. To avoid invasive measurements, non-invasive prediction of FFR from coronary CT angiography (CCTA) has emerged. For this, machine learning approaches, characterized by fast inference, are increasingly developed. However, these methods predict a single FFR value per artery i.e. they don't provide information about the stenosis location or treatment strategy. We propose a deep learning-based method to predict the FFR along the artery from CCTA scans. This study includes CCTA images of 110 patients who underwent invasive FFR pullback measurement in 112 arteries. First, a multi planar reconstruction (MP
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCC&#30340;&#23454;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#30340;&#25317;&#22622;&#25511;&#21046;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#24555;&#36895;&#21644;&#31361;&#28982;&#30340;&#32593;&#32476;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04905</link><description>&lt;p&gt;
GraphCC: &#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#30340;&#23454;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GraphCC: A Practical Graph Learning-based Approach to Congestion Control in Datacenters. (arXiv:2308.04905v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCC&#30340;&#23454;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#30340;&#25317;&#22622;&#25511;&#21046;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#24555;&#36895;&#21644;&#31361;&#28982;&#30340;&#32593;&#32476;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#22622;&#25511;&#21046;&#65288;CC&#65289;&#22312;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#32593;&#32476;&#65288;DCN&#65289;&#20013;&#30340;&#27969;&#37327;&#26041;&#38754;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#12290;&#30446;&#21069;&#65292;DCN&#20027;&#35201;&#23454;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;CC&#21327;&#35758;&#65306;DCTCP&#21644;DCQCN&#12290;&#36825;&#20004;&#20010;&#21327;&#35758;&#21450;&#20854;&#20027;&#35201;&#21464;&#20307;&#37117;&#22522;&#20110;&#26174;&#24335;&#25317;&#22622;&#36890;&#30693;&#65288;ECN&#65289;&#65292;&#20854;&#20013;&#20013;&#38388;&#20132;&#25442;&#26426;&#22312;&#26816;&#27979;&#21040;&#25317;&#22622;&#26102;&#26631;&#35760;&#25968;&#25454;&#21253;&#12290;ECN&#37197;&#32622;&#22240;&#27492;&#25104;&#20026;CC&#21327;&#35758;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#29616;&#22312;&#65292;&#32593;&#32476;&#19987;&#23478;&#35774;&#23450;&#38745;&#24577;ECN&#21442;&#25968;&#65292;&#31934;&#24515;&#36873;&#25321;&#20197;&#20248;&#21270;&#32593;&#32476;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#20170;&#30340;&#39640;&#36895;DCN&#32463;&#21382;&#24555;&#36895;&#21644;&#31361;&#28982;&#30340;&#21464;&#21270;&#65292;&#20005;&#37325;&#25913;&#21464;&#20102;&#32593;&#32476;&#29366;&#24577;&#65288;&#20363;&#22914;&#65292;&#21160;&#24577;&#27969;&#37327;&#24037;&#20316;&#36127;&#36733;&#65292;incast&#20107;&#20214;&#65292;&#25925;&#38556;&#65289;&#12290;&#36825;&#23548;&#33268;&#20102;&#20302;&#25928;&#21033;&#29992;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCC&#30340;&#26032;&#39062;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20869;CC&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Congestion Control (CC) plays a fundamental role in optimizing traffic in Data Center Networks (DCN). Currently, DCNs mainly implement two main CC protocols: DCTCP and DCQCN. Both protocols -- and their main variants -- are based on Explicit Congestion Notification (ECN), where intermediate switches mark packets when they detect congestion. The ECN configuration is thus a crucial aspect on the performance of CC protocols. Nowadays, network experts set static ECN parameters carefully selected to optimize the average network performance. However, today's high-speed DCNs experience quick and abrupt changes that severely change the network state (e.g., dynamic traffic workloads, incast events, failures). This leads to under-utilization and sub-optimal performance. This paper presents GraphCC, a novel Machine Learning-based framework for in-network CC optimization. Our distributed solution relies on a novel combination of Multi-agent Reinforcement Learning (MARL) and Graph Neural Networks (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29420;&#31435;&#26041;&#31243;&#21457;&#29616;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#27491;&#30830;&#26041;&#31243;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#21457;&#29616;&#26041;&#31243;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#22312;&#27809;&#26377;&#20808;&#39564;&#26041;&#31243;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#21457;&#29616;&#26041;&#31243;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04901</link><description>&lt;p&gt;
&#36808;&#21521;&#30495;&#27491;&#30340;&#24494;&#20998;&#26041;&#31243;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards true discovery of the differential equations. (arXiv:2308.04901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29420;&#31435;&#26041;&#31243;&#21457;&#29616;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#27491;&#30830;&#26041;&#31243;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#21457;&#29616;&#26041;&#31243;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#22312;&#27809;&#26377;&#20808;&#39564;&#26041;&#31243;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#21457;&#29616;&#26041;&#31243;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#20998;&#26041;&#31243;&#21457;&#29616;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#29992;&#20110;&#24320;&#21457;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#33258;&#28982;&#30456;&#20851;&#30340;&#24212;&#29992;&#20013;&#12290;&#36890;&#36807;&#24039;&#22937;&#22320;&#32467;&#21512;&#36816;&#21160;&#26041;&#31243;&#30340;&#19968;&#33324;&#21442;&#25968;&#24418;&#24335;&#21644;&#21512;&#36866;&#30340;&#24494;&#20998;&#39033;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#26041;&#31243;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29420;&#31435;&#26041;&#31243;&#21457;&#29616;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#24037;&#20855;&#65292;&#28040;&#38500;&#20102;&#23545;&#26041;&#31243;&#24418;&#24335;&#20551;&#35774;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#37325;&#28857;&#35299;&#20915;&#20102;&#22312;&#27491;&#30830;&#26041;&#31243;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#21457;&#29616;&#26041;&#31243;&#30340;&#36866;&#24403;&#24615;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#22312;&#27809;&#26377;&#20808;&#39564;&#26041;&#31243;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#21457;&#29616;&#26041;&#31243;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential equation discovery, a machine learning subfield, is used to develop interpretable models, particularly in nature-related applications. By expertly incorporating the general parametric form of the equation of motion and appropriate differential terms, algorithms can autonomously uncover equations from data. This paper explores the prerequisites and tools for independent equation discovery without expert input, eliminating the need for equation form assumptions. We focus on addressing the challenge of assessing the adequacy of discovered equations when the correct equation is unknown, with the aim of providing insights for reliable equation discovery without prior knowledge of the equation form.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21382;&#21490;&#36719;&#20214;&#20379;&#24212;&#38142;&#23433;&#20840;&#22833;&#36133;&#36827;&#34892;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20854;&#33021;&#21147;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#65292;&#21487;&#20197;&#38477;&#20302;&#25104;&#26412;&#24182;&#23454;&#29616;&#23545;&#26356;&#22810;&#22833;&#36133;&#26696;&#20363;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.04898</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#36719;&#20214;&#20379;&#24212;&#38142;&#23433;&#20840;&#22833;&#36133;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures. (arXiv:2308.04898v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21382;&#21490;&#36719;&#20214;&#20379;&#24212;&#38142;&#23433;&#20840;&#22833;&#36133;&#36827;&#34892;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20854;&#33021;&#21147;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#65292;&#21487;&#20197;&#38477;&#20302;&#25104;&#26412;&#24182;&#23454;&#29616;&#23545;&#26356;&#22810;&#22833;&#36133;&#26696;&#20363;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25105;&#20204;&#36234;&#26469;&#36234;&#20381;&#36182;&#36719;&#20214;&#31995;&#32479;&#65292;&#36719;&#20214;&#20379;&#24212;&#38142;&#34987;&#25915;&#30772;&#30340;&#21518;&#26524;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#20687;SolarWinds&#21644;ShadowHammer&#36825;&#26679;&#30340;&#39640;&#35843;&#32593;&#32476;&#25915;&#20987;&#23548;&#33268;&#20102;&#37325;&#22823;&#30340;&#36130;&#21153;&#21644;&#25968;&#25454;&#25439;&#22833;&#65292;&#20984;&#26174;&#20102;&#21152;&#24378;&#32593;&#32476;&#23433;&#20840;&#30340;&#38656;&#27714;&#12290;&#38450;&#27490;&#26410;&#26469;&#30340;&#30772;&#22351;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#30740;&#31350;&#36807;&#21435;&#30340;&#22833;&#36133;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#38405;&#35835;&#21644;&#24635;&#32467;&#25253;&#21578;&#12290;&#33258;&#21160;&#21270;&#30340;&#25903;&#25345;&#21487;&#20197;&#38477;&#20302;&#25104;&#26412;&#24182;&#20801;&#35768;&#20998;&#26512;&#26356;&#22810;&#30340;&#22833;&#36133;&#26696;&#20363;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#22914;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#29992;&#26469;&#36741;&#21161;&#20998;&#26512;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20998;&#26512;&#21382;&#21490;&#36719;&#20214;&#20379;&#24212;&#38142;&#36829;&#35268;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;LLM&#22797;&#21046;&#20102;Cloud Native Computing Foundation&#65288;CNCF&#65289;&#25104;&#21592;&#23545;69&#20010;&#36719;&#20214;&#20379;&#24212;&#38142;&#23433;&#20840;&#22833;&#36133;&#30340;&#25163;&#21160;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#29616;&#20195;&#25968;&#25454;&#26234;&#33021;&#23454;&#36341;&#65292;&#23545;&#20110;&#20225;&#19994;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#25968;&#25454;&#31185;&#23398;&#39033;&#30446;&#22833;&#36133;&#30340;&#21407;&#22240;&#20027;&#35201;&#22312;&#20110;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12289;&#31639;&#27861;&#21644;&#35745;&#31639;&#33021;&#21147;&#25110;&#22522;&#30784;&#35774;&#26045;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2308.04896</link><description>&lt;p&gt;
&#20026;&#20309;&#25968;&#25454;&#31185;&#23398;&#39033;&#30446;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Data Science Projects Fail. (arXiv:2308.04896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04896
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#29616;&#20195;&#25968;&#25454;&#26234;&#33021;&#23454;&#36341;&#65292;&#23545;&#20110;&#20225;&#19994;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#25968;&#25454;&#31185;&#23398;&#39033;&#30446;&#22833;&#36133;&#30340;&#21407;&#22240;&#20027;&#35201;&#22312;&#20110;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12289;&#31639;&#27861;&#21644;&#35745;&#31639;&#33021;&#21147;&#25110;&#22522;&#30784;&#35774;&#26045;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#29616;&#20195;&#25968;&#25454;&#26234;&#33021;&#23454;&#36341;&#65292;&#26159;&#35768;&#22810;&#20225;&#19994;&#30340;&#26680;&#24515;&#65292;&#24110;&#21161;&#20225;&#19994;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#19994;&#21153;&#25361;&#25112;&#24182;&#24314;&#31435;&#26234;&#33021;&#25112;&#30053;&#12290;&#25968;&#25454;&#31185;&#23398;&#23454;&#36341;&#36824;&#21487;&#20197;&#20351;&#29992;&#31639;&#27861;&#33258;&#21160;&#21270;&#19994;&#21153;&#27969;&#31243;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#20854;&#20182;&#38750;&#30408;&#21033;&#26694;&#26550;&#20013;&#30340;&#20248;&#21183;&#12290;&#23601;&#25968;&#25454;&#31185;&#23398;&#32780;&#35328;&#65292;&#24433;&#21709;&#25968;&#25454;&#31185;&#23398;&#39033;&#30446;&#26377;&#25928;&#32467;&#26524;&#30340;&#19977;&#20010;&#20027;&#35201;&#22240;&#32032;&#26159;&#65306;1. &#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;2. &#31639;&#27861;&#65292;3. &#35745;&#31639;&#33021;&#21147;&#25110;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Science is a modern Data Intelligence practice, which is the core of many businesses and helps businesses build smart strategies around to deal with businesses challenges more efficiently. Data Science practice also helps in automating business processes using the algorithm, and it has several other benefits, which also deliver in a non-profitable framework. In regards to data science, three key components primarily influence the effective outcome of a data science project. Those are 1.Availability of Data 2.Algorithm 3.Processing power or infrastructure
&lt;/p&gt;</description></item><item><title>&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04889</link><description>&lt;p&gt;
NLLG&#23395;&#24230;arXiv&#25253;&#21578; 06/23&#65306;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#26159;&#20160;&#20040;&#65311;&#65288;arXiv:2308.04889v1 [cs.CY]&#65289;
&lt;/p&gt;
&lt;p&gt;
NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?. (arXiv:2308.04889v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Generative Artificial Intelligence&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;Natural Language Processing&#65292;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;Machine Learning&#65292;ML&#65289;&#65289;&#20449;&#24687;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#20182;&#20204;&#38590;&#20197;&#36319;&#19978;&#26368;&#26032;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#30340;&#38382;&#39064;&#65292;Bielefeld&#22823;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#32452;&#22312;&#26412;&#25253;&#21578;&#20013;&#19987;&#27880;&#20110;&#35782;&#21035;arXiv&#19978;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#65292;&#29305;&#21035;&#20851;&#27880;NLP&#21644;ML&#12290;&#20854;&#30446;&#26631;&#26159;&#20026;&#26368;&#30456;&#20851;&#19988;&#34987;&#24191;&#27867;&#35752;&#35770;&#30340;&#30740;&#31350;&#25552;&#20379;&#24555;&#36895;&#25351;&#21335;&#65292;&#20197;&#24110;&#21161;&#26032;&#26469;&#32773;&#21644;&#24050;&#26377;&#30740;&#31350;&#20154;&#21592;&#36319;&#19978;&#24403;&#21069;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;2023&#24180;&#19978;&#21322;&#24180;&#30340;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#32534;&#21046;&#20102;&#19968;&#20010;&#30001;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#32452;&#25104;&#30340;&#21015;&#34920;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;ChatGPT&#26174;&#31034;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popul
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#36827;&#34892;&#36861;&#36394;&#21644;&#24191;&#21578;&#30340;&#35814;&#32454;&#27979;&#37327;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#65292;&#24182;&#23545;&#36807;&#20004;&#30334;&#19975;&#20010;&#32593;&#39029;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32773;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#20799;&#31461;&#32593;&#31449;&#21015;&#34920;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#32593;&#31449;&#36827;&#34892;&#29228;&#21462;&#21644;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#36861;&#36394;&#22120;&#12289;&#25351;&#32441;&#33050;&#26412;&#21644;&#24191;&#21578;&#37117;&#23384;&#22312;&#20110;&#36825;&#20123;&#32593;&#31449;&#19978;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#26816;&#27979;&#21040;&#36825;&#20123;&#24191;&#21578;&#26159;&#21542;&#21551;&#29992;&#20102;&#23450;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.04887</link><description>&lt;p&gt;
&#30596;&#20934;&#19988;&#40635;&#28902;&#65306;&#36861;&#36394;&#21644;&#24191;&#21578;&#22312;&#20799;&#31461;&#32593;&#31449;&#19978;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Targeted and Troublesome: Tracking and Advertising on Children's Websites. (arXiv:2308.04887v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04887
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#36827;&#34892;&#36861;&#36394;&#21644;&#24191;&#21578;&#30340;&#35814;&#32454;&#27979;&#37327;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#65292;&#24182;&#23545;&#36807;&#20004;&#30334;&#19975;&#20010;&#32593;&#39029;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32773;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#20799;&#31461;&#32593;&#31449;&#21015;&#34920;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#32593;&#31449;&#36827;&#34892;&#29228;&#21462;&#21644;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#36861;&#36394;&#22120;&#12289;&#25351;&#32441;&#33050;&#26412;&#21644;&#24191;&#21578;&#37117;&#23384;&#22312;&#20110;&#36825;&#20123;&#32593;&#31449;&#19978;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#26816;&#27979;&#21040;&#36825;&#20123;&#24191;&#21578;&#26159;&#21542;&#21551;&#29992;&#20102;&#23450;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#32593;&#32476;&#19978;&#65292;&#36861;&#36394;&#22120;&#21644;&#24191;&#21578;&#21830;&#32463;&#24120;&#22312;&#26410;&#32463;&#21516;&#24847;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#35814;&#32454;&#34892;&#20026;&#37197;&#32622;&#25991;&#20214;&#12290;&#23613;&#31649;&#23545;&#32593;&#32476;&#36861;&#36394;&#26426;&#21046;&#21644;&#24191;&#21578;&#36827;&#34892;&#20102;&#21508;&#31181;&#30740;&#31350;&#65292;&#20294;&#36824;&#27809;&#26377;&#23545;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#36827;&#34892;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#36827;&#34892;&#36861;&#36394;&#21644;&#65288;&#23450;&#21521;&#65289;&#24191;&#21578;&#30340;&#27979;&#37327;&#26041;&#27861;&#12290;&#22312;&#32570;&#20047;&#20840;&#38754;&#30340;&#38024;&#23545;&#20799;&#31461;&#30340;&#32593;&#31449;&#65288;&#21363;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#65289;&#21015;&#34920;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#39029;&#26631;&#39064;&#21644;&#25551;&#36848;&#30340;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#12290;&#23558;&#35813;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#36229;&#36807;&#20004;&#30334;&#19975;&#20010;&#32593;&#39029;&#20013;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#21315;&#20010;&#20799;&#31461;&#32593;&#31449;&#30340;&#21015;&#34920;&#12290;&#36890;&#36807;&#20174;&#20116;&#20010;&#35266;&#27979;&#28857;&#29228;&#21462;&#36825;&#20123;&#32593;&#31449;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#36861;&#36394;&#22120;&#12289;&#25351;&#32441;&#33050;&#26412;&#21644;&#24191;&#21578;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#29228;&#34411;&#26816;&#27979;&#21040;&#22312;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#19978;&#26174;&#31034;&#30340;&#24191;&#21578;&#65292;&#24182;&#36890;&#36807;&#25235;&#21462;&#24191;&#21578;&#25259;&#38706;&#39029;&#38754;&#26469;&#30830;&#23450;&#26159;&#21542;&#21551;&#29992;&#20102;&#24191;&#21578;&#23450;&#21521;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ar
&lt;/p&gt;
&lt;p&gt;
On the modern web, trackers and advertisers frequently construct and monetize users' detailed behavioral profiles without consent. Despite various studies on web tracking mechanisms and advertisements, there has been no rigorous study focusing on websites targeted at children. To address this gap, we present a measurement of tracking and (targeted) advertising on websites directed at children. Motivated by lacking a comprehensive list of child-directed (i.e., targeted at children) websites, we first build a multilingual classifier based on web page titles and descriptions. Applying this classifier to over two million pages, we compile a list of two thousand child-directed websites. Crawling these sites from five vantage points, we measure the prevalence of trackers, fingerprinting scripts, and advertisements. Our crawler detects ads displayed on child-directed websites and determines if ad targeting is enabled by scraping ad disclosure pages whenever available. Our results show that ar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25345;&#20037;&#24615;&#26041;&#27861;&#21435;&#38500;&#31070;&#32463;&#20803;&#20043;&#38388;&#39640;&#30456;&#20851;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#26368;&#23567;&#29983;&#25104;&#26641;&#30340;&#26435;&#37325;&#26469;&#26500;&#24314;&#27491;&#21017;&#21270;&#39033;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27491;&#21017;&#21270;&#39033;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24120;&#35265;&#30340;&#27491;&#21017;&#21270;&#39033;&#30456;&#27604;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#39033;&#33021;&#26356;&#22909;&#22320;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#21457;&#29616;&#20887;&#20313;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04870</link><description>&lt;p&gt;
&#20351;&#29992;&#25345;&#20037;&#24615;&#26041;&#27861;&#21435;&#30456;&#20851;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Decorrelating neurons using persistence. (arXiv:2308.04870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25345;&#20037;&#24615;&#26041;&#27861;&#21435;&#38500;&#31070;&#32463;&#20803;&#20043;&#38388;&#39640;&#30456;&#20851;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#26368;&#23567;&#29983;&#25104;&#26641;&#30340;&#26435;&#37325;&#26469;&#26500;&#24314;&#27491;&#21017;&#21270;&#39033;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27491;&#21017;&#21270;&#39033;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24120;&#35265;&#30340;&#27491;&#21017;&#21270;&#39033;&#30456;&#27604;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#39033;&#33021;&#26356;&#22909;&#22320;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#21457;&#29616;&#20887;&#20313;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#39640;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#32473;&#23450;&#32593;&#32476;&#30340;&#31070;&#32463;&#20803;&#65288;&#25110;&#20854;&#20013;&#19968;&#37096;&#20998;&#26679;&#26412;&#65289;&#26500;&#25104;&#30340;&#22242;&#20013;&#65292;&#35745;&#31639;&#26368;&#23567;&#29983;&#25104;&#26641;&#30340;&#26435;&#37325;&#26469;&#35745;&#31639;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#32780;&#36793;&#19978;&#30340;&#26435;&#37325;&#26159;&#30456;&#20851;&#24615;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#39033;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#20248;&#20110;&#24120;&#35265;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20165;&#20165;&#26368;&#23567;&#21270;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#25152;&#26377;&#30456;&#20851;&#24615;&#24471;&#21040;&#30340;&#20934;&#30830;&#29575;&#27604;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#39033;&#35201;&#20302;&#65292;&#36825;&#34920;&#26126;&#20887;&#20313;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#19968;&#28857;&#22312;&#31070;&#32463;&#31185;&#23398;&#30340;&#19968;&#20123;&#30740;&#31350;&#20013;&#20063;&#26377;&#25152;&#35777;&#26126;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#27491;&#21017;&#21270;&#39033;&#30340;&#21487;&#24494;&#24615;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#32771;&#34385;&#25972;&#20010;&#31070;&#32463;&#20803;&#38598;&#30340;&#22522;&#20110;&#25299;&#25169;&#25345;&#20037;&#24615;&#30340;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel way to improve the generalisation capacity of deep learning models by reducing high correlations between neurons. For this, we present two regularisation terms computed from the weights of a minimum spanning tree of the clique whose vertices are the neurons of a given network (or a sample of those), where weights on edges are correlation dissimilarities. We provide an extensive set of experiments to validate the effectiveness of our terms, showing that they outperform popular ones. Also, we demonstrate that naive minimisation of all correlations between neurons obtains lower accuracies than our regularisation terms, suggesting that redundancies play a significant role in artificial neural networks, as evidenced by some studies in neuroscience for real networks. We include a proof of differentiability of our regularisers, thus developing the first effective topological persistence-based regularisation terms that consider the whole set of neurons and that can be applie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20102;&#22312;&#22686;&#21152;&#28040;&#24687;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#21644;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#28040;&#24687;&#32534;&#30721;&#26041;&#27861;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#27604;&#27880;&#24847;&#21147;&#28040;&#24687;&#32534;&#30721;&#22120;&#26356;&#20248;&#65292;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04844</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#36830;&#32493;&#36890;&#20449;&#30340;&#28040;&#24687;&#32534;&#30721;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scalability of Message Encoding Techniques for Continuous Communication Learned with Multi-Agent Reinforcement Learning. (arXiv:2308.04844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20102;&#22312;&#22686;&#21152;&#28040;&#24687;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#21644;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#28040;&#24687;&#32534;&#30721;&#26041;&#27861;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#27604;&#27880;&#24847;&#21147;&#28040;&#24687;&#32534;&#30721;&#22120;&#26356;&#20248;&#65292;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#38656;&#35201;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#20197;&#30830;&#20445;&#27491;&#30830;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#36890;&#20449;&#21327;&#35758;&#21644;&#21160;&#20316;&#21327;&#35758;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#28789;&#27963;&#20915;&#23450;&#24212;&#35813;&#20849;&#20139;&#21738;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#25105;&#20204;&#38656;&#35201;&#21019;&#24314;&#21253;&#21547;&#22312;&#36825;&#20123;&#28040;&#24687;&#20013;&#30340;&#20449;&#24687;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22686;&#21152;&#28040;&#24687;&#20013;&#24212;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#21644;&#22686;&#21152;&#26234;&#33021;&#20307;&#25968;&#37327;&#23545;&#20004;&#31181;&#19981;&#21516;&#28040;&#24687;&#32534;&#30721;&#26041;&#27861;&#65288;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#21644;&#27880;&#24847;&#21147;&#28040;&#24687;&#32534;&#30721;&#22120;&#65289;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19968;&#20010;&#30697;&#38453;&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#24433;&#21709;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#22987;&#32456;&#20248;&#20110;&#27880;&#24847;&#21147;&#28040;&#24687;&#32534;&#30721;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#30340;&#26234;&#33021;&#20307;&#20351;&#29992;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;
&lt;/p&gt;
&lt;p&gt;
Many multi-agent systems require inter-agent communication to properly achieve their goal. By learning the communication protocol alongside the action protocol using multi-agent reinforcement learning techniques, the agents gain the flexibility to determine which information should be shared. However, when the number of agents increases we need to create an encoding of the information contained in these messages. In this paper, we investigate the effect of increasing the amount of information that should be contained in a message and increasing the number of agents. We evaluate these effects on two different message encoding methods, the mean message encoder and the attention message encoder. We perform our experiments on a matrix environment. Surprisingly, our results show that the mean message encoder consistently outperforms the attention message encoder. Therefore, we analyse the communication protocol used by the agents that use the mean message encoder and can conclude that the a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#24847;&#22806;&#35760;&#24518;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#30340;&#22522;&#30784;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#24847;&#22806;&#39044;&#27979;&#22120;&#30340;&#24847;&#22806;&#35760;&#24518;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04836</link><description>&lt;p&gt;
&#36890;&#36807;&#24847;&#22806;&#35760;&#24518;&#23454;&#29616;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Motivation via Surprise Memory. (arXiv:2308.04836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#24847;&#22806;&#35760;&#24518;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#30340;&#22522;&#30784;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#24847;&#22806;&#39044;&#27979;&#22120;&#30340;&#24847;&#22806;&#35760;&#24518;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#24847;&#22806;&#30340;&#25506;&#32034;&#30340;&#23616;&#38480;&#24615;&#12290;&#22870;&#21169;&#26159;&#24847;&#22806;&#30340;&#26032;&#39062;&#24615;&#65292;&#32780;&#19981;&#26159;&#24847;&#22806;&#30340;&#35268;&#33539;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35760;&#24518;&#32593;&#32476;&#20013;&#30340;&#26816;&#32034;&#38169;&#35823;&#26469;&#20272;&#35745;&#24847;&#22806;&#30340;&#26032;&#39062;&#24615;&#65292;&#35760;&#24518;&#23384;&#20648;&#21644;&#37325;&#24314;&#24847;&#22806;&#12290;&#25105;&#20204;&#30340;&#24847;&#22806;&#35760;&#24518;&#65288;SM&#65289;&#22686;&#21152;&#20102;&#22522;&#20110;&#24847;&#22806;&#30340;&#20869;&#22312;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#65292;&#20445;&#25345;&#20102;&#23545;&#28608;&#21160;&#20154;&#24515;&#30340;&#25506;&#32034;&#30340;&#20852;&#36259;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#19981;&#21487;&#39044;&#27979;&#25110;&#22122;&#22768;&#35266;&#23519;&#30340;&#19981;&#24517;&#35201;&#30340;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32467;&#21512;&#21508;&#31181;&#24847;&#22806;&#39044;&#27979;&#22120;&#30340;SM&#23637;&#31034;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#26368;&#32456;&#24615;&#33021;&#65292;&#21253;&#25324;&#22122;&#22768;&#30005;&#35270;&#12289;&#23548;&#33322;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Atari&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968; TSSR&#65292;&#20855;&#26377;&#22855;&#25968;&#12289;&#38750;&#32447;&#24615;&#12289;&#21333;&#35843;&#21644;&#21487;&#24494;&#20998;&#30340;&#29305;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;TSSR&#30456;&#27604;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04832</link><description>&lt;p&gt;
TSSR&#65306;&#19968;&#31181;&#25130;&#26029;&#21644;&#24102;&#31526;&#21495;&#30340;&#24179;&#26041;&#26681;&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks. (arXiv:2308.04832v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968; TSSR&#65292;&#20855;&#26377;&#22855;&#25968;&#12289;&#38750;&#32447;&#24615;&#12289;&#21333;&#35843;&#21644;&#21487;&#24494;&#20998;&#30340;&#29305;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;TSSR&#30456;&#27604;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#26159;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#31216;&#20026;&#25130;&#26029;&#21644;&#24102;&#31526;&#21495;&#24179;&#26041;&#26681;&#65288;TSSR&#65289;&#20989;&#25968;&#12290;&#35813;&#20989;&#25968;&#20855;&#26377;&#22855;&#25968;&#12289;&#38750;&#32447;&#24615;&#12289;&#21333;&#35843;&#21644;&#21487;&#24494;&#20998;&#30340;&#29305;&#24615;&#12290;&#20854;&#26799;&#24230;&#26159;&#36830;&#32493;&#19988;&#22987;&#32456;&#20026;&#27491;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24615;&#65292;&#23427;&#26377;&#28508;&#21147;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;TSSR&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#28608;&#27963;&#20989;&#25968;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#35813;&#20989;&#25968;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions are essential components of neural networks. In this paper, we introduce a new activation function called the Truncated and Signed Square Root (TSSR) function. This function is distinctive because it is odd, nonlinear, monotone and differentiable. Its gradient is continuous and always positive. Thanks to these properties, it has the potential to improve the numerical stability of neural networks. Several experiments confirm that the proposed TSSR has better performance than other stat-of-the-art activation functions. The proposed function has significant implications for the development of neural network models and can be applied to a wide range of applications in fields such as computer vision, natural language processing, and speech recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#20998;&#31867;&#35268;&#21017;&#22312;&#33033;&#20914;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#36125;&#21494;&#26031;&#39118;&#38505;&#19968;&#33268;&#24615;&#65292;&#24182;&#23548;&#20986;&#20102;&#26368;&#20248;&#30340;&#36125;&#21494;&#26031;&#35268;&#21017;&#21644;&#25554;&#20540;&#30340;&#38750;&#21442;&#25968;&#26680;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#28176;&#36817;&#24615;&#36136;&#35777;&#26126;&#20102;&#26680;&#20998;&#31867;&#22120;&#25910;&#25947;&#21040;&#36125;&#21494;&#26031;&#35268;&#21017;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#27169;&#25311;&#30740;&#31350;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.04796</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#20998;&#31867;&#35268;&#21017;&#22312;&#33033;&#20914;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#36125;&#21494;&#26031;&#39118;&#38505;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bayes Risk Consistency of Nonparametric Classification Rules for Spike Trains Data. (arXiv:2308.04796v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#20998;&#31867;&#35268;&#21017;&#22312;&#33033;&#20914;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#36125;&#21494;&#26031;&#39118;&#38505;&#19968;&#33268;&#24615;&#65292;&#24182;&#23548;&#20986;&#20102;&#26368;&#20248;&#30340;&#36125;&#21494;&#26031;&#35268;&#21017;&#21644;&#25554;&#20540;&#30340;&#38750;&#21442;&#25968;&#26680;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#28176;&#36817;&#24615;&#36136;&#35777;&#26126;&#20102;&#26680;&#20998;&#31867;&#22120;&#25910;&#25947;&#21040;&#36125;&#21494;&#26031;&#35268;&#21017;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#27169;&#25311;&#30740;&#31350;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#24207;&#21015;&#25968;&#25454;&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#12289;&#25104;&#20687;&#12289;&#27969;&#25968;&#25454;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#12290;&#33033;&#20914;&#24207;&#21015;&#30340;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#22522;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#21644;&#27010;&#29575;&#27169;&#22411;&#12290;&#27010;&#29575;&#26041;&#27861;&#20381;&#36182;&#20110;&#24213;&#23618;&#33033;&#20914;&#29983;&#25104;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#25110;&#38750;&#21442;&#25968;&#21270;&#35268;&#33539;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#30001;&#38750;&#21442;&#25968;&#24378;&#24230;&#20989;&#25968;&#34920;&#24449;&#30340;&#19968;&#31867;&#33033;&#20914;&#24207;&#21015;&#25968;&#25454;&#30340;&#20108;&#31867;&#32479;&#35745;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#36125;&#21494;&#26031;&#35268;&#21017;&#65292;&#24182;&#26500;&#24314;&#20102;&#25554;&#20540;&#30340;&#38750;&#21442;&#25968;&#26680;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35268;&#21017;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#21253;&#25324;&#38543;&#30528;&#35760;&#24405;&#26102;&#38388;&#38388;&#38548;&#21644;&#35757;&#32451;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#32780;&#25910;&#25947;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26680;&#20998;&#31867;&#22120;&#25910;&#25947;&#21040;&#36125;&#21494;&#26031;&#35268;&#21017;&#12290;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;&#27169;&#25311;&#30740;&#31350;&#39564;&#35777;&#20102;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spike trains data find a growing list of applications in computational neuroscience, imaging, streaming data and finance. Machine learning strategies for spike trains are based on various neural network and probabilistic models. The probabilistic approach is relying on parametric or nonparametric specifications of the underlying spike generation model. In this paper we consider the two-class statistical classification problem for a class of spike train data characterized by nonparametrically specified intensity functions. We derive the optimal Bayes rule and next form the plug-in nonparametric kernel classifier. Asymptotical properties of the rules are established including the limit with respect to the increasing recording time interval and the size of a training set. In particular the convergence of the kernel classifier to the Bayes rule is proved. The obtained results are supported by a finite sample simulation studies.
&lt;/p&gt;</description></item><item><title>PETformer&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21344;&#20301;&#31526;&#22686;&#24378;&#25216;&#26415;&#65292;&#38271;&#23376;&#24207;&#21015;&#21010;&#20998;&#21644;&#22810;&#36890;&#36947;&#20998;&#31163;&#19982;&#20132;&#20114;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;PETformer&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04791</link><description>&lt;p&gt;
PETformer: &#36890;&#36807;&#22686;&#24378;&#21344;&#20301;&#31526;&#30340;Transformer&#23454;&#29616;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer. (arXiv:2308.04791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04791
&lt;/p&gt;
&lt;p&gt;
PETformer&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21344;&#20301;&#31526;&#22686;&#24378;&#25216;&#26415;&#65292;&#38271;&#23376;&#24207;&#21015;&#21010;&#20998;&#21644;&#22810;&#36890;&#36947;&#20998;&#31163;&#19982;&#20132;&#20114;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;PETformer&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36825;&#24402;&#22240;&#20110;&#23427;&#20204;&#33021;&#22815;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;Transformer&#24212;&#29992;&#20110;LTSF&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#65292;&#23588;&#20854;&#26159;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;LTSF&#20013;&#24212;&#29992;Transformer&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;LTSF&#26102;&#30340;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#20449;&#24687;&#23494;&#24230;&#21644;&#22810;&#36890;&#36947;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21019;&#26032;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21344;&#20301;&#31526;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#65292;&#38271;&#23376;&#24207;&#21015;&#21010;&#20998;&#65288;LSD&#65289;&#21644;&#22810;&#36890;&#36947;&#20998;&#31163;&#19982;&#20132;&#20114;&#65288;MSI&#65289;&#65292;&#20849;&#21516;&#26500;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;PETformer&#30340;&#26032;&#27169;&#22411;&#12290;&#36825;&#19977;&#20010;&#20851;&#38190;&#35774;&#35745;&#24341;&#20837;&#20102;&#36866;&#21512;LTSF&#20219;&#21153;&#30340;&#20808;&#39564;&#20559;&#24046;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;PETformer&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformer-based models have shown remarkable performance in long-term time series forecasting (LTSF) tasks due to their ability to model long-term dependencies. However, the validity of Transformers for LTSF tasks remains debatable, particularly since recent work has shown that simple linear models can outperform numerous Transformer-based approaches. This suggests that there are limitations to the application of Transformer in LTSF. Therefore, this paper investigates three key issues when applying Transformer to LTSF: temporal continuity, information density, and multi-channel relationships. Accordingly, we propose three innovative solutions, including Placeholder Enhancement Technique (PET), Long Sub-sequence Division (LSD), and Multi-channel Separation and Interaction (MSI), which together form a novel model called PETformer. These three key designs introduce prior biases suitable for LTSF tasks. Extensive experiments have demonstrated that PETformer achieves state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20856;&#22411;&#20998;&#26512;&#30340;&#26032;&#22411;&#31232;&#30095;&#35299;&#28151;&#25216;&#26415;&#65288;SUnAA&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#27169;&#22411;&#24182;&#20351;&#29992;&#20027;&#21160;&#38598;&#31639;&#27861;&#36845;&#20195;&#22320;&#26368;&#23567;&#21270;&#20248;&#21270;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;&#28151;&#21512;&#25104;&#20998;&#30340;&#20934;&#30830;&#35299;&#28151;&#12290;&#23454;&#39564;&#35777;&#26126;SUnAA&#22312;&#20449;&#21495;&#37325;&#24314;&#35823;&#24046;&#26041;&#38754;&#20855;&#26377;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04771</link><description>&lt;p&gt;
SUnAA: &#20351;&#29992;&#20856;&#22411;&#20998;&#26512;&#30340;&#31232;&#30095;&#35299;&#28151;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SUnAA: Sparse Unmixing using Archetypal Analysis. (arXiv:2308.04771v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20856;&#22411;&#20998;&#26512;&#30340;&#26032;&#22411;&#31232;&#30095;&#35299;&#28151;&#25216;&#26415;&#65288;SUnAA&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#27169;&#22411;&#24182;&#20351;&#29992;&#20027;&#21160;&#38598;&#31639;&#27861;&#36845;&#20195;&#22320;&#26368;&#23567;&#21270;&#20248;&#21270;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;&#28151;&#21512;&#25104;&#20998;&#30340;&#20934;&#30830;&#35299;&#28151;&#12290;&#23454;&#39564;&#35777;&#26126;SUnAA&#22312;&#20449;&#21495;&#37325;&#24314;&#35823;&#24046;&#26041;&#38754;&#20855;&#26377;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20856;&#22411;&#20998;&#26512;&#65288;SUnAA&#65289;&#30340;&#26032;&#22411;&#31232;&#30095;&#35299;&#28151;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#20856;&#22411;&#20998;&#26512;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#20551;&#35774;&#24863;&#20852;&#36259;&#30340;&#25104;&#20998;&#26159;&#26469;&#33258;&#20809;&#35889;&#24211;&#25552;&#20379;&#30340;&#25104;&#20998;&#30340;&#20984;&#32452;&#21512;&#65292;&#24182;&#19988;&#24863;&#20852;&#36259;&#30340;&#25104;&#20998;&#25968;&#37327;&#26159;&#24050;&#30693;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#20256;&#32479;&#31232;&#30095;&#35299;&#28151;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#37324;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#26159;&#38750;&#20984;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#21160;&#38598;&#31639;&#27861;&#36845;&#20195;&#22320;&#26368;&#23567;&#21270;&#20248;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21021;&#22987;&#21270;&#40065;&#26834;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#24863;&#20852;&#36259;&#30340;&#25104;&#20998;&#25968;&#37327;&#12290;&#20351;&#29992;&#20004;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;SUnAA&#65292;&#32467;&#26524;&#35777;&#23454;&#20102;&#20854;&#22312;&#20449;&#21495;&#37325;&#24314;&#35823;&#24046;&#26041;&#38754;&#30456;&#27604;&#20854;&#20182;&#24120;&#35268;&#21644;&#20808;&#36827;&#25216;&#26415;&#30340;&#26356;&#22909;&#24615;&#33021;&#12290;SUnAA&#36824;&#24212;&#29992;&#20110;Cuprite&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;&#22320;&#36136;&#22270;&#36827;&#34892;&#20102;&#35270;&#35273;&#27604;&#36739;&#12290;&#23450;&#24615;&#35780;&#20272;&#34920;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new sparse unmixing technique using archetypal analysis (SUnAA). First, we design a new model based on archetypal analysis. We assume that the endmembers of interest are a convex combination of endmembers provided by a spectral library and that the number of endmembers of interest is known. Then, we propose a minimization problem. Unlike most conventional sparse unmixing methods, here the minimization problem is non-convex. We minimize the optimization objective iteratively using an active set algorithm. Our method is robust to the initialization and only requires the number of endmembers of interest. SUnAA is evaluated using two simulated datasets for which results confirm its better performance over other conventional and advanced techniques in terms of signal-to-reconstruction error. SUnAA is also applied to Cuprite dataset and the results are compared visually with the available geological map provided for this dataset. The qualitative assessment demonstrate
&lt;/p&gt;</description></item><item><title>Tram-FL&#26159;&#19968;&#31181;&#22522;&#20110;&#36335;&#30001;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20256;&#36755;&#20840;&#23616;&#27169;&#22411;&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#30340;&#20132;&#25442;&#21644;&#27719;&#32858;&#65292;&#20197;&#21450;&#21160;&#24577;&#27169;&#22411;&#36335;&#30001;&#31639;&#27861;&#65292;&#23454;&#29616;&#22312;&#38750;IID&#26465;&#20214;&#19979;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.04762</link><description>&lt;p&gt;
Tram-FL: &#22522;&#20110;&#36335;&#30001;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Tram-FL: Routing-based Model Training for Decentralized Federated Learning. (arXiv:2308.04762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04762
&lt;/p&gt;
&lt;p&gt;
Tram-FL&#26159;&#19968;&#31181;&#22522;&#20110;&#36335;&#30001;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20256;&#36755;&#20840;&#23616;&#27169;&#22411;&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#30340;&#20132;&#25442;&#21644;&#27719;&#32858;&#65292;&#20197;&#21450;&#21160;&#24577;&#27169;&#22411;&#36335;&#30001;&#31639;&#27861;&#65292;&#23454;&#29616;&#22312;&#38750;IID&#26465;&#20214;&#19979;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#20013;&#65292;&#39057;&#32321;&#30340;&#33410;&#28857;&#38388;&#36890;&#20449;&#21644;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#23548;&#33268;&#39640;&#31934;&#24230;&#27169;&#22411;&#33719;&#21462;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tram-FL&#30340;&#26032;&#22411;DFL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#20010;&#23558;&#20840;&#23616;&#27169;&#22411;&#20256;&#36755;&#21040;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#36880;&#27493;&#32454;&#21270;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#20132;&#25442;&#21644;&#27719;&#32858;&#26412;&#22320;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#36335;&#30001;&#31639;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20339;&#36335;&#24452;&#65292;&#26088;&#22312;&#22312;&#26368;&#23567;&#36716;&#21457;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#31934;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;MNIST&#12289;CIFAR-10&#21644;IMDb&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Tram-FL&#22312;&#38750;IID&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20248;&#20110;&#22522;&#20934;&#24182;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In decentralized federated learning (DFL), substantial traffic from frequent inter-node communication and non-independent and identically distributed (non-IID) data challenges high-accuracy model acquisition. We propose Tram-FL, a novel DFL method, which progressively refines a global model by transferring it sequentially amongst nodes, rather than by exchanging and aggregating local models. We also introduce a dynamic model routing algorithm for optimal route selection, aimed at enhancing model precision with minimal forwarding. Our experiments using MNIST, CIFAR-10, and IMDb datasets demonstrate that Tram-FL with the proposed routing delivers high model accuracy under non-IID conditions, outperforming baselines while reducing communication costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#30340;&#29305;&#24449;&#21305;&#37197;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25273;&#21435;&#30495;&#23454;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.04761</link><description>&lt;p&gt;
&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#30340;&#29305;&#24449;&#21305;&#37197;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Feature Matching Data Synthesis for Non-IID Federated Learning. (arXiv:2308.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#30340;&#29305;&#24449;&#21305;&#37197;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25273;&#21435;&#30495;&#23454;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#33539;&#24335;&#65292;&#23427;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32780;&#19981;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#25910;&#38598;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#22788;&#29702;&#35774;&#22791;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#29305;&#24449;&#21305;&#37197;&#25968;&#25454;&#21512;&#25104;&#65288;HFMDS&#65289;&#26041;&#27861;&#65292;&#38500;&#20102;&#26412;&#22320;&#27169;&#22411;&#22806;&#65292;&#36824;&#20849;&#20139;&#36741;&#21161;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23398;&#20064;&#30495;&#23454;&#26679;&#26412;&#30340;&#22522;&#26412;&#31867;&#30456;&#20851;&#29305;&#24449;&#24182;&#20002;&#24323;&#22810;&#20313;&#30340;&#29305;&#24449;&#65292;&#29983;&#25104;&#20102;&#21512;&#25104;&#25968;&#25454;&#65292;&#36825;&#26377;&#21161;&#20110;&#26377;&#25928;&#22788;&#29702;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#23558;&#30495;&#23454;&#29305;&#24449;&#36716;&#31227;&#21040;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#65292;&#20174;&#32780;&#21512;&#25104;&#25968;&#25454;&#19981;&#20165;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19988;&#25273;&#21435;&#20102;&#30495;&#23454;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23558;&#25552;&#20986;&#30340;HFMDS&#26041;&#27861;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a privacy-preserving paradigm that trains neural networks on edge devices without collecting data at a central server. However, FL encounters an inherent challenge in dealing with non-independent and identically distributed (non-IID) data among devices. To address this challenge, this paper proposes a hard feature matching data synthesis (HFMDS) method to share auxiliary data besides local models. Specifically, synthetic data are generated by learning the essential class-relevant features of real samples and discarding the redundant features, which helps to effectively tackle the non-IID issue. For better privacy preservation, we propose a hard feature augmentation method to transfer real features towards the decision boundary, with which the synthetic data not only improve the model generalization but also erase the information of real features. By integrating the proposed HFMDS method with FL, we present a novel FL framework with data augmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20581;&#24247;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#21512;&#20316;&#23398;&#20064;&#26041;&#36890;&#36807;&#20849;&#20139;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32479;&#35745;&#20272;&#35745;&#65292;&#23588;&#20854;&#23545;&#20110;&#21253;&#21547;&#23569;&#25968;&#32676;&#20307;&#30340;&#25968;&#25454;&#26041;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.04755</link><description>&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning From Distributed Data With Differentially Private Synthetic Twin Data. (arXiv:2308.04755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20581;&#24247;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#21512;&#20316;&#23398;&#20064;&#26041;&#36890;&#36807;&#20849;&#20139;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32479;&#35745;&#20272;&#35745;&#65292;&#23588;&#20854;&#23545;&#20110;&#21253;&#21547;&#23569;&#25968;&#32676;&#20307;&#30340;&#25968;&#25454;&#26041;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#22810;&#20010;&#25317;&#26377;&#25935;&#24863;&#25968;&#25454;&#30340;&#26041;&#22312;&#21512;&#20316;&#23398;&#20064;&#32676;&#20307;&#32423;&#21035;&#32479;&#35745;&#25968;&#25454;&#26102;&#65292;&#26080;&#27861;&#21512;&#24182;&#25935;&#24863;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#27599;&#20010;&#26041;&#20998;&#20139;&#20854;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#23545;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26469;&#33258;&#33521;&#22269;&#29983;&#29289;&#25968;&#25454;&#24211;&#30340;&#30495;&#23454;&#19990;&#30028;&#20581;&#24247;&#25968;&#25454;&#19978;&#32467;&#21512;&#36825;&#31181;&#21512;&#25104;&#23545;&#25968;&#25454;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#20849;&#20139;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#30340;&#26041;&#65292;&#30456;&#27604;&#20165;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#30446;&#26631;&#32479;&#35745;&#20272;&#35745;&#12290;&#36825;&#19968;&#21457;&#29616;&#21516;&#26679;&#36866;&#29992;&#20110;&#23567;&#22411;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#21442;&#19982;&#26041;&#36234;&#22810;&#65292;&#25913;&#36827;&#30340;&#35268;&#27169;&#21644;&#19968;&#33268;&#24615;&#36234;&#22823;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#20849;&#20139;&#21487;&#20197;&#24110;&#21161;&#21253;&#21547;&#23569;&#25968;&#32676;&#20307;&#30340;&#25968;&#25454;&#30340;&#26041;&#26356;&#22909;&#22320;&#36827;&#34892;&#35843;&#25972;&#20998;&#26512;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#20849;&#20139;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#20419;&#36827;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a setting where multiple parties holding sensitive data aim to collaboratively learn population level statistics, but pooling the sensitive data sets is not possible. We propose a framework in which each party shares a differentially private synthetic twin of their data. We study the feasibility of combining such synthetic twin data sets for collaborative learning on real-world health data from the UK Biobank. We discover that parties engaging in the collaborative learning via shared synthetic data obtain more accurate estimates of target statistics compared to using only their local data. This finding extends to the difficult case of small heterogeneous data sets. Furthermore, the more parties participate, the larger and more consistent the improvements become. Finally, we find that data sharing can especially help parties whose data contain underrepresented groups to perform better-adjusted analysis for said groups. Based on our results we conclude that sharing of synthetic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fuzz4All&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#38024;&#23545;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#35821;&#35328;&#21644;&#36825;&#20123;&#35821;&#35328;&#30340;&#35768;&#22810;&#19981;&#21516;&#21151;&#33021;&#36827;&#34892;&#27169;&#31946;&#27979;&#35797;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.04748</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36890;&#29992;&#27169;&#31946;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Universal Fuzzing via Large Language Models. (arXiv:2308.04748v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fuzz4All&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#38024;&#23545;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#35821;&#35328;&#21644;&#36825;&#20123;&#35821;&#35328;&#30340;&#35768;&#22810;&#19981;&#21516;&#21151;&#33021;&#36827;&#34892;&#27169;&#31946;&#27979;&#35797;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#27979;&#35797;&#22312;&#21457;&#29616;&#21508;&#31181;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#21644;&#33030;&#24369;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#25509;&#21463;&#32534;&#31243;&#25110;&#24418;&#24335;&#35821;&#35328;&#20316;&#20026;&#36755;&#20837;&#30340;&#27979;&#35797;&#31995;&#32479;&#65288;SUTs&#65289;&#65292;&#22914;&#32534;&#35793;&#22120;&#65292;&#36816;&#34892;&#26102;&#24341;&#25806;&#65292;&#32422;&#26463;&#27714;&#35299;&#22120;&#21644;&#20855;&#26377;&#21487;&#35775;&#38382;API&#30340;&#36719;&#20214;&#24211;&#65292;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#36719;&#20214;&#24320;&#21457;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#29616;&#26377;&#27169;&#31946;&#27979;&#35797;&#24037;&#20855;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#35821;&#35328;&#65292;&#22240;&#27492;&#26080;&#27861;&#36731;&#26131;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#35328;&#29978;&#33267;&#21516;&#19968;&#35821;&#35328;&#30340;&#20854;&#20182;&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#27169;&#31946;&#27979;&#35797;&#24037;&#20855;&#29983;&#25104;&#30340;&#36755;&#20837;&#36890;&#24120;&#23616;&#38480;&#20110;&#36755;&#20837;&#35821;&#35328;&#30340;&#29305;&#23450;&#21151;&#33021;&#65292;&#22240;&#27492;&#24456;&#38590;&#25581;&#31034;&#19982;&#20854;&#20182;&#21151;&#33021;&#30456;&#20851;&#30340;&#28431;&#27934;&#25110;&#26032;&#21151;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fuzz4All&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#31946;&#27979;&#35797;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#38024;&#23545;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#35821;&#35328;&#21644;&#36825;&#20123;&#35821;&#35328;&#30340;&#35768;&#22810;&#19981;&#21516;&#21151;&#33021;&#36827;&#34892;&#27979;&#35797;&#12290;Fuzz4All&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36755;&#20837;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input genera
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;Transformer&#30340;&#22320;&#38663;&#22788;&#29702;&#32593;&#32476;StorSeismic&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#25913;&#36827;&#20301;&#32622;&#32534;&#30721;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#29575;&#21644;&#34920;&#29616;&#21147;&#65292;&#24182;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04739</link><description>&lt;p&gt;
&#20248;&#21270;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#22320;&#38663;&#22788;&#29702;&#24037;&#20316;&#27969;&#31243;&#30340;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Optimizing a Transformer-based network for a deep learning seismic processing workflow. (arXiv:2308.04739v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04739
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;Transformer&#30340;&#22320;&#38663;&#22788;&#29702;&#32593;&#32476;StorSeismic&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#25913;&#36827;&#20301;&#32622;&#32534;&#30721;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#29575;&#21644;&#34920;&#29616;&#21147;&#65292;&#24182;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
StorSeismic&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20854;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#35757;&#32451;&#31574;&#30053;&#36866;&#24212;&#21508;&#31181;&#22320;&#38663;&#22788;&#29702;&#20219;&#21153;&#12290;&#22312;&#21407;&#22987;&#23454;&#29616;&#20013;&#65292;StorSeismic&#21033;&#29992;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#20013;&#20511;&#37492;&#30340;&#27491;&#24358;&#20301;&#32622;&#32534;&#30721;&#21644;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#23545;&#20110;&#22320;&#38663;&#22788;&#29702;&#65292;&#23427;&#20204;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#20063;&#25552;&#21040;&#20102;&#25928;&#29575;&#21644;&#34920;&#29616;&#21147;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#36825;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#30340;&#20462;&#25913;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#21644;&#20302;&#31209;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#26367;&#20195;&#21407;&#22987;&#32452;&#20214;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;Marmousi&#21644;&#28023;&#19978;&#22330;&#25968;&#25454;&#19978;&#23545;&#36825;&#20123;&#25913;&#36827;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#31574;&#30053;&#65292;&#20174;&#38477;&#22122;&#12289;&#30452;&#36798;&#27874;&#21435;&#38500;&#12289;&#22810;&#27425;&#34928;&#20943;&#65292;&#26368;&#21518;&#21040;&#22343;&#26041;&#26681;&#36895;&#24230;(VRMS)&#39044;&#27979;&#29992;&#20110;&#27491;&#24120;&#31227;&#21160;&#34917;&#20607;(NMO)&#26657;&#27491;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#19988;&#33021;&#22815;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
StorSeismic is a recently introduced model based on the Transformer to adapt to various seismic processing tasks through its pretraining and fine-tuning training strategy. In the original implementation, StorSeismic utilized a sinusoidal positional encoding and a conventional self-attention mechanism, both borrowed from the natural language processing (NLP) applications. For seismic processing they admitted good results, but also hinted to limitations in efficiency and expressiveness. We propose modifications to these two key components, by utilizing relative positional encoding and low-rank attention matrices as replacements to the vanilla ones. The proposed changes are tested on processing tasks applied to a realistic Marmousi and offshore field data as a sequential strategy, starting from denoising, direct arrival removal, multiple attenuation, and finally root-mean-squared velocity ($V_{RMS}$) prediction for normal moveout (NMO) correction. We observe faster pretraining and competi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20116;&#28857;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21453;&#24212;-&#25193;&#25955;&#26041;&#31243;&#28145;&#20837;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#36739;&#22823;&#24863;&#21463;&#37326;&#30340;&#28145;&#23618;FCNNs&#65292;&#21487;&#20197;&#39044;&#27979;&#20855;&#26377;&#22823;&#20110;CFL&#26465;&#20214;&#38408;&#20540;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.04735</link><description>&lt;p&gt;
&#29992;&#20116;&#28857;&#21367;&#31215;&#36827;&#34892;&#21453;&#24212;-&#25193;&#25955;&#26041;&#31243;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations. (arXiv:2308.04735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20116;&#28857;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21453;&#24212;-&#25193;&#25955;&#26041;&#31243;&#28145;&#20837;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#36739;&#22823;&#24863;&#21463;&#37326;&#30340;&#28145;&#23618;FCNNs&#65292;&#21487;&#20197;&#39044;&#27979;&#20855;&#26377;&#22823;&#20110;CFL&#26465;&#20214;&#38408;&#20540;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#22240;&#20026;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#22522;&#26412;&#19978;&#19981;&#38656;&#35201;&#35266;&#27979;&#25110;&#31163;&#25955;&#21270;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#24456;&#22256;&#38590;&#65292;&#24182;&#19988;&#36825;&#20123;&#21442;&#25968;&#24517;&#39035;&#38024;&#23545;&#27599;&#20010;&#19981;&#21516;&#30340;&#21021;&#22987;&#26465;&#20214;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#20108;&#38454;&#21453;&#24212;-&#25193;&#25955;&#31867;&#22411;&#26041;&#31243;&#20013;&#65292;&#19968;&#31181;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20116;&#28857;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;FCNNs&#65289;&#12290;FCNNs&#20351;&#29992;&#20004;&#20010;&#36830;&#32493;&#30340;&#24555;&#29031;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#26102;&#38388;&#27493;&#38271;&#23545;&#24212;&#20110;&#32473;&#23450;&#24555;&#29031;&#30340;&#27493;&#38271;&#12290;&#22240;&#27492;&#65292;FCNNs&#30340;&#26102;&#38388;&#28436;&#21270;&#21462;&#20915;&#20110;&#26102;&#38388;&#27493;&#38271;&#65292;&#24182;&#19988;&#26102;&#38388;&#27493;&#38271;&#24517;&#39035;&#28385;&#36275;&#20854;CFL&#26465;&#20214;&#65292;&#20197;&#36991;&#20813;&#29190;&#28856;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20855;&#26377;&#36739;&#22823;&#24863;&#21463;&#37326;&#30340;&#28145;&#23618;FCNNs&#65292;&#20197;&#39044;&#27979;&#20855;&#26377;&#22823;&#20110;CFL&#26465;&#20214;&#38408;&#20540;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#28909;&#37327;&#12289;&#36153;&#33293;&#23572;&#21644;Allen-Cahn&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks have been widely applied to partial differential equations with great success because the physics-informed loss essentially requires no observations or discretization. However, it is difficult to optimize model parameters, and these parameters must be trained for each distinct initial condition. To overcome these challenges in second-order reaction-diffusion type equations, a possible way is to use five-point stencil convolutional neural networks (FCNNs). FCNNs are trained using two consecutive snapshots, where the time step corresponds to the step size of the given snapshots. Thus, the time evolution of FCNNs depends on the time step, and the time step must satisfy its CFL condition to avoid blow-up solutions. In this work, we propose deep FCNNs that have large receptive fields to predict time evolutions with a time step larger than the threshold of the CFL condition. To evaluate our models, we consider the heat, Fisher's, and Allen-Cahn equations with
&lt;/p&gt;</description></item><item><title>JEN-1&#26159;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#21644;&#24310;&#32493;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04729</link><description>&lt;p&gt;
JEN-1&#65306;&#20855;&#26377;&#20840;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#24341;&#23548;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models. (arXiv:2308.04729v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04729
&lt;/p&gt;
&lt;p&gt;
JEN-1&#26159;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#21644;&#24310;&#32493;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#38899;&#20048;&#29983;&#25104;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#38899;&#20048;&#65288;&#21363;&#25991;&#26412;&#21040;&#38899;&#20048;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#38899;&#20048;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#21644;&#39640;&#37319;&#26679;&#29575;&#30340;&#35201;&#27714;&#12290;&#23613;&#31649;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#38899;&#20048;&#36136;&#37327;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;JEN-1&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#30340;&#36890;&#29992;&#39640;&#20445;&#30495;&#27169;&#22411;&#12290;JEN-1&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;JEN-1&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#20197;&#21450;&#24310;&#32493;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;JEN-1&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21487;&#22312;&#27492;&#32593;&#22336;&#33719;&#21462;&#65306;http://URL
&lt;/p&gt;
&lt;p&gt;
Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task's significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1's superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at this http URL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#27133;&#20301;&#24402;&#32435;&#20219;&#21153;&#20013;&#65292;&#25104;&#21151;&#22320;&#35825;&#23548;&#20102;&#27133;&#20301;&#36793;&#30028;&#65292;&#24182;&#22312;&#20004;&#20010;NLU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20196;&#29260;&#32423;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#33021;&#25552;&#20379;&#22686;&#24378;&#30340;&#27133;&#20301;&#26631;&#31614;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.04712</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#21644;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#27133;&#20301;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning. (arXiv:2308.04712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#27133;&#20301;&#24402;&#32435;&#20219;&#21153;&#20013;&#65292;&#25104;&#21151;&#22320;&#35825;&#23548;&#20102;&#27133;&#20301;&#36793;&#30028;&#65292;&#24182;&#22312;&#20004;&#20010;NLU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20196;&#29260;&#32423;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#33021;&#25552;&#20379;&#22686;&#24378;&#30340;&#27133;&#20301;&#26631;&#31614;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;&#22914;&#24847;&#22270;&#35782;&#21035;&#21644;&#27133;&#20301;&#22635;&#20805;&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#25165;&#33021;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#22312;&#29616;&#23454;&#20013;&#65292;&#26631;&#35760;&#30340;&#26102;&#38388;&#32423;&#21035;&#65288;&#27133;&#20301;&#26631;&#31614;&#65289;&#32791;&#26102;&#19988;&#38590;&#20197;&#33719;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27133;&#20301;&#24402;&#32435;(SI)&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#27809;&#26377;&#26174;&#24335;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#35825;&#23548;&#27133;&#20301;&#36793;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#25506;&#27979;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#26469;&#21033;&#29992;(1)&#20174;PLM&#20013;&#25552;&#21462;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#30693;&#35782;&#65292;&#21644;(2)&#20174;TOD&#20013;&#21487;&#29992;&#30340;&#39069;&#22806;&#21477;&#23376;&#32423;&#24847;&#22270;&#26631;&#31614;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27133;&#20301;&#24402;&#32435;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#33021;&#22815;&#24357;&#34917;&#19982;&#22522;&#20110;&#20196;&#29260;&#32423;&#30417;&#30563;&#27169;&#22411;&#22312;&#20004;&#20010;NLU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24046;&#36317;&#12290;&#24403;&#25512;&#24191;&#21040;&#26032;&#20986;&#29616;&#30340;&#24847;&#22270;&#26102;&#65292;&#25105;&#20204;&#30340;SI&#30446;&#26631;&#20063;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#27133;&#20301;&#26631;&#31614;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#24322;&#24120;&#24402;&#22240;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#25200;&#21160;&#26469;&#35745;&#31639;&#27599;&#20010;&#36755;&#20837;&#21464;&#37327;&#30340;&#24402;&#22240;&#24471;&#20998;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#24471;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04708</link><description>&lt;p&gt;
&#29983;&#25104;&#25200;&#21160;&#20998;&#26512;&#29992;&#20110;&#27010;&#29575;&#40657;&#30418;&#24322;&#24120;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution. (arXiv:2308.04708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#24322;&#24120;&#24402;&#22240;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#25200;&#21160;&#26469;&#35745;&#31639;&#27599;&#20010;&#36755;&#20837;&#21464;&#37327;&#30340;&#24402;&#22240;&#24471;&#20998;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#24471;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#40657;&#30418;&#22238;&#24402;&#27169;&#22411;&#35774;&#32622;&#20013;&#30340;&#27010;&#29575;&#24322;&#24120;&#24402;&#22240;&#20219;&#21153;&#65292;&#26088;&#22312;&#35745;&#31639;&#27599;&#20010;&#36755;&#20837;&#21464;&#37327;&#30340;&#24402;&#22240;&#24471;&#20998;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#32473;&#23450;&#19968;&#20010;&#35266;&#23519;&#21040;&#30340;&#24322;&#24120;&#12290;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#21487;&#29992;&#12290;&#19982;&#26631;&#20934;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24773;&#26223;&#19981;&#21516;&#65292;&#36825;&#20010;&#20219;&#21153;&#24076;&#26395;&#35299;&#37322;&#19982;&#40657;&#30418;&#39044;&#27979;&#30340;&#24322;&#24120;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#26412;&#36523;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#20027;&#27969;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26041;&#27861;&#65292;&#22914;Shapley&#20540;&#65292;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#8220;&#20559;&#24046;&#26080;&#20851;&#23646;&#24615;&#8221;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#24322;&#24120;&#24402;&#22240;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#35745;&#31639;&#24402;&#22240;&#24471;&#20998;&#20316;&#20026;&#39044;&#27979;&#22343;&#20540;&#65292;&#36824;&#21487;&#20197;&#37327;&#21270;&#36825;&#20123;&#24471;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#29983;&#25104;&#36807;&#31243;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#23545;&#25200;&#21160;&#36827;&#34892;&#21453;&#20107;&#23454;&#22320;&#23558;&#35266;&#27979;&#21040;&#30340;&#24322;&#24120;&#35266;&#23519;&#24674;&#22797;&#21040;&#27491;&#24120;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the task of probabilistic anomaly attribution in the black-box regression setting, where the goal is to compute the probability distribution of the attribution score of each input variable, given an observed anomaly. The training dataset is assumed to be unavailable. This task differs from the standard XAI (explainable AI) scenario, since we wish to explain the anomalous deviation from a black-box prediction rather than the black-box model itself.  We begin by showing that mainstream model-agnostic explanation methods, such as the Shapley values, are not suitable for this task because of their ``deviation-agnostic property.'' We then propose a novel framework for probabilistic anomaly attribution that allows us to not only compute attribution scores as the predictive mean but also quantify the uncertainty of those scores. This is done by considering a generative process for perturbations that counter-factually bring the observed anomalous observation back to normalcy. We int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Pareto Invariant Representation Learning&#65288;PaInvRL&#65289;&#30340;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#22810;&#23186;&#20307;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#21644;&#21464;&#20307;&#34920;&#31034;&#30340;&#21516;&#26102;&#26469;&#32531;&#35299;&#36890;&#29992;&#34920;&#31034;&#24341;&#20837;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#20174;IID-OOD&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#65292;PaInvRL&#20943;&#23569;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#23545;&#29992;&#25143;&#20559;&#22909;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04706</link><description>&lt;p&gt;
Pareto&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#22312;&#22810;&#23186;&#20307;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Pareto Invariant Representation Learning for Multimedia Recommendation. (arXiv:2308.04706v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Pareto Invariant Representation Learning&#65288;PaInvRL&#65289;&#30340;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#22810;&#23186;&#20307;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#21644;&#21464;&#20307;&#34920;&#31034;&#30340;&#21516;&#26102;&#26469;&#32531;&#35299;&#36890;&#29992;&#34920;&#31034;&#24341;&#20837;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#20174;IID-OOD&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#65292;PaInvRL&#20943;&#23569;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#23545;&#29992;&#25143;&#20559;&#22909;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#25512;&#33616;&#28041;&#21450;&#20010;&#24615;&#21270;&#25490;&#24207;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#36890;&#29992;&#32534;&#30721;&#22120;&#34920;&#31034;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36890;&#29992;&#34920;&#31034;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#30456;&#20851;&#24615;&#65292;&#26080;&#27861;&#25581;&#31034;&#29992;&#25143;&#30340;&#30495;&#23454;&#20559;&#22909;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24573;&#35270;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#21644;&#38750;&#20998;&#24067;&#65288;OOD&#65289;&#24191;&#20041;&#21270;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pareto Invariant Representation Learning&#65288;PaInvRL&#65289;&#30340;&#26694;&#26550;&#65292;&#20174;IID-OOD&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#20943;&#23569;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#65288;&#21560;&#24341;&#29992;&#25143;&#27880;&#24847;&#30340;&#20869;&#22312;&#22240;&#32032;&#65289;&#21644;&#21464;&#20307;&#34920;&#31034;&#65288;&#20854;&#20182;&#22240;&#32032;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PaInvRL&#21253;&#25324;&#19977;&#20010;&#36845;&#20195;&#25191;&#34892;&#30340;&#27169;&#22359;&#65306;&#65288;i&#65289;&#38750;&#21516;&#36136;&#35782;&#21035;&#27169;&#22359;&#65292;&#29992;&#20110;&#35782;&#21035;&#21453;&#26144;&#20998;&#24067;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multimedia recommendation involves personalized ranking tasks, where multimedia content is usually represented using a generic encoder. However, these generic representations introduce spurious correlations that fail to reveal users' true preferences. Existing works attempt to alleviate this problem by learning invariant representations, but overlook the balance between independent and identically distributed (IID) and out-of-distribution (OOD) generalization. In this paper, we propose a framework called Pareto Invariant Representation Learning (PaInvRL) to mitigate the impact of spurious correlations from an IID-OOD multi-objective optimization perspective, by learning invariant representations (intrinsic factors that attract user attention) and variant representations (other factors) simultaneously. Specifically, PaInvRL includes three iteratively executed modules: (i) heterogeneous identification module, which identifies the heterogeneous environments to reflect distributional shift
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#22411;&#29305;&#24449;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;PDF&#24694;&#24847;&#36719;&#20214;&#65292;&#26080;&#38656;&#22826;&#22810;&#39046;&#22495;&#30693;&#35782;&#12290;&#22312;&#20845;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;Random Forest&#27169;&#22411;&#26102;&#21487;&#20197;&#36798;&#21040;99.75%&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20010;&#20165;&#21253;&#21547;12&#20010;&#29305;&#24449;&#30340;&#29305;&#24449;&#38598;&#26159;&#26368;&#31616;&#27905;&#30340;&#20043;&#19968;&#65292;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#32467;&#26524;&#21487;&#19982;&#37319;&#29992;&#26356;&#22823;&#29305;&#24449;&#38598;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.04704</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;PDF&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#23567;&#23610;&#23544;&#29305;&#24449;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Feature Set of Small Size for the PDF Malware Detection. (arXiv:2308.04704v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#22411;&#29305;&#24449;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;PDF&#24694;&#24847;&#36719;&#20214;&#65292;&#26080;&#38656;&#22826;&#22810;&#39046;&#22495;&#30693;&#35782;&#12290;&#22312;&#20845;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;Random Forest&#27169;&#22411;&#26102;&#21487;&#20197;&#36798;&#21040;99.75%&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20010;&#20165;&#21253;&#21547;12&#20010;&#29305;&#24449;&#30340;&#29305;&#24449;&#38598;&#26159;&#26368;&#31616;&#27905;&#30340;&#20043;&#19968;&#65292;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#32467;&#26524;&#21487;&#19982;&#37319;&#29992;&#26356;&#22823;&#29305;&#24449;&#38598;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;&#30340;&#22686;&#21152;&#21644;&#26356;&#21152;&#22797;&#26434;&#21270;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;PDF&#25991;&#20214;&#36890;&#24120;&#34987;&#29992;&#20316;&#38035;&#40060;&#25915;&#20987;&#30340;&#30690;&#37327;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#21487;&#20449;&#30340;&#25968;&#25454;&#36164;&#28304;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#24179;&#21488;&#19978;&#37117;&#21487;&#20197;&#35775;&#38382;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;PDF&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#26816;&#27979;PDF&#24694;&#24847;&#36719;&#20214;&#30340;&#24615;&#33021;&#26377;&#24456;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23567;&#22411;&#29305;&#24449;&#38598;&#65292;&#19981;&#38656;&#35201;&#22826;&#22810;&#20851;&#20110;PDF&#25991;&#20214;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#29305;&#24449;&#12290;&#22312;&#20351;&#29992;Random Forest&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#26368;&#39640;99.75%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20165;&#21253;&#21547;12&#20010;&#29305;&#24449;&#30340;&#29305;&#24449;&#38598;&#26159;PDF&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#20013;&#26368;&#31616;&#27905;&#30340;&#20043;&#19968;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#37319;&#29992;&#26356;&#22823;&#29305;&#24449;&#38598;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML)-based malware detection systems are becoming increasingly important as malware threats increase and get more sophisticated. PDF files are often used as vectors for phishing attacks because they are widely regarded as trustworthy data resources, and are accessible across different platforms. Therefore, researchers have developed many different PDF malware detection methods. Performance in detecting PDF malware is greatly influenced by feature selection. In this research, we propose a small features set that don't require too much domain knowledge of the PDF file. We evaluate proposed features with six different machine learning models. We report the best accuracy of 99.75% when using Random Forest model. Our proposed feature set, which consists of just 12 features, is one of the most conciseness in the field of PDF malware detection. Despite its modest size, we obtain comparable results to state-of-the-art that employ a much larger set of features.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#31639;&#27861;&#23545;Covid-19&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#30142;&#30149;&#33647;&#29289;&#30740;&#21457;&#21644;&#20154;&#20204;&#20813;&#21463;&#22823;&#27969;&#34892;&#30149;&#20260;&#23475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.04697</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#31639;&#27861;&#23545;Covid-19&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Analytical Study of Covid-19 Dataset using Graph-Based Clustering Algorithms. (arXiv:2308.04697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#31639;&#27861;&#23545;Covid-19&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#30142;&#30149;&#33647;&#29289;&#30740;&#21457;&#21644;&#20154;&#20204;&#20813;&#21463;&#22823;&#27969;&#34892;&#30149;&#20260;&#23475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#30149;&#27602;&#30142;&#30149;(COVID-19)&#26159;&#19968;&#31181;&#26032;&#22411;&#30149;&#27602;&#65292;&#26368;&#26089;&#22312;2019&#24180;12&#26376;&#22312;&#20013;&#22269;&#27494;&#27721;&#21457;&#29616;&#65292;&#29616;&#22312;&#36825;&#31181;&#33268;&#21629;&#30142;&#30149;&#24050;&#32463;&#20256;&#25773;&#21040;&#19990;&#30028;&#21508;&#22320;&#12290;&#26681;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#65288;WHO&#65289;&#30340;&#25968;&#25454;&#65292;&#20174;2019&#24180;&#21040;2021&#24180;4&#26376;&#65292;&#20849;&#26377;312&#19975;4905&#20154;&#27515;&#20129;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35768;&#22810;&#26041;&#27861;&#12289;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34987;&#30740;&#31350;&#24182;&#29992;&#20110;&#25327;&#25937;&#20154;&#20204;&#20813;&#21463;&#36825;&#19968;&#22823;&#27969;&#34892;&#30149;&#30340;&#20260;&#23475;&#12290;SARS-CoV&#21644;2019-nCoV&#12289;SARS-CoV-2&#30149;&#27602;&#20405;&#20837;&#25105;&#20204;&#30340;&#36523;&#20307;&#65292;&#23548;&#33268;&#32454;&#32990;&#34507;&#30333;&#36136;&#32467;&#26500;&#19978;&#30340;&#19968;&#20123;&#24046;&#24322;&#12290;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;(PPI)&#26159;&#25105;&#20204;&#32454;&#32990;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36807;&#31243;&#65292;&#22312;&#33647;&#29289;&#30740;&#21457;&#21644;&#30142;&#30149;&#26041;&#38754;&#36215;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#30001;Covi-19&#25968;&#25454;&#38598;&#30340;92&#20010;&#22522;&#22240;&#29983;&#25104;&#30340;PPI&#32593;&#32476;&#36827;&#34892;&#20102;&#32858;&#31867;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#31181;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#20197;&#23545;&#32858;&#31867;&#20998;&#26512;&#25552;&#20379;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Corona VIrus Disease abbreviated as COVID-19 is a novel virus which is initially identified in Wuhan of China in December of 2019 and now this deadly disease has spread all over the world. According to World Health Organization (WHO), a total of 3,124,905 people died from 2019 to 2021, April. In this case, many methods, AI base techniques, and machine learning algorithms have been researched and are being used to save people from this pandemic. The SARS-CoV and the 2019-nCoV, SARS-CoV-2 virus invade our bodies, causing some differences in the structure of cell proteins. Protein-protein interaction (PPI) is an essential process in our cells and plays a very important role in the development of medicines and gives ideas about the disease. In this study, we performed clustering on PPI networks generated from 92 genes of the Covi-19 dataset. We have used three graph-based clustering algorithms to give intuition to the analysis of clusters.
&lt;/p&gt;</description></item><item><title>&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#39592;&#31185;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#20063;&#24102;&#26469;&#26426;&#36935;&#12290;&#20026;&#20102;&#23454;&#29616;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#38656;&#35201;&#24320;&#21457;&#27880;&#37325;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#31639;&#27861;&#65292;&#24182;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.04696</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#39592;&#31185;&#20013;&#30340;&#24212;&#29992;: &#25361;&#25112;&#12289;&#26426;&#36935;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Explainable AI in Orthopedics: Challenges, Opportunities, and Prospects. (arXiv:2308.04696v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04696
&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#39592;&#31185;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#20063;&#24102;&#26469;&#26426;&#36935;&#12290;&#20026;&#20102;&#23454;&#29616;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#38656;&#35201;&#24320;&#21457;&#27880;&#37325;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#31639;&#27861;&#65292;&#24182;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#24212;&#29992;&#65292;&#20294;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#21364;&#30053;&#26174;&#28382;&#21518;&#12290;&#20854;&#20013;&#19968;&#20123;&#22240;&#32032;&#21253;&#25324;&#30417;&#31649;&#26694;&#26550;&#12289;&#24739;&#32773;&#38544;&#31169;&#38382;&#39064;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#31561;&#24433;&#21709;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#22312;&#39592;&#31185;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#38459;&#30861;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#26045;&#12290;&#35299;&#20915;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#39592;&#31185;&#20013;&#30340;&#38382;&#39064;&#38656;&#35201;&#24320;&#21457;&#27880;&#37325;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#20351;&#20020;&#24202;&#21307;&#29983;&#12289;&#22806;&#31185;&#21307;&#29983;&#21644;&#24739;&#32773;&#33021;&#22815;&#29702;&#35299;&#20219;&#20309;&#20381;&#36182;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#25110;&#25551;&#36848;&#27169;&#22411;&#30340;&#22240;&#32032;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#39592;&#31185;&#23454;&#36341;&#20013;&#30340;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#20043;&#38388;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
While artificial intelligence (AI) has made many successful applications in various domains, its adoption in healthcare lags a little bit behind other high-stakes settings. Several factors contribute to this slower uptake, including regulatory frameworks, patient privacy concerns, and data heterogeneity. However, one significant challenge that impedes the implementation of AI in healthcare, particularly in orthopedics, is the lack of explainability and interpretability around AI models. Addressing the challenge of explainable AI (XAI) in orthopedics requires developing AI models and algorithms that prioritize transparency and interpretability, allowing clinicians, surgeons, and patients to understand the contributing factors behind any AI-powered predictive or descriptive models. The current contribution outlines several key challenges and opportunities that manifest in XAI in orthopedic practice. This work emphasizes the need for interdisciplinary collaborations between AI practitione
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04690</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Finite Element Operator Network for Solving Parametric PDEs. (arXiv:2308.04690v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#25105;&#20204;&#29702;&#35299;&#21644;&#39044;&#27979;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#37329;&#34701;&#31561;&#20247;&#22810;&#39046;&#22495;&#33258;&#28982;&#29616;&#35937;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#21442;&#25968;PDE&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26377;&#38480;&#20803;&#27861;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#37197;&#23545;&#30340;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#34920;&#26126;&#23427;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;FEONet&#26694;&#26550;&#22312;&#27169;&#25311;&#20855;&#26377;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#21644;&#22797;&#26434;&#22495;&#30340;&#21508;&#31181;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) underlie our understanding and prediction of natural phenomena across numerous fields, including physics, engineering, and finance. However, solving parametric PDEs is a complex task that necessitates efficient numerical methods. In this paper, we propose a novel approach for solving parametric PDEs using a Finite Element Operator Network (FEONet). Our proposed method leverages the power of deep learning in conjunction with traditional numerical methods, specifically the finite element method, to solve parametric PDEs in the absence of any paired input-output training data. We demonstrate the effectiveness of our approach on several benchmark problems and show that it outperforms existing state-of-the-art methods in terms of accuracy, generalization, and computational flexibility. Our FEONet framework shows potential for application in various fields where PDEs play a crucial role in modeling complex domains with diverse boundary conditions and sin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#21512;&#25104;&#21644;&#28210;&#26579;NeRF&#23545;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#28145;&#24230;&#22330;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21160;&#24577;&#38452;&#24433;&#24182;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2308.04669</link><description>&lt;p&gt;
&#24555;&#36895;NeRF&#21512;&#25104;&#21644;&#28210;&#26579;&#30340;&#36890;&#29992;&#38544;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#21512;&#25104;&#21644;&#28210;&#26579;NeRF&#23545;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#28145;&#24230;&#22330;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21160;&#24577;&#38452;&#24433;&#24182;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21508;&#31181;&#31070;&#32463;&#36752;&#23556;&#22330;&#26041;&#27861;&#22312;&#39640;&#28210;&#26579;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21152;&#36895;&#26041;&#27861;&#19987;&#38376;&#21270;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#21508;&#31181;&#38544;&#24335;&#26041;&#27861;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;NeRF&#20316;&#21697;&#36827;&#34892;&#23454;&#26102;&#21512;&#25104;&#12290;&#30001;&#20110;NeRF&#20381;&#36182;&#20110;&#27839;&#20809;&#32447;&#37319;&#26679;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#20379;&#19968;&#33324;&#24615;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#27969;&#27700;&#32447;&#26469;&#24555;&#36895;&#21512;&#25104;NeRF&#23545;&#35937;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#20351;&#24471;&#21160;&#24577;&#38452;&#24433;&#21487;&#20197;&#20351;&#29992;&#35299;&#26512;&#20809;&#28304;&#22312;&#23545;&#35937;&#20869;&#37096;&#25110;&#23545;&#35937;&#20043;&#38388;&#36827;&#34892;&#25237;&#23556;&#65292;&#21516;&#26102;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;&#20027;&#35201;&#22320;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#28145;&#24230;&#22330;&#65288;NeDF&#65289;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#20809;&#32447;&#21644;&#38544;&#24335;&#34920;&#38754;&#20043;&#38388;&#30340;&#30452;&#25509;&#30456;&#20132;&#35745;&#31639;&#26469;&#24555;&#36895;&#30830;&#23450;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#19968;&#20010;&#20132;&#28857;&#31070;&#32463;&#32593;&#32476;&#26469;&#21152;&#36895;&#26597;&#35810;NeRF&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a variety of Neural radiance fields methods have garnered remarkable success in high render speed. However, current accelerating methods is specialized and not compatible for various implicit method, which prevent a real-time composition over different kinds of NeRF works. Since NeRF relies on sampling along rays, it's possible to provide a guidance generally. We propose a general implicit pipeline to rapidly compose NeRF objects. This new method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration inste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#30149;&#29702;&#23398;&#20808;&#39564;&#30693;&#35782;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;CT&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#19982;&#30149;&#29702;&#23398;&#30693;&#35782;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#32954;&#30284;&#20122;&#22411;&#30340;&#20934;&#30830;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.04663</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#30149;&#29702;&#23398;&#20808;&#39564;&#30693;&#35782;&#22312;CT&#22270;&#20687;&#19978;&#23545;&#32954;&#30284;&#20122;&#22411;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of lung cancer subtypes on CT images with synthetic pathological priors. (arXiv:2308.04663v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#30149;&#29702;&#23398;&#20808;&#39564;&#30693;&#35782;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;CT&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#19982;&#30149;&#29702;&#23398;&#30693;&#35782;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#32954;&#30284;&#20122;&#22411;&#30340;&#20934;&#30830;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20934;&#30830;&#22320;&#23545;&#32954;&#30284;&#30340;&#30149;&#29702;&#20122;&#22411;&#36827;&#34892;&#35786;&#26029;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#29983;&#25104;&#28151;&#21512;&#29305;&#24449;&#32593;&#32476;&#65288;SGHF-Net&#65289;&#26469;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#19978;&#20934;&#30830;&#22320;&#23545;&#32954;&#30284;&#20122;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#20511;&#37492;&#20102;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#21516;&#19968;&#20363;CT&#22270;&#20687;&#21644;&#20854;&#30149;&#29702;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#36328;&#23610;&#24230;&#20851;&#32852;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#30149;&#29702;&#29305;&#24449;&#21512;&#25104;&#27169;&#22359;&#65288;PFSM&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#37327;&#26144;&#23556;&#36328;&#27169;&#24577;&#20851;&#32852;&#65292;&#20174;CT&#22270;&#20687;&#20013;&#33719;&#21462;&#23545;&#24212;&#30149;&#29702;&#22270;&#20687;&#20013;&#25152;&#21253;&#21547;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25918;&#23556;&#23398;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65288;RFEM&#65289;&#26469;&#30452;&#25509;&#33719;&#21462;CT&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#19982;&#30149;&#29702;&#23398;&#30693;&#35782;&#22312;&#19968;&#20010;&#26377;&#25928;&#30340;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;&#19979;&#38598;&#25104;&#65292;&#20351;&#25972;&#20010;&#20998;&#31867;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#25351;&#31034;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate diagnosis on pathological subtypes for lung cancer is of significant importance for the follow-up treatments and prognosis managements. In this paper, we propose self-generating hybrid feature network (SGHF-Net) for accurately classifying lung cancer subtypes on computed tomography (CT) images. Inspired by studies stating that cross-scale associations exist in the image patterns between the same case's CT images and its pathological images, we innovatively developed a pathological feature synthetic module (PFSM), which quantitatively maps cross-modality associations through deep neural networks, to derive the "gold standard" information contained in the corresponding pathological images from CT images. Additionally, we designed a radiological feature extraction module (RFEM) to directly acquire CT image information and integrated it with the pathological priors under an effective feature fusion framework, enabling the entire classification model to generate more indicative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#25442;&#22120;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#26680;&#23398;&#20064;&#21644;&#22810;&#20803;&#24322;&#26500;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#23398;&#20064;&#26469;&#20849;&#21516;&#39044;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#26377;&#25928;&#30340;&#28151;&#21512;&#21021;&#22987;&#21270;&#31574;&#30053;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#25910;&#25947;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04660</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65306;&#20351;&#29992;&#22522;&#20110;&#21464;&#25442;&#22120;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#26680;&#23398;&#20064;&#30340;&#22810;&#20803;&#24322;&#26500;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian Optimization with Deep Kernel Learning and Transformer Pre-trained on Multiple Heterogeneous Datasets. (arXiv:2308.04660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#25442;&#22120;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#26680;&#23398;&#20064;&#21644;&#22810;&#20803;&#24322;&#26500;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#23398;&#20064;&#26469;&#20849;&#21516;&#39044;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#26377;&#25928;&#30340;&#28151;&#21512;&#21021;&#22987;&#21270;&#31574;&#30053;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#25910;&#25947;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#24191;&#27867;&#24212;&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#26469;&#36817;&#20284;&#40657;&#30418;&#21709;&#24212;&#20989;&#25968;&#12290;&#38543;&#30528;&#35299;&#20915;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#30340;&#25968;&#37327;&#22686;&#21152;&#65292;&#23398;&#20064;&#22810;&#20010;&#20808;&#21069;&#20219;&#21153;&#20197;&#20849;&#21516;&#39044;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20248;&#21270;&#25928;&#29575;&#30340;&#33021;&#21147;&#22791;&#21463;&#26399;&#24453;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#39044;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#28145;&#24230;&#29305;&#24449;&#19978;&#23450;&#20041;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#65292;&#20351;&#29992;&#26469;&#33258;&#21487;&#33021;&#23384;&#22312;&#24322;&#26500;&#36755;&#20837;&#31354;&#38388;&#30340;&#20808;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28151;&#21512;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#23545;&#24212;&#20110;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#21464;&#37327;&#30340;&#36755;&#20837;&#20196;&#29260;&#65292;&#20174;&#32780;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#25910;&#25947;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#36801;&#31227;&#36125;&#21494;&#26031;&#20248;&#21270;&#31574;&#30053;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is widely adopted in black-box optimization problems and it relies on a surrogate model to approximate the black-box response function. With the increasing number of black-box optimization tasks solved and even more to solve, the ability to learn from multiple prior tasks to jointly pre-train a surrogate model is long-awaited to further boost optimization efficiency. In this paper, we propose a simple approach to pre-train a surrogate, which is a Gaussian process (GP) with a kernel defined on deep features learned from a Transformer-based encoder, using datasets from prior tasks with possibly heterogeneous input spaces. In addition, we provide a simple yet effective mix-up initialization strategy for input tokens corresponding to unseen input variables and therefore accelerate new tasks' convergence. Experiments on both synthetic and real benchmark problems demonstrate the effectiveness of our proposed pre-training and transfer BO strategy over existing metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#21069;&#21015;&#33146;&#30284;&#20998;&#21106;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;&#26368;&#20339;&#27169;&#22411;Attention R2U-Net&#22312;&#20998;&#21106;&#25152;&#26377;&#21306;&#22495;&#26102;&#20855;&#26377;&#26368;&#39640;&#30340;IoU&#21644;DSC&#65292;&#24182;&#19988;&#22312;&#36807;&#28193;&#21306;&#22495;&#21644;&#32959;&#30244;&#36793;&#30028;&#20855;&#26377;&#26368;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.04653</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#35780;&#20998;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21069;&#21015;&#33146;&#30284;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Assessing the performance of deep learning-based models for prostate cancer segmentation using uncertainty scores. (arXiv:2308.04653v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#21069;&#21015;&#33146;&#30284;&#20998;&#21106;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;&#26368;&#20339;&#27169;&#22411;Attention R2U-Net&#22312;&#20998;&#21106;&#25152;&#26377;&#21306;&#22495;&#26102;&#20855;&#26377;&#26368;&#39640;&#30340;IoU&#21644;DSC&#65292;&#24182;&#19988;&#22312;&#36807;&#28193;&#21306;&#22495;&#21644;&#32959;&#30244;&#36793;&#30028;&#20855;&#26377;&#26368;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#28857;&#27604;&#36739;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;MRI&#22270;&#20687;&#20013;&#36827;&#34892;&#21069;&#21015;&#33146;&#20998;&#21106;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30446;&#26631;&#26159;&#25913;&#21892;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#35780;&#20272;&#20102;&#19971;&#31181;&#22522;&#20110;U-Net&#30340;&#26550;&#26500;&#65292;&#25645;&#37197;Monte-Carlo dropout&#36827;&#34892;&#33258;&#21160;&#20998;&#21106;&#20013;&#24515;&#21306;&#22495;&#12289;&#22806;&#22260;&#21306;&#22495;&#12289;&#36807;&#28193;&#21306;&#22495;&#21644;&#32959;&#30244;&#65292;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26412;&#30740;&#31350;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;Attention R2U-Net&#65292;&#22312;&#20998;&#21106;&#25152;&#26377;&#21306;&#22495;&#26102;&#65292;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#27604;(IoU)&#36798;&#21040;76.3%&#65292;Dice&#30456;&#20284;&#24230;&#31995;&#25968;(DSC)&#36798;&#21040;85%&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;Attention R2U-Net&#22312;&#36807;&#28193;&#21306;&#22495;&#21644;&#32959;&#30244;&#36793;&#30028;&#34920;&#29616;&#20986;&#26368;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on comparing deep learning methods for the segmentation and quantification of uncertainty in prostate segmentation from MRI images. The aim is to improve the workflow of prostate cancer detection and diagnosis. Seven different U-Net-based architectures, augmented with Monte-Carlo dropout, are evaluated for automatic segmentation of the central zone, peripheral zone, transition zone, and tumor, with uncertainty estimation. The top-performing model in this study is the Attention R2U-Net, achieving a mean Intersection over Union (IoU) of 76.3% and Dice Similarity Coefficient (DSC) of 85% for segmenting all zones. Additionally, Attention R2U-Net exhibits the lowest uncertainty values, particularly in the boundaries of the transition zone and tumor, when compared to the other models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#34880;&#27969;&#21160;&#21147;&#23398;&#25512;&#29702;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#20405;&#20837;&#24615;&#20449;&#21495;&#36827;&#34892;&#24515;&#33039;&#21387;&#21147;&#30340;&#35780;&#20272;&#65292;&#26082;&#21487;&#20197;&#22312;&#20303;&#38498;&#29615;&#22659;&#20013;&#29992;&#20110;&#35786;&#26029;&#21644;&#27835;&#30103;&#39044;&#21518;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38376;&#35786;&#35774;&#32622;&#20013;&#12290;</title><link>http://arxiv.org/abs/2308.04650</link><description>&lt;p&gt;
&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#34880;&#27969;&#21160;&#21147;&#23398;&#25512;&#29702;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals. (arXiv:2308.04650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#34880;&#27969;&#21160;&#21147;&#23398;&#25512;&#29702;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#20405;&#20837;&#24615;&#20449;&#21495;&#36827;&#34892;&#24515;&#33039;&#21387;&#21147;&#30340;&#35780;&#20272;&#65292;&#26082;&#21487;&#20197;&#22312;&#20303;&#38498;&#29615;&#22659;&#20013;&#29992;&#20110;&#35786;&#26029;&#21644;&#27835;&#30103;&#39044;&#21518;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38376;&#35786;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#26159;&#19968;&#31181;&#24433;&#21709;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#30340;&#33268;&#27531;&#30142;&#24739;&#65292;&#23545;&#20182;&#20204;&#30340;&#29983;&#27963;&#36136;&#37327;&#21644;&#27515;&#20129;&#29575;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#24515;&#33039;&#21387;&#21147;&#30340;&#23458;&#35266;&#35780;&#20272;&#20173;&#28982;&#26159;&#35786;&#26029;&#21644;&#27835;&#30103;&#39044;&#21518;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#23613;&#31649;&#24515;&#33039;&#23548;&#31649;&#21270;&#39564;&#26159;&#20272;&#35745;&#20013;&#24515;&#34880;&#28082;&#21160;&#21147;&#23398;&#21387;&#21147;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#23427;&#26159;&#19968;&#31181;&#26377;&#28508;&#22312;&#39118;&#38505;&#30340;&#20405;&#20837;&#24615;&#25805;&#20316;&#65292;&#23545;&#26576;&#20123;&#24739;&#32773;&#26469;&#35828;&#21487;&#33021;&#26159;&#21361;&#38505;&#30340;&#12290;&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#20449;&#21495;&#65288;&#22914;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#65289;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#24120;&#35268;&#20272;&#35745;&#24515;&#33039;&#21387;&#21147;&#22312;&#20303;&#38498;&#21644;&#38376;&#35786;&#29615;&#22659;&#20013;&#25104;&#20026;&#21487;&#33021;&#12290;&#20808;&#21069;&#35757;&#32451;&#29992;&#20110;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#20272;&#35745;&#24515;&#33039;&#20869;&#21387;&#21147;&#65288;&#20363;&#22914;&#24179;&#22343;&#32954;&#27611;&#32454;&#34880;&#31649;&#26964;&#21387;&#65288;mPCWP&#65289;&#65289;&#30340;&#27169;&#22411;&#26174;&#31034;&#20102;&#33391;&#22909;&#30340;&#37492;&#21035;&#33021;&#21147;&#65292;&#20294;&#20165;&#38480;&#20110;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart failure is a debilitating condition that affects millions of people worldwide and has a significant impact on their quality of life and mortality rates. An objective assessment of cardiac pressures remains an important method for the diagnosis and treatment prognostication for patients with heart failure. Although cardiac catheterization is the gold standard for estimating central hemodynamic pressures, it is an invasive procedure that carries inherent risks, making it a potentially dangerous procedure for some patients. Approaches that leverage non-invasive signals - such as electrocardiogram (ECG) - have the promise to make the routine estimation of cardiac pressures feasible in both inpatient and outpatient settings. Prior models trained to estimate intracardiac pressures (e.g., mean pulmonary capillary wedge pressure (mPCWP)) in a supervised fashion have shown good discriminatory ability but have been limited to the labeled dataset from the heart failure cohort. To address th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#39640;&#26031;&#23849;&#28291;&#25628;&#32034;(GCS)&#19982;&#20256;&#32479;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#28151;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#24182;&#25171;&#24320;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04649</link><description>&lt;p&gt;
&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;: &#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#26031;&#23849;&#28291;&#25628;&#32034;&#21644;Powell's Method&#30340;&#28151;&#21512;&#29992;&#20110;&#26080;&#23548;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Optimization Performance: A Novel Hybridization of Gaussian Crunching Search and Powell's Method for Derivative-Free Optimization. (arXiv:2308.04649v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04649
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#39640;&#26031;&#23849;&#28291;&#25628;&#32034;(GCS)&#19982;&#20256;&#32479;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#28151;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#24182;&#25171;&#24320;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39640;&#26031;&#23849;&#28291;&#25628;&#32034;(GCS)&#21644;Powell's Method&#30340;&#28151;&#21512;&#26469;&#22686;&#24378;&#20248;&#21270;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#23613;&#31649;GCS&#24050;&#32463;&#26174;&#31034;&#20986;&#20811;&#26381;&#20256;&#32479;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#25152;&#38754;&#20020;&#25361;&#25112;&#30340;&#28508;&#21147;[1]&#65292;&#20294;&#23427;&#21487;&#33021;&#19981;&#24635;&#26159;&#22312;&#23547;&#25214;&#23616;&#37096;&#26368;&#23567;&#20540;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#20256;&#32479;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21487;&#33021;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;GCS&#23637;&#31034;&#20102;&#20854;&#22312;&#36867;&#31163;&#23616;&#37096;&#26497;&#23567;&#20540;&#38519;&#38449;&#21644;&#25509;&#36817;&#20840;&#23616;&#26497;&#23567;&#20540;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#23558;GCS&#19982;&#26576;&#20123;&#20256;&#32479;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20445;&#30041;&#27599;&#31181;&#26041;&#27861;&#30340;&#21508;&#33258;&#20248;&#28857;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#31181;&#28151;&#21512;&#26041;&#27861;&#20026;&#20248;&#21270;&#22797;&#26434;&#31995;&#32479;&#21644;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23547;&#25214;&#26368;&#20248;&#35299;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper presents a novel approach to enhance optimization performance through the hybridization of Gaussian Crunching Search (GCS) and Powell's Method for derivative-free optimization. While GCS has shown promise in overcoming challenges faced by traditional derivative-free optimization methods [1], it may not always excel in finding the local minimum. On the other hand, some traditional methods may have better performance in this regard. However, GCS demonstrates its strength in escaping the trap of local minima and approaching the global minima. Through experimentation, we discovered that by combining GCS with certain traditional derivative-free optimization methods, we can significantly boost performance while retaining the respective advantages of each method. This hybrid approach opens up new possibilities for optimizing complex systems and finding optimal solutions in a range of applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#20108;&#20540;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#21333;&#27493;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#23494;&#38598;&#28014;&#28857;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20004;&#20010;&#20462;&#25913;&#38477;&#20302;&#20102;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04637</link><description>&lt;p&gt;
&#31232;&#30095;&#20108;&#20540;&#21464;&#21387;&#22120;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sparse Binary Transformers for Multivariate Time Series Modeling. (arXiv:2308.04637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#20108;&#20540;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#21333;&#27493;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#23494;&#38598;&#28014;&#28857;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20004;&#20010;&#20462;&#25913;&#38477;&#20302;&#20102;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#26377;&#21487;&#33021;&#23454;&#29616;&#22312;&#26032;&#24212;&#29992;&#21644;&#36739;&#23567;&#30340;&#35745;&#31639;&#29615;&#22659;&#20013;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#36825;&#31181;&#27169;&#22411;&#22312;&#21738;&#20123;&#23398;&#20064;&#20219;&#21153;&#20013;&#33021;&#22815;&#25104;&#21151;&#30340;&#20102;&#35299;&#19981;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;&#21644;&#20108;&#20540;&#26435;&#37325;&#30340;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#19982;&#30456;&#21516;&#32467;&#26500;&#30340;&#23494;&#38598;&#28014;&#28857;&#21464;&#21387;&#22120;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#22909;&#30340;&#32467;&#26524;&#65306;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#21333;&#27493;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38477;&#20302;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#20462;&#25913;&#65292;&#36825;&#20004;&#20010;&#20462;&#25913;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#20960;&#20046;&#27809;&#26377;&#19979;&#38477;&#65306;1) &#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#28608;&#27963;&#24212;&#29992;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#25513;&#30721;&#65307;2) &#23545;&#20110;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#36825;&#20123;&#20219;&#21153;&#37117;&#20381;&#36182;&#20110;&#23545;&#21333;&#20010;&#26102;&#38388;&#28857;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attentio
&lt;/p&gt;</description></item><item><title>Bandit&#21453;&#39304;&#19979;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;Bandit Littlestone&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#65292;&#26080;&#35770;&#26631;&#31614;&#31354;&#38388;&#26159;&#21542;&#26080;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.04620</link><description>&lt;p&gt;
&#22810;&#31867;&#22312;&#32447;&#23398;&#20064;&#22312;Bandit&#21453;&#39304;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multiclass Online Learnability under Bandit Feedback. (arXiv:2308.04620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04620
&lt;/p&gt;
&lt;p&gt;
Bandit&#21453;&#39304;&#19979;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;Bandit Littlestone&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#65292;&#26080;&#35770;&#26631;&#31614;&#31354;&#38388;&#26159;&#21542;&#26080;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Bandit&#21453;&#39304;&#19979;&#30340;&#22810;&#31867;&#22312;&#32447;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;(daniely2013price)&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#23637;&#31034;Bandit Littlestone&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#26159;&#22810;&#31867;&#22312;&#32447;&#23398;&#20064;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#21363;&#20351;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#30028;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34917;&#20805;&#20102;(hanneke2023multiclass)&#30340;&#26368;&#36817;&#24037;&#20316;&#65292;&#20182;&#20204;&#22312;&#26631;&#31614;&#31354;&#38388;&#26080;&#30028;&#30340;&#20840;&#20449;&#24687;&#35774;&#32622;&#20013;&#65292;&#23637;&#31034;&#20102;Littlestone&#32500;&#24230;&#21051;&#30011;&#20102;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online multiclass classification under bandit feedback. We extend the results of (daniely2013price) by showing that the finiteness of the Bandit Littlestone dimension is necessary and sufficient for bandit online multiclass learnability even when the label space is unbounded. Our result complements the recent work by (hanneke2023multiclass) who show that the Littlestone dimension characterizes online multiclass learnability in the full-information setting when the label space is unbounded.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28608;&#27963;&#21098;&#35009;&#26041;&#27861;&#29992;&#20110;&#26222;&#36866;&#24615;&#21518;&#38376;&#32531;&#35299;&#21644;&#27979;&#35797;&#26102;&#38388;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#28608;&#27963;&#36793;&#30028;&#26469;&#25552;&#39640;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04617</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#28608;&#27963;&#21098;&#35009;&#26041;&#27861;&#29992;&#20110;&#26222;&#36866;&#24615;&#21518;&#38376;&#32531;&#35299;&#21644;&#27979;&#35797;&#26102;&#38388;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection. (arXiv:2308.04617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28608;&#27963;&#21098;&#35009;&#26041;&#27861;&#29992;&#20110;&#26222;&#36866;&#24615;&#21518;&#38376;&#32531;&#35299;&#21644;&#27979;&#35797;&#26102;&#38388;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#28608;&#27963;&#36793;&#30028;&#26469;&#25552;&#39640;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65288;&#29305;&#27931;&#20234;&#65289;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#22312;&#35757;&#32451;&#38598;&#20013;&#27880;&#20837;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#22312;&#27979;&#35797;&#26102;&#23558;&#35302;&#21457;&#22120;&#20998;&#31867;&#21040;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#31867;&#21035;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21518;&#38376;&#27880;&#20837;&#22312;&#21463;&#25915;&#20987;&#27169;&#22411;&#20013;&#24341;&#36215;&#36807;&#25311;&#21512;&#65288;&#24322;&#24120;&#22823;&#30340;&#28608;&#27963;&#65289;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#35757;&#32451;&#21518;&#30340;&#21098;&#35009;&#26041;&#27861;&#26469;&#32531;&#35299;&#21518;&#38376;&#65292;&#21363;&#36890;&#36807;&#29992;&#19968;&#23567;&#32452;&#24178;&#20928;&#26679;&#26412;&#23398;&#20064;&#20869;&#37096;&#23618;&#30340;&#28608;&#27963;&#36793;&#30028;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36873;&#25321;&#28608;&#27963;&#36793;&#30028;&#26469;&#26174;&#24335;&#22320;&#38480;&#21046;&#20998;&#31867;&#36793;&#30028;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#26631;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#33258;&#36866;&#24212;&#25915;&#20987;&#12289;X2X&#25915;&#20987;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24378;&#22823;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#21407;&#22987;&#21644;&#28608;&#27963;&#20043;&#38388;&#30340;&#36755;&#20986;&#24046;&#24322;&#30340;&#27979;&#35797;&#26102;&#38388;&#26816;&#27979;&#21644;&#32416;&#27491;&#26041;&#27861;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to backdoor attacks (Trojans), where an attacker poisons the training set with backdoor triggers so that the neural network learns to classify test-time triggers to the attacker's designated target class. Recent work shows that backdoor poisoning induces over-fitting (abnormally large activations) in the attacked model, which motivates a general, post-training clipping method for backdoor mitigation, i.e., with bounds on internal-layer activations learned using a small set of clean samples. We devise a new such approach, choosing the activation bounds to explicitly limit classification margins. This method gives superior performance against peer methods for CIFAR-10 image classification. We also show that this method has strong robustness against adaptive attacks, X2X attacks, and on different datasets. Finally, we demonstrate a method extension for test-time detection and correction based on the output differences between the original and activation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21387;&#21147;&#21644;&#21387;&#21147;&#30456;&#20851;&#31934;&#31070;&#38556;&#30861;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#19988;&#29983;&#29702;&#21442;&#25968;&#22914;&#24515;&#29575;&#27979;&#37327;&#21644;&#30382;&#32932;&#21453;&#24212;&#22312;&#21387;&#21147;&#39044;&#27979;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04616</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#22312;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#30417;&#27979;&#21387;&#21147;&#21644;&#21387;&#21147;&#30456;&#20851;&#31934;&#31070;&#38556;&#30861;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning, Deep Learning and Data Preprocessing Techniques for Detection, Prediction, and Monitoring of Stress and Stress-related Mental Disorders: A Scoping Review. (arXiv:2308.04616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21387;&#21147;&#21644;&#21387;&#21147;&#30456;&#20851;&#31934;&#31070;&#38556;&#30861;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#19988;&#29983;&#29702;&#21442;&#25968;&#22914;&#24515;&#29575;&#27979;&#37327;&#21644;&#30382;&#32932;&#21453;&#24212;&#22312;&#21387;&#21147;&#39044;&#27979;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22312;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#20998;&#26512;&#31934;&#31070;&#21387;&#21147;&#21450;&#20854;&#30456;&#20851;&#31934;&#31070;&#38556;&#30861;&#65288;MDs&#65289;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#33539;&#22260;&#23457;&#26597;&#36807;&#31243;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#21387;&#21147;&#19982;&#21387;&#21147;&#30456;&#20851;MDs&#32972;&#26223;&#19979;&#30340;&#26368;&#26032;ML&#31639;&#27861;&#12289;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#25968;&#25454;&#31867;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#22312;&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#32508;&#36848;&#24378;&#35843;&#65292;&#29983;&#29702;&#21442;&#25968;&#22914;&#24515;&#29575;&#27979;&#37327;&#21644;&#30382;&#32932;&#21453;&#24212;&#26159;ML&#31639;&#27861;&#20013;&#24120;&#29992;&#30340;&#21387;&#21147;&#39044;&#27979;&#22240;&#23376;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#33021;&#25552;&#20379;&#20851;&#20110;&#21387;&#21147;&#21644;&#21387;&#21147;&#30456;&#20851;MDs&#30340;&#20016;&#23500;&#35828;&#26126;&#24615;&#20449;&#24687;&#65292;&#24182;&#19988;&#25968;&#25454;&#37319;&#38598;&#30456;&#23545;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
This comprehensive review systematically evaluates Machine Learning (ML) methodologies employed in the detection, prediction, and analysis of mental stress and its consequent mental disorders (MDs). Utilizing a rigorous scoping review process, the investigation delves into the latest ML algorithms, preprocessing techniques, and data types employed in the context of stress and stress-related MDs. The findings highlight that Support Vector Machine (SVM), Neural Network (NN), and Random Forest (RF) models consistently exhibit superior accuracy and robustness among all machine learning algorithms examined. Furthermore, the review underscores that physiological parameters, such as heart rate measurements and skin response, are prevalently used as stress predictors in ML algorithms. This is attributed to their rich explanatory information concerning stress and stress-related MDs, as well as the relative ease of data acquisition. Additionally, the application of dimensionality reduction techn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#31232;&#30095;&#25968;&#32452;&#30340;&#20960;&#20010;&#26041;&#21521;&#25214;&#23547;&#24212;&#29992;&#65292;&#21253;&#25324;&#35748;&#30693;&#38647;&#36798;&#12289;&#26080;&#32447;&#36890;&#20449;&#21644;&#32508;&#21512;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#24212;&#29992;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#29305;&#24449;&#24037;&#31243;&#21644;&#20302;&#39044;&#27979;&#38454;&#27573;&#22797;&#26434;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04615</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#26041;&#21521;&#25214;&#23547;&#30340;&#31232;&#30095;&#25968;&#32452;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Sparse Array Design for Direction Finding using Deep Learning. (arXiv:2308.04615v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#31232;&#30095;&#25968;&#32452;&#30340;&#20960;&#20010;&#26041;&#21521;&#25214;&#23547;&#24212;&#29992;&#65292;&#21253;&#25324;&#35748;&#30693;&#38647;&#36798;&#12289;&#26080;&#32447;&#36890;&#20449;&#21644;&#32508;&#21512;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#24212;&#29992;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#29305;&#24449;&#24037;&#31243;&#21644;&#20302;&#39044;&#27979;&#38454;&#27573;&#22797;&#26434;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#24341;&#20837;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26469;&#35774;&#35745;&#31232;&#30095;&#25968;&#32452;&#12290;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#29305;&#24449;&#24037;&#31243;&#21644;&#20302;&#39044;&#27979;&#38454;&#27573;&#22797;&#26434;&#24615;&#30340;&#20248;&#21183;&#65292;&#22312;&#35299;&#20915;&#23547;&#25214;&#31232;&#30095;&#25968;&#32452;&#30340;&#32452;&#21512;&#25628;&#32034;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;DL&#31232;&#30095;&#25968;&#32452;&#30340;&#22810;&#20010;&#26041;&#21521;&#25214;&#23547;&#24212;&#29992;&#30340;&#27010;&#35201;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#22312;&#35748;&#30693;&#38647;&#36798;&#24212;&#29992;&#20013;&#36873;&#25321;&#31232;&#30095;&#25968;&#32452;&#30340;&#30417;&#30563;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#31561;&#20803;&#21551;&#21457;&#24335;&#23398;&#20064;&#31639;&#27861;&#26469;&#35774;&#35745;&#20108;&#32500;&#31232;&#30095;&#25968;&#32452;&#30340;&#24773;&#20917;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#30340;DL&#22522;&#30784;&#22825;&#32447;&#36873;&#25321;&#65292;&#22312;&#20854;&#20013;&#31232;&#30095;&#25968;&#32452;&#38382;&#39064;&#20063;&#21487;&#20197;&#19982;&#20449;&#36947;&#20272;&#35745;&#12289;&#27874;&#26463;&#25104;&#24418;&#25110;&#23450;&#20301;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#32508;&#21512;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#24212;&#29992;&#30340;&#28145;&#24230;&#31232;&#30095;&#25968;&#32452;&#25216;&#26415;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, deep learning (DL) techniques have been introduced for designing sparse arrays. These methods offer the advantages of feature engineering and low prediction-stage complexity, which is helpful in tackling the combinatorial search inherent to finding a sparse array. In this chapter, we provide a synopsis of several direction finding applications of DL-based sparse arrays. We begin by examining supervised and transfer learning techniques that have applications in selecting sparse arrays for a cognitive radar application. Here, we also discuss the use of meta-heuristic learning algorithms such as simulated annealing for the case of designing two-dimensional sparse arrays. Next, we consider DL-based antenna selection for wireless communications, wherein sparse array problem may also be combined with channel estimation, beamforming, or localization. Finally, we provide an example of deep sparse array technique for integrated sensing and communications (ISAC) applicatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#30005;&#31163;&#23618;&#20013;&#30340;&#20869;&#37096;&#37325;&#21147;&#27874;&#26469;&#23454;&#29616;&#28023;&#21880;&#30340;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#24320;&#25918;&#28023;&#27915;&#35206;&#30422;&#12290;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#37327;&#30340;GNSS&#25968;&#25454;&#65292;&#36890;&#36807;&#22788;&#29702;&#25104;&#21315;&#19978;&#19975;&#20010;&#25968;&#25454;&#27969;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#26469;&#22686;&#24378;&#28023;&#21880;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04611</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#28023;&#21880;&#30456;&#20851;&#20869;&#37096;&#37325;&#21147;&#27874;&#26816;&#27979;&#65306;&#36890;&#21521;&#24320;&#25918;&#28023;&#27915;&#33258;&#28982;&#28798;&#23475;&#26816;&#27979;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Driven Detection of Tsunami Related Internal GravityWaves: a path towards open-ocean natural hazards detection. (arXiv:2308.04611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04611
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#30005;&#31163;&#23618;&#20013;&#30340;&#20869;&#37096;&#37325;&#21147;&#27874;&#26469;&#23454;&#29616;&#28023;&#21880;&#30340;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#24320;&#25918;&#28023;&#27915;&#35206;&#30422;&#12290;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#37327;&#30340;GNSS&#25968;&#25454;&#65292;&#36890;&#36807;&#22788;&#29702;&#25104;&#21315;&#19978;&#19975;&#20010;&#25968;&#25454;&#27969;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#26469;&#22686;&#24378;&#28023;&#21880;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#21880;&#20250;&#22312;&#30005;&#31163;&#23618;&#20013;&#24341;&#21457;&#20869;&#37096;&#37325;&#21147;&#27874; (IGWs)&#65292;&#25200;&#21160;&#24635;&#30005;&#23376;&#21547;&#37327; (TEC)&#65292;&#31216;&#20026;&#34892;&#36827;&#30005;&#31163;&#23618;&#25200;&#21160; (TIDs)&#65292;&#21487;&#36890;&#36807;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479; (GNSS) &#36827;&#34892;&#26816;&#27979;&#12290;GNSS&#26159;&#19968;&#32452;&#21355;&#26143;&#65292;&#25552;&#20379;&#26469;&#33258;&#22320;&#29699;&#36712;&#36947;&#30340;&#20449;&#21495;&#65292;&#21253;&#25324;&#27431;&#27954;&#30340;&#20285;&#21033;&#30053;&#12289;&#32654;&#22269;&#30340;&#20840;&#29699;&#23450;&#20301;&#31995;&#32479; (GPS)&#12289;&#20420;&#32599;&#26031;&#30340;&#26684;&#27931;&#32435;&#26031; (GLONASS) &#21644;&#20013;&#22269;&#30340;&#21271;&#26007;&#12290;&#23454;&#26102;&#26816;&#27979;TIDs&#20026;&#28023;&#21880;&#26816;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#19981;&#21487;&#30001;&#22522;&#20110;&#28014;&#26631;&#30340;&#39044;&#35686;&#31995;&#32479;&#26381;&#21153;&#30340;&#22320;&#29702;&#21306;&#22495;&#20869;&#25552;&#20379;&#20102;&#24320;&#25918;&#28023;&#27915;&#35206;&#30422;&#12290;&#28145;&#24230;&#23398;&#20064;&#21033;&#29992;&#22823;&#37327;&#30340;GNSS&#25968;&#25454;&#65292;&#26377;&#25928;&#22788;&#29702;&#25104;&#21315;&#19978;&#19975;&#20010;&#25968;&#25454;&#27969;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;VARION&#65288;&#23454;&#26102;&#30005;&#31163;&#23618;&#35266;&#27979;&#30340;&#21464;&#24046;&#26041;&#27861;&#65289;&#31639;&#27861;&#30340;&#26012;&#21521;&#24635;&#30005;&#23376;&#21547;&#37327;&#65288;sTEC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tsunamis can trigger internal gravity waves (IGWs) in the ionosphere, perturbing the Total Electron Content (TEC) - referred to as Traveling Ionospheric Disturbances (TIDs) that are detectable through the Global Navigation Satellite System (GNSS). The GNSS are constellations of satellites providing signals from Earth orbit - Europe's Galileo, the United States' Global Positioning System (GPS), Russia's Global'naya Navigatsionnaya Sputnikovaya Sistema (GLONASS) and China's BeiDou. The real-time detection of TIDs provides an approach for tsunami detection, enhancing early warning systems by providing open-ocean coverage in geographic areas not serviceable by buoy-based warning systems. Large volumes of the GNSS data is leveraged by deep learning, which effectively handles complex non-linear relationships across thousands of data streams. We describe a framework leveraging slant total electron content (sTEC) from the VARION (Variometric Approach for Real-Time Ionosphere Observation) algor
&lt;/p&gt;</description></item><item><title>PSRFlow&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24335;&#27169;&#22411;&#30340;&#31185;&#23398;&#25968;&#25454;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#65292;&#33021;&#22815;&#37327;&#21270;&#36229;&#20998;&#36776;&#32467;&#26524;&#30340;&#35823;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#29983;&#25104;&#22810;&#20010;&#21487;&#34892;&#30340;&#36229;&#20998;&#36776;&#29575;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2308.04605</link><description>&lt;p&gt;
PSRFlow: &#22522;&#20110;&#27969;&#24335;&#27169;&#22411;&#30340;&#31185;&#23398;&#25968;&#25454;&#27010;&#29575;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data. (arXiv:2308.04605v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04605
&lt;/p&gt;
&lt;p&gt;
PSRFlow&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24335;&#27169;&#22411;&#30340;&#31185;&#23398;&#25968;&#25454;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#65292;&#33021;&#22815;&#37327;&#21270;&#36229;&#20998;&#36776;&#32467;&#26524;&#30340;&#35823;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#29983;&#25104;&#22810;&#20010;&#21487;&#34892;&#30340;&#36229;&#20998;&#36776;&#29575;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#25512;&#29702;&#38454;&#27573;&#32570;&#20047;&#30495;&#20540;&#65292;&#24456;&#23569;&#33021;&#22815;&#37327;&#21270;&#36229;&#20998;&#36776;&#32467;&#26524;&#30340;&#35823;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31185;&#23398;&#21487;&#35270;&#21270;&#24212;&#29992;&#26469;&#35828;&#65292;&#23558;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#36798;&#32473;&#31185;&#23398;&#23478;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#35823;&#23548;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27969;&#24335;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;PSRFlow&#65292;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#36229;&#20998;&#36776;&#29575;&#65292;&#24182;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#34701;&#20837;&#21040;&#36229;&#20998;&#36776;&#36807;&#31243;&#20013;&#12290;PSRFlow&#22522;&#20110;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#23398;&#20064;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#36890;&#36807;&#20174;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#20013;&#25429;&#33719;&#30340;&#32570;&#22833;&#20449;&#24687;&#30340;&#39640;&#26031;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#21487;&#34892;&#36229;&#20998;&#36776;&#29575;&#36755;&#20986;&#12290;&#22312;&#39640;&#26031;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#25928;&#37319;&#26679;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although many deep-learning-based super-resolution approaches have been proposed in recent years, because no ground truth is available in the inference stage, few can quantify the errors and uncertainties of the super-resolved results. For scientific visualization applications, however, conveying uncertainties of the results to scientists is crucial to avoid generating misleading or incorrect information. In this paper, we propose PSRFlow, a novel normalizing flow-based generative model for scientific data super-resolution that incorporates uncertainty quantification into the super-resolution process. PSRFlow learns the conditional distribution of the high-resolution data based on the low-resolution counterpart. By sampling from a Gaussian latent space that captures the missing information in the high-resolution data, one can generate different plausible super-resolution outputs. The efficient sampling in the Gaussian latent space allows our model to perform uncertainty quantification 
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#20960;&#24180;&#65292;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#35757;&#32451;&#20998;&#24067;&#24335;&#12289;&#22823;&#35268;&#27169;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20811;&#26381;&#38598;&#20013;&#24335;&#32534;&#25490;&#30340;&#21333;&#28857;&#25925;&#38556;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04604</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Decentralized Federated Learning. (arXiv:2308.04604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04604
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#35757;&#32451;&#20998;&#24067;&#24335;&#12289;&#22823;&#35268;&#27169;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20811;&#26381;&#38598;&#20013;&#24335;&#32534;&#25490;&#30340;&#21333;&#28857;&#25925;&#38556;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#20998;&#24067;&#24335;&#12289;&#22823;&#35268;&#27169;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#19982;&#26631;&#20934;ML&#19981;&#21516;&#65292;&#38656;&#35201;&#23558;&#25968;&#25454;&#25910;&#38598;&#22312;&#35757;&#32451;&#25191;&#34892;&#30340;&#30830;&#20999;&#20301;&#32622;&#65292;FL&#21033;&#29992;&#25968;&#30334;&#19975;&#36793;&#32536;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#21327;&#21516;&#35757;&#32451;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#20250;&#25259;&#38706;&#20854;&#26412;&#22320;&#31169;&#26377;&#25968;&#25454;&#12290;&#22312;&#20856;&#22411;&#30340;FL&#31995;&#32479;&#20013;&#65292;&#20013;&#22830;&#26381;&#21153;&#22120;&#21482;&#20805;&#24403;&#21327;&#35843;&#22120;&#30340;&#35282;&#33394;&#65307;&#23427;&#36845;&#20195;&#22320;&#25910;&#38598;&#21644;&#27719;&#24635;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#33258;&#24049;&#30340;&#31169;&#26377;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#30452;&#21040;&#25910;&#25947;&#12290;&#23613;&#31649;FL&#22312;&#35774;&#35745;&#19978;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65288;&#20363;&#22914;&#36890;&#36807;&#35774;&#35745;&#20445;&#25252;&#31169;&#26377;&#25968;&#25454;&#25152;&#26377;&#26435;&#65289;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#24369;&#28857;&#12290;&#20854;&#20013;&#26368;&#20851;&#38190;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#20811;&#26381;&#32463;&#20856;FL&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#26550;&#26500;&#30340;&#38598;&#20013;&#24335;&#32534;&#25490;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#26131;&#21463;&#21333;&#28857;&#25925;&#38556;&#25915;&#20987;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, federated learning (FL) has become a very popular paradigm for training distributed, large-scale, and privacy-preserving machine learning (ML) systems. In contrast to standard ML, where data must be collected at the exact location where training is performed, FL takes advantage of the computational capabilities of millions of edge devices to collaboratively train a shared, global model without disclosing their local private data. Specifically, in a typical FL system, the central server acts only as an orchestrator; it iteratively gathers and aggregates all the local models trained by each client on its private data until convergence. Although FL undoubtedly has several benefits over traditional ML (e.g., it protects private data ownership by design), it suffers from several weaknesses. One of the most critical challenges is to overcome the centralized orchestration of the classical FL client-server architecture, which is known to be vulnerable to single-point-of-failur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#23558;&#20854;&#20998;&#20026;&#23884;&#20837;&#22120;-&#25552;&#21462;&#22120;&#32852;&#21512;&#35757;&#32451;&#12289;&#28145;&#24230;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#21464;&#25442;&#21644;&#28151;&#21512;&#26041;&#26696;&#12290;&#23545;&#27599;&#20010;&#31867;&#21035;&#30340;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#24635;&#32467;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.04603</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#65306;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based Image Watermarking: A Brief Survey. (arXiv:2308.04603v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#23558;&#20854;&#20998;&#20026;&#23884;&#20837;&#22120;-&#25552;&#21462;&#22120;&#32852;&#21512;&#35757;&#32451;&#12289;&#28145;&#24230;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#21464;&#25442;&#21644;&#28151;&#21512;&#26041;&#26696;&#12290;&#23545;&#27599;&#20010;&#31867;&#21035;&#30340;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#24635;&#32467;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#27700;&#21360;&#26159;&#25351;&#22312;&#19968;&#24352;&#23553;&#38754;&#22270;&#20687;&#20013;&#31192;&#23494;&#23884;&#20837;&#21644;&#25552;&#21462;&#27700;&#21360;&#20197;&#20445;&#25252;&#22270;&#20687;&#30340;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#23618;&#20986;&#19981;&#31351;&#12290;&#20026;&#20102;&#30740;&#31350;&#26368;&#26032;&#30340;&#25216;&#26415;&#65292;&#26412;&#35843;&#26597;&#23558;&#21069;&#27839;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#20998;&#20026;&#23884;&#20837;&#22120;-&#25552;&#21462;&#22120;&#32852;&#21512;&#35757;&#32451;&#12289;&#28145;&#24230;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#21464;&#25442;&#21644;&#28151;&#21512;&#26041;&#26696;&#12290;&#36824;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The act of secretly embedding and extracting a watermark on a cover image to protect it is known as image watermarking. In recent years, deep learning-based image watermarking techniques have been emerging one after another. To study the state-of-the-art, this survey categorizes cutting-edge deep learning-based image watermarking techniques into Embedder-Extractor Joint Training, Deep Networks as a Feature Transformation, and Hybrid schemes. Research directions in each category are also analyzed and summarized. Additionally, potential future research directions are discussed to envision future studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#37327;&#21270;&#22240;&#23376;&#19978;&#25214;&#21040;&#24352;&#37327;&#36924;&#36817;&#65292;&#26082;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#21448;&#20445;&#25345;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#36827;&#34892;&#35268;&#33539;&#22810;&#32447;&#24615;&#20998;&#35299;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.04595</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#30340;&#37327;&#21270;&#24863;&#30693;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantization Aware Factorization for Deep Neural Network Compression. (arXiv:2308.04595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#37327;&#21270;&#22240;&#23376;&#19978;&#25214;&#21040;&#24352;&#37327;&#36924;&#36817;&#65292;&#26082;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#21448;&#20445;&#25345;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#36827;&#34892;&#35268;&#33539;&#22810;&#32447;&#24615;&#20998;&#35299;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20943;&#23569;&#21442;&#25968;&#21644;FLOP&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#30001;&#20110;&#31227;&#21160;&#25110;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#20869;&#23384;&#21644;&#21151;&#32791;&#38480;&#21046;&#65292;&#24403;&#37096;&#32626;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#37327;&#21270;&#27493;&#39588;&#12290;&#23545;&#20110;&#20855;&#26377;&#20998;&#35299;&#26435;&#37325;&#30340;&#32593;&#32476;&#24212;&#29992;&#20256;&#32479;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;&#20934;&#30830;&#24230;&#19979;&#38477;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#30452;&#25509;&#22312;&#37327;&#21270;&#22240;&#23376;&#19978;&#25214;&#21040;&#24352;&#37327;&#36924;&#36817;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#27169;&#22411;&#39044;&#27979;&#21697;&#36136;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#20174;&#21387;&#32553;&#25216;&#26415;&#20013;&#21463;&#30410;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;(ADMM)&#36827;&#34892;&#20855;&#26377;&#37327;&#21270;&#26684;&#28857;&#20803;&#32032;&#30340;&#35268;&#33539;&#22810;&#32447;&#24615;(CP)&#20998;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#35774;&#35745;&#30340;&#31639;&#27861;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#65292;&#24182;&#35780;&#20272;&#20854;&#39044;&#27979;&#21697;&#36136;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#26032;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor decomposition of convolutional and fully-connected layers is an effective way to reduce parameters and FLOP in neural networks. Due to memory and power consumption limitations of mobile or embedded devices, the quantization step is usually necessary when pre-trained models are deployed. A conventional post-training quantization approach applied to networks with decomposed weights yields a drop in accuracy. This motivated us to develop an algorithm that finds tensor approximation directly with quantized factors and thus benefit from both compression techniques while keeping the prediction quality of the model. Namely, we propose to use Alternating Direction Method of Multipliers (ADMM) for Canonical Polyadic (CP) decomposition with factors whose elements lie on a specified quantization grid. We compress neural network weights with a devised algorithm and evaluate it's prediction quality and performance. We compare our approach to state-of-the-art post-training quantization method
&lt;/p&gt;</description></item><item><title>ScatterUQ&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04588</link><description>&lt;p&gt;
ScatterUQ: &#29992;&#20110;&#22810;&#31867;&#21035;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#30340;&#20132;&#20114;&#24335;&#19981;&#30830;&#23450;&#24615;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems. (arXiv:2308.04588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04588
&lt;/p&gt;
&lt;p&gt;
ScatterUQ&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22810;&#31867;&#21035;&#26631;&#31614;&#38382;&#39064;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26657;&#20934;&#30340;&#31867;&#21035;&#39044;&#27979;&#27010;&#29575;&#21644;&#36229;&#20986;&#20998;&#24067;&#25351;&#31034;&#22120;&#65292;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#30340;&#20351;&#29992;&#32773;&#21644;&#24037;&#31243;&#24072;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20449;&#24687;&#22312;&#22810;&#20010;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#19979;&#23545;&#20219;&#24847;&#25968;&#25454;&#28304;&#36827;&#34892;&#21487;&#35270;&#21270;&#20256;&#36798;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ScatterUQ&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#21487;&#35270;&#21270;&#65292;&#35753;&#29992;&#25143;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, uncertainty-aware deep learning methods for multiclass labeling problems have been developed that provide calibrated class prediction probabilities and out-of-distribution (OOD) indicators, letting machine learning (ML) consumers and engineers gauge a model's confidence in its predictions. However, this extra neural network prediction information is challenging to scalably convey visually for arbitrary data sources under multiple uncertainty contexts. To address these challenges, we present ScatterUQ, an interactive system that provides targeted visualizations to allow users to better understand model performance in context-driven uncertainty settings. ScatterUQ leverages recent advances in distance-aware neural networks, together with dimensionality reduction techniques, to construct robust, 2-D scatter plots explaining why a model predicts a test example to be (1) in-distribution and of a particular class, (2) in-distribution but unsure of the class, and (3) out-of-distribu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#20195;&#29702;&#21464;&#37327;&#30340;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#21644;&#26368;&#22823;&#30697;&#32422;&#26463;&#30340;&#26041;&#27861;&#21487;&#20197;&#19968;&#33268;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.04585</link><description>&lt;p&gt;
&#20915;&#23450;&#24615;&#28151;&#28102;&#19979;&#30340;&#20869;&#26680;&#21333;&#19968;&#20195;&#29702;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Kernel Single Proxy Control for Deterministic Confounding. (arXiv:2308.04585v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#20195;&#29702;&#21464;&#37327;&#30340;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#21644;&#26368;&#22823;&#30697;&#32422;&#26463;&#30340;&#26041;&#27861;&#21487;&#20197;&#19968;&#33268;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#35266;&#27979;&#21040;&#19982;&#28151;&#28102;&#22240;&#32032;&#30456;&#20851;&#30340;&#20195;&#29702;&#21464;&#37327;&#12290;&#23613;&#31649;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#20351;&#29992;&#20004;&#20010;&#20195;&#29702;&#21464;&#37327;&#26469;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#65292;&#21017;&#20351;&#29992;&#21333;&#20010;&#20195;&#29702;&#21464;&#37327;&#23601;&#36275;&#20197;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#65292;&#24182;&#27010;&#25324;&#20102;&#25511;&#21046;&#32467;&#26524;&#26657;&#20934;&#27861;&#65288;COCA&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20004;&#38454;&#27573;&#22238;&#24402;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#30697;&#32422;&#26463;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#23454;&#39564;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of causal effect estimation with an unobserved confounder, where we observe a proxy variable that is associated with the confounder. Although Proxy Causal Learning (PCL) uses two proxy variables to recover the true causal effect, we show that a single proxy variable is sufficient for causal estimation if the outcome is generated deterministically, generalizing Control Outcome Calibration Approach (COCA). We propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. We prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on a synthetic dataset.
&lt;/p&gt;</description></item><item><title>RECipe&#26159;&#19968;&#20010;&#22810;&#29992;&#36884;&#30340;&#33756;&#35889;&#25512;&#33616;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#25903;&#25745;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25110;&#25552;&#20379;&#22270;&#20687;&#26102;&#21521;&#29992;&#25143;&#25512;&#33616;&#33756;&#35889;&#65292;&#36229;&#36234;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04579</link><description>&lt;p&gt;
RECipe: &#22810;&#27169;&#24577;&#33756;&#35889;&#30693;&#35782;&#22270;&#35889;&#36866;&#29992;&#20110;&#22810;&#29992;&#36884;&#25512;&#33616;&#31995;&#32479;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
RECipe: Does a Multi-Modal Recipe Knowledge Graph Fit a Multi-Purpose Recommendation System?. (arXiv:2308.04579v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04579
&lt;/p&gt;
&lt;p&gt;
RECipe&#26159;&#19968;&#20010;&#22810;&#29992;&#36884;&#30340;&#33756;&#35889;&#25512;&#33616;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#25903;&#25745;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25110;&#25552;&#20379;&#22270;&#20687;&#26102;&#21521;&#29992;&#25143;&#25512;&#33616;&#33756;&#35889;&#65292;&#36229;&#36234;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20004;&#21313;&#24180;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#24050;&#32463;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#21830;&#19994;&#25110;&#22312;&#32447;&#24179;&#21488;&#30340;&#23458;&#25143;&#25512;&#33616;&#30005;&#24433;&#12289;&#22270;&#20070;&#21644;&#39184;&#21381;&#31561;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#36825;&#20123;&#24212;&#29992;&#65292;&#33756;&#35889;&#25512;&#33616;&#23578;&#26410;&#24471;&#21040;&#24456;&#22810;&#20851;&#27880;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;RECipe&#20316;&#20026;&#19968;&#20010;&#22810;&#29992;&#36884;&#30340;&#33756;&#35889;&#25512;&#33616;&#26694;&#26550;&#65292;&#20855;&#26377;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MMKG&#65289;&#30340;&#25903;&#25745;&#12290;RECipe&#30340;&#21160;&#26426;&#26159;&#36890;&#36807;&#22312;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25110;&#25552;&#20379;&#22270;&#20687;&#26102;&#21521;&#29992;&#25143;&#25512;&#33616;&#33756;&#35889;&#65292;&#36229;&#36234;&#65288;&#28145;&#24230;&#65289;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;&#65288;NCF&#65289;&#12290;RECipe&#30001;3&#20010;&#23376;&#31995;&#32479;&#32452;&#25104;&#65306;&#22522;&#20110;&#34892;&#20026;&#30340;&#25512;&#33616;&#22120;&#12289;&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#22120;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#25512;&#33616;&#22120;&#12290;&#27599;&#20010;&#23376;&#31995;&#32479;&#37117;&#20381;&#36182;&#20110;&#22270;&#20013;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#24494;&#36719;&#30340;MPNet&#27169;&#22411;&#30340;&#24494;&#35843;&#27169;&#22411;&#20013;&#33719;&#21462;&#25991;&#26412;&#23454;&#20307;&#65288;&#20363;&#22914;&#35780;&#35770;&#25110;&#25104;&#20998;&#65289;&#30340;&#65288;&#39044;&#35757;&#32451;&#65289;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past two decades, recommendation systems (RSs) have used machine learning (ML) solutions to recommend items, e.g., movies, books, and restaurants, to clients of a business or an online platform. Recipe recommendation, however, has not yet received much attention compared to those applications. We introduce RECipe as a multi-purpose recipe recommendation framework with a multi-modal knowledge graph (MMKG) backbone. The motivation behind RECipe is to go beyond (deep) neural collaborative filtering (NCF) by recommending recipes to users when they query in natural language or by providing an image. RECipe consists of 3 subsystems: (1) behavior-based recommender, (2) review-based recommender, and (3) image-based recommender. Each subsystem relies on the embedding representations of entities and relations in the graph. We first obtain (pre-trained) embedding representations of textual entities, such as reviews or ingredients, from a fine-tuned model of Microsoft's MPNet. We initiali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#21040;&#19982;&#25968;&#25454;&#38598;&#20559;&#24046;&#30456;&#20851;&#30340;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04553</link><description>&lt;p&gt;
&#20174;&#20551;&#21040;&#30495;&#65288;FFR&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#19982;&#21512;&#25104;&#25968;&#25454;&#30456;&#20851;&#24615;&#38169;&#35823;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data. (arXiv:2308.04553v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#21040;&#19982;&#25968;&#25454;&#38598;&#20559;&#24046;&#30456;&#20851;&#30340;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23481;&#26131;&#23398;&#20064;&#21040;&#30001;&#20110;&#35757;&#32451;&#38598;&#30340;&#19981;&#24179;&#34913;&#23548;&#33268;&#30340;&#30456;&#20851;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#26576;&#20123;&#32676;&#20307;&#65288;&#22914;&#22899;&#24615;&#65289;&#22312;&#26576;&#20123;&#31867;&#21035;&#65288;&#22914;&#31243;&#24207;&#21592;&#65289;&#20013;&#20195;&#34920;&#24615;&#19981;&#36275;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#20026;&#23569;&#25968;&#26679;&#26412;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#36825;&#31181;&#20559;&#24046;&#65292;&#20174;&#32780;&#24179;&#34913;&#35757;&#32451;&#38598;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#24448;&#24448;&#33021;&#22815;&#23398;&#20064;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#21644;&#21512;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#28040;&#38500;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#20943;&#23569;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;1&#65289;&#25105;&#20204;&#22312;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;2&#65289;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#20351;&#29992;&#36825;&#20010;&#27969;&#31243;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#22312;&#31532;&#19968;&#27493;&#20013;&#25105;&#20204;&#23398;&#20064;&#21040;&#20102;&#25269;&#25239;&#20559;&#24046;&#30340;&#31283;&#20581;&#29305;&#24449;&#65292;&#22312;&#31532;&#20108;&#27493;&#20013;&#20943;&#36731;&#20102;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual recognition models are prone to learning spurious correlations induced by an imbalanced training set where certain groups (\eg Females) are under-represented in certain classes (\eg Programmers). Generative models offer a promising direction in mitigating this bias by generating synthetic data for the minority samples and thus balancing the training set. However, prior work that uses these approaches overlooks that visual recognition models could often learn to differentiate between real and synthetic images and thus fail to unlearn the bias in the original dataset. In our work, we propose a novel two-stage pipeline to mitigate this issue where 1) we pre-train a model on a balanced synthetic dataset and then 2) fine-tune on the real data. Using this pipeline, we avoid training on both real and synthetic data, thus avoiding the bias between real and synthetic data. Moreover, we learn robust features against the bias in the first step that mitigate the bias in the second step. Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22312;&#22122;&#22768;&#26631;&#31614;&#19979;&#20165;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#20026;&#22522;&#30784;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#20063;&#20855;&#26377;&#25913;&#36827;&#23398;&#20064;&#25928;&#26524;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04551</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25913;&#21892;&#22122;&#22768;&#26631;&#31614;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Image Classification in Noisy Labels Using Only Self-supervised Pretraining. (arXiv:2308.04551v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22312;&#22122;&#22768;&#26631;&#31614;&#19979;&#20165;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#20026;&#22522;&#30784;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#20063;&#20855;&#26377;&#25913;&#36827;&#23398;&#20064;&#25928;&#26524;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#26631;&#31614;&#20250;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30417;&#30563;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#65292;&#22240;&#20026;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#24230;&#25311;&#21512;&#22122;&#22768;&#24182;&#23398;&#20064;&#21040;&#26377;&#25439;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#20351;&#29992;&#23545;&#27604;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26435;&#37325;&#26469;&#21021;&#22987;&#21270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#29305;&#24449;&#25439;&#22351;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#26377;&#30740;&#31350;&#25506;&#32034;1&#65289;&#20854;&#20182;&#33258;&#30417;&#30563;&#26041;&#27861;&#65288;&#22914;&#39044;&#35757;&#32451;&#20219;&#21153;&#65289;&#22914;&#20309;&#24433;&#21709;&#22122;&#22768;&#26631;&#31614;&#19979;&#30340;&#23398;&#20064;&#65292;&#20197;&#21450;2&#65289;&#22312;&#22122;&#22768;&#26631;&#31614;&#35774;&#32622;&#19979;&#65292;&#20165;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#23545;&#21307;&#23398;&#22270;&#20687;&#30340;&#24433;&#21709;&#12290;&#21307;&#23398;&#22270;&#20687;&#24448;&#24448;&#20855;&#26377;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#23567;&#30340;&#31867;&#38388;&#21464;&#21270;&#65292;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#20445;&#35777;&#27491;&#30830;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#23578;&#19981;&#28165;&#26970;&#25913;&#21892;&#22122;&#22768;&#26631;&#31614;&#19979;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#23398;&#20064;&#30340;&#26041;&#27861;&#26159;&#21542;&#20063;&#23545;&#21307;&#23398;&#22270;&#20687;&#26377;&#24110;&#21161;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23545;&#27604;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#20026;&#22522;&#30784;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noisy labels hurt deep learning-based supervised image classification performance as the models may overfit the noise and learn corrupted feature extractors. For natural image classification training with noisy labeled data, model initialization with contrastive self-supervised pretrained weights has shown to reduce feature corruption and improve classification performance. However, no works have explored: i) how other self-supervised approaches, such as pretext task-based pretraining, impact the learning with noisy label, and ii) any self-supervised pretraining methods alone for medical images in noisy label settings. Medical images often feature smaller datasets and subtle inter class variations, requiring human expertise to ensure correct classification. Thus, it is not clear if the methods improving learning with noisy labels in natural image datasets such as CIFAR would also help with medical images. In this work, we explore contrastive and pretext task-based self-supervised pretr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26412;&#22320;&#35823;&#24046;&#20449;&#21495;&#23454;&#29616;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.04539</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#29289;&#21551;&#21457;&#30340;&#26550;&#26500;&#25552;&#39640;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures. (arXiv:2308.04539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26412;&#22320;&#35823;&#24046;&#20449;&#21495;&#23454;&#29616;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#20174;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#20013;&#26080;&#38388;&#26029;&#22320;&#23398;&#20064;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21450;&#20854;&#21464;&#20307;&#65292;&#36825;&#20123;&#26041;&#27861;&#37319;&#29992;&#20840;&#23616;&#35823;&#24046;&#26356;&#26032;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#31574;&#30053;&#65292;&#22914;&#20869;&#23384;&#32531;&#20914;&#21306;&#25110;&#22238;&#25918;&#65292;&#20197;&#35268;&#36991;&#20854;&#31283;&#23450;&#24615;&#12289;&#36138;&#23146;&#21644;&#30701;&#26399;&#35760;&#24518;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21253;&#25324;&#31361;&#35302;&#21487;&#22609;&#24615;&#26426;&#21046;&#21644;&#31070;&#32463;&#35843;&#33410;&#65292;&#24182;&#36890;&#36807;&#26412;&#22320;&#35823;&#24046;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#32780;&#26080;&#38656;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Split-MNIST&#12289;Split-CIFAR-10&#21644;Split-CIFAR-100&#25968;&#25454;&#38598;&#19978;&#27604;&#20854;&#20182;&#20869;&#23384;&#21463;&#38480;&#30340;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#20869;&#23384;&#23494;&#38598;&#22411;&#22238;&#25918;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn continuously from an incoming data stream without catastrophic forgetting is critical to designing intelligent systems. Many approaches to continual learning rely on stochastic gradient descent and its variants that employ global error updates, and hence need to adopt strategies such as memory buffers or replay to circumvent its stability, greed, and short-term memory limitations. To address this limitation, we have developed a biologically inspired lightweight neural network architecture that incorporates synaptic plasticity mechanisms and neuromodulation and hence learns through local error signals to enable online continual learning without stochastic gradient descent.  Our approach leads to superior online continual learning performance on Split-MNIST, Split-CIFAR-10, and Split-CIFAR-100 datasets compared to other memory-constrained learning approaches and matches that of the state-of-the-art memory-intensive replay-based approaches. We further demonstrate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21512;&#20316;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#20986;&#29256;&#29289;&#25968;&#37327;&#26377;&#22686;&#38271;&#36235;&#21183;&#65292;&#24182;&#19988;&#36825;&#20123;&#20986;&#29256;&#29289;&#24448;&#24448;&#27604;&#20165;&#30001;&#23398;&#26415;&#30028;&#20135;&#29983;&#30340;&#20986;&#29256;&#29289;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04524</link><description>&lt;p&gt;
&#25105;&#24212;&#35813;&#21644;&#35841;&#21512;&#20316;? &#20851;&#20110;NLP&#23398;&#26415;&#30028;&#19982;&#24037;&#19994;&#30028;&#21512;&#20316;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP. (arXiv:2308.04524v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21512;&#20316;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#20986;&#29256;&#29289;&#25968;&#37327;&#26377;&#22686;&#38271;&#36235;&#21183;&#65292;&#24182;&#19988;&#36825;&#20123;&#20986;&#29256;&#29289;&#24448;&#24448;&#27604;&#20165;&#30001;&#23398;&#26415;&#30028;&#20135;&#29983;&#30340;&#20986;&#29256;&#29289;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#35843;&#26597;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21512;&#20316;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;NLP&#35770;&#25991;&#20013;&#25552;&#21462;&#26426;&#26500;&#21644;&#24341;&#29992;&#20449;&#24687;&#30340;&#27969;&#31243;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#19977;&#31867;&#65306;&#23398;&#26415;&#30028;&#12289;&#24037;&#19994;&#30028;&#21644;&#28151;&#21512;&#22411;&#65288;&#23398;&#26415;&#30028;&#19982;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;-&#24037;&#19994;&#30028;&#21512;&#20316;&#20986;&#29256;&#29289;&#30340;&#25968;&#37327;&#21576;&#22686;&#38271;&#36235;&#21183;&#65292;&#24182;&#19988;&#36825;&#20123;&#31867;&#22411;&#30340;&#20986;&#29256;&#29289;&#19982;&#20165;&#22312;&#23398;&#26415;&#30028;&#20135;&#29983;&#30340;&#20986;&#29256;&#29289;&#30456;&#27604;&#65292;&#24448;&#24448;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of our research was to investigate the effects of collaboration between academia and industry on Natural Language Processing (NLP). To do this, we created a pipeline to extract affiliations and citations from NLP papers and divided them into three categories: academia, industry, and hybrid (collaborations between academia and industry). Our empirical analysis found that there is a trend towards an increase in industry and academia-industry collaboration publications and that these types of publications tend to have a higher impact compared to those produced solely within academia.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.04522</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#38544;&#20889;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#26159;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#30340;&#20004;&#20010;&#30456;&#20851;&#26041;&#38754;&#12290;&#38544;&#20889;&#26415;&#26088;&#22312;&#38544;&#34255;&#36890;&#20449;&#65292;&#32780;&#38544;&#20889;&#20998;&#26512;&#21017;&#26088;&#22312;&#25214;&#21040;&#36825;&#20123;&#38544;&#34255;&#20449;&#24687;&#65292;&#29978;&#33267;&#23581;&#35797;&#24674;&#22797;&#20854;&#25152;&#21253;&#21547;&#30340;&#25968;&#25454;&#12290;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#21463;&#21040;&#25191;&#27861;&#37096;&#38376;&#30340;&#20851;&#27880;&#12290;&#38544;&#20889;&#26415;&#24120;&#34987;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#29978;&#33267;&#24656;&#24598;&#20998;&#23376;&#29992;&#26469;&#36991;&#20813;&#22312;&#25317;&#26377;&#35777;&#25454;&#26102;&#34987;&#25429;&#65292;&#21363;&#20351;&#21152;&#23494;&#20063;&#19968;&#26679;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#22269;&#23478;&#31105;&#27490;&#25110;&#38480;&#21046;&#20351;&#29992;&#23494;&#30721;&#23398;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#25581;&#31034;&#38544;&#34255;&#20449;&#24687;&#30340;&#23574;&#31471;&#25216;&#26415;&#23545;&#25581;&#38706;&#38750;&#27861;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#35768;&#22810;&#24378;&#22823;&#21487;&#38752;&#30340;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-IceNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21271;&#26497;&#28023;&#20912;&#27987;&#24230;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31354;&#38388;&#21644;&#22810;&#26102;&#24207;&#32467;&#26500;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#36339;&#36291;&#36830;&#25509;&#22788;&#29702;&#22810;&#26102;&#24207;&#36755;&#20837;&#27969;&#65292;&#24182;&#21487;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#27493;&#39588;&#30340;&#31354;&#38388;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2308.04511</link><description>&lt;p&gt;
MT-IceNet - &#19968;&#31181;&#29992;&#20110;&#21271;&#26497;&#28023;&#20912;&#39044;&#27979;&#30340;&#31354;&#38388;&#21644;&#22810;&#26102;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MT-IceNet -- A Spatial and Multi-Temporal Deep Learning Model for Arctic Sea Ice Forecasting. (arXiv:2308.04511v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-IceNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21271;&#26497;&#28023;&#20912;&#27987;&#24230;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31354;&#38388;&#21644;&#22810;&#26102;&#24207;&#32467;&#26500;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#36339;&#36291;&#36830;&#25509;&#22788;&#29702;&#22810;&#26102;&#24207;&#36755;&#20837;&#27969;&#65292;&#24182;&#21487;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#27493;&#39588;&#30340;&#31354;&#38388;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#25918;&#22823;&#24050;&#32463;&#25913;&#21464;&#20102;&#21306;&#22495;&#21644;&#20840;&#29699;&#30340;&#27668;&#20505;&#27169;&#24335;&#65292;&#23548;&#33268;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#12290;&#21271;&#26497;&#25918;&#22823;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#21355;&#26143;&#35266;&#27979;&#25152;&#35777;&#23454;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#28023;&#20912;&#28040;&#22833;&#12290;&#20934;&#30830;&#22320;&#20174;&#27425;&#23395;&#33410;&#21040;&#23395;&#33410;&#24615;&#23610;&#24230;&#39044;&#27979;&#21271;&#26497;&#28023;&#20912;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#23384;&#22312;&#26681;&#26412;&#24615;&#25361;&#25112;&#12290;&#38500;&#20102;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65292;&#30740;&#31350;&#32773;&#19968;&#30452;&#24212;&#29992;&#22810;&#31181;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#28023;&#20912;&#39044;&#27979;&#12290;&#22238;&#39038;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30740;&#31350;&#28023;&#20912;&#21464;&#21270;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MT-IceNet - &#19968;&#31181;&#22522;&#20110;UNet&#30340;&#31354;&#38388;&#21644;&#22810;&#26102;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21271;&#26497;&#28023;&#20912;&#27987;&#24230;&#65288;SIC&#65289;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#22788;&#29702;&#22810;&#26102;&#24207;&#36755;&#20837;&#27969;&#20197;&#22312;&#26410;&#26469;&#30340;&#26102;&#38388;&#27493;&#39588;&#20013;&#37325;&#26032;&#29983;&#25104;&#31354;&#38388;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arctic amplification has altered the climate patterns both regionally and globally, resulting in more frequent and more intense extreme weather events in the past few decades. The essential part of Arctic amplification is the unprecedented sea ice loss as demonstrated by satellite observations. Accurately forecasting Arctic sea ice from sub-seasonal to seasonal scales has been a major research question with fundamental challenges at play. In addition to physics-based Earth system models, researchers have been applying multiple statistical and machine learning models for sea ice forecasting. Looking at the potential of data-driven approaches to study sea ice variations, we propose MT-IceNet - a UNet based spatial and multi-temporal (MT) deep learning model for forecasting Arctic sea ice concentration (SIC). The model uses an encoder-decoder architecture with skip connections and processes multi-temporal input streams to regenerate spatial maps at future timesteps. Using bi-monthly and m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26041;&#27861;&#26469;&#39044;&#27979;&#21387;&#32553;&#26426;&#21494;&#26629;&#30340;&#27969;&#22330;&#65292;&#32467;&#26524;&#34920;&#26126;PINNs&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#22788;&#29702;&#21453;&#21521;&#38382;&#39064;&#26041;&#38754;&#27604;&#20256;&#32479;CFD&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04501</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#26426;&#21494;&#26629;&#27969;&#22330;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Investigation of compressor cascade flow based on physics-informed neural networks. (arXiv:2308.04501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26041;&#27861;&#26469;&#39044;&#27979;&#21387;&#32553;&#26426;&#21494;&#26629;&#30340;&#27969;&#22330;&#65292;&#32467;&#26524;&#34920;&#26126;PINNs&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#22788;&#29702;&#21453;&#21521;&#38382;&#39064;&#26041;&#38754;&#27604;&#20256;&#32479;CFD&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20351;&#29992;&#26032;&#20852;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26041;&#27861;&#26469;&#39044;&#27979;&#21387;&#32553;&#26426;&#21494;&#26629;&#30340;&#27969;&#22330;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#20108;&#32500;&#38382;&#39064;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#21516;&#26102;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20013;&#30340;Navier-Stokes&#26041;&#31243;&#12290;&#22312;&#27491;&#21521;&#38382;&#39064;&#20013;&#65292;PINNs&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#21387;&#32553;&#26426;&#30340;&#27969;&#22330;&#12290;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30456;&#27604;&#65292;PINNs&#27169;&#22411;&#34701;&#21512;&#20102;&#30456;&#20851;&#37327;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#27809;&#26377;&#37096;&#20998;&#36793;&#30028;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;PINNs&#22312;&#22788;&#29702;&#21453;&#21521;&#38382;&#39064;&#26102;&#26174;&#31034;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;PINNs&#20165;&#22522;&#20110;&#37096;&#20998;&#36895;&#24230;&#21521;&#37327;&#21644;&#22721;&#21387;&#20449;&#24687;&#25104;&#21151;&#37325;&#26500;&#20102;&#21387;&#32553;&#26426;&#21494;&#26629;&#30340;&#27969;&#22330;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;PINNs&#20026;&#28065;&#36718;&#26426;&#26800;&#35774;&#35745;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#24403;&#21069;&#20027;&#23548;&#30340;CFD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we utilize the emerging Physics Informed Neural Networks (PINNs) approach for the first time to predict the flow field of a compressor cascade. The approach is demonstrated on a two-dimensional problem, incorporating Navier-Stokes equations in both the forward and inverse problems. In the forward problem, PINNs effectively predict the flow field of the compressor. The key advantage over Deep Neural Networks (DNNs) is that the PINNs model incorporates a physical relationship between the relevant quantities, resulting in more precise predictions. PINNs show obvious advantages over the traditional CFD approaches when dealing with inverse problems in the absence of partial boundary conditions. PINNs successfully reconstruct the flow field of the compressor cascade solely based on partial velocity vectors and wall pressure information. This research provides compelling evidence that PINNs offer turbomachinery designers a promising alternative to the current dominant CFD metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#20803;&#20809;&#23376;&#35745;&#31639;&#33455;&#29255;&#21644;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#30340;&#39640;&#25928;&#26399;&#26435;&#23450;&#20215;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#24133;&#24230;&#20272;&#35745;&#31639;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#27425;&#21152;&#36895;&#12290;&#20854;&#20013;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#39640;&#25928;&#23398;&#20064;&#21644;&#21152;&#36733;&#36164;&#20135;&#20998;&#24067;&#65292;&#20934;&#30830;&#25429;&#25417;&#24066;&#22330;&#36235;&#21183;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#37329;&#34701;&#24212;&#29992;&#20013;&#30340;&#19987;&#29992;&#20809;&#23376;&#22788;&#29702;&#22120;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2308.04493</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#20803;&#20809;&#23376;&#35745;&#31639;&#33455;&#29255;&#21644;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#30340;&#39640;&#25928;&#26399;&#26435;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Efficient option pricing with unary-based photonic computing chip and generative adversarial learning. (arXiv:2308.04493v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#20803;&#20809;&#23376;&#35745;&#31639;&#33455;&#29255;&#21644;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#30340;&#39640;&#25928;&#26399;&#26435;&#23450;&#20215;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#24133;&#24230;&#20272;&#35745;&#31639;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#27425;&#21152;&#36895;&#12290;&#20854;&#20013;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#39640;&#25928;&#23398;&#20064;&#21644;&#21152;&#36733;&#36164;&#20135;&#20998;&#24067;&#65292;&#20934;&#30830;&#25429;&#25417;&#24066;&#22330;&#36235;&#21183;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#37329;&#34701;&#24212;&#29992;&#20013;&#30340;&#19987;&#29992;&#20809;&#23376;&#22788;&#29702;&#22120;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#37329;&#34701;&#34892;&#19994;&#31995;&#32479;&#20013;&#65292;&#20135;&#21697;&#32467;&#26500;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20256;&#32479;&#35745;&#31639;&#33021;&#21147;&#30340;&#29942;&#39048;&#24050;&#32463;&#21046;&#32422;&#20102;&#37329;&#34701;&#34892;&#19994;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#27431;&#24335;&#26399;&#26435;&#23450;&#20215;&#30340;&#20809;&#23376;&#33455;&#29255;&#65292;&#32467;&#21512;&#37327;&#23376;&#24133;&#24230;&#20272;&#35745;&#31639;&#27861;&#65292;&#19982;&#32463;&#20856;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#20108;&#27425;&#21152;&#36895;&#12290;&#30005;&#36335;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#21152;&#36733;&#36164;&#20135;&#20215;&#26684;&#20998;&#24067;&#30340;&#27169;&#22359;&#12289;&#35745;&#31639;&#39044;&#26399;&#25910;&#30410;&#30340;&#27169;&#22359;&#20197;&#21450;&#25191;&#34892;&#37327;&#23376;&#24133;&#24230;&#20272;&#35745;&#31639;&#27861;&#20197;&#24341;&#20837;&#21152;&#36895;&#30340;&#27169;&#22359;&#12290;&#22312;&#20998;&#24067;&#27169;&#22359;&#20013;&#65292;&#23884;&#20837;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20197;&#20415;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#21152;&#36733;&#36164;&#20135;&#20998;&#24067;&#65292;&#20934;&#30830;&#25429;&#25417;&#24066;&#22330;&#36235;&#21183;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#19987;&#29992;&#20809;&#23376;&#22788;&#29702;&#22120;&#22312;&#37329;&#34701;&#24212;&#29992;&#20013;&#21457;&#23637;&#30340;&#19968;&#20010;&#26032;&#36827;&#23637;&#65292;&#26377;&#28508;&#21147;&#25913;&#36827;&#26399;&#26435;&#23450;&#20215;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern financial industry system, the structure of products has become more and more complex, and the bottleneck constraint of classical computing power has already restricted the development of the financial industry. Here, we present a photonic chip that implements the unary approach to European option pricing, in combination with the quantum amplitude estimation algorithm, to achieve a quadratic speedup compared to classical Monte Carlo methods. The circuit consists of three modules: a module loading the distribution of asset prices, a module computing the expected payoff, and a module performing the quantum amplitude estimation algorithm to introduce speed-ups. In the distribution module, a generative adversarial network is embedded for efficient learning and loading of asset distributions, which precisely capture the market trends. This work is a step forward in the development of specialized photonic processors for applications in finance, with the potential to improve the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21463;&#31361;&#35302;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36807;&#28388;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#28388;&#27874;&#22120;&#20013;&#27491;&#36127;&#26435;&#37325;&#30340;&#29420;&#31435;&#37325;&#35201;&#24615;&#65292;&#24182;&#25490;&#21517;&#36827;&#34892;&#20462;&#21098;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;FLOPs&#21644;Params&#25968;&#37327;&#24182;&#20445;&#25345;&#20934;&#30830;&#29575;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2308.04470</link><description>&lt;p&gt;
D-Score:&#19968;&#31181;&#21463;&#31361;&#35302;&#21551;&#21457;&#30340;&#29992;&#20110;&#36807;&#28388;&#20462;&#21098;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
D-Score: A Synapse-Inspired Approach for Filter Pruning. (arXiv:2308.04470v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21463;&#31361;&#35302;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36807;&#28388;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#28388;&#27874;&#22120;&#20013;&#27491;&#36127;&#26435;&#37325;&#30340;&#29420;&#31435;&#37325;&#35201;&#24615;&#65292;&#24182;&#25490;&#21517;&#36827;&#34892;&#20462;&#21098;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;FLOPs&#21644;Params&#25968;&#37327;&#24182;&#20445;&#25345;&#20934;&#30830;&#29575;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30830;&#23450;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#20013;&#19981;&#37325;&#35201;&#36807;&#28388;&#22120;&#25490;&#21517;&#30340;&#26032;&#26041;&#38754;&#12290;&#22312;&#20154;&#31867;&#31361;&#35302;&#31995;&#32479;&#20013;&#65292;&#26377;&#20004;&#20010;&#37325;&#35201;&#30340;&#36890;&#36947;&#34987;&#31216;&#20026;&#20852;&#22859;&#24615;&#21644;&#25233;&#21046;&#24615;&#31070;&#32463;&#36882;&#36136;&#65292;&#23427;&#20204;&#23558;&#20449;&#21495;&#20174;&#31070;&#32463;&#20803;&#20256;&#36882;&#21040;&#32454;&#32990;&#12290;&#37319;&#29992;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31361;&#35302;&#21551;&#21457;&#30340;&#36807;&#28388;&#20462;&#21098;&#26041;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#35780;&#20998;(D-Score)&#12290;D-Score&#20998;&#26512;&#28388;&#27874;&#22120;&#20013;&#27491;&#36127;&#26435;&#37325;&#30340;&#29420;&#31435;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#20998;&#25968;&#23545;&#29420;&#31435;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#21517;&#12290;&#20462;&#21098;&#20855;&#26377;&#20302;&#24635;&#20998;&#21644;&#23545;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#24433;&#21709;&#36739;&#20302;&#30340;&#28388;&#27874;&#22120;&#12290;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;FLOPs&#21644;Params&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#23545;&#20934;&#30830;&#29575;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new aspect for determining the rank of the unimportant filters for filter pruning on convolutional neural networks (CNNs). In the human synaptic system, there are two important channels known as excitatory and inhibitory neurotransmitters that transmit a signal from a neuron to a cell. Adopting the neuroscientific perspective, we propose a synapse-inspired filter pruning method, namely Dynamic Score (D-Score). D-Score analyzes the independent importance of positive and negative weights in the filters and ranks the independent importance by assigning scores. Filters having low overall scores, and thus low impact on the accuracy of neural networks are pruned. The experimental results on CIFAR-10 and ImageNet datasets demonstrate the effectiveness of our proposed method by reducing notable amounts of FLOPs and Params without significant Acc. Drop.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#22238;&#24402;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#26816;&#27979;&#21307;&#30103;&#20445;&#38505;&#32034;&#36180;&#20013;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#21306;&#20998;&#27450;&#35784;&#21644;&#38750;&#27450;&#35784;&#32034;&#36180;&#12290;</title><link>http://arxiv.org/abs/2308.04469</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#21307;&#30103;&#20445;&#38505;&#32034;&#36180;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Correlating Medi- Claim Service by Deep Learning Neural Networks. (arXiv:2308.04469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#22238;&#24402;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#26816;&#27979;&#21307;&#30103;&#20445;&#38505;&#32034;&#36180;&#20013;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#21306;&#20998;&#27450;&#35784;&#21644;&#38750;&#27450;&#35784;&#32034;&#36180;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#38505;&#32034;&#36180;&#20013;&#23384;&#22312;&#19982;&#24739;&#32773;&#12289;&#21307;&#29983;&#12289;&#35786;&#26029;&#20013;&#24515;&#21644;&#20445;&#38505;&#25552;&#20379;&#21830;&#30456;&#20851;&#30340;&#26377;&#32452;&#32455;&#29359;&#32618;&#65292;&#24418;&#25104;&#20102;&#24517;&#39035;&#19981;&#26029;&#30417;&#27979;&#30340;&#36830;&#38145;&#21453;&#24212;&#12290;&#36825;&#20123;&#27450;&#35784;&#34892;&#20026;&#24433;&#21709;&#34987;&#20445;&#38505;&#20154;&#21644;&#20581;&#24247;&#20445;&#38505;&#20844;&#21496;&#30340;&#36130;&#21153;&#22686;&#38271;&#12290;&#36890;&#36807;&#23545;&#22238;&#24402;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#26816;&#27979;&#27450;&#35784;&#32034;&#36180;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26816;&#27979;&#26469;&#33258;&#19981;&#21516;&#25552;&#20379;&#21830;&#30340;&#19981;&#21516;&#32034;&#36180;&#30340;&#27927;&#38065;&#34892;&#20026;&#12290;&#20351;&#29992;&#20102;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#27450;&#35784;&#21644;&#38750;&#27450;&#35784;&#32034;&#36180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical insurance claims are of organized crimes related to patients, physicians, diagnostic centers, and insurance providers, forming a chain reaction that must be monitored constantly. These kinds of frauds affect the financial growth of both insured people and health insurance companies. The Convolution Neural Network architecture is used to detect fraudulent claims through a correlation study of regression models, which helps to detect money laundering on different claims given by different providers. Supervised and unsupervised classifiers are used to detect fraud and non-fraud claims.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#20851;&#38190;&#23618;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#20123;&#23618;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#19979;&#23454;&#29616;&#25915;&#20987;&#25928;&#26524;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.04466</link><description>&lt;p&gt;
&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#65306;&#36890;&#36807;&#27745;&#26579;&#21518;&#38376;&#20851;&#38190;&#23618;
&lt;/p&gt;
&lt;p&gt;
Backdoor Federated Learning by Poisoning Backdoor-Critical Layers. (arXiv:2308.04466v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#20851;&#38190;&#23618;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#20123;&#23618;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#19979;&#23454;&#29616;&#25915;&#20987;&#25928;&#26524;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#25935;&#24863;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20998;&#25955;&#24335;&#23398;&#20064;&#33539;&#24335;&#21644;FL&#30340;&#24322;&#36136;&#24615;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#25915;&#20987;&#38754;&#12290;&#29616;&#26377;&#30340;FL&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#36890;&#24120;&#20250;&#20851;&#27880;&#25972;&#20010;&#27169;&#22411;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#26041;&#27861;&#24847;&#35782;&#21040;&#21518;&#38376;&#20851;&#38190;&#65288;BC&#65289;&#23618;&#30340;&#23384;&#22312;&#65292;&#21518;&#38376;&#20851;&#38190;&#23618;&#26159;&#25351;&#25511;&#21046;&#27169;&#22411;&#28431;&#27934;&#30340;&#19968;&#23567;&#37096;&#20998;&#23618;&#12290;&#25915;&#20987;BC&#23618;&#21487;&#20197;&#36798;&#21040;&#25915;&#20987;&#25972;&#20010;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20294;&#34987;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25163;&#27573;&#21457;&#29616;&#30340;&#26426;&#20250;&#35201;&#23567;&#24471;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#25915;&#20987;&#32773;&#30340;&#35282;&#24230;&#35782;&#21035;&#21644;&#39564;&#35777;BC&#23618;&#30340;&#26222;&#36866;&#24615;&#26041;&#27861;&#12290;&#22522;&#20110;&#35782;&#21035;&#20986;&#30340;BC&#23618;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#38450;&#24481;&#31574;&#30053;&#33258;&#36866;&#24212;&#22320;&#23547;&#27714;&#25915;&#20987;&#25928;&#26524;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32908;&#32905;&#25511;&#21046;&#22120;&#65292;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#24515;&#29366;&#24577;&#31354;&#38388;&#30417;&#27979;&#20154;&#31867;&#24179;&#34913;&#33021;&#21147;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04462</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32908;&#32905;&#25511;&#21046;&#22120;&#23545;&#20154;&#31867;&#24179;&#34913;&#36827;&#34892;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Characterization of Human Balance through a Reinforcement Learning-based Muscle Controller. (arXiv:2308.04462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32908;&#32905;&#25511;&#21046;&#22120;&#65292;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#24515;&#29366;&#24577;&#31354;&#38388;&#30417;&#27979;&#20154;&#31867;&#24179;&#34913;&#33021;&#21147;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36523;&#20307;&#24247;&#22797;&#26399;&#38388;&#65292;&#24179;&#34913;&#35780;&#20272;&#24120;&#24120;&#20381;&#36182;&#20110;&#35780;&#20998;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20027;&#35266;&#24615;&#12290;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#23458;&#35266;&#30340;&#24179;&#34913;&#35780;&#20272;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#20165;&#38480;&#20110;&#36319;&#36394;&#21387;&#21147;&#20013;&#24515;(COP)&#65292;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#20840;&#36523;&#23039;&#21183;&#31283;&#23450;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#36136;&#24515;(COM)&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#26395;&#29992;&#20110;&#30417;&#27979;&#20154;&#31867;&#24179;&#34913;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#39592;&#39612;&#32908;&#32905;&#27169;&#22411;&#19982;&#24179;&#34913;&#25511;&#21046;&#22120;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;(RL)&#36827;&#34892;&#35757;&#32451;&#65292;&#30740;&#31350;&#24179;&#34913;&#33021;&#21147;&#12290;RL&#26694;&#26550;&#30001;&#20004;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20998;&#21035;&#25511;&#21046;&#24179;&#34913;&#24674;&#22797;&#21644;&#32908;&#32905;&#21327;&#35843;&#65292;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#21442;&#32771;&#29366;&#24577;&#21021;&#22987;&#21270;&#12289;&#26089;&#20572;&#21644;&#22810;&#35757;&#32451;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;&#38543;&#26426;&#21021;&#22987;COM&#29366;&#24577;(&#20301;&#32622;&#21644;&#36895;&#24230;)&#31354;&#38388;&#20013;&#24674;&#22797;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balance assessment during physical rehabilitation often relies on rubric-oriented battery tests to score a patient's physical capabilities, leading to subjectivity. While some objective balance assessments exist, they are often limited to tracking the center of pressure (COP), which does not fully capture the whole-body postural stability. This study explores the use of the center of mass (COM) state space and presents a promising avenue for monitoring the balance capabilities in humans. We employ a musculoskeletal model integrated with a balance controller, trained through reinforcement learning (RL), to investigate balancing capabilities. The RL framework consists of two interconnected neural networks governing balance recovery and muscle coordination respectively, trained using Proximal Policy Optimization (PPO) with reference state initialization, early termination, and multiple training strategies. By exploring recovery from random initial COM states (position and velocity) space 
&lt;/p&gt;</description></item><item><title>Pangu&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#19982;&#19981;&#21516;&#30340;&#25968;&#20540;&#27169;&#25311;&#39044;&#25253;&#31995;&#32479;&#30340;&#27169;&#22411;&#21021;&#22987;&#26465;&#20214;&#20860;&#23481;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#23545;&#31283;&#23450;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04460</link><description>&lt;p&gt;
Pangu&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#19982;&#27668;&#35937;&#36816;&#34892;&#25968;&#25454;&#30340;&#20860;&#23481;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Compatibility between the Pangu Weather Forecasting Model and Meteorological Operational Data. (arXiv:2308.04460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04460
&lt;/p&gt;
&lt;p&gt;
Pangu&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#19982;&#19981;&#21516;&#30340;&#25968;&#20540;&#27169;&#25311;&#39044;&#25253;&#31995;&#32479;&#30340;&#27169;&#22411;&#21021;&#22987;&#26465;&#20214;&#20860;&#23481;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#23545;&#31283;&#23450;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20010;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#29992;&#20110;&#22825;&#27668;&#39044;&#25253;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#25968;&#20540;&#27169;&#25311;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;&#29305;&#21035;&#26159;Pangu&#22825;&#27668;&#27169;&#22411;&#26159;&#24320;&#28304;&#30340;&#38750;&#21830;&#19994;&#29992;&#36884;&#65292;&#24050;&#32463;&#36890;&#36807;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;&#65288;ECMWF&#65289;&#30340;&#39564;&#35777;&#20854;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#26368;&#36817;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;Pangu&#22825;&#27668;&#27169;&#22411;&#19982;&#20960;&#31181;&#24120;&#29992;&#30340;&#25968;&#20540;&#27169;&#25311;&#22825;&#27668;&#39044;&#25253;&#36816;&#34892;&#20998;&#26512;&#30340;&#20860;&#23481;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Pangu&#22825;&#27668;&#27169;&#22411;&#19982;&#19981;&#21516;&#30340;&#25968;&#20540;&#27169;&#25311;&#39044;&#25253;&#31995;&#32479;&#30340;&#27169;&#22411;&#21021;&#22987;&#26465;&#20214;&#20860;&#23481;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#23545;&#31283;&#23450;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25913;&#36827;&#20840;&#29699;&#25110;&#26412;&#22320;&#21021;&#22987;&#26465;&#20214;&#30340;&#36136;&#37327;&#23545;&#20110;&#22686;&#24378;&#39044;&#27979;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, multiple data-driven models based on machine learning for weather forecasting have emerged. These models are highly competitive in terms of accuracy compared to traditional numerical weather prediction (NWP) systems. In particular, the Pangu-Weather model, which is open source for non-commercial use, has been validated for its forecasting performance by the European Centre for Medium-Range Weather Forecasts (ECMWF) and has recently been published in the journal "Nature". In this paper, we evaluate the compatibility of the Pangu-Weather model with several commonly used NWP operational analyses through case studies. The results indicate that the Pangu-Weather model is compatible with different operational analyses from various NWP systems as the model initial conditions, and it exhibits a relatively stable forecasting capability. Furthermore, we have verified that improving the quality of global or local initial conditions significantly contributes to enhancing the forecasting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;MCTS&#24341;&#23548;&#30340;&#36951;&#20256;&#31639;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#20248;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26641;&#25628;&#32034;&#31574;&#30053;&#65292;&#29992;&#20110;&#25506;&#32034;&#25972;&#20010;&#22522;&#22240;&#26641;&#32467;&#26500;&#65292;&#20197;&#26368;&#20248;&#26041;&#24335;&#25628;&#32034;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04459</link><description>&lt;p&gt;
MCTS&#24341;&#23548;&#30340;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
MCTS guided Genetic Algorithm for optimization of neural network weights. (arXiv:2308.04459v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;MCTS&#24341;&#23548;&#30340;&#36951;&#20256;&#31639;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#20248;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26641;&#25628;&#32034;&#31574;&#30053;&#65292;&#29992;&#20110;&#25506;&#32034;&#25972;&#20010;&#22522;&#22240;&#26641;&#32467;&#26500;&#65292;&#20197;&#26368;&#20248;&#26041;&#24335;&#25628;&#32034;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25628;&#32034;&#31574;&#30053;&#24212;&#29992;&#20110;&#36951;&#20256;&#31639;&#27861;&#26469;&#25506;&#32034;&#25972;&#20010;&#22522;&#22240;&#26641;&#32467;&#26500;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#36827;&#34892;&#26641;&#25628;&#32034;&#65292;&#20294;&#26159;&#31616;&#21333;&#30340;&#31639;&#27861;&#22914;&#24191;&#24230;&#20248;&#20808;&#12289;&#28145;&#24230;&#20248;&#20808;&#21644;&#36845;&#20195;&#25216;&#26415;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#65292;&#24120;&#24120;&#23548;&#33268;&#25191;&#34892;&#26102;&#38388;&#36739;&#38271;&#12290;&#24403;&#36827;&#34892;&#27010;&#29575;&#25628;&#32034;&#26102;&#65292;&#23545;&#25163;&#31639;&#27861;&#24448;&#24448;&#26159;&#39318;&#36873;&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#24471;&#21040;&#26368;&#20248;&#32467;&#26524;&#12290;&#26412;&#25991;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#12290;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#24418;&#25104;&#20102;&#19968;&#20010;&#21487;&#33021;&#29366;&#24577;&#30340;&#26641;&#65292;&#24182;&#36890;&#36807;&#36866;&#24212;&#24230;&#20989;&#25968;&#25552;&#20379;&#22870;&#21169;&#26426;&#21046;&#12290;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26641;&#25628;&#32034;&#31574;&#30053;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#32467;&#21512;&#36825;&#20123;&#26041;&#27861;&#26469;&#26368;&#20248;&#22320;&#25628;&#32034;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we investigate the possibility of applying a search strategy to genetic algorithms to explore the entire genetic tree structure. Several methods aid in performing tree searches; however, simpler algorithms such as breadth-first, depth-first, and iterative techniques are computation-heavy and often result in a long execution time. Adversarial techniques are often the preferred mechanism when performing a probabilistic search, yielding optimal results more quickly. The problem we are trying to tackle in this paper is the optimization of neural networks using genetic algorithms. Genetic algorithms (GA) form a tree of possible states and provide a mechanism for rewards via the fitness function. Monte Carlo Tree Search (MCTS) has proven to be an effective tree search strategy given states and rewards; therefore, we will combine these approaches to optimally search for the best result generated with genetic algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#20102;&#29289;&#29702;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#22320;&#19979;&#33021;&#28304;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;PIML&#22312;&#22320;&#38663;&#24212;&#29992;&#12289;&#27833;&#34255;&#27169;&#25311;&#21644;&#27833;&#27668;&#29983;&#20135;&#31561;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04457</link><description>&lt;p&gt;
&#23545;&#22320;&#19979;&#33021;&#28304;&#31995;&#32479;&#20013;&#29289;&#29702;&#39537;&#21160;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#25209;&#21028;&#24615;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems. (arXiv:2308.04457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#20102;&#29289;&#29702;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#22320;&#19979;&#33021;&#28304;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;PIML&#22312;&#22320;&#38663;&#24212;&#29992;&#12289;&#27833;&#34255;&#27169;&#25311;&#21644;&#27833;&#27668;&#29983;&#20135;&#31561;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#23427;&#21487;&#20197;&#25581;&#31034;&#22823;&#25968;&#25454;&#38598;&#20013;&#38544;&#34255;&#30340;&#27169;&#24335;&#65292;&#24182;&#25581;&#31034;&#26080;&#19982;&#20262;&#27604;&#30340;&#27934;&#23519;&#21147;&#65292;&#20174;&#32780;&#38761;&#26032;&#35768;&#22810;&#34892;&#19994;&#21644;&#23398;&#31185;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#26377;&#38480;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#29702;&#21644;&#24037;&#31243;&#31561;&#24212;&#29992;&#20013;&#12290;&#30456;&#21453;&#65292;&#29289;&#29702;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#25216;&#26415;&#23558;&#29289;&#29702;&#21407;&#29702;&#34701;&#20837;&#21040;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;PIML&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36981;&#23432;&#27835;&#29702;&#29289;&#29702;&#23450;&#24459;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#19982;&#22320;&#19979;&#33021;&#28304;&#31995;&#32479;&#30456;&#20851;&#30340;PIML&#24212;&#29992;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#30707;&#27833;&#21644;&#22825;&#28982;&#27668;&#34892;&#19994;&#12290;&#22238;&#39038;&#31361;&#20986;&#20102;PIML&#22312;&#22320;&#38663;&#24212;&#29992;&#12289;&#27833;&#34255;&#27169;&#25311;&#21644;&#27833;&#27668;&#29983;&#20135;&#31561;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has emerged as a powerful tool in various fields, including computer vision, natural language processing, and speech recognition. It can unravel hidden patterns within large data sets and reveal unparalleled insights, revolutionizing many industries and disciplines. However, machine and deep learning models lack interpretability and limited domain-specific knowledge, especially in applications such as physics and engineering. Alternatively, physics-informed machine learning (PIML) techniques integrate physics principles into data-driven models. By combining deep learning with domain knowledge, PIML improves the generalization of the model, abidance by the governing physical laws, and interpretability. This paper comprehensively reviews PIML applications related to subsurface energy systems, mainly in the oil and gas industry. The review highlights the successful utilization of PIML for tasks such as seismic applications, reservoir simulation, hydrocarbons production fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;ResNets-10&#27169;&#22411;&#23545;&#37329;&#23646;-&#32477;&#32536;&#20307;-&#37329;&#23646;&#20171;&#36136;&#24067;&#38754;&#36827;&#34892;&#39640;&#31934;&#24230;&#39044;&#27979;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#21644;&#23567;&#23398;&#20064;&#29575;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36229;&#20302;&#35823;&#24046;&#20540;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#21462;&#20195;&#20256;&#32479;&#30340;&#30005;&#30913;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#21487;&#22823;&#24133;&#38477;&#20302;&#35774;&#35745;&#36807;&#31243;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.04450</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#37329;&#23646;-&#32477;&#32536;&#20307;-&#37329;&#23646;&#20171;&#36136;&#24067;&#38754;&#30340;&#39640;&#31934;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning. (arXiv:2308.04450v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;ResNets-10&#27169;&#22411;&#23545;&#37329;&#23646;-&#32477;&#32536;&#20307;-&#37329;&#23646;&#20171;&#36136;&#24067;&#38754;&#36827;&#34892;&#39640;&#31934;&#24230;&#39044;&#27979;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#21644;&#23567;&#23398;&#20064;&#29575;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36229;&#20302;&#35823;&#24046;&#20540;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#21462;&#20195;&#20256;&#32479;&#30340;&#30005;&#30913;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#21487;&#22823;&#24133;&#38477;&#20302;&#35774;&#35745;&#36807;&#31243;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#30005;&#30913;&#36719;&#20214;&#35745;&#31639;&#32467;&#26524;&#30340;&#39044;&#27979;&#26041;&#38754;&#19968;&#30452;&#26159;&#19968;&#20010;&#24191;&#27867;&#35752;&#35770;&#30340;&#38382;&#39064;&#12290;&#20294;&#39044;&#27979;&#20934;&#30830;&#29575;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;ResNets-10&#27169;&#22411;&#26469;&#39044;&#27979;&#31561;&#31163;&#23376;&#20803;&#24067;&#38754;&#30340;S11&#21442;&#25968;&#12290;&#36890;&#36807;k&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#23567;&#23398;&#20064;&#29575;&#36827;&#34892;&#20004;&#38454;&#27573;&#35757;&#32451;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;&#23545;&#20110;&#38109;&#12289;&#37329;&#21644;&#38134;&#37329;&#23646;-&#32477;&#32536;&#20307;-&#37329;&#23646;&#24067;&#38754;&#30340;&#39044;&#27979;&#25439;&#22833;&#20998;&#21035;&#20026;-48.45&#12289;-46.47&#21644;-35.54&#12290;&#30001;&#20110;&#35823;&#24046;&#20540;&#26497;&#20302;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#21487;&#20197;&#22312;&#19968;&#23450;&#32467;&#26500;&#33539;&#22260;&#20869;&#21462;&#20195;&#20256;&#32479;&#30340;&#30005;&#30913;&#35745;&#31639;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#35813;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#21487;&#20197;&#22312;&#19981;&#21040;1100&#20010;&#21608;&#26399;&#20869;&#23436;&#25104;&#12290;&#36825;&#24847;&#21619;&#30528;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#35774;&#35745;&#36807;&#31243;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;ResNets-10&#27169;&#22411;&#36824;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#20803;-&#34893;&#23556;&#35013;&#32622;&#21644;&#29983;&#29289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning prediction of electromagnetic software calculation results has been a widely discussed issue in recent years. But the prediction accuracy was still one of the challenges to be solved. In this work, we proposed that the ResNets-10 model was used for predicting plasmonic metasurface S11 parameters. The two-stage training was performed by the k-fold cross-validation and small learning rate. After the training was completed, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace the traditional electromagnetic computing method for calculation within a certain structural range. Besides, this network can finish the training process less than 1,100 epochs. This means that the network training process can effectively lower the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and bios
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;AI&#39046;&#22495;&#30340;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#38598;&#20013;&#30417;&#31649;&#19982;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#21452;&#37325;&#27835;&#29702;&#26041;&#24335;&#12290;&#38598;&#20013;&#30417;&#31649;&#23384;&#22312;&#32570;&#20047;&#26126;&#30830;&#24615;&#21644;&#32479;&#19968;&#24615;&#31561;&#38382;&#39064;&#65292;&#32780;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#21017;&#23384;&#22312;&#32570;&#20047;&#32479;&#19968;&#24615;&#21644;&#21512;&#35268;&#24615;&#30340;&#19981;&#36275;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20419;&#36827;&#29983;&#25104;AI&#39046;&#22495;&#30340;&#36127;&#36131;&#20219;&#21644;&#36947;&#24503;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.04448</link><description>&lt;p&gt;
&#21452;&#37325;&#27835;&#29702;&#65306;&#38598;&#20013;&#30417;&#31649;&#19982;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#22312;&#29983;&#25104;AI&#20013;&#30340;&#20132;&#38598;
&lt;/p&gt;
&lt;p&gt;
Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI. (arXiv:2308.04448v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;AI&#39046;&#22495;&#30340;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#38598;&#20013;&#30417;&#31649;&#19982;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#21452;&#37325;&#27835;&#29702;&#26041;&#24335;&#12290;&#38598;&#20013;&#30417;&#31649;&#23384;&#22312;&#32570;&#20047;&#26126;&#30830;&#24615;&#21644;&#32479;&#19968;&#24615;&#31561;&#38382;&#39064;&#65292;&#32780;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#21017;&#23384;&#22312;&#32570;&#20047;&#32479;&#19968;&#24615;&#21644;&#21512;&#35268;&#24615;&#30340;&#19981;&#36275;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20419;&#36827;&#29983;&#25104;AI&#39046;&#22495;&#30340;&#36127;&#36131;&#20219;&#21644;&#36947;&#24503;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26368;&#36817;&#22312;&#28040;&#36153;&#32773;&#38754;&#21521;&#30340;&#12289;&#24320;&#25918;&#24335;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#26679;&#30340;&#31995;&#32479;&#24341;&#21457;&#20102;&#37325;&#35201;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#21253;&#25324;&#38544;&#31169;&#20405;&#29359;&#12289;&#34394;&#20551;&#20449;&#24687;&#21644;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#12290;&#29983;&#25104;AI&#21487;&#33021;&#21462;&#20195;&#20154;&#31867;&#30340;&#21019;&#36896;&#21147;&#21644;&#35851;&#29983;&#26041;&#24335;&#20063;&#21463;&#21040;&#20102;&#20005;&#26684;&#23457;&#26597;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#65292;&#22312;&#29983;&#25104;AI&#39046;&#22495;&#38656;&#35201;&#19968;&#31181;&#36127;&#36131;&#20219;&#21644;&#36947;&#24503;&#30340;&#21457;&#23637;&#25919;&#31574;&#21644;&#30417;&#31649;&#26426;&#21046;&#12290;&#29616;&#26377;&#21644;&#25552;&#35758;&#30340;&#25919;&#24220;&#38598;&#20013;&#30417;&#31649;AI&#30340;&#35268;&#23450;&#38754;&#20020;&#35832;&#22810;&#25209;&#35780;&#65292;&#20363;&#22914;&#32570;&#20047;&#36275;&#22815;&#30340;&#26126;&#30830;&#24615;&#21644;&#32479;&#19968;&#24615;&#65292;&#22312;&#19981;&#21516;&#21496;&#27861;&#36758;&#21306;&#20043;&#38388;&#32570;&#20047;&#20114;&#25805;&#20316;&#24615;&#65292;&#38480;&#21046;&#21019;&#26032;&#65292;&#38459;&#30861;&#33258;&#30001;&#24066;&#22330;&#31454;&#20105;&#12290;&#20998;&#25955;&#30340;&#20247;&#21253;&#23433;&#20840;&#24037;&#20855;&#21644;&#26426;&#21046;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#32570;&#20047;&#32479;&#19968;&#24615;&#12289;&#21512;&#35268;&#24615;&#21644;&#36879;&#26126;&#24615;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) has seen mainstream adoption lately, especially in the form of consumer-facing, open-ended, text and image generating models. However, the use of such systems raises significant ethical and safety concerns, including privacy violations, misinformation and intellectual property theft. The potential for generative AI to displace human creativity and livelihoods has also been under intense scrutiny. To mitigate these risks, there is an urgent need of policies and regulations responsible and ethical development in the field of generative AI. Existing and proposed centralized regulations by governments to rein in AI face criticisms such as not having sufficient clarity or uniformity, lack of interoperability across lines of jurisdictions, restricting innovation, and hindering free market competition. Decentralized protections via crowdsourced safety tools and mechanisms are a potential alternative. However, they have clear deficiencies in terms of lac
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#26410;&#26469;&#30340;AI&#65292;&#38656;&#35201;&#20174;&#29983;&#25104;&#24335;AI&#36716;&#21521;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#36890;&#36807;&#22521;&#20859;&#22522;&#20110;&#26126;&#30830;&#30693;&#35782;&#21644;&#32463;&#39564;&#35268;&#21017;&#30340;AI&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#30340;&#38480;&#21046;&#24182;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#36182;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04445</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#24335;AI&#36208;&#21521;&#21487;&#20449;&#36182;&#30340;AI&#65306;LLM&#21487;&#20197;&#20174;Cyc&#20013;&#23398;&#21040;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc. (arXiv:2308.04445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04445
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26410;&#26469;&#30340;AI&#65292;&#38656;&#35201;&#20174;&#29983;&#25104;&#24335;AI&#36716;&#21521;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#36890;&#36807;&#22521;&#20859;&#22522;&#20110;&#26126;&#30830;&#30693;&#35782;&#21644;&#32463;&#39564;&#35268;&#21017;&#30340;AI&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#30340;&#38480;&#21046;&#24182;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#36182;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#26159;&#30446;&#21069;&#26368;&#27969;&#34892;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#30001;&#35757;&#32451;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#20449;&#65292;&#20294;&#19981;&#19968;&#23450;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#24120;&#24120;&#20196;&#20154;&#24778;&#21497;&#65292;&#20294;&#22312;&#25512;&#29702;&#26041;&#38754;&#23427;&#20204;&#23384;&#22312;&#32570;&#38519;&#65292;&#23548;&#33268;LLMs&#19981;&#23436;&#20840;&#21487;&#20449;&#36182;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#32467;&#26524;&#24448;&#24448;&#19981;&#21487;&#39044;&#27979;&#21644;&#19981;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#30340;16&#20010;&#26399;&#26395;&#65292;&#24182;&#35752;&#35770;&#20102;AI&#30340;&#21478;&#19968;&#31181;&#21487;&#33021;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#35768;&#22810;&#38480;&#21046;&#30340;&#26041;&#27861;&#65306;&#22521;&#20859;&#22522;&#20110;&#26126;&#30830;&#30693;&#35782;&#21644;&#32463;&#39564;&#35268;&#21017;&#30340;AI&#65292;&#20351;&#25512;&#29702;&#24341;&#25806;&#33021;&#22815;&#33258;&#21160;&#25512;&#23548;&#20986;&#25152;&#26377;&#30693;&#35782;&#30340;&#36923;&#36753;&#34164;&#21547;&#12290;&#21363;&#20351;&#26159;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#20135;&#29983;&#30340;&#38271;&#35770;&#35777;&#20063;&#21487;&#20197;&#26159;&#21487;&#20449;&#19988;&#21487;&#35299;&#37322;&#30340;&#65292;&#22240;&#20026;&#23436;&#25972;&#30340;&#36880;&#27493;&#25512;&#29702;&#36807;&#31243;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#65292;&#24182;&#19988;&#21487;&#20197;&#35760;&#24405;&#21644;&#23457;&#35745;&#27599;&#20010;&#27493;&#39588;&#20351;&#29992;&#30340;&#30693;&#35782;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI, the most popular current approach to AI, consists of large language models (LLMs) that are trained to produce outputs that are plausible, but not necessarily correct. Although their abilities are often uncanny, they are lacking in aspects of reasoning, leading LLMs to be less than completely trustworthy. Furthermore, their results tend to be both unpredictable and uninterpretable.  We lay out 16 desiderata for future AI, and discuss an alternative approach to AI which could theoretically address many of the limitations associated with current approaches: AI educated with curated pieces of explicit knowledge and rules of thumb, enabling an inference engine to automatically deduce the logical entailments of all that knowledge. Even long arguments produced this way can be both trustworthy and interpretable, since the full step-by-step line of reasoning is always available, and for each step the provenance of the knowledge used can be documented and audited. There is however
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#24503;&#22269;&#25512;&#29305;&#19978;COVID&#30123;&#24773;&#26399;&#38388;&#25919;&#31574;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#32454;&#31890;&#24230;&#25919;&#27835;&#20559;&#22909;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#25919;&#27835;&#35266;&#28857;&#22312;&#30123;&#24773;&#26399;&#38388;&#26377;&#25152;&#22686;&#21152;&#65292;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#19981;&#21516;&#25919;&#27835;&#31867;&#21035;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.04444</link><description>&lt;p&gt;
&#24503;&#22269;&#25512;&#29305;&#19978;COVID&#30123;&#24773;&#26399;&#38388;&#25919;&#31574;&#20559;&#22909;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Changes in Policy Preferences in German Tweets during the COVID Pandemic. (arXiv:2308.04444v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#24503;&#22269;&#25512;&#29305;&#19978;COVID&#30123;&#24773;&#26399;&#38388;&#25919;&#31574;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#32454;&#31890;&#24230;&#25919;&#27835;&#20559;&#22909;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#25919;&#27835;&#35266;&#28857;&#22312;&#30123;&#24773;&#26399;&#38388;&#26377;&#25152;&#22686;&#21152;&#65292;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#19981;&#21516;&#25919;&#27835;&#31867;&#21035;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#20132;&#27969;&#25919;&#27835;&#35266;&#28857;&#30340;&#37325;&#35201;&#35770;&#22363;&#12290;&#38024;&#23545;COVID&#25514;&#26045;&#65292;&#20844;&#27665;&#30452;&#25509;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#34920;&#36798;&#20102;&#33258;&#24049;&#30340;&#25919;&#31574;&#20559;&#22909;&#12290;&#22312;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#37327;&#21270;&#25919;&#27835;&#20559;&#22909;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#22823;&#37327;&#30340;&#20869;&#23481;&#38656;&#35201;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#25552;&#21462;&#25919;&#27835;&#20559;&#22909;--&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25919;&#27835;&#20559;&#22909;&#25552;&#21462;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#26377;&#32454;&#31890;&#24230;&#25919;&#27835;&#20559;&#22909;&#27880;&#37322;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#35757;&#32451;&#22312;&#36825;&#20010;&#25968;&#25454;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20174;2019&#24180;&#21040;2022&#24180;&#30340;&#24503;&#22269;Twitter&#35821;&#26009;&#24211;&#20013;&#30340;&#25919;&#31574;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;COVID&#30123;&#24773;&#65292;&#25919;&#27835;&#35266;&#28857;&#30340;&#34920;&#36798;&#22686;&#21152;&#20102;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#25104;&#29087;&#30340;&#25919;&#31574;&#20559;&#22909;&#20998;&#31867;&#27861;&#20998;&#26512;&#20102;&#32454;&#31890;&#24230;&#30340;&#25919;&#27835;&#35266;&#28857;&#65292;&#24182;&#31361;&#20986;&#20102;&#19981;&#21516;&#25919;&#27835;&#31867;&#21035;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online social media have become an important forum for exchanging political opinions. In response to COVID measures citizens expressed their policy preferences directly on these platforms. Quantifying political preferences in online social media remains challenging: The vast amount of content requires scalable automated extraction of political preferences -- however fine grained political preference extraction is difficult with current machine learning (ML) technology, due to the lack of data sets. Here we present a novel data set of tweets with fine grained political preference annotations. A text classification model trained on this data is used to extract policy preferences in a German Twitter corpus ranging from 2019 to 2022. Our results indicate that in response to the COVID pandemic, expression of political opinions increased. Using a well established taxonomy of policy preferences we analyse fine grained political views and highlight changes in distinct political categories. The
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#21644;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#65292;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2308.04396</link><description>&lt;p&gt;
&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining. (arXiv:2308.04396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#21644;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#65292;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#25366;&#25496;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#20174;&#20449;&#24687;&#31995;&#32479;&#30340;&#20107;&#20214;&#26085;&#24535;&#20013;&#21457;&#29616;&#27969;&#31243;&#27169;&#22411;&#12290;&#27969;&#31243;&#25366;&#25496;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#38754;&#21521;&#27969;&#31243;&#30340;&#20225;&#19994;&#31995;&#32479;&#65292;&#20294;&#23545;&#20110;&#38754;&#21521;&#36890;&#20449;&#21644;&#25991;&#26723;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#65288;ECS&#65289;&#26469;&#35828;&#19981;&#22826;&#36866;&#29992;&#12290;ECS&#20107;&#20214;&#26085;&#24535;&#38750;&#24120;&#32454;&#31890;&#24230;&#65292;&#23545;&#20854;&#26085;&#24535;&#24212;&#29992;&#27969;&#31243;&#25366;&#25496;&#20250;&#23548;&#33268;&#28151;&#20081;&#30340;&#27169;&#22411;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20107;&#20214;&#25277;&#35937;&#65292;&#21363;&#22312;&#36816;&#34892;&#21457;&#29616;&#31639;&#27861;&#20043;&#21069;&#23558;&#20302;&#32423;&#21035;&#26085;&#24535;&#36716;&#25442;&#20026;&#26356;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#26082;&#26377;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;ECS&#26085;&#24535;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23450;&#21046;&#30340;ECS&#20107;&#20214;&#25277;&#35937;&#65288;ECSEA&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#35760;&#24405;&#30340;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#65288;&#39640;&#32423;&#21035;&#36319;&#36394;&#65289;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#65288;&#20174;ECS&#20013;&#25552;&#21462;&#65289;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#33258;&#21160;&#23558;&#26410;&#26469;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
One aim of Process Mining (PM) is the discovery of process models from event logs of information systems. PM has been successfully applied to process-oriented enterprise systems but is less suited for communication- and document-oriented Enterprise Collaboration Systems (ECS). ECS event logs are very fine-granular and PM applied to their logs results in spaghetti models. A common solution for this is event abstraction, i.e., converting low-level logs into more abstract high-level logs before running discovery algorithms. ECS logs come with special characteristics that have so far not been fully addressed by existing event abstraction approaches. We aim to close this gap with a tailored ECS event abstraction (ECSEA) approach that trains a model by comparing recorded actual user activities (high-level traces) with the system-generated low-level traces (extracted from the ECS). The model allows us to automatically convert future low-level traces into an abstracted high-level log that can 
&lt;/p&gt;</description></item><item><title>SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04365</link><description>&lt;p&gt;
SLEM&#65306;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#36335;&#24452;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04365
&lt;/p&gt;
&lt;p&gt;
SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#31185;&#23398;&#30340;&#20851;&#38190;&#30446;&#26631;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24471;&#20986;&#20851;&#20110;&#23545;&#20551;&#23450;&#24178;&#39044;&#30340;&#39044;&#27979;&#30340;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#36335;&#24452;&#27169;&#22411;&#12289;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#33021;&#22815;&#26126;&#30830;&#22320;&#25351;&#23450;&#20851;&#20110;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#20551;&#35774;&#12290;&#19982;DAGs&#19981;&#21516;&#65292;SEMs&#20551;&#35774;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20989;&#25968;&#38169;&#35823;&#35268;&#33539;&#65292;&#20174;&#32780;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#21487;&#38752;&#30340;&#25928;&#26524;&#22823;&#23567;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;&#65288;SLEM&#65289;&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#38598;&#25104;&#30340;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;SLEM&#33021;&#22815;&#25552;&#20379;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#22312;&#19982;SEMs&#36827;&#34892;&#32447;&#24615;&#27169;&#22411;&#27604;&#36739;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#20248;&#20110;SEMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
&lt;/p&gt;</description></item><item><title>OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2308.04226</link><description>&lt;p&gt;
OpinionConv: &#36890;&#36807;&#22522;&#20110;&#30495;&#23454;&#20027;&#35266;&#20307;&#39564;&#30340;&#35266;&#28857;&#23454;&#29616;&#23545;&#35805;&#24335;&#20135;&#21697;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
OpinionConv: Conversational Product Search with Grounded Opinions. (arXiv:2308.04226v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04226
&lt;/p&gt;
&lt;p&gt;
OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#20135;&#21697;&#26102;&#65292;&#20182;&#20154;&#30340;&#35266;&#28857;&#22312;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#20135;&#21697;&#30340;&#20027;&#35266;&#20307;&#39564;&#21487;&#20197;&#26159;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#36825;&#22312;&#38144;&#21806;&#23545;&#35805;&#20013;&#20063;&#26159;&#22914;&#27492;&#65292;&#22312;&#36825;&#31181;&#23545;&#35805;&#20013;&#65292;&#23458;&#25143;&#21644;&#38144;&#21806;&#21161;&#25163;&#20132;&#25442;&#26377;&#20851;&#20135;&#21697;&#30340;&#20107;&#23454;&#21644;&#35266;&#28857;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#27492;&#31867;&#23545;&#35805;&#30340;AI&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#32463;&#39564;&#27809;&#26377;&#30495;&#23454;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#20135;&#21697;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#30495;&#23454;&#20027;&#35266;&#21465;&#36848;&#25903;&#25345;&#23545;&#35805;&#24335;AI&#12290;&#36890;&#36807;OpinionConv&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#12290;&#20026;&#20102;&#39564;&#35777;&#29983;&#25104;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#35266;&#28857;&#34987;&#35748;&#20026;&#26159;&#30495;&#23454;&#30340;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21592;&#20063;&#30830;&#35748;&#20102;&#35266;&#28857;&#23545;&#20110;&#20915;&#31574;&#30340;&#20449;&#24687;&#22522;&#30784;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#40657;&#30418;&#29305;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03999</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#29702;&#35299;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning. (arXiv:2308.03999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#40657;&#30418;&#29305;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Explainable AI&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20934;&#30830;&#35299;&#37322;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#65306;&#20934;&#30830;&#30340;&#35299;&#37322;&#23558;&#20026;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20869;&#37096;&#26816;&#27979;&#21040;&#30340;&#36755;&#20837;&#30456;&#20851;&#20869;&#23481;&#25552;&#20379;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#40657;&#30418;&#29305;&#24615;&#12290;&#29616;&#26377;&#25216;&#26415;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#38544;&#34255;&#33410;&#28857;&#30340;&#28608;&#27963;&#21487;&#20197;&#34987;&#20154;&#31867;&#29702;&#35299;&#65292;&#20294;&#26159;&#23545;&#38544;&#34255;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#35299;&#37322;&#36827;&#34892;&#20551;&#35774;&#21644;&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#33258;&#21160;&#21270;&#26041;&#27861;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#20174;&#32500;&#22522;&#30334;&#31185;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#20013;&#31579;&#36873;&#20986;&#30340;&#32422;200&#19975;&#20010;&#31867;&#21035;&#65292;&#20197;&#21450;&#19968;&#20010;&#31216;&#20026;&#27010;&#24565;&#24402;&#32435;&#30340;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#35821;&#20041;Web&#39046;&#22495;&#30340;&#24212;&#29992;&#32780;&#24320;&#21457;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, de-mystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03443</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces. (arXiv:2308.03443v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#32972;&#26223;&#19979;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#25240;&#34935;&#38382;&#39064;&#12290;&#21442;&#25968;&#21270;&#26041;&#27861;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#27491;&#30830;&#30340;&#27169;&#22411;&#32780;&#23548;&#33268;&#20559;&#24046;&#65292;&#32780;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#30001;&#20110;&#26041;&#24046;&#32780;&#20135;&#29983;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21028;&#21035;&#24335;&#30340;&#19981;&#33391;&#34892;&#20026;&#25233;&#21046;&#22120;&#65288;MIPS&#65289;&#26469;&#36890;&#36807;&#23545;&#21160;&#20316;&#30340;&#23884;&#20837;&#26469;&#20943;&#23567;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#20026;&#20102;&#20351;&#20272;&#35745;&#22120;&#26356;&#20934;&#30830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIPS&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#8212;&#8212;&#36793;&#38469;&#21270;&#21452;&#37325;&#31283;&#20581;&#65288;MDR&#65289;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#22312;&#27604;MIPS&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#26159;&#26080;&#20559;&#30340;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;IPS&#30340;&#26041;&#24046;&#20943;&#23567;&#65292;&#36825;&#26159;MIPS&#30340;&#20027;&#35201;&#20248;&#21183;&#12290;&#32463;&#39564;&#23454;&#39564;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#19968;&#33268;&#22686;&#24378;&#30340;Siamese&#32593;&#32476;&#29992;&#20110;&#21452;&#35270;&#22270;&#23545;&#24212;&#23398;&#20064;&#65292;&#36890;&#36807;&#22686;&#24378;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#24378;&#21270;&#20869;&#28857;&#30456;&#20851;&#24615;&#12289;&#25233;&#21046;&#24322;&#24120;&#28857;&#30340;&#24178;&#25200;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#21644;&#21453;&#21521;&#26144;&#23556;&#30340;&#20449;&#24687;&#26469;&#30417;&#30563;&#32593;&#32476;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.03217</link><description>&lt;p&gt;
&#26412;&#22320;&#19968;&#33268;&#22686;&#24378;&#30340;Siamese&#32593;&#32476;&#19982;&#24490;&#29615;&#25439;&#22833;&#29992;&#20110;&#21452;&#35270;&#22270;&#23545;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning. (arXiv:2308.03217v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#19968;&#33268;&#22686;&#24378;&#30340;Siamese&#32593;&#32476;&#29992;&#20110;&#21452;&#35270;&#22270;&#23545;&#24212;&#23398;&#20064;&#65292;&#36890;&#36807;&#22686;&#24378;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#24378;&#21270;&#20869;&#28857;&#30456;&#20851;&#24615;&#12289;&#25233;&#21046;&#24322;&#24120;&#28857;&#30340;&#24178;&#25200;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#21644;&#21453;&#21521;&#26144;&#23556;&#30340;&#20449;&#24687;&#26469;&#30417;&#30563;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21452;&#35270;&#22270;&#23545;&#24212;&#23398;&#20064;&#30740;&#31350;&#36890;&#24120;&#24314;&#31435;&#19968;&#20010;&#31471;&#21040;&#31471;&#32593;&#32476;&#26469;&#20849;&#21516;&#39044;&#27979;&#23545;&#24212;&#21487;&#38752;&#24615;&#21644;&#30456;&#23545;&#23039;&#24577;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#26041;&#38754;&#25913;&#36827;&#20102;&#36825;&#26679;&#19968;&#20010;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#29305;&#24449;&#19968;&#33268;&#24615;&#65288;LFC&#65289;&#25554;&#20214;&#22359;&#26469;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#32473;&#23450;&#19968;&#20010;&#23545;&#24212;&#29305;&#24449;&#65292;&#35813;&#22359;&#36890;&#36807;&#30456;&#20114;&#37051;&#22495;&#19968;&#33268;&#24615;&#22686;&#24378;&#20854;&#30456;&#37051;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#21512;&#20197;&#20135;&#29983;&#22686;&#24378;&#29305;&#24449;&#12290;&#30001;&#20110;&#20869;&#28857;&#26381;&#20174;&#32479;&#19968;&#30340;&#36328;&#35270;&#22270;&#21464;&#25442;&#65292;&#24182;&#19988;&#20849;&#20139;&#27604;&#24322;&#24120;&#28857;&#26356;&#19968;&#33268;&#30340;&#23398;&#20064;&#29305;&#24449;&#65292;&#29305;&#24449;&#19968;&#33268;&#24615;&#22686;&#24378;&#20102;&#20869;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25233;&#21046;&#20102;&#24322;&#24120;&#28857;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#20351;&#36755;&#20986;&#29305;&#24449;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#31867;&#20869;&#28857;/&#24322;&#24120;&#28857;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#32593;&#32476;&#35757;&#32451;&#20013;&#20351;&#29992;&#22320;&#38754;&#30495;&#23454;&#23545;&#24212;&#21644;&#26412;&#36136;&#30697;&#38453;&#23558;&#19968;&#20010;&#22270;&#20687;&#25237;&#24433;&#21040;&#21478;&#19968;&#20010;&#22270;&#20687;&#30340;&#36755;&#20837;&#22270;&#20687;&#23545;&#65292;&#32780;&#19981;&#32771;&#34385;&#21453;&#21521;&#26144;&#23556;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25193;&#23637;&#29616;&#26377;&#30340;&#36884;&#24452;&#26469;&#21516;&#26102;&#21033;&#29992;&#27491;&#21521;&#21644;&#21453;&#21521;&#26144;&#23556;&#30340;&#20449;&#24687;&#26469;&#30417;&#30563;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies of two-view correspondence learning usually establish an end-to-end network to jointly predict correspondence reliability and relative pose. We improve such a framework from two aspects. First, we propose a Local Feature Consensus (LFC) plugin block to augment the features of existing models. Given a correspondence feature, the block augments its neighboring features with mutual neighborhood consensus and aggregates them to produce an enhanced feature. As inliers obey a uniform cross-view transformation and share more consistent learned features than outliers, feature consensus strengthens inlier correlation and suppresses outlier distraction, which makes output features more discriminative for classifying inliers/outliers. Second, existing approaches supervise network training with the ground truth correspondences and essential matrix projecting one image to the other for an input image pair, without considering the information from the reverse mapping. We extend existi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#21442;&#25968;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TPCNN&#65289;&#65292;&#36890;&#36807;&#26102;&#38388;&#26174;&#24335;&#21021;&#22987;&#21270;&#30340;&#26680;&#20989;&#25968;&#26469;&#21442;&#25968;&#21270;&#21367;&#31215;&#23618;&#65292;&#20197;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#36825;&#19968;&#26041;&#27861;&#22686;&#24378;&#20102;&#23545;&#36830;&#32493;&#26102;&#38388;&#38544;&#34255;&#21160;&#24577;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.03210</link><description>&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#21442;&#25968;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series. (arXiv:2308.03210v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#21442;&#25968;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TPCNN&#65289;&#65292;&#36890;&#36807;&#26102;&#38388;&#26174;&#24335;&#21021;&#22987;&#21270;&#30340;&#26680;&#20989;&#25968;&#26469;&#21442;&#25968;&#21270;&#21367;&#31215;&#23618;&#65292;&#20197;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#36825;&#19968;&#26041;&#27861;&#22686;&#24378;&#20102;&#23545;&#36830;&#32493;&#26102;&#38388;&#38544;&#34255;&#21160;&#24577;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#23548;&#33268;&#35266;&#27979;&#20540;&#31232;&#30095;&#12289;&#19981;&#23436;&#20840;&#19988;&#19981;&#23545;&#40784;&#12290;&#26631;&#20934;&#30340;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#32771;&#34385;&#20102;&#35266;&#27979;&#26102;&#38388;&#20043;&#38388;&#30340;&#27491;&#21017;&#38388;&#38553;&#65292;&#23545;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#25552;&#20986;&#30340;&#26550;&#26500;&#37117;&#37319;&#29992;RNN&#21464;&#20307;&#26469;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#65292;&#20294;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#35268;&#21017;&#37319;&#26679;&#29615;&#22659;&#19979;&#30340;&#30740;&#31350;&#36824;&#19981;&#20805;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#26174;&#24335;&#21021;&#22987;&#21270;&#30340;&#26680;&#20989;&#25968;&#26469;&#21442;&#25968;&#21270;&#21367;&#31215;&#23618;&#12290;&#26102;&#38388;&#30340;&#36825;&#31181;&#36890;&#29992;&#20989;&#25968;&#22686;&#24378;&#20102;&#36830;&#32493;&#26102;&#38388;&#38544;&#34255;&#21160;&#24577;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#22320;&#21512;&#24182;&#21040;&#21367;&#31215;&#26680;&#26435;&#37325;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#21442;&#25968;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TPCNN&#65289;&#65292;&#23427;&#20849;&#20139;&#20102;&#30456;&#21516;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Irregularly sampled multivariate time series are ubiquitous in several application domains, leading to sparse, not fully-observed and non-aligned observations across different variables. Standard sequential neural network architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), consider regular spacing between observation times, posing significant challenges to irregular time series modeling. While most of the proposed architectures incorporate RNN variants to handle irregular time intervals, convolutional neural networks have not been adequately studied in the irregular sampling setting. In this paper, we parameterize convolutional layers by employing time-explicitly initialized kernels. Such general functions of time enhance the learning process of continuous-time hidden dynamics and can be efficiently incorporated into convolutional kernel weights. We, thus, propose the time-parameterized convolutional neural network (TPCNN), which shares sim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02582</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#29702;&#20986;&#23569;&#37327;&#26679;&#26412;&#65292;&#20197;&#21512;&#25104;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#27979;&#35797;&#26597;&#35810;&#30340;&#36816;&#34892;&#26102;&#25552;&#31034;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#23436;&#20840;&#35206;&#30422;SQL&#23376;&#21477;&#12289;&#36816;&#31639;&#31526;&#21644;&#20989;&#25968;&#65292;&#24182;&#22312;&#20801;&#35768;&#30340;&#20196;&#29260;&#38271;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#22823;&#39046;&#22495;&#35206;&#30422;&#12290;&#36825;&#26679;&#21487;&#20197;&#21512;&#25104;&#19968;&#20010;&#22266;&#23450;&#30340;&#36890;&#29992;&#25552;&#31034;&#65288;GP&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;NL&#27979;&#35797;&#26597;&#35810;&#20043;&#38388;&#20849;&#29992;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#38598;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;GP&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#24211;&#39046;&#22495;&#65288;DA-GP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#27867;&#21270;&#65307;&#28982;&#21518;&#37319;&#29992;&#20998;&#35299;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#65288;LTMP-DA-GP&#65289;&#26469;&#22788;&#29702;&#36328;&#32452;&#21512;&#27867;&#21270;&#12290;LTMP-DA-GP&#30340;&#21512;&#25104;&#26159;&#31163;&#32447;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
&lt;/p&gt;</description></item><item><title>AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.02182</link><description>&lt;p&gt;
AutoML4ETC: &#33258;&#21160;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23454;&#29616;&#29616;&#23454;&#19990;&#30028;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification. (arXiv:2308.02182v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02182
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;DL&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#38543;&#26102;&#38388;&#19981;&#21487;&#36991;&#20813;&#22320;&#19979;&#38477;&#12290;&#20165;&#20165;&#23545;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#21482;&#33021;&#37096;&#20998;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25163;&#21160;&#35843;&#25972;&#27169;&#22411;&#26550;&#26500;&#20197;&#28385;&#36275;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26399;&#26395;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;AutoML4ETC&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#36827;&#34892;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#24378;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#19987;&#38376;&#38024;&#23545;&#20351;&#29992;&#25968;&#25454;&#21253;&#22836;&#23383;&#33410;&#36827;&#34892;&#36817;&#23454;&#26102;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#25628;&#32034;&#31354;&#38388;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AutoML4ETC&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#30340;&#29305;&#28857;&#65292;&#24076;&#26395;&#23558;&#23545;&#31216;&#24615;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#25512;&#24191;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#24212;&#29992;&#20110;ConvNet&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.01621</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#30340;&#29305;&#28857;&#65292;&#24076;&#26395;&#23558;&#23545;&#31216;&#24615;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#25512;&#24191;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#24212;&#29992;&#20110;ConvNet&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNet)&#26550;&#26500;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#19968;&#31867;&#31216;&#20026;&#25311;&#32447;&#24615;&#21452;&#26354;&#31995;&#32479;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23427;&#20801;&#35768;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#12290;&#36825;&#26159;&#19982;&#20256;&#32479;&#27169;&#22411;&#20013;&#22522;&#26412;&#22266;&#23450;&#30340;&#26550;&#26500;&#21644;&#26435;&#37325;&#30456;&#27604;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;&#25105;&#20204;&#24076;&#26395;&#23558;(&#20869;&#37096;)&#23545;&#31216;&#24615;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#21560;&#24341;&#23545;PDE&#35270;&#35282;&#20998;&#26512;&#21644;&#35299;&#37322;ConvNet&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.01097</link><description>&lt;p&gt;
&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#30001;&#20110;&#26410;&#26469;&#23039;&#21183;&#30340;&#38543;&#26426;&#21644;&#19981;&#35268;&#21017;&#24615;&#36136;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24448;&#24448;&#38590;&#20197;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#30340;&#26102;&#31354;&#34920;&#31034;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#24573;&#35270;&#36816;&#21160;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#39592;&#26550;&#33410;&#28857;&#30340;&#26102;&#22495;&#21644;&#31354;&#22495;&#20381;&#36182;&#24615;&#26159;&#19981;&#21516;&#30340;&#12290;&#26102;&#22495;&#20851;&#31995;&#25429;&#25417;&#21040;&#38543;&#26102;&#38388;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#31354;&#22495;&#20851;&#31995;&#25551;&#36848;&#20102;&#36523;&#20307;&#32467;&#26500;&#21644;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#22686;&#37327;&#20449;&#24687;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#23427;&#35299;&#32806;&#20102;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#20102;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01011</link><description>&lt;p&gt;
&#20351;&#29992;Floss&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#39057;&#22495;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26159;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#24448;&#24448;&#19981;&#33021;&#34987;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20805;&#20998;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;&#23545;&#24863;&#20852;&#36259;&#30340;&#22522;&#30784;&#21160;&#24577;&#34892;&#20026;&#30340;&#34920;&#31034;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21483;&#20570;Floss&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#22320;&#22312;&#39057;&#22495;&#19978;&#35843;&#25972;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;Floss&#26041;&#27861;&#39318;&#20808;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20027;&#35201;&#21608;&#26399;&#24615;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#21608;&#26399;&#31227;&#20301;&#21644;&#35889;&#23494;&#24230;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;Floss&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#26377;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25512;&#23548;&#20986;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#26377;&#38480;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#26680;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31934;&#30830;&#26680;&#23545;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00824</link><description>&lt;p&gt;
&#26377;&#38480;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#26680;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
An Exact Kernel Equivalence for Finite Classification Models. (arXiv:2308.00824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25512;&#23548;&#20986;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#26377;&#38480;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#26680;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31934;&#30830;&#26680;&#23545;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25512;&#23548;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20219;&#24847;&#26377;&#38480;&#22823;&#23567;&#21442;&#25968;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#34920;&#31034;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31934;&#30830;&#34920;&#31034;&#19982;&#33879;&#21517;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#30456;&#23545;&#20110;NTK&#21644;&#20854;&#20182;&#38750;&#31934;&#30830;&#36335;&#24452;&#26680;&#20844;&#24335;&#30340;&#36817;&#20284;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#36924;&#30495;&#32593;&#32476;&#30340;&#26680;&#21040;&#26426;&#22120;&#31934;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#31934;&#30830;&#26680;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22914;&#20309;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#25552;&#20379;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#30340;&#27867;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the equivalence between neural networks and kernel methods by deriving the first exact representation of any finite-size parametric classification model trained with gradient descent as a kernel machine. We compare our exact representation to the well-known Neural Tangent Kernel (NTK) and discuss approximation error relative to the NTK and other non-exact path kernel formulations. We experimentally demonstrate that the kernel can be computed for realistic networks up to machine precision. We use this exact kernel to show that our theoretical contribution can provide useful insights into the predictions made by neural networks, particularly the way in which they generalize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#20808;&#39564;&#30693;&#35782;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#38598;&#25104;&#21040;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#19988;&#27867;&#21270;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.16186</link><description>&lt;p&gt;
ESP:&#21033;&#29992;&#23545;&#31216;&#24615;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#22810;Agent&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning. (arXiv:2307.16186v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#20808;&#39564;&#30693;&#35782;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#38598;&#25104;&#21040;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#19988;&#27867;&#21270;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#35201;&#27714;&#26500;&#24314;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22312;&#24403;&#21069;&#30340;MARL&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#21463;&#22810;Agent&#31995;&#32479;&#20013;&#23545;&#31216;&#29616;&#35937;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#22686;&#24378;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#19968;&#33268;&#24615;&#25439;&#22833;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;MARL&#26041;&#27861;&#20013;&#65292;&#26469;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;MARL&#31639;&#27861;&#12290;&#23545;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#19968;&#20010;&#29289;&#29702;&#22810;&#26426;&#22120;&#20154;&#23454;&#39564;&#24179;&#21488;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
&lt;/p&gt;</description></item><item><title>Auto-Tables&#31995;&#32479;&#33021;&#33258;&#21160;&#21512;&#25104;&#22810;&#27493;&#36716;&#25442;&#30340;&#27969;&#27700;&#32447;&#65292;&#23558;&#38750;&#20851;&#31995;&#24335;&#34920;&#26684;&#36716;&#25442;&#20026;&#20851;&#31995;&#24335;&#34920;&#26684;&#65292;&#35299;&#20915;&#20102;&#38750;&#25216;&#26415;&#29992;&#25143;&#20351;&#29992;SQL&#20998;&#26512;&#24037;&#20855;&#30340;&#30171;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.14565</link><description>&lt;p&gt;
Auto-Tables: &#26080;&#38656;&#20351;&#29992;&#31034;&#20363;&#21512;&#25104;&#22810;&#27493;&#36716;&#25442;&#20197;&#20351;&#34920;&#26684;&#20851;&#31995;&#21270;
&lt;/p&gt;
&lt;p&gt;
Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples. (arXiv:2307.14565v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14565
&lt;/p&gt;
&lt;p&gt;
Auto-Tables&#31995;&#32479;&#33021;&#33258;&#21160;&#21512;&#25104;&#22810;&#27493;&#36716;&#25442;&#30340;&#27969;&#27700;&#32447;&#65292;&#23558;&#38750;&#20851;&#31995;&#24335;&#34920;&#26684;&#36716;&#25442;&#20026;&#20851;&#31995;&#24335;&#34920;&#26684;&#65292;&#35299;&#20915;&#20102;&#38750;&#25216;&#26415;&#29992;&#25143;&#20351;&#29992;SQL&#20998;&#26512;&#24037;&#20855;&#30340;&#30171;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#34920;&#26684;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#26159;&#26631;&#20934;&#30340;&#65292;&#27599;&#19968;&#34892;&#23545;&#24212;&#19968;&#20010;&#23454;&#20307;&#65292;&#27599;&#19968;&#21015;&#23545;&#24212;&#19968;&#20010;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;"&#37326;&#29983;"&#34920;&#26684;&#26102;&#65292;&#36825;&#26679;&#30340;&#26631;&#20934;&#26080;&#27861;&#20445;&#35777;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#30495;&#23454;&#30340;&#30005;&#23376;&#34920;&#26684;&#21644;&#32593;&#39029;&#34920;&#26684;&#65292;&#21457;&#29616;&#36229;&#36807;30%&#30340;&#34920;&#26684;&#19981;&#31526;&#21512;&#20851;&#31995;&#26631;&#20934;&#65292;&#36825;&#23601;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#30340;&#34920;&#26684;&#37325;&#26500;&#36716;&#25442;&#65292;&#25165;&#33021;&#36731;&#26494;&#22320;&#20351;&#29992;&#22522;&#20110;SQL&#30340;&#20998;&#26512;&#24037;&#20855;&#36827;&#34892;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#25152;&#38656;&#30340;&#36716;&#25442;&#32534;&#31243;&#24182;&#19981;&#31616;&#21333;&#65292;&#36825;&#24050;&#32463;&#25104;&#20026;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#29992;&#25143;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#20174;StackOverflow&#21644;Excel/Tableau&#35770;&#22363;&#30340;&#22823;&#37327;&#38382;&#39064;&#21487;&#20197;&#35777;&#26126;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;Auto-Tables&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21512;&#25104;&#20855;&#26377;&#22810;&#27493;&#36716;&#25442;&#65288;&#20351;&#29992;Python&#25110;&#20854;&#20182;&#35821;&#35328;&#65289;&#30340;&#27969;&#27700;&#32447;&#65292;&#23558;&#38750;&#20851;&#31995;&#22411;&#34920;&#36716;&#25442;&#20026;&#26631;&#20934;&#30340;&#20851;&#31995;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Tableau forums.  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;Dikin&#27493;&#34892;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#24182;&#36866;&#24212;&#19968;&#33324;&#24230;&#37327;&#65292;&#20026;&#24102;&#32422;&#26463;&#30340;PSD&#38181;&#20307;&#25277;&#26679;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#30697;&#38453;&#20989;&#25968;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.12943</link><description>&lt;p&gt;
&#29992;&#24230;&#37327;&#30340;Dikin&#27493;&#39588;&#26377;&#25928;&#22320;&#25277;&#26679;PSD&#38181;&#20307;
&lt;/p&gt;
&lt;p&gt;
Efficiently Sampling the PSD Cone with the Metric Dikin Walk. (arXiv:2307.12943v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;Dikin&#27493;&#34892;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#24182;&#36866;&#24212;&#19968;&#33324;&#24230;&#37327;&#65292;&#20026;&#24102;&#32422;&#26463;&#30340;PSD&#38181;&#20307;&#25277;&#26679;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#30697;&#38453;&#20989;&#25968;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#23450;&#35268;&#21010;&#20195;&#34920;&#20102;&#39640;&#25928;&#35745;&#31639;&#30340;&#21069;&#27839;&#12290;&#23613;&#31649;&#22312;&#21322;&#23450;&#26368;&#20248;&#21270;&#19978;&#24050;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#22914;&#20170;&#20869;&#28857;&#27861;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#35299;&#20915;&#20013;&#31561;&#35268;&#27169;&#30340;&#38382;&#39064;&#65292;&#20294;&#26159;&#25277;&#26679;&#21322;&#23450;&#35299;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#30452;&#25509;&#24212;&#29992;&#24050;&#30693;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#25277;&#26679;&#19968;&#33324;&#20984;&#20307;&#30340;&#26041;&#27861;&#23548;&#33268;&#36816;&#34892;&#26102;&#38388;&#36807;&#38271;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#30340;&#36890;&#29992;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#33293;&#20837;&#38454;&#27573;&#20316;&#20026;&#39044;&#22788;&#29702;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;Dikin&#27493;&#34892;&#65292;&#24182;&#39318;&#20808;&#23558;&#20854;&#36866;&#24212;&#20110;&#19968;&#33324;&#24230;&#37327;&#65292;&#28982;&#21518;&#20026;&#24102;&#26377;&#20223;&#23556;&#32422;&#26463;&#30340;PSD&#38181;&#20307;&#35774;&#35745;&#21512;&#36866;&#30340;&#24230;&#37327;&#12290;&#25152;&#24471;&#21040;&#30340;&#28151;&#21512;&#26102;&#38388;&#21644;&#27599;&#27493;&#22797;&#26434;&#24230;&#30456;&#24403;&#23567;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#24230;&#37327;&#65292;&#21487;&#20197;&#20351;&#20854;&#23545;&#32422;&#26463;&#30340;&#20381;&#36182;&#20851;&#31995;&#21464;&#20026;&#22810;&#23545;&#25968;&#32423;&#30340;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#30697;&#38453;&#20989;&#25968;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#32452;&#21512;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-definite programs represent a frontier of efficient computation. While there has been much progress on semi-definite optimization, with moderate-sized instances currently solvable in practice by the interior-point method, the basic problem of sampling semi-definite solutions remains a formidable challenge. The direct application of known polynomial-time algorithms for sampling general convex bodies to semi-definite sampling leads to a prohibitively high running time. In addition, known general methods require an expensive rounding phase as pre-processing. Here we analyze the Dikin walk, by first adapting it to general metrics, then devising suitable metrics for the PSD cone with affine constraints. The resulting mixing time and per-step complexity are considerably smaller, and by an appropriate choice of the metric, the dependence on the number of constraints can be made polylogarithmic. We introduce a refined notion of self-concordant matrix functions and give rules for combining
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WEPRO&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#26435;&#37325;&#20013;&#30340;&#35268;&#24459;&#36235;&#21183;&#21152;&#36895;&#20102;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#30456;&#27604;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#65292;&#36895;&#24230;&#25552;&#39640;&#32422;2.25&#20493;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2.3%&#65292;&#25439;&#22833;&#38477;&#20302;&#20102;6.1%&#12290;</title><link>http://arxiv.org/abs/2307.12449</link><description>&lt;p&gt;
WEPRO: &#29992;&#20110;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#39640;&#25928;&#20248;&#21270;&#30340;&#26435;&#37325;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms. (arXiv:2307.12449v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WEPRO&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#26435;&#37325;&#20013;&#30340;&#35268;&#24459;&#36235;&#21183;&#21152;&#36895;&#20102;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#30456;&#27604;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#65292;&#36895;&#24230;&#25552;&#39640;&#32422;2.25&#20493;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2.3%&#65292;&#25439;&#22833;&#38477;&#20302;&#20102;6.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#27169;&#25311;&#22120;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#30340;&#25351;&#25968;&#32423;&#36816;&#34892;&#26102;&#38388;&#12289;&#30495;&#23454;&#37327;&#23376;&#35774;&#22791;&#30340;&#38271;&#38431;&#21015;&#28145;&#24230;&#21644;&#39640;&#25104;&#26412;&#32473;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;(VQA)&#22914;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(QNNs)&#12289;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;(VQE)&#21644;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;(QAOA)&#30340;&#26377;&#25928;&#35757;&#32451;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;WEPRO(&#26435;&#37325;&#39044;&#27979;)&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#26435;&#37325;&#20013;&#30340;&#35268;&#24459;&#36235;&#21183;&#26469;&#21152;&#24555;VQA&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#20248;&#21270;&#39044;&#27979;&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#21363;Naive Prediction(NaP)&#21644;Adaptive Prediction(AdaP)&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22810;&#20010;QNN&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#39564;&#21644;&#35757;&#32451;&#65292;&#25105;&#20204;&#35777;&#26126;WEPRO&#30456;&#23545;&#20110;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#21152;&#24555;&#20102;&#22823;&#32422;2.25&#20493;&#30340;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;(&#39640;&#36798;2.3%)&#21644;&#26356;&#20302;&#30340;&#25439;&#22833;(&#39640;&#36798;6.1%)&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential run time of quantum simulators on classical machines and long queue depths and high costs of real quantum devices present significant challenges in the effective training of Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA). To address these limitations, we propose a new approach, WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance namely, Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately $2.25\times$ compared to standard training methods, while also providing improved accuracy (up to $2.3\%$ higher) and loss (up to $6.1\%$ lower) with low storage and computational over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12306</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#35781;&#21650;(CoD)&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#26497;&#24230;&#31246;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#22312;&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#38754;&#20020;&#26497;&#22823;&#25361;&#25112;&#65292;&#27491;&#22914;Richard Bellman&#22312;60&#24180;&#21069;&#39318;&#27425;&#25351;&#20986;&#30340;&#37027;&#26679;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#39640;&#32500;&#24230;&#19978;&#25968;&#20540;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#36825;&#26679;&#30340;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#65292;&#32780;&#23558;&#19968;&#33324;&#38750;&#32447;&#24615;PDEs&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#20174;&#26410;&#23454;&#29616;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25193;&#23637;&#21040;&#35299;&#20915;&#20219;&#24847;&#39640;&#32500;PDEs&#12290;&#35813;&#26032;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#32500;&#24230;&#26799;&#24230;&#19979;&#38477;(SDGD)&#65292;&#23558;PDE&#30340;&#26799;&#24230;&#20998;&#35299;&#20026;&#19982;&#19981;&#21516;&#32500;&#24230;&#23545;&#24212;&#30340;&#37096;&#20998;&#65292;&#24182;&#22312;&#35757;&#32451;PINNs&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#36825;&#20123;&#32500;&#24230;&#37096;&#20998;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
&lt;/p&gt;</description></item><item><title>MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.10763</link><description>&lt;p&gt;
MSQNet: &#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10763
&lt;/p&gt;
&lt;p&gt;
MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#65292;&#22240;&#20026;&#28436;&#21592;&#20043;&#38388;&#20855;&#26377;&#22266;&#26377;&#30340;&#25299;&#25169;&#21644;&#26174;&#30528;&#24046;&#24322;&#12290;&#36825;&#23601;&#38656;&#35201;&#29305;&#23450;&#28436;&#21592;&#30340;&#23039;&#24577;&#20272;&#35745;&#65288;&#20363;&#22914;&#20154;&#31867;&#19982;&#21160;&#29289;&#65289;&#65292;&#23548;&#33268;&#27169;&#22411;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#21482;&#20851;&#27880;&#23398;&#20064;&#35270;&#35273;&#27169;&#24577;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#21487;&#29992;&#20449;&#24687;&#28304;&#65288;&#20363;&#22914;&#31867;&#21517;&#25991;&#26412;&#65289;&#21644;&#22810;&#20010;&#21160;&#20316;&#30340;&#21516;&#26102;&#21457;&#29983;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#8221;&#65292;&#20026;&#21253;&#25324;&#20154;&#31867;&#21644;&#21160;&#29289;&#22312;&#20869;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#28436;&#21592;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65288;&#20363;&#22914;DETR&#65289;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#26597;&#35810;&#32593;&#32476;&#65288;MSQNet&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#12290;&#28040;&#38500;&#20102;&#28436;&#21592;&#29305;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-spec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08873</link><description>&lt;p&gt;
&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#26367;&#20195;&#65306;&#22522;&#23612;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39118;&#38505;&#21388;&#24694;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#38480;&#21046;&#31574;&#30053;&#22238;&#25253;&#30340;&#26041;&#24046;&#26159;&#19968;&#31181;&#24120;&#35265;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26126;&#30830;&#30340;&#25968;&#23398;&#23450;&#20041;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#38480;&#21046;&#24635;&#22238;&#25253;&#26041;&#24046;&#65292;&#32780;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#27599;&#27493;&#22870;&#21169;&#26041;&#24046;&#20316;&#20026;&#20195;&#29702;&#12290;&#26412;&#25991;&#24443;&#24213;&#30740;&#31350;&#20102;&#36825;&#20123;&#22522;&#20110;&#26041;&#24046;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#25968;&#23383;&#23610;&#24230;&#30340;&#25935;&#24863;&#24615;&#21644;&#38459;&#30861;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26367;&#20195;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#8212;&#8212;&#22522;&#23612;&#31163;&#24046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#21270;&#22522;&#23612;&#31163;&#24046;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#22312;&#39118;&#38505;&#21388;&#24694;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#30340;&#39046;&#22495;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#22522;&#20110;&#26041;&#24046;&#30340;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#31574;&#30053;&#26080;&#27861;&#23398;&#21040;&#21512;&#29702;&#31574;&#30053;&#26102;&#23454;&#29616;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#65292;&#20197;&#26041;&#24046;&#21644;&#22522;&#23612;&#31163;&#24046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
&lt;/p&gt;</description></item><item><title>Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.08621</link><description>&lt;p&gt;
Retentive Network: &#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#30340;&#32487;&#20219;&#32773;
&lt;/p&gt;
&lt;p&gt;
Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08621
&lt;/p&gt;
&lt;p&gt;
Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Retentive Network (RetNet)&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20102;&#24490;&#29615;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#24314;&#27169;&#30340;&#20445;&#30041;&#26426;&#21046;&#65292;&#25903;&#25345;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;&#21363;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24182;&#34892;&#34920;&#31034;&#20801;&#35768;&#36827;&#34892;&#35757;&#32451;&#24182;&#34892;&#21270;&#12290;&#24490;&#29615;&#34920;&#31034;&#33021;&#22815;&#23454;&#29616;&#20302;&#25104;&#26412;&#30340;$O(1)$&#25512;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#35299;&#30721;&#21534;&#21520;&#37327;&#12289;&#24310;&#36831;&#21644;GPU&#20869;&#23384;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#20998;&#22359;&#24490;&#29615;&#34920;&#31034;&#20415;&#20110;&#20351;&#29992;&#32447;&#24615;&#22797;&#26434;&#24230;&#36827;&#34892;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#21487;&#20197;&#24182;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#36827;&#34892;&#24490;&#29615;&#25688;&#35201;&#12290;&#35821;&#35328;&#24314;&#27169;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RetNet&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25193;&#23637;&#32467;&#26524;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#20302;&#25104;&#26412;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient i
&lt;/p&gt;</description></item><item><title>INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08131</link><description>&lt;p&gt;
INFLECT-DGNN: &#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24433;&#21709;&#32773;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks. (arXiv:2307.08131v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08131
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#32593;&#32476;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#12290;&#22312;&#25512;&#33616;&#21644;&#23450;&#21521;&#33829;&#38144;&#39046;&#22495;&#20013;&#65292;&#24433;&#21709;&#32773;&#26816;&#27979;&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#21160;&#24577;&#32593;&#32476;&#34920;&#31034;&#22823;&#22823;&#21463;&#30410;&#30340;&#39046;&#22495;&#65292;&#21407;&#22240;&#26159;&#19981;&#26029;&#21457;&#23637;&#30340;&#23458;&#25143;-&#21697;&#29260;&#20851;&#31995;&#12290;&#20026;&#20102;&#38416;&#36848;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;INFLECT-DGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#20197;&#21450;&#31934;&#24515;&#35774;&#35745;&#30340;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;&#39044;&#27979;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#22478;&#24066;&#32593;&#32476;&#30340;&#29420;&#29305;&#20225;&#19994;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20197;&#21033;&#28070;&#20026;&#39537;&#21160;&#30340;&#24433;&#21709;&#32773;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#20197;&#21450;GNN&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive perform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.06713</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26080;&#30417;&#30563;&#26657;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#20808;&#39564;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26377;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#27491;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#22823;&#37327;&#26080;&#30417;&#30563;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#12289;&#26657;&#20934;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#26041;&#27861;&#36827;&#34892;&#36866;&#24212;&#20197;&#25191;&#34892;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#23558;LLM&#35270;&#20026;&#40657;&#30418;&#65292;&#22312;&#27169;&#22411;&#23631;&#38556;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#38454;&#27573;&#65292;&#29992;&#20110;&#26657;&#20934;&#27169;&#22411;&#21518;&#39564;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#25552;&#31034;&#35757;&#32451;&#26679;&#26412;&#21644;&#26080;&#36866;&#24212;&#25968;&#25454;&#19979;&#30340;&#26657;&#20934;&#26041;&#27861;&#20013;&#20248;&#20110;&#26410;&#36866;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.03759</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#25554;&#20540;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#26159;&#35760;&#24405;&#21160;&#24577;&#31995;&#32479;&#27979;&#37327;&#32467;&#26524;&#30340;&#20027;&#35201;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#29289;&#29702;&#20256;&#24863;&#22120;&#21644;&#22312;&#32447;&#36807;&#31243;&#65288;&#34394;&#25311;&#20256;&#24863;&#22120;&#65289;&#29983;&#25104;&#22823;&#37327;&#25968;&#25454;&#12290;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#25581;&#31034;&#21487;&#29992;&#25968;&#25454;&#20013;&#25152;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#20256;&#32479;&#21644;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21017;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;&#65288;GNN4TS&#65289;&#65292;&#21253;&#25324;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#25105;&#20204;&#26088;&#22312;&#25351;&#23548;&#35774;&#35745;&#24072;&#21644;&#23454;&#36341;&#32773;&#20102;&#35299;&#12289;&#26500;&#24314;&#24212;&#29992;&#21644;&#25512;&#21160;GNN4TS&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;GNN4TS&#20998;&#31867;&#20307;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. Approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: Forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#26469;&#20462;&#21098;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#22320;&#25214;&#21040;&#31232;&#30095;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#20855;&#26377;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03364</link><description>&lt;p&gt;
&#33976;&#39311;&#20462;&#21098;&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36194;&#24471;&#24425;&#31080;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distilled Pruning: Using Synthetic Data to Win the Lottery. (arXiv:2307.03364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#26469;&#20462;&#21098;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#22320;&#25214;&#21040;&#31232;&#30095;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#20855;&#26377;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#26469;&#20462;&#21098;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#20307;&#31995;&#32467;&#26500;&#25110;&#31639;&#27861;&#20248;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37325;&#26032;&#32771;&#34385;&#20102;&#25968;&#25454;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#20316;&#29992;&#12290;&#33976;&#39311;&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#26356;&#22823;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20462;&#21098;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;CIFAR-10&#19978;&#27604;&#36845;&#20195;&#24133;&#20540;&#20462;&#21098;&#26356;&#24555;&#22320;&#25214;&#21040;&#31232;&#30095;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65288;&#20063;&#31216;&#20026;&#24425;&#31080;&#31080;&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#36827;&#34892;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#12289;&#27169;&#22411;&#21387;&#32553;&#21644;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach to pruning deep learning models by using distilled data. Unlike conventional strategies which primarily focus on architectural or algorithmic optimization, our method reconsiders the role of data in these scenarios. Distilled datasets capture essential patterns from larger datasets, and we demonstrate how to leverage this capability to enable a computationally efficient pruning process. Our approach can find sparse, trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results highlight the potential of using distilled data for resource-efficient neural network pruning, model compression, and neural architecture search.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#35782;&#21035;&#38598;&#20307;&#21464;&#37327;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#24494;&#25200;&#29983;&#25104;&#22120;&#21644;&#36716;&#31227;&#31639;&#31526;&#30340;&#20027;&#23548;&#29305;&#24449;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#38598;&#20307;&#21464;&#37327;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#30340;&#25968;&#20540;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.00365</link><description>&lt;p&gt;
&#29702;&#35299;&#29992;&#20110;&#35782;&#21035;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Understanding recent deep-learning techniques for identifying collective variables of molecular dynamics. (arXiv:2307.00365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00365
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#35782;&#21035;&#38598;&#20307;&#21464;&#37327;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#24494;&#25200;&#29983;&#25104;&#22120;&#21644;&#36716;&#31227;&#31639;&#31526;&#30340;&#20027;&#23548;&#29305;&#24449;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#38598;&#20307;&#21464;&#37327;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#30340;&#25968;&#20540;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#24230;&#30340;&#20122;&#31283;&#24577;&#20998;&#23376;&#31995;&#32479;&#24120;&#24120;&#21487;&#20197;&#36890;&#36807;&#31995;&#32479;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#21363;&#38598;&#20307;&#21464;&#37327;&#65288;CVs&#65289;&#26469;&#25551;&#36848;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CV&#35782;&#21035;&#25216;&#26415;&#65292;&#21487;&#20197;&#23545;&#22797;&#26434;&#20998;&#23376;&#31995;&#32479;&#36827;&#34892;&#20934;&#30830;&#24314;&#27169;&#21644;&#39640;&#25928;&#20223;&#30495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#21035;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CV&#23547;&#25214;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#35745;&#31639;&#19982;&#24213;&#23618;&#21160;&#21147;&#23398;&#30456;&#20851;&#30340;&#24494;&#25200;&#29983;&#25104;&#22120;&#25110;&#36716;&#31227;&#31639;&#31526;&#30340;&#20027;&#23548;&#29305;&#24449;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#23398;&#20064;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#26469;&#23547;&#25214;&#38598;&#20307;&#21464;&#37327;&#12290;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#25968;&#23398;&#21407;&#29702;&#20570;&#20102;&#31616;&#35201;&#27010;&#36848;&#65292;&#24182;&#22312;&#20855;&#20307;&#31034;&#20363;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#30340;&#25968;&#20540;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional metastable molecular system can often be characterised by a few features of the system, i.e. collective variables (CVs). Thanks to the rapid advance in the area of machine learning and deep learning, various deep learning-based CV identification techniques have been developed in recent years, allowing accurate modelling and efficient simulation of complex molecular systems. In this paper, we look at two different categories of deep learning-based approaches for finding CVs, either by computing leading eigenfunctions of infinitesimal generator or transfer operator associated to the underlying dynamics, or by learning an autoencoder via minimisation of reconstruction error. We present a concise overview of the mathematics behind these two approaches and conduct a comparative numerical study of these two approaches on illustrative examples.
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09417</link><description>&lt;p&gt;
Diff-TTSG: &#21435;&#22122;&#27010;&#29575;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09417
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26391;&#35835;&#35821;&#38899;&#21512;&#25104;&#23454;&#29616;&#39640;&#33258;&#28982;&#24230;&#35780;&#20998;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#21512;&#25104;&#33258;&#28982;&#35328;&#35821;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#38754;&#23545;&#38754;&#30340;&#33258;&#21457;&#23545;&#35805;&#26082;&#26377;&#21475;&#22836;&#30340;&#65292;&#20063;&#26377;&#38750;&#35821;&#35328;&#30340;&#65288;&#20363;&#22914;&#65292;&#20849;&#21516;&#35328;&#35821;&#25163;&#21183;&#65289;&#12290;&#26368;&#36817;&#25165;&#24320;&#22987;&#30740;&#31350;&#32852;&#21512;&#21512;&#25104;&#36825;&#20004;&#31181;&#27169;&#24577;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#31995;&#32479;&#20013;&#30340;&#22909;&#22788;&#12290;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20351;&#29992;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#30340;&#20266;&#24433;&#21644;&#27425;&#20248;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026; Diff-TTSG&#65292;&#20849;&#21516;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#32452;&#23567;&#24515;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#20027;&#35266;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#31995;&#32479;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#21512;&#25104;&#30340;&#26679;&#20363;&#32780;&#35328;&#65292;Diff-TTSG&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
With read-aloud speech synthesis achieving high naturalness scores, there is a growing research interest in synthesising spontaneous speech. However, human spontaneous face-to-face conversation has both spoken and non-verbal aspects (here, co-speech gestures). Only recently has research begun to explore the benefits of jointly synthesising these two modalities in a single system. The previous state of the art used non-probabilistic methods, which fail to capture the variability of human speech and motion, and risk producing oversmoothing artefacts and sub-optimal synthesis quality. We present the first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together. Our method can be trained on small datasets from scratch. Furthermore, we describe a set of careful uni- and multi-modal subjective tests for evaluating integrated speech and gesture synthesis systems, and use them to validate our proposed approach. For synthesised examp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2306.01951</link><description>&lt;p&gt;
GAD-NR: &#36890;&#36807;&#37051;&#22495;&#37325;&#26500;&#23454;&#29616;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#31038;&#20132;&#23186;&#20307;&#22403;&#22334;&#26816;&#27979;&#21644;&#20854;&#20182;&#21508;&#31181;&#39046;&#22495;&#20013;&#26377;&#24212;&#29992;&#12290;GAD&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAEs&#65289;&#65292;&#23427;&#23558;&#22270;&#24418;&#25968;&#25454;&#32534;&#30721;&#25104;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#34920;&#31034;&#26469;&#35780;&#20272;&#22270;&#24418;&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#20197;&#35782;&#21035;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#20027;&#35201;&#38024;&#23545;&#30452;&#25509;&#38142;&#25509;&#37325;&#26500;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36830;&#25509;&#22270;&#20013;&#30340;&#33410;&#28857;&#34987;&#32858;&#31867;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#25797;&#38271;&#26816;&#27979;&#32858;&#31867;&#22411;&#32467;&#26500;&#24322;&#24120;&#65292;&#20294;&#23545;&#19981;&#31526;&#21512;&#32858;&#31867;&#30340;&#26356;&#22797;&#26434;&#30340;&#32467;&#26500;&#24322;&#24120;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;GAD-NR&#65292;&#23427;&#26159;GAE&#30340;&#19968;&#20010;&#26032;&#21464;&#20307;&#65292;&#34701;&#21512;&#37051;&#22495;&#37325;&#26500;&#36827;&#34892;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#12290;GAD-NR&#30340;&#30446;&#26631;&#26159;&#37325;&#26500;&#33410;&#28857;&#30340;&#25972;&#20010;&#37051;&#22495;&#65292;&#28085;&#30422;&#26412;&#22320;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#20462;&#34917;&#25216;&#26415;&#20174;&#26377;&#30149;&#21464;&#30340;&#33041;&#37096;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#31639;&#27861;&#26080;&#27861;&#20998;&#26512;&#30149;&#21464;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08992</link><description>&lt;p&gt;
&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#36187;2023&#65306;&#36890;&#36807;&#20462;&#22797;&#29983;&#25104;&#20581;&#24247;&#33041;&#32452;&#32455;&#30340;&#23616;&#37096;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting. (arXiv:2305.08992v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#20462;&#34917;&#25216;&#26415;&#20174;&#26377;&#30149;&#21464;&#30340;&#33041;&#37096;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#31639;&#27861;&#26080;&#27861;&#20998;&#26512;&#30149;&#21464;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#65292;&#25552;&#20379;&#20102;&#35768;&#22810;&#33258;&#21160;&#20998;&#26512;&#33041;&#37096;MR&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#33041;&#32959;&#30244;&#24739;&#32773;&#65292;&#22270;&#20687;&#37319;&#38598;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22987;&#20110;&#24050;&#32463;&#30149;&#29702;&#24615;&#30340;&#25195;&#25551;&#12290;&#36825;&#20250;&#24102;&#26469;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#31639;&#27861;&#26159;&#35774;&#35745;&#29992;&#20110;&#20998;&#26512;&#20581;&#24247;&#30340;&#22823;&#33041;&#22270;&#20687;&#65292;&#24182;&#19988;&#27809;&#26377;&#20026;&#21253;&#21547;&#30149;&#21464;&#30340;&#22270;&#20687;&#25552;&#20379;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#36827;&#34892;&#33041;&#37096;&#35299;&#21078;&#20998;&#21106;&#12289;&#32452;&#32455;&#20998;&#21106;&#21644;&#33041;&#37096;&#25552;&#21462;&#30340;&#31639;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#25506;&#32034;&#20462;&#22797;&#25216;&#26415;&#65292;&#20174;&#26377;&#30149;&#21464;&#30340;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#30340;&#33041;&#37096;&#25195;&#25551;&#12290;&#19979;&#38754;&#30340;&#25163;&#31295;&#21253;&#21547;&#20102;&#20219;&#21153;&#20844;&#24335;&#12289;&#25968;&#25454;&#38598;&#21644;&#25552;&#20132;&#31243;&#24207;&#12290;&#20043;&#21518;&#20250;&#26356;&#26032;&#20197;&#24635;&#32467;&#25361;&#25112;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#25361;&#25112;&#26159;&#20316;&#20026;BraTS 2023&#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#65292;&#30001;&#21152;&#25343;&#22823;&#28201;&#21733;&#21326;MICCAI 2023&#20250;&#35758;&#20027;&#21150;&#12290;
&lt;/p&gt;
&lt;p&gt;
A myriad of algorithms for the automatic analysis of brain MR images is available to support clinicians in their decision-making. For brain tumor patients, the image acquisition time series typically starts with a scan that is already pathological. This poses problems, as many algorithms are designed to analyze healthy brains and provide no guarantees for images featuring lesions. Examples include but are not limited to algorithms for brain anatomy parcellation, tissue segmentation, and brain extraction. To solve this dilemma, we introduce the BraTS 2023 inpainting challenge. Here, the participants' task is to explore inpainting techniques to synthesize healthy brain scans from lesioned ones. The following manuscript contains the task formulation, dataset, and submission procedure. Later it will be updated to summarize the findings of the challenge. The challenge is organized as part of the BraTS 2023 challenge hosted at the MICCAI 2023 conference in Vancouver, Canada.
&lt;/p&gt;</description></item><item><title>DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05738</link><description>&lt;p&gt;
DOCTOR&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05738
&lt;/p&gt;
&lt;p&gt;
DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#36793;&#32536;&#35774;&#22791;&#20013;&#30340;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#65288;WMS&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#26234;&#33021;&#21307;&#30103;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#25104;&#20026;&#21487;&#33021;&#12290;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#20026;&#27599;&#31181;&#30142;&#30149;&#21644;&#30456;&#24212;&#30340;WMS&#25968;&#25454;&#23450;&#21046;&#20010;&#21035;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26032;&#20219;&#21153;&#20998;&#31867;&#30340;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#27979;&#27599;&#20010;&#26032;&#30142;&#30149;&#65292;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#26500;&#24314;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;WMS&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;DOCTOR&#12290;&#23427;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#19968;&#31181;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;CL&#31639;&#27861;&#20351;&#24471;&#26694;&#26550;&#33021;&#22815;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12289;&#20998;&#31867;&#31867;&#21035;&#21644;&#30142;&#30149;&#26816;&#27979;&#20219;&#21153;&#12290;DOCTOR&#22312;&#20351;&#29992;&#26469;&#33258;&#23454;&#38469;WMS&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#22235;&#31181;&#24120;&#35265;&#30142;&#30149;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#65292;DOCTOR&#20063;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven disease detection methods rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. Also, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and an exemplar-replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AttentionViz&#65292;&#19968;&#31181;&#20197;&#32852;&#21512;&#23884;&#20837;&#20026;&#22522;&#30784;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#20840;&#23616;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25552;&#39640;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#24182;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#25552;&#20379;&#26032;&#30340;&#20132;&#20114;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.03210</link><description>&lt;p&gt;
AttentionViz&#65306;Transformer Attention&#30340;&#20840;&#23616;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AttentionViz&#65292;&#19968;&#31181;&#20197;&#32852;&#21512;&#23884;&#20837;&#20026;&#22522;&#30784;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#20840;&#23616;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25552;&#39640;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#24182;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#25552;&#20379;&#26032;&#30340;&#20132;&#20114;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#27491;&#22312;&#38761;&#26032;&#26426;&#22120;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#30340;&#20869;&#37096;&#36816;&#20316;&#20173;&#28982;&#31070;&#31192;&#33707;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24207;&#21015;&#20013;&#20803;&#32032;&#20043;&#38388;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21487;&#35270;&#21270;Transformer&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#27880;&#24847;&#21147;&#30340;&#26597;&#35810;&#21644;&#38190;&#21521;&#37327;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#19982;&#20197;&#21069;&#30340;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#20840;&#23616;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#32852;&#21512;&#26597;&#35810;-&#38190;&#23884;&#20837;&#21019;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;AttentionViz&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#30740;&#31350;&#35821;&#35328;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#36890;&#36807;&#20960;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#21644;&#25552;&#20379;&#26377;&#20851;&#26597;&#35810;-&#38190;&#20132;&#20114;&#30340;&#26032;&#35265;&#35299;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz, based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#38477;&#20302;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#36798;&#21040;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02396</link><description>&lt;p&gt;
&#29305;&#24449;&#24037;&#31243;&#33021;&#24110;&#21161;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Feature Engineering Help Quantum Machine Learning for Malware Detection?. (arXiv:2305.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#38477;&#20302;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#36798;&#21040;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#25968;&#37327;&#21644;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#35768;&#22810;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#27969;&#34892;ML&#27169;&#22411;&#37117;&#26159;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#20123;&#26377;&#30417;&#30563;&#20998;&#31867;&#22120;&#36890;&#24120;&#23545;&#26032;&#22411;&#24694;&#24847;&#36719;&#20214;&#30340;&#25512;&#24191;&#25928;&#26524;&#19981;&#22909;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#32463;&#24120;&#37325;&#26032;&#35757;&#32451;&#23427;&#20204;&#20197;&#26816;&#27979;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#37327;&#23376;ML&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#38477;&#20302;&#25968;&#25454;&#22823;&#23567;&#21644;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;XGBoost&#36873;&#25321;&#30340;&#29305;&#24449;&#30340;VQC&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#33719;&#24471;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;XGBoost&#36873;&#25321;&#30340;&#29305;&#24449;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;IBM 5 qubits&#26426;&#22120;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#20026;74&#65285;&#65288;+-11.35&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing number and sophistication of malware attacks, malware detection systems based on machine learning (ML) grow in importance. At the same time, many popular ML models used in malware classification are supervised solutions. These supervised classifiers often do not generalize well to novel malware. Therefore, they need to be re-trained frequently to detect new malware specimens, which can be time-consuming. Our work addresses this problem in a hybrid framework of theoretical Quantum ML, combined with feature selection strategies to reduce the data size and malware classifier training time. The preliminary results show that VQC with XGBoost selected features can get a 78.91% test accuracy on the simulator. The average accuracy for the model trained using the features selected with XGBoost was 74% (+- 11.35%) on the IBM 5 qubits machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#30340;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#21644;&#36991;&#20813;&#26114;&#36149;&#30697;&#38453;&#25805;&#20316;&#21644;&#35745;&#31639;&#40654;&#26364;&#26799;&#24230;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02041</link><description>&lt;p&gt;
&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#20302;&#22797;&#26434;&#24230;&#30340;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#30340;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#21644;&#36991;&#20813;&#26114;&#36149;&#30697;&#38453;&#25805;&#20316;&#21644;&#35745;&#31639;&#40654;&#26364;&#26799;&#24230;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#19978;&#23545;&#20989;&#25968;&#36827;&#34892;&#26368;&#23567;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#21464;&#20307;&#19981;&#21516;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992; carefully chosen &#30340;&#23376;&#31354;&#38388;&#65292;&#20351;&#24471;&#26356;&#26032;&#21487;&#20197;&#20889;&#25104;&#36845;&#20195;&#30340; Cholesky &#22240;&#23376;&#21644;&#19968;&#20010;&#31232;&#30095;&#30697;&#38453;&#30340;&#20056;&#31215;&#24418;&#24335;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26356;&#26032;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#30697;&#38453;&#25805;&#20316;&#65292;&#22914;&#30697;&#38453;&#25351;&#25968;&#21644;&#23494;&#38598;&#30697;&#38453;&#20056;&#27861;&#65292;&#36825;&#20123;&#25805;&#20316;&#36890;&#24120;&#22312;&#20960;&#20046;&#25152;&#26377;&#20854;&#20182; Riemannian &#20248;&#21270;&#31639;&#27861;&#20013;&#37117;&#26159;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work puts forth low-complexity Riemannian subspace descent algorithms for the minimization of functions over the symmetric positive definite (SPD) manifold. Different from the existing Riemannian gradient descent variants, the proposed approach utilizes carefully chosen subspaces that allow the update to be written as a product of the Cholesky factor of the iterate and a sparse matrix. The resulting updates avoid the costly matrix operations like matrix exponentiation and dense matrix multiplication, which are generally required in almost all other Riemannian optimization algorithms on SPD manifold. We further identify a broad class of functions, arising in diverse applications, such as kernel matrix learning, covariance estimation of Gaussian distributions, maximum likelihood parameter estimation of elliptically contoured distributions, and parameter estimation in Gaussian mixture model problems, over which the Riemannian gradients can be calculated efficiently. The proposed uni-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;(QNPG)&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#30340;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#22522;&#20110;&#19968;&#38454;&#26799;&#24230;&#30340;&#35757;&#32451;&#65292;QNPG&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13571</link><description>&lt;p&gt;
&#37327;&#23376;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65306;&#21521;&#26679;&#26412;&#39640;&#25928;&#22686;&#24378;&#23398;&#20064;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Quantum Natural Policy Gradients: Towards Sample-Efficient Reinforcement Learning. (arXiv:2304.13571v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;(QNPG)&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#30340;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#22522;&#20110;&#19968;&#38454;&#26799;&#24230;&#30340;&#35757;&#32451;&#65292;QNPG&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#26041;&#21521;&#65292;&#20294;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#24456;&#32791;&#36153;&#36164;&#28304;&#12290;&#20351;&#29992;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#20316;&#20026;&#20989;&#25968;&#36924;&#36817;&#22120;&#21487;&#20197;&#20943;&#23569;&#25104;&#26412;&#65292;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#23376;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;(QNPG)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#30340;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#65292;&#26159;&#19968;&#31181;&#20108;&#38454;&#26799;&#24230;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#12290;&#22312;Contextual Bandits&#29615;&#22659;&#19979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QNPG &#27604;&#22522;&#20110;&#19968;&#38454;&#26799;&#24230;&#30340;&#35757;&#32451;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#65292;&#24182;&#22312;12&#37327;&#23376;&#27604;&#29305;&#30340;&#30828;&#20214;&#35774;&#22791;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is a growing field in AI with a lot of potential. Intelligent behavior is learned automatically through trial and error in interaction with the environment. However, this learning process is often costly. Using variational quantum circuits as function approximators can reduce this cost. In order to implement this, we propose the quantum natural policy gradient (QNPG) algorithm -- a second-order gradient-based routine that takes advantage of an efficient approximation of the quantum Fisher information matrix. We experimentally demonstrate that QNPG outperforms first-order based training on Contextual Bandits environments regarding convergence speed and stability and thereby reduces the sample complexity. Furthermore, we provide evidence for the practical feasibility of our approach by training on a 12-qubit hardware device.
&lt;/p&gt;</description></item><item><title>TiDE&#26159;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20855;&#22791;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#65292;&#30456;&#36739;&#20110;&#26368;&#20339;&#30340;Transformer&#27169;&#22411;&#65292;&#36895;&#24230;&#24555;5-10&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.08424</link><description>&lt;p&gt;
&#29992;TiDE&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#65306;&#26102;&#38388;&#24207;&#21015;&#31264;&#23494;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08424
&lt;/p&gt;
&lt;p&gt;
TiDE&#26159;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20855;&#22791;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#65292;&#30456;&#36739;&#20110;&#26368;&#20339;&#30340;Transformer&#27169;&#22411;&#65292;&#36895;&#24230;&#24555;5-10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#21363;&#26102;&#38388;&#24207;&#21015;&#31264;&#23494;&#32534;&#30721;&#22120;(TiDE)&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20139;&#26377;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26368;&#31616;&#32447;&#24615;&#31867;&#27604;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;(LDS)&#30340;&#36817;&#20046;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#21305;&#37197;&#25110;&#32988;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#27604;&#26368;&#20339;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;5-10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;CWE&#30340;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#20316;&#32773;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#25913;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#32467;&#21512;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#21069;&#36884;&#30340;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#28431;&#27934;&#20173;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00409</link><description>&lt;p&gt;
DiverseVul: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#30340;&#26032;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection. (arXiv:2304.00409v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;CWE&#30340;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#20316;&#32773;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#25913;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#32467;&#21512;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#21069;&#36884;&#30340;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#28431;&#27934;&#20173;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#29228;&#21462;&#23433;&#20840;&#38382;&#39064;&#32593;&#31449;&#65292;&#25552;&#21462;&#30456;&#24212;&#39033;&#30446;&#30340;&#28431;&#27934;&#20462;&#22797;&#25552;&#20132;&#21644;&#28304;&#20195;&#30721;&#65292;&#31579;&#36873;&#20986;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;150&#20010;CWE&#65292;26,635&#20010;&#26131;&#21463;&#25915;&#20987;&#30340;&#20989;&#25968;&#21644;352,606&#20010;&#19981;&#26131;&#21463;&#25915;&#20987;&#30340;&#20989;&#25968;&#65292;&#25552;&#21462;&#33258;7,861&#20010;&#25552;&#20132;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35206;&#30422;&#20102;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;11&#20010;&#27169;&#22411;&#26550;&#26500;&#65292;&#23646;&#20110;4&#20010;&#23478;&#26063;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#65292;&#28145;&#24230;&#23398;&#20064;&#20173;&#26410;&#20934;&#22791;&#22909;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;......
&lt;/p&gt;
&lt;p&gt;
We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerable functions extracted from 7,861 commits. Our dataset covers 305 more projects than all previous datasets combined. We show that increasing the diversity and volume of training data improves the performance of deep learning models for vulnerability detection.  Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#23610;&#24230;&#30340;&#38750;&#38646;&#26435;&#37325;&#30340;&#26080;&#38480;&#23485;2&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#23637;&#31034;&#35813;&#38382;&#39064;&#20855;&#26377;&#19968;&#20010;&#26080;&#38480;&#32500;&#24230;&#30340;&#20984;&#23545;&#24212;&#65292;&#38543;&#30528;&#21021;&#22987;&#21270;&#30340;&#23610;&#24230;&#22312;0&#21040;+&#8734;&#33539;&#22260;&#20869;&#21464;&#21270;&#65292;&#20851;&#32852;&#36335;&#24452;&#22312;&#25152;&#35859;&#30340;&#20869;&#26680;&#21644;&#20016;&#23500;&#30340;&#21306;&#22495;&#20043;&#38388;&#36830;&#32493;&#25554;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.17805</link><description>&lt;p&gt;
&#20851;&#20110;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#65306;2&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks. (arXiv:2303.17805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#23610;&#24230;&#30340;&#38750;&#38646;&#26435;&#37325;&#30340;&#26080;&#38480;&#23485;2&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#23637;&#31034;&#35813;&#38382;&#39064;&#20855;&#26377;&#19968;&#20010;&#26080;&#38480;&#32500;&#24230;&#30340;&#20984;&#23545;&#24212;&#65292;&#38543;&#30528;&#21021;&#22987;&#21270;&#30340;&#23610;&#24230;&#22312;0&#21040;+&#8734;&#33539;&#22260;&#20869;&#21464;&#21270;&#65292;&#20851;&#32852;&#36335;&#24452;&#22312;&#25152;&#35859;&#30340;&#20869;&#26680;&#21644;&#20016;&#23500;&#30340;&#21306;&#22495;&#20043;&#38388;&#36830;&#32493;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27491;&#21017;&#21270;&#36335;&#24452;&#26377;&#26102;&#34987;&#29992;&#20316;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#36335;&#24452;&#30340;&#26041;&#20415;&#29702;&#35770;&#20195;&#29702;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#23610;&#24230;&#30340;&#38750;&#38646;&#26435;&#37325;&#30340;&#26080;&#38480;&#23485;2&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#20462;&#25913;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;2&#23618;&#32593;&#32476;&#35757;&#32451;&#30340;&#38750;&#20984;&#24615;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#19968;&#20010;&#26080;&#38480;&#32500;&#24230;&#30340;&#20984;&#23545;&#24212;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#30456;&#24212;&#30340;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#24182;&#35843;&#26597;&#20102;&#20854;&#20027;&#35201;&#29305;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#30528;&#21021;&#22987;&#21270;&#30340;&#23610;&#24230;&#22312;0&#21040;+&#8734;&#33539;&#22260;&#20869;&#21464;&#21270;&#65292;&#20851;&#32852;&#36335;&#24452;&#22312;&#25152;&#35859;&#30340;&#20869;&#26680;&#21644;&#20016;&#23500;&#30340;&#21306;&#22495;&#20043;&#38388;&#36830;&#32493;&#25554;&#20540;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#32553;&#25918;&#36335;&#24452;&#21644;&#20248;&#21270;&#36335;&#24452;&#30340;&#26368;&#32456;&#29366;&#24577;&#34892;&#20026;&#31867;&#20284;&#65292;&#29978;&#33267;&#36229;&#36234;&#20102;&#36825;&#20123;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In supervised learning, the regularization path is sometimes used as a convenient theoretical proxy for the optimization path of gradient descent initialized with zero. In this paper, we study a modification of the regularization path for infinite-width 2-layer ReLU neural networks with non-zero initial distribution of the weights at different scales. By exploiting a link with unbalanced optimal transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite dimensional convex counterpart. We formulate the corresponding functional optimization problem and investigate its main properties. In particular, we show that as the scale of the initialization ranges between $0$ and $+\infty$, the associated path interpolates continuously between the so-called kernel and rich regimes. The numerical experiments confirm that, in our setting, the scaling path and the final states of the optimization path behave similarly even beyond these ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16839</link><description>&lt;p&gt;
MaMMUT: &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#32852;&#21512;&#23398;&#20064;&#30340;&#31616;&#21333;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24050;&#20174;&#32534;&#30721;-&#35299;&#30721;&#36716;&#21521;&#20165;&#35299;&#30721;&#30340;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#26222;&#36941;&#35748;&#20026;&#65292;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#29983;&#25104;&#20219;&#21153;&#21644;&#23545;&#27604;&#20219;&#21153;&#65292;&#24448;&#24448;&#20114;&#30456;&#20914;&#31361;&#65292;&#38590;&#20197;&#22312;&#19968;&#20010;&#26550;&#26500;&#20013;&#23481;&#32435;&#65292;&#24182;&#36827;&#19968;&#27493;&#38656;&#35201;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22797;&#26434;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#20165;&#35299;&#30721;&#27169;&#22411;&#65292;&#36825;&#22312;&#32852;&#21512;&#23398;&#20064;&#36825;&#20123;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;MaMMUT&#23454;&#29616;&#30340;&#12290;&#23427;&#30001;&#21333;&#19968;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25991;&#26412;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#35299;&#30721;&#22120;&#19978;&#30340;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#19981;&#21516;&#30446;&#26631;&#20219;&#21153;&#30340;&#32852;&#21512;&#35757;&#32451;&#26159;&#31616;&#21333;&#30340;&#65292;&#26377;&#25928;&#30340;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#27169;&#22411;&#30340;&#26435;&#37325;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26550;&#26500;&#20351;&#24471;&#23545;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#26816;&#27979;&#30340;&#31616;&#21333;&#25193;&#23637;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07122</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#37327;&#21270;&#21271;&#26497;&#25918;&#22823;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#21464;&#26262;&#65292;&#20063;&#31216;&#21271;&#26497;&#25918;&#22823;&#65292;&#30001;&#22810;&#31181;&#22823;&#27668;&#21644;&#28023;&#27915;&#22240;&#32032;&#23548;&#33268;&#65292;&#20294;&#20854;&#22522;&#30784;&#28909;&#21147;&#22240;&#32032;&#30340;&#35814;&#32454;&#24773;&#20917;&#20173;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22266;&#23450;&#27835;&#30103;&#25928;&#24212;&#31574;&#30053;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;&#19981;&#29616;&#23454;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#28151;&#28102;&#30340;&#24433;&#21709;&#32780;&#24341;&#36215;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCINet - &#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36830;&#32493;&#27835;&#30103;&#26041;&#24335;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#22914;&#20309;&#22823;&#22823;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#32463;&#27982;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#21098;&#65292;&#25552;&#20986;&#20102;EcoOptiGen&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20351;&#29992;GPT-3.5/GPT-4&#27169;&#22411;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.04673</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference. (arXiv:2303.04673v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#32463;&#27982;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#21098;&#65292;&#25552;&#20986;&#20102;EcoOptiGen&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20351;&#29992;GPT-3.5/GPT-4&#27169;&#22411;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20854;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#21508;&#31181;&#21830;&#19994;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#25104;&#26412;&#39537;&#20351;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#32773;&#22312;&#26377;&#38480;&#30340;&#25512;&#29702;&#39044;&#31639;&#19979;&#26368;&#22823;&#21270;&#29983;&#25104;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20248;&#21270;&#25512;&#29702;&#36229;&#21442;&#25968;&#65288;&#22914;&#22238;&#22797;&#25968;&#37327;&#12289;&#28201;&#24230;&#21644;&#26368;&#22823;token&#25968;&#65289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26174;&#33879;&#24433;&#21709;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#29992;/&#25104;&#26412;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;EcoOptiGen&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#32463;&#27982;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#21098;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#20351;&#29992;GPT-3.5/GPT-4&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;EcoOptiGen&#24050;&#22312;FLAML&#24211;&#30340;`autogen'&#21253;&#20013;&#23454;&#29616;&#65306;\url{https://aka.ms/autogen}&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have sparked significant interest in their generative capabilities, leading to the development of various commercial applications. The high cost of using the models drives application builders to maximize the value of generation under a limited inference budget. This paper presents a study of optimizing inference hyperparameters such as the number of responses, temperature and max tokens, which significantly affects the utility/cost of text generation. We design a framework named EcoOptiGen which leverages economical hyperparameter optimization and cost-based pruning. Experiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its effectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML library: \url{https://aka.ms/autogen}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#26368;&#22823;&#29109;&#26102;&#21051;&#27861;&#31283;&#23450;&#24212;&#29992;&#20110;&#21333;&#31934;&#24230;&#19979;&#30340;&#31232;&#30095;&#27668;&#20307;&#21160;&#21147;&#23398;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#27169;&#25311;&#38750;&#24120;&#24378;&#30340;&#27491;&#24120;&#28608;&#27874;&#12290;</title><link>http://arxiv.org/abs/2303.02898</link><description>&lt;p&gt;
&#23558;&#26368;&#22823;&#29109;&#26102;&#21051;&#27861;&#31283;&#23450;&#24212;&#29992;&#20110;&#21333;&#31934;&#24230;&#19979;&#30340;&#31232;&#30095;&#27668;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stabilizing the Maximal Entropy Moment Method for Rarefied Gas Dynamics at Single-Precision. (arXiv:2303.02898v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#26368;&#22823;&#29109;&#26102;&#21051;&#27861;&#31283;&#23450;&#24212;&#29992;&#20110;&#21333;&#31934;&#24230;&#19979;&#30340;&#31232;&#30095;&#27668;&#20307;&#21160;&#21147;&#23398;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#27169;&#25311;&#38750;&#24120;&#24378;&#30340;&#27491;&#24120;&#28608;&#27874;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#36866;&#29992;&#20110;&#23494;&#38598;&#21644;&#31232;&#30095;&#27668;&#20307;&#30340;&#25193;&#23637;&#27969;&#20307;&#21147;&#23398;&#26041;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#36825;&#20010;&#25361;&#25112;&#30340;&#19968;&#20010;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#27668;&#20307;&#20998;&#23376;&#36895;&#24230;&#20998;&#24067;&#30340;&#30697;&#27861;&#25551;&#36848;&#23494;&#38598;&#21644;&#31232;&#30095;&#27668;&#20307;&#34892;&#20026;&#12290;&#22312;&#20247;&#22810;&#30340;&#30697;&#27861;&#20013;&#65292;&#26368;&#22823;&#29109;&#26102;&#21051;&#27861;&#65288;MEM&#65289;&#22240;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#35299;&#20915;&#24615;&#21644;&#31283;&#23450;&#24615;&#32780;&#31361;&#20986;&#65292;&#23427;&#21033;&#29992;&#20102;&#29109;&#26368;&#22823;&#21270;&#30340;&#36895;&#24230;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#36825;&#26679;&#30340;&#20998;&#24067;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#26465;&#20214;&#30149;&#24577;&#19988;&#35745;&#31639;&#38656;&#27714;&#36739;&#22823;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#25968;&#20540;&#31934;&#24230;&#19981;&#36275;&#26102;&#20250;&#23548;&#33268;&#25968;&#20540;&#28322;&#20986;&#21644;&#23849;&#28291;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#39640;&#36895;&#28608;&#27874;&#31561;&#27969;&#21160;&#29616;&#35937;&#12290;&#23427;&#36824;&#38459;&#30861;&#20102;&#29616;&#20195;GPU&#21033;&#29992;&#20854;&#24040;&#22823;&#30340;&#21333;&#31934;&#24230;&#35745;&#31639;&#33021;&#21147;&#26469;&#21152;&#36895;&#20248;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#31283;&#23450;MEM&#65292;&#20351;&#20854;&#22312;&#29616;&#20195;GPU&#19978;&#20197;&#21333;&#31934;&#24230;&#23454;&#29992;&#20110;&#27169;&#25311;&#38750;&#24120;&#24378;&#30340;&#27491;&#24120;&#28608;&#27874;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing extended hydrodynamics equations valid for both dense and rarefied gases remains a great challenge. A systematical solution for this challenge is the moment method describing both dense and rarefied gas behaviors with moments of gas molecule velocity distributions. Among moment methods, the maximal entropy moment method (MEM) stands out for its well-posedness and stability, which utilizes velocity distributions with maximized entropy. However, finding such distributions requires solving an ill-conditioned and computation-demanding optimization problem. This problem causes numerical overflow and breakdown when the numerical precision is insufficient, especially for flows like high-speed shock waves. It also prevents modern GPUs from accelerating optimization with their enormous single floating-point precision computation power. This paper aims to stabilize MEM, making it practical for simulating very strong normal shock waves on modern GPUs at single precision. We propose the
&lt;/p&gt;</description></item><item><title>&#36830;&#36890;&#24615;&#20248;&#21270;&#21644;&#26356;&#28145;&#30340;&#28040;&#24687;&#20989;&#25968;&#22312;&#26230;&#20307;&#32467;&#26500;&#30740;&#31350;&#20013;&#30340;&#23884;&#22871;&#22270;&#32593;&#32476;&#20013;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.14102</link><description>&lt;p&gt;
&#36830;&#36890;&#24615;&#20248;&#21270;&#30340;&#23884;&#22871;&#22270;&#32593;&#32476;&#29992;&#20110;&#26230;&#20307;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Connectivity Optimized Nested Graph Networks for Crystal Structures. (arXiv:2302.14102v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14102
&lt;/p&gt;
&lt;p&gt;
&#36830;&#36890;&#24615;&#20248;&#21270;&#21644;&#26356;&#28145;&#30340;&#28040;&#24687;&#20989;&#25968;&#22312;&#26230;&#20307;&#32467;&#26500;&#30740;&#31350;&#20013;&#30340;&#23884;&#22871;&#22270;&#32593;&#32476;&#20013;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#24212;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#24635;&#32467;&#20102;&#23545;&#32467;&#26230;(&#21608;&#26399;&#24615;)&#26448;&#26009;&#30340;&#22270;&#26500;&#36896;&#65292;&#24182;&#30740;&#31350;&#20854;&#23545;GNN&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#38750;&#23545;&#31216;&#21333;&#20803;&#26684;&#20316;&#20026;&#34920;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#25152;&#26377;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#21407;&#23376;&#25968;&#37327;&#12290;&#36825;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35757;&#32451;&#22823;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#21644;&#32447;&#22270;&#27169;&#26495;&#30340;&#31616;&#21333;&#20294;&#31995;&#32479;&#22320;&#26500;&#24314;GNN&#26550;&#26500;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26550;&#26500;(Nested Graph Network, NGN)&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24314;&#35758;&#30340;&#27169;&#22411;&#22312;MatBench&#22522;&#20934;&#27979;&#35797;&#30340;&#25152;&#26377;&#20219;&#21153;&#20013;&#31995;&#32479;&#22320;&#25913;&#21892;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#20248;&#21270;&#30340;&#36830;&#25509;&#24615;&#21644;&#26356;&#28145;&#30340;&#28040;&#24687;&#20989;&#25968;&#26159;&#25913;&#36827;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have been applied to a large variety of applications in materials science and chemistry. Here, we recapitulate the graph construction for crystalline (periodic) materials and investigate its impact on the GNNs model performance. We suggest the asymmetric unit cell as a representation to reduce the number of atoms by using all symmetries of the system. This substantially reduced the computational cost and thus time needed to train large graph neural networks without any loss in accuracy. Furthermore, with a simple but systematically built GNN architecture based on message passing and line graph templates, we introduce a general architecture (Nested Graph Network, NGN) that is applicable to a wide range of tasks. We show that our suggested models systematically improve state-of-the-art results across all tasks within the MatBench benchmark. Further analysis shows that optimized connectivity and deeper message functions are responsible for the improvement. Asy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#24635;&#32467;&#20102;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.00722</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32508;&#36848;&#65306;&#20174;&#28608;&#27963;&#20989;&#25968;&#21040;Transformer
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Learning: From Activations to Transformers. (arXiv:2302.00722v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00722
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#24635;&#32467;&#20102;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24471;&#30410;&#20110;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#28044;&#29616;&#12290;&#36825;&#20123;&#21253;&#25324;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#22810;&#31181;&#21464;&#20307;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21521;&#20855;&#26377;&#28145;&#24230;&#23398;&#20064;&#22522;&#26412;&#29702;&#35299;&#30340;&#20154;&#25552;&#20379;&#23545;&#36825;&#20123;&#39046;&#22495;&#20013;&#26368;&#26032;&#37325;&#35201;&#36129;&#29486;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#26399;&#26395;&#26159;&#36890;&#36807;&#23545;&#37325;&#35201;&#26368;&#26032;&#20316;&#21697;&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#25506;&#35752;&#65292;&#20419;&#36827;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#24418;&#25104;&#26032;&#30340;&#32852;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#35768;&#22810;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#23545;&#26368;&#36817;&#19968;&#20123;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#20363;&#22914;OpenAI&#30340;GPT-4&#21644;Google&#30340;PaLM 2&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#22312;&#36870;&#38382;&#39064;&#20013;&#25104;&#20026;&#21487;&#35299;&#37322;&#30340;&#22240;&#23376;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#19982;&#35299;&#23494;&#21464;&#25442;&#32467;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#22122;&#22768;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#32534;&#30721;&#20449;&#21495;&#27169;&#22411;&#30340;&#32467;&#26500;</title><link>http://arxiv.org/abs/2301.07820</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#22240;&#23376;&#21270;&#30340;&#20986;&#29616;&#22312;&#36870;&#38382;&#39064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Emergence of the SVD as an interpretable factorization in deep learning for inverse problems. (arXiv:2301.07820v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07820
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#22312;&#36870;&#38382;&#39064;&#20013;&#25104;&#20026;&#21487;&#35299;&#37322;&#30340;&#22240;&#23376;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#19982;&#35299;&#23494;&#21464;&#25442;&#32467;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#22122;&#22768;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#32534;&#30721;&#20449;&#21495;&#27169;&#22411;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#35299;&#37322;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#24403;&#19982;&#35299;&#23494;&#21464;&#25442;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#38024;&#23545;&#22122;&#22768;&#21442;&#25968;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#25216;&#26415;&#12290;&#36890;&#36807;&#32771;&#34385;&#20256;&#36882;&#32473;&#35299;&#23494;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#24179;&#22343;&#25928;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#26497;&#38480;&#19979;&#65292;&#35299;&#23494;&#21464;&#25442;&#21487;&#20197;&#29992;NN&#26435;&#37325;&#30340;SVD&#21644;&#36755;&#20837;&#33258;&#30456;&#20851;&#30697;&#38453;&#26469;&#34920;&#31034;&#12290;&#21033;&#29992;&#36825;&#20010;&#20107;&#23454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#31867;&#20013;&#65292;SVD&#21487;&#20197;&#26159;&#35757;&#32451;&#32593;&#32476;&#32534;&#30721;&#20449;&#21495;&#27169;&#22411;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#29992;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20449;&#21495;&#27169;&#22411;&#30340;&#23454;&#35777;&#35777;&#25454;&#36827;&#19968;&#27493;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#25968;&#23398;&#29702;&#35770;&#21644;&#35821;&#20041;&#21457;&#23637;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Within the framework of deep learning we demonstrate the emergence of the singular value decomposition (SVD) of the weight matrix as a tool for interpretation of neural networks (NN) when combined with the descrambling transformation--a recently-developed technique for addressing interpretability in noisy parameter estimation neural networks \cite{amey2021neural}. By considering the averaging effect of the data passed to the descrambling minimization problem, we show that descrambling transformations--in the large data limit--can be expressed in terms of the SVD of the NN weights and the input autocorrelation matrix. Using this fact, we show that within the class of noisy parameter estimation problems the SVD may be the structure through which trained networks encode a signal model. We substantiate our theoretical findings with empirical evidence from both linear and non-linear signal models. Our results also illuminate the connections between a mathematical theory of semantic developm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#32654;&#39135;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#22238;&#25910;&#22522;&#20110;&#21516;&#19968;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#26679;&#36741;&#21161;&#20219;&#21153;&#19978;&#30340;&#22810;&#27425;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#31181;&#31574;&#30053;&#26088;&#22312;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#27169;&#22411;&#26435;&#37325;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10445</link><description>&lt;p&gt;
&#27169;&#22411;&#32654;&#39135;&#65306;&#22810;&#26679;&#27169;&#22411;&#30340;&#22238;&#25910;&#21033;&#29992;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization. (arXiv:2212.10445v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#32654;&#39135;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#22238;&#25910;&#22522;&#20110;&#21516;&#19968;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#26679;&#36741;&#21161;&#20219;&#21153;&#19978;&#30340;&#22810;&#27425;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#31181;&#31574;&#30053;&#26088;&#22312;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#27169;&#22411;&#26435;&#37325;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#27491;&#22312;&#37325;&#26032;&#23450;&#20041;AI&#31995;&#32479;&#30340;&#26500;&#24314;&#26041;&#24335;&#12290;&#20174;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#24320;&#22987;&#65292;&#20174;&#19994;&#32773;&#29616;&#22312;&#37117;&#36981;&#24490;&#19968;&#20010;&#26631;&#20934;&#30340;&#27969;&#31243;&#26469;&#26500;&#24314;&#20182;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65306;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#24494;&#35843;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;&#20114;&#32852;&#32593;&#19978;&#20805;&#26021;&#30528;&#35768;&#22810;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;&#65306;&#36825;&#20123;&#21333;&#29420;&#30340;&#24494;&#35843;&#36807;&#31243;&#23384;&#22312;&#23396;&#31435;&#65292;&#27809;&#26377;&#30456;&#20114;&#21463;&#30410;&#12290;&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#36825;&#26159;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#26426;&#20250;&#65292;&#22240;&#20026;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#21253;&#21547;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#32654;&#39135;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#19978;&#22238;&#25910;&#30456;&#21516;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#20010;&#24494;&#35843;&#30340;&#26032;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36741;&#21161;&#26435;&#37325;&#37325;&#26032;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#22810;&#20010;&#24182;&#34892;&#24494;&#35843;&#30340;&#21021;&#22987;&#21270;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#25152;&#26377;&#24494;&#35843;&#21518;&#30340;&#26435;&#37325;&#21462;&#24179;&#22343;&#20540;&#65292;&#24471;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#36825;&#31181;&#22238;&#25910;&#31574;&#30053;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#26469;&#26368;&#22823;&#21270;&#26435;&#37325;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#27585;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20219;&#21153;&#38459;&#26029;&#33539;&#24335;&#23454;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#19979;&#28216;&#20351;&#29992;&#30340;&#26356;&#31934;&#30830;&#25511;&#21046;&#65292;&#20174;&#32780;&#38477;&#20302;&#22522;&#30784;&#27169;&#22411;&#30340;&#26377;&#23475;&#21452;&#37325;&#29992;&#36884;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2211.14946</link><description>&lt;p&gt;
&#33258;&#27585;&#27169;&#22411;&#65306;&#22686;&#21152;&#22522;&#30784;&#27169;&#22411;&#26377;&#23475;&#21452;&#37325;&#29992;&#36884;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models. (arXiv:2211.14946v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#27585;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20219;&#21153;&#38459;&#26029;&#33539;&#24335;&#23454;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#19979;&#28216;&#20351;&#29992;&#30340;&#26356;&#31934;&#30830;&#25511;&#21046;&#65292;&#20174;&#32780;&#38477;&#20302;&#22522;&#30784;&#27169;&#22411;&#30340;&#26377;&#23475;&#21452;&#37325;&#29992;&#36884;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26085;&#30410;&#22686;&#38271;&#30340;&#22823;&#35268;&#27169;&#24320;&#28304;&#22522;&#30784;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#65292;&#38477;&#20302;&#20102;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#35768;&#22810;&#26032;&#38382;&#39064;&#25152;&#38656;&#30340;&#26631;&#27880;&#25968;&#25454;&#21644;&#25216;&#26415;&#19987;&#38271;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#30340;&#21452;&#37325;&#29992;&#36884;&#39118;&#38505;&#65292;&#26080;&#24046;&#21035;&#22320;&#38477;&#20302;&#20102;&#26500;&#24314;&#26377;&#23475;&#21644;&#26377;&#30410;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#12290;&#30446;&#21069;&#65292;&#25919;&#31574;&#24037;&#20855;&#22914;&#38480;&#21046;&#27169;&#22411;&#35775;&#38382;&#21644;&#20986;&#21475;&#31649;&#21046;&#26159;&#32531;&#35299;&#27492;&#31867;&#21452;&#37325;&#29992;&#36884;&#39118;&#38505;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#28508;&#22312;&#30340;&#23433;&#20840;&#21457;&#24067;&#31574;&#30053;&#65292;&#24182;&#35748;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#37117;&#23558;&#21463;&#30410;&#20110;&#33021;&#22815;&#26356;&#31934;&#30830;&#25511;&#21046;&#24320;&#28304;&#22522;&#30784;&#27169;&#22411;&#19979;&#28216;&#20351;&#29992;&#30340;&#22522;&#30784;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65306;&#20219;&#21153;&#38459;&#26029;&#33539;&#24335;&#65292;&#21363;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#39069;&#22806;&#26426;&#21046;&#38459;&#30861;&#23545;&#26377;&#23475;&#20219;&#21153;&#30340;&#36866;&#24212;&#32780;&#19981;&#25439;&#22833;&#23545;&#29702;&#24819;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#27169;&#22411;&#20026;&#33258;&#27585;&#27169;&#22411;&#65292;&#21463;&#21040;&#26426;&#26800;&#35774;&#22791;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. We call the resulting models self-destructing models, inspired by mechanis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.06108</link><description>&lt;p&gt;
RaLiBEV: &#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#28608;&#20809;&#38647;&#36798;&#21644;&#38647;&#36798;&#22312;&#24863;&#30693;&#21608;&#22260;&#29615;&#22659;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28608;&#20809;&#38647;&#36798;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#31354;&#38388;&#24863;&#30693;&#20449;&#24687;&#65292;&#20294;&#22312;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#19979;&#26080;&#27861;&#24037;&#20316;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38647;&#36798;&#20449;&#21495;&#30001;&#20110;&#27874;&#38271;&#30340;&#29305;&#24615;&#22312;&#36935;&#21040;&#38632;&#28404;&#25110;&#38654;&#31890;&#26102;&#20250;&#21457;&#29983;&#34893;&#23556;&#65292;&#20294;&#23427;&#21463;&#21040;&#22823;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#26368;&#36817;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#34701;&#21512;&#21487;&#20197;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#23454;&#29616;&#24378;&#20581;&#30340;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20174;&#27599;&#20010;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#23545;&#40784;&#21644;&#27719;&#32858;&#20004;&#20010;&#20998;&#25903;&#30340;&#29305;&#24449;&#20197;&#39044;&#27979;&#29289;&#20307;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#31614;&#20998;&#37197;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#31616;&#21333;&#35774;&#35745;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#36793;&#30028;&#26694;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#26469;&#33258;&#38647;&#36798;&#30340;&#36317;&#31163;-&#26041;&#20301;&#29305;&#24449;&#34701;&#21512;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment. LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23384;&#22312;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#21521;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25554;&#20837;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#36827;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2211.02408</link><description>&lt;p&gt;
&#25554;&#20837;&#21518;&#38376;&#20803;&#32032;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis. (arXiv:2211.02408v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23384;&#22312;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#21521;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25554;&#20837;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#36827;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#25216;&#26415;&#22312;&#30740;&#31350;&#32773;&#21644;&#20844;&#20247;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20381;&#36182;&#20110;&#22806;&#37096;&#26469;&#28304;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#29992;&#25143;&#30456;&#20449;&#26816;&#32034;&#21040;&#30340;&#27169;&#22411;&#20250;&#20687;&#25215;&#35834;&#30340;&#37027;&#26679;&#36816;&#34892;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#21521;&#38376;&#25915;&#20987;&#25991;&#26412;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21482;&#26159;&#36731;&#24494;&#22320;&#25913;&#21464;&#20102;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;&#23545;&#20110;&#24102;&#26377;&#24178;&#20928;&#25552;&#31034;&#30340;&#22270;&#20687;&#29983;&#25104;&#27809;&#26377;&#21487;&#30097;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#25554;&#20837;&#25552;&#31034;&#20013;&#65292;&#20363;&#22914;&#19968;&#20010;&#38750;&#25289;&#19969;&#23383;&#31526;&#25110;&#34920;&#24773;&#31526;&#21495;&#65292;&#25915;&#20987;&#32773;&#23601;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;Stable Diffusion&#21644;highligh&#19978;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highligh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;3D&#23545;&#35937;&#29702;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#21333;&#20010;RGB&#22270;&#20687;&#26500;&#24314;&#32479;&#19968;&#32780;&#32039;&#20945;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#65292;&#22914;&#26032;&#35270;&#35282;&#28210;&#26579;&#12289;3D&#37325;&#24314;&#12289;&#30896;&#25758;&#26816;&#26597;&#21644;&#31283;&#23450;&#25235;&#21462;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20174;&#26032;&#35270;&#35282;&#36827;&#34892;&#28210;&#26579;&#24182;&#39044;&#27979;&#25104;&#21151;&#30340;&#25235;&#21462;&#12290;</title><link>http://arxiv.org/abs/2210.12126</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#31070;&#32463;&#22330;&#29992;&#20110;3D&#23545;&#35937;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
One-Shot Neural Fields for 3D Object Understanding. (arXiv:2210.12126v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;3D&#23545;&#35937;&#29702;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#21333;&#20010;RGB&#22270;&#20687;&#26500;&#24314;&#32479;&#19968;&#32780;&#32039;&#20945;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#65292;&#22914;&#26032;&#35270;&#35282;&#28210;&#26579;&#12289;3D&#37325;&#24314;&#12289;&#30896;&#25758;&#26816;&#26597;&#21644;&#31283;&#23450;&#25235;&#21462;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20174;&#26032;&#35270;&#35282;&#36827;&#34892;&#28210;&#26579;&#24182;&#39044;&#27979;&#25104;&#21151;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#30340;&#32479;&#19968;&#19988;&#32039;&#20945;&#30340;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#20854;&#20013;&#22330;&#26223;&#20013;&#30340;&#27599;&#20010;&#23545;&#35937;&#30001;&#25429;&#25417;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#28508;&#22312;&#20195;&#30721;&#26469;&#25551;&#36848;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#21487;&#20197;&#35299;&#30721;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20363;&#22914;&#26032;&#35270;&#35282;&#28210;&#26579;&#65292;3D&#37325;&#24314;&#65288;&#20363;&#22914;&#24674;&#22797;&#28145;&#24230;&#65292;&#28857;&#20113;&#25110;&#20307;&#32032;&#22270;&#65289;&#65292;&#30896;&#25758;&#26816;&#26597;&#21644;&#31283;&#23450;&#25235;&#21462;&#39044;&#27979;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#26032;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#22312;&#27979;&#35797;&#26102;&#20174;&#21333;&#20010;RGB&#36755;&#20837;&#22270;&#20687;&#26500;&#24314;&#25105;&#20204;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#22810;&#35270;&#22270;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#31867;&#21035;&#32423;&#20808;&#39564;&#30693;&#35782;&#65292;&#28982;&#21518;&#22312;&#23569;&#25968;&#25110;&#20165;&#19968;&#20010;&#35270;&#22270;&#30340;&#26032;&#23545;&#35937;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;NeRF&#27169;&#22411;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25235;&#21462;&#36755;&#20986;&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#30340;&#26041;&#27861;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#20174;&#20165;&#19968;&#20010;&#35270;&#28857;&#35266;&#23519;&#21040;&#30340;&#21333;&#20010;RGB&#36755;&#20837;&#22270;&#20687;&#26500;&#24314;&#34920;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#24674;&#22797;&#30340;&#34920;&#31034;&#26041;&#27861;&#20801;&#35768;&#20174;&#26032;&#35270;&#35282;&#36827;&#34892;&#28210;&#26579;&#65292;&#21253;&#25324;&#36974;&#25377;&#30340;&#29289;&#20307;&#37096;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#25104;&#21151;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified and compact scene representation for robotics, where each object in the scene is depicted by a latent code capturing geometry and appearance. This representation can be decoded for various tasks such as novel view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or voxel maps), collision checking, and stable grasp prediction. We build our representation from a single RGB input image at test time by leveraging recent advances in Neural Radiance Fields (NeRF) that learn category-level priors on large multiview datasets, then fine-tune on novel objects from one or few views. We expand the NeRF model for additional grasp outputs and explore ways to leverage this representation for robotics. At test-time, we build the representation from a single RGB input image observing the scene from only one viewpoint. We find that the recovered representation allows rendering from novel views, including of occluded object parts, and also for predicting successful 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#36234;&#22495;&#37492;&#21035;&#22120;&#65292;&#24182;&#25506;&#35752;&#20102;BNN&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#31867;&#22411;&#20197;&#21450;&#22914;&#20309;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36739;&#22909;&#34920;&#31034;&#30340;&#22270;&#20687;&#30340;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#36739;&#20302;&#65292;&#32780;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36739;&#24046;&#34920;&#31034;&#30340;&#22270;&#20687;&#20013;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#36739;&#39640;&#12290;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BNN&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#30340;&#36234;&#22495;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10780</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#30340;&#36234;&#22495;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
An out-of-distribution discriminator based on Bayesian neural network epistemic uncertainty. (arXiv:2210.10780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#36234;&#22495;&#37492;&#21035;&#22120;&#65292;&#24182;&#25506;&#35752;&#20102;BNN&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#31867;&#22411;&#20197;&#21450;&#22914;&#20309;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36739;&#22909;&#34920;&#31034;&#30340;&#22270;&#20687;&#30340;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#36739;&#20302;&#65292;&#32780;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36739;&#24046;&#34920;&#31034;&#30340;&#22270;&#20687;&#20013;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#36739;&#39640;&#12290;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BNN&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#30340;&#36234;&#22495;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20855;&#26377;&#22686;&#24378;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#38656;&#35201;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65292;&#20855;&#26377;&#20869;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;BNN&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#31867;&#22411;&#20197;&#21450;&#22914;&#20309;&#35745;&#31639;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#20363;&#23376;&#36827;&#34892;&#23454;&#39564;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#35782;&#21035;&#22270;&#20687;&#20013;&#20107;&#20214;&#30340;&#24133;&#24230;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26377;&#24456;&#22909;&#34920;&#31034;&#30340;&#22270;&#20687;&#30340;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#36739;&#20302;&#65292;&#32780;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#24046;&#34920;&#31034;&#30340;&#22270;&#20687;&#20013;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#36739;&#39640;&#12290;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;BNN&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#30340;&#36234;&#22495;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#24433;&#21709;&#36234;&#22495;&#26816;&#27979;&#33021;&#21147;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have revolutionized the field of machine learning with increased predictive capability. In addition to improving the predictions of neural networks, there is a simultaneous demand for reliable uncertainty quantification on estimates made by machine learning methods such as neural networks. Bayesian neural networks (BNNs) are an important type of neural network with built-in capability for quantifying uncertainty. This paper discusses aleatoric and epistemic uncertainty in BNNs and how they can be calculated. With an example dataset of images where the goal is to identify the amplitude of an event in the image, it is shown that epistemic uncertainty tends to be lower in images which are well-represented in the training dataset and tends to be high in images which are not well-represented. An algorithm for out-of-distribution (OoD) detection with BNN epistemic uncertainty is introduced along with various experiments demonstrating factors influencing the OoD detection capa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#26032;&#22411;&#20613;&#37324;&#21494;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#36716;&#25442;&#20449;&#21495;&#65292;&#20851;&#32852;&#25968;&#25454;&#19982;&#20854;&#22240;&#26524;&#20851;&#31995;&#65292;&#25193;&#23637;&#20102;&#32452;&#21512;&#25968;&#23398;&#20013;&#30340;&#32463;&#20856;&#27169;&#27604;&#20044;&#26031;&#21453;&#28436;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2209.07970</link><description>&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;&#21644;&#20559;&#24207;&#38598;&#19978;&#30340;&#22240;&#26524;&#20613;&#37324;&#21494;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Fourier Analysis on Directed Acyclic Graphs and Posets. (arXiv:2209.07970v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#26032;&#22411;&#20613;&#37324;&#21494;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#36716;&#25442;&#20449;&#21495;&#65292;&#20851;&#32852;&#25968;&#25454;&#19982;&#20854;&#22240;&#26524;&#20851;&#31995;&#65292;&#25193;&#23637;&#20102;&#32452;&#21512;&#25968;&#23398;&#20013;&#30340;&#32463;&#20856;&#27169;&#27604;&#20044;&#26031;&#21453;&#28436;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#20449;&#21495;&#22788;&#29702;&#27010;&#24565;&#65292;&#29992;&#20110;&#30001;&#24102;&#26435;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#32034;&#24341;&#30340;&#20449;&#21495;&#65288;&#25110;&#25968;&#25454;&#65289;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#20613;&#37324;&#21494;&#22522;&#20989;&#25968;&#21487;&#20197;&#23558;&#36866;&#24403;&#30340;&#31227;&#20301;&#21644;&#21367;&#31215;&#31639;&#23376;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#36825;&#31181;&#31639;&#23376;&#12290; DAG&#26159;&#25429;&#25417;&#25968;&#25454;&#20540;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#24120;&#35265;&#27169;&#22411;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#23558;&#25968;&#25454;&#19982;&#20854;&#22240;&#26524;&#20851;&#31995;&#32852;&#31995;&#36215;&#26469;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#25105;&#20204;&#23450;&#20041;&#30340;&#32447;&#24615;&#20551;&#35774;&#12290;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#23450;&#20041;&#38656;&#35201;&#23545;&#21152;&#26435;DAG&#36827;&#34892;&#20256;&#36882;&#38381;&#21253;&#22788;&#29702;&#65292;&#26681;&#25454;&#36793;&#26435;&#37325;&#30340;&#35299;&#37322;&#65292;&#21487;&#20197;&#26377;&#20960;&#31181;&#24418;&#24335;&#12290;&#31034;&#20363;&#21253;&#25324;&#24433;&#21709;&#27700;&#24179;&#12289;&#36317;&#31163;&#25110;&#27745;&#26579;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#22270;&#20449;&#21495;&#22788;&#29702;&#65306;&#23427;&#19987;&#38376;&#38024;&#23545;DAG&#24182;&#21033;&#29992;&#24182;&#25193;&#23637;&#20102;&#32452;&#21512;&#25968;&#23398;&#20013;&#30340;&#32463;&#20856;&#27169;&#27604;&#20044;&#26031;&#21453;&#28436;&#29702;&#35770;&#12290;&#23545;&#20110;&#19968;&#20010;&#20856;&#22411;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24314;&#27169;&#21160;&#24577;&#32593;&#32476;&#30340;DAGs&#65292;&#20854;&#20013;&#36793;&#32447;&#34920;&#31034;&#26102;&#38388;&#39034;&#24207;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel form of Fourier analysis, and associated signal processing concepts, for signals (or data) indexed by edge-weighted directed acyclic graphs (DAGs). This means that our Fourier basis yields an eigendecomposition of a suitable notion of shift and convolution operators that we define. DAGs are the common model to capture causal relationships between data values and in this case our proposed Fourier analysis relates data with its causes under a linearity assumption that we define. The definition of the Fourier transform requires the transitive closure of the weighted DAG for which several forms are possible depending on the interpretation of the edge weights. Examples include level of influence, distance, or pollution distribution. Our framework is different from prior GSP: it is specific to DAGs and leverages, and extends, the classical theory of Moebius inversion from combinatorics. For a prototypical application we consider DAGs modeling dynamic networks in which edge
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21028;&#21035;&#22120;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;4D GAN&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;3D&#24863;&#30693;&#35270;&#39057;&#65292;&#20135;&#29983;&#20855;&#26377;&#22810;&#35270;&#22270;&#21644;&#26102;&#24577;&#19968;&#33268;&#24615;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#23398;&#20064;&#20102;&#20016;&#23500;&#30340;&#21487;&#20998;&#35299;&#30340;3D&#32467;&#26500;&#21644;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2206.14797</link><description>&lt;p&gt;
3D&#24863;&#30693;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
3D-Aware Video Generation. (arXiv:2206.14797v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21028;&#21035;&#22120;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;4D GAN&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;3D&#24863;&#30693;&#35270;&#39057;&#65292;&#20135;&#29983;&#20855;&#26377;&#22810;&#35270;&#22270;&#21644;&#26102;&#24577;&#19968;&#33268;&#24615;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#23398;&#20064;&#20102;&#20016;&#23500;&#30340;&#21487;&#20998;&#35299;&#30340;3D&#32467;&#26500;&#21644;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#24050;&#25104;&#20026;&#35768;&#22810;&#22270;&#20687;&#21512;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#37325;&#35201;&#26500;&#24314;&#22359;&#12290;&#26368;&#36817;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#20063;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#35270;&#22270;&#25110;&#26102;&#24577;&#19968;&#33268;&#24615;&#30340;&#39640;&#36136;&#37327;3D&#25110;&#35270;&#39057;&#20869;&#23481;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23398;&#20064;&#26080;&#26465;&#20214;&#29983;&#25104;3D&#24863;&#30693;&#35270;&#39057;&#30340;4D&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#36890;&#36807;&#23558;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#19982;&#26102;&#38388;&#24863;&#30693;&#30340;&#21028;&#21035;&#22120;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;GAN&#26694;&#26550;&#65292;&#20165;&#36890;&#36807;&#21333;&#30446;&#35270;&#39057;&#30417;&#30563;&#21512;&#25104;3D&#35270;&#39057;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#20016;&#23500;&#30340;&#21487;&#20998;&#35299;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#20316;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26032;&#30340;&#26102;&#31354;&#28210;&#26579;&#30340;&#35270;&#35273;&#25928;&#26524;&#65292;&#21516;&#26102;&#20135;&#29983;&#20102;&#19982;&#29616;&#26377;3D&#25110;&#35270;&#39057;GAN&#36136;&#37327;&#30456;&#23218;&#32654;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have emerged as an essential building block for many image synthesis and editing tasks. Recent advances in this field have also enabled high-quality 3D or video content to be generated that exhibits either multi-view or temporal consistency. With our work, we explore 4D generative adversarial networks (GANs) that learn unconditional generation of 3D-aware videos. By combining neural implicit representations with time-aware discriminator, we develop a GAN framework that synthesizes 3D video supervised only with monocular videos. We show that our method learns a rich embedding of decomposable 3D structures and motions that enables new visual effects of spatio-temporal renderings while producing imagery with quality comparable to that of existing 3D or video GANs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#20998;&#22359;&#36317;&#31163;&#27169;&#22411;&#65288;HBDM&#65289;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#32593;&#32476;&#20998;&#26512;&#65292;&#32771;&#34385;&#22810;&#23610;&#24230;&#32467;&#26500;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#21040;&#32593;&#32476;&#30340;&#21516;&#36136;&#24615;&#21644;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.05885</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#27425;&#20998;&#22359;&#36317;&#31163;&#27169;&#22411;&#36827;&#34892;&#36229;&#20302;&#32500;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph Representations. (arXiv:2204.05885v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#20998;&#22359;&#36317;&#31163;&#27169;&#22411;&#65288;HBDM&#65289;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#32593;&#32476;&#20998;&#26512;&#65292;&#32771;&#34385;&#22810;&#23610;&#24230;&#32467;&#26500;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#21040;&#32593;&#32476;&#30340;&#21516;&#36136;&#24615;&#21644;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#34920;&#24449;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#21644;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#12289;&#32593;&#32476;&#37325;&#26500;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#20219;&#21153;&#30340;&#26680;&#24515;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#29983;&#25104;&#24335;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#35768;&#22810;&#26041;&#27861;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#36807;&#39640;&#32780;&#38480;&#21046;&#20102;&#22823;&#35268;&#27169;&#32593;&#32476;&#20998;&#26512;&#65292;&#24456;&#23569;&#33021;&#22815;&#26126;&#30830;&#32771;&#34385;&#22810;&#23610;&#24230;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#24182;&#19988;&#21482;&#26377;&#24456;&#23569;&#19968;&#37096;&#20998;&#26126;&#30830;&#32771;&#34385;&#21040;&#37325;&#35201;&#30340;&#32593;&#32476;&#23646;&#24615;&#65292;&#22914;&#21516;&#36136;&#24615;&#21644;&#20256;&#36882;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#23618;&#27425;&#20998;&#22359;&#36317;&#31163;&#27169;&#22411;&#65288;HBDM&#65289;&#12290;HBDM &#24378;&#21046;&#23454;&#26045;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#38543;&#26426;&#20998;&#22359;&#24314;&#27169;&#65288;SBM&#65289;&#30340;&#22810;&#23610;&#24230;&#20998;&#22359;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#36924;&#36817;&#28508;&#22312;&#36317;&#31163;&#27169;&#22411;&#65288;LDM&#65289;&#22312;&#25512;&#29702;&#30340;&#20998;&#23618;&#20013;&#32771;&#34385;&#21516;&#36136;&#24615;&#21644;&#20256;&#36882;&#24615;&#12290;HBDM &#33258;&#28982;&#22320;&#36866;&#29992;&#20110;&#21333;&#20998;&#21306;&#12289;&#26377;&#21521;&#21644;&#21452;&#20998;&#21306;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning (GRL) has become central for characterizing structures of complex networks and performing tasks such as link prediction, node classification, network reconstruction, and community detection. Whereas numerous generative GRL models have been proposed, many approaches have prohibitive computational requirements hampering large-scale network analysis, fewer are able to explicitly account for structure emerging at multiple scales, and only a few explicitly respect important network properties such as homophily and transitivity. This paper proposes a novel scalable graph representation learning method named the Hierarchical Block Distance Model (HBDM). The HBDM imposes a multiscale block structure akin to stochastic block modeling (SBM) and accounts for homophily and transitivity by accurately approximating the latent distance model (LDM) throughout the inferred hierarchy. The HBDM naturally accommodates unipartite, directed, and bipartite networks whereas the h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.01937</link><description>&lt;p&gt;
BoMD&#65306;&#36866;&#29992;&#20110;&#22024;&#26434;X&#20809;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#21253;
&lt;/p&gt;
&lt;p&gt;
BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#38382;&#39064;&#30340;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20855;&#26377;&#28165;&#27905;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#36825;&#31181;&#25163;&#21160;&#27880;&#37322;&#30340;&#39640;&#25104;&#26412;&#65292;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#20381;&#36182;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#20107;&#23454;&#19978;&#65292;&#35768;&#22810;&#33016;&#37096;X&#20809;&#20998;&#31867;&#22120;&#24050;&#32463;&#20174;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#19981;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#23548;&#33268;&#27425;&#20248;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;CXR&#25968;&#25454;&#38598;&#22823;&#22810;&#26159;&#22810;&#26631;&#35760;&#30340;&#65292;&#22240;&#27492;&#24403;&#21069;&#35774;&#35745;&#29992;&#20110;&#22810;&#31867;&#38382;&#39064;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#19981;&#33021;&#36731;&#26494;&#22320;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22024;&#26434;&#22810;&#26631;&#31614;CXR&#23398;&#20064;&#65292;&#20854;&#20013;&#26816;&#27979;&#24182;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#24120;&#35265;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#20351;&#29992;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#20462;&#25913;&#30340;SIRD&#27969;&#34892;&#30149;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#27515;&#20129;&#20154;&#25968;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#31867;&#21035;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#34913;&#37327;&#23454;&#38469;&#24863;&#26579;&#24635;&#25968;&#19982;&#23448;&#26041;&#32479;&#35745;&#24863;&#26579;&#20154;&#25968;&#24046;&#24322;&#30340;&#21442;&#25968;&#12290;&#36890;&#36807;&#20381;&#36182;&#21069;7&#22825;&#30340;&#25968;&#25454;&#36827;&#34892;&#20272;&#35745;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#34892;&#30149;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2203.00407</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#20462;&#25913;&#30340;SIRD&#27969;&#34892;&#30149;&#27169;&#22411;&#20013;&#36827;&#34892;&#21442;&#25968;&#35782;&#21035;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Inverse problem for parameters identification in a modified SIRD epidemic model using ensemble neural networks. (arXiv:2203.00407v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#20462;&#25913;&#30340;SIRD&#27969;&#34892;&#30149;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#27515;&#20129;&#20154;&#25968;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#31867;&#21035;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#34913;&#37327;&#23454;&#38469;&#24863;&#26579;&#24635;&#25968;&#19982;&#23448;&#26041;&#32479;&#35745;&#24863;&#26579;&#20154;&#25968;&#24046;&#24322;&#30340;&#21442;&#25968;&#12290;&#36890;&#36807;&#20381;&#36182;&#21069;7&#22825;&#30340;&#25968;&#25454;&#36827;&#34892;&#20272;&#35745;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#34892;&#30149;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SIRD&#27169;&#22411;&#30340;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#65292;&#36825;&#26159;&#23545;&#32463;&#20856;SIR&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#23558;&#27515;&#20129;&#20154;&#25968;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#31867;&#21035;&#21152;&#20197;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21253;&#25324;&#19968;&#20010;&#21442;&#25968;&#65292;&#21363;&#23454;&#38469;&#24863;&#26579;&#24635;&#25968;&#19982;&#23448;&#26041;&#32479;&#35745;&#30340;&#24863;&#26579;&#20154;&#25968;&#20043;&#27604;&#12290;&#30001;&#20110;&#35768;&#22810;&#22240;&#32032;&#65292;&#22914;&#25919;&#24220;&#20915;&#31574;&#12289;&#22810;&#31181;&#21464;&#20307;&#30340;&#20256;&#25773;&#12289;&#23398;&#26657;&#30340;&#24320;&#38381;&#31561;&#65292;&#27169;&#22411;&#21442;&#25968;&#22312;&#38271;&#26102;&#38388;&#20869;&#20445;&#25345;&#19981;&#21464;&#30340;&#20856;&#22411;&#20551;&#35774;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#31181;&#36866;&#29992;&#20110;&#30701;&#26102;&#38388;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#36890;&#36807;&#20381;&#36182;&#21069;7&#22825;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#20272;&#35745;&#65292;&#28982;&#21518;&#21033;&#29992;&#30830;&#23450;&#30340;&#21442;&#25968;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#24179;&#22343;&#20540;&#26041;&#27861;&#12290;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#26159;&#22522;&#20110;&#36890;&#36807;&#27714;&#35299;SIRD&#27169;&#22411;&#24471;&#21040;&#30340;7&#22825;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a parameter identification methodology of the SIRD model, an extension of the classical SIR model, that considers the deceased as a separate category. In addition, our model includes one parameter which is the ratio between the real total number of infected and the number of infected that were documented in the official statistics.  Due to many factors, like governmental decisions, several variants circulating, opening and closing of schools, the typical assumption that the parameters of the model stay constant for long periods of time is not realistic. Thus our objective is to create a method which works for short periods of time. In this scope, we approach the estimation relying on the previous 7 days of data and then use the identified parameters to make predictions.  To perform the estimation of the parameters we propose the average of an ensemble of neural networks. Each neural network is constructed based on a database built by solving the SIRD for 7 day
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#22238;&#39038;&#32463;&#39564;&#37325;&#25918;&#30340;&#31616;&#21333;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#30446;&#26631;&#20219;&#21153;&#65292;&#26080;&#38656;&#22797;&#26434;&#30340;&#22870;&#21169;&#24037;&#31243;&#12290;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#25968;&#25454;&#21644;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#27492;&#26041;&#27861;&#22312;OpenAI Gym Fetch Robotics&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2110.02414</link><description>&lt;p&gt;
&#34394;&#25311;&#22238;&#39038;&#32463;&#39564;&#37325;&#25918;&#65306;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#30340;&#22909;&#22855;&#27169;&#22411;&#22522;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imaginary Hindsight Experience Replay: Curious Model-based Learning for Sparse Reward Tasks. (arXiv:2110.02414v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02414
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#22238;&#39038;&#32463;&#39564;&#37325;&#25918;&#30340;&#31616;&#21333;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#30446;&#26631;&#20219;&#21153;&#65292;&#26080;&#38656;&#22797;&#26434;&#30340;&#22870;&#21169;&#24037;&#31243;&#12290;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#25968;&#25454;&#21644;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#27492;&#26041;&#27861;&#22312;OpenAI Gym Fetch Robotics&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#26395;&#24212;&#29992;&#20110;&#23454;&#38469;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#22240;&#20026;&#23427;&#30456;&#27604;&#20110;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#35774;&#35745;&#21644;&#23454;&#29616;&#22256;&#38590;&#30340;&#24418;&#29366;&#22870;&#21169;&#20449;&#21495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#30446;&#26631;&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#22870;&#21169;&#24037;&#31243;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#31216;&#20026;&#34394;&#25311;&#22238;&#39038;&#32463;&#39564;&#37325;&#25918;&#65292;&#36890;&#36807;&#23558;&#34394;&#25311;&#25968;&#25454;&#32435;&#20837;&#21040;&#31574;&#30053;&#26356;&#26032;&#20013;&#65292;&#20197;&#20943;&#23569;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#25913;&#21892;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#30340;&#25506;&#32034;&#65292;&#35813;&#31574;&#30053;&#37319;&#29992;&#26631;&#20934;&#30340;&#22238;&#39038;&#32463;&#39564;&#37325;&#25918;&#35757;&#32451;&#65292;&#24182;&#37197;&#22791;&#20102;&#22522;&#20110;&#22909;&#22855;&#24515;&#30340;&#20869;&#22312;&#22870;&#21169;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;OpenAI Gym Fetch Robotics&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#25968;&#25454;&#25928;&#29575;&#24179;&#22343;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#30456;&#27604;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning is a promising learning strategy for practical robotic applications due to its improved data-efficiency versus model-free counterparts. However, current state-of-the-art model-based methods rely on shaped reward signals, which can be difficult to design and implement. To remedy this, we propose a simple model-based method tailored for sparse-reward multi-goal tasks that foregoes the need for complicated reward engineering. This approach, termed Imaginary Hindsight Experience Replay, minimises real-world interactions by incorporating imaginary data into policy updates. To improve exploration in the sparse-reward setting, the policy is trained with standard Hindsight Experience Replay and endowed with curiosity-based intrinsic rewards. Upon evaluation, this approach provides an order of magnitude increase in data-efficiency on average versus the state-of-the-art model-free method in the benchmark OpenAI Gym Fetch Robotics tasks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26080;&#27880;&#24847;&#21147;&#30340;&#24207;&#21015;&#27169;&#22411;SANSformer&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#27979;&#65292;&#20805;&#20998;&#25366;&#25496;&#20102;Transformer&#22312;EHR&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;&#20027;&#35201;&#24212;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#21307;&#30103;&#36164;&#28304;&#21033;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#24739;&#32773;&#23376;&#32452;&#65292;&#22914;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2108.13672</link><description>&lt;p&gt;
SANSformers: &#26080;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models. (arXiv:2108.13672v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.13672
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#27880;&#24847;&#21147;&#30340;&#24207;&#21015;&#27169;&#22411;SANSformer&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#27979;&#65292;&#20805;&#20998;&#25366;&#25496;&#20102;Transformer&#22312;EHR&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;&#20027;&#35201;&#24212;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#21307;&#30103;&#36164;&#28304;&#21033;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#24739;&#32773;&#23376;&#32452;&#65292;&#22914;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#25361;&#25112;&#65292;&#30001;&#20110;EHR&#25968;&#25454;&#20855;&#26377;&#29420;&#29305;&#30340;&#22810;&#32500;&#39034;&#24207;&#32467;&#26500;&#65292;&#25152;&#20197;&#19982;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#30456;&#27604;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;Transformer&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#21644;&#25913;&#36827;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;EHR&#24212;&#29992;&#20013;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SANSformer&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#27880;&#24847;&#21147;&#24207;&#21015;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#20197;&#36866;&#24212;EHR&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#26159;&#39044;&#27979;&#26410;&#26469;&#30340;&#21307;&#30103;&#36164;&#28304;&#21033;&#29992;&#65292;&#36825;&#26159;&#26377;&#25928;&#20998;&#37197;&#21307;&#30103;&#36164;&#28304;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#24403;&#22788;&#29702;&#19981;&#21516;&#30340;&#24739;&#32773;&#23376;&#32452;&#26102;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#29305;&#21035;&#22256;&#38590;&#12290;&#36825;&#20123;&#34987;&#21807;&#19968;&#30340;&#20581;&#24247;&#36712;&#36857;&#25152;&#29305;&#24449;&#21270;&#30340;&#23376;&#32452;&#65292;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#65292;&#22914;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#65292;&#38656;&#35201;&#29305;&#27530;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#23376;&#32452;&#30340;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#20010;&#25972;&#20307;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Transformer neural networks to Electronic Health Records (EHR) is challenging due to the distinct, multidimensional sequential structure of EHR data, often leading to underperformance when compared to simpler linear models. Thus, the advantages of Transformers, such as efficient transfer learning and improved scalability are not fully exploited in EHR applications. To overcome these challenges, we introduce SANSformer, a novel attention-free sequential model designed specifically with inductive biases to cater for the unique characteristics of EHR data.  Our main application area is predicting future healthcare utilization, a crucial task for effectively allocating healthcare resources. This task becomes particularly difficult when dealing with divergent patient subgroups. These subgroups, characterized by unique health trajectories and often small in size, such as patients with rare diseases, require specialized modeling approaches. To address this, we adopt a self-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;&#20540;&#21450;&#20855;&#26377;&#24179;&#28369;&#35268;&#21010;&#39044;&#35328;&#26426;&#30340;&#949;-NE&#31574;&#30053;&#26041;&#38754;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2007.07461</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity. (arXiv:2007.07461v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.07461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;&#20540;&#21450;&#20855;&#26377;&#24179;&#28369;&#35268;&#21010;&#39044;&#35328;&#26426;&#30340;&#949;-NE&#31574;&#30053;&#26041;&#38754;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#34987;&#35748;&#20026;&#26159;RL&#30340;&#22522;&#30707;&#20043;&#19968;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#39564;&#27169;&#22411;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#23427;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;RL&#65288;MARL&#65289;&#65292;&#22240;&#20026;&#23427;&#33258;&#28982;&#22320;&#23558;&#23398;&#20064;&#21644;&#35268;&#21010;&#38454;&#27573;&#35299;&#32806;&#65292;&#24182;&#36991;&#20813;&#20102;&#22312;&#25152;&#26377;&#26234;&#33021;&#20307;&#21516;&#26102;&#20351;&#29992;&#26679;&#26412;&#25913;&#36827;&#31574;&#30053;&#26102;&#30340;&#38750;&#31283;&#24577;&#38382;&#39064;&#12290;&#23613;&#31649;&#30452;&#35266;&#19988;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22522;&#20110;&#27169;&#22411;&#30340;MARL&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#20851;&#20110;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#33021;&#26159;&#26368;&#22522;&#26412;&#30340;MARL&#35774;&#32622;&#65306;&#21482;&#33021;&#35775;&#38382;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#20004;&#20154;&#25240;&#25187;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;MARL&#22312;&#23547;&#25214;&#21040;&#26576;&#20010;&#949;&#35823;&#24046;&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#20540;&#20197;&#21450;&#20855;&#26377;&#24179;&#28369;&#35268;&#21010;&#39044;&#35328;&#26426;&#30340;&#949;-NE&#31574;&#30053;&#26041;&#38754;&#36798;&#21040;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde O(|S||A||B|(1-\gamma)^{-3}\epsilon^{-2})$&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#947;&#26159;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (RL), which finds an optimal policy using an empirical model, has long been recognized as one of the corner stones of RL. It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has not been fully investigated. In this paper, our goal is to address the fundamental question about its sample complexity. We study arguably the most basic MARL setting: two-player discounted zero-sum Markov games, given only access to a generative model. We show that model-based MARL achieves a sample complexity of $\tilde O(|S||A||B|(1-\gamma)^{-3}\epsilon^{-2})$ for finding the Nash equilibrium (NE) value up to some $\epsilon$ error, and the $\epsilon$-NE policies with a smooth planning oracle, where $\gamma$ is t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24182;&#34892;&#36817;&#31471;&#26041;&#27861;&#23454;&#29616;&#31232;&#30095;&#21644;&#20302;&#31209;&#39640;&#38454;&#24352;&#37327;&#22238;&#24402;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;$\ell_1$&#33539;&#25968;&#21644;&#24352;&#37327;&#26680;&#33539;&#25968;&#26469;&#20445;&#30041;&#24352;&#37327;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#39640;&#38454;&#32467;&#26500;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/1911.12965</link><description>&lt;p&gt;
&#31232;&#30095;&#21644;&#20302;&#31209;&#39640;&#38454;&#24352;&#37327;&#22238;&#24402;&#36890;&#36807;&#24182;&#34892;&#36817;&#31471;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse and Low-Rank High-Order Tensor Regression via Parallel Proximal Method. (arXiv:1911.12965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.12965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24182;&#34892;&#36817;&#31471;&#26041;&#27861;&#23454;&#29616;&#31232;&#30095;&#21644;&#20302;&#31209;&#39640;&#38454;&#24352;&#37327;&#22238;&#24402;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;$\ell_1$&#33539;&#25968;&#21644;&#24352;&#37327;&#26680;&#33539;&#25968;&#26469;&#20445;&#30041;&#24352;&#37327;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#39640;&#38454;&#32467;&#26500;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#20195;&#24212;&#29992;&#20013;&#29983;&#25104;&#20102;&#24352;&#37327;&#25968;&#25454;&#65288;&#25110;&#22810;&#32500;&#25968;&#32452;&#65289;&#65292;&#20363;&#22914;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20197;&#21450;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#35270;&#39057;&#12290;&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#39044;&#27979;&#24352;&#37327;&#29305;&#24449;&#19982;&#21333;&#21464;&#37327;&#21709;&#24212;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#35201;&#20040;&#20002;&#22833;&#20102;&#24352;&#37327;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#35201;&#20040;&#22312;&#22788;&#29702;&#39640;&#38454;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#26102;&#26102;&#38388;&#25104;&#26412;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#21644;&#20302;&#31209;&#24352;&#37327;&#22238;&#24402;&#65288;SLTR&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;$\ell_1$&#33539;&#25968;&#21644;&#24352;&#37327;&#26680;&#33539;&#25968;&#26469;&#24378;&#21046;&#24352;&#37327;&#31995;&#25968;&#30340;&#31232;&#30095;&#24615;&#21644;&#20302;&#31209;&#24615;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#24352;&#37327;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#20102;&#20351;&#27714;&#35299;&#36807;&#31243;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#65292;SLTR&#21033;&#29992;&#20102;&#36817;&#31471;&#26799;&#24230;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24182;&#34892;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SLTR&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, tensor data (or multidimensional array) have been generated in many modern applications, such as functional magnetic resonance imaging (fMRI) in neuroscience and videos in video analysis. Many efforts are made in recent years to predict the relationship between tensor features and univariate responses. However, previously proposed methods either lose structural information within tensor data or have prohibitively expensive time costs, especially for large-scale data with high-order structures. To address such problems, we propose the Sparse and Low-rank Tensor Regression (SLTR) model. Our model enforces sparsity and low-rankness of the tensor coefficient by directly applying $\ell_1$ norm and tensor nuclear norm, such that it preserves structural information of the tensor. To make the solving procedure scalable and efficient, SLTR makes use of the proximal gradient method, which can be easily implemented parallelly. We evaluate SLTR on several simulated datasets and one video
&lt;/p&gt;</description></item></channel></rss>