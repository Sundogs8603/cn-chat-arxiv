<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02012</link><description>&lt;p&gt;
EGC: &#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#19982;&#20998;&#31867;&#22270;&#20687;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02012
&lt;/p&gt;
&lt;p&gt;
EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30456;&#21516;&#30340;&#32593;&#32476;&#21442;&#25968;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#22312;&#19968;&#39033;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#21478;&#19968;&#39033;&#20219;&#21153;&#19978;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGC&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#20998;&#31867;&#22120;&#21644;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#22120;&#36755;&#20986;&#32473;&#23450;&#22270;&#20687;&#30340;&#26631;&#31614;&#65288;&#21363;&#26465;&#20214;&#20998;&#24067;$p(y|\mathbf{x})$&#65289;&#19981;&#21516;&#65292;EGC&#30340;&#21069;&#21521;&#20256;&#36882;&#22120;&#26159;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#23427;&#36755;&#20986;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;$p(\mathbf{x},y)$&#65292;&#22312;&#21518;&#21521;&#20256;&#36882;&#22120;&#20013;&#36890;&#36807;&#36793;&#32536;&#21270;&#26631;&#31614;$y$&#23454;&#29616;&#29983;&#25104;&#22120;&#12290;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#65292;&#20272;&#35745;&#32473;&#23450;&#22122;&#22768;&#22270;&#20687;&#30340;&#33021;&#37327;&#21644;&#20998;&#31867;&#27010;&#29575;&#65292;&#32780;&#22312;&#21518;&#21521;&#20256;&#36882;&#20013;&#65292;&#36890;&#36807;&#20272;&#35745;&#24471;&#20998;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#21435;&#22122;&#12290;EGC&#22312;ImageNet-1k&#12289;CelebA-HQ&#21644;LSUN Church&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-1k&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.02011</link><description>&lt;p&gt;
FakET: &#21033;&#29992;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#27169;&#25311;&#20919;&#20923;&#30005;&#23376;&#26029;&#23618;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#26159;&#35745;&#31639;&#26174;&#24494;&#23398;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#36825;&#20123;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#26159;&#19982;&#27169;&#25311;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#29289;&#29702;&#30340;&#22797;&#26434;&#25968;&#20540;&#27491;&#21521;&#27169;&#22411;&#20013;&#30340;&#31890;&#23376;&#27169;&#22411;&#32467;&#21512;&#29983;&#25104;&#30340;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#26426;&#23454;&#29616;&#38750;&#24120;&#32791;&#36153;&#35745;&#31639;&#36164;&#28304;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24050;&#32463;&#24314;&#31435;&#30340;&#29366;&#24577;&#20043;&#19968;&#23545;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#19982;&#22522;&#20934;&#27979;&#35797;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#36816;&#31639;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Particle localization and -classification constitute two of the most fundamental problems in computational microscopy. In recent years, deep learning based approaches have been introduced for these tasks with great success. A key shortcoming of these supervised learning methods is their need for large training data sets, typically generated from particle models in conjunction with complex numerical forward models simulating the physics of transmission electron microscopes. Computer implementations of such forward models are computationally extremely demanding and limit the scope of their applicability. In this paper we propose a simple method for simulating the forward operator of an electron microscope based on additive noise and Neural Style Transfer techniques. We evaluate the method on localization and classification tasks using one of the established state-of-the-art architectures showing performance on par with the benchmark. In contrast to previous approaches, our method acceler
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#31070;&#32463;&#24352;&#37327;&#32593;&#32476;&#65288;ANTN&#65289;&#26469;&#26725;&#25509;&#24352;&#37327;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#22810;&#20307;&#37327;&#23376;&#27169;&#25311;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#31934;&#24230;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.01996</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#31070;&#32463;&#24352;&#37327;&#32593;&#32476;: &#26725;&#25509;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#36827;&#34892;&#22810;&#20307;&#37327;&#23376;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Neural TensorNet: Bridging Neural Networks and Tensor Networks for Quantum Many-Body Simulation. (arXiv:2304.01996v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#31070;&#32463;&#24352;&#37327;&#32593;&#32476;&#65288;ANTN&#65289;&#26469;&#26725;&#25509;&#24352;&#37327;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#22810;&#20307;&#37327;&#23376;&#27169;&#25311;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#31934;&#24230;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20307;&#37327;&#23376;&#29289;&#29702;&#30340;&#27169;&#25311;&#23545;&#20110;&#29702;&#35299;&#22522;&#30784;&#31185;&#23398;&#21450;&#24212;&#29992;&#20110;&#37327;&#23376;&#26448;&#26009;&#35774;&#35745;&#21644;&#37327;&#23376;&#25216;&#26415;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#22823;&#23567;&#38543;&#31890;&#23376;&#25968;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#30452;&#25509;&#27169;&#25311;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#24352;&#37327;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#26159;&#36817;&#20284;&#27169;&#25311;&#30340;&#20004;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20294;&#22312;&#34920;&#36798;&#33021;&#21147;&#21644;&#20248;&#21270;&#26041;&#38754;&#21508;&#33258;&#26377;&#20854;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26550;&#26500;&#8212;&#8212;&#33258;&#22238;&#24402;&#31070;&#32463;&#24352;&#37327;&#32593;&#32476;&#65288;ANTN&#65289;&#65292;&#23427;&#26725;&#25509;&#20102;&#24352;&#37327;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ANTN&#29992;&#20110;&#21442;&#25968;&#21270;&#20855;&#26377;&#31934;&#30830;&#37319;&#26679;&#30340;&#24402;&#19968;&#21270;&#27874;&#20989;&#25968;&#65292;&#25193;&#23637;&#20102;&#24352;&#37327;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#21147;&#65292;&#32487;&#25215;&#20102;&#35768;&#22810;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#22312;&#20108;&#32500; $J_1$-$J_2$ &#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;ANTN&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24352;&#37327;&#32593;&#32476;&#26041;&#27861;&#65292;&#24182;&#22312;&#29616;&#26377;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#20013;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum many-body physics simulation has important impacts on understanding fundamental science and has applications to quantum materials design and quantum technology. However, due to the exponentially growing size of the Hilbert space with respect to the particle number, a direct simulation is intractable. While representing quantum states with tensor networks and neural networks are the two state-of-the-art methods for approximate simulations, each has its own limitations in terms of expressivity and optimization. To address these challenges, we develop a novel architecture, Autoregressive Neural TensorNet (ANTN), which bridges tensor networks and autoregressive neural networks. We show that Autoregressive Neural TensorNet parameterizes normalized wavefunctions with exact sampling, generalizes the expressivity of tensor networks and autoregressive neural networks, and inherits a variety of symmetries from autoregressive neural networks. We demonstrate our approach on the 2D $J_1$-$J
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#27169;&#22359;DWA&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#24182;&#25233;&#21046;&#22122;&#22768;&#12290;&#22312;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#38598;&#25104;DWA&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01994</link><description>&lt;p&gt;
DWA&#65306;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
DWA: Differential Wavelet Amplifier for Image Super-Resolution. (arXiv:2304.01994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#27169;&#22359;DWA&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#24182;&#25233;&#21046;&#22122;&#22768;&#12290;&#22312;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#38598;&#25104;DWA&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;(DWA)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;(SR)&#27169;&#22359;&#12290;DWA&#20026;&#26368;&#36817;&#25910;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#28151;&#21512;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;(DWT)&#26041;&#27861;&#27880;&#20837;&#27963;&#21147;&#12290;DWT&#33021;&#22815;&#26377;&#25928;&#22320;&#20026;SR&#25552;&#20379;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#31354;&#38388;&#38754;&#31215;&#20943;&#23569;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23567;&#20102;&#27169;&#22411;&#24635;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#25104;&#20026;&#21487;&#25345;&#32493;ML&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DWA&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#65292;&#24378;&#35843;&#23616;&#37096;&#23545;&#27604;&#24230;&#24182;&#25233;&#21046;&#36755;&#20837;&#20449;&#21495;&#20013;&#30340;&#24120;&#35265;&#22122;&#22768;&#12290;&#23558;&#20854;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;SR&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;DWA&#20351;DWSR&#21644;MWCNN&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#36755;&#20837;&#22270;&#20687;&#31354;&#38388;&#65292;&#22240;&#20026;&#23427;&#30465;&#30053;&#20102;DWT&#34920;&#31034;&#30340;&#36890;&#36947;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing the DWT representation channel-wise since it omits 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#30340;DTW&#31639;&#27861;&#20013;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;Flush+Reload&#25915;&#20987;&#36827;&#34892;&#31363;&#21462;&#12290;</title><link>http://arxiv.org/abs/2304.01990</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#20013;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Side Channel-Assisted Inference Leakage from Machine Learning-based ECG Classification. (arXiv:2304.01990v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#30340;DTW&#31639;&#27861;&#20013;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;Flush+Reload&#25915;&#20987;&#36827;&#34892;&#31363;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#27979;&#37327;&#30001;&#24515;&#33039;&#20135;&#29983;&#30340;&#30005;&#24615;&#24515;&#33039;&#27963;&#21160;&#65292;&#20197;&#26816;&#27979;&#24322;&#24120;&#24515;&#33039;&#36339;&#21160;&#21644;&#24515;&#33039;&#30149;&#21457;&#20316;&#12290;&#28982;&#32780;&#65292;&#24322;&#24120;&#21457;&#29983;&#30340;&#19981;&#35268;&#21017;&#24615;&#35201;&#27714;&#23545;&#24515;&#36339;&#36827;&#34892;&#25345;&#32493;&#30417;&#27979;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34987;&#21033;&#29992;&#26469;&#33258;&#21160;&#21270;&#20219;&#21153;&#65292;&#20197;&#20943;&#23569;&#30417;&#27979;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#20844;&#21496;&#25512;&#20986;&#20102;&#24102;&#26377;ECG&#30417;&#27979;&#21644;&#24322;&#24120;&#24515;&#36339;&#35686;&#25253;&#21151;&#33021;&#30340;&#20135;&#21697;&#12290;&#22312;&#25152;&#26377;&#20998;&#31867;&#31639;&#27861;&#20013;&#65292;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#31639;&#27861;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#25191;&#34892;ECG&#20998;&#31867;&#20219;&#21153;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;DTW&#30340;ECG&#20998;&#31867;&#20063;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#21363;&#27844;&#38706;&#24739;&#32773;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20391;&#20449;&#36947;&#25915;&#20987;Flush+Reload&#21487;&#20197;&#31363;&#21462;ECG&#36755;&#20837;&#26679;&#26412;&#30340;&#26631;&#31614;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#35748;&#20102;ECG&#20998;&#31867;&#20013;DTW&#30340;&#28431;&#27934;&#65292;&#21363;&#21464;&#24418;&#36335;&#24452;&#36873;&#25321;&#19982;ECG&#30340;&#20998;&#31867;&#32467;&#26524;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Electrocardiogram (ECG) measures the electrical cardiac activity generated by the heart to detect abnormal heartbeat and heart attack. However, the irregular occurrence of the abnormalities demands continuous monitoring of heartbeats. Machine learning techniques are leveraged to automate the task to reduce labor work needed during monitoring. In recent years, many companies have launched products with ECG monitoring and irregular heartbeat alert. Among all classification algorithms, the time series-based algorithm dynamic time warping (DTW) is widely adopted to undertake the ECG classification task. Though progress has been achieved, the DTW-based ECG classification also brings a new attacking vector of leaking the patients' diagnosis results. This paper shows that the ECG input samples' labels can be stolen via a side-channel attack, Flush+Reload. In particular, we first identify the vulnerability of DTW for ECG classification, i.e., the correlation between warping path choice and
&lt;/p&gt;</description></item><item><title>ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01973</link><description>&lt;p&gt;
ERM++&#65306;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ERM++: An Improved Baseline for Domain Generalization. (arXiv:2304.01973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01973
&lt;/p&gt;
&lt;p&gt;
ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#22495;&#36890;&#29992;&#24615;&#65288;DG&#65289;&#34913;&#37327;&#20998;&#31867;&#22120;&#23545;&#20110;&#23427;&#27809;&#26377;&#25509;&#21463;&#36807;&#35757;&#32451;&#30340;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#20010;&#35757;&#32451;&#22495;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#28304;DG&#26041;&#27861;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22495;&#26631;&#31614;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#22312;&#28304;&#22495;&#19978;&#31616;&#21333;&#22320;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#21487;&#20197;&#32988;&#36807;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;DG&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#20505;&#36873;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ERM&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#31216;&#20026;ERM ++&#65292;&#24182;&#23637;&#31034;&#23427;&#30456;&#23545;&#20110;&#26631;&#20934;ERM&#22312;&#20116;&#20010;&#22810;&#28304;&#25968;&#25454;&#38598;&#19978;&#23558;DG&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#20102;5&#65285;&#20197;&#19978;&#65292;&#24182;&#19988;&#23613;&#31649;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#20294;&#20987;&#36133;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;ERM ++&#22312;WILDS-FMOW&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-source Domain Generalization (DG) measures a classifier's ability to generalize to new distributions of data it was not trained on, given several training domains. While several multi-source DG methods have been proposed, they incur additional complexity during training by using domain labels. Recent work has shown that a well-tuned Empirical Risk Minimization (ERM) training procedure, that is simply minimizing the empirical risk on the source domains, can outperform most existing DG methods. We identify several key candidate techniques to further improve ERM performance, such as better utilization of training data, model parameter selection, and weight-space regularization. We call the resulting method ERM++, and show it significantly improves the performance of DG on five multi-source datasets by over 5% compared to standard ERM, and beats state-of-the-art despite being less computationally expensive. Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20462;&#27491;&#30340;&#23398;&#20064;&#20027;&#23545;&#20598;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#24555;&#36895;&#26377;&#38480;&#35270;&#35282;&#20809;&#22768;&#23618;&#26512;&#25104;&#20687;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#20849;&#21516;&#23398;&#20064;&#27169;&#22411;&#20462;&#27491;&#19982;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#23398;&#20064;&#26356;&#26032;&#25805;&#20316;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#30340;&#21069;&#21521;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#19968;&#23450;&#30340;&#29702;&#35770;&#25351;&#23548;&#24847;&#20041;&#21644;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.01963</link><description>&lt;p&gt;
&#27169;&#22411;&#20462;&#27491;&#30340;&#23398;&#20064;&#20027;&#23545;&#20598;&#27169;&#22411;&#29992;&#20110;&#24555;&#36895;&#26377;&#38480;&#35270;&#35282;&#20809;&#22768;&#23618;&#26512;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Model-corrected learned primal-dual models for fast limited-view photoacoustic tomography. (arXiv:2304.01963v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20462;&#27491;&#30340;&#23398;&#20064;&#20027;&#23545;&#20598;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#24555;&#36895;&#26377;&#38480;&#35270;&#35282;&#20809;&#22768;&#23618;&#26512;&#25104;&#20687;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#20849;&#21516;&#23398;&#20064;&#27169;&#22411;&#20462;&#27491;&#19982;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#23398;&#20064;&#26356;&#26032;&#25805;&#20316;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#30340;&#21069;&#21521;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#19968;&#23450;&#30340;&#29702;&#35770;&#25351;&#23548;&#24847;&#20041;&#21644;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36845;&#20195;&#37325;&#24314;&#22312;&#32463;&#39564;&#40065;&#26834;&#24615;&#21644;&#21152;&#36895;&#23618;&#26512;&#25104;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#20255;&#22823;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#20854;&#22312;&#20809;&#22768;&#23618;&#26512;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38656;&#35201;&#21453;&#22797;&#35780;&#20272;&#35745;&#31639;&#24320;&#38144;&#21069;&#21521;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#36817;&#20284;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#35745;&#31639;&#21487;&#34892;&#24615;&#65292;&#20294;&#38656;&#35201;&#34917;&#20607;&#27169;&#22411;&#35823;&#24046;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#27169;&#22411;&#20462;&#27491;&#23884;&#20837;&#21040;&#23398;&#20064;&#20027;&#23545;&#20598;&#26694;&#26550;&#20013;&#65292;&#25512;&#36827;&#20102;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#20013;&#27169;&#22411;&#20462;&#27491;&#30340;&#26041;&#27861;&#35770;&#21644;&#29702;&#35770;&#22522;&#30784;&#12290;&#22312;&#26410;&#32463;&#23637;&#24320;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#36845;&#20195;&#37325;&#24314;&#26041;&#27861;&#20013;&#65292;&#27169;&#22411;&#20462;&#27491;&#19982;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#23398;&#20064;&#26356;&#26032;&#25805;&#20316;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#20849;&#21516;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#20844;&#24335;&#20801;&#35768;&#25193;&#23637;&#21040;&#20027;&#23545;&#20598;&#28145;&#24179;&#34913;&#27169;&#22411;&#65292;&#25552;&#20379;&#22266;&#23450;&#28857;&#25910;&#25947;&#20197;&#21450;&#20943;&#23569;&#35757;&#32451;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#24555;&#36895;&#26377;&#38480;&#35270;&#35282;&#20809;&#22768;&#23618;&#26512;&#25104;&#20687;&#19978;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#24615;&#33021;&#21644;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned iterative reconstructions hold great promise to accelerate tomographic imaging with empirical robustness to model perturbations. Nevertheless, an adoption for photoacoustic tomography is hindered by the need to repeatedly evaluate the computational expensive forward model. Computational feasibility can be obtained by the use of fast approximate models, but a need to compensate model errors arises. In this work we advance the methodological and theoretical basis for model corrections in learned image reconstructions by embedding the model correction in a learned primal-dual framework. Here, the model correction is jointly learned in data space coupled with a learned updating operator in image space within an unrolled end-to-end learned iterative reconstruction approach. The proposed formulation allows an extension to a primal-dual deep equilibrium model providing fixed-point convergence as well as reduced memory requirements for training. We provide theoretical and empirical ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#23545;&#25239;&#39118;&#26684;&#25200;&#21160;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23545;&#25239;&#24615;&#25200;&#21160;&#29305;&#24449;&#39118;&#26684;&#36798;&#21040;&#22495;&#27867;&#21270;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#32467;&#21512;&#28151;&#21512;&#21407;&#22987;&#29305;&#24449;&#30340;&#26041;&#27861;&#32531;&#35299;&#25200;&#21160;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.01959</link><description>&lt;p&gt;
&#38543;&#26426;&#23545;&#25239;&#39118;&#26684;&#25200;&#21160;&#25216;&#26415;&#29992;&#20110;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Randomized Adversarial Style Perturbations for Domain Generalization. (arXiv:2304.01959v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#23545;&#25239;&#39118;&#26684;&#25200;&#21160;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23545;&#25239;&#24615;&#25200;&#21160;&#29305;&#24449;&#39118;&#26684;&#36798;&#21040;&#22495;&#27867;&#21270;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#32467;&#21512;&#28151;&#21512;&#21407;&#22987;&#29305;&#24449;&#30340;&#26041;&#27861;&#32531;&#35299;&#25200;&#21160;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#31216;&#20026;&#38543;&#26426;&#23545;&#25239;&#39118;&#26684;&#25200;&#21160;&#25216;&#26415;&#65288;RASP&#65289;&#65292;&#20854;&#21160;&#26426;&#22312;&#20110;&#29305;&#24449;&#32479;&#35745;&#23398;&#25429;&#25417;&#21040;&#27599;&#20010;&#22495;&#30340;&#29305;&#24449;&#12290;&#35813;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#26041;&#21521;&#19978;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#30340;&#39118;&#26684;&#65292;&#26397;&#30528;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#31867;&#21035;&#26041;&#21521;&#24182;&#20351;&#27169;&#22411;&#23398;&#20064;&#36991;&#20813;&#34987;&#22312;&#26410;&#30693;&#30446;&#26631;&#22495;&#20013;&#35266;&#23519;&#21040;&#30340;&#24847;&#22806;&#39118;&#26684;&#25152;&#35823;&#23548;&#12290;&#34429;&#28982;RASP&#33021;&#26377;&#25928;&#22788;&#29702;&#22495;&#28418;&#31227;&#65292;&#20294;&#23427;&#30340;&#31616;&#21333;&#34701;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#38477;&#20302;&#20174;&#28304;&#22495;&#23398;&#20064;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#24182;&#19981;&#38480;&#21046;&#34920;&#24449;&#30340;&#25200;&#21160;&#12290;&#36825;&#20010;&#25361;&#25112;&#30001;&#35268;&#19968;&#21270;&#29305;&#24449;Mixup&#65288;NFM&#65289;&#32531;&#35299;&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#28151;&#21512;&#26469;&#20419;&#36827;&#21407;&#22987;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#23545;&#25200;&#21160;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel domain generalization technique, referred to as Randomized Adversarial Style Perturbation (RASP), which is motivated by the observation that the characteristics of each domain are captured by the feature statistics corresponding to style. The proposed algorithm perturbs the style of a feature in an adversarial direction towards a randomly selected class, and makes the model learn against being misled by the unexpected styles observed in unseen target domains. While RASP is effective to handle domain shifts, its naive integration into the training procedure might degrade the capability of learning knowledge from source domains because it has no restriction on the perturbations of representations. This challenge is alleviated by Normalized Feature Mixup (NFM), which facilitates the learning of the original features while achieving robustness to perturbed representations via their mixup during training. We evaluate the proposed algorithm via extensive experiments on var
&lt;/p&gt;</description></item><item><title>TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.01951</link><description>&lt;p&gt;
TransPimLib&#65306;&#29992;&#20110;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01951
&lt;/p&gt;
&lt;p&gt;
TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#65288;PIM&#65289;&#25215;&#35834;&#20943;&#36731;&#29616;&#20195;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30495;&#23454;PIM&#31995;&#32479;&#26377;&#19968;&#20010;&#20869;&#22312;&#30340;&#21155;&#21183;&#65292;&#21363;&#23427;&#20204;&#30340;&#30828;&#20214;&#27604;&#20256;&#32479;&#30340;&#22788;&#29702;&#22120;&#65288;CPU&#12289;GPU&#65289;&#26356;&#21152;&#21463;&#38480;&#65292;&#22240;&#20026;&#22312;&#20869;&#23384;&#38468;&#36817;&#25110;&#20869;&#37096;&#26500;&#24314;&#22788;&#29702;&#20803;&#20214;&#30340;&#38590;&#24230;&#21644;&#25104;&#26412;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#36890;&#29992;&#30340;PIM&#26550;&#26500;&#25903;&#25345;&#30456;&#24403;&#26377;&#38480;&#30340;&#25351;&#20196;&#38598;&#65292;&#24182;&#19988;&#38590;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;&#36229;&#36234;&#20989;&#25968;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#30340;&#25805;&#20316;&#65288;&#20363;&#22914;&#24179;&#26041;&#26681;&#65289;&#12290;&#36825;&#20123;&#25805;&#20316;&#23545;&#20110;&#19968;&#20123;&#29616;&#20195;&#24037;&#20316;&#36127;&#36733;&#23588;&#20854;&#37325;&#35201;&#65292;&#20363;&#22914;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#20026;&#20102;&#22312;&#36890;&#29992;&#30340;PIM&#31995;&#32479;&#20013;&#25552;&#20379;&#23545;&#36229;&#36234;&#65288;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#65289;&#20989;&#25968;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TransPimLib&#65292;&#36825;&#26159;&#19968;&#20010;&#24211;&#65292;&#25552;&#20379;&#22522;&#20110;CORDIC&#21644;LUT&#30340;&#19977;&#35282;&#20989;&#25968;&#12289;&#21452;&#26354;&#20989;&#25968;&#12289;&#25351;&#25968;&#12289;&#23545;&#25968;&#12289;&#24179;&#26041;&#26681;&#31561;&#38590;&#20197;&#35745;&#31639;&#30340;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#28151;&#21512;&#26597;&#35810;&#65292;&#26082;&#26377;&#21521;&#37327;&#30456;&#20284;&#24230;&#25628;&#32034;&#65292;&#21448;&#26377;&#19982;&#24213;&#23618;&#25968;&#25454;&#21521;&#37327;&#30456;&#20851;&#30340;&#20851;&#31995;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01926</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#39640;&#21534;&#21520;&#37327;&#21521;&#37327;&#30456;&#20284;&#24230;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
High-Throughput Vector Similarity Search in Knowledge Graphs. (arXiv:2304.01926v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#28151;&#21512;&#26597;&#35810;&#65292;&#26082;&#26377;&#21521;&#37327;&#30456;&#20284;&#24230;&#25628;&#32034;&#65292;&#21448;&#26377;&#19982;&#24213;&#23618;&#25968;&#25454;&#21521;&#37327;&#30456;&#20851;&#30340;&#20851;&#31995;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#22320;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#21521;&#37327;&#20197;&#25552;&#20379;&#22312;&#32447;&#25512;&#33616;&#21644;&#25628;&#32034;&#29992;&#20363;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#20351;&#29992;&#21576;&#36882;&#22686;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#25552;&#20379;&#20102;&#22312;&#32447;&#21521;&#37327;&#30456;&#20284;&#24230;&#25628;&#32034;&#30340;&#26597;&#35810;&#22788;&#29702;&#22686;&#24378;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#36827;&#34892;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#26041;&#27861;&#12290;&#22312;&#26597;&#35810;&#24050;&#26377;KG&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30456;&#20851;&#30340;KG&#26597;&#35810;&#21644;&#23454;&#20307;&#20219;&#21153;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28151;&#21512;&#21521;&#37327;&#30456;&#20284;&#24230;&#25628;&#32034;&#65288;&#28151;&#21512;&#26597;&#35810;&#65289;&#65292;&#20854;&#20013;&#26597;&#35810;&#30340;&#19968;&#37096;&#20998;&#23545;&#24212;&#21521;&#37327;&#30456;&#20284;&#24230;&#25628;&#32034;&#65292;&#32780;&#26597;&#35810;&#30340;&#21478;&#19968;&#37096;&#20998;&#23545;&#24212;&#19982;&#24213;&#23618;&#25968;&#25454;&#21521;&#37327;&#30456;&#20851;&#30340;&#20851;&#31995;&#23646;&#24615;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#36807;&#21435;&#20026;&#27468;&#26354;&#23454;&#20307;&#30340;KG&#26597;&#35810;&#65292;&#25105;&#20204;&#24076;&#26395;&#26500;&#24314;&#26032;&#30340;&#26597;&#35810;&#65292;&#20197;&#23547;&#25214;&#21521;&#37327;&#34920;&#31034;&#25509;&#36817;&#36807;&#21435;KG&#26597;&#35810;&#20013;&#30340;&#23454;&#20307;&#21521;&#37327;&#34920;&#31034;&#30340;&#26032;&#27468;&#26354;&#23454;&#20307;&#12290;&#20294;&#26159;&#65292;&#22312;KG&#20013;&#30340;&#23454;&#20307;&#20063;&#20855;&#26377;&#38750;&#21521;&#37327;&#23646;&#24615;&#65292;&#20363;&#22914;&#19982;&#33402;&#26415;&#23478;&#30456;&#20851;&#30340;&#27468;&#26354;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing adoption of machine learning for encoding data into vectors to serve online recommendation and search use cases. As a result, recent data management systems propose augmenting query processing with online vector similarity search. In this work, we explore vector similarity search in the context of Knowledge Graphs (KGs). Motivated by the tasks of finding related KG queries and entities for past KG query workloads, we focus on hybrid vector similarity search (hybrid queries for short) where part of the query corresponds to vector similarity search and part of the query corresponds to predicates over relational attributes associated with the underlying data vectors. For example, given past KG queries for a song entity, we want to construct new queries for new song entities whose vector representations are close to the vector representation of the entity in the past KG query. But entities in a KG also have non-vector attributes such as a song associated with an arti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110; Massive MIMO CSI &#21453;&#39304;&#30340;&#21152;&#36895;&#21644;&#21387;&#32553;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#32593;&#32476;&#20462;&#21098;&#12289;&#35757;&#32451;&#21518;&#21160;&#24577;&#33539;&#22260;&#37327;&#21270;&#21644;&#26435;&#37325;&#32858;&#31867;&#31561;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.01914</link><description>&lt;p&gt;
&#21152;&#36895;&#21644;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110; Massive MIMO CSI &#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Accelerating and Compressing Deep Neural Networks for Massive MIMO CSI Feedback. (arXiv:2304.01914v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110; Massive MIMO CSI &#21453;&#39304;&#30340;&#21152;&#36895;&#21644;&#21387;&#32553;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#32593;&#32476;&#20462;&#21098;&#12289;&#35757;&#32451;&#21518;&#21160;&#24577;&#33539;&#22260;&#37327;&#21270;&#21644;&#26435;&#37325;&#32858;&#31867;&#31561;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#27493;&#20351;&#24471;&#23427;&#20204;&#25104;&#20026;&#20102;&#26080;&#32447;&#36890;&#20449;&#30340;&#20505;&#36873;&#32773;&#65292;&#22914;&#20449;&#36947;&#20272;&#35745;&#12289;&#35299;&#30721;&#21644;&#19979;&#34892;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21387;&#32553;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#24222;&#22823;&#21644;&#20302;&#25928;&#30340;&#65292;&#36825;&#25104;&#20026;&#23427;&#20204;&#22312;&#38656;&#35201;&#20010;&#21035;&#32593;&#32476;&#21151;&#33021;&#20302;&#24310;&#36831;&#21644;&#20302;&#20869;&#23384;&#21344;&#29992;&#30340;&#23454;&#38469;&#26080;&#32447;&#31995;&#32479;&#37096;&#32626;&#20013;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110; Massive MIMO CSI &#21453;&#39304;&#30340;&#21152;&#36895;&#21644;&#21387;&#32553;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#37319;&#29992;&#32593;&#32476;&#20462;&#21098;&#12289;&#35757;&#32451;&#21518;&#21160;&#24577;&#33539;&#22260;&#37327;&#21270;&#21644;&#26435;&#37325;&#32858;&#31867;&#26469;&#20248;&#21270; Massive MIMO &#31995;&#32479;&#30340; CSI &#21453;&#39304;&#21387;&#32553;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#21830;&#29992;&#30828;&#20214;&#19978;&#37096;&#32626;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20026;&#23454;&#29616;&#25512;&#29702;&#22686;&#30410;&#65292;&#38656;&#35201;&#21152;&#36895;&#31232;&#30095;&#32593;&#32476;&#35745;&#31639;&#30340;&#19987;&#29992;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in machine learning and deep neural networks have made them attractive candidates for wireless communications functions such as channel estimation, decoding, and downlink channel state information (CSI) compression. However, most of these neural networks are large and inefficient making it a barrier for deployment in practical wireless systems that require low-latency and low memory footprints for individual network functions. To mitigate these limitations, we propose accelerated and compressed efficient neural networks for massive MIMO CSI feedback. Specifically, we have thoroughly investigated the adoption of network pruning, post-training dynamic range quantization, and weight clustering to optimize CSI feedback compression for massive MIMO systems. Furthermore, we have deployed the proposed model compression techniques on commodity hardware and demonstrated that in order to achieve inference gains, specialized libraries that accelerate computations for sparse ne
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#23454;&#38469;&#20013;&#26356;&#23569;&#36935;&#21040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#24182;&#35777;&#26126;&#26041;&#24046;&#20027;&#35201;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#25152;&#23548;&#33268;&#12290;</title><link>http://arxiv.org/abs/2304.01910</link><description>&lt;p&gt;
&#26657;&#20934;&#28151;&#20081;&#65306;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#26080;&#24847;&#20013;&#19988;&#26080;&#23475;
&lt;/p&gt;
&lt;p&gt;
Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable. (arXiv:2304.01910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01910
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#23454;&#38469;&#20013;&#26356;&#23569;&#36935;&#21040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#24182;&#35777;&#26126;&#26041;&#24046;&#20027;&#35201;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#25152;&#23548;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#22312;&#37325;&#22797;&#27979;&#35797;&#26102;&#20250;&#26377;&#26174;&#33879;&#30340;&#27979;&#35797;&#38598;&#24615;&#33021;&#24046;&#24322;&#65292;&#24433;&#21709;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#27604;&#36739;&#21644;&#35757;&#32451;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#27604;&#36739;&#26469;&#35299;&#37322;&#36825;&#31181;&#21464;&#21270;&#65306;&#65288;1&#65289;&#23613;&#31649;&#22312;&#27979;&#35797;&#38598;&#19978;&#26377;&#26174;&#33879;&#30340;&#26041;&#24046;&#65292;&#20294;&#26631;&#20934;&#30340;CIFAR-10&#19982;ImageNet&#35757;&#32451;&#22312;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#34920;&#29616;&#21364;&#38750;&#24120;&#19968;&#33268;&#65292;&#36825;&#34920;&#26126;&#26041;&#24046;&#19981;&#20687;&#20043;&#21069;&#24819;&#35937;&#30340;&#37027;&#20040;&#20005;&#37325;&#12290;&#65288;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#65292;&#20197;&#32039;&#23494;&#36817;&#20284;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#20998;&#24067;&#32467;&#26500;&#12290;&#65288;3&#65289;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#20197;&#19979;&#20004;&#20010;&#24847;&#20041;&#19978;&#65292;&#27979;&#35797;&#38598;&#26041;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#24046;&#20027;&#35201;&#26159;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#38543;&#26426;&#28304;&#65288;&#22914;&#25968;&#25454;&#25490;&#24207;&#21644;&#25193;&#20805;&#65289;&#25152;&#23548;&#33268;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26041;&#24046;&#26159;&#39057;&#29575;&#26497;&#38480;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#26469;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical neural network trainings have substantial variance in test-set performance between repeated runs, impeding hyperparameter comparison and training reproducibility. We present the following results towards understanding this variation. (1) Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have very little variance in their performance on the test-distributions from which those test-sets are sampled, suggesting that variance is less of a practical issue than previously thought. (2) We present a simplifying statistical assumption which closely approximates the structure of the test-set accuracy distribution. (3) We argue that test-set variance is inevitable in the following two senses. First, we show that variance is largely caused by high sensitivity of the training process to initial conditions, rather than by specific sources of randomness like the data order and augmentations. Second, we prove that variance is u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;Deepfake&#26816;&#27979;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21028;&#26029;Deepfake&#30340;&#19981;&#21516;&#25216;&#26415;&#65292;&#24182;&#26088;&#22312;&#23454;&#29616;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01908</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;Deepfake&#26816;&#27979;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Leveraging Deep Learning Approaches for Deepfake Detection: A Review. (arXiv:2304.01908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;Deepfake&#26816;&#27979;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21028;&#26029;Deepfake&#30340;&#19981;&#21516;&#25216;&#26415;&#65292;&#24182;&#26088;&#22312;&#23454;&#29616;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26174;&#33879;&#36827;&#23637;&#20351;&#24471;&#39640;&#24230;&#36924;&#30495;&#30340;&#34394;&#20551;&#23186;&#20307;&#8212;&#8212;Deepfake&#65288;&#28145;&#24230;&#20266;&#36896;&#65289;&#30340;&#20986;&#29616;&#25104;&#20026;&#21487;&#33021;&#12290;Deepfake&#26159;&#36890;&#36807;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#34394;&#20551;&#23186;&#20307;&#65292;&#26377;&#26102;&#24456;&#38590;&#19982;&#30495;&#23454;&#23186;&#20307;&#21306;&#20998;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#31181;&#23186;&#20307;&#21487;&#20197;&#19978;&#20256;&#21040;&#21508;&#31181;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#22240;&#27492;&#23459;&#20256;&#21464;&#24471;&#23481;&#26131;&#65292;&#36825;&#38656;&#35201;&#37319;&#21462;&#26377;&#25928;&#30340;&#23545;&#31574;&#12290;&#22240;&#27492;&#65292;&#23545;Deepfake&#30340;&#20048;&#35266;&#23545;&#31574;&#20043;&#19968;&#23558;&#26159;Deepfake&#26816;&#27979;&#12290;&#20026;&#20102;&#23545;&#20184;&#36825;&#19968;&#23041;&#32961;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#20154;&#21592;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#31561;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21019;&#24314;&#20102;&#26816;&#27979;Deepfake&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#26356;&#39640;&#20934;&#30830;&#24615;&#30340;&#25104;&#26412;&#26377;&#25928;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conspicuous progression in the field of machine learning and deep learning have led the jump of highly realistic fake media, these media oftentimes referred as deepfakes. Deepfakes are fabricated media which are generated by sophisticated AI that are at times very difficult to set apart from the real media. So far, this media can be uploaded to the various social media platforms, hence advertising it to the world got easy, calling for an efficacious countermeasure. Thus, one of the optimistic counter steps against deepfake would be deepfake detection. To undertake this threat, researchers in the past have created models to detect deepfakes based on ML/DL techniques like Convolutional Neural Networks. This paper aims to explore different methodologies with an intention to achieve a cost-effective model with a higher accuracy with different types of the datasets, which is to address the generalizability of the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026; Torch-Choice &#30340; PyTorch &#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#31649;&#29702;&#25968;&#25454;&#24211;&#12289;&#26500;&#24314;&#22810;&#39033;&#24335;Logit&#21644;&#23884;&#22871;Logit&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;GPU&#21152;&#36895;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01906</link><description>&lt;p&gt;
Torch-Choice: &#29992;Python&#23454;&#29616;&#22823;&#35268;&#27169;&#36873;&#25321;&#24314;&#27169;&#30340;PyTorch&#21253;
&lt;/p&gt;
&lt;p&gt;
Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python. (arXiv:2304.01906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026; Torch-Choice &#30340; PyTorch &#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#31649;&#29702;&#25968;&#25454;&#24211;&#12289;&#26500;&#24314;&#22810;&#39033;&#24335;Logit&#21644;&#23884;&#22871;Logit&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;GPU&#21152;&#36895;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$\texttt{torch-choice}$ &#26159;&#19968;&#27454;&#24320;&#28304;&#36719;&#20214;&#21253;&#65292;&#20351;&#29992;Python&#21644;PyTorch&#23454;&#29616;&#28789;&#27963;&#12289;&#24555;&#36895;&#30340;&#36873;&#25321;&#24314;&#27169;&#12290;&#23427;&#25552;&#20379;&#20102; $\texttt{ChoiceDataset}$ &#25968;&#25454;&#32467;&#26500;&#65292;&#20197;&#20415;&#28789;&#27963;&#32780;&#39640;&#25928;&#22320;&#31649;&#29702;&#25968;&#25454;&#24211;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20174;&#21508;&#31181;&#26684;&#24335;&#30340;&#25968;&#25454;&#24211;&#20013;&#26500;&#24314; $\texttt{ChoiceDataset}$&#65292;&#24182;&#23637;&#31034;&#20102; $\texttt{ChoiceDataset}$ &#30340;&#21508;&#31181;&#21151;&#33021;&#12290;&#35813;&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#20004;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;: &#22810;&#39033;&#24335;Logit&#21644;&#23884;&#22871;Logit&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#27169;&#22411;&#20272;&#35745;&#26399;&#38388;&#30340;&#27491;&#21017;&#21270;&#12290;&#35813;&#36719;&#20214;&#21253;&#36824;&#25903;&#25345;&#20351;&#29992;GPU&#36827;&#34892;&#20272;&#35745;&#65292;&#20351;&#20854;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32780;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;R&#39118;&#26684;&#30340;&#20844;&#24335;&#23383;&#31526;&#20018;&#25110;Python&#23383;&#20856;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102; $\texttt{torch-choice}$ &#21644; R&#20013;&#30340; $\texttt{mlogit}$ &#22312;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#30340;&#35745;&#31639;&#25928;&#29575;: (1) &#35266;&#27979;&#25968;&#22686;&#21152;&#26102;&#65292;(2) &#21327;&#21464;&#37327;&#20010;&#25968;&#22686;&#21152;&#26102;&#65292; (3) &#27979;&#35797;&#25968;&#21319;&#39640;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $\texttt{torch-choice}$ is an open-source library for flexible, fast choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a $\texttt{ChoiceDataset}$ data structure to manage databases flexibly and memory-efficiently. The paper demonstrates constructing a $\texttt{ChoiceDataset}$ from databases of various formats and functionalities of $\texttt{ChoiceDataset}$. The package implements two widely used models, namely the multinomial logit and nested logit models, and supports regularization during model estimation. The package incorporates the option to take advantage of GPUs for estimation, allowing it to scale to massive datasets while being computationally efficient. Models can be initialized using either R-style formula strings or Python dictionaries. We conclude with a comparison of the computational efficiencies of $\texttt{torch-choice}$ and $\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the number of covariates increases, and (3) th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#35745;&#31639;&#36335;&#24452;&#65292;&#20174;&#32780;&#25552;&#39640;&#21796;&#37266;&#35789;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;90&#65285;&#32780;&#20165;&#22686;&#21152;1&#65285;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20013;&#22823;&#26377;&#35048;&#30410;&#12290;</title><link>http://arxiv.org/abs/2304.01905</link><description>&lt;p&gt;
&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#26102;&#30340;&#39640;&#25928;&#21796;&#37266;&#35789;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#35745;&#31639;&#36335;&#24452;&#65292;&#20174;&#32780;&#25552;&#39640;&#21796;&#37266;&#35789;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;90&#65285;&#32780;&#20165;&#22686;&#21152;1&#65285;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20013;&#22823;&#26377;&#35048;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#39640;&#21796;&#37266;&#35789;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#24182;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#21033;&#29992;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#21738;&#20010;&#27880;&#24847;&#21147;&#32593;&#32476;&#25191;&#34892;&#36755;&#20837;&#38899;&#39057;&#24103;&#30340;&#36816;&#34892;&#26102;&#35745;&#31639;&#36335;&#24452;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#20316;&#32773;&#26377;&#25928;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#30340;&#36816;&#34892;&#26102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#20351;&#29992;&#20316;&#32773;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#26102;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21452;&#20851;&#27880;&#32593;&#32476;&#21487;&#20197;&#23558;&#21796;&#37266;&#35789;&#38899;&#39057;&#24103;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;$90\%$&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#20165;&#22686;&#21152;$1\%$&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;F1&#24471;&#20998;$16\%$&#65292;&#24182;&#23558;&#19968;&#33324;&#30340;&#32597;&#35265;&#35789;&#38169;&#35823;&#29575;&#25552;&#39640;&#20102;$3\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\%$ for WW audio frames, with only $1\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\%$ relative and improves generic rare word error rate by $3\%$ relative compared to the baselines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#23548;&#36712;&#36857;&#25193;&#25955;&#25511;&#21046;&#30340;&#34892;&#20154;&#21160;&#30011;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#34892;&#20154;&#36712;&#36857;&#21644;&#20840;&#36523;&#21160;&#30011;&#30340;&#31934;&#20934;&#25511;&#21046;&#21644;&#27169;&#25311;&#65292;&#20026;&#29305;&#23450;&#22330;&#26223;&#30340;&#22788;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.01893</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#36712;&#36857;&#25193;&#25955;&#25511;&#21046;&#30340;&#34892;&#20154;&#21160;&#30011;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion. (arXiv:2304.01893v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01893
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#36712;&#36857;&#25193;&#25955;&#25511;&#21046;&#30340;&#34892;&#20154;&#21160;&#30011;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#34892;&#20154;&#36712;&#36857;&#21644;&#20840;&#36523;&#21160;&#30011;&#30340;&#31934;&#20934;&#25511;&#21046;&#21644;&#27169;&#25311;&#65292;&#20026;&#29305;&#23450;&#22330;&#26223;&#30340;&#22788;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#25104;&#30495;&#23454;&#34892;&#20154;&#36712;&#36857;&#21644;&#20840;&#36523;&#21160;&#30011;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20197;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#24341;&#23548;&#25193;&#25955;&#24314;&#27169;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#27979;&#35797;&#26102;&#38388;&#23545;&#36712;&#36857;&#36827;&#34892;&#21487;&#25511;&#21046;&#65292;&#36825;&#36890;&#24120;&#21482;&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#30446;&#26631;&#36335;&#24452;&#28857;&#12289;&#36895;&#24230;&#21644;&#25351;&#23450;&#30340;&#31038;&#20132;&#32676;&#20307;&#26469;&#38480;&#21046;&#36712;&#36857;&#65292;&#21516;&#26102;&#32771;&#34385;&#21608;&#22260;&#29615;&#22659;&#24773;&#20917;&#12290;&#27492;&#36712;&#36857;&#25193;&#25955;&#27169;&#22411;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#20154;&#24418;&#25511;&#21046;&#22120;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#38381;&#29615;&#12289;&#20840;&#36523;&#34892;&#20154;&#21160;&#30011;&#31995;&#32479;&#65292;&#33021;&#22815;&#23558;&#22823;&#25209;&#20154;&#32676;&#25918;&#32622;&#22312;&#20855;&#26377;&#19981;&#21516;&#22320;&#24418;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#21033;&#29992;&#22312;RL&#35757;&#32451;&#21160;&#30011;&#25511;&#21046;&#22120;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#20540;&#20989;&#25968;&#26469;&#24341;&#23548;&#25193;&#25955;&#65292;&#20197;&#29983;&#25104;&#26356;&#36866;&#21512;&#29305;&#23450;&#22330;&#26223;&#30340;&#36712;&#36857;&#65292;&#20363;&#22914;&#36991;&#20813;&#30896;&#25758;&#21644;&#31359;&#36234;&#19981;&#24179;&#22320;&#24418;&#12290;&#35270;&#39057;&#32467;&#26524;&#26159;&#20196;&#20154;&#28385;&#24847;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are a
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26377;&#38480;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#24615;&#36136;&#65292;&#32473;&#20986;&#20102;&#33719;&#24471;&#23494;&#24230;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#24230;&#37327;&#35770;&#26465;&#20214;&#65292;&#21516;&#26102;&#38024;&#23545;&#19968;&#31181;&#29305;&#23450;&#28608;&#27963;&#20989;&#25968;&#21644;&#22266;&#23450;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#20854;&#22312;&#36830;&#32493;&#20989;&#25968;&#31354;&#38388;&#20013;&#20855;&#26377;&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01880</link><description>&lt;p&gt;
&#26377;&#38480;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#35770;&#36924;&#36817;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Measure theoretic results for approximation by neural networks with limited weights. (arXiv:2304.01880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26377;&#38480;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#24615;&#36136;&#65292;&#32473;&#20986;&#20102;&#33719;&#24471;&#23494;&#24230;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#24230;&#37327;&#35770;&#26465;&#20214;&#65292;&#21516;&#26102;&#38024;&#23545;&#19968;&#31181;&#29305;&#23450;&#28608;&#27963;&#20989;&#25968;&#21644;&#22266;&#23450;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#20854;&#22312;&#36830;&#32493;&#20989;&#25968;&#31354;&#38388;&#20013;&#20855;&#26377;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#26435;&#37325;&#22312;&#26377;&#38480;&#26041;&#21521;&#21644;&#24320;&#21306;&#38388;&#38408;&#20540;&#19978;&#21464;&#21270;&#30340;&#36924;&#36817;&#24615;&#36136;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#24517;&#35201;&#19988;&#21516;&#26102;&#20805;&#20998;&#30340;&#24230;&#37327;&#35770;&#26465;&#20214;&#65292;&#20197;&#20351;&#27492;&#31867;&#32593;&#32476;&#22312;&#36830;&#32493;&#20989;&#25968;&#31354;&#38388;&#20013;&#20855;&#26377;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#26500;&#24314;&#28608;&#27963;&#20989;&#25968;&#21644;&#22266;&#23450;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#23494;&#24230;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study approximation properties of single hidden layer neural networks with weights varying on finitely many directions and thresholds from an open interval. We obtain a necessary and at the same time sufficient measure theoretic condition for density of such networks in the space of continuous functions. Further, we prove a density result for neural networks with a specifically constructed activation function and a fixed number of neurons.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#22522;&#20110;&#35774;&#35745;&#26032;&#30340;&#29702;&#35770;&#12289;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#22686;&#37327;&#19982;&#23436;&#20840;&#39564;&#35777;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;MNIST&#21644;CIFAR10&#20197;&#21450;ACAS-XU&#20998;&#31867;&#22120;&#30340;&#26356;&#39640;&#25928;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.01874</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#37327;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Incremental Verification of Neural Networks. (arXiv:2304.01874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01874
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#22522;&#20110;&#35774;&#35745;&#26032;&#30340;&#29702;&#35770;&#12289;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#22686;&#37327;&#19982;&#23436;&#20840;&#39564;&#35777;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;MNIST&#21644;CIFAR10&#20197;&#21450;ACAS-XU&#20998;&#31867;&#22120;&#30340;&#26356;&#39640;&#25928;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#23436;&#20840;&#39564;&#35777;&#21487;&#20197;&#30830;&#23450;DNNs&#26159;&#21542;&#22312;&#26080;&#38480;&#36755;&#20837;&#38598;&#19978;&#28385;&#36275;&#25152;&#38656;&#30340;&#21487;&#20449;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#40065;&#26834;&#24615;&#65292;&#20844;&#27491;&#24615;&#65289;&#12290;&#23613;&#31649;&#22810;&#24180;&#26469;&#24050;&#32463;&#21462;&#24471;&#20102;&#26497;&#22823;&#30340;&#36827;&#23637;&#65292;&#20197;&#25913;&#21892;&#23436;&#20840;&#39564;&#35777;&#22120;&#22312;&#21333;&#20010;DNNs&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#26159;&#24403;&#37096;&#32626;&#30340;DNN&#36827;&#34892;&#26356;&#26032;&#20197;&#25552;&#39640;&#20854;&#25512;&#29702;&#36895;&#24230;&#25110;&#20934;&#30830;&#24615;&#26102;&#65292;&#23427;&#20204;&#22312;&#26412;&#36136;&#19978;&#25928;&#29575;&#20302;&#19979;&#12290;&#36825;&#26159;&#22240;&#20026;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#36816;&#34892;&#26114;&#36149;&#30340;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#26356;&#26032;&#21518;&#30340;DNN&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#22522;&#20110;&#35774;&#35745;&#26032;&#30340;&#29702;&#35770;&#12289;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#30340;&#22686;&#37327;&#21644;&#23436;&#20840;DNN&#39564;&#35777;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#19968;&#20010;&#21517;&#20026;IVAN&#30340;&#24037;&#20855;&#20013;&#23454;&#29616;&#65292;&#23545;&#20110;&#39564;&#35777;MNIST&#21644;CIFAR10&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#30340;&#24635;&#20307;&#20960;&#20309;&#24179;&#22343;&#21152;&#36895;&#27604;&#20026;2.4&#20493;&#65292;&#23545;&#20110;ACAS-XU&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#30340;&#24635;&#20307;&#20960;&#20309;&#24179;&#22343;&#21152;&#36895;&#27604;&#20026;3.8&#20493;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complete verification of deep neural networks (DNNs) can exactly determine whether the DNN satisfies a desired trustworthy property (e.g., robustness, fairness) on an infinite set of inputs or not. Despite the tremendous progress to improve the scalability of complete verifiers over the years on individual DNNs, they are inherently inefficient when a deployed DNN is updated to improve its inference speed or accuracy. The inefficiency is because the expensive verifier needs to be run from scratch on the updated DNN. To improve efficiency, we propose a new, general framework for incremental and complete DNN verification based on the design of novel theory, data structure, and algorithms. Our contributions implemented in a tool named IVAN yield an overall geometric mean speedup of 2.4x for verifying challenging MNIST and CIFAR10 classifiers and a geometric mean speedup of 3.8x for the ACAS-XU classifiers over the state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#22402;&#30452;&#20998;&#21306;&#24773;&#20917;&#30340;&#26377;&#21069;&#36884;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20016;&#23500;&#20102;&#26679;&#26412;&#25551;&#36848;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#30828;&#20214;&#23618;&#21040;&#22402;&#30452;&#32852;&#37030;&#31995;&#32479;&#23618;&#21508;&#20010;&#26041;&#38754;&#20570;&#20102;&#36129;&#29486;&#65292;VFL&#30340;&#24212;&#29992;&#24050;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#23588;&#20854;&#38544;&#31169;&#20445;&#25252;&#26159;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01829</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#32508;&#36848;&#65306;&#20197;&#20998;&#23618;&#35270;&#35282;&#20026;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
A Survey on Vertical Federated Learning: From a Layered Perspective. (arXiv:2304.01829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01829
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#22402;&#30452;&#20998;&#21306;&#24773;&#20917;&#30340;&#26377;&#21069;&#36884;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20016;&#23500;&#20102;&#26679;&#26412;&#25551;&#36848;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#30828;&#20214;&#23618;&#21040;&#22402;&#30452;&#32852;&#37030;&#31995;&#32479;&#23618;&#21508;&#20010;&#26041;&#38754;&#20570;&#20102;&#36129;&#29486;&#65292;VFL&#30340;&#24212;&#29992;&#24050;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#23588;&#20854;&#38544;&#31169;&#20445;&#25252;&#26159;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#30068;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#22402;&#30452;&#20998;&#21306;&#24182;&#20998;&#24067;&#22312;&#21508;&#26041;&#20043;&#38388;&#30340;&#24773;&#20917;&#12290;VFL&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#26041;&#30340;&#29305;&#24449;&#26469;&#20016;&#23500;&#26679;&#26412;&#25551;&#36848;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#12290;&#19982;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#30456;&#27604;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;VFL&#24212;&#29992;&#20110;&#20844;&#21496;&#30340;&#21830;&#19994;&#21512;&#20316;&#22330;&#26223;&#20013;&#65292;&#22240;&#27492;VFL&#21253;&#21547;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;VFL&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#20998;&#23618;&#35270;&#35282;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;VFL&#30340;&#29616;&#26377;&#30740;&#31350;&#24037;&#20316;&#12290;&#20174;&#30828;&#20214;&#23618;&#21040;&#22402;&#30452;&#32852;&#37030;&#31995;&#32479;&#23618;&#65292;&#30740;&#31350;&#20154;&#21592;&#36129;&#29486;&#20102;VFL&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;VFL&#30340;&#24212;&#29992;&#24050;&#35206;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#65292;&#20363;&#22914;&#37329;&#34701;&#12289;&#21307;&#30103;&#31561;&#12290;&#22312;&#27599;&#20010;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#24037;&#20316;&#36827;&#34892;&#20998;&#31867;&#24182;&#25506;&#35752;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21457;&#23637;VFL&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#20851;&#27880;&#38544;&#31169;&#20445;&#25252;&#65292;&#36825;&#26159;VFL&#20013;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#22312;&#21508;&#26041;&#20043;&#38388;&#32780;&#20855;&#26377;&#20851;&#38190;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) is a promising category of federated learning for the scenario where data is vertically partitioned and distributed among parties. VFL enriches the description of samples using features from different parties to improve model capacity. Compared with horizontal federated learning, in most cases, VFL is applied in the commercial cooperation scenario of companies. Therefore, VFL contains tremendous business values. In the past few years, VFL has attracted more and more attention in both academia and industry. In this paper, we systematically investigate the current work of VFL from a layered perspective. From the hardware layer to the vertical federated system layer, researchers contribute to various aspects of VFL. Moreover, the application of VFL has covered a wide range of areas, e.g., finance, healthcare, etc. At each layer, we categorize the existing work and explore the challenges for the convenience of further research and development of VFL. Espec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31283;&#23450;&#21644;&#40065;&#26834;&#30340;&#32447;&#24615;&#21442;&#25968;&#21487;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#20004;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#25910;&#32553;&#24847;&#20041;&#25110;&#36890;&#36807;&#29992;&#25143;&#23450;&#20041;&#30340;&#20540;&#34987;&#38480;&#21046;&#22312; Lipschitz &#24120;&#25968;&#20869;&#65292;&#23545;&#36827;&#19968;&#27493;&#30340;&#20984;&#20998;&#26512;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.01828</link><description>&lt;p&gt;
&#23398;&#20064;&#31283;&#23450;&#21644;&#40065;&#26834;&#30340;&#32447;&#24615;&#21442;&#25968;&#21487;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Stable and Robust Linear Parameter-Varying State-Space Models. (arXiv:2304.01828v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01828
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31283;&#23450;&#21644;&#40065;&#26834;&#30340;&#32447;&#24615;&#21442;&#25968;&#21487;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#20004;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#25910;&#32553;&#24847;&#20041;&#25110;&#36890;&#36807;&#29992;&#25143;&#23450;&#20041;&#30340;&#20540;&#34987;&#38480;&#21046;&#22312; Lipschitz &#24120;&#25968;&#20869;&#65292;&#23545;&#36827;&#19968;&#27493;&#30340;&#20984;&#20998;&#26512;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#31283;&#23450;&#21644;&#40065;&#26834;&#32447;&#24615;&#21442;&#25968;&#21487;&#21464;&#29366;&#24577;&#31354;&#38388; (LPV-SS) &#27169;&#22411;&#12290;&#27169;&#22411;&#21442;&#25968;&#21270;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#35777;&#25152;&#26377;&#21442;&#25968;&#20540;&#30340;&#27169;&#22411;&#37117;&#26159;&#31283;&#23450;&#30340;&#65292;&#20855;&#26377;&#25910;&#32553;&#24847;&#20041;&#65292;&#25110;&#20854; Lipschitz &#24120;&#25968;&#34987;&#29992;&#25143;&#23450;&#20041;&#30340;&#20540; $\gamma$ &#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21442;&#25968;&#21270;&#26159;&#30452;&#25509;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#26080;&#32422;&#26463;&#20248;&#21270;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#30001;&#20110;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#23646;&#20110; LPV-SS &#31867;&#65292;&#22240;&#27492;&#23427;&#20204;&#23545;&#36827;&#19968;&#27493;&#30340;&#20984;&#20998;&#26512;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#38750;&#24120;&#26377;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312; LPV &#35782;&#21035;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two direct parameterizations of stable and robust linear parameter-varying state-space (LPV-SS) models. The model parametrizations guarantee a priori that for all parameter values during training, the allowed models are stable in the contraction sense or have their Lipschitz constant bounded by a user-defined value $\gamma$. Furthermore, since the parametrizations are direct, the models can be trained using unconstrained optimization. The fact that the trained models are of the LPV-SS class makes them useful for, e.g., further convex analysis or controller design. The effectiveness of the approach is demonstrated on an LPV identification problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#24191;&#20041;&#25193;&#25955;&#27169;&#22411;&#65288;CoreDiff&#65289;&#65292;&#29992;&#20110;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#30340;&#21435;&#22122;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;LDCT&#22270;&#20687;&#26469;&#28040;&#38500;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#24182;&#27169;&#25311;CT&#36864;&#21270;&#30340;&#29289;&#29702;&#36807;&#31243;&#65292;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#20197;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01814</link><description>&lt;p&gt;
CoreDiff: &#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#24191;&#20041;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20302;&#21058;&#37327;CT&#21435;&#22122;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization. (arXiv:2304.01814v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#24191;&#20041;&#25193;&#25955;&#27169;&#22411;&#65288;CoreDiff&#65289;&#65292;&#29992;&#20110;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#30340;&#21435;&#22122;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;LDCT&#22270;&#20687;&#26469;&#28040;&#38500;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#24182;&#27169;&#25311;CT&#36864;&#21270;&#30340;&#29289;&#29702;&#36807;&#31243;&#65292;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#20197;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#21058;&#37327;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#30001;&#20110;&#20809;&#23376;&#21294;&#20047;&#21644;&#30005;&#23376;&#22122;&#22768;&#32780;&#21463;&#21040;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#20197;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#22122;&#27169;&#22411;&#36935;&#21040;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#29702;&#26102;&#38388;&#24456;&#38271;&#12290;&#26368;&#36817;&#65292;&#20919;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#20102;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#21463;&#20919;&#25193;&#25955;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#21435;&#22122;&#30340;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#24191;&#20041;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;CoreDiff&#12290;&#39318;&#20808;&#65292;CoreDiff&#21033;&#29992;LDCT&#22270;&#20687;&#26469;&#28040;&#38500;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#22343;&#20540;&#20445;&#25345;&#36864;&#21270;&#31639;&#23376;&#26469;&#27169;&#25311;CT&#36864;&#21270;&#30340;&#29289;&#29702;&#36807;&#31243;&#65292;&#30001;&#20110;&#21551;&#21160;&#37319;&#26679;&#36807;&#31243;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;LDCT&#22270;&#20687;&#65292;&#22823;&#24133;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#12290;&#20854;&#27425;&#65292;&#20026;&#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#20013;&#36807;&#22810;&#37319;&#26679;&#27493;&#39588;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#65292;&#20351;CoreDiff&#26356;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-dose computed tomography (CT) images suffer from noise and artifacts due to photon starvation and electronic noise. Recently, some works have attempted to use diffusion models to address the over-smoothness and training instability encountered by previous deep-learning-based denoising models. However, diffusion models suffer from long inference times due to the large number of sampling steps involved. Very recently, cold diffusion model generalizes classical diffusion models and has greater flexibility. Inspired by the cold diffusion, this paper presents a novel COntextual eRror-modulated gEneralized Diffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First, CoreDiff utilizes LDCT images to displace the random Gaussian noise and employs a novel mean-preserving degradation operator to mimic the physical process of CT degradation, significantly reducing sampling steps thanks to the informative LDCT images as the starting point of the sampling process. Second, to allevi
&lt;/p&gt;</description></item><item><title>HarsanyiNet &#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#12290;</title><link>http://arxiv.org/abs/2304.01811</link><description>&lt;p&gt;
HarsanyiNet: &#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#20934;&#30830;&#30340; Shapley &#20540;
&lt;/p&gt;
&lt;p&gt;
HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation. (arXiv:2304.01811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01811
&lt;/p&gt;
&lt;p&gt;
HarsanyiNet &#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley &#20540;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#21487;&#20449;&#30340;&#23646;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#20154;&#20204;&#20351;&#29992; Shapley &#20540;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#36755;&#20837;&#21464;&#37327;&#30340;&#23646;&#24615;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#38750;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#25165;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36817;&#20284;&#35745;&#31639;&#20986;&#27604;&#36739;&#31934;&#30830;&#30340; Shapley &#20540;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500; HarsanyiNet&#65292;&#22312;&#36755;&#20837;&#26679;&#26412;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#21069;&#21521;&#20256;&#25773;&#21363;&#21487;&#12290;HarsanyiNet &#26159;&#26500;&#24314;&#22312; Shapley &#20540;&#21487;&#20197;&#34987;&#37325;&#26032;&#26500;&#24314;&#20026;&#32593;&#32476;&#32534;&#30721;&#30340; Harsanyi &#20132;&#20114;&#37325;&#26032;&#20998;&#37197;&#30340;&#29702;&#35770;&#22522;&#30784;&#20043;&#19978;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Shapley value is widely regarded as a trustworthy attribution metric. However, when people use Shapley values to explain the attribution of input variables of a deep neural network (DNN), it usually requires a very high computational cost to approximate relatively accurate Shapley values in real-world applications. Therefore, we propose a novel network architecture, the HarsanyiNet, which makes inferences on the input sample and simultaneously computes the exact Shapley values of the input variables in a single forward propagation. The HarsanyiNet is designed on the theoretical foundation that the Shapley value can be reformulated as the redistribution of Harsanyi interactions encoded by the network.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25214;&#21040;&#20102;&#22312;&#31561;&#20960;&#20309;&#20998;&#26512;&#20013;&#26500;&#36896;&#21018;&#24230;&#21644;&#36136;&#37327;&#30697;&#38453;&#30340;&#26368;&#20339;&#27714;&#31215;&#35268;&#21017;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;Gauss&#22411;&#27714;&#31215;&#35268;&#21017;&#21644;&#26631;&#20934;&#20013;&#28857;&#35268;&#21017;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22312;&#35768;&#22810;&#22522;&#20934;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01802</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#31561;&#20960;&#20309;&#20998;&#26512;&#26368;&#20339;&#27714;&#31215;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Discovery of Optimal Quadrature Rules for Isogeometric Analysis. (arXiv:2304.01802v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25214;&#21040;&#20102;&#22312;&#31561;&#20960;&#20309;&#20998;&#26512;&#20013;&#26500;&#36896;&#21018;&#24230;&#21644;&#36136;&#37327;&#30697;&#38453;&#30340;&#26368;&#20339;&#27714;&#31215;&#35268;&#21017;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;Gauss&#22411;&#27714;&#31215;&#35268;&#21017;&#21644;&#26631;&#20934;&#20013;&#28857;&#35268;&#21017;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22312;&#35768;&#22810;&#22522;&#20934;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25214;&#21040;&#31561;&#20960;&#20309;&#20998;&#26512;&#65288;IGA&#65289;&#20013;&#26500;&#36896;&#21018;&#24230;&#21644;&#36136;&#37327;&#30697;&#38453;&#30340;&#26368;&#20339;&#27714;&#31215;&#35268;&#21017;&#12290;&#25105;&#20204;&#26368;&#21021;&#32771;&#34385;&#36328;&#36234;&#32479;&#19968;&#21644;&#38750;&#32479;&#19968;&#33410;&#28857;&#24207;&#21015;&#30340;&#20219;&#24847;&#27425;1D&#26679;&#26465;&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#24352;&#37327;&#31215;&#24847;&#20041;&#19979;&#29983;&#25104;&#30340;&#26368;&#20339;&#35268;&#21017;&#36827;&#34892;&#39640;&#32500;&#31354;&#38388;&#30340;&#31215;&#20998;&#12290; &#27714;&#31215;&#35268;&#21017;&#25628;&#32034;&#34987;&#25552;&#20986;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20248;&#21270;&#31354;&#38388;&#39640;&#24230;&#38750;&#20984;&#65292;&#25628;&#32034;&#30340;&#25104;&#21151;&#24378;&#28872;&#20381;&#36182;&#20110;&#27714;&#31215;&#28857;&#25968;&#21644;&#21442;&#25968;&#21021;&#22987;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#20855;&#26377;&#36739;&#23569;&#33410;&#28857;&#30340;&#26679;&#26465;&#31354;&#38388;&#19978;&#30340;&#26368;&#20248;&#35299;&#26469;&#21021;&#22987;&#21270;&#21442;&#25968;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#20351;&#29992;&#39640;&#36798;50&#20010;&#22343;&#21248;&#20803;&#32032;&#21644;&#22810;&#39033;&#24335;&#24230;&#25968;&#39640;&#36798;8&#30340;IGA&#31163;&#25955;&#21270;&#26102;&#65292;&#26679;&#26465;&#31354;&#38388;&#30340;&#26368;&#20339;&#27714;&#31215;&#35268;&#21017;&#12290;&#29983;&#25104;&#30340;&#27714;&#31215;&#35268;&#21017;&#26159;&#31283;&#20581;&#39640;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#22522;&#20934;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#27604;Gauss&#22411;&#27714;&#31215;&#35268;&#21017;&#21644;&#26631;&#20934;&#20013;&#28857;&#35268;&#21017;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the use of machine learning techniques to find optimal quadrature rules for the construction of stiffness and mass matrices in isogeometric analysis (IGA). We initially consider 1D spline spaces of arbitrary degree spanned over uniform and non-uniform knot sequences, and then the generated optimal rules are used for integration over higher-dimensional spaces using tensor product sense. The quadrature rule search is posed as an optimization problem and solved by a machine learning strategy based on gradient-descent. However, since the optimization space is highly non-convex, the success of the search strongly depends on the number of quadrature points and the parameter initialization. Thus, we use a dynamic programming strategy that initializes the parameters from the optimal solution over the spline space with a lower number of knots. With this method, we found optimal quadrature rules for spline spaces when using IGA discretizations with up to 50 uniform elements and polyno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedLA&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20837;&#20010;&#24615;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.01783</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#19982;&#26412;&#22320;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning with Local Attention. (arXiv:2304.01783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedLA&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20837;&#20010;&#24615;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20351;&#24471;&#20013;&#22830;&#26381;&#21153;&#22120;&#21487;&#20197;&#24110;&#21161;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#19981;&#24517;&#35775;&#38382;&#20854;&#26412;&#22320;&#25968;&#25454;&#12290;&#32852;&#37030;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#19981;&#21516;&#23458;&#25143;&#31471;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#20363;&#22914;&#24322;&#36136;&#26631;&#31614;&#20998;&#24067;&#21644;&#29305;&#24449;&#20559;&#31227;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#27169;&#22411;&#30340;&#26174;&#30528;&#24615;&#33021;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21363;&#20855;&#26377;&#26412;&#22320;&#27880;&#24847;&#21147;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFedLA&#65289;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#24182;&#20837;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#27880;&#24847;&#22359;&#29305;&#23450;&#20110;&#23458;&#25143;&#31471;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;pFedLA&#25552;&#20986;&#20102;&#20004;&#20010;&#27169;&#22359;&#65292;&#21363;&#20010;&#24615;&#21270;&#21333;&#27880;&#24847;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#28151;&#21512;&#27880;&#24847;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;FL&#25968;&#25454;&#38598;SplitMNIST-C&#65292;&#36890;&#36807;&#24341;&#20837;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20559;&#31227;&#12290;&#22312;SplitMNIST-C&#21644;EMNIST&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;pFedLA&#22312;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;FL&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aims to learn a single global model that enables the central server to help the model training in local clients without accessing their local data. The key challenge of FL is the heterogeneity of local data in different clients, such as heterogeneous label distribution and feature shift, which could lead to significant performance degradation of the learned models. Although many studies have been proposed to address the heterogeneous label distribution problem, few studies attempt to explore the feature shift issue. To address this issue, we propose a simple yet effective algorithm, namely \textbf{p}ersonalized \textbf{Fed}erated learning with \textbf{L}ocal \textbf{A}ttention (pFedLA), by incorporating the attention mechanism into personalized models of clients while keeping the attention blocks client-specific. Specifically, two modules are proposed in pFedLA, i.e., the personalized single attention module and the personalized hybrid attention module. In addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; Q-loss &#20989;&#25968;&#65292;&#36890;&#36807;&#31934;&#30830;&#23884;&#20837;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#20197;&#21450; Gauss-Newton &#36817;&#20284;&#25439;&#22833;&#21152;&#36895;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#24102;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.01782</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#30830; Q-loss &#21450;&#20854; Gauss-Newton &#36817;&#20284;&#20174;&#38750;&#32447;&#24615; MPC &#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Nonlinear MPC via the Exact Q-Loss and its Gauss-Newton Approximation. (arXiv:2304.01782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; Q-loss &#20989;&#25968;&#65292;&#36890;&#36807;&#31934;&#30830;&#23884;&#20837;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#20197;&#21450; Gauss-Newton &#36817;&#20284;&#25439;&#22833;&#21152;&#36895;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#24102;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23398;&#20064;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Q &#20989;&#25968;&#30340;&#25439;&#22833;&#65292;&#30452;&#25509;&#23884;&#20837;&#20102;&#19982;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30456;&#20851;&#30340;&#24615;&#33021;&#30446;&#26631;&#21644;&#32422;&#26463;&#28385;&#36275;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; Gauss-Newton &#36817;&#20284;&#30340;&#31532;&#20108;&#20010; Q-loss&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#36890;&#36807;&#22312;&#24102;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#25511;&#21046;&#19978;&#23545;&#25105;&#20204;&#30340;&#25439;&#22833;&#36827;&#34892;&#39564;&#35777;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#34892;&#20026;&#20811;&#38534;&#30456;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110; Q &#20989;&#25968;&#30340;&#25439;&#22833;&#26174;&#33879;&#20943;&#23569;&#20102;&#32422;&#26463;&#36829;&#35268;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel loss function for learning nonlinear Model Predictive Control policies via Imitation Learning. Standard approaches to Imitation Learning neglect information about the expert and generally adopt a loss function based on the distance between expert and learned controls. In this work, we present a loss based on the Q-function directly embedding the performance objectives and constraint satisfaction of the associated Optimal Control Problem (OCP). However, training a Neural Network with the Q-loss requires solving the associated OCP for each new sample. To alleviate the computational burden, we derive a second Q-loss based on the Gauss-Newton approximation of the OCP resulting in a faster training time. We validate our losses against Behavioral Cloning, the standard approach to Imitation Learning, on the control of a nonlinear system with constraints. The final results show that the Q-function-based losses significantly reduce the amount of constraint violations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#30340;&#28151;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#38024;&#23545;&#24230;&#37327;&#20219;&#21153;&#31995;&#32479;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;$O(\ell^2)$&#30340;&#31454;&#20105;&#27604;&#65292;&#21487;&#20197;&#20351;&#31639;&#27861;&#36319;&#38543;&#19981;&#21516;&#30340;&#39044;&#27979;&#22120;&#65292;&#23545;&#38480;&#21046;&#20999;&#25442;&#27425;&#25968;&#30340;&#24773;&#20917;&#21487;&#20197;&#33719;&#24471;$(1+\epsilon)$-&#31454;&#20105;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01781</link><description>&lt;p&gt;
&#22312;&#32447;&#25351;&#26631;&#31639;&#27861;&#30340;&#28151;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mixing predictions for online metric algorithms. (arXiv:2304.01781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#30340;&#28151;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#38024;&#23545;&#24230;&#37327;&#20219;&#21153;&#31995;&#32479;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;$O(\ell^2)$&#30340;&#31454;&#20105;&#27604;&#65292;&#21487;&#20197;&#20351;&#31639;&#27861;&#36319;&#38543;&#19981;&#21516;&#30340;&#39044;&#27979;&#22120;&#65292;&#23545;&#38480;&#21046;&#20999;&#25442;&#27425;&#25968;&#30340;&#24773;&#20917;&#21487;&#20197;&#33719;&#24471;$(1+\epsilon)$-&#31454;&#20105;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;-&#22686;&#24378;&#30340;&#22312;&#32447;&#31639;&#27861;&#20013;&#65292;&#20027;&#35201;&#25216;&#26415;&#20043;&#19968;&#26159;&#32452;&#21512;&#22810;&#20010;&#31639;&#27861;&#25110;&#39044;&#27979;&#22120;&#12290;&#30001;&#20110;&#27599;&#20010;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#22240;&#27492;&#24076;&#26395;&#20351;&#29992;&#21160;&#24577;&#32452;&#21512;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#26102;&#38388;&#36319;&#38543;&#19981;&#21516;&#30340;&#39044;&#27979;&#22120;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#21333;&#20010;&#26368;&#20339;&#39044;&#27979;&#22120;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#32452;&#21512;&#39044;&#27979;&#24182;&#38024;&#23545;&#24191;&#27867;&#30340;&#22312;&#32447;&#38382;&#39064;&#31867;&#21035;&#65288;&#21363;&#24230;&#37327;&#20219;&#21153;&#31995;&#32479;&#65289;&#19982;&#36825;&#26679;&#30340;&#21160;&#24577;&#32452;&#21512;&#36827;&#34892;&#31454;&#20105;&#12290;&#38024;&#23545;&#26368;&#20339;&#65288;&#20107;&#21518;&#65289;&#26080;&#32422;&#26463;&#32452;&#21512;&#30340;$\ell$&#20010;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;$O(\ell^2)$&#30340;&#31454;&#20105;&#27604;&#65292;&#24182;&#35777;&#26126;&#36825;&#26159;&#26368;&#20248;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20010;&#32422;&#26463;&#22312;&#19981;&#21516;&#39044;&#27979;&#22120;&#20043;&#38388;&#20999;&#25442;&#27425;&#25968;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;$(1+\epsilon)$-&#31454;&#20105;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#31867;&#20284;&#20110;&#36172;&#21338;&#26426;&#24335;&#30340;&#35775;&#38382;&#39044;&#27979;&#22120;&#30340;&#26041;&#24335;&#65292;&#27599;&#27425;&#26597;&#35810;&#19968;&#20010;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#20854;&#20013;&#19968;&#26465;&#19979;&#30028;&#30340;&#19968;&#20010;&#24847;&#22806;&#25512;&#35770;&#26159;&#65292;&#20986;&#29616;&#20102;&#26032;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major technique in learning-augmented online algorithms is combining multiple algorithms or predictors. Since the performance of each predictor may vary over time, it is desirable to use not the single best predictor as a benchmark, but rather a dynamic combination which follows different predictors at different times. We design algorithms that combine predictions and are competitive against such dynamic combinations for a wide class of online problems, namely, metrical task systems. Against the best (in hindsight) unconstrained combination of $\ell$ predictors, we obtain a competitive ratio of $O(\ell^2)$, and show that this is best possible. However, for a benchmark with slightly constrained number of switches between different predictors, we can get a $(1+\epsilon)$-competitive algorithm. Moreover, our algorithms can be adapted to access predictors in a bandit-like fashion, querying only one predictor at a time. An unexpected implication of one of our lower bounds is a new structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38500;&#20102;&#25903;&#25345;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#20027;&#39064;&#22806;&#65292;&#36824;&#25903;&#25345;&#30446;&#26631;&#20027;&#39064;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#35789;&#24314;&#35758;&#21151;&#33021;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#24456;&#22909;&#22320;&#36845;&#20195;&#22320;&#23436;&#21892;&#27169;&#22411;&#65292;&#19988;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2304.01774</link><description>&lt;p&gt;
&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System. (arXiv:2304.01774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38500;&#20102;&#25903;&#25345;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#20027;&#39064;&#22806;&#65292;&#36824;&#25903;&#25345;&#30446;&#26631;&#20027;&#39064;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#35789;&#24314;&#35758;&#21151;&#33021;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#24456;&#22909;&#22320;&#36845;&#20195;&#22320;&#23436;&#21892;&#27169;&#22411;&#65292;&#19988;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#23558;&#29992;&#25143;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#24314;&#27169;&#36807;&#31243;&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#36845;&#20195;&#22320;&#23436;&#21892;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#29992;&#25143;&#21453;&#39304;&#30340;&#20215;&#20540;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#38590;&#20197;&#36319;&#36394;&#21464;&#21270;&#12289;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20197;&#21450;&#32570;&#20047;&#22522;&#20110;&#23454;&#38469;&#20351;&#29992;&#30340;&#30495;&#23454;&#19990;&#30028;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#65292;&#20854;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#27604;&#36739;&#21644;&#35760;&#24405;&#27599;&#20010;&#27493;&#39588;&#65292;&#24182;&#20855;&#26377;&#26032;&#39062;&#30340;&#20027;&#39064;&#35789;&#24314;&#35758;&#21151;&#33021;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25552;&#20379;&#30495;&#23454;&#21487;&#38752;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#19981;&#20165;&#25903;&#25345;&#20256;&#32479;&#20027;&#39064;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#21363;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#20027;&#39064;&#65292;&#36824;&#25903;&#25345;&#30446;&#26631;&#20027;&#39064;&#24314;&#27169;&#65292;&#21363;&#38024;&#23545;&#35821;&#26009;&#24211;&#29305;&#23450;&#26041;&#38754;&#36827;&#34892;&#20027;&#39064;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#35813;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#29992;&#25143;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#20197;&#35780;&#20272;&#20854;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-loop topic modelling incorporates users' knowledge into the modelling process, enabling them to refine the model iteratively. Recent research has demonstrated the value of user feedback, but there are still issues to consider, such as the difficulty in tracking changes, comparing different models and the lack of evaluation based on real-world examples of use. We developed a novel, interactive human-in-the-loop topic modeling system with a user-friendly interface that enables users compare and record every step they take, and a novel topic words suggestion feature to help users provide feedback that is faithful to the ground truth. Our system also supports not only what traditional topic models can do, i.e., learning the topics from the whole corpus, but also targeted topic modelling, i.e., learning topics for specific aspects of the corpus. In this article, we provide an overview of the system and present the results of a series of user studies designed to assess the value
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#27169;&#25311;&#33258;&#26059;&#31995;&#32479;&#65292;&#22312;&#20234;&#36763;&#27169;&#22411;&#12289;&#27874;&#33576;&#27169;&#22411;&#21644;&#32454;&#32990;&#27874;&#33576;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01772</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#26059;&#27169;&#22411;&#30340;&#21487;&#24494;&#32534;&#31243;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A differentiable programming framework for spin models. (arXiv:2304.01772v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#27169;&#25311;&#33258;&#26059;&#31995;&#32479;&#65292;&#22312;&#20234;&#36763;&#27169;&#22411;&#12289;&#27874;&#33576;&#27169;&#22411;&#21644;&#32454;&#32990;&#27874;&#33576;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26059;&#31995;&#32479;&#26159;&#24314;&#27169;&#21508;&#31181;&#29289;&#29702;&#31995;&#32479;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#24494;&#32534;&#31243;&#24314;&#27169;&#33258;&#26059;&#31995;&#32479;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#27169;&#25311;&#33258;&#26059;&#31995;&#32479;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22797;&#26434;&#31995;&#32479;&#20013;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;&#33258;&#26059;&#31995;&#32479;&#8212;&#8212;&#20234;&#36763;&#27169;&#22411;&#12289;&#27874;&#33576;&#27169;&#22411;&#21644;&#32454;&#32990;&#27874;&#33576;&#27169;&#22411;&#8212;&#8212;&#26469;&#35777;&#26126;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#30828;&#20214;&#26550;&#26500;&#65288;&#21253;&#25324;&#22270;&#24418;&#22788;&#29702;&#22120;&#21644;&#24352;&#37327;&#22788;&#29702;&#22120;&#65289;&#19978;&#39640;&#25928;&#22320;&#25191;&#34892;&#20195;&#30721;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spin systems are a powerful tool for modeling a wide range of physical systems. In this paper, we propose a novel framework for modeling spin systems using differentiable programming. Our approach enables us to efficiently simulate spin systems, making it possible to model complex systems at scale. Specifically, we demonstrate the effectiveness of our technique by applying it to three different spin systems: the Ising model, the Potts model, and the Cellular Potts model. Our simulations show that our framework offers significant speedup compared to traditional simulation methods, thanks to its ability to execute code efficiently across different hardware architectures, including Graphical Processing Units and Tensor Processing Units.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23383;&#20856;&#23398;&#20064;&#20013;&#20004;&#31181;&#20132;&#26367;&#26497;&#23567;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#22312;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#33021;&#22815;&#20197;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#25910;&#25947;&#20110;&#29983;&#25104;&#30340;&#23383;&#20856;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01768</link><description>&lt;p&gt;
&#23383;&#20856;&#23398;&#20064;&#20013;&#20132;&#26367;&#26497;&#23567;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of alternating minimisation algorithms for dictionary learning. (arXiv:2304.01768v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23383;&#20856;&#23398;&#20064;&#20013;&#20004;&#31181;&#20132;&#26367;&#26497;&#23567;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#22312;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#33021;&#22815;&#20197;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#25910;&#25947;&#20110;&#29983;&#25104;&#30340;&#23383;&#20856;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23548;&#20986;&#20102;&#38024;&#23545;&#23383;&#20856;&#23398;&#20064;&#20004;&#31181;&#27969;&#34892;&#30340;&#20132;&#26367;&#26497;&#23567;&#21270;&#31639;&#27861; - &#26368;&#20248;&#26041;&#21521;&#27861;&#65288;MOD&#65289;&#21644;&#22312;&#32447;&#23383;&#20856;&#23398;&#20064;&#65288;ODL&#65289;&#30340;&#25910;&#25947;&#24615;&#36275;&#22815;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#33391;&#22909;&#65292;&#21363;&#36317;&#31163;&#29983;&#25104;&#30340;&#23383;&#20856;&#19981;&#36229;&#36807;$1/\log(K)$&#25110;&#20855;&#26377;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#30830;&#20445;&#21021;&#22987;&#20540;&#20013;&#30340;&#27599;&#20010;&#20803;&#32032;&#21482;&#25351;&#21521;&#19968;&#20010;&#29983;&#25104;&#20803;&#65292;&#20004;&#31181;&#31639;&#27861;&#23558;&#20197;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#25910;&#25947;&#20110;&#29983;&#25104;&#30340;&#23383;&#20856;&#12290;&#36825;&#22312;&#20855;&#26377;&#38750;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#27169;&#22411;&#19978;&#20063;&#33021;&#23454;&#29616;&#65292;&#35813;&#27169;&#22411;&#20013;&#31232;&#30095;&#31995;&#25968;&#30340;&#25903;&#25745;&#38598;&#30340;&#20986;&#29616;&#39057;&#29575;&#21487;&#20197;&#21464;&#21270;&#24456;&#22823;&#65292;&#20174;&#32780;&#26356;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we derive sufficient conditions for the convergence of two popular alternating minimisation algorithms for dictionary learning - the Method of Optimal Directions (MOD) and Online Dictionary Learning (ODL), which can also be thought of as approximative K-SVD. We show that given a well-behaved initialisation that is either within distance at most $1/\log(K)$ to the generating dictionary or has a special structure ensuring that each element of the initialisation only points to one generating element, both algorithms will converge with geometric convergence rate to the generating dictionary. This is done even for data models with non-uniform distributions on the supports of the sparse coefficients. These allow the appearance frequency of the dictionary elements to vary heavily and thus model real data more closely.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.01762</link><description>&lt;p&gt;
&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#32435;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Unlabelled Data into Bayesian Neural Networks. (arXiv:2304.01762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#23398;&#20064;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#20248;&#21270;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;BNN&#31639;&#27861;&#65292;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#26681;&#25454;&#21407;&#21017;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a contrastive framework for learning better prior distributions for Bayesian Neural Networks (BNNs) using unlabelled data. With this framework, we propose a practical BNN algorithm that offers the label-efficiency of self-supervised learning and the principled uncertainty estimates of Bayesian methods. Finally, we demonstrate the advantages of our approach for data-efficient learning in semi-supervised and low-budget active learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.01752</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#23569;&#26679;&#26412;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#36719;&#25552;&#31034;&#23398;&#20064;&#26159;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#29992;&#30340;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26032;&#39046;&#22495;&#24341;&#21457;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#32553;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#34429;&#28982;&#35813;&#26041;&#27861;&#24615;&#33021;&#39640;&#25928;&#65292;&#20294;&#20173;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#27169;&#22411;&#19978;&#21487;&#33021;&#20250;&#23548;&#33268;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340; V-L &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
&lt;/p&gt;</description></item><item><title>AdaLED&#26159;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#24615;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#35268;&#27169;&#27169;&#25311;&#19982;&#38477;&#38454;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#65292;&#20197;&#33258;&#36866;&#24212;&#26041;&#24335;&#25552;&#21462;&#21644;&#39044;&#27979;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#26377;&#25928;&#21160;&#21147;&#23398;&#12290;&#23427;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#31995;&#32479;&#21160;&#24577;&#30340;&#38477;&#38454;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#38598;&#21512;&#20316;&#20026;&#28508;&#22312;&#26102;&#38388;&#27493;&#36827;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.01732</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#20064;&#26377;&#25928;&#21160;&#21147;&#23398;&#65306;&#38754;&#21521;&#22797;&#26434;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#23454;&#26102;&#22312;&#32447;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Adaptive learning of effective dynamics: Adaptive real-time, online modeling for complex systems. (arXiv:2304.01732v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01732
&lt;/p&gt;
&lt;p&gt;
AdaLED&#26159;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#24615;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#35268;&#27169;&#27169;&#25311;&#19982;&#38477;&#38454;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#65292;&#20197;&#33258;&#36866;&#24212;&#26041;&#24335;&#25552;&#21462;&#21644;&#39044;&#27979;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#26377;&#25928;&#21160;&#21147;&#23398;&#12290;&#23427;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#31995;&#32479;&#21160;&#24577;&#30340;&#38477;&#38454;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#38598;&#21512;&#20316;&#20026;&#28508;&#22312;&#26102;&#38388;&#27493;&#36827;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#27169;&#25311;&#22312;&#20174;&#22825;&#27668;&#39044;&#25253;&#21040;&#26448;&#26009;&#35774;&#35745;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#20854;&#25429;&#33719;&#26377;&#25928;&#30340;&#31995;&#32479;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#25311;&#36890;&#36807;&#35299;&#26512;&#25152;&#26377;&#26102;&#31354;&#23610;&#24230;&#26469;&#39044;&#27979;&#31995;&#32479;&#21160;&#24577;&#65292;&#24448;&#24448;&#20197;&#25104;&#26412;&#20316;&#20026;&#20195;&#20215;&#38459;&#27490;&#20102;&#23454;&#39564;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38477;&#38454;&#27169;&#22411;&#36895;&#24230;&#24555;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#31995;&#32479;&#21160;&#21147;&#23398;&#32447;&#24615;&#21270;&#21644;&#25152;&#37319;&#29992;&#30340;&#21551;&#21457;&#24335;&#23553;&#38381;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#24615;&#26694;&#26550;&#65292;&#23558;&#22823;&#35268;&#27169;&#27169;&#25311;&#19982;&#38477;&#38454;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#65292;&#20197;&#33258;&#36866;&#24212;&#26041;&#24335;&#25552;&#21462;&#21644;&#39044;&#27979;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#26377;&#25928;&#21160;&#21147;&#23398;&#65288;AdaLED&#65289;&#12290;AdaLED&#37319;&#29992;&#33258;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#31995;&#32479;&#21160;&#24577;&#30340;&#38477;&#38454;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#38598;&#21512;&#20316;&#20026;&#28508;&#22312;&#26102;&#38388;&#27493;&#36827;&#22120;&#12290;&#35813;&#26694;&#26550;&#22312;&#35745;&#31639;&#27714;&#35299;&#22120;&#21644;&#26367;&#20195;&#27169;&#22411;&#20043;&#38388;&#20132;&#26367;&#65292;&#21152;&#36895;&#23398;&#20064;&#21160;&#21147;&#23398;&#21516;&#26102;&#20445;&#30041;&#20102;&#35299;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive simulations are essential for applications ranging from weather forecasting to material design. The veracity of these simulations hinges on their capacity to capture the effective system dynamics. Massively parallel simulations predict the systems dynamics by resolving all spatiotemporal scales, often at a cost that prevents experimentation. On the other hand, reduced order models are fast but often limited by the linearization of the system dynamics and the adopted heuristic closures. We propose a novel systematic framework that bridges large scale simulations and reduced order models to extract and forecast adaptively the effective dynamics (AdaLED) of multiscale systems. AdaLED employs an autoencoder to identify reduced-order representations of the system dynamics and an ensemble of probabilistic recurrent neural networks (RNNs) as the latent time-stepper. The framework alternates between the computational solver and the surrogate, accelerating learned dynamics while leav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01731</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#33976;&#39311;&#30340;&#26377;&#36873;&#25321;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher. (arXiv:2304.01731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#26159;&#23481;&#26131;&#21463;&#21040;&#30333;&#30418;&#25915;&#20987;&#65292;&#24182;&#19988;&#38590;&#20197;&#36866;&#24212;&#24322;&#26500;&#23458;&#25143;&#31471;&#12290;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#33976;&#39311;&#26159;&#19968;&#31181;&#25552;&#20379;&#22686;&#24378;&#38544;&#31169;&#20445;&#35777;&#24182;&#35299;&#20915;&#27169;&#22411;&#24322;&#26500;&#24615;&#30340;&#26367;&#20195;&#33539;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#26469;&#24212;&#23545;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#23458;&#25143;&#31471;&#36873;&#25321;&#22120;&#21644;&#26381;&#21153;&#22120;&#36873;&#25321;&#22120;&#65292;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Selective-FD&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While federated learning is promising for privacy-preserving collaborative learning without revealing local data, it remains vulnerable to white-box attacks and struggles to adapt to heterogeneous clients. Federated distillation (FD), built upon knowledge distillation--an effective technique for transferring knowledge from a teacher model to student models--emerges as an alternative paradigm, which provides enhanced privacy guarantees and addresses model heterogeneity. Nevertheless, challenges arise due to variations in local data distributions and the absence of a well-trained teacher model, which leads to misleading and ambiguous knowledge sharing that significantly degrades model performance. To address these issues, this paper proposes a selective knowledge sharing mechanism for FD, termed Selective-FD. It includes client-side selectors and a server-side selector to accurately and precisely identify knowledge from local and ensemble predictions, respectively. Empirical studies, bac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;XAI&#26041;&#27861;&#20013;&#32771;&#34385;&#39044;&#27979;&#21464;&#37327;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#27169;&#22411;&#26080;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01717</link><description>&lt;p&gt;
&#25581;&#31034;XAI&#26041;&#27861;&#20013;&#20381;&#36182;&#29305;&#24449;&#30340;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Characterizing the contribution of dependent features in XAI methods. (arXiv:2304.01717v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;XAI&#26041;&#27861;&#20013;&#32771;&#34385;&#39044;&#27979;&#21464;&#37327;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#27169;&#22411;&#26080;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25552;&#20379;&#20102;&#24037;&#20855;&#65292;&#24110;&#21161;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#23454;&#29616;&#29305;&#23450;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#23427;&#26377;&#21161;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#20026;&#21487;&#20449;&#21644;&#36879;&#26126;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35768;&#22810;XAI&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#20854;&#20013;SHAP&#21644;LIME&#26368;&#24191;&#20026;&#20154;&#30693;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#39044;&#27979;&#21464;&#37327;&#30456;&#20114;&#29420;&#31435;&#65292;&#36825;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#12290;&#36825;&#31181;&#20551;&#35774;&#20351;&#24471;XAI&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21463;&#21040;&#24433;&#21709;&#65292;&#27604;&#22914;&#20449;&#24687;&#39044;&#27979;&#21464;&#37327;&#30340;&#21015;&#34920;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#29992;&#30340;&#20195;&#29702;&#65292;&#20462;&#25913;&#20219;&#20309;XAI&#29305;&#24449;&#25490;&#21517;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#20351;&#20854;&#33021;&#22815;&#32771;&#34385;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#27169;&#22411;&#26080;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#31616;&#21333;&#22320;&#35745;&#31639;&#22312;&#20849;&#32447;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#27599;&#20010;&#39044;&#27979;&#21464;&#37327;&#22312;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) provides tools to help understanding how the machine learning models work and reach a specific outcome. It helps to increase the interpretability of models and makes the models more trustworthy and transparent. In this context, many XAI methods were proposed being SHAP and LIME the most popular. However, the proposed methods assume that used predictors in the machine learning models are independent which in general is not necessarily true. Such assumption casts shadows on the robustness of the XAI outcomes such as the list of informative predictors. Here, we propose a simple, yet useful proxy that modifies the outcome of any XAI feature ranking method allowing to account for the dependency among the predictors. The proposed approach has the advantage of being model-agnostic as well as simple to calculate the impact of each predictor in the model in presence of collinearity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#32416;&#27491;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#21442;&#25968;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#20801;&#35768;&#20272;&#35745;&#26356;&#22797;&#26434;&#30340;&#29305;&#24449;&#24182;&#32771;&#34385;&#22810;&#31181;&#25945;&#24072;&#30340;&#24178;&#39044;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.01701</link><description>&lt;p&gt;
&#29992;&#26368;&#20248;&#20256;&#36755;&#20248;&#21270;&#32416;&#27491;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Correctional Learning. (arXiv:2304.01701v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#32416;&#27491;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#21442;&#25968;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#20801;&#35768;&#20272;&#35745;&#26356;&#22797;&#26434;&#30340;&#29305;&#24449;&#24182;&#32771;&#34385;&#22810;&#31181;&#25945;&#24072;&#30340;&#24178;&#39044;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#32416;&#27491;&#23398;&#20064;&#36890;&#29992;&#20844;&#24335;&#65292;&#23427;&#26159;&#20851;&#20110;&#22914;&#20309;&#26368;&#20248;&#22320;&#23558;&#19968;&#20010;&#36136;&#37327;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#30340;&#12290;&#32416;&#27491;&#23398;&#20064;&#26159;&#19968;&#31181;&#21033;&#29992;&#24072;&#29983;&#26041;&#27861;&#22686;&#24378;&#21442;&#25968;&#20272;&#35745;&#36807;&#31243;&#20934;&#30830;&#24615;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#19968;&#20010;&#19987;&#23478;&#20195;&#29702;&#65288;&#31216;&#20043;&#20026;&#25945;&#24072;&#65289;&#20462;&#25913;&#23398;&#20064;&#20195;&#29702;&#65288;&#31216;&#20043;&#20026;&#23398;&#29983;&#65289;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#20197;&#25913;&#21892;&#20854;&#20272;&#35745;&#36807;&#31243;&#12290;&#25945;&#24072;&#30340;&#30446;&#26631;&#26159;&#25913;&#21464;&#25968;&#25454;&#65292;&#20351;&#23398;&#29983;&#30340;&#20272;&#35745;&#35823;&#24046;&#26368;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#23450;&#30340;&#24178;&#39044;&#39044;&#31639;&#12290;&#19982;&#29616;&#26377;&#30340;&#32416;&#27491;&#23398;&#20064;&#20844;&#24335;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#25552;&#20379;&#20102;&#22810;&#31181;&#22909;&#22788;&#12290;&#23427;&#20801;&#35768;&#20272;&#35745;&#26356;&#22797;&#26434;&#30340;&#29305;&#24449;&#65292;&#21516;&#26102;&#32771;&#34385;&#22810;&#31181;&#25945;&#24072;&#30340;&#24178;&#39044;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#29702;&#35770;&#23454;&#20363;&#21644;&#19968;&#20010;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The contribution of this paper is a generalized formulation of correctional learning using optimal transport, which is about how to optimally transport one mass distribution to another. Correctional learning is a framework developed to enhance the accuracy of parameter estimation processes by means of a teacher-student approach. In this framework, an expert agent, referred to as the teacher, modifies the data used by a learning agent, known as the student, to improve its estimation process. The objective of the teacher is to alter the data such that the student's estimation error is minimized, subject to a fixed intervention budget. Compared to existing formulations of correctional learning, our novel optimal transport approach provides several benefits. It allows for the estimation of more complex characteristics as well as the consideration of multiple intervention policies for the teacher. We evaluate our approach on two theoretical examples, and on a human-robot interaction applica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#25237;&#24433;&#36136;&#37327;&#34920;&#38754;&#23494;&#24230;&#22270;&#39044;&#27979;&#24040;&#22823;&#20998;&#23376;&#20113;&#30340;&#20307;&#31215;&#25110;&#25968;&#23494;&#24230;&#65292;&#24182;&#22312;&#25968;&#23494;&#24230;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#33719;&#24471;&#20102;&#19968;&#20010;&#37327;&#32423;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.01670</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#39044;&#27979;&#20998;&#23376;&#20113;&#23494;&#24230;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models to Predict the Density of Molecular Clouds. (arXiv:2304.01670v1 [astro-ph.GA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#25237;&#24433;&#36136;&#37327;&#34920;&#38754;&#23494;&#24230;&#22270;&#39044;&#27979;&#24040;&#22823;&#20998;&#23376;&#20113;&#30340;&#20307;&#31215;&#25110;&#25968;&#23494;&#24230;&#65292;&#24182;&#22312;&#25968;&#23494;&#24230;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#33719;&#24471;&#20102;&#19968;&#20010;&#37327;&#32423;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#20316;&#20026;&#19968;&#31181;&#20174;&#25237;&#24433;&#36136;&#37327;&#34920;&#38754;&#23494;&#24230;&#22270;&#25512;&#26029;&#24040;&#22823;&#20998;&#23376;&#20113;&#65288;GMCs&#65289;&#20307;&#31215;&#25110;&#25968;&#23494;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20855;&#26377;&#19981;&#21516;&#20840;&#23616;&#30913;&#22330;&#24378;&#24230;&#21644;&#22823;&#23610;&#24230;&#21160;&#24577;&#12289;&#21363;&#19981;&#30896;&#25758;&#21644;&#30896;&#25758;GMCs&#30340;&#30913;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#27169;&#25311;&#20013;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#36136;&#37327;&#34920;&#38754;&#23494;&#24230;&#22270;&#21450;&#20854;&#23545;&#24212;&#30340;&#36136;&#37327;&#21152;&#26435;&#25968;&#23494;&#24230;&#22270;&#12290;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;&#26356;&#20256;&#32479;&#30340;&#32463;&#39564;&#20108;&#20998;&#27861;&#21644;&#19977;&#20998;&#27861;&#24130;&#24459;&#25311;&#21512;&#26041;&#27861;&#20197;&#21450;&#26356;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;CASI-2D &#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#39044;&#27979;&#25968;&#23494;&#24230;&#30340;&#20934;&#30830;&#24230;&#19978;&#25552;&#39640;&#20102;&#19968;&#20010;&#37327;&#32423;&#12290;&#25105;&#20204;&#23558;&#25193;&#25955;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20123;&#31034;&#20363;&#22825;&#25991;&#26609;&#23494;&#24230;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the state-of-the-art deep learning Denoising Diffusion Probabilistic Model (DDPM) as a method to infer the volume or number density of giant molecular clouds (GMCs) from projected mass surface density maps. We adopt magnetohydrodynamic simulations with different global magnetic field strengths and large-scale dynamics, i.e., noncolliding and colliding GMCs. We train a diffusion model on both mass surface density maps and their corresponding mass-weighted number density maps from different viewing angles for all the simulations. We compare the diffusion model performance with a more traditional empirical two-component and three-component power-law fitting method and with a more traditional neural network machine learning approach (CASI-2D). We conclude that the diffusion model achieves an order of magnitude improvement on the accuracy of predicting number density compared to that by other methods. We apply the diffusion method to some example astronomical column density map
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20248;&#21270;&#30446;&#26631;&#21644;&#19968;&#20010;&#26032;&#22411;&#30340;&#8220;&#27169;&#22411;&#22686;&#24378;&#8221;&#24605;&#36335;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01669</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Re-thinking Model Inversion Attacks Against Deep Neural Networks. (arXiv:2304.01669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20248;&#21270;&#30446;&#26631;&#21644;&#19968;&#20010;&#26032;&#22411;&#30340;&#8220;&#27169;&#22411;&#22686;&#24378;&#8221;&#24605;&#36335;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36870;&#25512;&#65288;MI&#65289;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#28389;&#29992;&#23545;&#27169;&#22411;&#30340;&#35775;&#38382;&#26469;&#25512;&#26029;&#21644;&#37325;&#26500;&#31169;&#26377;&#22521;&#35757;&#25968;&#25454;&#12290;MI&#25915;&#20987;&#24341;&#36215;&#20102;&#26377;&#20851;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#65288;&#20363;&#22914;&#29992;&#20110;&#35757;&#32451;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#31169;&#20154;&#38754;&#37096;&#22270;&#20687;&#65289;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#26469;&#25913;&#21892;MI&#30340;&#25915;&#20987;&#34920;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;MI&#65292;&#30740;&#31350;&#20102;&#25152;&#26377;&#26368;&#20808;&#36827;&#65288;SOTA&#65289; MI&#31639;&#27861;&#25152;&#28041;&#21450;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25152;&#26377;SOTA MI&#30340;&#25915;&#20987;&#34920;&#29616;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#25105;&#20204;&#20998;&#26512;&#20102;SOTA MI&#31639;&#27861;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#35748;&#20026;&#35813;&#30446;&#26631;&#23545;&#20110;&#23454;&#29616;MI&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25915;&#20987;&#24615;&#33021;&#12290;2&#65289;&#25105;&#20204;&#20998;&#26512;&#20102;&#8220;MI&#36807;&#24230;&#25311;&#21512;&#8221;&#65292;&#23637;&#31034;&#20102;&#23427;&#20250;&#38459;&#27490;&#37325;&#26500;&#22270;&#20687;&#20174;&#23398;&#20064;&#22521;&#35757;&#25968;&#25454;&#30340;&#35821;&#20041;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#8220;&#27169;&#22411;&#22686;&#24378;&#8221;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model inversion (MI) attacks aim to infer and reconstruct private training data by abusing access to a model. MI attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for MI have been proposed to improve the attack performance. In this work, we revisit MI, study two fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all SOTA MI. In particular, our contributions are two-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue that the objective is sub-optimal for achieving MI, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze "MI overfitting", show that it would prevent reconstructed images from learning semantics of training data, and propose a novel "model augmentation" ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#31867;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#22914;&#20309;&#26377;&#25928;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#26435;&#34913;&#38382;&#39064;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#31639;&#27861;&#26356;&#20542;&#21521;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#32780;&#19981;&#26159;&#21487;&#22609;&#24615;&#65292;&#24182;&#23545;&#35757;&#32451;&#22312;&#21021;&#22987;&#31867;&#19978;&#30340;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2304.01663</link><description>&lt;p&gt;
&#23545;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#22256;&#22659;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the Stability-Plasticity Dilemma of Class-Incremental Learning. (arXiv:2304.01663v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#31867;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#22914;&#20309;&#26377;&#25928;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#26435;&#34913;&#38382;&#39064;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#31639;&#27861;&#26356;&#20542;&#21521;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#32780;&#19981;&#26159;&#21487;&#22609;&#24615;&#65292;&#24182;&#23545;&#35757;&#32451;&#22312;&#21021;&#22987;&#31867;&#19978;&#30340;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#21363;&#27169;&#22411;&#24212;&#35813;&#26082;&#31283;&#23450;&#21040;&#36275;&#20197;&#20445;&#30041;&#20174;&#20808;&#21069;&#30475;&#21040;&#30340;&#31867;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#21448;&#24212;&#35813;&#20855;&#22791;&#36275;&#22815;&#30340;&#21487;&#22609;&#24615;&#65292;&#21487;&#20197;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#26368;&#36817;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#22914;&#20309;&#26377;&#25928;&#22320;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#24230;&#37327;&#29305;&#24449;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#26469;&#30740;&#31350;&#21508;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#30340;&#31867;&#22686;&#37327;&#22522;&#20934;&#27979;&#35797;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#37096;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#37117;&#38750;&#24120;&#20542;&#21521;&#20110;&#31283;&#23450;&#24615;&#32780;&#19981;&#26159;&#21487;&#22609;&#24615;&#65292;&#20197;&#33267;&#20110;&#35757;&#32451;&#22312;&#21021;&#22987;&#31867;&#19978;&#30340;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary goal of class-incremental learning is to strike a balance between stability and plasticity, where models should be both stable enough to retain knowledge learned from previously seen classes, and plastic enough to learn concepts from new classes. While previous works demonstrate strong performance on class-incremental benchmarks, it is not clear whether their success comes from the models being stable, plastic, or a mixture of both. This paper aims to shed light on how effectively recent class-incremental learning algorithms address the stability-plasticity trade-off. We establish analytical tools that measure the stability and plasticity of feature representations, and employ such tools to investigate models trained with various algorithms on large-scale class-incremental benchmarks. Surprisingly, we find that the majority of class-incremental learning algorithms heavily favor stability over plasticity, to the extent that the feature extractor of a model trained on the initi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GC-EI-MS&#20809;&#35889;&#30340;&#26032;&#22411;\emph{de-novo}&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#20174;&#36136;&#35889;&#25968;&#25454;&#25512;&#23548;&#23567;&#20998;&#23376;&#30340;&#32467;&#26500;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#38752;&#30340;&#20809;&#35889;&#25968;&#25454;&#24211;&#26080;&#27861;&#35206;&#30422;&#36275;&#22815;&#23494;&#38598;&#28508;&#22312;&#21270;&#23398;&#31354;&#38388;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01634</link><description>&lt;p&gt;
GC-EI-MS&#20809;&#35889;&#20013;&#23567;&#20998;&#23376;&#30340;&#26032;&#22411;&#24322;&#26500;&#20307;&#37492;&#23450;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
De-novo Identification of Small Molecules from Their GC-EI-MS Spectra. (arXiv:2304.01634v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01634
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GC-EI-MS&#20809;&#35889;&#30340;&#26032;&#22411;\emph{de-novo}&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#20174;&#36136;&#35889;&#25968;&#25454;&#25512;&#23548;&#23567;&#20998;&#23376;&#30340;&#32467;&#26500;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#38752;&#30340;&#20809;&#35889;&#25968;&#25454;&#24211;&#26080;&#27861;&#35206;&#30422;&#36275;&#22815;&#23494;&#38598;&#28508;&#22312;&#21270;&#23398;&#31354;&#38388;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24403;&#21069;&#21487;&#38752;&#30340;&#20809;&#35889;&#25968;&#25454;&#24211;&#26080;&#27861;&#35206;&#30422;&#36275;&#22815;&#23494;&#38598;&#30340;&#28508;&#22312;&#21270;&#23398;&#31354;&#38388;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#22411;\emph{de-novo}&#26041;&#27861;&#24050;&#21463;&#21040;&#20851;&#27880;&#30452;&#25509;&#20174;&#36136;&#35889;&#25968;&#25454;&#25512;&#23548;&#20998;&#23376;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;GC-EI-MS&#20809;&#35889;&#30340;&#29305;&#23450;&#29992;&#20363;&#65292;&#30001;&#20110;&#32570;&#20047;&#20808;&#21069;&#24050;&#21457;&#34920;&#26041;&#27861;&#25152;&#20381;&#36182;&#30340;MS / MS&#23454;&#39564;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#22240;&#27492;&#35813;&#29992;&#20363;&#23588;&#20854;&#22256;&#38590;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification of experimentally acquired mass spectra of unknown compounds presents a~particular challenge because reliable spectral databases do not cover the potential chemical space with sufficient density. Therefore machine learning based \emph{de-novo} methods, which derive molecular structure directly from its mass spectrum gained attention recently. We present a~novel method in this family, addressing a~specific usecase of GC-EI-MS spectra, which is particularly hard due to lack of additional information from the first stage of MS/MS experiments, on which the previously published methods rely. We analyze strengths and drawbacks or our approach and discuss future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#22312;&#26550;&#26500;&#20013;&#21512;&#24182;&#20102;&#26230;&#20307;&#30340;&#21333;&#20803;&#26684;&#23545;&#31216;&#24615;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#22810;&#23380;&#32467;&#26500;&#65292;&#21487;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#23380;&#26230;&#20307;&#26448;&#26009;&#30340;&#21560;&#38468;&#28909;&#12290;</title><link>http://arxiv.org/abs/2304.01628</link><description>&lt;p&gt;
&#22810;&#23380;&#26230;&#24577;&#26448;&#26009;&#30340;&#31561;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Equivariant Networks for Porous Crystalline Materials. (arXiv:2304.01628v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#22312;&#26550;&#26500;&#20013;&#21512;&#24182;&#20102;&#26230;&#20307;&#30340;&#21333;&#20803;&#26684;&#23545;&#31216;&#24615;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#22810;&#23380;&#32467;&#26500;&#65292;&#21487;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#23380;&#26230;&#20307;&#26448;&#26009;&#30340;&#21560;&#38468;&#28909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#39044;&#27979;&#22810;&#23380;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#20855;&#26377;&#21152;&#36895;&#24320;&#21457;&#26032;&#26448;&#26009;&#30340;&#39640;&#36890;&#37327;&#31579;&#36873;&#36807;&#31243;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#22240;&#20026;&#20351;&#29992;&#31532;&#19968;&#21407;&#29702;&#27169;&#22411;&#36827;&#34892;&#30340;&#27169;&#25311;&#24448;&#24448;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#24314;&#27169;&#36825;&#20123;&#26448;&#26009;&#65292;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#26230;&#20307;&#20013;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#65292;&#36825;&#20123;&#23545;&#31216;&#24615;&#30001;&#23427;&#20204;&#30340;&#31354;&#38388;&#32676;&#23450;&#20041;&#12290;&#29616;&#26377;&#30340;&#26230;&#20307;&#24615;&#36136;&#39044;&#27979;&#26041;&#27861;&#35201;&#20040;&#20855;&#26377;&#36807;&#20110;&#20005;&#26684;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#65292;&#35201;&#20040;&#20165;&#21253;&#25324;&#21333;&#20803;&#26684;&#20043;&#38388;&#30340;&#23545;&#31216;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#27809;&#26377;&#26126;&#30830;&#22320;&#24314;&#27169;&#26230;&#20307;&#30340;&#22810;&#23380;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#22312;&#20854;&#26550;&#26500;&#20013;&#21512;&#24182;&#20102;&#26230;&#20307;&#30340;&#21333;&#20803;&#26684;&#23545;&#31216;&#24615;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#22810;&#23380;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#19981;&#21516;&#26500;&#22411;&#30340;&#33707;&#23572;&#23450;&#30707;&#27832;&#30707;&#30340;CO$_2$&#21560;&#38468;&#28909;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26230;&#20307;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently predicting properties of porous crystalline materials has great potential to accelerate the high throughput screening process for developing new materials, as simulations carried out using first principles model are often computationally expensive. To effectively make use of Deep Learning methods to model these materials, we need to utilize the symmetries present in the crystals, which are defined by their space group. Existing methods for crystal property prediction either have symmetry constraints that are too restrictive or only incorporate symmetries between unit cells. In addition, these models do not explicitly model the porous structure of the crystal. In this paper, we develop a model which incorporates the symmetries of the unit cell of a crystal in its architecture and explicitly models the porous structure. We evaluate our model by predicting the heat of adsorption of CO$_2$ for different configurations of the mordenite zeolite. Our results confirm that our metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MESAHA-Net&#30340;&#39640;&#25928;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36880;&#23618;2D&#20998;&#21106;&#65292;&#23454;&#29616;&#20102; CT&#25195;&#25551;&#20013;&#31934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.01576</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#30340;&#26368;&#22823;&#24378;&#24230;&#25237;&#24433;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;CT&#25195;&#25551;&#32954;&#32467;&#33410;&#20998;&#21106; MESAHA-Net&#65288;arXiv&#65306;2304.01576v1 [eess.IV]&#65289;
&lt;/p&gt;
&lt;p&gt;
MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan. (arXiv:2304.01576v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MESAHA-Net&#30340;&#39640;&#25928;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36880;&#23618;2D&#20998;&#21106;&#65292;&#23454;&#29616;&#20102; CT&#25195;&#25551;&#20013;&#31934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#23545;&#26089;&#26399;&#32954;&#30284;&#35786;&#26029;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#34987;&#24191;&#27867;&#29992;&#20110;&#32954;&#32467;&#33410;&#20998;&#26512;&#30340;&#26089;&#26399;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#32954;&#32467;&#33410;&#30340;&#24322;&#36136;&#24615;&#65292;&#22823;&#23567;&#22810;&#26679;&#24615;&#20197;&#21450;&#21608;&#22260;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#23545;&#24320;&#21457;&#40065;&#26834;&#30340;&#32467;&#33410;&#20998;&#21106;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#30340;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MESAHA-Net&#65289;&#65292;&#29992;&#20110;CT&#25195;&#25551;&#20013;&#31934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#12290;MESAHA-Net&#21253;&#25324;&#19977;&#20010;&#32534;&#30721;&#36335;&#24452;&#65292;&#19968;&#20010;&#27880;&#24847;&#21147;&#22359;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#22359;&#65292;&#26377;&#21161;&#20110;&#38598;&#25104;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65306;CT&#20999;&#29255;&#34917;&#19969;&#65292;&#21069;&#21521;&#21644;&#21518;&#21521;&#30340;&#26368;&#22823;&#24378;&#24230;&#25237;&#24433;&#65288;MIP&#65289;&#22270;&#20687;&#20197;&#21450;&#21253;&#21547;&#32467;&#33410;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#25513;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;MESAHA-Net&#36880;&#23618;&#25191;&#34892;&#36880;&#23618;2D&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate lung nodule segmentation is crucial for early-stage lung cancer diagnosis, as it can substantially enhance patient survival rates. Computed tomography (CT) images are widely employed for early diagnosis in lung nodule analysis. However, the heterogeneity of lung nodules, size diversity, and the complexity of the surrounding environment pose challenges for developing robust nodule segmentation methods. In this study, we propose an efficient end-to-end framework, the multi-encoder-based self-adaptive hard attention network (MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Net comprises three encoding paths, an attention block, and a decoder block, facilitating the integration of three types of inputs: CT slice patches, forward and backward maximum intensity projection (MIP) images, and region of interest (ROI) masks encompassing the nodule. By employing a novel adaptive hard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2D segmentation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27744;&#21270;&#31639;&#23376;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26631;&#20934;&#26469;&#36873;&#25321;&#25110;&#35774;&#35745;&#27744;&#21270;&#31639;&#23376;&#12290;</title><link>http://arxiv.org/abs/2304.01575</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#27744;&#21270;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The expressive power of pooling in Graph Neural Networks. (arXiv:2304.01575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27744;&#21270;&#31639;&#23376;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26631;&#20934;&#26469;&#36873;&#25321;&#25110;&#35774;&#35745;&#27744;&#21270;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#65292;&#20998;&#23618;&#27744;&#21270;&#31639;&#23376;&#36890;&#36807;&#21019;&#24314;&#22270;&#32467;&#26500;&#21644;&#20854;&#39030;&#28857;&#29305;&#24449;&#30340;&#26412;&#22320;&#25688;&#35201;&#26469;&#29983;&#25104;&#36755;&#20837;&#25968;&#25454;&#30340;&#26356;&#31895;&#31961;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#24050;&#32463;&#33268;&#21147;&#20110;&#30740;&#31350;GNN&#20013;&#28040;&#24687;&#20256;&#36882;&#65288;MP&#65289;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#20851;&#20110;&#27744;&#21270;&#31639;&#23376;&#22914;&#20309;&#24433;&#21709;GNN&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#26368;&#36817;&#22312;&#26377;&#25928;&#27744;&#21270;&#31639;&#23376;&#30340;&#35774;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26631;&#20934;&#26469;&#27604;&#36739;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#36275;&#22815;&#30340;&#26465;&#20214;&#20351;&#27744;&#21270;&#31639;&#23376;&#22312;&#20854;&#20043;&#21069;&#30340;MP&#23618;&#20013;&#23436;&#20840;&#20445;&#30041;&#34920;&#36798;&#33021;&#21147;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36825;&#20123;&#26465;&#20214;&#20316;&#20026;&#36873;&#25321;&#29616;&#26377;&#27744;&#21270;&#31639;&#23376;&#25110;&#35774;&#35745;&#26032;&#30340;&#27744;&#21270;&#31639;&#23376;&#30340;&#36890;&#29992;&#21644;&#29702;&#35770;&#22522;&#30784;&#30340;&#26631;&#20934;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#20960;&#20010;&#29616;&#26377;&#30340;&#27744;&#21270;&#31639;&#23376;&#65292;&#24182;&#30830;&#23450;&#20102;&#37027;&#20123;&#19981;&#33021;&#28385;&#36275;&#34920;&#36798;&#24615;&#20551;&#35774;&#30340;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Graph Neural Networks (GNNs), hierarchical pooling operators generate a coarser representation of the input data by creating local summaries of the graph structure and its vertex features. Considerable attention has been devoted to studying the expressive power of message-passing (MP) layers in GNNs, while a study on how pooling operators affect the expressivity of a GNN is still lacking. Additionally, despite the recent advances in the design of effective pooling operators, there is not a principled criterion to compare them. Our work aims to fill this gap by providing sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically-grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we reviewed several existing pooling operators and identified those that fail to satisfy the expressiveness assumptions. Finally,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;STS&#27169;&#22411;&#26469;&#39044;&#27979;&#22478;&#24066;&#24322;&#24120;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#24322;&#24120;&#25968;&#25454;&#31232;&#30095;&#38646;&#33192;&#32960;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#32479;&#19968;&#39044;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#20132;&#36890;&#20107;&#25925;&#21644;&#29359;&#32618;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.01569</link><description>&lt;p&gt;
&#26102;&#31354;&#35821;&#20041;&#38646;&#33192;&#32960;&#22478;&#24066;&#24322;&#24120;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal and Semantic Zero-inflated Urban Anomaly Prediction. (arXiv:2304.01569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;STS&#27169;&#22411;&#26469;&#39044;&#27979;&#22478;&#24066;&#24322;&#24120;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#24322;&#24120;&#25968;&#25454;&#31232;&#30095;&#38646;&#33192;&#32960;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#32479;&#19968;&#39044;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#20132;&#36890;&#20107;&#25925;&#21644;&#29359;&#32618;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#24322;&#24120;&#39044;&#27979;&#65292;&#22914;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#21644;&#29359;&#32618;&#39044;&#27979;&#65292;&#23545;&#26234;&#24935;&#22478;&#24066;&#30340;&#23433;&#20840;&#21644;&#32500;&#25252;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#32500;&#24230;&#20869;&#30340;&#20869;&#37096;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#35768;&#22810;&#20851;&#38190;&#25361;&#25112;&#65292;&#20363;&#22914;&#65292;&#30001;&#20110;&#22478;&#24066;&#24322;&#24120;&#21457;&#29983;&#39057;&#29575;&#20302;&#23548;&#33268;&#30340;&#31232;&#30095;&#38646;&#33192;&#32960;&#25968;&#25454;&#65288;&#21487;&#33021;&#20250;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#65289;&#65292;&#20197;&#21450;&#36328;&#36234;&#31354;&#38388;&#65292;&#26102;&#38388;&#21644;&#35821;&#20041;&#32500;&#24230;&#30340;&#24322;&#24120;&#27169;&#24335;&#30340;&#20869;&#37096;&#21644;&#20114;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#38656;&#35201;&#25506;&#32034;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22810;&#31181;&#24322;&#24120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STS&#26469;&#20849;&#21516;&#25429;&#25417;&#19977;&#20010;&#32500;&#24230;&#20869;&#30340;&#27169;&#24335;&#21644;&#24433;&#21709;&#22240;&#32032;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#20114;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#23450;&#21046;&#25439;&#22833;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22359;&#26469;&#35299;&#20915;&#38646;&#33192;&#32960;&#38382;&#39064;&#12290;&#20026;&#20102;&#39564;&#35777;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#22478;&#24066;&#24322;&#24120;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#21644;&#29359;&#32618;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#19968;&#31995;&#21015;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban anomaly predictions, such as traffic accident prediction and crime prediction, are of vital importance to smart city security and maintenance. Existing methods typically use deep learning to capture the intra-dependencies in spatial and temporal dimensions. However, numerous key challenges remain unsolved, for instance, sparse zero-inflated data due to urban anomalies occurring with low frequency (which can lead to poor performance on real-world datasets), and both intra- and inter-dependencies of abnormal patterns across spatial, temporal, and semantic dimensions. Moreover, a unified approach to predict multiple kinds of anomaly is left to explore. In this paper, we propose STS to jointly capture the intra- and inter-dependencies between the patterns and the influential factors in three dimensions. Further, we use a multi-task prediction module with a customized loss function to solve the zero-inflated issue. To verify the effectiveness of the model, we apply it to two urban ano
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;ECG&#20449;&#21495;&#36827;&#34892;5&#31867;&#21644;17&#31867;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;&#36229;&#36731;&#37327;&#32423;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;(BNN)&#65292;&#22312;&#23384;&#20648;&#20351;&#29992;&#29575;&#26368;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;96.90%&#21644;97.50%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.01568</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#36731;&#37327;&#32423;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network. (arXiv:2304.01568v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;ECG&#20449;&#21495;&#36827;&#34892;5&#31867;&#21644;17&#31867;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;&#36229;&#36731;&#37327;&#32423;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;(BNN)&#65292;&#22312;&#23384;&#20648;&#20351;&#29992;&#29575;&#26368;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;96.90%&#21644;97.50%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#29702;&#26377;&#25928;&#22320;&#36890;&#36807;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#30417;&#27979;&#24515;&#24459;&#22833;&#24120;&#23545;&#20154;&#31867;&#20581;&#24247;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ECG&#20998;&#31867;&#31639;&#27861;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#22312;&#39640;&#31934;&#24230;&#21644;&#22797;&#26434;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#23548;&#33268;&#23384;&#20648;&#20351;&#29992;&#29575;&#21644;&#21151;&#32791;&#24456;&#39640;&#12290;&#36825;&#20063;&#19981;&#21487;&#36991;&#20813;&#22320;&#22686;&#21152;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21487;&#31359;&#25140;&#20154;&#24037;&#26234;&#33021;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#23454;&#29616;&#30340;&#38590;&#24230;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36229;&#36731;&#37327;&#32423;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;(BNN)&#65292;&#33021;&#22815;&#36890;&#36807;ECG&#20449;&#21495;&#36827;&#34892;5&#31867;&#21644;17&#31867;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;BNN&#22312;5&#31867;&#21644;17&#31867;&#20998;&#31867;&#20013;&#20998;&#21035;&#36798;&#21040;&#20102;96.90% (&#23436;&#20840;&#31934;&#24230;97.09%)&#21644;97.50% (&#23436;&#20840;&#31934;&#24230;98.00%)&#30340;&#20934;&#30830;&#29575;&#65292;&#23384;&#20648;&#20351;&#29992;&#29575;&#26368;&#20302;(3.76 KB&#21644;4.45 KB)&#12290;&#19982;&#20854;&#20182;&#20108;&#20540;&#21270;&#20316;&#21697;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25903;&#25345;&#20004;&#20010;&#22810;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasonably and effectively monitoring arrhythmias through ECG signals has significant implications for human health. With the development of deep learning, numerous ECG classification algorithms based on deep learning have emerged. However, most existing algorithms trade off high accuracy for complex models, resulting in high storage usage and power consumption. This also inevitably increases the difficulty of implementation on wearable Artificial Intelligence-of-Things (AIoT) devices with limited resources. In this study, we proposed a universally applicable ultra-lightweight binary neural network(BNN) that is capable of 5-class and 17-class arrhythmia classification based on ECG signals. Our BNN achieves 96.90% (full precision 97.09%) and 97.50% (full precision 98.00%) accuracy for 5-class and 17-class classification, respectively, with state-of-the-art storage usage (3.76 KB and 4.45 KB). Compared to other binarization works, our approach excels in supporting two multi-classificatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20026;&#22270;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#31185;&#23398;&#20013;AI&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#30340;&#32508;&#36848;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#20026;&#21508;&#39046;&#22495;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#36235;&#21183;&#65292;&#28085;&#30422;&#20102;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.01565</link><description>&lt;p&gt;
&#22270;&#25193;&#25955;&#27169;&#22411;&#32508;&#36848;&#65306;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#29983;&#25104;&#22411; AI
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material. (arXiv:2304.01565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20026;&#22270;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#31185;&#23398;&#20013;AI&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#30340;&#32508;&#36848;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#20026;&#21508;&#39046;&#22495;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#36235;&#21183;&#65292;&#28085;&#30422;&#20102;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#26032;&#30340; SOTA &#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#26377;&#22810;&#31687;&#32508;&#36848;&#25991;&#29486;&#20026;&#27492;&#36827;&#34892;&#20102;&#24635;&#20307;&#27010;&#36848;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#25991;&#31456;&#25968;&#37327;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#32508;&#36848;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#23545;&#22270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35843;&#26597;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#28085;&#30422;&#22270;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#39318;&#20808;&#31616;&#35201;&#24635;&#32467;&#20102;&#20854;&#20182;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#22312;&#22270;&#20013;&#30340;&#24212;&#29992;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#25193;&#25955;&#27169;&#22411;&#26426;&#21046;&#65292;&#36825;&#26377;&#21161;&#20110;&#35752;&#35770;&#22270;&#25193;&#25955;&#27169;&#22411;&#12290;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#24212;&#29992;&#20027;&#35201;&#24402;&#20026;&#31185;&#23398;&#20013;&#30340;&#29983;&#25104;&#22411; AI &#20869;&#23481;&#65288;AIGC&#65289;&#31867;&#21035;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#22270;&#25193;&#25955;&#27169;&#22411;&#22914;&#20309;&#29992;&#20110;&#29983;&#25104;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#65292;&#20294;&#20063;&#35206;&#30422;&#20854;&#20182;&#26696;&#20363;&#65292;&#21253;&#25324;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become a new SOTA generative modeling method in various fields, for which there are multiple survey works that provide an overall survey. With the number of articles on diffusion models increasing exponentially in the past few years, there is an increasing need for surveys of diffusion models on specific fields. In this work, we are committed to conducting a survey on the graph diffusion models. Even though our focus is to cover the progress of diffusion models in graphs, we first briefly summarize how other generative modeling methods are used for graphs. After that, we introduce the mechanism of diffusion models in various forms, which facilitates the discussion on the graph diffusion models. The applications of graph diffusion models mainly fall into the category of AI-generated content (AIGC) in science, for which we mainly focus on how graph diffusion models are utilized for generating molecules and proteins but also cover other cases, including materials des
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#23481;&#37327;&#65292;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#36924;&#36817;&#36895;&#29575;&#12290;&#24212;&#29992;&#21040;&#38750;&#21442;&#25968;&#22238;&#24402;&#19978;&#65292;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;H\"older&#20989;&#25968;&#30340;&#26368;&#20248;&#28176;&#36827;&#36895;&#29575;&#65292;&#34917;&#20805;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01561</link><description>&lt;p&gt;
Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#36895;&#29575;&#21450;&#20854;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression. (arXiv:2304.01561v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#23481;&#37327;&#65292;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#36924;&#36817;&#36895;&#29575;&#12290;&#24212;&#29992;&#21040;&#38750;&#21442;&#25968;&#22238;&#24402;&#19978;&#65292;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;H\"older&#20989;&#25968;&#30340;&#26368;&#20248;&#28176;&#36827;&#36895;&#29575;&#65292;&#34917;&#20805;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#19982;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#30340;&#21464;&#24322;&#31354;&#38388;&#30340;&#36924;&#36817;&#23481;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#21464;&#24322;&#33539;&#25968;&#19979;&#65292;&#23481;&#32435;&#20102;&#36275;&#22815;&#24179;&#28369;&#30340;&#20989;&#25968;&#12290;&#23545;&#20110;&#36739;&#23569;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#26681;&#25454;&#21464;&#24322;&#33539;&#25968;&#24314;&#31435;&#20102;&#36924;&#36817;&#36895;&#29575;&#12290;&#36816;&#29992;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#36924;&#36817;&#36895;&#29575;&#12290;&#21516;&#26102;&#38416;&#26126;&#20102;&#36825;&#20123;&#32467;&#26524;&#22914;&#20309;&#29992;&#20110;&#25512;&#23548;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#36924;&#36817;&#30028;&#38480;&#12290;&#20026;&#24212;&#29992;&#30740;&#31350;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#31181;ReLU&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#36229;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#21644;CNN&#36827;&#34892;&#38750;&#21442;&#25968;&#22238;&#24402;&#25910;&#25947;&#36895;&#29575;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;H\"older&#20989;&#25968;&#30340;&#26368;&#20248;&#28176;&#36827;&#36895;&#29575;&#65292;&#36825;&#34917;&#20805;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\"older functions, which complements recent results for deep neural networks. It is also proven th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36816;&#34892;&#22312;&#36793;&#32536;AI&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#39550;&#39542;&#21592;&#30417;&#25511;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32463;&#36807;&#27169;&#22411;&#25163;&#26415;&#65292;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#24110;&#21161;&#19979;&#23454;&#29616;&#20102;&#39640;&#24103;&#29575;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01555</link><description>&lt;p&gt;
&#36793;&#32536;AI&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#39550;&#39542;&#21592;&#30417;&#25511;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Real-time Driver Monitoring Systems on Edge AI Device. (arXiv:2304.01555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36816;&#34892;&#22312;&#36793;&#32536;AI&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#39550;&#39542;&#21592;&#30417;&#25511;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32463;&#36807;&#27169;&#22411;&#25163;&#26415;&#65292;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#24110;&#21161;&#19979;&#23454;&#29616;&#20102;&#39640;&#24103;&#29575;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21496;&#26426;&#19981;&#27880;&#24847;&#21147;&#23548;&#33268;&#36335;&#19978;&#20107;&#25925;&#30340;&#22686;&#21152;&#65292;&#33258;&#21160;&#21270;&#39550;&#39542;&#21592;&#30417;&#25511;&#31995;&#32479;(DMS)&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36816;&#34892;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;DMS&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#30001;&#32418;&#22806;&#25668;&#20687;&#22836;&#35760;&#24405;&#39550;&#39542;&#21592;&#30340;&#30011;&#38754;&#21644;&#36793;&#32536;&#35774;&#22791;&#22788;&#29702;&#25968;&#25454;&#32452;&#25104;&#12290;&#20026;&#20102;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#25104;&#21151;&#31227;&#26893;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#36827;&#34892;&#20102;&#27169;&#22411;&#25163;&#26415;&#12290;&#26368;&#32456;&#30340;DMS&#31995;&#32479;&#22312;TI-TDA4VM&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;63&#24103;&#27599;&#31186;(FPS)&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As road accident cases are increasing due to the inattention of the driver, automated driver monitoring systems (DMS) have gained an increase in acceptance. In this report, we present a real-time DMS system that runs on a hardware-accelerator-based edge device. The system consists of an InfraRed camera to record the driver footage and an edge device to process the data. To successfully port the deep learning models to run on the edge device taking full advantage of the hardware accelerators, model surgery was performed. The final DMS system achieves 63 frames per second (FPS) on the TI-TDA4VM edge device.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22826;&#38451;&#22823;&#27668;&#20013;&#19981;&#21516;&#32467;&#26500;&#30340;&#22826;&#38451;&#39118;&#28304;&#21306;&#12289;&#24418;&#25104;&#39640;&#24230;&#20197;&#21450;&#21152;&#28909;&#26426;&#21046;&#65292;&#25581;&#31034;&#22826;&#38451;&#39118;&#19982;&#26085;&#20885;&#31354;&#27934;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#27979;&#21644;&#35686;&#21578;&#36817;&#22320;&#29699;&#31354;&#38388;&#22825;&#27668;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01553</link><description>&lt;p&gt;
&#22826;&#38451;&#22823;&#27668;&#30340;&#21152;&#28909;&#21644;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Heating and dynamics of the Solar atmosphere. (arXiv:2304.01553v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22826;&#38451;&#22823;&#27668;&#20013;&#19981;&#21516;&#32467;&#26500;&#30340;&#22826;&#38451;&#39118;&#28304;&#21306;&#12289;&#24418;&#25104;&#39640;&#24230;&#20197;&#21450;&#21152;&#28909;&#26426;&#21046;&#65292;&#25581;&#31034;&#22826;&#38451;&#39118;&#19982;&#26085;&#20885;&#31354;&#27934;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#27979;&#21644;&#35686;&#21578;&#36817;&#22320;&#29699;&#31354;&#38388;&#22825;&#27668;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#22823;&#27668;&#20174;5500 K&#30340;&#20809;&#29699;&#21040;&#30334;&#19975;&#24230;&#24320;&#23572;&#25991;&#30340;&#26085;&#20885;&#37117;&#21576;&#29616;&#20986;&#24322;&#24120;&#30340;&#28201;&#24230;&#21464;&#21270;&#12290;&#26085;&#20885;&#26412;&#36523;&#21521;&#26143;&#38469;&#20171;&#36136;&#25193;&#23637;&#20026;&#27969;&#21160;&#30340;&#22826;&#38451;&#39118;&#65292;&#24433;&#21709;&#30528;&#36817;&#22320;&#29699;&#31354;&#38388;&#22825;&#27668;&#12290;&#19981;&#21516;&#26085;&#20885;&#32467;&#26500;&#30340;&#31934;&#30830;&#28304;&#21306;&#12289;&#20854;&#24418;&#25104;&#39640;&#24230;&#20197;&#21450;&#22826;&#38451;&#22823;&#27668;&#30340;&#21152;&#28909;&#22312;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#26159;&#23494;&#19981;&#21487;&#20998;&#19988;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#20919;&#21364;&#12289;&#24378;&#24230;&#32570;&#38519;&#32467;&#26500;&#30340;&#26085;&#20885;&#31354;&#27934;(Coronal Holes, CHs)&#19982;&#22826;&#38451;&#39118;&#20013;&#30340;&#32467;&#26500;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#35266;&#27979;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#24130;&#24459;&#20998;&#24067;&#30340;&#33033;&#20914;&#20107;&#20214;&#22312;&#26085;&#20885;&#20013;&#23616;&#37096;&#21152;&#28909;&#31561;&#12290;&#26412;&#25991;&#20351;&#29992;&#29421;&#24102;&#20809;&#24230;&#35745;&#12289;&#20809;&#35889;&#23398;&#12289;&#20197;&#21450;&#20174;&#36817;&#32043;&#22806;&#32447;&#21040;X&#23556;&#32447;&#30340;&#22826;&#38451;&#22823;&#27668;&#30340;&#30424;&#38469;&#36752;&#23556;&#65292;&#32467;&#21512;&#29616;&#22330;&#22826;&#38451;&#39118;&#27979;&#37327;&#65292;&#26469;&#29702;&#35299;&#65288;i&#65289;&#19981;&#21516;&#32467;&#26500;&#30340;&#22826;&#38451;&#39118;&#28304;&#21306;&#12289;&#65288;ii&#65289;&#23427;&#20204;&#22312;&#21738;&#37324;&#24418;&#25104;&#12289;&#65288;iii&#65289;&#26085;&#20885;&#20013;&#30340;&#21152;&#28909;&#26159;&#22914;&#20309;&#21457;&#29983;&#30340;&#65292;&#20197;&#21450;&#65288;iv&#65289;&#22914;&#20309;&#20026;&#36817;&#22320;&#31354;&#38388;&#22825;&#27668;&#25552;&#20379;&#39044;&#27979;&#21644;&#39044;&#35686;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solar atmosphere shows anomalous variation in temperature, starting from the 5500 K photosphere to the million-degree Kelvin corona. The corona itself expands into the interstellar medium as the free streaming solar wind, which modulates and impacts the near-Earth space weather. The precise source regions of different structures in the solar wind, their formation height, and the heating of the solar atmosphere are inextricably linked and unsolved problems in astrophysics. Observations suggest correlations between Coronal holes (CHs), which are cool, intensity deficit structures in the solar corona, with structures in the solar wind. Observations also suggest the local plasma heating in the corona through power-law distributed impulsive events. In this thesis, we use narrowband photometric, spectroscopic, and disc-integrated emission of the solar atmosphere ranging from Near Ultraviolet to X-rays along with in-situ solar wind measurements to understand (i). the source regions of the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01545</link><description>&lt;p&gt;
&#21306;&#22495;&#39118;&#21147;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;CNN&#30340;&#39118;&#36895;&#39044;&#27979;&#65306;&#26469;&#33258;&#26102;&#31354;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis. (arXiv:2304.01545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#31354;&#25968;&#25454;&#32500;&#24230;&#23545;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21152;&#20837;&#31354;&#38388;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#25913;&#36827;&#30340;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#20339;&#26102;&#38388;&#38271;&#24230;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#30740;&#31350;&#20063;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#22312;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#39044;&#27979;&#39118;&#36895;&#26102;&#65292;&#37319;&#29992;&#20855;&#26377;&#19981;&#21516;&#26102;&#31354;&#32500;&#24230;&#30340;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#35780;&#20272;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22810;&#26102;&#38388;&#25968;&#25454;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the impact of spatiotemporal data dimensions on the precision of a wind forecasting model developed using an artificial neural network. Although previous studies have shown that incorporating spatial data can enhance the accuracy of wind forecasting models, few investigations have explored the extent of the improvement owing to different spatial scales in neural network-based predictive models. Additionally, there are limited studies on the optimal temporal length of the input data for these models. To address this gap, this study employs data with various spatiotemporal dimensions as inputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN) and assesses their predictive performance. The results indicate that using spatial data of the surrounding area for 3D-CNN training can achieve better predictive performance than using only single-point information. Additionally, multi-time data had a more positive effect on the predictive performance than
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#21644;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#24179;&#22343;&#20540;&#21644;&#39057;&#29575;&#20272;&#35745;&#30340;&#26368;&#20248;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#27599;&#20010;&#23458;&#25143;&#31471;&#21482;&#38656;&#21457;&#36865;$\Theta\left( n \min\left(\varepsilon, \varepsilon^2\right)\right)$&#27604;&#29305;&#30340;FL&#38382;&#39064;&#21644;$\Theta\left(\log\left( n\min\left(\varepsilon, \varepsilon^2\right) \right)\right)$&#27604;&#29305;&#30340;FA&#38382;&#39064;&#21363;&#21487;&#23454;&#29616;&#26368;&#20248;&#35823;&#24046;&#65292;&#20174;&#32780;&#22312;&#32852;&#21512;&#23398;&#20064;&#21644;&#20998;&#26512;&#20013;&#23454;&#29616;&#20102;&#38544;&#31169;&#12289;&#31934;&#30830;&#24615;&#21644;&#36890;&#20449;&#30340;&#26368;&#20248;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.01541</link><description>&lt;p&gt;
&#36890;&#36807;&#21387;&#32553;&#23454;&#29616;&#38544;&#31169;&#25918;&#22823;&#65306;&#22312;&#20998;&#24067;&#24335;&#22343;&#20540;&#20272;&#35745;&#20013;&#23454;&#29616;&#26368;&#20248;&#38544;&#31169;-&#31934;&#24230;-&#36890;&#20449;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation. (arXiv:2304.01541v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#21644;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#24179;&#22343;&#20540;&#21644;&#39057;&#29575;&#20272;&#35745;&#30340;&#26368;&#20248;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#27599;&#20010;&#23458;&#25143;&#31471;&#21482;&#38656;&#21457;&#36865;$\Theta\left( n \min\left(\varepsilon, \varepsilon^2\right)\right)$&#27604;&#29305;&#30340;FL&#38382;&#39064;&#21644;$\Theta\left(\log\left( n\min\left(\varepsilon, \varepsilon^2\right) \right)\right)$&#27604;&#29305;&#30340;FA&#38382;&#39064;&#21363;&#21487;&#23454;&#29616;&#26368;&#20248;&#35823;&#24046;&#65292;&#20174;&#32780;&#22312;&#32852;&#21512;&#23398;&#20064;&#21644;&#20998;&#26512;&#20013;&#23454;&#29616;&#20102;&#38544;&#31169;&#12289;&#31934;&#30830;&#24615;&#21644;&#36890;&#20449;&#30340;&#26368;&#20248;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#21644;&#36890;&#20449;&#32422;&#26463;&#26159;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#20998;&#26512;&#65288;FA&#65289;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32852;&#21512;&#36890;&#20449;&#21644;$(\varepsilon, \delta)$-&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#32422;&#26463;&#19979;&#24179;&#22343;&#20540;&#21644;&#39057;&#29575;&#20272;&#35745;&#65288;FL&#21644;FA&#30340;&#26631;&#20934;&#27169;&#22411;&#65289;&#30340;&#26368;&#20248;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20026;&#20102;&#22312;$(\varepsilon, \delta)$-DP&#19979;&#36798;&#21040;&#26368;&#20248;&#35823;&#24046;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#21482;&#38656;&#35201;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;$\Theta\left( n \min\left(\varepsilon, \varepsilon^2\right)\right)$&#27604;&#29305;&#30340;FL&#38382;&#39064;&#21644;$\Theta\left(\log\left( n\min\left(\varepsilon, \varepsilon^2\right) \right)\right)$&#27604;&#29305;&#30340;FA&#38382;&#39064;&#12290;&#22914;&#26524;&#27809;&#26377;&#21387;&#32553;&#65292;&#27599;&#20010;&#23458;&#25143;&#26426;&#38656;&#35201;$O(d)$&#27604;&#29305;&#21644;$\log d$&#27604;&#29305;&#26469;&#35299;&#20915;&#24179;&#22343;&#20540;&#20272;&#35745;&#21644;&#39057;&#29575;&#20272;&#35745;&#38382;&#39064;&#65288;&#20854;&#20013;$d$&#23545;&#24212;&#20110;FL&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#25110;FA&#20013;&#22495;&#30340;&#22823;&#23567;&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#22312;$n\min\left(\varepsilon,\varepsilon^2\right)$&#30340;&#21306;&#38388;&#20013;&#33719;&#24471;&#26174;&#33879;&#30340;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy and communication constraints are two major bottlenecks in federated learning (FL) and analytics (FA). We study the optimal accuracy of mean and frequency estimation (canonical models for FL and FA respectively) under joint communication and $(\varepsilon, \delta)$-differential privacy (DP) constraints. We show that in order to achieve the optimal error under $(\varepsilon, \delta)$-DP, it is sufficient for each client to send $\Theta\left( n \min\left(\varepsilon, \varepsilon^2\right)\right)$ bits for FL and $\Theta\left(\log\left( n\min\left(\varepsilon, \varepsilon^2\right) \right)\right)$ bits for FA to the server, where $n$ is the number of participating clients. Without compression, each client needs $O(d)$ bits and $\log d$ bits for the mean and frequency estimation problems respectively (where $d$ corresponds to the number of trainable parameters in FL or the domain size in FA), which means that we can get significant savings in the regime $ n \min\left(\varepsilon, \va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20960;&#20046;&#30830;&#23450;&#30340;&#26041;&#24335;&#25910;&#25947;&#21040; $\mu$ &#30340;&#24322;&#27493;&#22312;&#32447;&#31639;&#27861;&#65292;&#24212;&#29992;&#20102;&#24494;&#20998;&#21253;&#23481;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#20142;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.01525</link><description>&lt;p&gt;
&#24102;&#23545;&#25163;&#30340;&#22312;&#32447;&#23398;&#20064;&#65306;&#24494;&#20998;&#21253;&#23481;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Online Learning with Adversaries: A Differential Inclusion Analysis. (arXiv:2304.01525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20960;&#20046;&#30830;&#23450;&#30340;&#26041;&#24335;&#25910;&#25947;&#21040; $\mu$ &#30340;&#24322;&#27493;&#22312;&#32447;&#31639;&#27861;&#65292;&#24212;&#29992;&#20102;&#24494;&#20998;&#21253;&#23481;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#20142;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#27979;&#37327;&#27169;&#22411; $Y = AX$&#65292;&#20854;&#20013; $X$ &#21644; $Y$ &#26159;&#38543;&#26426;&#21464;&#37327;&#65292;$A$ &#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#39640;&#30697;&#38453;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#23454;&#20363;&#65292;&#21487;&#20197;&#33719;&#24471; $Y$ &#30340;&#19968;&#20010;&#22352;&#26631;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#36890;&#36807;&#36825;&#20123;&#26679;&#26412;&#20272;&#35745; $\mu := \mathbb{E}[X]$&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#65306;&#23567;&#20294;&#26410;&#30693;&#30340; $Y$ &#30340;&#22352;&#26631;&#23376;&#38598;&#30001;&#23545;&#25163;&#25511;&#21046;&#65292;&#24182;&#20855;&#26377;&#26080;&#38480;&#30340;&#33021;&#21147;&#65306;&#27599;&#27425;&#26597;&#35810;&#26679;&#26412;&#26102;&#65292;&#20182;&#20204;&#21487;&#20197;&#36820;&#22238;&#20219;&#20309;&#23454;&#25968;&#12290;&#23545;&#20110;&#36825;&#31181;&#23545;&#25239;&#24615;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20960;&#20046;&#30830;&#23450;&#30340;&#26041;&#24335;&#25910;&#25947;&#21040; $\mu$ &#30340;&#24322;&#27493;&#22312;&#32447;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#20998;&#21253;&#23481;&#22522;&#20110;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#20004;&#20010;&#20851;&#38190;&#20142;&#28857;&#21253;&#25324;&#65306;(a) &#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340; Lyapunov &#20989;&#25968;&#26469;&#35777;&#26126; $\mu$ &#26159;&#25105;&#20204;&#31639;&#27861;&#26497;&#38480;&#21160;&#24577;&#30340;&#21807;&#19968;&#20840;&#23616;&#21560;&#24341;&#23376;&#65292;(b) &#20351;&#29992;&#38789;&#21644;&#20572;&#26102;&#29702;&#35770;&#26469;&#35777;&#26126;&#25105;&#20204;&#31639;&#27861;&#30340;&#36845;&#20195;&#20960;&#20046;&#24517;&#23450;&#26377;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the measurement model $Y = AX,$ where $X$ and, hence, $Y$ are random variables and $A$ is an a priori known tall matrix. At each time instance, a sample of one of $Y$'s coordinates is available, and the goal is to estimate $\mu := \mathbb{E}[X]$ via these samples. However, the challenge is that a small but unknown subset of $Y$'s coordinates are controlled by adversaries with infinite power: they can return any real number each time they are queried for a sample. For such an adversarial setting, we propose the first asynchronous online algorithm that converges to $\mu$ almost surely. We prove this result using a novel differential inclusion based two-timescale analysis. Two key highlights of our proof include: (a) the use of a novel Lyapunov function for showing that $\mu$ is the unique global attractor for our algorithm's limiting dynamics, and (b) the use of martingale and stopping time theory to show that our algorithm's iterates are almost surely bounded.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#12289;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#21644;&#26657;&#20934;&#39044;&#27979;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#32463;&#23454;&#39564;&#34920;&#26126;&#22312;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#23545;&#20110;&#22122;&#22768;&#26679;&#26412;&#20855;&#26377;&#33391;&#22909;&#25269;&#25239;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#39046;&#22495;&#20043;&#22806;&#30340;&#26816;&#27979;&#26159;&#21487;&#38752;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.01518</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multimodal Neural Processes for Uncertainty Estimation. (arXiv:2304.01518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#12289;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#21644;&#26657;&#20934;&#39044;&#27979;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#32463;&#23454;&#39564;&#34920;&#26126;&#22312;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#23545;&#20110;&#22122;&#22768;&#26679;&#26412;&#20855;&#26377;&#33391;&#22909;&#25269;&#25239;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#39046;&#22495;&#20043;&#22806;&#30340;&#26816;&#27979;&#26159;&#21487;&#38752;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243;( Neural Processes, NPs)&#23558;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#38750;&#21442;&#25968;&#39640;&#26031;&#36807;&#31243;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#32467;&#21512;&#22312;&#20102;&#19968;&#36215;&#12290;&#34429;&#28982;&#26368;&#36817;NPs&#30340;&#21457;&#23637;&#24050;&#32463;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#22914;&#20309;&#23558;NPs&#36866;&#24212;&#22810;&#27169;&#24577;&#25968;&#25454;&#23578;&#26410;&#21463;&#21040;&#20180;&#32454;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NP&#23478;&#26063;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#25972;&#20307;&#30340;&#12289;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#20998;&#31867;&#35823;&#24046;&#26356;&#26032;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#65292;&#19968;&#20010;&#32858;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#26426;&#21046;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#26657;&#20934;&#39044;&#27979;&#30340;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#21560;&#24341;&#21147;&#65292;&#21363;&#33021;&#22815;&#25269;&#25239;&#22122;&#22768;&#26679;&#26412;&#30340;&#24178;&#25200;&#65292;&#24182;&#21487;&#38752;&#22320;&#22312;&#39046;&#22495;&#20043;&#22806;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural processes (NPs) have brought the representation power of parametric deep neural networks and the reliable uncertainty estimation of non-parametric Gaussian processes together. Although recent development of NPs has shown success in both regression and classification, how to adapt NPs to multimodal data has not be carefully studied. For the first time, we propose a new model of NP family for multimodal uncertainty estimation, namely Multimodal Neural Processes. In a holistic and principled way, we develop a dynamic context memory updated by the classification error, a multimodal Bayesian aggregation mechanism to aggregate multimodal representations, and a new attention mechanism for calibrated predictions. In extensive empirical evaluation, our method achieves the state-of-the-art multimodal uncertainty estimation performance, showing its appealing ability of being robust against noisy samples and reliable in out-of-domain detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20840;&#29699;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#22635;&#34917;&#20102;&#22788;&#29702;&#20998;&#31867;&#39046;&#22495;&#20013;&#27010;&#24565;&#28418;&#31227;&#26041;&#27861;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.01512</link><description>&lt;p&gt;
&#20840;&#29699;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Handling Concept Drift in Global Time Series Forecasting. (arXiv:2304.01512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20840;&#29699;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#22635;&#34917;&#20102;&#22788;&#29702;&#20998;&#31867;&#39046;&#22495;&#20013;&#27010;&#24565;&#28418;&#31227;&#26041;&#27861;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#24182;&#20551;&#35774;&#25968;&#25454;&#22312;&#20135;&#29983;&#39044;&#27979;&#26102;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#24179;&#31283;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#20998;&#24067;&#19981;&#26159;&#31283;&#24577;&#30340;&#65292;&#32780;&#23427;&#20204;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#25913;&#21464;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#34987;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#12290;&#22788;&#29702;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#23545;&#20110;&#35768;&#22810;&#29616;&#20170;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#20840;&#29699;&#39044;&#27979;&#27169;&#22411;&#65288;GFM&#65289;&#20013;&#22788;&#29702;&#27010;&#24565;&#28418;&#31227;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65306;&#35823;&#24046;&#36129;&#29486;&#21152;&#26435;&#65288;ECW&#65289;&#21644;&#26799;&#24230;&#19979;&#38477;&#21152;&#26435;&#65288;GDW&#65289;&#65292;&#22522;&#20110;&#36830;&#32493;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#27010;&#24565;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) based time series forecasting models often require and assume certain degrees of stationarity in the data when producing forecasts. However, in many real-world situations, the data distributions are not stationary and they can change over time while reducing the accuracy of the forecasting models, which in the ML literature is known as concept drift. Handling concept drift in forecasting is essential for many ML methods in use nowadays, however, the prior work only proposes methods to handle concept drift in the classification domain. To fill this gap, we explore concept drift handling methods in particular for Global Forecasting Models (GFM) which recently have gained popularity in the forecasting domain. We propose two new concept drift handling methods, namely: Error Contribution Weighting (ECW) and Gradient Descent Weighting (GDW), based on a continuous adaptive weighting concept. These methods use two forecasting models which are separately trained with the m
&lt;/p&gt;</description></item><item><title>EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2304.01508</link><description>&lt;p&gt;
EPVT: &#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#22312;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#39046;&#22495;&#19968;&#33324;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition. (arXiv:2304.01508v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01508
&lt;/p&gt;
&lt;p&gt;
EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#37096;&#32626;&#36825;&#20123;&#31995;&#32479;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#20110;&#19982;&#30142;&#30149;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#26263;&#35282;&#12289;&#27987;&#23494;&#27611;&#21457;&#65289;&#65292;&#23548;&#33268;&#22312;&#30475;&#19981;&#35265;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#19968;&#33324;&#21270;&#26041;&#27861;&#8212;&#8212;EPVT&#65292;&#23427;&#23558;&#25552;&#31034;&#23884;&#20837;&#21040;Vision Transformer&#20013;&#65292;&#20197;&#21327;&#21516;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPVT&#21033;&#29992;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#65292;&#27599;&#20010;&#39046;&#22495;&#25552;&#31034;&#37117;&#25198;&#28436;&#39046;&#22495;&#19987;&#23478;&#30340;&#35282;&#33394;&#65292;&#20197;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65307;&#20197;&#21450;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#33719;&#24471;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#21644;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#23427;&#20351;&#24471;&#39046;&#22495;&#25552;&#31034;&#19982;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#20302;&#31209;&#20056;&#24615;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A
&lt;/p&gt;</description></item><item><title>RARE&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22312;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01507</link><description>&lt;p&gt;
RARE&#65306;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
RARE: Robust Masked Graph Autoencoder. (arXiv:2304.01507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01507
&lt;/p&gt;
&lt;p&gt;
RARE&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22312;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;MGAE&#65289;&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#26377;&#25928;&#30340;&#29305;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#22270;&#39044;&#35757;&#32451;&#65288;SGP&#65289;&#26041;&#38754;&#24050;&#25104;&#20026;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#20013;&#25191;&#34892;&#25513;&#30721;-&#37325;&#26500;&#25805;&#20316;&#65292;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#32780;&#24573;&#30053;&#20102;&#22270;&#25968;&#25454;&#30340;&#37325;&#35201;&#38750;&#27431;&#20960;&#37324;&#24471;&#23646;&#24615;&#12290;&#32467;&#26524;&#65292;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#32467;&#26500;&#22823;&#22823;&#22686;&#21152;&#20102;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#38477;&#20302;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#65292;&#23548;&#33268;&#19979;&#28216;&#35780;&#20272;&#20013;&#30340;&#34920;&#31034;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SGP&#26041;&#27861;&#65292;&#31216;&#20026;Robust mAsked gRaph autoEncoder&#65288;RARE&#65289;&#65292;&#36890;&#36807;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26356;&#22810;&#30340;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;RARE&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked graph autoencoder (MGAE) has emerged as a promising self-supervised graph pre-training (SGP) paradigm due to its simplicity and effectiveness. However, existing efforts perform the mask-then-reconstruct operation in the raw data space as is done in computer vision (CV) and natural language processing (NLP) areas, while neglecting the important non-Euclidean property of graph data. As a result, the highly unstable local connection structures largely increase the uncertainty in inferring masked data and decrease the reliability of the exploited self-supervision signals, leading to inferior representations for downstream evaluations. To address this issue, we propose a novel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the certainty in inferring masked data and the reliability of the self-supervision mechanism by further masking and reconstructing node samples in the high-order latent feature space. Through both theoretical and empirical analyses, we have dis
&lt;/p&gt;</description></item><item><title>OneShotSTL&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#65292;&#22312;&#22788;&#29702;&#26102;&#38388;&#19978;&#20165;&#38656;O(1)&#30340;&#26356;&#26032;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25209;&#22788;&#29702;&#26041;&#27861;&#26080;&#27861;&#25903;&#25345;&#23454;&#26102;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.01506</link><description>&lt;p&gt;
OneShotSTL&#65306;&#19968;&#31181;&#21333;&#27425;&#23395;&#33410;&#36235;&#21183;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series Anomaly Detection And Forecasting. (arXiv:2304.01506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01506
&lt;/p&gt;
&lt;p&gt;
OneShotSTL&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#65292;&#22312;&#22788;&#29702;&#26102;&#38388;&#19978;&#20165;&#38656;O(1)&#30340;&#26356;&#26032;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25209;&#22788;&#29702;&#26041;&#27861;&#26080;&#27861;&#25903;&#25345;&#23454;&#26102;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23395;&#33410;&#36235;&#21183;&#20998;&#35299;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#20043;&#19968;&#65292;&#23427;&#25903;&#25345;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#22312;&#20869;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#35299;&#26041;&#27861;&#20381;&#36182;&#20110;&#25209;&#22788;&#29702;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(W)&#65292;&#20854;&#20013;W&#26159;&#26102;&#38388;&#31383;&#21475;&#20869;&#30340;&#25968;&#25454;&#28857;&#25968;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#22987;&#32456;&#26377;&#25928;&#22320;&#25903;&#25345;&#38656;&#35201;&#20302;&#22788;&#29702;&#24310;&#36831;&#30340;&#23454;&#26102;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OneShotSTL&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#19978;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#35299;&#65292;&#26356;&#26032;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(1)&#12290;OneShotSTL&#27604;&#25209;&#22788;&#29702;&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#65292;&#31934;&#24230;&#19982;&#26368;&#20339;&#23545;&#25163;&#30456;&#24403;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#19979;&#28216;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;OneShotSTL&#27604;&#29616;&#26377;&#25216;&#26415;&#24555;10&#20493;&#20197;&#19978;&#65292;&#21516;&#26102;&#20173;&#25552;&#20379;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seasonal-trend decomposition is one of the most fundamental concepts in time series analysis that supports various downstream tasks, including time series anomaly detection and forecasting. However, existing decomposition methods rely on batch processing with a time complexity of O(W), where W is the number of data points within a time window. Therefore, they cannot always efficiently support real-time analysis that demands low processing delay. To address this challenge, we propose OneShotSTL, an efficient and accurate algorithm that can decompose time series online with an update time complexity of O(1). OneShotSTL is more than $1,000$ times faster than the batch methods, with accuracy comparable to the best counterparts. Extensive experiments on real-world benchmark datasets for downstream time series anomaly detection and forecasting tasks demonstrate that OneShotSTL is from 10 to over 1,000 times faster than the state-of-the-art methods, while still providing comparable or even be
&lt;/p&gt;</description></item><item><title>SLPerf&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#30740;&#31350;&#21644;&#24320;&#25918;&#24335;&#30740;&#31350;&#24211;&#65292;&#29992;&#20110;&#20849;&#20139;&#23398;&#20064;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#24773;&#20917;&#19979;&#19981;&#21516;&#20849;&#20139;&#23398;&#20064;&#33539;&#24335;&#30340;&#22522;&#20934;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#25913;&#36827;&#20849;&#20139;&#23398;&#20064;&#33539;&#24335;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01502</link><description>&lt;p&gt;
SLPerf&#65306;&#22522;&#20934;&#27979;&#35797;&#20849;&#20139;&#23398;&#20064;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SLPerf: a Unified Framework for Benchmarking Split Learning. (arXiv:2304.01502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01502
&lt;/p&gt;
&lt;p&gt;
SLPerf&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#30740;&#31350;&#21644;&#24320;&#25918;&#24335;&#30740;&#31350;&#24211;&#65292;&#29992;&#20110;&#20849;&#20139;&#23398;&#20064;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#24773;&#20917;&#19979;&#19981;&#21516;&#20849;&#20139;&#23398;&#20064;&#33539;&#24335;&#30340;&#22522;&#20934;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#25913;&#36827;&#20849;&#20139;&#23398;&#20064;&#33539;&#24335;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#26085;&#30410;&#20005;&#37325;&#65292;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#20013;&#22830;&#21270;&#35757;&#32451;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#38656;&#35201;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#26694;&#26550;&#65306;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#12290;&#34429;&#28982;FL&#24050;&#32463;&#24314;&#31435;&#20102;&#21508;&#31181;&#22522;&#20934;&#26694;&#26550;&#21644;&#30740;&#31350;&#24211;&#65292;&#20294;SL&#30446;&#21069;&#23578;&#32570;&#20047;&#32479;&#19968;&#30340;&#24211;&#65292;&#23588;&#20854;&#26159;&#22312;&#26631;&#31614;&#20849;&#20139;&#12289;&#27169;&#22411;&#32858;&#21512;&#21644;&#20999;&#21106;&#23618;&#36873;&#25321;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#36825;&#31181;&#26631;&#20934;&#21270;&#32570;&#20047;&#20351;&#24471;&#27604;&#36739;SL&#33539;&#24335;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLPerf&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20849;&#20139;&#23398;&#20064;&#30740;&#31350;&#26694;&#26550;&#21644;&#24320;&#25918;&#24335;&#30740;&#31350;&#24211;&#65292;&#24182;&#22312;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;IID&#21644;&#38750;IID&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;SL&#33539;&#24335;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#19981;&#21516;SL&#33539;&#24335;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#36827;&#34892;&#35814;&#32454;&#30340;&#22522;&#20934;&#27604;&#36739;&#65292;&#20197;&#21450;&#29992;&#20110;&#25913;&#36827;SL&#33539;&#24335;&#30340;&#20016;&#23500;&#24037;&#31243;&#24102;&#36208;&#20449;&#24687;&#21644;&#30740;&#31350;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy concerns has made centralized training of data, which is scattered across silos, infeasible, leading to the need for collaborative learning frameworks. To address that, two prominent frameworks emerged, i.e., federated learning (FL) and split learning (SL). While FL has established various benchmark frameworks and research libraries, SL currently lacks a unified library despite its diversity in terms of label sharing, model aggregation, and cut layer choice. This lack of standardization makes comparing SL paradigms difficult. To address this, we propose SLPerf, a unified research framework and open research library for SL, and conduct extensive experiments on four widely-used datasets under both IID and Non-IID data settings. Our contributions include a comprehensive survey of recently proposed SL paradigms, a detailed benchmark comparison of different SL paradigms in different situations, and rich engineering take-away messages and research insights for improving SL parad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24863;&#30693;&#30340;&#34893;&#23556;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#31895;&#31961;&#24230;&#24314;&#27169;&#27491;&#21017;&#21270;&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;DONN&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25552;&#20379;&#23454;&#38469;&#37096;&#32626;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.01500</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#30340;&#31895;&#31961;&#24230;&#20248;&#21270;&#29992;&#20110;&#34893;&#23556;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Roughness Optimization for Diffractive Optical Neural Networks. (arXiv:2304.01500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24863;&#30693;&#30340;&#34893;&#23556;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#31895;&#31961;&#24230;&#24314;&#27169;&#27491;&#21017;&#21270;&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;DONN&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25552;&#20379;&#23454;&#38469;&#37096;&#32626;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34893;&#23556;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65288;DONN&#65289;&#20316;&#20026;&#19968;&#31181;&#19979;&#19968;&#20195;&#35774;&#22791;/&#30005;&#36335;&#25216;&#26415;&#24050;&#26174;&#31034;&#20986;&#27604;&#20256;&#32479;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#65288;&#20809;&#36895;&#65289;&#21644;&#20302;&#33021;&#32791;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34893;&#23556;&#23618;&#20869;&#30340;&#20687;&#32032;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;DONN&#30340;&#25968;&#20540;&#24314;&#27169;&#21644;&#29289;&#29702;&#20809;&#23398;&#22120;&#20214;&#37096;&#32626;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24863;&#30693;&#30340;&#34893;&#23556;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20943;&#23569;&#25968;&#20540;&#27169;&#25311;&#21644;&#23454;&#38469;&#37096;&#32626;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#31895;&#31961;&#24230;&#24314;&#27169;&#27491;&#21017;&#21270;&#65292;&#24182;&#38598;&#25104;&#20102;&#29289;&#29702;&#24863;&#30693;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#24341;&#20837;&#31232;&#30095;&#24615;&#21040;&#30456;&#20301;&#25513;&#27169;&#20013;&#65292;&#20197;&#20943;&#23569;&#34893;&#23556;&#23618;&#20013;&#30456;&#37051;&#20687;&#32032;&#20043;&#38388;&#30340;&#23574;&#38160;&#30456;&#20301;&#21464;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;$2\pi$&#21608;&#26399;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#32771;&#34385;&#29289;&#29702;&#34893;&#23556;&#36807;&#31243;&#24182;&#38477;&#20302;&#30456;&#20301;&#25513;&#33180;&#30340;&#31895;&#31961;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;DONN&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#24182;&#20026;DONN&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a representative next-generation device/circuit technology beyond CMOS, diffractive optical neural networks (DONNs) have shown promising advantages over conventional deep neural networks due to extreme fast computation speed (light speed) and low energy consumption. However, there is a mismatch, i.e., significant prediction accuracy loss, between the DONN numerical modelling and physical optical device deployment, because of the interpixel interaction within the diffractive layers. In this work, we propose a physics-aware diffractive optical neural network training framework to reduce the performance difference between numerical modeling and practical deployment. Specifically, we propose the roughness modeling regularization in the training process and integrate the physics-aware sparsification method to introduce sparsity to the phase masks to reduce sharp phase changes between adjacent pixels in diffractive layers. We further develop $2\pi$ periodic optimization to reduce the roug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;LSTM&#30340;&#22810;&#27169;&#22411;&#26694;&#26550;&#26469;&#36827;&#34892;&#36712;&#36857;&#20851;&#32852;&#65292;&#33021;&#22815;&#26356;&#22909;&#30340;&#25429;&#25417;&#33337;&#21482;&#36712;&#36857;&#30340;&#19981;&#21516;&#27169;&#24335;&#21644;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#20851;&#32852;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01491</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#25968;&#25454;&#30340;&#36712;&#36857;&#20851;&#32852;&#30340;&#22810;&#27169;&#22411;LSTM&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Multi model LSTM architecture for Track Association based on Automatic Identification System Data. (arXiv:2304.01491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;LSTM&#30340;&#22810;&#27169;&#22411;&#26694;&#26550;&#26469;&#36827;&#34892;&#36712;&#36857;&#20851;&#32852;&#65292;&#33021;&#22815;&#26356;&#22909;&#30340;&#25429;&#25417;&#33337;&#21482;&#36712;&#36857;&#30340;&#19981;&#21516;&#27169;&#24335;&#21644;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#20851;&#32852;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#33322;&#36857;&#20851;&#32852;&#19968;&#30452;&#26159;&#28023;&#27915;&#30417;&#35270;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#23545;&#33337;&#21482;&#35266;&#27979;&#25968;&#25454;&#30340;&#35782;&#21035;&#21644;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#65288;AIS&#65289;&#36890;&#36807;&#25552;&#20379;&#22823;&#37327;&#30340;&#33337;&#21482;&#21160;&#24577;&#21644;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26426;&#20250;&#12290;&#38543;&#30528;&#22914;&#27492;&#22823;&#22411;&#25968;&#25454;&#24211;&#30340;&#21487;&#29992;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#29616;&#22312;&#21487;&#20197;&#24320;&#21457;&#22797;&#26434;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#36712;&#36857;&#20851;&#32852;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#36712;&#36857;&#20851;&#32852;&#29616;&#22312;&#21487;&#20197;&#34987;&#35270;&#20026;&#26159;&#19968;&#20010;&#25968;&#25454;&#23494;&#38598;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#22810;&#27169;&#22411;&#26694;&#26550;&#26469;&#36827;&#34892;&#36712;&#36857;&#20851;&#32852;&#12290; LSTM&#26159;&#19968;&#31181;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#39034;&#24207;&#26041;&#24335;&#22788;&#29702;&#22810;&#20803;&#26102;&#38388;&#25968;&#25454;&#65292;&#20174;&#32780;&#39044;&#27979;&#33337;&#21482;&#30340;&#24403;&#21069;&#36712;&#36857;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#38598;&#25104;&#20102;&#22810;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20102;&#33337;&#21482;&#36712;&#36857;&#30340;&#19981;&#21516;&#27169;&#24335;&#21644;&#29305;&#24449;&#65292;&#20363;&#22914;&#33337;&#21482;&#30340;&#26041;&#21521;&#65292;&#36895;&#24230;&#21644;&#20301;&#32622;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36712;&#36857;&#20851;&#32852;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;AIS&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
For decades, track association has been a challenging problem in marine surveillance, which involves the identification and association of vessel observations over time. However, the Automatic Identification System (AIS) has provided a new opportunity for researchers to tackle this problem by offering a large database of dynamic and geo-spatial information of marine vessels. With the availability of such large databases, researchers can now develop sophisticated models and algorithms that leverage the increased availability of data to address the track association challenge effectively. Furthermore, with the advent of deep learning, track association can now be approached as a data-intensive problem. In this study, we propose a Long Short-Term Memory (LSTM) based multi-model framework for track association. LSTM is a recurrent neural network architecture that is capable of processing multivariate temporal data collected over time in a sequential manner, enabling it to predict current v
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01487</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#65292;&#36824;&#26159;&#19981;&#32842;&#22825;GPT&#65306;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01487
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;GPT&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20840;&#29699;&#24863;&#30693;&#12290;&#38543;&#30528;&#32842;&#22825;GPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#20182;&#20204;&#30340;&#35823;&#29992;&#30340;&#25285;&#24551;&#20063;&#22686;&#21152;&#20102;&#65292;&#20363;&#22914;&#20256;&#25773;&#34394;&#20551;&#28040;&#24687;&#65292;&#25220;&#34989;&#65292;&#25805;&#32437;&#20844;&#20247;&#33286;&#35770;&#65292;&#27450;&#39575;&#21644;&#27450;&#35784;&#12290;&#22240;&#27492;&#65292;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#65292;&#20174;&#22522;&#26412;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#21040;&#26356;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#20123;&#26816;&#27979;&#25216;&#26415;&#20381;&#36182;&#20110;&#32479;&#35745;&#29305;&#24449;&#25110;&#21477;&#27861;&#27169;&#24335;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#21253;&#21547;&#35821;&#20041;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#26368;&#26032;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#21644;&#29616;&#20195;&#21270;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20854;&#20182;&#26410;&#19987;&#38376;&#22768;&#31216;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#24037;&#32534;&#20889;&#21644;&#32842;&#22825;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#37096;&#32626;&#38376;&#27099;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.01483</link><description>&lt;p&gt;
&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20998;&#22359;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#37096;&#32626;&#38376;&#27099;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;GPT-3&#12289;ChatGPT&#21644;GPT-4&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#30340;&#24040;&#22823;&#35745;&#31639;&#36164;&#28304;&#21644;&#23384;&#20648;&#24320;&#38144;&#20173;&#28982;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;Softmax&#12289;&#23618;&#35268;&#33539;&#21270;&#20197;&#21450;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#39640;&#25928;&#27169;&#22411;&#20351;&#29992;BCT&#36827;&#34892;&#20102;&#21387;&#32553;&#24182;&#22312;&#22810;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;BCT&#21482;&#20250;&#24102;&#26469;&#23569;&#20110;0.90%&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have recently attracted increasing interest, research enthusiasm, and business demand. However, their massive computation resources and huge memory footprint are inevitable challenges. To tackle this issue, we propose BCT, a framework of blockwise compression for transformers without retraining, to lower deployment thresholds. BCT achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, Softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient model with BCT and evaluate it on several General Language Understanding Evaluation (GLUE) datasets. The results show that BCT can achieve a less than 0.90% accuracy drop in most tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#32593;&#32476;&#26550;&#26500;TSFF-Net&#65292;&#23558;&#26102;&#38388;-&#31354;&#38388;-&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#21333;&#27169;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#25110;&#26102;&#38388;-&#39057;&#29575;&#27169;&#24577;&#19979;&#30340;&#38480;&#21046;&#12290;TSFF-Net&#21253;&#25324;&#26102;&#38388;-&#39057;&#29575;&#34920;&#31034;&#12289;&#26102;&#38388;-&#39057;&#29575;&#29305;&#24449;&#25552;&#21462;&#12289;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#29305;&#24449;&#34701;&#21512;&#19982;&#20998;&#31867;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.01461</link><description>&lt;p&gt;
&#26102;&#38388;-&#31354;&#38388;-&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#30340;3&#36890;&#36947;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Time-space-frequency feature Fusion for 3-channel motor imagery classification. (arXiv:2304.01461v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#32593;&#32476;&#26550;&#26500;TSFF-Net&#65292;&#23558;&#26102;&#38388;-&#31354;&#38388;-&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#21333;&#27169;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#25110;&#26102;&#38388;-&#39057;&#29575;&#27169;&#24577;&#19979;&#30340;&#38480;&#21046;&#12290;TSFF-Net&#21253;&#25324;&#26102;&#38388;-&#39057;&#29575;&#34920;&#31034;&#12289;&#26102;&#38388;-&#39057;&#29575;&#29305;&#24449;&#25552;&#21462;&#12289;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#29305;&#24449;&#34701;&#21512;&#19982;&#20998;&#31867;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36890;&#36947;EEG&#35774;&#22791;&#23545;&#20110;&#20415;&#25658;&#21644;&#23089;&#20048;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;EEG&#20302;&#31354;&#38388;&#20998;&#36776;&#29575;&#23384;&#22312;&#25361;&#25112;&#65292;&#38590;&#20197;&#35299;&#30721;&#20302;&#36890;&#36947;&#36816;&#21160;&#24819;&#35937;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;TSFF-Net&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#26102;&#38388;-&#31354;&#38388;-&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#25110;&#26102;&#38388;-&#39057;&#29575;&#27169;&#24577;&#19979;&#30340;&#21333;&#27169;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#30340;&#38480;&#21046;&#19979;&#21457;&#25381;&#20316;&#29992;&#12290;TSFF-Net&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#26102;&#38388;-&#39057;&#29575;&#34920;&#31034;&#12289;&#26102;&#38388;-&#39057;&#29575;&#29305;&#24449;&#25552;&#21462;&#12289;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#29305;&#24449;&#34701;&#21512;&#19982;&#20998;&#31867;&#12290;&#26102;&#38388;-&#39057;&#29575;&#34920;&#31034;&#21644;&#29305;&#24449;&#25552;&#21462;&#23558;&#21407;&#22987;EEG&#20449;&#21495;&#36716;&#25442;&#20026;&#26102;&#38388;-&#39057;&#29575;&#35889;&#22270;&#24182;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#12290;&#26102;&#31354;&#32593;&#32476;&#23558;&#26102;&#38388;&#24207;&#21015;EEG&#35797;&#39564;&#20316;&#20026;&#36755;&#20837;&#22788;&#29702;&#65292;&#24182;&#25552;&#21462;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#12290;&#29305;&#24449;&#34701;&#21512;&#37319;&#29992;MMD&#25439;&#22833;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#32422;&#26463;&#26102;&#38388;-&#39057;&#29575;&#21644;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-channel EEG devices are crucial for portable and entertainment applications. However, the low spatial resolution of EEG presents challenges in decoding low-channel motor imagery. This study introduces TSFF-Net, a novel network architecture that integrates time-space-frequency features, effectively compensating for the limitations of single-mode feature extraction networks based on time-series or time-frequency modalities. TSFF-Net comprises four main components: time-frequency representation, time-frequency feature extraction, time-space feature extraction, and feature fusion and classification. Time-frequency representation and feature extraction transform raw EEG signals into time-frequency spectrograms and extract relevant features. The time-space network processes time-series EEG trials as input and extracts temporal-spatial features. Feature fusion employs MMD loss to constrain the distribution of time-frequency and time-space features in the Reproducing Kernel Hilbert Space, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#21033;&#29992;&#19981;&#24179;&#34913;&#31639;&#27861;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#25913;&#36827;&#21518;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#20998;&#31867;&#20934;&#30830;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#31867;&#65292;&#24615;&#33021;&#25552;&#21319;&#24456;&#22823;&#12290;</title><link>http://arxiv.org/abs/2304.01457</link><description>&lt;p&gt;
&#25506;&#32034;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Vision-Language Models for Imbalanced Learning. (arXiv:2304.01457v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#21033;&#29992;&#19981;&#24179;&#34913;&#31639;&#27861;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#25913;&#36827;&#21518;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#20998;&#31867;&#20934;&#30830;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#31867;&#65292;&#24615;&#33021;&#25552;&#21319;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31867;&#30340;&#20998;&#24067;&#20542;&#26012;&#65292;&#23548;&#33268;&#22312;&#39044;&#27979;&#23569;&#25968;&#31867;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#21521;VLM&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#22823;&#37327;&#31867;&#21035;&#23548;&#33268;&#30340;&#20869;&#23384;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#25429;&#25417;&#23614;&#37096;&#31867;&#21035;&#30340;&#24494;&#22937;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#12289;&#24494;&#35843;&#20197;&#21450;&#21152;&#20837;&#19981;&#24179;&#34913;&#31639;&#27861;&#65288;&#20363;&#22914;Focal Loss&#12289;Balanced SoftMax&#21644;Distribution Alignment&#65289;&#26469;&#25913;&#36827;VLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#35299;&#30721;&#22120;&#21644;&#19981;&#24179;&#34913;&#26041;&#27861;&#26102;&#65292;VLM&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25913;&#36827;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#24179;&#22343;&#25552;&#39640;&#20102;6.58%&#12289;69.82%&#21644;10.43%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OffPA2&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31163;&#32447;&#31574;&#30053;&#34892;&#21160;&#39044;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#39044;&#27979;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.01447</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#34892;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning. (arXiv:2304.01447v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OffPA2&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31163;&#32447;&#31574;&#30053;&#34892;&#21160;&#39044;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#39044;&#27979;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39044;&#27979;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#19968;&#31181;&#25512;&#29702;&#33539;&#24335;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#38454;&#26799;&#24230;&#65288;HOG&#65289;&#26041;&#27861;&#22312;&#38750;&#21487;&#24494;&#20998;&#21338;&#24328;&#25110;&#29366;&#24577;&#31354;&#38388;&#36739;&#22823;&#30340;&#21338;&#24328;&#20013;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;OffPA2&#26694;&#26550;&#65292;&#21033;&#29992;&#31163;&#32447;&#31574;&#30053;&#34892;&#21160;&#39044;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#39044;&#27979;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning anticipation in Multi-Agent Reinforcement Learning (MARL) is a reasoning paradigm where agents anticipate the learning steps of other agents to improve cooperation among themselves. As MARL uses gradient-based optimization, learning anticipation requires using Higher-Order Gradients (HOG), with so-called HOG methods. Existing HOG methods are based on policy parameter anticipation, i.e., agents anticipate the changes in policy parameters of other agents. Currently, however, these existing HOG methods have only been applied to differentiable games or games with small state spaces. In this work, we demonstrate that in the case of non-differentiable games with large state spaces, existing HOG methods do not perform well and are inefficient due to their inherent limitations related to policy parameter anticipation and multiple sampling stages. To overcome these problems, we propose Off-Policy Action Anticipation (OffPA2), a novel framework that approaches learning anticipation thro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#32593;&#32476;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#37319;&#29992;&#28145;&#24230;&#22810;&#27169;&#24577;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#25552;&#39640;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36229;&#36234;&#29616;&#26377;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#25991;&#29486;&#20316;&#21697;&#65292;&#20855;&#26377;&#26816;&#27979;&#32593;&#32476;&#25915;&#20987;&#30340;&#36739;&#39640;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;f-measure&#12290;</title><link>http://arxiv.org/abs/2304.01440</link><description>&lt;p&gt;
&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Deep Multi-Modal Cyber-Attack Detection in Industrial Control Systems. (arXiv:2304.01440v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#32593;&#32476;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#37319;&#29992;&#28145;&#24230;&#22810;&#27169;&#24577;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#25552;&#39640;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36229;&#36234;&#29616;&#26377;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#25991;&#29486;&#20316;&#21697;&#65292;&#20855;&#26377;&#26816;&#27979;&#32593;&#32476;&#25915;&#20987;&#30340;&#36739;&#39640;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;f-measure&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#36973;&#21463;&#30340;&#32593;&#32476;&#25915;&#20987;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#30001;&#20110;&#20854;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#24433;&#21709;&#65292;&#20154;&#20204;&#23545;&#20854;&#23433;&#20840;&#24615;&#36234;&#26469;&#36234;&#20851;&#27880;&#12290;&#32771;&#34385;&#21040;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#65292;&#26816;&#27979;&#20854;&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#38656;&#35201;&#36816;&#29992;&#33021;&#22815;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30340;&#32593;&#32476;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20351;&#29992;&#28145;&#24230;&#22810;&#27169;&#24577;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#25991;&#29486;&#20316;&#21697;&#65292;&#20854;&#31934;&#24230;&#36798;&#21040;0.99&#12289;&#21484;&#22238;&#29575;&#20026;0.98&#12289;f-measure&#20026;0.98&#65292;&#34920;&#26126;&#20102;&#32467;&#21512;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#26816;&#27979;&#32593;&#32476;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing number of cyber-attacks against Industrial Control Systems (ICS) in recent years has elevated security concerns due to the potential catastrophic impact. Considering the complex nature of ICS, detecting a cyber-attack in them is extremely challenging and requires advanced methods that can harness multiple data modalities. This research utilizes network and sensor modality data from ICS processed with a deep multi-modal cyber-attack detection model for ICS. Results using the Secure Water Treatment (SWaT) system show that the proposed model can outperform existing single modality models and recent works in the literature by achieving 0.99 precision, 0.98 recall, and 0.98 f-measure, which shows the effectiveness of using both modalities in a combined model for detecting cyber-attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#28748;&#28297;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;DRLIC&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#26368;&#20339;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#21040;&#24403;&#21069;&#30340;&#22303;&#22756;&#27700;&#20998;&#27979;&#37327;&#21644;&#26410;&#26469;&#30340;&#22303;&#22756;&#27700;&#20998;&#25439;&#22833;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#28748;&#28297;&#22870;&#21169;&#20989;&#25968;&#65292;DRLIC&#21487;&#20197;&#20174;&#20197;&#24448;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DRLIC&#21644;&#20854;&#31616;&#21270;&#29256;&#26412;&#22312;&#27700;&#20998;&#21033;&#29992;&#25928;&#29575;&#21644;&#20316;&#29289;&#20135;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#24403;&#21069;&#30340;&#28748;&#28297;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01435</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30000;&#38388;&#28748;&#28297;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optimizing Irrigation Efficiency using Deep Reinforcement Learning in the Field. (arXiv:2304.01435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#28748;&#28297;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;DRLIC&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#26368;&#20339;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#21040;&#24403;&#21069;&#30340;&#22303;&#22756;&#27700;&#20998;&#27979;&#37327;&#21644;&#26410;&#26469;&#30340;&#22303;&#22756;&#27700;&#20998;&#25439;&#22833;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#28748;&#28297;&#22870;&#21169;&#20989;&#25968;&#65292;DRLIC&#21487;&#20197;&#20174;&#20197;&#24448;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DRLIC&#21644;&#20854;&#31616;&#21270;&#29256;&#26412;&#22312;&#27700;&#20998;&#21033;&#29992;&#25928;&#29575;&#21644;&#20316;&#29289;&#20135;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#24403;&#21069;&#30340;&#28748;&#28297;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#28748;&#28297;&#26159;&#28129;&#27700;&#28040;&#32791;&#30340;&#37325;&#35201;&#36129;&#29486;&#32773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30000;&#38388;&#20351;&#29992;&#30340;&#28748;&#28297;&#31995;&#32479;&#25928;&#29575;&#19981;&#39640;&#12290;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#22303;&#22756;&#28287;&#24230;&#20256;&#24863;&#22120;&#21644;&#31181;&#26893;&#32773;&#30340;&#32463;&#39564;&#65292;&#20294;&#24182;&#19981;&#32771;&#34385;&#26410;&#26469;&#30340;&#22303;&#22756;&#27700;&#20998;&#25439;&#22833;&#12290;&#39044;&#27979;&#22303;&#22756;&#27700;&#20998;&#25439;&#22833;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#21463;&#21040;&#35768;&#22810;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22303;&#22756;&#36136;&#22320;&#12289;&#22825;&#27668;&#26465;&#20214;&#21644;&#26893;&#29289;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRLIC&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;&#28748;&#28297;&#25928;&#29575;&#12290;DRLIC&#26159;&#19968;&#31181;&#22797;&#26434;&#30340;&#28748;&#28297;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;DRL&#25511;&#21046;&#20195;&#29702;&#65292;&#21487;&#20197;&#23398;&#20064;&#32771;&#34385;&#24403;&#21069;&#22303;&#22756;&#27700;&#20998;&#27979;&#37327;&#21644;&#26410;&#26469;&#22303;&#22756;&#27700;&#20998;&#25439;&#22833;&#30340;&#26368;&#20339;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28748;&#28297;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#25511;&#21046;&#20195;&#29702;&#33021;&#22815;&#20174;&#20197;&#24448;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#65292;DRLIC&#30340;&#25805;&#20316;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#23454;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DRLIC&#30340;&#31616;&#21270;&#29256;&#26412;&#65292;&#23427;&#20351;&#29992;&#26356;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26356;&#23569;&#30340;&#36755;&#20837;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DRLIC&#21644;&#31616;&#21270;&#29256;&#26412;&#22312;&#27700;&#20998;&#21033;&#29992;&#25928;&#29575;&#21644;&#20316;&#29289;&#20135;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#24403;&#21069;&#30340;&#28748;&#28297;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agricultural irrigation is a significant contributor to freshwater consumption. However, the current irrigation systems used in the field are not efficient. They rely mainly on soil moisture sensors and the experience of growers, but do not account for future soil moisture loss. Predicting soil moisture loss is challenging because it is influenced by numerous factors, including soil texture, weather conditions, and plant characteristics. This paper proposes a solution to improve irrigation efficiency, which is called DRLIC. DRLIC is a sophisticated irrigation system that uses deep reinforcement learning (DRL) to optimize its performance. The system employs a neural network, known as the DRL control agent, which learns an optimal control policy that considers both the current soil moisture measurement and the future soil moisture loss. We introduce an irrigation reward function that enables our control agent to learn from previous experiences. However, there may be instances where the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#35268;&#33539;&#21270;&#34920;&#31034;&#30340;von Neumann&#29109;( VNE ) &#26469;&#25913;&#21892;&#28145;&#24230;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#20540;&#20998;&#24067;&#26469;&#20248;&#21270;&#34920;&#31034;&#21697;&#36136;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#20854;&#39046;&#22495;&#36890;&#29992;&#24615;&#12289;&#20803;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.01434</link><description>&lt;p&gt;
VNE: &#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#20540;&#20998;&#24067;&#26469;&#25552;&#39640;&#28145;&#24230;&#34920;&#31034;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution. (arXiv:2304.01434v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#35268;&#33539;&#21270;&#34920;&#31034;&#30340;von Neumann&#29109;( VNE ) &#26469;&#25913;&#21892;&#28145;&#24230;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#20540;&#20998;&#24067;&#26469;&#20248;&#21270;&#34920;&#31034;&#21697;&#36136;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#20854;&#39046;&#22495;&#36890;&#29992;&#24615;&#12289;&#20803;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#28145;&#24230;&#23398;&#20064;&#34987;&#24341;&#20837;&#20197;&#26469;&#65292;&#24456;&#22810;&#34920;&#31034;&#29305;&#24615; (&#22914;&#21435;&#30456;&#20851;&#12289;&#30333;&#21270;&#12289;&#35299;&#32544;&#12289;&#31209;&#12289;&#31561;&#24230;&#24615;&#21644;&#20114;&#20449;&#24687;) &#24050;&#32463;&#34987;&#30740;&#31350;&#20986;&#26469;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#21697;&#36136;&#12290;&#28982;&#32780;&#65292;&#25805;&#32437;&#36825;&#20123;&#29305;&#24615;&#22312;&#23454;&#29616;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#36866;&#29992;&#24615;&#26041;&#38754;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#34920;&#31034;&#30340;von Neumann&#29109;(VNE)&#36827;&#34892;&#35268;&#33539;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;VNE&#30340;&#25968;&#23398;&#34920;&#36848;&#22312;&#26377;&#25928;&#25805;&#32437;&#34920;&#31034;&#33258;&#30456;&#20851;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#26041;&#38754;&#26159;&#20248;&#36234;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#39046;&#22495;&#36890;&#29992;&#24615;&#65292;&#20803;&#23398;&#20064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#31561;&#26041;&#38754;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25552;&#39640;&#29616;&#26377;&#20808;&#36827;&#31639;&#27861;&#25110;&#27969;&#34892;&#22522;&#20934;&#31639;&#27861;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#34920;&#31034;&#30340;&#31209;&#12289;&#35299;&#32544;&#21644;&#31561;&#24230;&#24615;&#30340;&#32852;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of deep learning, a wide scope of representation properties, such as decorrelation, whitening, disentanglement, rank, isotropy, and mutual information, have been studied to improve the quality of representation. However, manipulating such properties can be challenging in terms of implementational effectiveness and general applicability. To address these limitations, we propose to regularize von Neumann entropy~(VNE) of representation. First, we demonstrate that the mathematical formulation of VNE is superior in effectively manipulating the eigenvalues of the representation autocorrelation matrix. Then, we demonstrate that it is widely applicable in improving state-of-the-art algorithms or popular benchmark algorithms by investigating domain-generalization, meta-learning, self-supervised learning, and generative models. In addition, we formally establish theoretical connections with rank, disentanglement, and isotropy of representation. Finally, we provide discuss
&lt;/p&gt;</description></item><item><title>TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.01433</link><description>&lt;p&gt;
TPU v4&#65306;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. (arXiv:2304.01433v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01433
&lt;/p&gt;
&lt;p&gt;
TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21019;&#26032;&#65292;&#29983;&#20135;&#24037;&#20316;&#36127;&#36733;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#21644;&#36805;&#36895;&#30340;&#21464;&#21270;&#12290;TPU v4&#26159;&#35895;&#27468;&#30340;&#31532;&#20116;&#20195;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#26550;&#26500;&#65288;DSA&#65289;&#65292;&#26159;&#20854;&#31532;&#19977;&#20010;&#29992;&#20110;&#22788;&#29702;&#27492;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#32423;&#35745;&#31639;&#26426;&#12290;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#65288;OCS&#65289;&#21160;&#24577;&#37325;&#26032;&#37197;&#32622;&#20854;&#20114;&#36830;&#25299;&#25169;&#65292;&#20197;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#12290;&#37096;&#32626;&#33258;2020&#24180;&#20197;&#26469;&#65292;TPU v4&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#34920;&#29616;&#20248;&#20110;TPU v3&#65292;&#21516;&#26102;&#24615;&#33021;/Watt&#25552;&#39640;&#20102;2.7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are &lt;5% of system cost and &lt;3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For sim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#27861;&#65306;&#19968;&#20010;&#22810;&#27493;&#30340;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#20197;&#21450;&#19968;&#31181;&#20855;&#26377;&#36739;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#21152;&#36895;&#21040;$O(1/k^{3/2})$&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01432</link><description>&lt;p&gt;
&#38477;&#20302;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Reducing Discretization Error in the Frank-Wolfe Method. (arXiv:2304.01432v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#27861;&#65306;&#19968;&#20010;&#22810;&#27493;&#30340;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#20197;&#21450;&#19968;&#31181;&#20855;&#26377;&#36739;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#21152;&#36895;&#21040;$O(1/k^{3/2})$&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Frank-Wolfe&#31639;&#27861;&#26159;&#32467;&#26500;&#21463;&#38480;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#24555;&#36895;&#36845;&#20195;&#22797;&#26434;&#24230;&#32780;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#65292;&#30001;&#20110;&#27493;&#38271;&#26041;&#21521;&#30340;&#19981;&#35268;&#21017;&#38663;&#33633;&#32780;&#38590;&#20197;&#21152;&#36895;&#65292;&#21363;&#20351;&#22312;&#25509;&#36817;&#35299;&#30340;&#28176;&#36817;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#31163;&#25955;&#21270;&#30340;&#20135;&#29289;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;Frank-Wolfe&#30340;&#27969;&#65288;&#21363;&#28176;&#36817;&#23567;&#27493;&#38271;&#24773;&#20917;&#19979;&#30340;&#36712;&#36857;&#65289;&#19981;&#20250;&#20986;&#29616;&#19981;&#35268;&#21017;&#38663;&#33633;&#65292;&#22240;&#27492;&#20943;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#23558;&#19982;&#20135;&#29983;&#26356;&#31283;&#23450;&#30340;&#26041;&#27861;&#21644;&#26356;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#65306;&#19968;&#20010;&#22810;&#27493;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#21644;&#19968;&#20010;&#20855;&#26377;&#38477;&#20302;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#22312;&#19968;&#33324;&#20984;&#38598;&#19978;&#30340;&#23616;&#37096;&#25910;&#25947;&#36895;&#29575;&#20174;$O(1/k)$&#21152;&#36895;&#21040;$O(1/k^{3/2})$ &#12290;
&lt;/p&gt;
&lt;p&gt;
The Frank-Wolfe algorithm is a popular method in structurally constrained machine learning applications, due to its fast per-iteration complexity. However, one major limitation of the method is a slow rate of convergence that is difficult to accelerate due to erratic, zig-zagging step directions, even asymptotically close to the solution. We view this as an artifact of discretization; that is to say, the Frank-Wolfe \emph{flow}, which is its trajectory at asymptotically small step sizes, does not zig-zag, and reducing discretization error will go hand-in-hand in producing a more stabilized method, with better convergence properties. We propose two improvements: a multistep Frank-Wolfe method that directly applies optimized higher-order discretization schemes; and an LMO-averaging scheme with reduced discretization error, and whose local convergence rate over general convex sets accelerates from a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.01430</link><description>&lt;p&gt;
&#20998;&#31163;&#30340;&#20851;&#27880;&#21147;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#20998;&#31163;&#27133;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots. (arXiv:2304.01430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#22522;&#30784;&#30495;&#20540;&#25110;&#30417;&#30563;&#12290;&#23427;&#30001;&#22522;&#20110;&#27133;&#20851;&#27880;&#30340;&#23545;&#25239;&#26465;&#20214;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#32452;&#25104;&#65292;&#20462;&#25913;&#20026;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#35299;&#30721;&#20809;&#27969;&#65292;&#32780;&#19981;&#26159;&#23581;&#35797;&#37325;&#26500;&#22270;&#20687;&#26412;&#36523;&#12290;&#22312;&#32467;&#26524;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#20013;&#65292;&#19968;&#31181;&#27169;&#24335;&#65288;&#27969;&#65289;&#23558;&#39304;&#36865;&#32473;&#32534;&#30721;&#22120;&#20197;&#20135;&#29983;&#21333;&#29420;&#30340;&#28508;&#22312;&#20195;&#30721;&#65288;&#27133;&#65289;&#65292;&#32780;&#21478;&#19968;&#31181;&#27169;&#24335;&#65288;&#22270;&#20687;&#65289;&#23558;&#20915;&#23450;&#35299;&#30721;&#22120;&#20174;&#27133;&#29983;&#25104;&#31532;&#19968;&#20010;&#27169;&#24335;&#65288;&#27969;&#65289;&#12290;&#30001;&#20110;&#24815;&#24120;&#30340;&#33258;&#32534;&#30721;&#22522;&#20110;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#24182;&#19981;&#33021;&#38450;&#27490;&#25972;&#20010;&#27969;&#34987;&#32534;&#30721;&#21040;&#19968;&#20010;&#27133;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#25439;&#22833;&#20462;&#25913;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20998;&#31163;&#30340;&#23545;&#25239;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to segment the visual field into independently moving regions, trained with no ground truth or supervision. It consists of an adversarial conditional encoder-decoder architecture based on Slot Attention, modified to use the image as context to decode optical flow without attempting to reconstruct the image itself. In the resulting multi-modal representation, one modality (flow) feeds the encoder to produce separate latent codes (slots), whereas the other modality (image) conditions the decoder to generate the first (flow) from the slots. This design frees the representation from having to encode complex nuisance variability in the image due to, for instance, illumination and reflectance properties of the scene. Since customary autoencoding based on minimizing the reconstruction error does not preclude the entire flow from being encoded into a single slot, we modify the loss to an adversarial criterion based on Contextual Information Separation. The resulting min-m
&lt;/p&gt;</description></item><item><title>&#24102;&#26377;&#32467;&#26500;&#24615;&#32570;&#22833;&#30340;&#25968;&#25454;&#23398;&#20064;&#26159;&#19968;&#20010;&#23578;&#26410;&#34987;&#31995;&#32479;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#23427;&#23545;&#35268;&#27169;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26500;&#25104;&#20102;&#37325;&#35201;&#38459;&#30861;&#65292;&#24182;&#19988;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.01429</link><description>&lt;p&gt;
&#24102;&#26377;&#32467;&#26500;&#24615;&#32570;&#22833;&#30340;&#25968;&#25454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from data with structured missingness. (arXiv:2304.01429v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01429
&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#32467;&#26500;&#24615;&#32570;&#22833;&#30340;&#25968;&#25454;&#23398;&#20064;&#26159;&#19968;&#20010;&#23578;&#26410;&#34987;&#31995;&#32479;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#23427;&#23545;&#35268;&#27169;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26500;&#25104;&#20102;&#37325;&#35201;&#38459;&#30861;&#65292;&#24182;&#19988;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#24403;&#25968;&#25454;&#26159;&#8220;&#38543;&#26426;&#32570;&#22833;&#8221;&#26102;&#65292;&#23384;&#22312;&#19968;&#31995;&#21015;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21464;&#24471;&#26356;&#21152;&#38596;&#24515;&#21187;&#21187;&#65292;&#24182;&#19988;&#35797;&#22270;&#20174;&#36234;&#26469;&#36234;&#22823;&#37327;&#30340;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#36234;&#26469;&#36234;&#26222;&#36941;&#30340;&#38382;&#39064;&#26159;&#20986;&#29616;&#20102;&#32570;&#22833;&#20540;&#30340;&#20851;&#32852;&#25110;&#32467;&#26500;&#65292;&#26080;&#35770;&#26159;&#26126;&#30830;&#36824;&#26159;&#38544;&#21547;&#12290;&#36825;&#31181;&#8220;&#32467;&#26500;&#24615;&#32570;&#22833;&#8221;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23578;&#26410;&#31995;&#32479;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#24182;&#23545;&#35268;&#27169;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26500;&#25104;&#20102;&#37325;&#35201;&#38459;&#30861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#30340;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#20851;&#20110;&#22914;&#20309;&#22312;&#24102;&#26377;&#32467;&#26500;&#24615;&#32570;&#22833;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data are an unavoidable complication in many machine learning tasks. When data are `missing at random' there exist a range of tools and techniques to deal with the issue. However, as machine learning studies become more ambitious, and seek to learn from ever-larger volumes of heterogeneous data, an increasingly encountered problem arises in which missing values exhibit an association or structure, either explicitly or implicitly. Such `structured missingness' raises a range of challenges that have not yet been systematically addressed, and presents a fundamental hindrance to machine learning at scale. Here, we outline the current literature and propose a set of grand challenges in learning from data with structured missingness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;SoRTS&#65292;&#29992;&#20110;&#22312;&#20849;&#20139;&#31354;&#22495;&#20013;&#23454;&#29616;&#31038;&#20132;&#26426;&#22120;&#20154;&#38271;&#26399;&#23548;&#33322;&#65292;&#24182;&#36890;&#36807;FAA&#35748;&#35777;&#39134;&#34892;&#21592;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#34920;&#29616;&#19982;&#19968;&#21517;&#29087;&#32451;&#30340;&#20154;&#31867;&#39134;&#34892;&#21592;&#30456;&#24403;&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01428</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#29992;&#20110;&#22312;&#20849;&#20139;&#31354;&#22495;&#20013;&#23454;&#29616;&#31038;&#20132;&#26426;&#22120;&#20154;&#38271;&#26399;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Learned Tree Search for Long-Horizon Social Robot Navigation in Shared Airspace. (arXiv:2304.01428v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;SoRTS&#65292;&#29992;&#20110;&#22312;&#20849;&#20139;&#31354;&#22495;&#20013;&#23454;&#29616;&#31038;&#20132;&#26426;&#22120;&#20154;&#38271;&#26399;&#23548;&#33322;&#65292;&#24182;&#36890;&#36807;FAA&#35748;&#35777;&#39134;&#34892;&#21592;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#34920;&#29616;&#19982;&#19968;&#21517;&#29087;&#32451;&#30340;&#20154;&#31867;&#39134;&#34892;&#21592;&#30456;&#24403;&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#26080;&#20154;&#26426;&#22312;&#25317;&#25380;&#21160;&#24577;&#30340;&#20849;&#20139;&#31354;&#38388;&#20013;&#36827;&#34892;&#33258;&#20027;&#25805;&#20316;&#30340;&#38656;&#27714;&#36805;&#36895;&#22686;&#38271;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#21487;&#20449;&#30340;&#20195;&#29702;&#31243;&#24207;&#20197;&#23454;&#29616;&#26080;&#32541;&#23433;&#20840;&#23548;&#33322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Social Robot Tree Search (SoRTS)&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#31038;&#20132;&#39046;&#22495;&#20013;&#31227;&#21160;&#26426;&#22120;&#20154;&#23433;&#20840;&#23548;&#33322;&#30340;&#31639;&#27861;&#12290;SoRTS&#26088;&#22312;&#36890;&#36807;Monte Carlo Tree Search&#35268;&#21010;&#22120;&#22686;&#24378;&#29616;&#26377;&#30340;&#31038;&#20132;&#24863;&#30693;&#36712;&#36857;&#39044;&#27979;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#19979;&#28216;&#23548;&#33322;&#25928;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#33324;&#33322;&#31354;&#39046;&#22495;&#30340;&#31038;&#20132;&#23548;&#33322;&#24212;&#29992;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;X-Plane ROS&#65288;&#26426;&#36733;&#25805;&#20316;&#31995;&#32479;&#65289;&#39134;&#34892;&#27169;&#25311;&#22120;&#65292;&#20197;&#23454;&#29616;&#22312;&#23436;&#20840;&#33258;&#20027;&#25805;&#20316;&#19978;&#30340;&#26356;&#22810;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;26&#21517;FAA&#35748;&#35777;&#39134;&#34892;&#21592;&#30340;&#34892;&#19994;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SoRTS&#30340;&#34920;&#29616;&#19982;&#19968;&#21517;&#29087;&#32451;&#30340;&#20154;&#31867;&#39134;&#34892;&#21592;&#30456;&#24403;&#65292;&#26126;&#26174;&#20248;&#20110;&#25105;&#20204;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34917;&#20805;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fast-growing demand for fully autonomous aerial operations in shared spaces necessitates developing trustworthy agents that can safely and seamlessly navigate in crowded, dynamic spaces. In this work, we propose Social Robot Tree Search (SoRTS), an algorithm for the safe navigation of mobile robots in social domains. SoRTS aims to augment existing socially-aware trajectory prediction policies with a Monte Carlo Tree Search planner for improved downstream navigation of mobile robots. To evaluate the performance of our method, we choose the use case of social navigation for general aviation. To aid this evaluation, within this work, we also introduce X-PlaneROS, a high-fidelity aerial simulator, to enable more research in full-scale aerial autonomy. By conducting a user study based on the assessments of 26 FAA certified pilots, we show that SoRTS performs comparably to a competent human pilot, significantly outperforming our baseline algorithm. We further complement these results wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#25512;&#26029;&#36807;&#31243;&#65292;&#23558;&#31526;&#21512;&#39044;&#27979;&#19982;&#26465;&#20214;&#26080;&#38480;&#20998;&#20301;&#25968;&#22238;&#24402;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#36866;&#24212;&#24322;&#26041;&#24046;&#24615;&#24182;&#25552;&#20379;&#36879;&#26126;&#30340;&#35206;&#30422;&#20445;&#35777;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01426</link><description>&lt;p&gt;
&#26465;&#20214;&#26080;&#38480;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#19968;&#31181;&#21512;&#35268;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformalized Unconditional Quantile Regression. (arXiv:2304.01426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#25512;&#26029;&#36807;&#31243;&#65292;&#23558;&#31526;&#21512;&#39044;&#27979;&#19982;&#26465;&#20214;&#26080;&#38480;&#20998;&#20301;&#25968;&#22238;&#24402;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#36866;&#24212;&#24322;&#26041;&#24046;&#24615;&#24182;&#25552;&#20379;&#36879;&#26126;&#30340;&#35206;&#30422;&#20445;&#35777;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23558;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#19982;&#26465;&#20214;&#26080;&#38480;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;QR&#65289;&#30456;&#32467;&#21512;&#30340;&#39044;&#27979;&#25512;&#26029;&#36807;&#31243;&#65292;&#21518;&#32773;&#26159;&#32463;&#27982;&#35745;&#37327;&#23398;&#20013;&#24120;&#29992;&#30340;&#24037;&#20855;&#65292;&#28041;&#21450;&#22312;&#36755;&#20837;&#21327;&#21464;&#37327;&#19978;&#22238;&#24402;&#20998;&#20301;&#20989;&#25968;&#30340;&#37325;&#26032;&#23621;&#20013;&#24433;&#21709;&#20989;&#25968;&#65288;RIF&#65289;&#12290;&#19982;&#26356;&#20026;&#24191;&#20026;&#20154;&#30693;&#30340;&#26465;&#20214;QR&#19981;&#21516;&#65292;&#26465;&#20214;&#26080;&#38480;QR&#26126;&#30830;&#25429;&#25417;&#21327;&#21464;&#37327;&#20998;&#24067;&#21464;&#21270;&#23545;&#32467;&#26524;&#36793;&#38469;&#20998;&#24067;&#30340;&#20998;&#20301;&#25968;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#25311;&#21512;RIFs&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#28982;&#21518;&#23545;&#20110;&#20219;&#20309;&#27979;&#35797;&#21327;&#21464;&#37327;&#65292;&#22312;&#22260;&#32469;&#26032;&#23454;&#20363;&#30340;&#8220;&#20551;&#35774;&#8221;&#21327;&#21464;&#37327;&#20998;&#24067;&#19978;&#24212;&#29992;CP&#36807;&#31243;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#24322;&#26041;&#24046;&#24615;&#65292;&#25552;&#20379;&#19982;&#25163;&#22836;&#27979;&#35797;&#23454;&#20363;&#30456;&#20851;&#30340;&#36879;&#26126;&#35206;&#30422;&#20445;&#35777;&#65292;&#24182;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a predictive inference procedure that combines conformal prediction (CP) with unconditional quantile regression (QR) -- a commonly used tool in econometrics that involves regressing the recentered influence function (RIF) of the quantile functional over input covariates. Unlike the more widely-known conditional QR, unconditional QR explicitly captures the impact of changes in covariate distribution on the quantiles of the marginal distribution of outcomes. Leveraging this property, our procedure issues adaptive predictive intervals with localized frequentist coverage guarantees. It operates by fitting a machine learning model for the RIFs using training data, and then applying the CP procedure for any test covariate with respect to a ``hypothetical'' covariate distribution localized around the new instance. Experiments show that our procedure is adaptive to heteroscedasticity, provides transparent coverage guarantees that are relevant to the test instance at hand, and perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26550;&#26500;&#19981;&#21487;&#30693;&#30340;&#21453;&#39304;&#23545;&#40784;&#24037;&#20316;&#29702;&#35770;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20449;&#24687;&#23884;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#24449;&#65292;&#32780;&#19981;&#26159;&#20687;BP&#19968;&#26679;&#29992;&#21516;&#26679;&#30340;&#21442;&#25968;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;&#24182;&#22522;&#20110;&#36825;&#19968;&#29702;&#35770;&#35774;&#35745;&#20102;&#19977;&#31181;&#23454;&#29616;FA&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01406</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30446;&#26631;&#20449;&#24687;&#65306;&#21453;&#39304;&#23545;&#40784;&#30340;&#26367;&#20195;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Learning with augmented target information: An alternative theory of Feedback Alignment. (arXiv:2304.01406v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26550;&#26500;&#19981;&#21487;&#30693;&#30340;&#21453;&#39304;&#23545;&#40784;&#24037;&#20316;&#29702;&#35770;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20449;&#24687;&#23884;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#24449;&#65292;&#32780;&#19981;&#26159;&#20687;BP&#19968;&#26679;&#29992;&#21516;&#26679;&#30340;&#21442;&#25968;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;&#24182;&#22522;&#20110;&#36825;&#19968;&#29702;&#35770;&#35774;&#35745;&#20102;&#19977;&#31181;&#23454;&#29616;FA&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21344;&#25454;&#30528;&#20960;&#20046;&#25152;&#26377;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#23427;&#23384;&#22312;&#30528;&#35768;&#22810;&#29983;&#29289;&#21512;&#29702;&#24615;&#38382;&#39064;&#65292;&#22914;&#23545;&#31216;&#26435;&#37325;&#35201;&#27714;&#21644;&#21516;&#27493;&#26356;&#26032;&#12290;&#21453;&#39304;&#23545;&#40784;&#65288;FA&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;BP&#30340;&#26367;&#20195;&#26041;&#26696;&#20197;&#35299;&#20915;&#36825;&#20123;&#22256;&#22659;&#65292;&#24182;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#19978;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#23427;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#20294;&#36824;&#32570;&#20047;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#37322;&#65292;&#35299;&#37322;FA&#22914;&#20309;&#22312;&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26550;&#26500;&#19981;&#21487;&#30693;&#30340;FA&#24037;&#20316;&#29702;&#35770;&#65292;&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#23558;&#30446;&#26631;&#20449;&#24687;&#23884;&#20837;&#21040;&#34987;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#24449;&#65292;&#32780;&#19981;&#26159;&#29992;&#30456;&#21516;&#30340;&#21442;&#25968;&#26469;&#36817;&#20284;BP&#35745;&#31639;&#20986;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29702;&#24819;&#24773;&#20917;&#19979;&#30340;FA&#21160;&#24577;&#36827;&#34892;&#20998;&#26512;&#26469;&#23637;&#31034;&#36825;&#19968;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#39564;&#35777;&#12290;&#22522;&#20110;&#36825;&#19968;&#29702;&#35770;&#30340;&#24847;&#20041;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#23454;&#29616;FA&#30340;&#26041;&#27861;&#65292;&#24182;&#27979;&#35797;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While error backpropagation (BP) has dominated the training of nearly all modern neural networks for a long time, it suffers from several biological plausibility issues such as the symmetric weight requirement and synchronous updates. Feedback Alignment (FA) was proposed as an alternative to BP to address those dilemmas and has been demonstrated to be effective on various tasks and network architectures. Despite its simplicity and effectiveness, a satisfying explanation of how FA works across different architectures is still lacking. Here we propose a novel, architecture-agnostic theory of how FA works through the lens of information theory: Instead of approximating gradients calculated by BP with the same parameter, FA learns effective representations by embedding target information into neural networks to be trained. We show this through the analysis of FA dynamics in idealized settings and then via a series of experiments. Based on the implications of this theory, we designed three 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26144;&#23556;&#26041;&#27861;&#26469;&#26356;&#39640;&#25928;&#22320;&#35782;&#21035;&#26448;&#26009;&#34920;&#38754;&#30340;&#32570;&#38519;&#21306;&#22495;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#36880;&#28857;&#27979;&#37327;&#23548;&#33268;&#32791;&#26102;&#22823;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20027;&#21160;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20197;&#38477;&#20302;&#27979;&#37327;&#27425;&#25968;&#21644;&#21033;&#29992;&#20808;&#21069;&#29983;&#20135;&#26448;&#26009;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.01404</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#36801;&#31227;&#23398;&#20064;&#30340;&#27700;&#24179;&#38598;&#20272;&#35745;&#20013;&#29289;&#20307;&#34920;&#38754;&#32570;&#38519;&#21306;&#22495;&#30340;&#33258;&#36866;&#24212;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Adaptive Defective Area Identification in Material Surface Using Active Transfer Learning-based Level Set Estimation. (arXiv:2304.01404v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26144;&#23556;&#26041;&#27861;&#26469;&#26356;&#39640;&#25928;&#22320;&#35782;&#21035;&#26448;&#26009;&#34920;&#38754;&#30340;&#32570;&#38519;&#21306;&#22495;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#36880;&#28857;&#27979;&#37327;&#23548;&#33268;&#32791;&#26102;&#22823;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20027;&#21160;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20197;&#38477;&#20302;&#27979;&#37327;&#27425;&#25968;&#21644;&#21033;&#29992;&#20808;&#21069;&#29983;&#20135;&#26448;&#26009;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#34920;&#24449;&#20013;&#65292;&#35782;&#21035;&#29289;&#20307;&#34920;&#38754;&#19978;&#30340;&#32570;&#38519;&#21306;&#22495;&#26159;&#22522;&#30784;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#26159;&#22312;&#34920;&#38754;&#39044;&#35774;&#30340;&#32593;&#26684;&#28857;&#19978;&#36880;&#28857;&#27979;&#37327;&#30456;&#20851;&#29289;&#29702;&#37327;&#65292;&#24182;&#30830;&#23450;&#26410;&#36798;&#21040;&#26399;&#26395;&#27700;&#24179;&#30340;&#21306;&#22495;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#35782;&#21035;&#32570;&#38519;&#21306;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#26144;&#23556;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#20351;&#29992;&#27979;&#37327;&#36164;&#28304;&#26469;&#26816;&#27979;&#32570;&#38519;&#21306;&#22495;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#35270;&#20026;&#27700;&#24179;&#38598;&#20272;&#35745;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#39046;&#22495;&#12290;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#27700;&#24179;&#38598;&#20272;&#35745;&#30340;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#27979;&#37327;&#27425;&#25968;&#26469;&#30830;&#23450;&#23450;&#20041;&#22312;&#34920;&#38754;&#19978;&#30340;&#29289;&#29702;&#20989;&#25968;&#30340;&#27700;&#24179;&#38598;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#37325;&#22797;&#29983;&#20135;&#20855;&#26377;&#30456;&#20284;&#35268;&#26684;&#30340;&#26448;&#26009;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20415;&#26377;&#25928;&#21033;&#29992;&#20808;&#21069;&#29983;&#20135;&#30340;&#26448;&#26009;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In material characterization, identifying defective areas on a material surface is fundamental. The conventional approach involves measuring the relevant physical properties point-by-point at the predetermined mesh grid points on the surface and determining the area at which the property does not reach the desired level. To identify defective areas more efficiently, we propose adaptive mapping methods in which measurement resources are used preferentially to detect the boundaries of defective areas. We interpret this problem as an active-learning (AL) of the level set estimation (LSE) problem. The goal of AL-based LSE is to determine the level set of the physical property function defined on the surface with as small number of measurements as possible. Furthermore, to handle the situations in which materials with similar specifications are repeatedly produced, we introduce a transfer learning approach so that the information of previously produced materials can be effectively utilized.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31995;&#32479;&#35782;&#21035;&#23398;&#20064;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23558;&#22810;&#20010;&#31995;&#32479;&#21010;&#20998;&#20026;&#32676;&#38598;&#65292;&#21516;&#19968;&#31751;&#20013;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#20854;&#20182;&#31995;&#32479;&#30340;&#35266;&#23519;&#20013;&#33719;&#30410;&#12290;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#27491;&#30830;&#20272;&#35745;&#32676;&#38598;&#26631;&#35782;&#24182;&#20855;&#26377;&#39640;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#31995;&#32479;&#35782;&#21035;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.01395</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#31995;&#32479;&#35782;&#21035;&#23398;&#20064;&#20010;&#24615;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Models with Clustered System Identification. (arXiv:2304.01395v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31995;&#32479;&#35782;&#21035;&#23398;&#20064;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23558;&#22810;&#20010;&#31995;&#32479;&#21010;&#20998;&#20026;&#32676;&#38598;&#65292;&#21516;&#19968;&#31751;&#20013;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#20854;&#20182;&#31995;&#32479;&#30340;&#35266;&#23519;&#20013;&#33719;&#30410;&#12290;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#27491;&#30830;&#20272;&#35745;&#32676;&#38598;&#26631;&#35782;&#24182;&#20855;&#26377;&#39640;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#31995;&#32479;&#35782;&#21035;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20174;&#19981;&#21516;&#31995;&#32479;&#21160;&#21147;&#23398;&#35266;&#23519;&#22810;&#20010;&#36712;&#36857;&#20013;&#23398;&#20064;&#32447;&#24615;&#31995;&#32479;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#21327;&#20316;&#22330;&#26223;&#65292;&#20854;&#20013;&#23547;&#27714;&#20272;&#35745;&#20854;&#21160;&#21147;&#23398;&#30340;&#22810;&#20010;&#31995;&#32479;&#34987;&#21010;&#20998;&#20026;&#26681;&#25454;&#20854;&#31995;&#32479;&#30456;&#20284;&#24615;&#30340;&#32676;&#38598;&#12290;&#22240;&#27492;&#65292;&#21516;&#19968;&#31751;&#20013;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#20854;&#20182;&#31995;&#32479;&#30340;&#35266;&#23519;&#20013;&#33719;&#30410;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#27599;&#20010;&#31995;&#32479;&#20132;&#26367;&#20272;&#35745;&#20854;&#32676;&#38598;&#26631;&#35782;&#24182;&#25191;&#34892;&#21160;&#24577;&#20272;&#35745;&#12290;&#28982;&#21518;&#32858;&#21512;&#20197;&#26356;&#26032;&#27599;&#20010;&#32676;&#38598;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#27491;&#30830;&#22320;&#20272;&#35745;&#20102;&#32676;&#38598;&#26631;&#35782;&#65292;&#24182;&#23454;&#29616;&#20102;&#36817;&#20284;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20854;&#19982;&#32676;&#38598;&#20013;&#31995;&#32479;&#25968;&#25104;&#21453;&#27604;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26356;&#39640;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#31995;&#32479;&#35782;&#21035;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of learning linear system models from observing multiple trajectories from different system dynamics. This framework encompasses a collaborative scenario where several systems seeking to estimate their dynamics are partitioned into clusters according to their system similarity. Thus, the systems within the same cluster can benefit from the observations made by the others. Considering this framework, we present an algorithm where each system alternately estimates its cluster identity and performs an estimation of its dynamics. This is then aggregated to update the model of each cluster. We show that under mild assumptions, our algorithm correctly estimates the cluster identities and achieves an approximate sample complexity that scales inversely with the number of systems in the cluster, thus facilitating a more efficient and personalized system identification process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#38142;&#36335;&#39044;&#27979;&#31561;&#19981;&#21516;&#24212;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.01391</link><description>&lt;p&gt;
&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Learning on Graphs: A Survey. (arXiv:2304.01391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#38142;&#36335;&#39044;&#27979;&#31561;&#19981;&#21516;&#24212;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#22914;&#31038;&#20132;&#32593;&#32476;&#12289;&#20998;&#23376;&#22270;&#21644;&#20132;&#26131;&#32593;&#32476;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20419;&#36827;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;GNN&#20855;&#26377;&#19968;&#20123;&#32570;&#28857;&#65292;&#22914;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#23481;&#26131;&#32487;&#25215;&#35757;&#32451;&#25968;&#25454;&#30340;&#20559;&#35265;&#65292;&#19981;&#33021;&#24314;&#27169;&#22240;&#26524;&#20851;&#31995;&#12290;&#26368;&#36817;&#65292;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#32531;&#35299;&#36825;&#20123;&#32570;&#28857;&#26041;&#38754;&#20855;&#26377;&#24456;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#30340;&#21457;&#23637;&#65292;&#26412;&#32508;&#36848;&#23558;&#20998;&#31867;&#21644;&#20840;&#38754;&#22320;&#35780;&#20272;&#21453;&#20107;&#23454;&#22270;&#23398;&#20064;&#35770;&#25991;&#65292;&#20026;&#27599;&#20010;&#31867;&#21035;&#25552;&#20379;&#32972;&#26223;&#21644;&#28608;&#21169;&#24615;&#20363;&#23376;&#12289;&#19968;&#33324;&#26694;&#26550;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-structured data are pervasive in the real-world such as social networks, molecular graphs and transaction networks. Graph neural networks (GNNs) have achieved great success in representation learning on graphs, facilitating various downstream tasks. However, GNNs have several drawbacks such as lacking interpretability, can easily inherit the bias of the training data and cannot model the casual relations. Recently, counterfactual learning on graphs has shown promising results in alleviating these drawbacks. Various graph counterfactual learning approaches have been proposed for counterfactual fairness, explainability, link prediction and other applications on graphs. To facilitate the development of this promising direction, in this survey, we categorize and comprehensively review papers on graph counterfactual learning. We divide existing methods into four categories based on research problems studied. For each category, we provide background and motivating examples, a general f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;PON&#31995;&#32479;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25925;&#38556;&#20998;&#25903;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#38169;&#35823;&#20998;&#25903;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#22810;&#20010;&#38271;&#24230;&#30456;&#20284;&#30340;&#20998;&#25903;&#20135;&#29983;&#21453;&#23556;&#37325;&#21472;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36827;&#34892;&#38548;&#31163;&#12290;</title><link>http://arxiv.org/abs/2304.01376</link><description>&lt;p&gt;
&#12298;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#34987;&#21160;&#24335;&#20809;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#20998;&#25903;&#12299;
&lt;/p&gt;
&lt;p&gt;
Faulty Branch Identification in Passive Optical Networks using Machine Learning. (arXiv:2304.01376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;PON&#31995;&#32479;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25925;&#38556;&#20998;&#25903;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#38169;&#35823;&#20998;&#25903;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#22810;&#20010;&#38271;&#24230;&#30456;&#20284;&#30340;&#20998;&#25903;&#20135;&#29983;&#21453;&#23556;&#37325;&#21472;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36827;&#34892;&#38548;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#21160;&#24335;&#20809;&#32593;&#32476;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23485;&#24102;&#25509;&#20837;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#30830;&#20445;&#21487;&#38752;&#20256;&#36755;&#24182;&#28385;&#36275;&#26381;&#21153;&#27700;&#24179;&#21327;&#35758;&#65292;PON&#31995;&#32479;&#24517;&#39035;&#19981;&#26029;&#30417;&#27979;&#65292;&#20197;&#24555;&#36895;&#35782;&#21035;&#21644;&#23450;&#20301;&#32593;&#32476;&#25925;&#38556;&#12290;&#38024;&#23545;PON&#31995;&#32479;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25925;&#38556;&#20998;&#25903;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#35760;&#24405;&#30340;&#20809;&#26102;&#22495;&#21453;&#23556;&#65288;OTDR&#65289;&#36319;&#36394;&#20449;&#24687;&#26469;&#36827;&#34892;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#20004;&#20010;&#25110;&#26356;&#22810;&#38271;&#24230;&#30456;&#20284;&#30340;&#20998;&#25903;&#20135;&#29983;&#30340;&#21453;&#23556;&#37325;&#21472;&#26102;&#65292;&#25925;&#38556;&#20998;&#25903;&#30340;&#38548;&#31163;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#31649;&#29702;PON&#31995;&#32479;&#20013;&#30340;&#20809;&#23398;&#25925;&#38556;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passive optical networks (PONs) have become a promising broadband access network solution. To ensure a reliable transmission, and to meet service level agreements, PON systems have to be monitored constantly in order to quickly identify and localize networks faults. Typically, a service disruption in a PON system is mainly due to fiber cuts and optical network unit (ONU) transmitter/receiver failures. When the ONUs are located at different distances from the optical line terminal (OLT), the faulty ONU or branch can be identified by analyzing the recorded optical time domain reflectometry (OTDR) traces. However, faulty branch isolation becomes very challenging when the reflections originating from two or more branches with similar length overlap, which makes it very hard to discriminate the faulty branches given the global backscattered signal. Recently, machine learning (ML) based approaches have shown great potential for managing optical faults in PON systems. Such techniques perform 
&lt;/p&gt;</description></item><item><title>Ada-SpikeDeep-Classifier&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#33041;&#26426;&#25509;&#21475;&#20449;&#21495;&#22788;&#29702;&#30340;&#33258;&#36866;&#24212;&#33258;&#32452;&#32455;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#20102;SpikeDeeptector&#36827;&#34892;&#20449;&#36947;&#36873;&#25321;&#12289;Ada-BAR&#36827;&#34892;&#20449;&#21495;&#39044;&#22788;&#29702;&#12289;OCM&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#25552;&#39640;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#35299;&#30721;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01355</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;SpikeDeep-&#20998;&#31867;&#22120;:&#29992;&#20110;&#23454;&#26102;&#33041;&#26426;&#25509;&#21475;&#20449;&#21495;&#22788;&#29702;&#30340;&#33258;&#32452;&#32455;&#33258;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive SpikeDeep-Classifier: Self-organizing and self-supervised machine learning algorithm for online spike sorting. (arXiv:2304.01355v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01355
&lt;/p&gt;
&lt;p&gt;
Ada-SpikeDeep-Classifier&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#33041;&#26426;&#25509;&#21475;&#20449;&#21495;&#22788;&#29702;&#30340;&#33258;&#36866;&#24212;&#33258;&#32452;&#32455;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#20102;SpikeDeeptector&#36827;&#34892;&#20449;&#36947;&#36873;&#25321;&#12289;Ada-BAR&#36827;&#34892;&#20449;&#21495;&#39044;&#22788;&#29702;&#12289;OCM&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#25552;&#39640;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#35299;&#30721;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#35299;&#30721;&#25928;&#26524;&#65292;&#36890;&#36807;&#38024;&#23545;&#23494;&#38598;&#24494;&#30005;&#26497;&#38453;&#21015;&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#32452;&#32455;&#31639;&#27861;&#8212;&#8212;&#33258;&#36866;&#24212;SpikeDeep-&#20998;&#31867;&#22120;(Ada-SpikeDeepClassifier)&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;SpikeDeeptector&#36827;&#34892;&#20449;&#36947;&#36873;&#25321;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#32972;&#26223;&#27963;&#21160;&#25298;&#32477;&#22120;(Ada-BAR)&#36827;&#34892;&#20449;&#21495;&#39044;&#22788;&#29702;&#65292;&#24182;&#37319;&#29992;&#33258;&#30417;&#30563;&#22312;&#32447;&#32858;&#31867;&#27169;&#22359;(OCM)&#36827;&#34892;&#20998;&#31867;&#12290;Ada-SpikeDeep-Classifier&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#35760;&#24405;&#20013;&#22343;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#24378;&#20581;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#26395;&#25193;&#23637;&#21040;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#20854;&#20182;&#31070;&#32463;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective. Research on brain-computer interfaces (BCIs) is advancing towards rehabilitating severely disabled patients in the real world. Two key factors for successful decoding of user intentions are the size of implanted microelectrode arrays and a good online spike sorting algorithm. A small but dense microelectrode array with 3072 channels was recently developed for decoding user intentions. The process of spike sorting determines the spike activity (SA) of different sources (neurons) from recorded neural data. Unfortunately, current spike sorting algorithms are unable to handle the massively increasing amount of data from dense microelectrode arrays, making spike sorting a fragile component of the online BCI decoding framework. Approach. We proposed an adaptive and self-organized algorithm for online spike sorting, named Adaptive SpikeDeep-Classifier (Ada-SpikeDeepClassifier), which uses SpikeDeeptector for channel selection, an adaptive background activity rejector (Ada-BAR) for 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#21333;&#35843;&#31639;&#23376;&#23398;&#20064;&#65288;MOL&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#21333;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#20849;&#36717;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#21152;&#36895;&#24182;&#34892;MRI&#65292;&#22312;&#20869;&#23384;&#25928;&#29575;&#21644;&#24615;&#33021;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.01351</link><description>&lt;p&gt;
&#21033;&#29992;&#20869;&#23384;&#39640;&#25928;&#21644;&#40065;&#26834;&#21333;&#35843;&#31639;&#23376;&#23398;&#20064;&#30340;&#21152;&#36895;&#24182;&#34892;MRI&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerated parallel MRI using memory efficient and robust monotone operator learning (MOL). (arXiv:2304.01351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#21333;&#35843;&#31639;&#23376;&#23398;&#20064;&#65288;MOL&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#21333;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#20849;&#36717;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#21152;&#36895;&#24182;&#34892;MRI&#65292;&#22312;&#20869;&#23384;&#25928;&#29575;&#21644;&#24615;&#33021;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#25104;&#20687;&#29289;&#29702;&#19982;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#24050;&#25104;&#20026;&#21152;&#36895;&#24182;&#34892;MRI&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#30830;&#23450;&#21333;&#35843;&#31639;&#23376;&#23398;&#20064;&#65288;MOL&#65289;&#26694;&#26550;&#22312;&#24182;&#34892;MRI&#35774;&#32622;&#20013;&#30340;&#25928;&#29992;&#12290;MOL&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#21333;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#20197;&#21450;&#20351;&#29992;&#20849;&#36717;&#26799;&#24230;&#31639;&#27861;&#40723;&#21169;&#25968;&#25454;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#20132;&#26367;&#20248;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22909;&#22788;&#21253;&#25324;&#20855;&#26377;&#21387;&#32553;&#24863;&#30693;&#31639;&#27861;&#31867;&#20284;&#30340;&#29420;&#29305;&#24615;&#12289;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#27604;&#23637;&#24320;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#22312;&#38745;&#24577;&#21644;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#19981;&#21516;&#23637;&#24320;&#31639;&#27861;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based deep learning methods that combine imaging physics with learned regularization priors have been emerging as powerful tools for parallel MRI acceleration. The main focus of this paper is to determine the utility of the monotone operator learning (MOL) framework in the parallel MRI setting. The MOL algorithm alternates between a gradient descent step using a monotone convolutional neural network (CNN) and a conjugate gradient algorithm to encourage data consistency. The benefits of this approach include similar guarantees as compressive sensing algorithms including uniqueness, convergence, and stability, while being significantly more memory efficient than unrolled methods. We validate the proposed scheme by comparing it with different unrolled algorithms in the context of accelerated parallel MRI for static and dynamic settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#21644;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#23454;&#29616;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01347</link><description>&lt;p&gt;
&#26102;&#38388;&#21160;&#24577;&#21516;&#27493;&#21151;&#33021;&#33041;&#32593;&#32476;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis. (arXiv:2304.01347v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#21644;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#23454;&#29616;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#21487;&#20197;&#25429;&#25417;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#20013;&#30340;&#33041;&#27963;&#21160;&#26102;&#21464;&#24322;&#24120;&#65292;&#24182;&#22312;&#25581;&#31034;&#31934;&#31070;&#20998;&#35010;&#30151;&#65288;SZ&#65289;&#24739;&#32773;&#24322;&#24120;&#33041;&#27963;&#21160;&#26426;&#21046;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#21160;&#24577;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#8212;&#8212;&#26102;&#24577;&#33041;&#31867;&#21035;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;temporal-BCGCN&#65289;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#29420;&#29305;&#30340;&#21160;&#24577;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22359;DSF-BrainNet&#65292;&#29992;&#20110;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;TemporalConv&#65292;&#22522;&#20110;&#29305;&#24449;&#30340;&#21516;&#27493;&#26102;&#38388;&#23646;&#24615;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#21270;&#24322;&#24120;&#21322;&#29699;&#20391;&#21270;&#26816;&#27979;&#24037;&#20855;&#65292;&#31216;&#20026;CategoryPool&#12290;&#35813;&#30740;&#31350;&#22312;COBRE&#21644;UCLA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#20998;&#21035;&#36798;&#21040;83.62&#65285;&#21644;89.71&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Available evidence suggests that dynamic functional connectivity (dFC) can capture time-varying abnormalities in brain activity in rs-fMRI data and has a natural advantage in uncovering mechanisms of abnormal brain activity in schizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysis model called the temporal brain category graph convolutional network (temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis module, DSF-BrainNet, was designed to construct dynamic synchronization features. Subsequently, a revolutionary graph convolution method, TemporalConv, was proposed, based on the synchronous temporal properties of feature. Finally, the first modular abnormal hemispherical lateralization test tool in deep learning based on rs-fMRI data, named CategoryPool, was proposed. This study was validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average accuracy, respectively, outperforming the baseline model and other State-of-the-Art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31616;&#21333;&#27979;&#37327;&#21644;&#31616;&#21270;&#27169;&#22411;&#26500;&#24314;&#28151;&#21512;&#22411;&#38669;&#22855;&#37329;-&#36203;&#32997;&#40654;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#27979;&#37327;&#37327;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#34920;&#31034;&#31070;&#32463;&#20803;&#21160;&#20316;&#30005;&#20301;&#12290;</title><link>http://arxiv.org/abs/2304.01346</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#22411;&#38669;&#22855;&#37329;-&#36203;&#32997;&#40654;&#21160;&#20316;&#30005;&#20301;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an Hybrid Hodgkin-Huxley Action Potential Generation Model. (arXiv:2304.01346v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31616;&#21333;&#27979;&#37327;&#21644;&#31616;&#21270;&#27169;&#22411;&#26500;&#24314;&#28151;&#21512;&#22411;&#38669;&#22855;&#37329;-&#36203;&#32997;&#40654;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#27979;&#37327;&#37327;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#34920;&#31034;&#31070;&#32463;&#20803;&#21160;&#20316;&#30005;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#23398;&#27169;&#22411;&#23545;&#20110;&#31070;&#32463;&#20803;&#30005;&#27963;&#21160;&#24341;&#36215;&#29983;&#29702;&#26426;&#21046;&#30340;&#29702;&#35299;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;&#20854;&#20013;&#65292;&#19968;&#20123;&#21253;&#25324;&#33180;&#30005;&#20301;&#32463;&#39564;&#20989;&#25968;&#30340;&#26041;&#31243;&#34987;&#23450;&#20041;&#12290;&#26368;&#24120;&#35265;&#30340;&#38669;&#22855;&#37329;-&#36203;&#32997;&#40654;&#27169;&#22411;&#26159;&#36825;&#31181;&#33539;&#20363;&#30340;&#19968;&#20010;&#20363;&#23376;&#65292;&#22240;&#20026;&#23427;&#23558;&#31163;&#23376;&#36890;&#36947;&#30340;&#30005;&#23548;&#23450;&#20041;&#20026;&#36890;&#36947;&#20013;&#30340;&#27599;&#31181;&#38376;&#25171;&#24320;&#21644;&#20851;&#38381;&#36895;&#29575;&#12290;&#36825;&#20123;&#20989;&#25968;&#38656;&#35201;&#20174;&#23454;&#39564;&#23460;&#27979;&#37327;&#20013;&#33719;&#24471;&#65292;&#32780;&#36825;&#20123;&#23454;&#39564;&#36890;&#24120;&#38750;&#24120;&#26114;&#36149;&#65292;&#21448;&#22240;&#20026;&#28041;&#21450;&#21333;&#20010;&#32454;&#32990;&#33180;&#36890;&#36947;&#30005;&#21387;&#30340;&#26102;&#38388;&#31354;&#38388;&#29420;&#31435;&#27979;&#37327;&#65292;&#22240;&#27492;&#20135;&#29983;&#30340;&#25968;&#25454;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20165;&#20351;&#29992;&#20004;&#20010;&#31616;&#21333;&#30340;&#27979;&#37327;&#65288;&#26102;&#38388;&#30340;&#33180;&#30005;&#21387;&#20989;&#25968;&#21644;&#35302;&#21457;&#35813;&#30005;&#21387;&#30340;&#27880;&#20837;&#30005;&#27969;&#65289;&#20197;&#21450;&#23558;&#38669;&#22855;&#37329;-&#36203;&#32997;&#40654;&#27169;&#22411;&#19982;&#31616;&#21333;&#31561;&#25928;&#30005;&#36335;&#32452;&#21512;&#32780;&#25104;&#30340;&#31616;&#21270;&#27169;&#22411;&#26469;&#23547;&#25214;&#38669;&#22855;&#37329;-&#36203;&#32997;&#40654;&#27169;&#22411;&#21442;&#25968;&#20989;&#25968;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#28151;&#21512;&#27169;&#22411;&#33021;&#22815;&#20197;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24615;&#34920;&#31034;&#21160;&#20316;&#30005;&#20301;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#27979;&#37327;&#37327;&#26174;&#33879;&#20943;&#23569;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#30740;&#31350;&#31070;&#32463;&#20803;&#30005;&#27963;&#21160;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical models for the generation of the action potential can improve the understanding of physiological mechanisms that are consequence of the electrical activity in neurons. In such models, some equations involving empirically obtained functions of the membrane potential are usually defined. The best known of these models, the Hodgkin-Huxley model, is an example of this paradigm since it defines the conductances of ion channels in terms of the opening and closing rates of each type of gate present in the channels. These functions need to be derived from laboratory measurements that are often very expensive and produce little data because they involve a time-space-independent measurement of the voltage in a single channel of the cell membrane. In this work, we investigate the possibility of finding the Hodgkin-Huxley model's parametric functions using only two simple measurements (the membrane voltage as a function of time and the injected current that triggered that voltage) and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#30740;&#31350;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20302;&#25439;&#22833;&#21306;&#22495;&#23384;&#22312;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#30001;&#38752;&#36817;&#20998;&#31867;&#20915;&#31574;&#36793;&#30028;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20915;&#23450;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01335</link><description>&lt;p&gt;
&#29992;&#28909;&#22122;&#22768;&#25551;&#32472;&#31070;&#32463;&#32593;&#32476;&#26223;&#35266;&#30340;&#22320;&#24418;
&lt;/p&gt;
&lt;p&gt;
Charting the Topography of the Neural Network Landscape with Thermal-Like Noise. (arXiv:2304.01335v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#30740;&#31350;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20302;&#25439;&#22833;&#21306;&#22495;&#23384;&#22312;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#30001;&#38752;&#36817;&#20998;&#31867;&#20915;&#31574;&#36793;&#30028;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20915;&#23450;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#39640;&#32500;&#12289;&#38750;&#20984;&#19988;&#22024;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#29702;&#35770;&#29702;&#35299;&#22312;&#24212;&#29992;&#35282;&#24230;&#21644;&#22522;&#30784;&#30740;&#31350;&#26041;&#38754;&#22343;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#37319;&#29992;&#26631;&#20934;&#30340;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;Langevin&#21160;&#24577;&#30456;&#31354;&#38388;&#25506;&#27979;&#26041;&#27861;&#30740;&#31350;&#36807;&#21442;&#25968;&#20840;&#36830;&#25509;&#32593;&#32476;&#22312;&#38543;&#26426;&#25968;&#25454;&#19978;&#25191;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#12290;&#36890;&#36807;&#20998;&#26512;&#28072;&#33853;&#32479;&#35745;&#25968;&#25454;&#65292;&#31867;&#27604;&#20110;&#20307;&#31995;&#22312;&#24658;&#23450;&#28201;&#24230;&#19979;&#30340;&#28909;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#22320;&#24418;&#25551;&#36848;&#8212;&#8212;&#20302;&#25439;&#22833;&#21306;&#22495;&#26159;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20854;&#32500;&#24230;&#21487;&#20197;&#36731;&#26131;&#22320;&#20174;&#27874;&#21160;&#24615;&#20013;&#33719;&#24471;&#12290;&#27492;&#22806;&#65292;&#35813;&#32500;&#24230;&#21463;&#21040;&#38752;&#36817;&#20998;&#31867;&#20915;&#31574;&#36793;&#30028;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#25511;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#22235;&#38454;&#30456;&#20114;&#20316;&#29992;&#26159;&#20851;&#38190;&#30340;&#65292;&#32780;&#26631;&#20934;&#30340; Langevin &#26041;&#27861;&#19981;&#33021;&#20934;&#30830;&#25551;&#36848;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of neural networks is a complex, high-dimensional, non-convex and noisy optimization problem whose theoretical understanding is interesting both from an applicative perspective and for fundamental reasons. A core challenge is to understand the geometry and topography of the landscape that guides the optimization. In this work, we employ standard Statistical Mechanics methods, namely, phase-space exploration using Langevin dynamics, to study this landscape for an over-parameterized fully connected network performing a classification task on random data. Analyzing the fluctuation statistics, in analogy to thermal dynamics at a constant temperature, we infer a clear geometric description of the low-loss region. We find that it is a low-dimensional manifold whose dimension can be readily obtained from the fluctuations. Furthermore, this dimension is controlled by the number of data points that reside near the classification decision boundary. Importantly, we find that a quadra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21028;&#26029;&#36136;&#25968;&#25972;&#38500;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20851;&#38190;&#22312;&#20110;&#25552;&#20379;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#21830;&#19994;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26080;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#38656;&#35201;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.01333</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21028;&#26029;&#36136;&#25968;&#25972;&#38500;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Prime Number Divisibility by Deep Learning. (arXiv:2304.01333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21028;&#26029;&#36136;&#25968;&#25972;&#38500;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20851;&#38190;&#22312;&#20110;&#25552;&#20379;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#21830;&#19994;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26080;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#38656;&#35201;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30830;&#23450;&#19968;&#20010;&#25972;&#25968;&#26159;&#21542;&#33021;&#22815;&#34987;2&#12289;3&#25110;&#20854;&#20182;&#36136;&#25968;&#25972;&#38500;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#21487;&#33021;&#24456;&#31616;&#21333;&#65292;&#20294;&#22312;&#27809;&#26377;&#39044;&#20808;&#25351;&#23450;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#23545;&#20110;&#35745;&#31639;&#26426;&#26469;&#35828;&#21487;&#33021;&#24182;&#19981;&#23481;&#26131;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#21644;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#30830;&#23450;&#22823;&#26377;&#38480;&#25972;&#25968;&#65288;&#39640;&#36798;$2^{32}$&#65289;&#26159;&#21542;&#33021;&#22815;&#34987;&#23567;&#36136;&#25968;&#25972;&#38500;&#30340;&#24773;&#20917;&#19979;&#65292;&#21508;&#31181;&#26694;&#26550;&#21644;&#32593;&#32476;&#32467;&#26500;&#65288;CNN&#12289;RNN&#12289;Transformer&#31561;&#65289;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#36136;&#25968;&#25972;&#38500;&#24615;&#30340;&#33021;&#21147;&#26497;&#22823;&#22320;&#21462;&#20915;&#20110;&#25552;&#20379;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#32593;&#32476;&#26694;&#26550;&#25110;&#32593;&#32476;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65288;CNN&#12289;RNN&#12289;Transformer&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26469;&#33258;&#20122;&#39532;&#36874;&#12289;&#35895;&#27468;&#21644;&#24494;&#36719;&#30340;&#21830;&#19994;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#31649;&#36947;&#65292;&#24182;&#35777;&#26126;&#38500;&#38750;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#21542;&#21017;&#23427;&#20204;&#26080;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certain tasks such as determining whether a given integer can be divided by 2, 3, or other prime numbers may be trivial for human beings, but can be less straightforward for computers in the absence of pre-specified algorithms. In this paper, we tested multiple deep learning architectures and feature engineering approaches, and evaluated the scenario of determining divisibility of large finite integers (up to $2^{32}$) by small prime numbers. It turns out that, regardless of the network frameworks or the complexity of the network structures (CNN, RNN, Transformer, etc.), the ability to predict the prime number divisibility critically depends on the feature space fed into the deep learning models. We also evaluated commercially available Automated Machine Learning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstrated that they failed to address this issue unless appropriately engineered features were provided. We further proposed a closed form solution to the problem us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#28382;&#24494;&#20998;&#26041;&#31243;&#30340;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21644;&#26102;&#28382;&#65292;&#20855;&#26377;&#23398;&#20064;DDE&#21442;&#25968;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01329</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#26102;&#28382;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Learning the Delay Using Neural Delay Differential Equations. (arXiv:2304.01329v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#28382;&#24494;&#20998;&#26041;&#31243;&#30340;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21644;&#26102;&#28382;&#65292;&#20855;&#26377;&#23398;&#20064;DDE&#21442;&#25968;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#21160;&#21147;&#31995;&#32479;&#30340;&#20132;&#21449;&#28857;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODEs&#65289;&#20195;&#34920;&#20102;&#36825;&#20123;&#39046;&#22495;&#20043;&#38388;&#20016;&#23500;&#30340;&#20132;&#21472;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#28382;&#24494;&#20998;&#26041;&#31243;&#65288;DDEs&#65289;&#30340;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21644;&#26102;&#28382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;NODEs&#30340;&#21551;&#21457;&#65292;&#24182;&#25193;&#23637;&#20102;&#26089;&#26399;&#30340;&#31070;&#32463;DDE&#27169;&#22411;&#65292;&#21518;&#32773;&#20551;&#35774;&#26102;&#28382;&#30340;&#20540;&#26159;&#24050;&#30693;&#30340;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28789;&#25935;&#24230;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20174;&#22522;&#20934;&#31995;&#32479;&#20013;&#23398;&#20064;DDE&#21442;&#25968;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#35752;&#35770;&#20013;&#24471;&#20986;&#32467;&#35770;&#65292;&#25552;&#20986;&#26410;&#26469;&#21487;&#33021;&#30340;&#26041;&#21521;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of machine learning and dynamical systems has generated considerable interest recently. Neural Ordinary Differential Equations (NODEs) represent a rich overlap between these fields. In this paper, we develop a continuous time neural network approach based on Delay Differential Equations (DDEs). Our model uses the adjoint sensitivity method to learn the model parameters and delay directly from data. Our approach is inspired by that of NODEs and extends earlier neural DDE models, which have assumed that the value of the delay is known a priori. We perform a sensitivity analysis on our proposed approach and demonstrate its ability to learn DDE parameters from benchmark systems. We conclude our discussion with potential future directions and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#22914;&#20309;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#30340;&#36164;&#28304;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#35777;&#35774;&#35745;&#30340;&#25361;&#25112;&#65292;&#24182;&#24357;&#34917;&#23454;&#35777;&#30740;&#31350;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#24369;&#30340;&#32479;&#35745;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.01315</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#35777;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Empirical Design in Reinforcement Learning. (arXiv:2304.01315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#22914;&#20309;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#30340;&#36164;&#28304;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#35777;&#35774;&#35745;&#30340;&#25361;&#25112;&#65292;&#24182;&#24357;&#34917;&#23454;&#35777;&#30740;&#31350;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#24369;&#30340;&#32479;&#35745;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#35777;&#35774;&#35745;&#19981;&#26159;&#23567;&#20219;&#21153;&#12290;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#38656;&#35201;&#35762;&#31350;&#32454;&#33410;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26102;&#20505;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24120;&#29992;&#31639;&#27861;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#21644;&#23454;&#29616;&#32454;&#33410;&#25935;&#24863;&#65292;&#24182;&#19988;&#24120;&#35265;&#30340;&#23454;&#35777;&#20570;&#27861;&#20250;&#23548;&#33268;&#24369;&#30340;&#32479;&#35745;&#35777;&#25454;&#12290;&#26412;&#25991;&#19981;&#20165;&#21628;&#21505;&#34892;&#21160;&#65292;&#32780;&#19988;&#26159;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#30340;&#20840;&#38754;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical design in reinforcement learning is no small task. Running good experiments requires attention to detail and at times significant computational resources. While compute resources available per dollar have continued to grow rapidly, so have the scale of typical experiments in reinforcement learning. It is now common to benchmark agents with millions of parameters against dozens of tasks, each using the equivalent of 30 days of experience. The scale of these experiments often conflict with the need for proper statistical evidence, especially when comparing algorithms. Recent studies have highlighted how popular algorithms are sensitive to hyper-parameter settings and implementation details, and that common empirical practice leads to weak statistical evidence (Machado et al., 2018; Henderson et al., 2018). Here we take this one step further.  This manuscript represents both a call to action, and a comprehensive resource for how to do good experiments in reinforcement learning. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#35848;19&#20301;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#65292;&#21457;&#29616;KG&#26500;&#24314;&#32773;&#38656;&#27714;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#65292;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#65292;KG&#28040;&#36153;&#32773;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#65292;&#24182;&#25351;&#20986;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;KG&#38656;&#35201;&#25216;&#26415;&#21644;&#31038;&#20132;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.01311</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30693;&#35782;&#22270;&#35889;&#29992;&#25143;&#12289;&#25361;&#25112;&#21644;&#21487;&#35270;&#21270;&#38656;&#27714;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice. (arXiv:2304.01311v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#35848;19&#20301;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#65292;&#21457;&#29616;KG&#26500;&#24314;&#32773;&#38656;&#27714;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#65292;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#65292;KG&#28040;&#36153;&#32773;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#65292;&#24182;&#25351;&#20986;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;KG&#38656;&#35201;&#25216;&#26415;&#21644;&#31038;&#20132;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;19&#20301;&#26469;&#33258;&#20225;&#19994;&#21644;&#23398;&#26415;&#29615;&#22659;&#19979;&#12289;&#28041;&#21450;&#21508;&#31181;&#29992;&#20363;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#30340;&#35775;&#35848;&#65292;&#25552;&#20986;&#20102;KG&#23454;&#36341;&#32773;&#22312;&#21019;&#24314;&#12289;&#25506;&#32034;&#21644;&#20998;&#26512;KG&#26102;&#36935;&#21040;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#21487;&#35270;&#21270;&#35774;&#35745;&#26469;&#32531;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;KG&#23454;&#36341;&#32773;&#21487;&#20197;&#20998;&#20026;&#19977;&#31867;&#65306;KG&#26500;&#24314;&#32773;&#12289;&#20998;&#26512;&#24072;&#21644;&#28040;&#36153;&#32773;&#65292;&#27599;&#20010;&#20154;&#37117;&#26377;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#38656;&#27714;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;KG&#26500;&#24314;&#32773;&#21487;&#20197;&#20174;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#20013;&#33719;&#30410;&#65292;&#32780;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#25552;&#20379;&#20013;&#38388;&#26597;&#35810;&#32467;&#26524;&#30340;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#12290;&#23545;&#20110;KG&#28040;&#36153;&#32773;&#65292;&#25105;&#20204;&#30830;&#23450;&#33410;&#28857;&#38142;&#25509;&#22270;&#30340;&#25928;&#21147;&#19981;&#36275;&#65292;&#24182;&#38656;&#35201;&#23450;&#21046;&#30340;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#26469;&#20419;&#36827;KG&#30340;&#37319;&#29992;&#21644;&#29702;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#22320;&#23454;&#26045;KG&#38656;&#35201;&#19981;&#20165;&#25216;&#26415;&#19978;&#30340;&#65292;&#36824;&#26377;&#31038;&#20132;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30446;&#21069;&#24182;&#26410;&#34987;&#24403;&#21069;&#30340;&#24037;&#20855;&#12289;&#25216;&#26415;&#21644;&#26368;&#20339;&#23454;&#36341;&#25152;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners - KG Builders, Analysts, and Consumers - each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#36864;&#28779;&#30340;&#19979;&#30028;&#65292;&#23545;&#38500;$\log L$&#20043;&#22806;&#30340;&#25152;&#26377;&#21442;&#25968;&#20855;&#26377;&#22810;&#39033;&#24335;&#20381;&#36182;&#24615;&#65292;&#20854;&#25913;&#36827;&#20102;&#29616;&#26377;&#30028;&#38480;&#12290;&#22240;&#27492;&#65292;&#35813;&#31639;&#27861;&#30340;&#28151;&#21512;&#26102;&#38388;&#21487;&#33021;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.01303</link><description>&lt;p&gt;
&#24182;&#34892;&#36864;&#28779;&#28151;&#21512;&#26102;&#38388;&#30340;&#25913;&#36827;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Improved Bound for Mixing Time of Parallel Tempering. (arXiv:2304.01303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#36864;&#28779;&#30340;&#19979;&#30028;&#65292;&#23545;&#38500;$\log L$&#20043;&#22806;&#30340;&#25152;&#26377;&#21442;&#25968;&#20855;&#26377;&#22810;&#39033;&#24335;&#20381;&#36182;&#24615;&#65292;&#20854;&#25913;&#36827;&#20102;&#29616;&#26377;&#30028;&#38480;&#12290;&#22240;&#27492;&#65292;&#35813;&#31639;&#27861;&#30340;&#28151;&#21512;&#26102;&#38388;&#21487;&#33021;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37319;&#26679;&#31639;&#27861;&#39046;&#22495;&#20013;&#65292;&#24403;&#30452;&#25509;&#37319;&#26679;&#19981;&#21487;&#34892;&#26102;&#65292;MCMC&#65288;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65289;&#26041;&#27861;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#36890;&#24120;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#21644;&#28151;&#21512;&#19981;&#20339;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24182;&#34892;&#36864;&#28779;&#12290;&#23613;&#31649;&#20854;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20854;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20851;&#20110;&#24182;&#34892;&#36864;&#28779;&#35889;&#38388;&#38553;&#30340;&#19979;&#30028;&#65292;&#20854;&#23545;&#38500;$\log L$&#20043;&#22806;&#30340;&#25152;&#26377;&#21442;&#25968;&#20855;&#26377;&#22810;&#39033;&#24335;&#20381;&#36182;&#24615;&#65292;&#20854;&#20013;$(L + 1)$&#26159;&#32423;&#25968;&#30340;&#25968;&#37327;&#12290;&#36825;&#25913;&#36827;&#20102;&#29616;&#26377;&#30028;&#38480;&#65292;&#20854;&#19982;&#27169;&#24577;&#25968;&#21576;&#25351;&#25968;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#35889;&#38388;&#38553;&#30340;&#20551;&#35774;&#19978;&#30028;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#20854;&#23545;$\log L$&#20855;&#26377;&#25351;&#25968;&#20381;&#36182;&#24615;&#65292;&#36825;&#34920;&#26126;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#32039;&#23494;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of sampling algorithms, MCMC (Markov Chain Monte Carlo) methods are widely used when direct sampling is not possible. However, multimodality of target distributions often leads to slow convergence and mixing. One common solution is parallel tempering. Though highly effective in practice, theoretical guarantees on its performance are limited. In this paper, we present a new lower bound for parallel tempering on the spectral gap that has a polynomial dependence on all parameters except $\log L$, where $(L + 1)$ is the number of levels. This improves the best existing bound which depends exponentially on the number of modes. Moreover, we complement our result with a hypothetical upper bound on spectral gap that has an exponential dependence on $\log L$, which shows that, in some sense, our bound is tight.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01300</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#28857;&#30340;&#20984;&#21253;&#26469;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#20026;&#20960;&#20309;&#20307;&#65292;&#20174;&#32780;&#38544;&#34255;&#26377;&#20851;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20984;&#21253;&#26426;&#65288;KAHM&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20174;&#32467;&#26524;&#26377;&#30028;&#20960;&#20309;&#20307;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;KAHM&#26159;&#24191;&#27867;&#21644;&#28145;&#20837;&#30340;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23427;&#20204;&#20351;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#24212;&#29992;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#26679;&#26412;&#36890;&#36807;&#36716;&#25442;&#36807;&#31243;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#12290;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#19981;&#20165;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#19988;&#30830;&#20445;KAHM&#24314;&#27169;&#35823;&#24046;&#19981;&#22823;&#20110;&#21407;&#22987;&#25968;&#25454;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
&lt;/p&gt;</description></item><item><title>NG-EBM&#26159;&#19968;&#31181;&#38750;&#29983;&#25104;&#24335;&#30340;&#33021;&#37327;&#22522;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#36817;&#20284;&#36136;&#37327;&#20316;&#20026;&#34913;&#37327;&#27169;&#22411;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#12289;&#26356;&#23569;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01297</link><description>&lt;p&gt;
&#38750;&#29983;&#25104;&#30340;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Non-Generative Energy Based Models. (arXiv:2304.01297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01297
&lt;/p&gt;
&lt;p&gt;
NG-EBM&#26159;&#19968;&#31181;&#38750;&#29983;&#25104;&#24335;&#30340;&#33021;&#37327;&#22522;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#36817;&#20284;&#36136;&#37327;&#20316;&#20026;&#34913;&#37327;&#27169;&#22411;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#12289;&#26356;&#23569;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;EBM&#23558;&#27010;&#29575;&#26041;&#27861;&#24341;&#20837;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#20013;&#65292;&#24182;&#24050;&#32463;&#34920;&#26126;&#22312;&#26657;&#20934;&#12289;&#36234;&#30028;&#26816;&#27979;&#21644;&#23545;&#25239;&#24615;&#25269;&#25239;&#31561;&#39046;&#22495;&#20013;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20248;&#28857;&#20184;&#20986;&#20102;&#20272;&#31639;&#36755;&#20837;&#25968;&#25454;&#27010;&#29575;&#30340;&#20195;&#20215;&#65292;&#36890;&#24120;&#20351;&#29992;Langevin&#26041;&#27861;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#65288;SGLD&#65289;&#65292;&#36825;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#38656;&#35201;&#21442;&#25968;&#21270;&#12289;&#32531;&#23384;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#36935;&#21040;&#31283;&#23450;&#24615;&#21644;&#35268;&#27169;&#21270;&#38382;&#39064;&#12290;EBM&#20351;&#29992;&#21160;&#24577;&#26041;&#27861;&#20174;&#30001;&#32593;&#32476;&#24403;&#21069;&#29366;&#24577;&#23450;&#20041;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;&#20013;&#25277;&#21462;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#26041;&#27861;&#23558;&#20854;&#19982;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#23398;&#20064;&#27491;&#30830;&#30340;PDF&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29983;&#25104;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#38750;&#29983;&#25104;&#24335;EBM&#65288;NG-EBM&#65289;&#65292;&#23427;&#21033;&#29992;Grathwohl&#31561;&#20154;&#30830;&#23450;&#30340;&#8220;&#36817;&#20284;&#36136;&#37327;&#8221;&#20316;&#20026;&#34913;&#37327;&#27169;&#22411;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#32780;&#19981;&#26159;&#20272;&#31639;&#36755;&#20837;&#25968;&#25454;&#27010;&#29575;&#12290;&#36825;&#31181;&#36716;&#21464;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#12289;&#26356;&#23569;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based models (EBM) have become increasingly popular within computer vision. EBMs bring a probabilistic approach to training deep neural networks (DNN) and have been shown to enhance performance in areas such as calibration, out-of-distribution detection, and adversarial resistance. However, these advantages come at the cost of estimating input data probabilities, usually using a Langevin based method such as Stochastic Gradient Langevin Dynamics (SGLD), which bring additional computational costs, require parameterization, caching methods for efficiency, and can run into stability and scaling issues. EBMs use dynamical methods to draw samples from the probability density function (PDF) defined by the current state of the network and compare them to the training data using a maximum log likelihood approach to learn the correct PDF.  We propose a non-generative training approach, Non-Generative EBM (NG-EBM), that utilizes the {\it{Approximate Mass}}, identified by Grathwohl et al.,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#26222;&#23572;&#37329;&#27663;&#22270;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21407;&#22411;&#35774;&#22791;&#65292;&#21487;&#29992;&#20110;&#21160;&#24577;&#27880;&#35270;&#21644;&#35843;&#33410;&#27979;&#37327;&#65292;&#39044;&#27979;&#35843;&#33410;&#21487;&#20197;&#31934;&#30830;&#21040;0.25D&#65292;&#27491;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.01296</link><description>&lt;p&gt;
&#21033;&#29992;&#26222;&#23572;&#37329;&#27663;&#22270;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#21160;&#24577;&#35843;&#33410;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Dynamic Accommodation Measurement using Purkinje Images and ML Algorithms. (arXiv:2304.01296v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#26222;&#23572;&#37329;&#27663;&#22270;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21407;&#22411;&#35774;&#22791;&#65292;&#21487;&#29992;&#20110;&#21160;&#24577;&#27880;&#35270;&#21644;&#35843;&#33410;&#27979;&#37327;&#65292;&#39044;&#27979;&#35843;&#33410;&#21487;&#20197;&#31934;&#30830;&#21040;0.25D&#65292;&#27491;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;4&#20010;&#26222;&#23572;&#37329;&#27663;&#21453;&#23556;&#65288;PR&#65289;&#30340;&#21407;&#22411;&#35774;&#22791;&#65292;&#29992;&#20110;&#36866;&#29992;&#20110;AR&#21644;&#30524;&#31185;&#24212;&#29992;&#30340;&#21160;&#24577;&#27880;&#35270;&#21644;&#35843;&#33410;&#27979;&#37327;&#12290; PR1&#21644;2&#20197;&#21450;PR3&#21644;4&#20998;&#21035;&#29992;&#20110;&#20934;&#30830;&#27979;&#37327;&#20957;&#35270;&#21644;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#30524;&#30555;&#27169;&#22411;&#22312;ZEMAX&#20013;&#24320;&#21457;&#65292;&#24182;&#19982;&#23454;&#39564;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290; &#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;&#36229;&#36807;0.25D&#30340;&#31934;&#24230;&#20174;4&#24230;&#21040;1&#24230;&#39044;&#27979;&#35843;&#33410;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#37325;&#22797;&#24615;&#27979;&#35797;&#65292;&#24182;&#20174;&#21463;&#35797;&#32773;&#36523;&#19978;&#33719;&#24471;&#20102;&#20934;&#30830;&#30340;&#20957;&#35270;&#21644;&#35843;&#33410;&#20272;&#35745;&#12290;&#25105;&#20204;&#27491;&#22312;&#20351;&#29992;&#29289;&#29702;&#31934;&#30830;&#30340;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We developed a prototype device for dynamic gaze and accommodation measurements based on 4 Purkinje reflections (PR) suitable for use in AR and ophthalmology applications. PR1&amp;2 and PR3&amp;4 are used for accurate gaze and accommodation measurements, respectively. Our eye model was developed in ZEMAX and matches the experiments well. Our model predicts the accommodation from 4 diopters to 1 diopter with better than 0.25D accuracy. We performed repeatability tests and obtained accurate gaze and accommodation estimations from subjects. We are generating a large synthetic data set using physically accurate models and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26234;&#33021;&#23398;&#20064;&#30340;Cyber Gym for Intelligent Learning&#65288;CyGIL&#65289;&#20013;&#25552;&#20379;&#39640;&#24230;&#30495;&#23454;&#30340;&#32593;&#32476;Cyber Operations&#65288;CyOp&#65289;&#35757;&#32451;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#22120;&#29983;&#25104;&#21644;&#20195;&#29702;&#35757;&#32451;&#36807;&#31243;&#26469;&#38477;&#20302;&#20195;&#29702;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.01244</link><description>&lt;p&gt;
&#33258;&#20027;&#32593;&#32476;&#25915;&#20987;&#20195;&#29702;&#30340;&#32479;&#19968;&#20223;&#30495;&#27169;&#25311;&#35757;&#32451;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Unified Emulation-Simulation Training Environment for Autonomous Cyber Agents. (arXiv:2304.01244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26234;&#33021;&#23398;&#20064;&#30340;Cyber Gym for Intelligent Learning&#65288;CyGIL&#65289;&#20013;&#25552;&#20379;&#39640;&#24230;&#30495;&#23454;&#30340;&#32593;&#32476;Cyber Operations&#65288;CyOp&#65289;&#35757;&#32451;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#22120;&#29983;&#25104;&#21644;&#20195;&#29702;&#35757;&#32451;&#36807;&#31243;&#26469;&#38477;&#20302;&#20195;&#29702;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL / DRL&#65289;&#65292;&#21487;&#20197;&#24320;&#21457;&#33258;&#20027;&#32593;&#32476;&#25915;&#20987;&#20195;&#29702;&#65292;&#24182;&#22312;&#20195;&#34920;&#24615;&#29615;&#22659;&#20013;&#23545;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#35757;&#32451;&#29615;&#22659;&#24517;&#39035;&#39640;&#24230;&#30495;&#23454;&#22320;&#27169;&#25311;&#20195;&#29702;&#25152;&#35201;&#25506;&#32034;&#30340;&#32593;&#32476;Cyber Operations&#65288;CyOp&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26234;&#33021;&#23398;&#20064;&#30340;Cyber Gym for Intelligent Learning&#65288;CyGIL&#65289;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#12290;&#36890;&#36807;&#34920;&#24449;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#65292;CyGIL&#25552;&#20379;&#32479;&#19968;&#30340;CyOp&#22521;&#35757;&#29615;&#22659;&#65292;&#20854;&#20013;&#20223;&#30495;&#30340;CyGIL-S&#30001;&#33258;&#21160;&#29983;&#25104;&#30340;CyGIL-E&#29983;&#25104;&#12290;&#23558;&#27169;&#25311;&#22120;&#29983;&#25104;&#19982;&#20195;&#29702;&#35757;&#32451;&#36807;&#31243;&#38598;&#25104;&#65292;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#25152;&#38656;&#30340;&#20195;&#29702;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;CyGIL-S&#20013;&#35757;&#32451;&#30340;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#34987;&#20256;&#36755;&#21040;CyGIL-E&#65292;&#23436;&#20840;&#21487;&#36716;&#31227;&#33267;&#20223;&#30495;&#30340;&#8220;&#30495;&#23454;&#8221;&#32593;&#32476;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous cyber agents may be developed by applying reinforcement and deep reinforcement learning (RL/DRL), where agents are trained in a representative environment. The training environment must simulate with high-fidelity the network Cyber Operations (CyOp) that the agent aims to explore. Given the complexity of net-work CyOps, a good simulator is difficult to achieve. This work presents a systematic solution to automatically generate a high-fidelity simulator in the Cyber Gym for Intelligent Learning (CyGIL). Through representation learning and continuous learning, CyGIL provides a unified CyOp training environment where an emulated CyGIL-E automatically generates a simulated CyGIL-S. The simulator generation is integrated with the agent training process to further reduce the required agent training time. The agent trained in CyGIL-S is transferrable directly to CyGIL-E showing full transferability to the emulated "real" network. Experimental results are presented to demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28909;&#25104;&#20687;&#30340;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#36731;&#37327;&#32423;&#21644;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#19981;&#21463;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01243</link><description>&lt;p&gt;
CoReFusion: &#22522;&#20110;&#23545;&#27604;&#27491;&#21017;&#21270;&#34701;&#21512;&#23454;&#29616;&#28909;&#32418;&#22806;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
CoReFusion: Contrastive Regularized Fusion for Guided Thermal Super-Resolution. (arXiv:2304.01243v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28909;&#25104;&#20687;&#30340;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#36731;&#37327;&#32423;&#21644;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#19981;&#21463;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#25104;&#20687;&#30001;&#20110;&#22312;&#20302;&#20809;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#32780;&#27604;&#19968;&#33324;&#21487;&#35265;&#20809;&#25104;&#20687;&#20855;&#26377;&#26356;&#22810;&#20248;&#21183;&#12290; &#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20302;&#25104;&#26412;&#12289;&#20302;&#20998;&#36776;&#29575;&#30340;&#28909;&#25104;&#20687;&#20256;&#24863;&#22120;&#27979;&#37327;&#26469;&#22797;&#21046;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#28909;&#25104;&#20687;&#22270;&#29255;&#65292;&#32780;&#19988;&#22312;&#20851;&#38190;&#22320;&#21306;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#21487;&#35265;&#20809;&#20877;&#29983;&#33021;&#21147;&#30340;&#24212;&#29992;&#31243;&#24207;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#20687;&#20809;&#35889;&#33539;&#22260;&#19981;&#19968;&#33268;&#65292;&#20351;&#29992;&#21487;&#35265;&#20809;&#22270;&#20687;&#36827;&#34892;&#28909;&#25104;&#20687;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;&#24456;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28909;&#25104;&#20687;&#30340;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#20307;&#31995;&#32467;&#26500;&#22312;&#20002;&#22833;&#19968;&#20010;&#27169;&#24577;&#65288;&#39640;&#20998;&#36776;&#29575;RGB&#22270;&#20687;&#25110;&#26356;&#20302;&#20998;&#36776;&#29575;&#30340;&#28909;&#25104;&#20687;&#65289;&#26102;&#20173;&#20855;&#22791;&#35745;&#31639;&#25928;&#29575;&#21644;&#36731;&#37327;&#32423;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#23458;&#35266;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thermal imaging has numerous advantages over regular visible-range imaging since it performs well in low-light circumstances. Super-Resolution approaches can broaden their usefulness by replicating accurate high-resolution thermal pictures using measurements from low-cost, low-resolution thermal sensors. Because of the spectral range mismatch between the images, Guided Super-Resolution of thermal images utilizing visible range images is difficult. However, In case of failure to capture Visible Range Images can prevent the operations of applications in critical areas. We present a novel data fusion framework and regularization technique for Guided Super Resolution of Thermal images. The proposed architecture is computationally in-expensive and lightweight with the ability to maintain performance despite missing one of the modalities, i.e., high-resolution RGB image or the lower-resolution thermal image, and is designed to be robust in the presence of missing data. The proposed method pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24503;&#25289;&#32500;&#36798;&#35821;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#20415;&#23558;&#20854;&#35782;&#21035;&#20026;&#24656;&#21516;&#12289;&#36328;&#24615;&#21035;&#27495;&#35270;&#21644;&#38750;&#21453;LGBT+&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2304.01241</link><description>&lt;p&gt;
&#21457;&#29616;&#24503;&#25289;&#32500;&#36798;&#35821;&#20013;&#30340;&#24656;&#21516;&#21644;&#36328;&#24615;&#21035;&#27495;&#35270;&#65306;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detection of Homophobia &amp; Transphobia in Dravidian Languages: Exploring Deep Learning Methods. (arXiv:2304.01241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24503;&#25289;&#32500;&#36798;&#35821;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#20415;&#23558;&#20854;&#35782;&#21035;&#20026;&#24656;&#21516;&#12289;&#36328;&#24615;&#21035;&#27495;&#35270;&#21644;&#38750;&#21453;LGBT+&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#36785;&#39554;&#24615;&#20869;&#23481;&#30340;&#22686;&#21152;&#27491;&#22312;&#24433;&#21709;&#22312;&#32447;&#29992;&#25143;&#30340;&#31038;&#20132;&#29983;&#27963;&#12290;&#20351;&#29992;&#20882;&#29359;&#21644;&#20167;&#24680;&#35328;&#35770;&#20351;&#24471;&#31038;&#20132;&#23186;&#20307;&#21464;&#24471;&#26377;&#27602;&#12290;&#24656;&#21516;&#21644;&#36328;&#24615;&#21035;&#27495;&#35270;&#26159;&#38024;&#23545;LGBT+&#31038;&#32676;&#30340;&#20882;&#29359;&#24615;&#35780;&#35770;&#12290;&#21450;&#26102;&#26816;&#27979;&#21644;&#22788;&#29702;&#36825;&#20123;&#35780;&#35770;&#65292;&#21521;&#28041;&#21450;&#27492;&#31867;&#34892;&#20026;&#30340;&#29992;&#25143;&#21457;&#20986;&#35686;&#21578;&#25110;&#21450;&#26102;&#26631;&#35760;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#24503;&#25289;&#32500;&#36798;&#35821;&#36825;&#31181;&#34987;&#35748;&#20026;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#20013;&#65292;&#33258;&#21160;&#26816;&#27979;&#27492;&#31867;&#20869;&#23481;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35797;&#22270;&#25506;&#35752;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39532;&#26469;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#20415;&#23558;&#20854;&#35782;&#21035;&#20026;&#24656;&#21516;&#12289;&#36328;&#24615;&#21035;&#27495;&#35270;&#21644;&#38750;&#21453;LGBT+&#20869;&#23481;&#12290;&#32780;&#20854;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#22312;GloVe&#23884;&#20837;&#19979;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#23398;&#20064;&#27169;&#22411;&#65288;&#22810;&#35821;&#35328;BERT&#21644;IndicBERT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increase in abusive content on online social media platforms is impacting the social life of online users. Use of offensive and hate speech has been making so-cial media toxic. Homophobia and transphobia constitute offensive comments against LGBT+ community. It becomes imperative to detect and handle these comments, to timely flag or issue a warning to users indulging in such behaviour. However, automated detection of such content is a challenging task, more so in Dravidian languages which are identified as low resource languages. Motivated by this, the paper attempts to explore applicability of different deep learning mod-els for classification of the social media comments in Malayalam and Tamil lan-guages as homophobic, transphobic and non-anti-LGBT+content. The popularly used deep learning models- Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) using GloVe embedding and transformer-based learning models (Multilingual BERT and IndicBERT) are applied to the class
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#65292;&#25552;&#39640;&#20102;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01240</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;
&lt;/p&gt;
&lt;p&gt;
Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#65292;&#25552;&#39640;&#20102;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30140;&#30171;&#26159;&#35775;&#38382;&#21307;&#30103;&#36164;&#28304;&#30340;&#24120;&#35265;&#21407;&#22240;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#24515;&#29702;&#20581;&#24247;&#30340;&#37325;&#21472;&#26041;&#38754;&#12290;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26159;&#30740;&#31350;&#27492;&#37325;&#21472;&#30340;&#33391;&#22909;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30140;&#30171;&#30340;&#22823;&#37327;&#20449;&#24687;&#20445;&#23384;&#22312;&#36825;&#20123;&#35760;&#24405;&#30340;&#33258;&#30001;&#25991;&#26412;&#20013;&#65292;&#30001;&#20110;&#20854;&#27495;&#20041;&#24615;&#65292;&#30140;&#30171;&#30340;&#25552;&#21450;&#21576;&#29616;&#20986;&#29420;&#29305;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#12290;&#26412;&#39033;&#30446;&#20351;&#29992;&#21311;&#21517;&#30340;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#20026;&#35752;&#35770;&#24739;&#32773;&#30140;&#30171;&#25110;&#19981;&#35752;&#35770;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#20174;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#30140;&#30171;&#20449;&#24687;&#65292;&#24182;&#23558;&#36825;&#20123;&#36755;&#20986;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#12290;&#20849;&#25163;&#21160;&#19977;&#37325;&#27880;&#37322;&#20102;1,985&#20221;&#25991;&#20214;&#65292;&#20197;&#21019;&#24314;&#40644;&#37329;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#19977;&#31181;&#24120;&#29992;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#26368;&#20339;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#20026;0.787&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#30340;&#21487;&#34892;&#24615;&#65292;&#36825;&#21487;&#20197;&#25913;&#21892;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pain is a common reason for accessing healthcare resources and is a growing area of research, especially in its overlap with mental health. Mental health electronic health records are a good data source to study this overlap. However, much information on pain is held in the free text of these records, where mentions of pain present a unique natural language processing problem due to its ambiguous nature. This project uses data from an anonymised mental health electronic health records database. The data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not. This will facilitate the extraction of relevant pain information from large databases, and the use of such outputs for further studies on pain and mental health. 1,985 documents were manually triple-annotated for creation of gold standard training data, which was used to train three commonly used classification algorithms. The best performing model achieved an F1-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#19982;&#25345;&#32493;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#39046;&#22495;&#21464;&#21270;&#24341;&#36215;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.01239</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#39046;&#22495;&#21464;&#21270;&#30340;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#19982;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Distillation with Continual Learning for Cyclic Domain Shifts. (arXiv:2304.01239v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#19982;&#25345;&#32493;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#39046;&#22495;&#21464;&#21270;&#24341;&#36215;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#32531;&#24930;&#20294;&#20934;&#30830;&#30340;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35843;&#25972;&#30340;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#24403;&#39046;&#22495;&#21457;&#29983;&#21464;&#21270;&#26102;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#26159;&#24403;&#23398;&#29983;&#27169;&#22411;&#20351;&#29992;&#26032;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#26356;&#26032;&#26102;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#25152;&#23548;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#26469;&#38477;&#20302;&#39046;&#22495;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#38598;&#25104;&#21040;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#24490;&#29615;&#39046;&#22495;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#39640;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, online distillation has emerged as a powerful technique for adapting real-time deep neural networks on the fly using a slow, but accurate teacher model. However, a major challenge in online distillation is catastrophic forgetting when the domain shifts, which occurs when the student model is updated with data from the new domain and forgets previously learned knowledge. In this paper, we propose a solution to this issue by leveraging the power of continual learning methods to reduce the impact of domain shifts. Specifically, we integrate several state-of-the-art continual learning methods in the context of online distillation and demonstrate their effectiveness in reducing catastrophic forgetting. Furthermore, we provide a detailed analysis of our proposed solution in the case of cyclic domain shifts. Our experimental results demonstrate the efficacy of our approach in improving the robustness and accuracy of online distillation, with potential applications in domains 
&lt;/p&gt;</description></item><item><title>ADMG&#22240;&#26524;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#33021;&#22815;&#22312;&#24050;&#30693;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22914;&#26524;&#19981;&#21512;&#36866;&#22320;&#20351;&#29992;&#65292;&#23427;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.01237</link><description>&lt;p&gt;
ADMG&#22240;&#26524;&#25968;&#25454;&#22686;&#24378;&#30340;&#23454;&#38469;&#24212;&#29992;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Guide for Practical Use of ADMG Causal Data Augmentation. (arXiv:2304.01237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01237
&lt;/p&gt;
&lt;p&gt;
ADMG&#22240;&#26524;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#33021;&#22815;&#22312;&#24050;&#30693;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22914;&#26524;&#19981;&#21512;&#36866;&#22320;&#20351;&#29992;&#65292;&#23427;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#25968;&#25454;&#29615;&#22659;&#19979;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26102;&#65292;&#25968;&#25454;&#22686;&#24378;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#23427;&#21487;&#20197;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#36981;&#24490;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#21516;&#26102;&#22686;&#21152;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#25913;&#21892;&#20182;&#20204;&#30340;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#23558;&#20854;&#37096;&#32626;&#21040;&#23454;&#38469;&#19990;&#30028;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#20351;&#29992;&#23427;&#20173;&#26377;&#24453;&#25913;&#36827;&#65292;&#22240;&#20026;&#24456;&#23569;&#32771;&#34385;&#21040;&#24213;&#23618;&#25968;&#25454;&#26426;&#21046;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#22240;&#26524;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#20381;&#38752;&#22240;&#26524;&#22270;&#20013;&#32534;&#30721;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;ADMG&#22240;&#26524;&#22686;&#24378;&#26041;&#27861;&#24182;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#20197;&#25903;&#25345;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#20102;&#35299;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#20808;&#39564;&#30693;&#35782;&#26377;&#21161;&#20110;&#29983;&#25104;&#26032;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#22240;&#27492;&#22686;&#24378;&#20182;&#20204;&#30340;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;ADMG&#30340;&#22240;&#26524;&#25968;&#25454;&#22686;&#24378;&#30830;&#23454;&#21487;&#20197;&#22312;&#24050;&#30693;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22914;&#26524;&#19981;&#21512;&#36866;&#22320;&#20351;&#29992;&#65292;&#23427;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is essential when applying Machine Learning in small-data regimes. It generates new samples following the observed data distribution while increasing their diversity and variability to help researchers and practitioners improve their models' robustness and, thus, deploy them in the real world. Nevertheless, its usage in tabular data still needs to be improved, as prior knowledge about the underlying data mechanism is seldom considered, limiting the fidelity and diversity of the generated data. Causal data augmentation strategies have been pointed out as a solution to handle these challenges by relying on conditional independence encoded in a causal graph. In this context, this paper experimentally analyzed the ADMG causal augmentation method considering different settings to support researchers and practitioners in understanding under which conditions prior knowledge helps generate new data points and, consequently, enhances the robustness of their models. The results
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;ConvEntion&#65292;&#29992;&#20110;&#23545;&#22825;&#20307;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#24182;&#33021;&#22815;&#30452;&#25509;&#20351;&#29992;&#22270;&#20687;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#31354;&#38388;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.01236</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#27880;&#24847;&#21147;&#65288;ConvEntion&#65289;&#23545;&#22825;&#20307;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Astronomical image time series classification using CONVolutional attENTION (ConvEntion). (arXiv:2304.01236v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;ConvEntion&#65292;&#29992;&#20110;&#23545;&#22825;&#20307;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#24182;&#33021;&#22815;&#30452;&#25509;&#20351;&#29992;&#22270;&#20687;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#31354;&#38388;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22825;&#20307;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#30340;&#22788;&#29702;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20107;&#23454;&#19978;&#65292;&#35768;&#22810;&#36319;&#36394;&#30636;&#24577;&#22825;&#20307;&#30340;&#35843;&#26597;&#27491;&#22312;&#36827;&#34892;&#25110;&#24314;&#35774;&#20013;&#65292;&#20363;&#22914; Vera Rubin Observatory Legacy Survey for Space and Time (LSST)&#65292;&#35813;&#30636;&#21464;&#20107;&#20214;&#35266;&#27979;&#35745;&#21010;&#23558;&#20135;&#29983;&#22823;&#37327;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#30456;&#20851;&#30340;&#31185;&#23398;&#20027;&#39064;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#25105;&#20204;&#38134;&#27827;&#31995;&#20013;&#30340;&#29289;&#20307;&#30740;&#31350;&#21040;&#35266;&#27979;&#27979;&#37327;&#23431;&#23449;&#33192;&#32960;&#30340;&#26368;&#36828;&#30340;&#36229;&#26032;&#26143;&#12290;&#30001;&#20110;&#26377;&#22914;&#27492;&#22823;&#37327;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#38656;&#35201;&#31283;&#20581;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#31867;&#22825;&#20307;&#29289;&#20307;&#30340;&#24037;&#20855;&#30340;&#38656;&#27714;&#27491;&#22312;&#31283;&#27493;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#22825;&#20307;&#22270;&#20687;&#21253;&#21547;&#27604;&#20809;&#26354;&#32447;&#26356;&#22810;&#30340;&#20449;&#24687;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#30452;&#25509;&#20351;&#29992;&#22270;&#20687;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#31354;&#38388;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#21629;&#21517;&#20026;ConvEntion&#65292;&#21363;&#21367;&#31215;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aims. The treatment of astronomical image time series has won increasing attention in recent years. Indeed, numerous surveys following up on transient objects are in progress or under construction, such as the Vera Rubin Observatory Legacy Survey for Space and Time (LSST), which is poised to produce huge amounts of these time series. The associated scientific topics are extensive, ranging from the study of objects in our galaxy to the observation of the most distant supernovae for measuring the expansion of the universe. With such a large amount of data available, the need for robust automatic tools to detect and classify celestial objects is growing steadily. Methods. This study is based on the assumption that astronomical images contain more information than light curves. In this paper, we propose a novel approach based on deep learning for classifying different types of space objects directly using images. We named our approach ConvEntion, which stands for CONVolutional attENTION. I
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#36866;&#29992;&#20110;GMNN&#30340;&#26032;&#27979;&#35797;&#26041;&#27861;&#65292;&#23545;&#19977;&#31867;&#19981;&#21516;&#20449;&#24687;&#28304;&#23545;GMNN&#22312;WikiVitals&#25968;&#25454;&#38598;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#26631;&#31614;&#30456;&#20851;&#24615;&#26159;&#24110;&#21161;GMNN&#33719;&#24471;&#20248;&#21183;&#30340;&#20851;&#38190;&#20449;&#24687;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.01235</link><description>&lt;p&gt;
&#22270;&#39532;&#23572;&#21487;&#22827;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fair Evaluation of Graph Markov Neural Networks. (arXiv:2304.01235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#36866;&#29992;&#20110;GMNN&#30340;&#26032;&#27979;&#35797;&#26041;&#27861;&#65292;&#23545;&#19977;&#31867;&#19981;&#21516;&#20449;&#24687;&#28304;&#23545;GMNN&#22312;WikiVitals&#25968;&#25454;&#38598;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#26631;&#31614;&#30456;&#20851;&#24615;&#26159;&#24110;&#21161;GMNN&#33719;&#24471;&#20248;&#21183;&#30340;&#20851;&#38190;&#20449;&#24687;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#37319;&#29992;&#22270;&#39532;&#23572;&#21487;&#22827;&#31070;&#32463;&#32593;&#32476;&#65288;GMNN&#65289;&#25913;&#36827;&#24120;&#35268;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#23558;&#26631;&#31614;&#20381;&#36182;&#24615;&#32435;&#20837;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;GMNN&#20174;&#29702;&#35770;&#19978;&#20197;&#20005;&#35880;&#30340;&#26041;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#19977;&#31867;&#20449;&#24687;&#26469;&#39044;&#27979;&#26631;&#31614;&#12290;&#19982;&#24120;&#35268;&#30340;GNN&#19968;&#26679;&#65292;&#20182;&#20204;&#20351;&#29992;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#65292;&#20294;&#20182;&#20204;&#36824;&#21033;&#29992;&#30456;&#37051;&#33410;&#28857;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;WikiVitals&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;48k&#20010;&#20114;&#30456;&#24341;&#29992;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#65292;&#34987;&#20998;&#31867;&#20026;32&#20010;&#31867;&#21035;&#65292;&#30001;2.3M&#36793;&#36830;&#25509;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23545;GMNN&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#36129;&#29486;&#30340;&#19977;&#31181;&#19981;&#21516;&#20449;&#24687;&#28304;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65306;&#25991;&#31456;&#20869;&#23481;&#12289;&#25991;&#31456;&#20114;&#30456;&#20043;&#38388;&#30340;&#36830;&#25509;&#20197;&#21450;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#36866;&#29992;&#20110;GNN&#30340;&#26679;&#26412;&#22806;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#20110;GMNN&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GMNN&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;GNN&#65292;&#24182;&#19988;&#26631;&#31614;&#30456;&#20851;&#24615;&#26159;&#24110;&#21161;GMNN&#23454;&#29616;&#36825;&#20123;&#22686;&#30410;&#30340;&#20851;&#38190;&#20449;&#24687;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Markov Neural Networks (GMNN) have recently been proposed to improve regular graph neural networks (GNN) by including label dependencies into the semi-supervised node classification task. GMNNs do this in a theoretically principled way and use three kinds of information to predict labels. Just like ordinary GNNs, they use the node features and the graph structure but they moreover leverage information from the labels of neighboring nodes to improve the accuracy of their predictions. In this paper, we introduce a new dataset named WikiVitals which contains a graph of 48k mutually referred Wikipedia articles classified into 32 categories and connected by 2.3M edges. Our aim is to rigorously evaluate the contributions of three distinct sources of information to the prediction accuracy of GMNN for this dataset: the content of the articles, their connections with each other and the correlations among their labels. For this purpose we adapt a method which was recently proposed for perf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28508;&#22312;&#22330;&#28304;&#34920;&#38754;&#30913;&#22270;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#39044;&#27979;&#22826;&#38451;&#39118;&#36895;&#24230;&#65292;&#24182;&#19988;&#22312;&#36830;&#32493;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#30456;&#20851;&#31995;&#25968;&#20026;0.52&#12290;</title><link>http://arxiv.org/abs/2304.01234</link><description>&lt;p&gt;
&#24212;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#28508;&#22312;&#22330;&#28304;&#38754;&#30913;&#22270;&#39044;&#27979;&#22826;&#38451;&#39118;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Prediction of solar wind speed by applying convolutional neural network to potential field source surface (PFSS) magnetograms. (arXiv:2304.01234v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28508;&#22312;&#22330;&#28304;&#34920;&#38754;&#30913;&#22270;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#39044;&#27979;&#22826;&#38451;&#39118;&#36895;&#24230;&#65292;&#24182;&#19988;&#22312;&#36830;&#32493;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#30456;&#20851;&#31995;&#25968;&#20026;0.52&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22826;&#38451;&#39118;&#36895;&#27169;&#22411;&#23545;&#20110;&#22826;&#31354;&#22825;&#27668;&#39044;&#27979;&#12289;&#28798;&#38590;&#24615;&#20107;&#20214;&#35686;&#21578;&#21644;&#20854;&#20182;&#19982;&#22826;&#38451;&#39118;-&#30913;&#23618;&#30456;&#20114;&#20316;&#29992;&#26377;&#20851;&#30340;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#28508;&#22312;&#22330;&#28304;&#34920;&#38754;&#65288;PFSS&#65289;&#30913;&#22270;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#32771;&#34385;&#22826;&#38451;&#39118;&#28304;&#34920;&#38754;$R_{\rm SS}=2.5R_\odot$&#65292;&#26088;&#22312;&#39044;&#27979;&#22826;&#38451;-&#22320;&#29699;&#31995;&#32479;&#30340;Lagrange 1&#65288;L1&#65289;&#28857;&#30340;&#22826;&#38451;&#39118;&#36895;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36755;&#20837;&#30001;$R_{\rm SS}$&#22788;&#30340;&#22235;&#24352;PFSS&#30913;&#22270;&#32452;&#25104;&#65292;&#36825;&#20123;&#30913;&#22270;&#36317;&#30446;&#26631;&#26102;&#21051;&#20998;&#21035;&#20026;7&#12289;6&#12289;5&#21644;4&#22825;&#12290;&#20026;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24402;&#19968;&#21270;&#30913;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#20840;&#29699;&#25391;&#33633;&#32593;&#32476;&#32452;&#65288;GONG&#65289;&#20809;&#29699;&#30913;&#22270;&#21644;&#28508;&#22312;&#22330;&#22806;&#25512;&#27169;&#22411;&#29983;&#25104;PFSS&#30913;&#22270;&#12290;&#35813;&#27169;&#22411;&#23545;&#36830;&#32493;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#24179;&#22343;&#30456;&#20851;&#31995;&#25968;&#65288;CC&#65289;&#20026;0.52&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#20026;...
&lt;/p&gt;
&lt;p&gt;
An accurate solar wind speed model is important for space weather predictions, catastrophic event warnings, and other issues concerning solar wind - magnetosphere interaction. In this work, we construct a model based on convolutional neural network (CNN) and Potential Field Source Surface (PFSS) magnetograms, considering a solar wind source surface of $R_{\rm SS}=2.5R_\odot$, aiming to predict the solar wind speed at the Lagrange 1 (L1) point of the Sun-Earth system. The input of our model consists of four Potential Field Source Surface (PFSS) magnetograms at $R_{\rm SS}$, which are 7, 6, 5, and 4 days before the target epoch. Reduced magnetograms are used to promote the model's efficiency. We use the Global Oscillation Network Group (GONG) photospheric magnetograms and the potential field extrapolation model to generate PFSS magnetograms at the source surface. The model provides predictions of the continuous test dataset with an averaged correlation coefficient (CC) of 0.52 and a root
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#30693;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26524;&#39044;&#27979;&#21644;&#24739;&#32773;&#20998;&#35786;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24613;&#35786;&#31185;&#20020;&#24202;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01233</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#24613;&#35786;&#23460;&#32467;&#26524;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department. (arXiv:2304.01233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#30693;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26524;&#39044;&#27979;&#21644;&#24739;&#32773;&#20998;&#35786;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24613;&#35786;&#31185;&#20020;&#24202;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#24050;&#32463;&#22312;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#20934;&#30830;&#24615;&#21644;&#39640;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#29992;&#20110;&#29305;&#23450;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#19977;&#35282;&#27954;&#21306;&#22495;&#35760;&#24405;&#30340;&#39318;&#24109;&#25237;&#35785;&#20449;&#24687;&#21644;&#29983;&#21629;&#20307;&#24449;&#30340;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#20102;&#32467;&#26524;&#39044;&#27979;&#21644;&#24739;&#32773;&#20998;&#35786;&#12290;&#25105;&#20204;&#25913;&#32534;&#20102;Perceiver&#8212;&#8212;&#19968;&#31181;&#36890;&#29992;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#23427;&#24050;&#32463;&#22312;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#23637;&#29616;&#20986;&#21487;&#34892;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#29983;&#21629;&#20307;&#24449;&#27169;&#24577;&#20197;&#34920;&#26684;&#24418;&#24335;&#21576;&#29616;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;Perceiver&#30340;&#20301;&#32622;&#32534;&#30721;&#20197;&#30830;&#20445;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;MIMIC-IV ED&#25968;&#25454;&#38598;&#19978;&#30340;120K&#27425;&#35775;&#38382;&#26469;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#23545;&#35786;&#26029;&#20195;&#30721;&#39044;&#27979;&#30340;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#25913;&#21892;&#20102;&#39044;&#27979;&#34920;&#29616;&#65292;&#30456;&#27604;&#20110;&#21333;&#27169;&#24577;&#27169;&#22411;&#65292;&#24182;&#19988;Perceiver&#22312;&#20219;&#21153;&#20013;&#32988;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#25913;&#36827;&#24613;&#35786;&#31185;&#20020;&#24202;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language modeling have shown impressive progress in generating compelling text with good accuracy and high semantic coherence. An interesting research direction is to augment these powerful models for specific applications using contextual information. In this work, we explore multi-modal language modeling for healthcare applications. We are interested in outcome prediction and patient triage in hospital emergency department based on text information in chief complaints and vital signs recorded at triage. We adapt Perceiver - a modality-agnostic transformer-based model that has shown promising results in several applications. Since vital-sign modality is represented in tabular format, we modified Perceiver position encoding to ensure permutation invariance. We evaluated the multi-modal language model for the task of diagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In the experimental analysis, we show that mutli-modality improves the prediction performance compared w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEENN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#27493;&#25968;&#36827;&#34892;&#32454;&#31890;&#24230;&#35843;&#25972;&#65292;&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24182;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;SEENN&#36798;&#21040;&#20102;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24230;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01230</link><description>&lt;p&gt;
SEENN: &#23454;&#29616;&#26102;&#38388;&#32534;&#30721;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SEENN: Towards Temporal Spiking Early-Exit Neural Networks. (arXiv:2304.01230v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEENN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#27493;&#25968;&#36827;&#34892;&#32454;&#31890;&#24230;&#35843;&#25972;&#65292;&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24182;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;SEENN&#36798;&#21040;&#20102;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24230;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22240;&#20854;&#29983;&#29289;&#23398;&#29305;&#24615;&#25104;&#20026;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#26367;&#20195;&#21697;&#65292;&#26368;&#36817;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;SNNs&#26082;&#36153;&#29992;&#25928;&#30410;&#21448;&#26131;&#20110;&#37096;&#32626;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29992;&#20108;&#36827;&#21046;&#33033;&#20914;&#20197;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#24335;&#22788;&#29702;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SNNs&#20013;&#30340;&#20449;&#24687;&#23481;&#37327;&#21463;&#21040;&#26102;&#38388;&#27493;&#39588;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SNNs&#20013;&#26102;&#38388;&#27493;&#39588;&#25968;&#37327;&#30340;&#32454;&#31890;&#24230;&#35843;&#25972;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#27493;&#25968;&#35270;&#20026;&#19968;&#20010;&#21464;&#37327;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#36755;&#20837;&#26679;&#26412;&#26469;&#20943;&#23569;&#20887;&#20313;&#26102;&#38388;&#27493;&#39588;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#26089;&#26399;&#36864;&#20986;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SEENN&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#36866;&#24403;&#30340;&#26102;&#38388;&#27493;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEENN-I&#65292;&#23427;&#20351;&#29992;&#32622;&#20449;&#24230;&#38408;&#20540;&#26469;&#36807;&#28388;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#65292;&#20197;&#21450;SEENN-II&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30830;&#23450;&#26102;&#38388;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;SEENN&#27604;&#20256;&#32479;SNNs&#26356;&#26377;&#25928;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have recently become more popular as a biologically plausible substitute for traditional Artificial Neural Networks (ANNs). SNNs are cost-efficient and deployment-friendly because they process input in both spatial and temporal manners using binary spikes. However, we observe that the information capacity in SNNs is affected by the number of timesteps, leading to an accuracy-efficiency tradeoff. In this work, we study a fine-grained adjustment of the number of timesteps in SNNs. Specifically, we treat the number of timesteps as a variable conditioned on different input samples to reduce redundant timesteps for certain data. We call our method Spiking Early-Exit Neural Networks (SEENNs). To determine the appropriate number of timesteps, we propose SEENN-I which uses a confidence score thresholding to filter out the uncertain predictions, and SEENN-II which determines the number of timesteps by reinforcement learning. Moreover, we demonstrate that SEENN is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#22270;&#20687;&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#31639;&#23376;&#36817;&#20284;&#65292;&#24182;&#23558;CNNs&#36716;&#25442;&#20026;FNOs&#12290;</title><link>http://arxiv.org/abs/2304.01227</link><description>&lt;p&gt;
&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#20998;&#31867;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Resolution-Invariant Image Classification based on Fourier Neural Operators. (arXiv:2304.01227v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#22270;&#20687;&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#31639;&#23376;&#36817;&#20284;&#65292;&#24182;&#23558;CNNs&#36716;&#25442;&#20026;FNOs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNOs&#65289;&#30456;&#23545;&#20110;&#26631;&#20934;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#20248;&#21183;&#12290;&#31070;&#32463;&#31639;&#23376;&#26159;&#19968;&#31181;&#31163;&#25955;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#24191;&#65292;&#29992;&#20110;&#36817;&#20284;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#31639;&#23376;&#12290;FNOs&#26159;&#19968;&#31181;&#20855;&#26377;&#29305;&#23450;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#21442;&#25968;&#21270;PDEs&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#23558;FNO&#26550;&#26500;&#23548;&#20986;&#20026;Lebesgue&#31354;&#38388;&#19978;&#36830;&#32493;&#21644;Fr&#233;chet&#21487;&#24494;&#31070;&#32463;&#31639;&#23376;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;CNN&#36716;&#25442;&#20026;FNO&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#24182;&#25552;&#20986;&#20102;&#31561;&#20215;&#24046;&#20540;&#36866;&#24212;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate the use of Fourier Neural Operators (FNOs) for image classification in comparison to standard Convolutional Neural Networks (CNNs). Neural operators are a discretization-invariant generalization of neural networks to approximate operators between infinite dimensional function spaces. FNOs - which are neural operators with a specific parametrization have been applied successfully in the context of parametric PDEs. We derive the FNO architecture as an example for continuous and Fr\'echet-differentiable neural operators on Lebesgue spaces. We further show how CNNs can be converted into FNOs and vice versa and propose an interpolation-equivariant adaptation of the architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23436;&#20840;&#25429;&#25417;&#24322;&#24120;&#20107;&#20214;&#27169;&#24335;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01226</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Abnormal Event Detection via Hypergraph Contrastive Learning. (arXiv:2304.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23436;&#20840;&#25429;&#25417;&#24322;&#24120;&#20107;&#20214;&#27169;&#24335;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#23427;&#25351;&#30340;&#26159;&#25366;&#25496;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#19981;&#23547;&#24120;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290; &#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#23558;&#27492;&#20219;&#21153;&#31616;&#21270;&#20026;&#26816;&#27979;&#24322;&#24120;&#30340;&#25104;&#23545;&#20132;&#20114;&#20316;&#29992;&#12290; &#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#20214;&#21487;&#33021;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#30340;&#23646;&#24615;&#23454;&#20307;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#24418;&#25104;&#20102;&#23646;&#24615;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#12290;&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#23646;&#24615;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20294;&#24456;&#23569;&#34987;&#25506;&#32034;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abnormal event detection, which refers to mining unusual interactions among involved entities, plays an important role in many real applications. Previous works mostly over-simplify this task as detecting abnormal pair-wise interactions. However, real-world events may contain multi-typed attributed entities and complex interactions among them, which forms an Attributed Heterogeneous Information Network (AHIN). With the boom of social networks, abnormal event detection in AHIN has become an important, but seldom explored task. In this paper, we firstly study the unsupervised abnormal event detection problem in AHIN. The events are considered as star-schema instances of AHIN and are further modeled by hypergraphs. A novel hypergraph contrastive learning method, named AEHCL, is proposed to fully capture abnormal event patterns. AEHCL designs the intra-event and inter-event contrastive modules to exploit self-supervised AHIN information. The intra-event contrastive module captures the pair
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; "STI-KNN" &#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30701;&#26102;&#38388;&#20869;&#23545; KNN &#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#30340;&#37197;&#23545;&#20132;&#20114; Shapley &#20540;&#35745;&#31639;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35780;&#20272;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#20215;&#20540;&#65292;&#25552;&#39640;&#35757;&#32451;&#32467;&#26524;&#21644;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01224</link><description>&lt;p&gt;
&#20248;&#21270; KNN &#27169;&#22411;&#30340; Shapley Interaction &#35745;&#31639;&#20174; O(2^n) &#21040; O(t n^2)&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing Data Shapley Interaction Calculation from O(2^n) to O(t n^2) for KNN models. (arXiv:2304.01224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; "STI-KNN" &#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30701;&#26102;&#38388;&#20869;&#23545; KNN &#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#30340;&#37197;&#23545;&#20132;&#20114; Shapley &#20540;&#35745;&#31639;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35780;&#20272;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#20215;&#20540;&#65292;&#25552;&#39640;&#35757;&#32451;&#32467;&#26524;&#21644;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#20351;&#29992;&#29575;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#37327;&#21270;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#38468;&#21152;&#20215;&#20540;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20851;&#38190;&#30340;&#36807;&#31243;&#12290;Shapley &#20540;&#24050;&#34987;&#20844;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#20351;&#24471;&#35757;&#32451;&#38598;&#27719;&#24635;&#12289;&#33719;&#21462;&#21644;&#24322;&#24120;&#20540;&#21024;&#38500;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#31639;&#27861; "STI-KNN"&#65292;&#23427;&#21487;&#20197;&#22312;O(t n^2)&#26102;&#38388;&#20869;&#35745;&#31639;&#20934;&#30830;&#30340; KNN &#27169;&#22411;&#30340;&#31934;&#30830;&#37197;&#23545;&#20132;&#20114; Shapley &#20540;&#65292;&#36825;&#26159;&#27604;&#22522;&#32447;&#26041;&#27861;&#30340; O(2^n) &#26102;&#38388;&#22797;&#26434;&#24230;&#26174;&#33879;&#25552;&#39640;&#20102;&#12290;&#20351;&#29992; STI-KNN&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#35780;&#20272;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#20215;&#20540;&#65292;&#20174;&#32780;&#25913;&#21892;&#35757;&#32451;&#32467;&#26524;&#65292;&#26368;&#32456;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of data availability and usage, quantifying the added value of each training data point has become a crucial process in the field of artificial intelligence. The Shapley values have been recognized as an effective method for data valuation, enabling efficient training set summarization, acquisition, and outlier removal. In this paper, we introduce "STI-KNN", an innovative algorithm that calculates the exact pair-interaction Shapley values for KNN models in O(t n^2) time, which is a significant improvement over the O(2^n)$ time complexity of baseline methods. By using STI-KNN, we can efficiently and accurately evaluate the value of individual data points, leading to improved training outcomes and ultimately enhancing the effectiveness of artificial intelligence applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#38598;&#20013;&#24335;&#35757;&#32451;&#20998;&#24067;&#24335;&#25191;&#34892;&#26694;&#26550;&#30340;&#22810;&#24494;&#30005;&#32593;&#21327;&#21516;&#20248;&#21270;&#35843;&#24230;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25913;&#36827;&#30340;MASAC&#31639;&#27861;&#23545;&#33021;&#28304;&#31649;&#29702;&#38382;&#39064;&#36827;&#34892;&#22788;&#29702;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#19981;&#21516;&#23454;&#20307;&#20043;&#38388;&#30340;&#21151;&#29575;&#20114;&#34917;&#21644;&#38477;&#20302;&#31995;&#32479;&#36816;&#34892;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.01223</link><description>&lt;p&gt;
&#22522;&#20110;&#25913;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#36719;actor-critic&#31639;&#27861;&#30340;&#22810;&#24494;&#30005;&#32593;&#21327;&#21516;&#20248;&#21270;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Multi-Microgrid Collaborative Optimization Scheduling Using an Improved Multi-Agent Soft Actor-Critic Algorithm. (arXiv:2304.01223v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#38598;&#20013;&#24335;&#35757;&#32451;&#20998;&#24067;&#24335;&#25191;&#34892;&#26694;&#26550;&#30340;&#22810;&#24494;&#30005;&#32593;&#21327;&#21516;&#20248;&#21270;&#35843;&#24230;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25913;&#36827;&#30340;MASAC&#31639;&#27861;&#23545;&#33021;&#28304;&#31649;&#29702;&#38382;&#39064;&#36827;&#34892;&#22788;&#29702;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#19981;&#21516;&#23454;&#20307;&#20043;&#38388;&#30340;&#21151;&#29575;&#20114;&#34917;&#21644;&#38477;&#20302;&#31995;&#32479;&#36816;&#34892;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24494;&#30005;&#32593;&#31995;&#32479;&#23454;&#29616;&#20102;&#22810;&#31181;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20114;&#34917;&#20849;&#23384;&#65292;&#20419;&#36827;&#20102;&#30005;&#21147;&#20132;&#26131;&#12290;&#26412;&#25991;&#38024;&#23545;&#30001;&#19981;&#21516;&#36816;&#33829;&#23454;&#20307;&#25317;&#26377;&#30340;&#22810;&#20010;&#21487;&#20877;&#29983;&#33021;&#28304;&#24494;&#30005;&#32593;&#32452;&#25104;&#30340;&#22810;&#24494;&#30005;&#32593;&#31995;&#32479;&#30340;&#33021;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#38598;&#20013;&#24335;&#35757;&#32451;&#20998;&#24067;&#24335;&#25191;&#34892;&#26694;&#26550;&#30340;&#22810;&#24494;&#30005;&#32593;&#21327;&#21516;&#20248;&#21270;&#35843;&#24230;&#27169;&#22411;&#12290;&#20026;&#20102;&#22686;&#24378;&#22788;&#29702;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#36719;actor-critic&#65288;MASAC&#65289;&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#23454;&#29616;&#33021;&#37327;&#20132;&#26131;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26469;&#20248;&#21270;MASAC&#36229;&#21442;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#23454;&#29616;&#20102;&#19981;&#21516;&#23454;&#20307;&#20043;&#38388;&#30340;&#21151;&#29575;&#20114;&#34917;&#65292;&#38477;&#20302;&#20102;&#22810;&#24494;&#30005;&#32593;&#31995;&#32479;&#30340;&#36816;&#34892;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The implementation of a multi-microgrid (MMG) system with multiple renewable energy sources enables the facilitation of electricity trading. To tackle the energy management problem of a MMG system, which consists of multiple renewable energy microgrids belonging to different operating entities, this paper proposes a MMG collaborative optimization scheduling model based on a multi-agent centralized training distributed execution framework. To enhance the generalization ability of dealing with various uncertainties, we also propose an improved multi-agent soft actor-critic (MASAC) algorithm, which facilitates en-ergy transactions between multi-agents in MMG, and employs automated machine learning (AutoML) to optimize the MASAC hyperparameters to further improve the generalization of deep reinforcement learning (DRL). The test results demonstrate that the proposed method successfully achieves power complementarity between different entities, and reduces the MMG system operating cost. Addi
&lt;/p&gt;</description></item><item><title>NeuroDAVIS&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#25968;&#25454;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#24182;&#22312;&#26356;&#20302;&#30340;&#32500;&#24230;&#19978;&#36827;&#34892;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.01222</link><description>&lt;p&gt;
NeuroDAVIS: &#29992;&#20110;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeuroDAVIS: A neural network model for data visualization. (arXiv:2304.01222v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01222
&lt;/p&gt;
&lt;p&gt;
NeuroDAVIS&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#25968;&#25454;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#24182;&#22312;&#26356;&#20302;&#30340;&#32500;&#24230;&#19978;&#36827;&#34892;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#38477;&#32500;&#21644;&#21487;&#35270;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#20195;&#39640;&#36890;&#37327;&#25216;&#26415;&#20135;&#29983;&#20102;&#22810;&#31181;&#35270;&#22270;&#30340;&#26032;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26032;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#21487;&#35270;&#21270;&#38656;&#35201;&#36866;&#24403;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#20010;&#20219;&#21153;&#30340;&#26041;&#27861;&#38750;&#24120;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;NeuroDAVIS&#65292;&#29992;&#20110;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;NeuroDAVIS&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#20551;&#35774;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#26356;&#20302;&#30340;&#32500;&#24230;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#21487;&#35270;&#21270;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#39640;&#32500;&#25968;&#25454;&#30340;&#37051;&#36817;&#20851;&#31995;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#24471;&#21040;&#20102;&#20445;&#30041;&#12290;NeuroDAVIS&#30340;&#24615;&#33021;&#24050;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of dimensionality reduction and visualization of high-dimensional datasets remains a challenging problem since long. Modern high-throughput technologies produce newer high-dimensional datasets having multiple views with relatively new data types. Visualization of these datasets require proper methodology that can uncover hidden patterns in the data without affecting the local and global structures within the data. To this end, however, very few such methodology exist, which can realise this task. In this work, we have introduced a novel unsupervised deep neural network model, called NeuroDAVIS, for data visualization. NeuroDAVIS is capable of extracting important features from the data, without assuming any data distribution, and visualize effectively in lower dimension. It has been shown theoritically that neighbourhood relationship of the data in high dimension remains preserved in lower dimension. The performance of NeuroDAVIS has been evaluated on a wide variety of synthet
&lt;/p&gt;</description></item><item><title>DoE2Vec &#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20219;&#20309;&#23454;&#39564;&#35774;&#35745;&#65288;DoE&#65289;&#30340;&#20449;&#24687;&#28508;&#22312;&#34920;&#36798;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#20248;&#21270;&#26223;&#35266;&#29305;&#24449;&#30340;&#19979;&#28216;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32463;&#20856;ELA&#20998;&#26512;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#19982;&#32463;&#20856;ELA&#29305;&#24449;&#20114;&#34917;&#20351;&#29992;&#26102;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01219</link><description>&lt;p&gt;
DoE2Vec&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#29992;&#20110;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DoE2Vec: Deep-learning Based Features for Exploratory Landscape Analysis. (arXiv:2304.01219v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01219
&lt;/p&gt;
&lt;p&gt;
DoE2Vec &#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20219;&#20309;&#23454;&#39564;&#35774;&#35745;&#65288;DoE&#65289;&#30340;&#20449;&#24687;&#28508;&#22312;&#34920;&#36798;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#20248;&#21270;&#26223;&#35266;&#29305;&#24449;&#30340;&#19979;&#28216;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32463;&#20856;ELA&#20998;&#26512;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#19982;&#32463;&#20856;ELA&#29305;&#24449;&#20114;&#34917;&#20351;&#29992;&#26102;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;DoE2Vec&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20248;&#21270;&#26223;&#35266;&#29305;&#24449;&#65292;&#20197;&#29992;&#20110;&#19979;&#28216;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#33258;&#21160;&#36873;&#25321;&#20248;&#21270;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#20989;&#25968;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;DoE2Vec&#21487;&#20197;&#33258;&#23398;&#20064;&#20219;&#20309;&#23454;&#39564;&#35774;&#35745;&#65288;DoE&#65289;&#30340;&#20449;&#24687;&#28508;&#22312;&#34920;&#36798;&#12290;&#19982;&#32463;&#20856;&#30340;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65288;ELA&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#24449;&#24037;&#31243;&#65292;&#24182;&#19988;&#26131;&#20110;&#24212;&#29992;&#20110;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#12290;&#20026;&#20102;&#39564;&#35777;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#28508;&#22312;&#37325;&#24314;&#30340;&#36136;&#37327;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#28508;&#22312;&#34920;&#36798;&#24335;&#12290;&#36825;&#20123;&#28508;&#22312;&#34920;&#36798;&#24335;&#19981;&#20165;&#22312;&#35782;&#21035;&#31867;&#20284;&#65288;&#26131;&#20110;&#35780;&#20272;&#65289;&#30340;&#26367;&#20195;&#20989;&#25968;&#19978;&#26174;&#31034;&#20986;&#26377;&#21069;&#36884;&#30340;&#28508;&#21147;&#65292;&#32780;&#19988;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#19982;&#32463;&#20856;&#30340;ELA&#29305;&#24449;&#20114;&#34917;&#20351;&#29992;&#26102;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose DoE2Vec, a variational autoencoder (VAE)-based methodology to learn optimization landscape characteristics for downstream meta-learning tasks, e.g., automated selection of optimization algorithms. Principally, using large training data sets generated with a random function generator, DoE2Vec self-learns an informative latent representation for any design of experiments (DoE). Unlike the classical exploratory landscape analysis (ELA) method, our approach does not require any feature engineering and is easily applicable for high dimensional search spaces. For validation, we inspect the quality of latent reconstructions and analyze the latent representations using different experiments. The latent representations not only show promising potentials in identifying similar (cheap-to-evaluate) surrogate functions, but also can significantly boost performances when being used complementary to the classical ELA features in classification tasks.
&lt;/p&gt;</description></item><item><title>POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01218</link><description>&lt;p&gt;
POLAR-Express: &#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#39640;&#25928;&#20934;&#30830;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems. (arXiv:2304.01218v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01218
&lt;/p&gt;
&lt;p&gt;
POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#38382;&#39064;&#19978;&#65292;&#25198;&#28436;&#25511;&#21046;&#22120;&#35282;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476; (NN) &#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23454;&#39564;&#24615;&#33021;&#12290;&#20294;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479; (NNCS) &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#37319;&#29992;&#20063;&#24341;&#36215;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#23545;&#36825;&#20123; NNCS &#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; POLAR-Express&#65292;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777; NNCS &#30340;&#23433;&#20840;&#24615;&#12290;POLAR-Express &#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#65292;&#36880;&#23618;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#26469;&#20256;&#25773; Taylor &#27169;&#22411; (TM) &#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#20219;&#20309;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#26356;&#26377;&#25928;&#22320;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;POLAR-Express &#20026;&#36880;&#23618;&#20256;&#25773;&#25552;&#20379;&#20102;&#24182;&#34892;&#35745;&#31639;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) playing the role of controllers have demonstrated impressive empirical performances on challenging control problems. However, the potential adoption of NN controllers in real-life applications also gives rise to a growing concern over the safety of these neural-network controlled systems (NNCSs), especially when used in safety-critical applications. In this work, we present POLAR-Express, an efficient and precise formal reachability analysis tool for verifying the safety of NNCSs. POLAR-Express uses Taylor model arithmetic to propagate Taylor models (TMs) across a neural network layer-by-layer to compute an overapproximation of the neural-network function. It can be applied to analyze any feed-forward neural network with continuous activation functions. We also present a novel approach to propagate TMs more efficiently and precisely across ReLU activation functions. In addition, POLAR-Express provides parallel computation support for the layer-by-layer propagation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26641;&#27169;&#22411;&#26041;&#27861;&#39044;&#27979;&#24847;&#22823;&#21033;&#34562;&#31665;&#30340;&#34588;&#34562;&#29983;&#20135;&#37327;&#21464;&#21270;&#65292;&#24110;&#21161;&#34588;&#34562;&#20859;&#27542;&#32773;&#35780;&#20272;&#39118;&#38505;&#65292;&#20445;&#25252;&#34588;&#34562;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.01215</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#34588;&#34562;&#29983;&#20135;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods. (arXiv:2304.01215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01215
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26641;&#27169;&#22411;&#26041;&#27861;&#39044;&#27979;&#24847;&#22823;&#21033;&#34562;&#31665;&#30340;&#34588;&#34562;&#29983;&#20135;&#37327;&#21464;&#21270;&#65292;&#24110;&#21161;&#34588;&#34562;&#20859;&#27542;&#32773;&#35780;&#20272;&#39118;&#38505;&#65292;&#20445;&#25252;&#34588;&#34562;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#34562;&#20859;&#27542;&#19994;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#32463;&#21382;&#20102;&#21487;&#35266;&#30340;&#29983;&#20135;&#21464;&#21270;&#65292;&#36825;&#26159;&#30001;&#20110;&#36880;&#28176;&#24694;&#21270;&#30340;&#27668;&#20505;&#26465;&#20214;&#36896;&#25104;&#30340;&#12290;&#36825;&#20123;&#29616;&#35937;&#21487;&#33021;&#20250;&#23545;&#34588;&#34562;&#27963;&#21160;&#19981;&#21033;&#12290;&#25105;&#20204;&#20351;&#29992;&#26641;&#27169;&#22411;&#26041;&#27861;&#21306;&#20998;&#34588;&#34562;&#29983;&#20135;&#37327;&#30340;&#24433;&#21709;&#22240;&#32032;&#65292;&#24182;&#39044;&#27979;&#24847;&#22823;&#21033;&#34562;&#31665;&#30340;&#34588;&#34562;&#29983;&#20135;&#37327;&#21464;&#21270;&#65292;&#24847;&#22823;&#21033;&#26159;&#27431;&#27954;&#26368;&#22823;&#30340;&#34562;&#34588;&#29983;&#20135;&#22269;&#20043;&#19968;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547;&#20102;&#20174;2019&#24180;&#21040;2022&#24180;&#25910;&#38598;&#30340;&#25968;&#30334;&#20010;&#34562;&#31665;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#31934;&#24230;&#34588;&#34562;&#20859;&#27542;&#25216;&#26415;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#26377;&#25351;&#23548;&#24615;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#24615;&#12290;&#19982;&#26631;&#20934;&#32447;&#24615;&#25216;&#26415;&#30456;&#27604;&#65292;&#26641;&#27169;&#22411;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#25252;&#34588;&#34562;&#27963;&#21160;&#24182;&#35780;&#20272;&#39118;&#38505;&#31649;&#29702;&#20013;&#34588;&#34562;&#20859;&#27542;&#32773;&#30340;&#28508;&#22312;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
The beekeeping sector has undergone considerable production variations over the past years due to adverse weather conditions, occurring more frequently as climate change progresses. These phenomena can be high-impact and cause the environment to be unfavorable to the bees' activity. We disentangle the honey production drivers with tree-based methods and predict honey production variations for hives in Italy, one of the largest honey producers in Europe. The database covers hundreds of beehive data from 2019-2022 gathered with advanced precision beekeeping techniques. We train and interpret the machine learning models making them prescriptive other than just predictive. Superior predictive performances of tree-based methods compared to standard linear techniques allow for better protection of bees' activity and assess potential losses for beekeepers for risk management.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01209</link><description>&lt;p&gt;
PromptORE -- &#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction. (arXiv:2304.01209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#36825;&#23545;&#20110;&#27809;&#26377;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#29305;&#23450;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#21644;&#20808;&#39564;&#26410;&#30693;&#20851;&#31995;&#31867;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#29305;&#21035;&#30456;&#20851;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#36229;&#21442;&#25968;&#65292;&#35843;&#25972;&#36825;&#20123;&#36229;&#21442;&#25968;&#36890;&#24120;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#36229;&#21442;&#25968;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptORE&#65292;&#21363;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#33539;&#20363;&#36866;&#24212;&#20110;&#26080;&#30417;&#30563;&#35774;&#32622;&#65292;&#24182;&#29992;&#23427;&#26469;&#23884;&#20837;&#34920;&#36798;&#20851;&#31995;&#30340;&#21477;&#23376;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#65292;&#21457;&#29616;&#20505;&#36873;&#20851;&#31995;&#65292;&#24182;&#23581;&#35797;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#33258;&#21160;&#20272;&#35745;&#36866;&#24403;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;PromptORE&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Relation Extraction (RE) aims to identify relations between entities in text, without having access to labeled data during training. This setting is particularly relevant for domain specific RE where no annotated dataset is available and for open-domain RE where the types of relations are a priori unknown. Although recent approaches achieve promising results, they heavily depend on hyperparameters whose tuning would most often require labeled data. To mitigate the reliance on hyperparameters, we propose PromptORE, a ''Prompt-based Open Relation Extraction'' model. We adapt the novel prompt-tuning paradigm to work in an unsupervised setting, and use it to embed sentences expressing a relation. We then cluster these embeddings to discover candidate relations, and we experiment different strategies to automatically estimate an adequate number of clusters. To the best of our knowledge, PromptORE is the first unsupervised RE model that does not need hyperparameter tuning. Resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#27431;&#25289;&#26041;&#31243;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#26159;&#19968;&#20010;&#26368;&#20248;&#36136;&#37327;&#36755;&#36816;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#38382;&#39064;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#29256;&#26412;&#30340;Eulerian OMT&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.00595</link><description>&lt;p&gt;
Euler Equation&#19978;&#30340;&#26368;&#20248;&#36136;&#37327;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Mass Transport over the Euler Equation. (arXiv:2304.00595v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#27431;&#25289;&#26041;&#31243;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#26159;&#19968;&#20010;&#26368;&#20248;&#36136;&#37327;&#36755;&#36816;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#38382;&#39064;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#29256;&#26412;&#30340;Eulerian OMT&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#26368;&#20339;&#25805;&#32437;&#38382;&#39064;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#30001;&#27431;&#25289;&#26041;&#31243;&#25511;&#21046;&#30340;&#32852;&#21512;&#29366;&#24577;&#27010;&#29575;&#20998;&#24067;&#30340;&#20248;&#21270;&#20256;&#36882;&#12290;&#35813;&#38382;&#39064;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#30456;&#24403;&#20110;&#36890;&#36807;&#21453;&#39304;&#25511;&#21046;&#21018;&#20307;&#30340;&#26059;&#36716;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#22312;&#20855;&#26377;&#38543;&#26426;&#21021;&#22987;&#21644;&#32456;&#31471;&#29366;&#24577;&#30340;&#33322;&#22825;&#22120;&#30340;&#35282;&#31283;&#23450;&#20013;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;&#20855;&#26377;&#21452;&#32447;&#24615;&#20808;&#39564;&#28418;&#31227;&#30340;&#26368;&#20248;&#36136;&#37327;&#36755;&#36816;&#38382;&#39064;&#30340;&#19968;&#20010;&#23454;&#20363;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;Eulerian OMT&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#29256;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#25511;&#21046;&#22120;&#32508;&#21512;&#30340;&#20998;&#26512;&#21644;&#25968;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite horizon optimal steering of the joint state probability distribution subject to the angular velocity dynamics governed by the Euler equation. The problem and its solution amounts to controlling the spin of a rigid body via feedback, and is of practical importance, for example, in angular stabilization of a spacecraft with stochastic initial and terminal states. We clarify how this problem is an instance of the optimal mass transport (OMT) problem with bilinear prior drift. We deduce both static and dynamic versions of the Eulerian OMT, and provide analytical and numerical results for the synthesis of the optimal controller.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.00553</link><description>&lt;p&gt;
&#20174;&#23396;&#31435;&#30340;&#23707;&#23679;&#21040;&#27867;&#22823;&#38470;&#65306;&#32479;&#19968;&#35821;&#20041;&#31354;&#38388;&#29992;&#20110;&#20154;&#31867;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#29702;&#35299;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#24182;&#19988;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20174;&#34892;&#20026;&#30340;&#29289;&#29702;&#31354;&#38388;&#21040;&#35821;&#20041;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#20250;&#26681;&#25454;&#29420;&#29305;&#30340;&#36873;&#25321;&#26500;&#24314;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#20197;&#23450;&#20041;&#21508;&#31181;&#31867;&#21035;&#24182;&#23558;&#22522;&#20934;&#32447;&#25512;&#21521;&#26497;&#38480;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#21644;&#19981;&#21516;&#30340;&#31867;&#21035;&#31890;&#24230;&#65292;&#23601;&#20687;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#19968;&#26679;&#20114;&#19981;&#20860;&#23481;&#65292;&#20363;&#22914;&#25968;&#25454;&#38598;A&#20013;&#30340;&#23478;&#21153;&#21644;&#25968;&#25454;&#38598;B&#20013;&#30340;&#27927;&#30424;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#20010;&#26356;&#20855;&#21407;&#21017;&#24615;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#38598;&#20013;&#31038;&#21306;&#30340;&#21147;&#37327;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#19968;&#36215;&#20351;&#29992;&#25152;&#26377;&#25968;&#25454;&#38598;&#20197;&#36861;&#27714;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#32473;&#23450;&#21160;&#35789;&#20998;&#31867;&#23618;&#27425;&#32467;&#26500;&#24182;&#28085;&#30422;&#22823;&#37327;&#34892;&#20026;&#12290;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#25105;&#20204;&#30340;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25105;&#20204;&#23558;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#25910;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#26631;&#31614;&#31995;&#32479;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35821;&#20041;&#31354;&#38388;&#21644;&#32479;&#19968;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action understanding matters and attracts attention. It can be formed as the mapping from the action physical space to the semantic space. Typically, researchers built action datasets according to idiosyncratic choices to define classes and push the envelope of benchmarks respectively. Thus, datasets are incompatible with each other like "Isolated Islands" due to semantic gaps and various class granularities, e.g., do housework in dataset A and wash plate in dataset B. We argue that a more principled semantic space is an urgent need to concentrate the community efforts and enable us to use all datasets together to pursue generalizable action learning. To this end, we design a Poincare action semantic space given verb taxonomy hierarchy and covering massive actions. By aligning the classes of previous datasets to our semantic space, we gather (image/video/skeleton/MoCap) datasets into a unified database in a unified label system, i.e., bridging "isolated islands" into a "Pangea". Accord
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#25511;&#21046;&#65292;&#26082;&#32771;&#34385;&#20102;&#31995;&#32479;&#19981;&#21464;&#23494;&#24230;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21448;&#33021;&#23545;&#31995;&#32479;&#36827;&#34892;&#26377;&#25928;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.00423</link><description>&lt;p&gt;
&#20960;&#20309;&#32422;&#26463;&#25552;&#39640;&#20102;&#23545;&#31232;&#30095;&#35266;&#27979;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Geometric constraints improve inference of sparsely observed stochastic dynamics. (arXiv:2304.00423v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#25511;&#21046;&#65292;&#26082;&#32771;&#34385;&#20102;&#31995;&#32479;&#19981;&#21464;&#23494;&#24230;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21448;&#33021;&#23545;&#31995;&#32479;&#36827;&#34892;&#26377;&#25928;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#30001;&#24230;&#30340;&#31995;&#32479;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#28436;&#21270;&#30340;&#21160;&#21147;&#23398;&#36890;&#24120;&#20197;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24418;&#24335;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#24120;&#36825;&#20123;&#26041;&#31243;&#30340;&#32467;&#26500;&#24418;&#24335;&#26159;&#26410;&#30693;&#30340;&#65292;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#21807;&#19968;&#34920;&#29616;&#24418;&#24335;&#26159;&#22312;&#31163;&#25955;&#26102;&#38388;&#28857;&#19978;&#30340;&#35266;&#27979;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20934;&#30830;&#22320;&#20174;&#31232;&#30095;&#26102;&#22495;&#35266;&#27979;&#20013;&#25512;&#26029;&#36825;&#20123;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#25512;&#26029;&#26041;&#27861;&#35201;&#20040;&#38598;&#20013;&#20110;&#35266;&#27979;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#24573;&#30053;&#31995;&#32479;&#19981;&#21464;&#23494;&#24230;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#35201;&#20040;&#20351;&#29992;&#31995;&#32479;&#19981;&#21464;&#23494;&#24230;&#30340;&#20960;&#20309;&#36924;&#36817;&#65292;&#36825;&#20123;&#36924;&#36817;&#20165;&#36866;&#29992;&#20110;&#20445;&#23432;&#30340;&#39537;&#21160;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#22312;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#36825;&#20004;&#20010;&#35270;&#35282;&#35843;&#21644;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36335;&#24452;&#22686;&#24378;&#26041;&#26696;&#65292;&#23427;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#25511;&#21046;&#26469;&#32771;&#34385;&#19981;&#21464;&#31995;&#32479;&#23494;&#24230;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#23545;&#22686;&#24378;&#36335;&#24452;&#30340;&#38750;&#21442;&#25968;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#23545;&#31995;&#32479;&#30340;&#26377;&#25928;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of systems of many degrees of freedom evolving on multiple scales are often modeled in terms of stochastic differential equations. Usually the structural form of these equations is unknown and the only manifestation of the system's dynamics are observations at discrete points in time. Despite their widespread use, accurately inferring these systems from sparse-in-time observations remains challenging. Conventional inference methods either focus on the temporal structure of observations, neglecting the geometry of the system's invariant density, or use geometric approximations of the invariant density, which are limited to conservative driving forces. To address these limitations, here, we introduce a novel approach that reconciles these two perspectives. We propose a path augmentation scheme that employs data-driven control to account for the geometry of the invariant system's density. Non-parametric inference on the augmented paths, enables efficient identification of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#25968;&#20540;&#35299;&#20915;&#21442;&#25968;&#21270;PDEs&#30340;&#22810;&#32423;CNN&#26041;&#27861;&#65292;&#26377;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#24182;&#33021;&#20197;&#20219;&#24847;&#31934;&#24230;&#36817;&#20284;&#22810;&#37325;&#32593;&#26684;V&#24490;&#29615;&#12290;</title><link>http://arxiv.org/abs/2304.00388</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;PDE&#30340;&#22810;&#32423;CNN
&lt;/p&gt;
&lt;p&gt;
Multilevel CNNs for Parametric PDEs. (arXiv:2304.00388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#25968;&#20540;&#35299;&#20915;&#21442;&#25968;&#21270;PDEs&#30340;&#22810;&#32423;CNN&#26041;&#27861;&#65292;&#26377;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#24182;&#33021;&#20197;&#20219;&#24847;&#31934;&#24230;&#36817;&#20284;&#22810;&#37325;&#32593;&#26684;V&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#22810;&#32423;&#27714;&#35299;&#22120;&#30340;&#27010;&#24565;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#39640;&#32500;&#21442;&#25968;PDEs&#30340;&#26377;&#25928;&#25968;&#20540;&#26041;&#27861;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20197;&#20219;&#24847;&#31934;&#24230;&#36817;&#20284;&#22810;&#37325;&#32593;&#26684;V&#24490;&#29615;&#65292;&#20854;&#26435;&#37325;&#25968;&#37327;&#20165;&#19982;&#26368;&#32454;&#32593;&#26684;&#30340;&#20998;&#36776;&#29575;&#23545;&#25968;&#26377;&#20851;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We combine concepts from multilevel solvers for partial differential equations (PDEs) with neural network based deep learning and propose a new methodology for the efficient numerical solution of high-dimensional parametric PDEs. An in-depth theoretical analysis shows that the proposed architecture is able to approximate multigrid V-cycles to arbitrary precision with the number of weights only depending logarithmically on the resolution of the finest mesh. As a consequence, approximation bounds for the solution of parametric PDEs by neural networks that are independent on the (stochastic) parameter dimension can be derived. The performance of the proposed method is illustrated on high-dimensional parametric linear elliptic PDEs that are common benchmark problems in uncertainty quantification. We find substantial improvements over state-of-the-art deep learning-based solvers. As particularly challenging examples, random conductivity with high-dimensional non-affine Gaussian fields in 10
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;</title><link>http://arxiv.org/abs/2304.00252</link><description>&lt;p&gt;
RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#20445;&#25252;&#65306;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#25915;&#20987;&#21487;&#20197;&#20351;&#24694;&#24847;&#29992;&#25143;&#25805;&#32437;&#29615;&#22659;&#25110;&#30772;&#22351;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23558;&#19968;&#20010;&#38544;&#34255;&#30340;&#21518;&#38376;&#25554;&#20837;&#21040;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#20013;&#12290;&#36825;&#31181;&#25915;&#20987;&#21361;&#21450;RL&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#65292;&#22312;&#21508;&#20010;&#20851;&#38190;&#39046;&#22495;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#12290;&#19982;&#27492;&#30456;&#27604;&#65292;&#23545;&#20110;RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#26377;&#25928;&#30340;&#38450;&#24481;&#25514;&#26045;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#25252;&#21463;&#23475;&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290; RTS&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#12290;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#20195;&#29702;&#20013;&#38544;&#34255;&#30340;&#21518;&#38376;&#12290;&#22312;&#35757;&#32451;&#26367;&#20195;&#32593;&#32476;&#26469;&#39044;&#27979;&#29366;&#24577;&#26102;&#65292;&#25105;&#20204;&#23558;&#20195;&#29702;&#21160;&#20316;&#20449;&#24687;&#24182;&#20837;&#65292;&#20943;&#23569;&#20195;&#29702;&#22312;&#39044;&#27979;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#21644;&#23454;&#38469;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real sta
&lt;/p&gt;</description></item><item><title>oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17612</link><description>&lt;p&gt;
oBERTa: &#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#21644;&#21098;&#26525;&#26469;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17612
&lt;/p&gt;
&lt;p&gt;
oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;oBERTa&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23427;&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;3.8&#21040;24.3&#20493;&#30340;&#26356;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;oBERTa&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21098;&#26525;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#24037;&#20316;&#65292;&#24182;&#21033;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#22312;&#24191;&#27867;&#30340;&#20256;&#36882;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#29983;&#25104;oBERTa&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;RoBERTa&#19982;BERT&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#21098;&#26525;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#24494;&#35843;&#26399;&#38388;&#19981;&#22826;&#36866;&#21512;&#21387;&#32553;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;oBERTa&#22312;&#19971;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#36827;&#30340;&#21387;&#32553;&#25216;&#26415;&#20351;&#24471;&#32463;&#36807;&#21098;&#26525;&#30340;oBERTa&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;BERTBASE&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;SQUAD V1.1&#38382;&#31572;&#25968;&#25454;&#30340;Prune OFA Large&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25351;&#20986;&#20256;&#32479;&#30340;&#22270;&#29255;&#20998;&#31867;&#22120;&#23398;&#20064;&#36807;&#31243;&#24573;&#35270;&#27880;&#37322;&#36807;&#31243;&#20013;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#26469;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17595</link><description>&lt;p&gt;
&#34987;&#24573;&#35270;&#30340;&#20813;&#36153;&#21320;&#39184;&#8212;&#8212;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts. (arXiv:2303.17595v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25351;&#20986;&#20256;&#32479;&#30340;&#22270;&#29255;&#20998;&#31867;&#22120;&#23398;&#20064;&#36807;&#31243;&#24573;&#35270;&#27880;&#37322;&#36807;&#31243;&#20013;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#26469;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#30417;&#30563;&#23398;&#20064;&#23558;&#20154;&#31867;&#30693;&#35782;&#36890;&#36807;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#65288;X&#65292;Y&#65289;&#30340;&#23545;&#24212;&#20851;&#31995;&#36716;&#21270;&#20026;&#21442;&#25968;&#27169;&#22411;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#31616;&#21333;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#20154;&#31867;&#30693;&#35782;&#34920;&#31034;&#24573;&#35270;&#20102;&#27880;&#37322;&#36807;&#31243;&#20013;&#20016;&#23500;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#20363;&#22914;&#22312;&#22270;&#20687;&#36873;&#25321;&#21518;&#30041;&#19979;&#30340;&#40736;&#26631;&#36712;&#36857;&#21644;&#28857;&#20987;&#30340;&#26102;&#38388;&#24207;&#21015;&#31561;&#12290;&#25105;&#20204;&#30340;&#27934;&#35265;&#26159;&#65292;&#36825;&#20123;&#27880;&#37322;&#21103;&#20135;&#21697;Z&#25552;&#20379;&#20102;&#36817;&#20284;&#30340;&#20154;&#31867;&#20851;&#27880;&#20449;&#24687;&#65292;&#24369;&#21270;&#20102;&#27169;&#22411;&#23545;&#21069;&#26223;&#32447;&#32034;&#30340;&#20851;&#27880;&#65292;&#20943;&#23569;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#38450;&#27490;&#20102;&#25463;&#24452;&#23398;&#20064;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ImageNet-AB&#21644;COCO-AB&#12290;&#23427;&#20204;&#26159;&#36890;&#36807;&#22797;&#21046;&#30456;&#24212;&#30340;&#21407;&#22987;&#27880;&#37322;&#20219;&#21153;&#26469;&#33719;&#24471;&#30340;ImageNet&#21644;COCO&#35757;&#32451;&#38598;&#65292;&#22686;&#21152;&#20102;&#26679;&#26412;&#32423;&#21035;&#30340;&#27880;&#37322;&#21103;&#20135;&#21697;&#12290;&#25105;&#20204;&#31216;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#26469;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#20026;&#23398;&#20064;&#27880;&#37322;&#21103;&#20135;&#21697;&#65288;LUAB&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#25439;&#22833;&#65292;&#29992;&#20110;&#21516;&#26102;&#22238;&#24402;Z&#21644;Y&#24050;&#32463;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning of image classifiers distills human knowledge into a parametric model through pairs of images and corresponding labels (X,Y). We argue that this simple and widely used representation of human knowledge neglects rich auxiliary information from the annotation procedure, such as the time-series of mouse traces and clicks left after image selection. Our insight is that such annotation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues, reducing spurious correlations and discouraging shortcut learning. To verify this, we create ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annotation byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing Z together with Y already improves 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#28508;&#22312;&#20301;&#32622;&#27169;&#22411;&#19978;&#30340;&#22270;&#24418;Nadaraya-Watson&#20272;&#35745;&#22120;&#30340;&#24615;&#36136;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#26377;&#29702;&#35770;&#25351;&#23548;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.17229</link><description>&lt;p&gt;
&#28508;&#22312;&#20301;&#32622;&#27169;&#22411;&#19978;&#30340;&#22270;&#24418;Nadaraya-Watson&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
The Graphical Nadaraya-Watson Estimator on Latent Position Models. (arXiv:2303.17229v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17229
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#28508;&#22312;&#20301;&#32622;&#27169;&#22411;&#19978;&#30340;&#22270;&#24418;Nadaraya-Watson&#20272;&#35745;&#22120;&#30340;&#24615;&#36136;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#26377;&#29702;&#35770;&#25351;&#23548;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26377;&#26631;&#35760;&#33410;&#28857;&#30340;&#22270;&#24418;&#65292;&#25105;&#20204;&#23545;&#20272;&#35745;&#22120;&#30340;&#36136;&#37327;&#24863;&#20852;&#36259;&#65292;&#35813;&#20272;&#35745;&#22120;&#38024;&#23545;&#26410;&#26631;&#35760;&#33410;&#28857;&#39044;&#27979;&#20854;&#26631;&#35760;&#37051;&#23621;&#30340;&#35266;&#27979;&#20540;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20005;&#26684;&#30740;&#31350;&#20102;&#27987;&#24230;&#23646;&#24615;&#12289;&#26041;&#24046;&#30028;&#21644;&#39118;&#38505;&#30028;&#12290;&#34429;&#28982;&#20272;&#35745;&#22120;&#26412;&#36523;&#38750;&#24120;&#31616;&#21333;&#65292;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#36807;&#20110;&#29702;&#24819;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#23567;&#27493;&#39588;&#23558;&#26377;&#21161;&#20110;&#26356;&#22797;&#26434;&#26041;&#27861;&#65288;&#22914;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph with a subset of labeled nodes, we are interested in the quality of the averaging estimator which for an unlabeled node predicts the average of the observations of its labeled neighbours. We rigorously study concentration properties, variance bounds and risk bounds in this context. While the estimator itself is very simple and the data generating process is too idealistic for practical applications, we believe that our small steps will contribute towards the theoretical understanding of more sophisticated methods such as Graph Neural Networks.
&lt;/p&gt;</description></item><item><title>&#24314;&#35758;&#23558;&#25512;&#26029;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#25972;&#21512;&#21040;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#20197;&#25913;&#21892;&#39640;&#26031;&#22270;&#27169;&#22411;&#22312;&#35266;&#23519;&#21547;&#22122;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16796</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#21487;&#25913;&#21892;&#35266;&#23519;&#21547;&#22122;&#25968;&#25454;&#30340;&#39640;&#26031;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Module-based regularization improves Gaussian graphical models when observing noisy data. (arXiv:2303.16796v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16796
&lt;/p&gt;
&lt;p&gt;
&#24314;&#35758;&#23558;&#25512;&#26029;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#25972;&#21512;&#21040;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#20197;&#25913;&#21892;&#39640;&#26031;&#22270;&#27169;&#22411;&#22312;&#35266;&#23519;&#21547;&#22122;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#39640;&#26031;&#22270;&#27169;&#22411;&#34920;&#31034;&#22810;&#21464;&#37327;&#30456;&#20851;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#27491;&#21017;&#21270;&#26469;&#31232;&#30095;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#23558;&#25512;&#26029;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#25972;&#21512;&#36215;&#26469;&#20197;&#24179;&#34913;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20351;&#29992;&#39640;&#26031;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#30340;&#26631;&#20934;&#26041;&#27861;&#65288;&#22270;&#24418;&#22871;&#32034;&#27861;&#65289;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#24674;&#22797;&#21644;&#25512;&#26029;&#21547;&#22122;&#22768;&#25968;&#25454;&#20013;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers often represent relations in multi-variate correlational data using Gaussian graphical models, which require regularization to sparsify the models. Acknowledging that they often study the modular structure of the inferred network, we suggest integrating it in the cross-validation of the regularization strength to balance under- and overfitting. Using synthetic and real data, we show that this approach allows us to better recover and infer modular structure in noisy data compared with the graphical lasso, a standard approach using the Gaussian log-likelihood when cross-validating the regularization strength.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#21015;&#34920;&#30340;&#22312;&#32447;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102; $b$-ary Littlestone &#32500;&#24230;&#21487;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#25077;&#25026;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#21487;&#20197;&#20351;&#29992;&#25913;&#32534;&#33258; Littlestone &#30340; SOA &#21644; Rosenblatt &#30340;&#24863;&#30693;&#22120;&#31561;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#36824;&#24314;&#31435;&#20102;&#21015;&#34920;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15383</link><description>&lt;p&gt;
&#22522;&#20110;&#21015;&#34920;&#30340;&#22312;&#32447;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
List Online Classification. (arXiv:2303.15383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#21015;&#34920;&#30340;&#22312;&#32447;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102; $b$-ary Littlestone &#32500;&#24230;&#21487;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#25077;&#25026;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#21487;&#20197;&#20351;&#29992;&#25913;&#32534;&#33258; Littlestone &#30340; SOA &#21644; Rosenblatt &#30340;&#24863;&#30693;&#22120;&#31561;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#36824;&#24314;&#31435;&#20102;&#21015;&#34920;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22810;&#20998;&#31867;&#22312;&#32447;&#39044;&#27979;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#21487;&#20197;&#20351;&#29992;&#22810;&#20010;&#26631;&#31614;&#30340;&#21015;&#34920;&#36827;&#34892;&#39044;&#27979;&#65288;&#19982;&#20256;&#32479;&#35774;&#32622;&#20013;&#20165;&#20351;&#29992;&#19968;&#31181;&#26631;&#31614;&#19981;&#21516;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992; $b$-ary Littlestone &#32500;&#24230;&#34920;&#24449;&#20102;&#35813;&#27169;&#22411;&#20013;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#35813;&#32500;&#24230;&#26159;&#32463;&#20856; Littlestone &#32500;&#24230;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20108;&#36827;&#21046;&#38169;&#35823;&#26641;&#34987;&#26367;&#25442;&#20026; $(k+1)$-ary &#38169;&#35823;&#26641;&#65292;&#20854;&#20013; k &#26159;&#21015;&#34920;&#20013;&#26631;&#31614;&#30340;&#25968;&#37327;&#12290;&#22312;&#25077;&#25026;&#30340;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#27604;&#36739;&#31867;&#20013;&#26159;&#21542;&#21253;&#21547;&#21333;&#26631;&#31614;&#25110;&#22810;&#26631;&#31614;&#20989;&#25968;&#20197;&#21450;&#23427;&#19982;&#31639;&#27861;&#20351;&#29992;&#30340;&#21015;&#34920;&#22823;&#23567;&#20043;&#38388;&#30340;&#26435;&#34913;&#26469;&#25506;&#32034;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#36127;&#24724;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20160;&#20040;&#24773;&#20917;&#19979;&#23454;&#29616;&#36127;&#24724;&#30340;&#23436;&#25972;&#29305;&#24615;&#21270;&#12290;&#20316;&#20026;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102;&#32463;&#20856;&#31639;&#27861;&#65292;&#22914; Littlestone &#30340; SOA &#21644; Rosenblatt &#30340;&#24863;&#30693;&#22120;&#65292;&#20197;&#20351;&#29992;&#26631;&#31614;&#21015;&#34920;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#20026;&#21487;&#20197;&#36827;&#34892;&#21015;&#34920;&#23398;&#20064;&#30340;&#32452;&#21512;&#32467;&#26524;&#24314;&#31435;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multiclass online prediction where the learner can predict using a list of multiple labels (as opposed to just one label in the traditional setting). We characterize learnability in this model using the $b$-ary Littlestone dimension. This dimension is a variation of the classical Littlestone dimension with the difference that binary mistake trees are replaced with $(k+1)$-ary mistake trees, where $k$ is the number of labels in the list. In the agnostic setting, we explore different scenarios depending on whether the comparator class consists of single-labeled or multi-labeled functions and its tradeoff with the size of the lists the algorithm uses. We find that it is possible to achieve negative regret in some cases and provide a complete characterization of when this is possible. As part of our work, we adapt classical algorithms such as Littlestone's SOA and Rosenblatt's Perceptron to predict using lists of labels. We also establish combinatorial results for list-learnable c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#24335;-Chat-Rec&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#23545;&#35805;&#24335;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;Chat-Rec&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#24314;&#31435;&#29992;&#25143;-&#20135;&#21697;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.14524</link><description>&lt;p&gt;
Chat-REC&#65306;&#38754;&#21521;&#20114;&#21160;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;LLM&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. (arXiv:2303.14524v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#24335;-Chat-Rec&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#23545;&#35805;&#24335;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;Chat-Rec&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#24314;&#31435;&#29992;&#25143;-&#20135;&#21697;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#21508;&#31181;&#24212;&#29992;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#20173;&#38754;&#20020;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#22914;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#24046;&#65292;&#36825;&#23454;&#38469;&#19978;&#20063;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#24335;&#65292;&#31216;&#20026;Chat-REC&#65288;ChatGPT&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65289;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#21644;&#21382;&#21490;&#20132;&#20114;&#36716;&#25442;&#20026;&#25552;&#31034;&#65292;&#21019;&#26032;&#22320;&#22686;&#24378;LLMs&#29992;&#20110;&#26500;&#24314;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65292;Chat-Rec&#34987;&#35777;&#26126;&#22312;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#21644;&#24314;&#31435;&#29992;&#25143;&#19982;&#20135;&#21697;&#20043;&#38388;&#30340;&#32852;&#31995;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36825;&#20063;&#20351;&#24471;&#25512;&#33616;&#36807;&#31243;&#26356;&#20855;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;Chat-Rec&#26694;&#26550;&#20869;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#21487;&#20197;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#20135;&#21697;&#36827;&#34892;&#36328;&#39046;&#22495;&#25512;&#33616;&#65292;&#24182;&#19988;&#22522;&#20110;&#25552;&#31034;&#30340;&#27880;&#20837;&#20801;&#35768;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#23545;&#25512;&#33616;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of informa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CoILS&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#31283;&#23450;&#25511;&#21046;&#22120;&#65292;&#21516;&#26102;&#23398;&#20064;&#21442;&#25968;&#21270;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65292;&#20174;&#32780;&#20351;&#21160;&#21147;&#23398;&#27169;&#22411;&#26412;&#36523;&#20855;&#26377;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#30001;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03157</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#22312;&#26446;&#20122;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Control with Inherent Lyapunov Stability. (arXiv:2303.03157v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CoILS&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#31283;&#23450;&#25511;&#21046;&#22120;&#65292;&#21516;&#26102;&#23398;&#20064;&#21442;&#25968;&#21270;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65292;&#20174;&#32780;&#20351;&#21160;&#21147;&#23398;&#27169;&#22411;&#26412;&#36523;&#20855;&#26377;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#30001;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23398;&#20064;&#22411;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23427;&#20511;&#21161;&#28145;&#24230;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#20363;&#22914;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#27169;&#25311;&#21463;&#25511;&#21160;&#24577;&#31995;&#32479;&#38543;&#26102;&#38388;&#30340;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21160;&#24577;&#27169;&#22411;&#21644;&#31283;&#23450;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#22240;&#20026;&#23545;&#20110;&#24050;&#30693;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#21512;&#25104;&#31283;&#23450;&#21453;&#39304;&#24459;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#26356;&#20309;&#20917;&#23545;&#20110;&#24517;&#39035;&#36866;&#24212;&#25968;&#25454;&#30340;&#22797;&#26434;&#21442;&#25968;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#22312;&#26446;&#20122;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#65288;CoILS&#65289;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#32852;&#21512;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#31283;&#23450;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#23398;&#20064;&#21442;&#25968;&#21270;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65292;&#20174;&#32780;&#20351;&#21160;&#21147;&#23398;&#27169;&#22411;&#26412;&#36523;&#20855;&#26377;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#30001;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#23454;&#29616;&#12290;&#38500;&#20102;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#20855;&#26377;&#23398;&#20064;&#21160;&#24577;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#20351;&#30495;&#23454;&#21160;&#24577;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in learning-based control leverage deep function approximators, such as neural networks, to model the evolution of controlled dynamical systems over time. However, the problem of learning a dynamics model and a stabilizing controller persists, since the synthesis of a stabilizing feedback law for known nonlinear systems is a difficult task, let alone for complex parametric representations that must be fit to data. To this end, we propose Control with Inherent Lyapunov Stability (CoILS), a method for jointly learning parametric representations of a nonlinear dynamics model and a stabilizing controller from data. To do this, our approach simultaneously learns a parametric Lyapunov function which intrinsically constrains the dynamics model to be stabilizable by the learned controller. In addition to the stabilizability of the learned dynamics guaranteed by our novel construction, we show that the learned controller stabilizes the true dynamics under certain assumptions on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#20960;&#20309;&#30340;&#20840;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;3D&#22836;&#37096;&#21644;&#36523;&#20307;&#32593;&#26684;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#28608;&#21169;&#28508;&#22312;&#21464;&#37327;&#36981;&#24490;&#29305;&#24449;&#21521;&#37327;&#25237;&#24433;&#24182;&#25913;&#21892;&#28508;&#22312;&#31354;&#38388;&#35299;&#32806;&#65292;&#23454;&#29616;&#23545;&#29983;&#25104;&#26412;&#22320;&#24418;&#29366;&#23646;&#24615;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.12798</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#22320;&#29305;&#24449;&#21521;&#37327;&#25237;&#24433;&#30340;3D&#29983;&#25104;&#27169;&#22411;&#28508;&#22312;&#31354;&#38388;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
3D Generative Model Latent Disentanglement via Local Eigenprojection. (arXiv:2302.12798v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#20960;&#20309;&#30340;&#20840;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;3D&#22836;&#37096;&#21644;&#36523;&#20307;&#32593;&#26684;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#28608;&#21169;&#28508;&#22312;&#21464;&#37327;&#36981;&#24490;&#29305;&#24449;&#21521;&#37327;&#25237;&#24433;&#24182;&#25913;&#21892;&#28508;&#22312;&#31354;&#38388;&#35299;&#32806;&#65292;&#23454;&#29616;&#23545;&#29983;&#25104;&#26412;&#22320;&#24418;&#29366;&#23646;&#24615;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#36924;&#30495;&#30340;&#25968;&#23383;&#20154;&#29289;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#12290;&#22823;&#22810;&#25968;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#31616;&#21270;&#24213;&#23618;&#20960;&#20309;&#24418;&#29366;&#30340;&#21019;&#24314;&#24182;&#19981;&#25552;&#20379;&#23545;&#26412;&#22320;&#24418;&#29366;&#23646;&#24615;&#29983;&#25104;&#30340;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#35889;&#20960;&#20309;&#30340;&#20840;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#22836;&#37096;&#21644;&#36523;&#20307;&#32593;&#26684;&#29983;&#25104;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#12290;&#36890;&#36807;&#40723;&#33310;&#32593;&#26684;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28508;&#22312;&#21464;&#37327;&#36981;&#24490;&#29305;&#24449;&#21521;&#37327;&#25237;&#24433;&#65292;&#25105;&#20204;&#25913;&#21892;&#20102;&#28508;&#22312;&#31354;&#38388;&#35299;&#32806;&#24182;&#27491;&#30830;&#22320;&#20998;&#31163;&#20102;&#23646;&#24615;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26412;&#22320;&#29305;&#24449;&#21521;&#37327;&#25237;&#24433;&#35299;&#32806;&#27169;&#22411;&#19981;&#20165;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#26377;&#25913;&#36827;&#65292;&#32780;&#19988;&#22312;&#20445;&#25345;&#33391;&#22909;&#30340;&#29983;&#25104;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#20854;&#35757;&#32451;&#26102;&#38388;&#20063;&#19982;&#27169;&#22411;&#30340;&#22522;&#26412;&#23454;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing realistic digital humans is extremely complex. Most data-driven generative models used to simplify the creation of their underlying geometric shape do not offer control over the generation of local shape attributes. In this paper, we overcome this limitation by introducing a novel loss function grounded in spectral geometry and applicable to different neural-network-based generative models of 3D head and body meshes. Encouraging the latent variables of mesh variational autoencoders (VAEs) or generative adversarial networks (GANs) to follow the local eigenprojections of identity attributes, we improve latent disentanglement and properly decouple the attribute creation. Experimental results show that our local eigenprojection disentangled (LED) models not only offer improved disentanglement with respect to the state-of-the-art, but also maintain good generation capabilities with training times comparable to the vanilla implementations of the models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#65292;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#30340;&#19987;&#29992;&#39033;&#26469;&#23454;&#29616;&#24494;&#20998;&#26041;&#31243;&#30340;&#27714;&#35299;&#12290;&#36890;&#36807;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#39044;&#27979;&#35299;&#65292;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#24369;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.12260</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#30340;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#65306;&#22522;&#20110;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#36341;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests. (arXiv:2302.12260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#65292;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#30340;&#19987;&#29992;&#39033;&#26469;&#23454;&#29616;&#24494;&#20998;&#26041;&#31243;&#30340;&#27714;&#35299;&#12290;&#36890;&#36807;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#39044;&#27979;&#35299;&#65292;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#24369;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#35775;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#23558;&#26041;&#31243;&#30693;&#35782;&#32435;&#20837;&#32771;&#34385;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#19987;&#29992;&#39033;&#21040;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#25152;&#35859;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#21508;&#31181;&#23398;&#26415;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#33021;&#21147;&#65292;&#20197;&#31361;&#26174;&#19982;&#26631;&#20934;&#31215;&#20998;&#26041;&#27861;&#30456;&#27604;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#37325;&#28857;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26368;&#23569;&#37327;&#30340;&#25968;&#25454;&#12290;&#22238;&#39038;&#20102;&#36890;&#36807;&#24809;&#32602;&#39033;&#24378;&#21046;&#25191;&#34892;&#29289;&#29702;&#23450;&#24459;&#26469;&#35299;&#20915;&#24494;&#20998;&#26041;&#31243;&#30340;PINNs&#30340;&#21407;&#29702;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#31243;&#27169;&#22411;&#30340;&#25945;&#31243;&#21521;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#12290;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#24403;&#38382;&#39064;&#30340;&#38750;&#32447;&#24615;&#24369;&#26102;&#65292;&#38750;&#24120;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#39044;&#27979;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the original approach of using deep learning and neural networks to solve differential equations by incorporating the knowledge of the equation. This is done by adding a dedicated term to the loss function during the optimization procedure in the training process. The so-called physics-informed neural networks (PINNs) are tested on a variety of academic ordinary differential equations in order to highlight the benefits and drawbacks of this approach with respect to standard integration methods. We focus on the possibility to use the least possible amount of data into the training process. The principles of PINNs for solving differential equations by enforcing physical laws via penalizing terms are reviewed. A tutorial on a simple equation model illustrates how to put into practice the method for ordinary differential equations. Benchmark tests show that a very small amount of training data is sufficient to predict the solution when the non linearity of the problem is weak. H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#35768;&#22810;&#20986;&#29616;&#22312;&#25991;&#29486;&#20013;&#30340;&#22240;&#26524;&#21059;&#20992;&#65292;&#24182;&#29305;&#21035;&#30740;&#31350;&#20102;&#22312;&#22810;&#39033;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#19981;&#22826;&#21463;&#27426;&#36814;&#30340;&#22240;&#26524;&#21059;&#20992;&#8212;&#8212;&#21442;&#25968;&#26368;&#23567;&#24615;&#12290;&#36923;&#36753;&#32467;&#26524;&#25581;&#31034;&#20102;&#36873;&#25321;&#21512;&#29702;&#24471;&#20998;&#26631;&#20934;&#26102;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2302.10331</link><description>&lt;p&gt;
&#22240;&#26524;&#21059;&#20992;
&lt;/p&gt;
&lt;p&gt;
Causal Razors. (arXiv:2302.10331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#35768;&#22810;&#20986;&#29616;&#22312;&#25991;&#29486;&#20013;&#30340;&#22240;&#26524;&#21059;&#20992;&#65292;&#24182;&#29305;&#21035;&#30740;&#31350;&#20102;&#22312;&#22810;&#39033;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#19981;&#22826;&#21463;&#27426;&#36814;&#30340;&#22240;&#26524;&#21059;&#20992;&#8212;&#8212;&#21442;&#25968;&#26368;&#23567;&#24615;&#12290;&#36923;&#36753;&#32467;&#26524;&#25581;&#31034;&#20102;&#36873;&#25321;&#21512;&#29702;&#24471;&#20998;&#26631;&#20934;&#26102;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#26102;&#65292;&#24517;&#39035;&#23545;&#30495;&#23454;&#22240;&#26524;&#26426;&#21046;&#22914;&#20309;&#19982;&#24213;&#23618;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#30456;&#23545;&#24212;&#20570;&#20986;&#20551;&#35774;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#20551;&#35774;&#31216;&#20026;&#22240;&#26524;&#21059;&#20992;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#35768;&#22810;&#20986;&#29616;&#22312;&#25991;&#29486;&#20013;&#30340;&#22240;&#26524;&#21059;&#20992;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#36923;&#36753;&#27604;&#36739;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;&#22312;&#22810;&#39033;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#19981;&#22826;&#21463;&#27426;&#36814;&#30340;&#22240;&#26524;&#21059;&#20992;&#8212;&#8212;&#21442;&#25968;&#26368;&#23567;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#22240;&#26524;&#21059;&#20992;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#36923;&#36753;&#32467;&#26524;&#22312;&#20026;&#22522;&#20110;&#20998;&#25968;&#30340;&#22240;&#26524;&#25628;&#32034;&#31639;&#27861;&#36873;&#25321;&#21512;&#29702;&#24471;&#20998;&#26631;&#20934;&#26102;&#25552;&#20986;&#20102;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
When performing causal discovery, assumptions have to be made on how the true causal mechanism corresponds to the underlying joint probability distribution. These assumptions are labeled as causal razors in this work. We review numerous causal razors that appeared in the literature, and offer a comprehensive logical comparison of them. In particular, we scrutinize an unpopular causal razor, namely parameter minimality, in multinomial causal models and its logical relations with other well-studied causal razors. Our logical result poses a dilemma in selecting a reasonable scoring criterion for score-based casual search algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;VAMoH&#65292;&#32467;&#21512;&#20102;INRs&#23545;&#36830;&#32493;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#21644;VAEs&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#20197;&#21450;&#24402;&#19968;&#21270;&#27969;&#21644;&#36229;&#32593;&#32476;&#28151;&#21512;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;VAMoH&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36830;&#32493;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#25191;&#34892;&#19982;&#25512;&#26029;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.06223</link><description>&lt;p&gt;
&#21464;&#20998;&#28151;&#21512;&#36229;&#29983;&#25104;&#22120;&#29992;&#20110;&#23398;&#20064;&#20989;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational Mixture of HyperGenerators for Learning Distributions Over Functions. (arXiv:2302.06223v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;VAMoH&#65292;&#32467;&#21512;&#20102;INRs&#23545;&#36830;&#32493;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#21644;VAEs&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#20197;&#21450;&#24402;&#19968;&#21270;&#27969;&#21644;&#36229;&#32593;&#32476;&#28151;&#21512;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;VAMoH&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36830;&#32493;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#25191;&#34892;&#19982;&#25512;&#26029;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#22522;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#25552;&#20986;&#20102;&#20989;&#25968;&#31354;&#38388;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#25512;&#26029;&#20219;&#21153;&#65288;&#22914;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65289;&#26102;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#26681;&#26412;&#19981;&#33021;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;VAMoH&#12290;VAMoH&#32467;&#21512;&#20102;&#20351;&#29992;INRs&#23545;&#36830;&#32493;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#25512;&#26029;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;VAMoH&#20381;&#36182;&#20110;&#19968;&#20010;&#24402;&#19968;&#21270;&#27969;&#26469;&#23450;&#20041;&#20808;&#39564;&#65292;&#20197;&#21450;&#19968;&#20010;&#36229;&#32593;&#32476;&#28151;&#21512;&#26469;&#21442;&#25968;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#12290;&#36825;&#20351;&#24471;VAMoH&#20855;&#26377;&#39640;&#24230;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#65288;&#22914;&#22270;&#20687;&#12289;&#20307;&#32032;&#21644;&#27668;&#20505;&#25968;&#25454;&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;VAMoH&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36830;&#32493;&#20989;&#25968;&#30340;&#20016;&#23500;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#25191;&#34892;&#19982;&#25512;&#26029;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#22914;&#26465;&#20214;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#21644;&#20462;&#22797;&#65292;&#25928;&#26524;&#20248;&#20110;&#25110;&#19981;&#20122;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches build on implicit neural representations (INRs) to propose generative models over function spaces. However, they are computationally costly when dealing with inference tasks, such as missing data imputation, or directly cannot tackle them. In this work, we propose a novel deep generative model, named VAMoH. VAMoH combines the capabilities of modeling continuous functions using INRs and the inference capabilities of Variational Autoencoders (VAEs). In addition, VAMoH relies on a normalizing flow to define the prior, and a mixture of hypernetworks to parametrize the data log-likelihood. This gives VAMoH a high expressive capability and interpretability. Through experiments on a diverse range of data types, such as images, voxels, and climate data, we show that VAMoH can effectively learn rich distributions over continuous functions. Furthermore, it can perform inference-related tasks, such as conditional super-resolution generation and in-painting, as well or better tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;11&#20010;&#22833;&#36133;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#12289;&#20107;&#23454;&#38169;&#35823;&#12289;&#25968;&#23398;&#12289;&#32534;&#30721;&#21644;&#20559;&#35265;&#12290;&#25214;&#20986;&#22833;&#36133;&#21407;&#22240;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25913;&#36827;&#26410;&#26469;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;</title><link>http://arxiv.org/abs/2302.03494</link><description>&lt;p&gt;
ChatGPT&#22833;&#36133;&#20998;&#31867;&#23384;&#26723;
&lt;/p&gt;
&lt;p&gt;
A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v8 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;11&#20010;&#22833;&#36133;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#12289;&#20107;&#23454;&#38169;&#35823;&#12289;&#25968;&#23398;&#12289;&#32534;&#30721;&#21644;&#20559;&#35265;&#12290;&#25214;&#20986;&#22833;&#36133;&#21407;&#22240;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25913;&#36827;&#26410;&#26469;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#19981;&#21516;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#12290;&#30001;OpenAI&#24320;&#21457;&#30340;ChatGPT&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#26469;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#12290;&#23427;&#22240;&#33021;&#22815;&#26377;&#25928;&#22320;&#22238;&#31572;&#24191;&#27867;&#30340;&#20154;&#31867;&#38382;&#39064;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#20854;&#27969;&#21033;&#21644;&#20840;&#38754;&#30340;&#31572;&#26696;&#22312;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20844;&#20849;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;ChatGPT&#22833;&#25928;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#36825;&#26159;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;11&#20010;&#22833;&#36133;&#31867;&#21035;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#20107;&#23454;&#38169;&#35823;&#12289;&#25968;&#23398;&#12289;&#32534;&#30721;&#21644;&#20559;&#35265;&#12290;&#36824;&#31361;&#20986;&#20102;ChatGPT&#30340;&#39118;&#38505;&#12289;&#38480;&#21046;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#22686;&#24378;&#26410;&#26469;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#21487;&#36801;&#31227;&#27169;&#22359;&#21644;&#36866;&#24212;&#24615;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#26377;&#22768;&#33410;&#28857;&#30340;&#30693;&#35782;&#36716;&#31227;&#32473;&#26080;&#22768;&#33410;&#28857;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#21547;&#26377;&#8220;&#27785;&#40664;&#22823;&#22810;&#25968;&#8221;&#30340;&#22270;&#34920;&#30340;&#25239;&#24046;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00873</link><description>&lt;p&gt;
&#38024;&#23545;&#21547;&#26377;&#8220;&#27785;&#40664;&#22823;&#22810;&#25968;&#8221;&#30340;&#22270;&#34920;&#36827;&#34892;&#25239;&#24046;&#24314;&#27169;&#30340;&#30693;&#35782;&#21487;&#36801;&#31227;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Predicting the Silent Majority on Graphs: Knowledge Transferable Graph Neural Network. (arXiv:2302.00873v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#21487;&#36801;&#31227;&#27169;&#22359;&#21644;&#36866;&#24212;&#24615;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#26377;&#22768;&#33410;&#28857;&#30340;&#30693;&#35782;&#36716;&#31227;&#32473;&#26080;&#22768;&#33410;&#28857;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#21547;&#26377;&#8220;&#27785;&#40664;&#22823;&#22810;&#25968;&#8221;&#30340;&#22270;&#34920;&#30340;&#25239;&#24046;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23384;&#22312;&#30001;&#26377;&#22768;&#33410;&#28857;&#65288;&#8220;&#26377;&#22768;&#23569;&#25968;&#8221;&#65289;&#21644;&#26080;&#22768;&#33410;&#28857;&#65288;&#8220;&#27785;&#40664;&#22823;&#22810;&#25968;&#8221;&#65289;&#32452;&#25104;&#30340;&#22270;&#34920;&#65292;&#21363; VS-Graph&#65292;&#20854;&#20013;&#26377;&#22768;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#20016;&#23500;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#32780;&#26080;&#22768;&#33410;&#28857;&#20165;&#20855;&#26377;&#19981;&#23436;&#25972;&#30340;&#29305;&#24449;&#21644;&#31232;&#32570;&#30340;&#26631;&#31614;&#12290; &#39044;&#27979;&#27785;&#40664;&#22823;&#22810;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#21487;&#36801;&#31227;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;KT-GNN&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#26377;&#22768;&#33410;&#28857;&#30340;&#30693;&#35782;&#36716;&#31227;&#32473;&#26080;&#22768;&#33410;&#28857;&#26469;&#23454;&#29616;&#28040;&#24687;&#20256;&#36882;&#21644;&#34920;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#24314;&#27169;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KT-GNN &#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs consisting of vocal nodes ("the vocal minority") and silent nodes ("the silent majority"), namely VS-Graph, are ubiquitous in the real world. The vocal nodes tend to have abundant features and labels. In contrast, silent nodes only have incomplete features and rare labels, e.g., the description and political tendency of politicians (vocal) are abundant while not for ordinary people (silent) on the twitter's social network. Predicting the silent majority remains a crucial yet challenging problem. However, most existing message-passing based GNNs assume that all nodes belong to the same domain, without considering the missing features and distribution-shift between domains, leading to poor ability to deal with VS-Graph. To combat the above challenges, we propose Knowledge Transferable Graph Neural Network (KT-GNN), which models distribution shifts during message passing and representation learning by transferring knowledge from vocal nodes to silent nodes. Specifically, we design 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#28431;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#40657;&#30418;&#25552;&#21462;&#12289;&#25512;&#26029;&#21644;&#37325;&#24314;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.00539</link><description>&lt;p&gt;
&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#35782;&#21035;&#20449;&#24687;&#27844;&#38706;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Analyzing Leakage of Personally Identifiable Information in Language Models. (arXiv:2302.00539v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#28431;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#40657;&#30418;&#25552;&#21462;&#12289;&#25512;&#26029;&#21644;&#37325;&#24314;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#20250;&#36890;&#36807;&#21477;&#23376;&#32423;&#25104;&#21592;&#25512;&#26029;&#21644;&#37325;&#26500;&#25915;&#20987;&#27844;&#28431;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#27844;&#38706;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#39118;&#38505;&#20102;&#35299;&#19981;&#36275;&#12290;&#30446;&#21069;&#24050;&#32463;&#20551;&#35774;&#25968;&#25454;&#38598;&#25972;&#29702;&#25216;&#26415;&#65288;&#22914;&#25968;&#25454;&#28165;&#27927;&#65289;&#36275;&#20197;&#38450;&#27490;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#27844;&#38706;&#65292;&#20294;&#36825;&#19968;&#20551;&#35774;&#26159;&#38169;&#35823;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#25968;&#25454;&#28165;&#27927;&#25216;&#26415;&#21487;&#20197;&#20943;&#23569;Pll&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#20294;&#24182;&#19981;&#33021;&#23436;&#20840;&#32477;&#23545;&#22320;&#38450;&#27490;&#27844;&#38706;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#27844;&#28431;&#30340;&#20005;&#26684;&#22522;&#20110;&#21338;&#24328;&#30340;&#23450;&#20041;&#65292;&#36890;&#36807;API&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#25552;&#21462;&#12289;&#25512;&#26029;&#21644;&#37325;&#24314;&#25915;&#20987;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the 
&lt;/p&gt;</description></item><item><title>DIFFormer&#26159;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#34701;&#21512;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#65292;&#23548;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#33021;&#22815;&#25581;&#31034;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2301.09474</link><description>&lt;p&gt;
DIFFormer&#65306;&#36890;&#36807;&#21463;&#33021;&#37327;&#38480;&#21046;&#30340;&#25193;&#25955;&#24341;&#20986;&#30340;&#21487;&#25193;&#23637;&#65288;&#22270;&#24418;&#65289;Transformer
&lt;/p&gt;
&lt;p&gt;
DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion. (arXiv:2301.09474v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09474
&lt;/p&gt;
&lt;p&gt;
DIFFormer&#26159;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#34701;&#21512;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#65292;&#23548;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#33021;&#22815;&#25581;&#31034;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#29983;&#25104;&#24120;&#24120;&#28041;&#21450;&#23454;&#20363;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20381;&#36182;&#65292;&#36829;&#21453;&#20102;&#26631;&#20934;&#23398;&#20064;&#33539;&#24335;&#30340;IID&#25968;&#25454;&#20551;&#35774;&#65292;&#20174;&#32780;&#23545;&#25581;&#31034;&#20960;&#20309;&#32467;&#26500;&#20197;&#23398;&#20064;&#25152;&#38656;&#35201;&#30340;&#23454;&#20363;&#34920;&#31034;&#24418;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#19968;&#25209;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#32534;&#30721;&#20026;&#36880;&#28176;&#34701;&#21512;&#20102;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#12290;&#25193;&#25955;&#36807;&#31243;&#21463;&#38480;&#20110;&#22522;&#20110;&#21512;&#29702;&#33021;&#37327;&#20989;&#25968;&#30340;&#19979;&#38477;&#26631;&#20934;&#65292;&#35813;&#20989;&#25968;&#34920;&#24449;&#20102;&#28508;&#22312;&#32467;&#26500;&#19978;&#23454;&#20363;&#34920;&#31034;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#35880;&#30340;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#26263;&#31034;&#20102;&#20219;&#24847;&#23454;&#20363;&#23545;&#20043;&#38388;&#30340;&#26368;&#20248;&#25193;&#25955;&#24378;&#24230;&#30340;&#38381;&#21512;&#24418;&#24335;&#20272;&#35745;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#30340;&#20135;&#29983;&#65306;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#31616;&#21333;&#29256;&#26412;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#38754;&#20020;&#30528;&#31105;&#24524;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t.~a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instanc
&lt;/p&gt;</description></item><item><title>D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.07733</link><description>&lt;p&gt;
&#36890;&#36807;D&#36866;&#24212;&#23454;&#29616;&#23398;&#20064;&#29575;&#33258;&#30001;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07733
&lt;/p&gt;
&lt;p&gt;
D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
D&#36866;&#24212;&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#26080;&#38656;&#22238;&#28335;&#25110;&#32447;&#24615;&#25628;&#32034;&#65292;&#24182;&#19988;&#27599;&#27493;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#20989;&#25968;&#20540;&#25110;&#26799;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36825;&#19968;&#31867;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#26080;&#36229;&#21442;&#25968;&#19988;&#25910;&#25947;&#36895;&#29575;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;SGD&#21644;Adam&#21464;&#20307;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#35813;&#26041;&#27861;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#65292;&#22312;&#21313;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#24212;&#29992;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#38382;&#39064;&#12290;&#24320;&#28304;&#23454;&#29616;&#22312; \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;
&lt;p&gt;
D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24322;&#27493;HFL&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#12289;&#23618;&#27425;&#21270;&#30340;IoT&#32593;&#32476;&#20013;&#30340;&#32852;&#21512;&#23398;&#20064;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20102;&#24322;&#27493;&#32858;&#21512;&#26469;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#65292;&#24182;&#22312;&#32593;&#20851;&#21644;&#20113;&#32423;&#21035;&#19978;&#37319;&#29992;&#35774;&#22791;&#36873;&#25321;&#21644;&#35774;&#22791;&#32593;&#20851;&#35843;&#24230;&#26469;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.06646</link><description>&lt;p&gt;
&#24322;&#27493;HFL&#65306;&#22312;&#20998;&#23618;IoT&#32593;&#32476;&#20013;&#36827;&#34892;&#39640;&#25928;&#12289;&#40065;&#26834;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064; (arXiv:2301.06646v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Async-HFL: Efficient and Robust Asynchronous Federated Learning in Hierarchical IoT Networks. (arXiv:2301.06646v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06646
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24322;&#27493;HFL&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#12289;&#23618;&#27425;&#21270;&#30340;IoT&#32593;&#32476;&#20013;&#30340;&#32852;&#21512;&#23398;&#20064;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20102;&#24322;&#27493;&#32858;&#21512;&#26469;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#65292;&#24182;&#22312;&#32593;&#20851;&#21644;&#20113;&#32423;&#21035;&#19978;&#37319;&#29992;&#35774;&#22791;&#36873;&#25321;&#21644;&#35774;&#22791;&#32593;&#20851;&#35843;&#24230;&#26469;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064; (FL) &#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#35774;&#22791;&#23398;&#20064;&#33539;&#24335;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#30495;&#23454;&#29289;&#32852;&#32593; (IoT) &#32593;&#32476;&#20013;&#37096;&#32626;FL&#20173;&#28982;&#38754;&#20020;&#30528;&#22810;&#37325;&#25361;&#25112;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#31995;&#32479;&#24322;&#36136;&#24615;&#12289;&#24847;&#22806;&#24310;&#36831;&#32773;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#38382;&#39064;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#25552;&#20379;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#20998;&#23618;&#21644;&#19981;&#21487;&#38752;&#30340;IoT&#32593;&#32476;&#20013;&#30340;&#25152;&#26377;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24322;&#27493;&#21644;&#20998;&#23618;&#26694;&#26550; (Async-HFL)&#65292;&#29992;&#20110;&#22312;&#24120;&#35265;&#30340;&#19977;&#23618;IoT&#32593;&#32476;&#32467;&#26500;&#20013;&#25191;&#34892;FL&#12290;&#38024;&#23545;&#22823;&#37327;&#19981;&#21516;&#30340;&#24310;&#36831;&#65292;Async-HFL&#22312;&#32593;&#20851;&#21644;&#20113;&#32423;&#21035;&#22343;&#37319;&#29992;&#24322;&#27493;&#32858;&#21512;&#65292;&#20174;&#32780;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#22312;&#31995;&#32479;&#24322;&#36136;&#24615;&#21644;&#24310;&#36831;&#32773;&#19979;&#24322;&#27493;HFL&#30340;&#25910;&#25947;&#36895;&#24230;&#28508;&#21147;&#65292;&#25105;&#20204;&#20998;&#21035;&#35774;&#35745;&#20102;&#32593;&#20851;&#32423;&#21035;&#30340;&#35774;&#22791;&#36873;&#25321;&#21644;&#20113;&#32423;&#21035;&#30340;&#35774;&#22791;&#32593;&#20851;&#35843;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Async-HFL&#22312;&#25910;&#25947;&#36895;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has gained increasing interest in recent years as a distributed on-device learning paradigm. However, multiple challenges remain to be addressed for deploying FL in real-world Internet-of-Things (IoT) networks with hierarchies. Although existing works have proposed various approaches to account data heterogeneity, system heterogeneity, unexpected stragglers and scalibility, none of them provides a systematic solution to address all of the challenges in a hierarchical and unreliable IoT network. In this paper, we propose an asynchronous and hierarchical framework (Async-HFL) for performing FL in a common three-tier IoT network architecture. In response to the largely varied delays, Async-HFL employs asynchronous aggregations at both the gateway and the cloud levels thus avoids long waiting time. To fully unleash the potential of Async-HFL in converging speed under system heterogeneities and stragglers, we design device selection at the gateway level and device-ga
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#20840;&#23616;&#23616;&#37096;&#27880;&#24847;&#32593;&#32476;&#65288;MGLAN&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#35875;&#35328;&#20256;&#25773;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#26377;&#25928;&#35299;&#20915;&#35875;&#35328;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.04341</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#31038;&#20132;&#23186;&#20307;&#35875;&#35328;&#26816;&#27979;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Meta Path-based Approach for Rumor Detection on Social Media. (arXiv:2301.04341v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#20840;&#23616;&#23616;&#37096;&#27880;&#24847;&#32593;&#32476;&#65288;MGLAN&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#35875;&#35328;&#20256;&#25773;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#26377;&#25928;&#35299;&#20915;&#35875;&#35328;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22312;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;&#24471;&#20154;&#20204;&#26356;&#20542;&#21521;&#20110;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#32780;&#19981;&#26159;&#20256;&#32479;&#26469;&#28304;&#33719;&#21462;&#26032;&#38395;&#12290;&#36825;&#31181;&#20844;&#20247;&#34892;&#20026;&#30340;&#36716;&#21464;&#20026;&#19968;&#20123;&#20154;&#20256;&#25773;&#20551;&#26032;&#38395;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#38543;&#20043;&#24102;&#26469;&#20102;&#36127;&#38754;&#30340;&#32463;&#27982;&#12289;&#25919;&#27835;&#21644;&#31038;&#20250;&#21518;&#26524;&#65292;&#20063;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#19981;&#20449;&#20219;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#20840;&#23616;&#23616;&#37096;&#27880;&#24847;&#32593;&#32476;&#65288;MGLAN&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#24322;&#36136;&#35875;&#35328;&#20256;&#25773;&#20013;&#25277;&#21462;&#32467;&#26500;&#29305;&#24449;&#26469;&#35299;&#20915;&#35875;&#35328;&#26816;&#27979;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MGLAN&#36229;&#36234;&#20854;&#20182;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#21644;&#32593;&#32476;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prominent role of social media in people's daily lives has made them more inclined to receive news through social networks than traditional sources. This shift in public behavior has opened doors for some to diffuse fake news on social media; and subsequently cause negative economic, political, and social consequences as well as distrust among the public.  There are many proposed methods to solve the rumor detection problem, most of which do not take full advantage of the heterogeneous nature of news propagation networks. With this intention, we considered a previously proposed architecture as our baseline and performed the idea of structural feature extraction from the heterogeneous rumor propagation over its architecture using the concept of meta path-based embeddings. We named our model Meta Path-based Global Local Attention Network (MGLAN). Extensive experimental analysis on three state-of-the-art datasets has demonstrated that MGLAN outperforms other models by capturing node-l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21407;&#22411;&#35299;&#37322;&#22120;&#65292;&#33021;&#22815;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#21160;&#20316;&#30340;&#20998;&#31867;&#65292;&#21516;&#26102;&#33021;&#22815;&#23558;&#31867;&#21644;&#21407;&#22411;&#24314;&#31435;&#25104;&#26356;&#26377;&#23618;&#27425;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00436</link><description>&lt;p&gt;
&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#20998;&#23618;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Explanations for Video Action Recognition. (arXiv:2301.00436v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21407;&#22411;&#35299;&#37322;&#22120;&#65292;&#33021;&#22815;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#21160;&#20316;&#30340;&#20998;&#31867;&#65292;&#21516;&#26102;&#33021;&#22815;&#23558;&#31867;&#21644;&#21407;&#22411;&#24314;&#31435;&#25104;&#26356;&#26377;&#23618;&#27425;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#26159;&#20998;&#35299;&#35270;&#35273;&#36755;&#20837;&#24182;&#25214;&#21040;&#36127;&#36131;&#20998;&#31867;&#30340;&#20856;&#22411;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#36825;&#20123;&#21407;&#22411;&#20043;&#38388;&#30340;&#20998;&#23618;&#20851;&#31995;&#65292;&#22240;&#27492;&#26080;&#27861;&#22312;&#26356;&#39640;&#23618;&#27425;&#65288;&#20363;&#22914;&#65292;&#27700;&#19978;&#36816;&#21160;&#65289;&#21644;&#26356;&#20302;&#23618;&#27425;&#65288;&#20363;&#22914;&#65292;&#28216;&#27891;&#65289;&#19978;&#35299;&#37322;&#35821;&#20041;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#20998;&#23618;&#20449;&#24687;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65306;&#24403;&#25105;&#20204;&#35266;&#23519;&#21040;&#27700;&#21644;&#20154;&#31867;&#27963;&#21160;&#65292;&#20294;&#27809;&#26377;&#26126;&#30830;&#30340;&#21160;&#20316;&#26102;&#65292;&#21487;&#20197;&#23558;&#20854;&#35782;&#21035;&#20026;&#27700;&#19978;&#36816;&#21160;&#30340;&#29238;&#31867;&#12290;&#21482;&#26377;&#35266;&#23519;&#21040;&#19968;&#20010;&#20154;&#22312;&#28216;&#27891;&#21518;&#65292;&#25105;&#20204;&#25165;&#33021;&#26126;&#30830;&#23558;&#20854;&#32454;&#20998;&#20026;&#28216;&#27891;&#21160;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23618;&#21407;&#22411;&#35299;&#37322;&#22120;&#65288;HIPE&#65289;&#26469;&#24314;&#31435;&#21407;&#22411;&#21644;&#31867;&#20043;&#38388;&#30340;&#20998;&#23618;&#20851;&#31995;&#12290; HIPE&#36890;&#36807;&#22312;&#31867;&#23618;&#27425;&#32467;&#26500;&#30340;&#22810;&#20010;&#32423;&#21035;&#19978;&#20998;&#35299;&#36755;&#20837;&#35270;&#39057;&#24103;&#65292;&#23454;&#29616;&#20102;&#35270;&#39057;&#21160;&#20316;&#20998;&#31867;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#36866;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#20043;&#22806;&#30340;&#20854;&#20182;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
To interpret deep neural networks, one main approach is to dissect the visual input and find the prototypical parts responsible for the classification. However, existing methods often ignore the hierarchical relationship between these prototypes, and thus can not explain semantic concepts at both higher level (e.g., water sports) and lower level (e.g., swimming). In this paper inspired by human cognition system, we leverage hierarchal information to deal with uncertainty: When we observe water and human activity, but no definitive action it can be recognized as the water sports parent class. Only after observing a person swimming can we definitively refine it to the swimming action. To this end, we propose HIerarchical Prototype Explainer (HIPE) to build hierarchical relations between prototypes and classes. HIPE enables a reasoning process for video action classification by dissecting the input video frames on multiple levels of the class hierarchy, our method is also applicable to ot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.12380</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#25193;&#23637;&#29289;&#29702;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#25968;&#25454;&#39537;&#21160;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (arXiv:2212.12380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#34987;&#25910;&#38598;&#65292;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#22312;&#29289;&#29702;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#20294;&#36890;&#24120;&#24456;&#38590;&#35782;&#21035;&#21644;&#25193;&#23637;&#65292;&#24182;&#19988;&#21463;&#20854;&#26377;&#38480;&#30340;&#34920;&#29616;&#21147;&#24433;&#21709;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24120;&#24120;&#20381;&#36182;&#31070;&#32463;&#32593;&#32476; (NNs) &#30340;&#32463;&#20856;&#40657;&#30418;&#26041;&#27861;&#36890;&#24120;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#32479;&#35745;&#27169;&#24335;&#65292;&#21363;&#20351;&#22312;&#25193;&#23637;&#26041;&#38754;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#28508;&#22312;&#30340;&#29289;&#29702;&#23450;&#24459;&#23436;&#20840;&#26080;&#35270;&#65292;&#22914;&#26524;&#22522;&#20110;&#23427;&#20204;&#20570;&#20915;&#31574;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#26368;&#36817;&#24320;&#21457;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476; (PCNNs) &#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992; NNs &#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558; PCNN &#25193;&#23637;&#21040;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#19982;&#32463;&#20856;&#28784;&#30418;&#21644;&#40657;&#30418;&#26041;&#27861;&#30340;&#24443;&#24213;&#27604;&#36739;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#22810;&#21306;&#22495;&#24314;&#31569;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#21306;&#22495;&#30340;&#28909;&#34892;&#20026;&#30001;&#33021;&#37327;&#24179;&#34913;&#26041;&#31243;&#24335;&#32479;&#27835;&#65292;&#20854;&#21442;&#25968;&#24517;&#39035;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#35782;&#21035;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#28041;&#21450;&#35768;&#22810;&#30456;&#20114;&#20316;&#29992;&#30340;&#32452;&#20214;&#26500;&#25104;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#31995;&#32479;&#65292;PCNNs &#20063;&#21487;&#20197;&#22312;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126; PCNN &#22312;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy.  In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23398;&#20064;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#36873;&#39033;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#32452;&#20214;&#65292;&#21152;&#36895;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.11726</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#20803;&#23398;&#20064;&#30340;&#37325;&#22797;&#20351;&#29992;&#30340;&#36873;&#39033;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reusable Options through Gradient-based Meta Learning. (arXiv:2212.11726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23398;&#20064;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#36873;&#39033;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#32452;&#20214;&#65292;&#21152;&#36895;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#23618;&#26041;&#27861;&#26377;&#28508;&#21147;&#20943;&#23569;&#26234;&#33021;&#20307;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38656;&#35201;&#25191;&#34892;&#30340;&#20915;&#31574;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#23547;&#25214;&#26377;&#29992;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#26102;&#38388;&#25277;&#35937;&#26041;&#38754;&#65292;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#36825;&#20123;&#26102;&#38388;&#25277;&#35937;&#65292;&#24418;&#25104;&#20102;&#36873;&#39033;&#12290;&#26412;&#25991;&#25351;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#20960;&#20010;&#19981;&#36275;&#20043;&#22788;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#21487;&#37325;&#22797;&#20351;&#29992;&#36873;&#39033;&#30340;&#24895;&#26395;&#21644;&#20351;&#29992;&#36825;&#20123;&#24895;&#26395;&#26469;&#23558;&#23398;&#20064;&#36873;&#39033;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#19968;&#31181;&#30446;&#26631;&#65292;&#26126;&#30830;&#28608;&#21169;&#33021;&#22815;&#20351;&#39640;&#23618;&#20915;&#31574;&#32773;&#33021;&#22815;&#22312;&#23569;&#25968;&#27493;&#39588;&#20013;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#36873;&#39033;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#32452;&#20214;&#65292;&#21152;&#36895;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical methods in reinforcement learning have the potential to reduce the amount of decisions that the agent needs to perform when learning new tasks. However, finding reusable useful temporal abstractions that facilitate fast learning remains a challenging problem. Recently, several deep learning approaches were proposed to learn such temporal abstractions in the form of options in an end-to-end manner. In this work, we point out several shortcomings of these methods and discuss their potential negative consequences. Subsequently, we formulate the desiderata for reusable options and use these to frame the problem of learning options as a gradient-based meta-learning problem. This allows us to formulate an objective that explicitly incentivizes options which allow a higher-level decision maker to adjust in few steps to different tasks. Experimentally, we show that our method is able to learn transferable components which accelerate learning and performs better than existing prior
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#27979;&#35797;&#36755;&#20837;&#20998;&#24067;&#38543;&#26102;&#38388;&#25345;&#32493;&#21464;&#21270;&#30340;&#27010;&#29575;&#26694;&#26550;PETAL&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#20351;&#29992;&#28304;&#27169;&#22411;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#26469;&#25512;&#26029;&#26102;&#27491;&#21017;&#21270;&#27169;&#22411;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#32456;&#36523;TTA&#12290;</title><link>http://arxiv.org/abs/2212.09713</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32456;&#36523;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#27010;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Framework for Lifelong Test-Time Adaptation. (arXiv:2212.09713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#27979;&#35797;&#36755;&#20837;&#20998;&#24067;&#38543;&#26102;&#38388;&#25345;&#32493;&#21464;&#21270;&#30340;&#27010;&#29575;&#26694;&#26550;PETAL&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#20351;&#29992;&#28304;&#27169;&#22411;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#26469;&#25512;&#26029;&#26102;&#27491;&#21017;&#21270;&#27169;&#22411;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#32456;&#36523;TTA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#22312;&#25512;&#29702;&#26102;&#38024;&#23545;&#26469;&#33258;&#19981;&#21516;&#30446;&#26631;&#22495;&#30340;&#27979;&#35797;&#36755;&#20837;&#26356;&#26032;&#39044;&#20808;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;TTA&#26041;&#27861;&#37117;&#20551;&#35774;&#30446;&#26631;&#22495;&#26159;&#38745;&#24577;&#30340;&#65292;&#21363;&#25152;&#26377;&#27979;&#35797;&#36755;&#20837;&#37117;&#26469;&#33258;&#21333;&#20010;&#30446;&#26631;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#27979;&#35797;&#36755;&#20837;&#20998;&#24067;&#21487;&#33021;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21457;&#29983;&#32456;&#36523;/&#25345;&#32493;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;TTA&#26041;&#27861;&#20063;&#32570;&#20047;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#32780;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#21457;&#29983;&#20998;&#24067;&#21464;&#21270;&#26102;&#65292;&#36825;&#19968;&#28857;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PETAL&#65288;&#20855;&#26377;&#33258;&#25105;&#35757;&#32451;&#20808;&#39564;&#30693;&#35782;&#30340;&#27010;&#29575;&#32456;&#36523;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65289;&#65292;&#23427;&#20351;&#29992;&#27010;&#29575;&#26041;&#27861;&#35299;&#20915;&#20102;&#32456;&#36523;TTA&#38382;&#39064;&#65292;&#33258;&#28982;&#22320;&#24471;&#21040;&#20102;&#65288;1&#65289;&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#26159;&#23398;&#29983;&#27169;&#22411;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#28304;&#27169;&#22411;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#26469;&#25512;&#26029;&#26102;&#27491;&#21017;&#21270;&#27169;&#22411;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA) is the problem of updating a pre-trained source model at inference time given test input(s) from a different target domain. Most existing TTA approaches assume the setting in which the target domain is stationary, i.e., all the test inputs come from a single target domain. However, in many practical settings, the test input distribution might exhibit a lifelong/continual shift over time. Moreover, existing TTA approaches also lack the ability to provide reliable uncertainty estimates, which is crucial when distribution shifts occur between the source and target domain. To address these issues, we present PETAL (Probabilistic lifElong Test-time Adaptation with seLf-training prior), which solves lifelong TTA using a probabilistic approach, and naturally results in (1) a student-teacher framework, where the teacher model is an exponential moving average of the student model, and (2) regularizing the model updates at inference time using the source model as a reg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#26469;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22522;&#32447;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2212.08123</link><description>&lt;p&gt;
&#24102;&#26377;&#38543;&#26426;&#38598;&#21512;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Bayesian posterior approximation with stochastic ensembles. (arXiv:2212.08123v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#26469;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22522;&#32447;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#38598;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#12290;&#23427;&#23558;&#38543;&#26426;&#26041;&#27861;&#65288;&#22914;dropout&#65289;&#19982;&#28145;&#24230;&#38598;&#25104;&#30456;&#32467;&#21512;&#65292;&#24182;&#23558;&#38543;&#26426;&#38598;&#21512;&#20844;&#24335;&#21270;&#20026;&#20998;&#24067;&#26063;&#65292;&#24182;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#35757;&#32451;&#20197;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29609;&#20855;&#38382;&#39064;&#21644;CIFAR&#22270;&#20687;&#20998;&#31867;&#19978;&#23454;&#29616;&#20102;&#22522;&#20110;Monte Carlo Dropout&#65292;DropConnect&#21644;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#29256;&#26412;&#30340;&#38543;&#26426;&#38598;&#21512;&#65292;&#24182;&#30452;&#25509;&#19982;&#21704;&#23494;&#39039;&#39532;&#23572;&#21487;&#22827;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#27604;&#36739;&#36136;&#37327;&#26469;&#27979;&#35797;&#21518;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#38598;&#21512;&#25552;&#20379;&#20102;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22522;&#32447;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ensembles of stochastic neural networks to approximate the Bayesian posterior, combining stochastic methods such as dropout with deep ensembles. The stochastic ensembles are formulated as families of distributions and trained to approximate the Bayesian posterior with variational inference. We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and a novel non-parametric version of dropout and evaluate them on a toy problem and CIFAR image classification. For both tasks, we test the quality of the posteriors directly against Hamiltonian Monte Carlo simulations. Our results show that stochastic ensembles provide more accurate posterior estimates than other popular baselines for Bayesian inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#25805;&#20316;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#34920;&#38754;&#21644;&#21033;&#29992;&#20854;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#25552;&#39640;3D&#24863;&#30693;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.05867</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#27773;&#36710;&#28608;&#20809;&#38647;&#36798;&#36890;&#36807;&#21344;&#25454;&#20272;&#35745;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
ALSO: Automotive Lidar Self-supervision by Occupancy estimation. (arXiv:2212.05867v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#25805;&#20316;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#34920;&#38754;&#21644;&#21033;&#29992;&#20854;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#25552;&#39640;3D&#24863;&#30693;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#22312;&#28857;&#20113;&#19978;&#36816;&#34892;&#30340;&#28145;&#24230;&#24863;&#30693;&#27169;&#22411;&#30340;&#39592;&#24178;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#39044;&#25991;&#26412;&#20219;&#21153;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#20219;&#21153;&#26159;&#37325;&#26500;3D&#28857;&#25152;&#37319;&#26679;&#30340;&#34920;&#38754;&#65292;&#24182;&#23558;&#28508;&#22312;&#21521;&#37327;&#29992;&#20316;&#24863;&#30693;&#22836;&#30340;&#36755;&#20837;&#12290;&#30452;&#35273;&#26159;&#65292;&#22914;&#26524;&#32593;&#32476;&#33021;&#22815;&#22312;&#20165;&#26377;&#31232;&#30095;&#36755;&#20837;&#28857;&#30340;&#24773;&#20917;&#19979;&#37325;&#26500;&#22330;&#26223;&#34920;&#38754;&#65292;&#21017;&#23427;&#21487;&#33021;&#36824;&#25429;&#33719;&#20102;&#19968;&#20123;&#35821;&#20041;&#20449;&#24687;&#30340;&#29255;&#27573;&#65292;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#23454;&#38469;&#24863;&#30693;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new self-supervised method for pre-training the backbone of deep perception models operating on point clouds. The core idea is to train the model on a pretext task which is the reconstruction of the surface on which the 3D points are sampled, and to use the underlying latent vectors as input to the perception head. The intuition is that if the network is able to reconstruct the scene surface, given only sparse input points, then it probably also captures some fragments of semantic information, that can be used to boost an actual perception task. This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object detection. In fact, it supports a single-stream pipeline, as opposed to most contrastive learning approaches, allowing training on limited resources. We conducted extensive experiments on various autonomous driving datasets, involving ve
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#37327;&#23376;&#32534;&#35793;&#22120;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#37327;&#23376;&#32534;&#35793;&#30340;&#25928;&#29575;&#21644;&#21697;&#36136;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#32534;&#35793;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2212.04508</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#32534;&#35793;&#22120;&#20248;&#21270;&#37327;&#23376;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Compiler Optimization for Quantum Computing Using Reinforcement Learning. (arXiv:2212.04508v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#37327;&#23376;&#32534;&#35793;&#22120;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#37327;&#23376;&#32534;&#35793;&#30340;&#25928;&#29575;&#21644;&#21697;&#36136;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#32534;&#35793;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#19968;&#20010;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#65292;&#19968;&#26086;&#34987;&#32534;&#30721;&#25104;&#37327;&#23376;&#30005;&#36335;&#65292;&#23601;&#24517;&#39035;&#22312;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#20043;&#21069;&#36827;&#34892;&#32534;&#35793;&#12290;&#19982;&#32463;&#20856;&#32534;&#35793;&#30456;&#20284;&#65292;&#37327;&#23376;&#32534;&#35793;&#26159;&#19968;&#20010;&#26377;&#24207;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#26377;&#35768;&#22810;&#32534;&#35793;&#27493;&#39588;&#21644;&#35768;&#22810;&#21487;&#20379;&#20248;&#21270;&#30340;&#25805;&#20316;&#12290;&#23613;&#31649;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#20294;&#37327;&#23376;&#35745;&#31639;&#26426;&#32534;&#35793;&#22120;&#30340;&#24320;&#21457;&#20173;&#28982;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#32570;&#20047;&#23545;&#26368;&#20339;&#25805;&#20316;&#24207;&#21015;&#12289;&#20860;&#23481;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#20849;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20960;&#21313;&#24180;&#32463;&#20856;&#32534;&#35793;&#22120;&#20248;&#21270;&#30340;&#32463;&#39564;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#32463;&#36807;&#20248;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#32534;&#35793;&#27969;&#31243;&#12290;&#36890;&#36807;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#23558;&#19981;&#21516;&#32534;&#35793;&#22120;&#21644;&#20248;&#21270;&#24037;&#20855;&#30340;&#25216;&#26415;&#32452;&#21512;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#32534;&#35793;&#27969;&#31243;&#20013;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550; - &#20511;&#21161;IBM&#30340;Qiskit&#21644;Google&#30340;Cirq&#21644;TensorFlow Quantum&#30340;&#19968;&#31995;&#21015;&#32534;&#35793;&#25805;&#20316;&#65292;&#26088;&#22312;&#25552;&#39640;&#37327;&#23376;&#32534;&#35793;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#20854;&#32534;&#35793;&#27969;&#31243;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#21551;&#21457;&#24335;&#30340;&#32570;&#28857;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#32534;&#35793;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any quantum computing application, once encoded as a quantum circuit, must be compiled before being executable on a quantum computer. Similar to classical compilation, quantum compilation is a sequential process with many compilation steps and numerous possible optimization passes. Despite the similarities, the development of compilers for quantum computing is still in its infancy -lacking mutual consolidation on the best sequence of passes, compatibility, adaptability, and flexibility. In this work, we take advantage of decades of classical compiler optimization and propose a reinforcement learning framework for developing optimized quantum circuit compilation flows. Through distinct constraints and a unifying interface, the framework supports the combination of techniques from different compilers and optimization tools in a single compilation flow. Experimental evaluations show that the proposed framework -set up with a selection of compilation passes from IBM's Qiskit and Quanti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15136</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;2D&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collective Intelligence for 2D Push Manipulation with Mobile Robots. (arXiv:2211.15136v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#31995;&#32479;&#36890;&#24120;&#34920;&#29616;&#20986;&#33021;&#22815;&#33258;&#25105;&#32452;&#32455;&#21644;&#36866;&#24212;&#21464;&#21270;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#20154;&#24037;&#31995;&#32479;&#32570;&#20047;&#36825;&#31181;&#31561;&#25928;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#36827;&#34892;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#30340;&#38598;&#20307;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20174;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#27966;&#29983;&#30340;&#35268;&#21010;&#22120;&#25552;&#28860;&#20026;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#65292;&#25105;&#20204;&#30340;&#22810;&#26426;&#22120;&#20154;&#25512;&#21160;&#25805;&#20316;&#31995;&#32479;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#36866;&#24212;&#22806;&#37096;&#25200;&#21160;&#21644;&#29615;&#22659;&#21464;&#21270;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While natural systems often present collective intelligence that allows them to self-organize and adapt to changes, the equivalent is missing in most artificial systems. We explore the possibility of such a system in the context of cooperative 2D push manipulations using mobile robots. Although conventional works demonstrate potential solutions for the problem in restricted settings, they have computational and learning difficulties. More importantly, these systems do not possess the ability to adapt when facing environmental changes. In this work, we show that by distilling a planner derived from a differentiable soft-body physics simulator into an attention-based neural network, our multi-robot push manipulation system achieves better performance than baselines. In addition, our system also generalizes to configurations not seen during training and is able to adapt toward task completions when external turbulence and environmental changes are applied. Supplementary videos can be foun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.15072</link><description>&lt;p&gt;
FaiREE&#65306;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#20445;&#35777;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee. (arXiv:2211.15072v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#21644;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#30340;&#20844;&#24179;&#20445;&#35777;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#26679;&#26412;&#37327;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#36829;&#21453;&#20844;&#24179;&#24615;&#65292;&#32780;&#36825;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#19979;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;FaiREE&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#26426;&#20250;&#24179;&#31561;&#65292;&#24179;&#34913;&#20960;&#29575;&#65292;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#31561;&#65289;&#24182;&#23454;&#29616;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#29702;&#35770;&#20445;&#35777;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#23454;&#39564;&#25903;&#25345;&#12290;FaiREE&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depends on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm that can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfy various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>GAMMT&#26159;&#19968;&#31181;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#20010;Transformer&#22788;&#29702;&#27169;&#31946;&#19981;&#30830;&#23450;&#30340;&#27010;&#29575;&#12290;&#35813;&#27169;&#22411;&#26377;&#26395;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2211.09812</link><description>&lt;p&gt;
GAMMT: &#20351;&#29992;&#22810;&#20010;Transformer&#30340;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GAMMT: Generative Ambiguity Modeling Using Multiple Transformers. (arXiv:2211.09812v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09812
&lt;/p&gt;
&lt;p&gt;
GAMMT&#26159;&#19968;&#31181;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#20010;Transformer&#22788;&#29702;&#27169;&#31946;&#19981;&#30830;&#23450;&#30340;&#27010;&#29575;&#12290;&#35813;&#27169;&#22411;&#26377;&#26395;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;GAMMT&#65288;&#20351;&#29992;&#22810;&#20010;Transformer&#30340;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#27010;&#29575;&#38598;&#21512;&#30340;&#24207;&#21015;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35748;&#20026;&#24207;&#21015;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#26159;&#27169;&#31946;&#30340;&#65292;&#24182;&#21463;&#21040;&#19968;&#32452;&#27010;&#29575;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;GAMMT&#37319;&#29992;&#20102;&#22810;&#20010;&#24182;&#34892;&#30340;Transformer&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#30456;&#20114;&#20851;&#32852;&#65292;&#20801;&#35768;&#36817;&#20284;&#22788;&#29702;&#27169;&#31946;&#19981;&#30830;&#23450;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#29983;&#25104;&#29305;&#24615;&#36824;&#20351;&#24471;&#36755;&#20837;&#31526;&#21495;&#21644;&#24207;&#21015;&#21487;&#20197;&#26377;&#22810;&#20010;&#34920;&#24449;&#24418;&#24335;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#27169;&#22411;&#23578;&#26410;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24314;&#27169;&#20855;&#26377;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#24207;&#21015;&#30340;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel model called GAMMT (Generative Ambiguity Models using Multiple Transformers) for sequential data that is based on sets of probabilities. Unlike conventional models, our approach acknowledges that the data generation process of a sequence is not deterministic, but rather ambiguous and influenced by a set of probabilities. To capture this ambiguity, GAMMT employs multiple parallel transformers that are linked by a selection mechanism, allowing for the approximation of ambiguous probabilities. The generative nature of our approach also enables multiple representations of input tokens and sequences. While our models have not yet undergone experimental validation, we believe that our model has great potential to achieve high quality and diversity in modeling sequences with uncertain data generation processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;PU&#25439;&#22833;&#20989;&#25968;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;PU GNN&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;GraphSMOTE&#31639;&#27861;&#26469;&#22788;&#29702;P2E MMORPGs&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.08604</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19982;&#19981;&#24179;&#34913;PU&#26631;&#31614;&#30340;P2E MMORPGs&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels. (arXiv:2211.08604v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;PU&#25439;&#22833;&#20989;&#25968;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;PU GNN&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;GraphSMOTE&#31639;&#27861;&#26469;&#22788;&#29702;P2E MMORPGs&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#22810;&#20154;&#22312;&#32447;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#65292;&#28216;&#25103;&#28857;&#21345;&#33021;&#22815;&#30452;&#25509;&#36716;&#25442;&#20026;&#27604;&#29305;&#24065;&#12289;&#20197;&#22826;&#22346;&#25110;Klaytn&#31561;&#21152;&#23494;&#36135;&#24065;&#65292;&#22240;&#27492;play-to-earn&#65288;P2E&#65289;&#31995;&#32479;&#30340;&#20986;&#29616;&#20351;&#24471;&#28216;&#25103;&#29289;&#21697;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#20215;&#20540;&#20132;&#25442;&#27604;&#20197;&#24448;&#26356;&#21152;&#39057;&#32321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;PU GNN&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;PU&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#29609;&#23478;&#30340;&#28216;&#25103;&#34892;&#20026;&#12289;P2E&#20195;&#24065;&#20132;&#26131;&#27169;&#24335;&#65292;&#21516;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;GraphSMOTE&#31639;&#27861;&#22788;&#29702;&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#38469;&#30340;P2E MMORPGs&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of play-to-earn (P2E) systems in massively multiplayer online role-playing games (MMORPGs) has made in-game goods interchangeable with real-world values more than ever before. The goods in the P2E MMORPGs can be directly exchanged with cryptocurrencies such as Bitcoin, Ethereum, or Klaytn via blockchain networks. Unlike traditional in-game goods, once they had been written to the blockchains, P2E goods cannot be restored by the game operation teams even with chargeback fraud such as payment fraud, cancellation, or refund. To tackle the problem, we propose a novel chargeback fraud prediction method, PU GNN, which leverages graph attention networks with PU loss to capture both the players' in-game behavior with P2E token transaction patterns. With the adoption of modified GraphSMOTE, the proposed model handles the imbalanced distribution of labels in chargeback fraud datasets. The conducted experiments on three real-world P2E MMORPG datasets demonstrate that PU GNN achi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36890;&#29992;&#30340;&#27169;&#22411;&#65292;&#23558;&#31435;&#26041;&#20307;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#32593;&#32476;&#65292;&#29983;&#25104;&#30340;&#32593;&#32476;&#26356;&#21152;&#25509;&#36817;&#23454;&#38469;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2211.02811</link><description>&lt;p&gt;
&#20174;&#31435;&#26041;&#20307;&#21040;&#32593;&#32476;&#65306;&#29992;&#20110;&#21512;&#25104;&#32593;&#32476;&#29983;&#25104;&#30340;&#24555;&#36895;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Cubes to Networks: Fast Generic Model for Synthetic Networks Generation. (arXiv:2211.02811v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36890;&#29992;&#30340;&#27169;&#22411;&#65292;&#23558;&#31435;&#26041;&#20307;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#32593;&#32476;&#65292;&#29983;&#25104;&#30340;&#32593;&#32476;&#26356;&#21152;&#25509;&#36817;&#23454;&#38469;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#20998;&#26512;&#21644;&#31435;&#26041;&#20307;&#65288;&#21363;&#22810;&#32500;&#25968;&#25454;&#38598;&#65289;&#30340;&#25506;&#32034;&#26159;&#20004;&#20010;&#24403;&#21069;&#19981;&#21516;&#31574;&#30053;&#30340;&#29420;&#31435;&#30740;&#31350;&#39046;&#22495;&#12290;&#20026;&#20102;&#36890;&#36807;&#29420;&#29305;&#30340;&#32593;&#32476;&#22495;&#26041;&#27861;&#33719;&#24471;&#20851;&#20110;&#31435;&#26041;&#20307;&#21160;&#24577;&#30340;&#26356;&#22810;&#27934;&#23519;&#65292;&#24182;&#33719;&#24471;&#20016;&#23500;&#30340;&#21512;&#25104;&#32593;&#32476;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#23558;&#31435;&#26041;&#20307;&#36716;&#21270;&#20026;&#30456;&#24212;&#32593;&#32476;&#30340;&#36716;&#21270;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FGM&#65292;&#19968;&#31181;&#24555;&#36895;&#36890;&#29992;&#27169;&#22411;&#65292;&#23558;&#26679;&#26412;&#37325;&#22609;&#20026;&#33410;&#28857;&#65292;&#24182;&#22312;&#26368;&#36817;&#37051;&#27010;&#24565;&#30340;&#25351;&#23548;&#19979;&#24341;&#23548;&#32593;&#32476;&#21160;&#24577;&#12290;&#36890;&#36807;&#19982;&#20197;&#21069;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;FGM&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#26356;&#25509;&#36817;&#23454;&#38469;&#32593;&#32476;&#30340;&#20856;&#22411;&#27169;&#24335;&#30340;&#32593;&#32476;&#65292;&#20363;&#22914;&#26356;&#30495;&#23454;&#30340;&#24230;&#20998;&#24067;&#12289;&#24130;&#24459;&#24179;&#22343;&#26368;&#36817;&#37051;&#24230;&#20381;&#36182;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#32593;&#32476;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#34928;&#20943;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#31435;&#26041;&#20307;&#35780;&#20272;FGM&#29983;&#25104;&#30340;&#32593;&#32476;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Analytical explorations on complex networks and cubes (i.e., multi-dimensional datasets) are currently two separate research fields with different strategies. To gain more insights into cube dynamics via unique network-domain methodologies and to obtain abundant synthetic networks, we need a transformation approach from cubes into associated networks. To this end, we propose FGM, a fast generic model converting cubes into interrelated networks, whereby samples are remodeled into nodes and network dynamics are guided under the concept of nearest-neighbor searching. Through comparison with previous models, we show that FGM can cost-efficiently generate networks exhibiting typical patterns more closely aligned to factual networks, such as more authentic degree distribution, power-law average nearest-neighbor degree dependency, and the influence decay phenomenon we consider vital for networks. Furthermore, we evaluate the networks that FGM generates through various cubes. Results show that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#19968;&#33324;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#26680;&#24515;&#38598;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2210.04260</link><description>&lt;p&gt;
Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#30340;&#26680;&#24515;&#38598;
&lt;/p&gt;
&lt;p&gt;
Coresets for Wasserstein Distributionally Robust Optimization Problems. (arXiv:2210.04260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#19968;&#33324;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#26680;&#24515;&#38598;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;WDRO&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#21547;&#31946;&#25968;&#25454;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#40065;&#26834;&#24615;&#30340;&#27969;&#34892;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;WDRO&#30340;&#22797;&#26434;&#24230;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26159;&#31105;&#27490;&#24615;&#30340;&#65292;&#22240;&#20026;&#35299;&#20915;&#20854;&#8220;&#26497;&#23567;&#26497;&#22823;&#8221;&#34920;&#36798;&#24335;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#38024;&#23545;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65288;&#20363;&#22914;&#36923;&#36753;&#22238;&#24402;&#65289;&#30340;&#24555;&#36895;WDRO&#35757;&#32451;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#22823;&#35268;&#27169;WDRO&#30340;&#35774;&#35745;&#39640;&#25928;&#31639;&#27861;&#30340;&#30740;&#31350;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#26680;&#24515;&#38598;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21387;&#32553;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20943;&#23569;&#35768;&#22810;&#20248;&#21270;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26500;&#24314;&#19968;&#33324;WDRO&#38382;&#39064;&#30340;$\epsilon$-coreset&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23613;&#31649;&#30001;&#20110;&#19981;&#30830;&#23450;&#24615;&#32780;&#33719;&#21462;WDRO&#30340;&#20256;&#32479;&#26680;&#24515;&#38598;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein distributionally robust optimization (\textsf{WDRO}) is a popular model to enhance the robustness of machine learning with ambiguous data. However, the complexity of \textsf{WDRO} can be prohibitive in practice since solving its ``minimax'' formulation requires a great amount of computation. Recently, several fast \textsf{WDRO} training algorithms for some specific machine learning tasks (e.g., logistic regression) have been developed. However, the research on designing efficient algorithms for general large-scale \textsf{WDRO}s is still quite limited, to the best of our knowledge. \textit{Coreset} is an important tool for compressing large dataset, and thus it has been widely applied to reduce the computational complexities for many optimization problems. In this paper, we introduce a unified framework to construct the $\epsilon$-coreset for the general \textsf{WDRO} problems. Though it is challenging to obtain a conventional coreset for \textsf{WDRO} due to the uncertaint
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36741;&#21161;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#31934;&#24230;&#21644;&#25910;&#25947;&#26102;&#38388;</title><link>http://arxiv.org/abs/2210.02614</link><description>&lt;p&gt;
&#24102;&#26377;&#26381;&#21153;&#22120;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#25552;&#39640;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Server Learning: Enhancing Performance for Non-IID Data. (arXiv:2210.02614v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02614
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36741;&#21161;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#31934;&#24230;&#21644;&#25910;&#25947;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#20351;&#29992;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#20854;&#20013;&#21327;&#35843;&#26381;&#21153;&#22120;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#23458;&#25143;&#31471;&#25968;&#25454;&#19981;&#29420;&#31435;&#21516;&#20998;&#24067;&#26102;&#65292;FL&#21487;&#33021;&#20250;&#36973;&#21463;&#24615;&#33021;&#19979;&#38477;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#26032;&#30340;&#34917;&#20805;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#24615;&#33021;&#19979;&#38477;&#65292;&#21363;&#20801;&#35768;&#26381;&#21153;&#22120;&#20174;&#23567;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#36741;&#21161;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26381;&#21153;&#22120;&#25968;&#25454;&#38598;&#24456;&#23567;&#19988;&#20854;&#20998;&#24067;&#19982;&#25152;&#26377;&#23458;&#25143;&#31471;&#32858;&#21512;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#20063;&#21487;&#20197;&#22312;&#27169;&#22411;&#31934;&#24230;&#21644;&#25910;&#25947;&#26102;&#38388;&#26041;&#38754;&#23454;&#29616;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a means of distributed learning using local data stored at clients with a coordinating server. Recent studies showed that FL can suffer from poor performance and slower convergence when training data at clients are not independent and identically distributed. Here we consider a new complementary approach to mitigating this performance degradation by allowing the server to perform auxiliary learning from a small dataset. Our analysis and experiments show that this new approach can achieve significant improvements in both model accuracy and convergence time even when the server dataset is small and its distribution differs from that of the aggregated data from all clients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26925;&#29699;&#30340;&#38556;&#30861;&#29289;&#35782;&#21035;&#19982;&#27979;&#36895;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#22522;&#20110;&#26925;&#29699;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#24102;&#26377;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#65292;&#20854;&#36816;&#34892;&#36895;&#24230;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#24555;&#19988;&#19981;&#38656;&#35201;&#39044;&#20808;&#30693;&#36947;&#32858;&#31867;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.14233</link><description>&lt;p&gt;
&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#19979;&#24555;&#36895;&#36816;&#21160;&#35268;&#21010;&#30340;&#38556;&#30861;&#29289;&#35782;&#21035;&#19982;&#26925;&#29699;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Obstacle Identification and Ellipsoidal Decomposition for Fast Motion Planning in Unknown Dynamic Environments. (arXiv:2209.14233v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26925;&#29699;&#30340;&#38556;&#30861;&#29289;&#35782;&#21035;&#19982;&#27979;&#36895;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#22522;&#20110;&#26925;&#29699;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#24102;&#26377;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#65292;&#20854;&#36816;&#34892;&#36895;&#24230;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#24555;&#19988;&#19981;&#38656;&#35201;&#39044;&#20808;&#30693;&#36947;&#32858;&#31867;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#20154;&#31995;&#32479;&#20013;&#65292;&#36991;&#20813;&#19982;&#26410;&#30693;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#30896;&#25758;&#26159;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26925;&#29699;&#26469;&#35782;&#21035;&#38556;&#30861;&#29289;&#65292;&#24182;&#20272;&#35745;&#20854;&#32447;&#24615;&#21644;&#35282;&#36895;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20219;&#20309;&#29289;&#20307;&#37117;&#21487;&#20197;&#36817;&#20284;&#34920;&#31034;&#20026;&#26925;&#29699;&#30340;&#29702;&#24565;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;Kyachiyan&#31639;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#30693;&#36947;&#32858;&#31867;&#25968;&#30446;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#26102;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#22522;&#20110;&#26925;&#29699;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#21305;&#37197;&#32473;&#23450;&#30340;&#20004;&#20010;&#26102;&#38388;&#25509;&#36817;&#30340;&#28857;&#24103;&#30340;&#38556;&#30861;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#24102;&#26377;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;&#20855;&#26377;&#26059;&#36716;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#20854;&#20182;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collision avoidance in the presence of dynamic obstacles in unknown environments is one of the most critical challenges for unmanned systems. In this paper, we present a method that identifies obstacles in terms of ellipsoids to estimate linear and angular obstacle velocities. Our proposed method is based on the idea of any object can be approximately expressed by ellipsoids. To achieve this, we propose a method based on variational Bayesian estimation of Gaussian mixture model, the Kyachiyan algorithm, and a refinement algorithm. Our proposed method does not require knowledge of the number of clusters and can operate in real-time, unlike existing optimization-based methods. In addition, we define an ellipsoid-based feature vector to match obstacles given two timely close point frames. Our method can be applied to any environment with static and dynamic obstacles, including the ones with rotating obstacles. We compare our algorithm with other clustering methods and show that when coupl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#20223;&#36816;&#21160;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#65292;&#24182;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#65292;&#26412;&#25991;&#25104;&#21151;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.05135</link><description>&lt;p&gt;
&#35821;&#35328;&#31526;&#21495;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#31034;&#33539;&#30340;&#20154;&#26426;&#20132;&#20114;&#20013;&#65292;&#20307;&#24863;&#25163;&#35821;&#25163;&#25351;&#25340;&#20889;&#30340;&#32763;&#35793;&#26426;&#22120;&#20154;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#20223;&#36816;&#21160;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#65292;&#24182;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#65292;&#26412;&#25991;&#25104;&#21151;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26426;&#22120;&#20154;&#20013;&#32454;&#33268;&#30340;&#21160;&#20316;&#26159;&#19968;&#20010;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#25163;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#26080;&#39069;&#22806;&#20449;&#24687;&#19979;&#30340;&#29087;&#32451;&#36816;&#21160;&#27169;&#20223;&#65292;&#20197;&#33719;&#24471;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;URDF&#27169;&#22411;&#65292;&#24182;&#20351;&#27599;&#20010;&#20851;&#33410;&#21482;&#26377;&#19968;&#20010;&#33268;&#21160;&#22120;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(&#21363;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#21644;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;)&#26469;&#35757;&#32451;&#19968;&#31181;&#33021;&#22815;&#22797;&#21046;&#31034;&#33539;&#36816;&#21160;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22522;&#20110;&#21442;&#32771;&#36816;&#21160;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20845;&#20010;&#23545;&#24212;&#20110;&#25340;&#20889;&#23383;&#27597;&#30340;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning fine-grained movements is a challenging topic in robotics, particularly in the context of robotic hands. One specific instance of this challenge is the acquisition of fingerspelling sign language in robots. In this paper, we propose an approach for learning dexterous motor imitation from video examples without additional information. To achieve this, we first build a URDF model of a robotic hand with a single actuator for each joint. We then leverage pre-trained deep vision models to extract the 3D pose of the hand from RGB videos. Next, using state-of-the-art reinforcement learning algorithms for motion imitation (namely, proximal policy optimization and soft actor-critic), we train a policy to reproduce the movement extracted from the demonstrations. We identify the optimal set of hyperparameters for imitation based on a reference motion. Finally, we demonstrate the generalizability of our approach by testing it on six different tasks, corresponding to fingerspelled letters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#20998;&#26512;AM&#31867;&#22411;&#30340;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#30740;&#31350;&#22522;&#20110;&#38750;&#21333;&#35843;&#30340;$j$-&#27493;&#20805;&#20998;&#20943;&#23569;&#26465;&#20214;&#21644;Kurdyka-Lojasiewicz&#65288;KL&#65289;&#24615;&#36136;&#65292;&#24182;&#22312;KL&#25351;&#25968;$ \theta $&#22312;$ [0,1) $&#21464;&#21270;&#26102;&#23637;&#31034;&#20102;&#35814;&#32454;&#30340;&#23616;&#37096;&#25910;&#25947;&#36895;&#29575;&#21644;&#23616;&#37096;R-&#32447;&#24615;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2208.14318</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates of Training Deep Neural Networks via Alternating Minimization Methods. (arXiv:2208.14318v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#20998;&#26512;AM&#31867;&#22411;&#30340;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#30740;&#31350;&#22522;&#20110;&#38750;&#21333;&#35843;&#30340;$j$-&#27493;&#20805;&#20998;&#20943;&#23569;&#26465;&#20214;&#21644;Kurdyka-Lojasiewicz&#65288;KL&#65289;&#24615;&#36136;&#65292;&#24182;&#22312;KL&#25351;&#25968;$ \theta $&#22312;$ [0,1) $&#21464;&#21270;&#26102;&#23637;&#31034;&#20102;&#35814;&#32454;&#30340;&#23616;&#37096;&#25910;&#25947;&#36895;&#29575;&#21644;&#23616;&#37096;R-&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#38750;&#20984;&#24615;&#21644;&#38750;&#21487;&#20998;&#24615;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20132;&#26367;&#26368;&#23567;&#21270;&#65288;AM&#65289;&#26041;&#27861;&#22312;DNN&#30340;&#22797;&#21512;&#32467;&#26500;&#26041;&#38754;&#36827;&#34892;&#20102;&#25286;&#20998;&#65292;&#24182;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#31038;&#21306;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;AM&#31867;&#22411;&#30340;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#38750;&#21333;&#35843;&#30340;$j$-&#27493;&#20805;&#20998;&#20943;&#23569;&#26465;&#20214;&#21644;Kurdyka-Lojasiewicz&#65288;KL&#65289;&#24615;&#36136;&#65292;&#36825;&#25918;&#23485;&#20102;&#35774;&#35745;&#19979;&#38477;&#31639;&#27861;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;KL&#25351;&#25968;$ \theta $&#22312;$ [0,1) $&#21464;&#21270;&#26102;&#23637;&#31034;&#20102;&#35814;&#32454;&#30340;&#23616;&#37096;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#22312;&#26356;&#24378;&#30340;$j$-&#27493;&#20805;&#20998;&#20943;&#23569;&#26465;&#20214;&#19979;&#35752;&#35770;&#20102;&#23616;&#37096;R-&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) is an important and challenging optimization problem in machine learning due to its non-convexity and non-separable structure. The alternating minimization (AM) approaches split the composition structure of DNNs and have drawn great interest in the deep learning and optimization communities. In this paper, we propose a unified framework for analyzing the convergence rate of AM-type network training methods. Our analysis is based on the non-monotone $j$-step sufficient decrease conditions and the Kurdyka-Lojasiewicz (KL) property, which relaxes the requirement of designing descent algorithms. We show the detailed local convergence rate if the KL exponent $\theta$ varies in $[0,1)$. Moreover, the local R-linear convergence is discussed under a stronger $j$-step sufficient decrease condition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#29702;&#35770;&#26469;&#35782;&#21035;&#21487;&#20877;&#29983;&#30005;&#21147;&#20998;&#24067;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#20013;&#22826;&#38451;&#33021;&#30005;&#27744;&#26495;&#30340;&#25176;&#31649;&#33021;&#21147;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11543</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#22797;&#26434;&#32593;&#32476;&#35782;&#21035;&#21487;&#20877;&#29983;&#30005;&#21147;&#20998;&#24067;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A methodology for identifying resiliency in renewable electrical distribution system using complex network. (arXiv:2208.11543v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#29702;&#35770;&#26469;&#35782;&#21035;&#21487;&#20877;&#29983;&#30005;&#21147;&#20998;&#24067;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#20013;&#22826;&#38451;&#33021;&#30005;&#27744;&#26495;&#30340;&#25176;&#31649;&#33021;&#21147;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30005;&#21147;&#37197;&#30005;&#31995;&#32479;&#24191;&#27867;&#37319;&#29992;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#65288;DER&#65289;&#20197;&#28385;&#36275;&#33021;&#28304;&#38656;&#27714;&#65292;&#26222;&#36941;&#35748;&#20026;&#36825;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65288;&#22914;&#38388;&#27463;&#24615;&#21487;&#29992;&#24615;&#12289;&#22825;&#27668;&#26465;&#20214;&#30340;&#21160;&#24577;&#21464;&#21270;&#12289;&#38750;&#32447;&#24615;&#31561;&#65289;&#21487;&#33021;&#23545;&#30005;&#32593;&#36816;&#33829;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#29702;&#35770;&#26469;&#35782;&#21035;&#24102;&#26377;&#22826;&#38451;&#33021;&#20809;&#20239;&#21457;&#30005;&#30340;&#37197;&#30005;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#26465;&#20214;&#33719;&#24471;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#22797;&#26434;&#30456;&#20851;&#32593;&#32476;&#65292;&#24182;&#35745;&#31639;&#20102;&#21508;&#31181;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35782;&#21035;&#32593;&#32476;&#30340;&#24377;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#20013;&#22826;&#38451;&#33021;&#30005;&#27744;&#26495;&#30340;&#25176;&#31649;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#19981;&#33391;&#26465;&#20214;&#19979;&#20445;&#25345;&#31995;&#32479;&#30340;&#24377;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Electrical Distribution Systems are extensively penetrated with the Distributed Energy Resources (DERs) to cater the energy demands with general perception that it enhances the system resiliency. However, it may be adverse for the grid operation due to various factors like its intermittent availability, dynamics in weather condition, introduction of nonlinearity, complexity etc. This needs a detailed understanding of system resiliency that our method proposes here. We introduce a methodology using complex network theory to identify the resiliency of distribution system when incorporated with Solar PV generation under various undesirable configurations. Complex correlated networks for different conditions were obtained and various network parameters were computed for identifying the resiliency of those networks. The proposed methodology identifies the hosting capacity of solar panels in the system while maintaining the resiliency under different unwanted conditions hence helps
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25277;&#22870;&#27744;&#65288;Lottery Pools&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#24179;&#22343;&#30456;&#37051;&#23398;&#20064;&#24471;&#21040;&#30340;&#23376;&#32593;&#32476;&#30340;&#26435;&#37325;&#25110;&#32773;&#36890;&#36807;&#31616;&#21333;&#30340;&#25554;&#20540;&#31574;&#30053;&#23545;&#36845;&#20195;&#21098;&#26525;&#30830;&#23450;&#30340;&#23376;&#32593;&#32476;&#25191;&#34892;&#8220;&#38598;&#25104;&#8221;&#65292;&#20174;&#32780;&#25552;&#39640;&#25277;&#22870;&#31080;&#65288;LTs&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.10842</link><description>&lt;p&gt;
&#25277;&#22870;&#27744;&#65306;&#36890;&#36807;&#25554;&#20540;&#31080;&#25454;&#32780;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#25512;&#29702;&#25104;&#26412;&#26469;&#33719;&#32988;
&lt;/p&gt;
&lt;p&gt;
Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost. (arXiv:2208.10842v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25277;&#22870;&#27744;&#65288;Lottery Pools&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#24179;&#22343;&#30456;&#37051;&#23398;&#20064;&#24471;&#21040;&#30340;&#23376;&#32593;&#32476;&#30340;&#26435;&#37325;&#25110;&#32773;&#36890;&#36807;&#31616;&#21333;&#30340;&#25554;&#20540;&#31574;&#30053;&#23545;&#36845;&#20195;&#21098;&#26525;&#30830;&#23450;&#30340;&#23376;&#32593;&#32476;&#25191;&#34892;&#8220;&#38598;&#25104;&#8221;&#65292;&#20174;&#32780;&#25552;&#39640;&#25277;&#22870;&#31080;&#65288;LTs&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#22870;&#31080;&#65288;LTs&#65289;&#21487;&#20197;&#21457;&#29616;&#31934;&#30830;&#19988;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#20123;&#23376;&#32593;&#32476;&#21487;&#20197;&#34987;&#21333;&#29420;&#35757;&#32451;&#20197;&#21305;&#37197;&#23494;&#38598;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#32780;&#38598;&#25104;&#65288;Ensemble&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21476;&#32769;&#30340;&#32463;&#36807;&#26102;&#38388;&#39564;&#35777;&#30340;&#25216;&#24039;&#20043;&#19968;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;LTs&#30340;&#32972;&#26223;&#19979;&#65292;&#38598;&#25104;&#30340;&#22909;&#22788;&#20250;&#34987;&#31232;&#30095;&#23376;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#25152;&#21066;&#24369;&#12290;&#26412;&#25991;&#39318;&#20808;&#35266;&#23519;&#21040;&#30452;&#25509;&#24179;&#22343;&#30456;&#37051;&#23398;&#20064;&#24471;&#21040;&#30340;&#27425;&#32423;&#23376;&#32593;&#32476;&#30340;&#26435;&#37325;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LTs&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#40723;&#33310;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31616;&#21333;&#30340;&#25554;&#20540;&#31574;&#30053;&#23545;&#36845;&#20195;&#21098;&#26525;&#30830;&#23450;&#30340;&#23376;&#32593;&#32476;&#25191;&#34892;&#8220;&#38598;&#25104;&#8221;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#25277;&#22870;&#27744;&#12290;&#19982;&#27809;&#26377;&#24615;&#33021;&#22686;&#30410;&#30340;&#26420;&#32032;&#38598;&#25104;&#19981;&#21516;&#65292;&#25193;&#23637;&#25277;&#22870;&#27744;&#21487;&#20197;&#25552;&#39640;&#27599;&#20010;&#21333;&#29420;&#30340;&#23376;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lottery tickets (LTs) is able to discover accurate and sparse subnetworks that could be trained in isolation to match the performance of dense networks. Ensemble, in parallel, is one of the oldest time-proven tricks in machine learning to improve performance by combining the output of multiple independent models. However, the benefits of ensemble in the context of LTs will be diluted since ensemble does not directly lead to stronger sparse subnetworks, but leverages their predictions for a better decision. In this work, we first observe that directly averaging the weights of the adjacent learned subnetworks significantly boosts the performance of LTs. Encouraged by this observation, we further propose an alternative way to perform an 'ensemble' over the subnetworks identified by iterative magnitude pruning via a simple interpolating strategy. We call our method Lottery Pools. In contrast to the naive ensemble which brings no performance gains to each single subnetwork, Lottery Pools yi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2208.07316</link><description>&lt;p&gt;
MENLI: &#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#34987;&#25552;&#20986;&#30340;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26131;&#21463;&#21040;&#23545;&#20449;&#24687;&#27491;&#30830;&#24615;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#27492;&#31867;&#27169;&#22411;&#26159;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36825;&#31181;&#25351;&#26631;&#26356;&#36866;&#21512;&#24314;&#27169;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#27604;&#26368;&#36817;&#30340;BERT&#22522;&#30784;&#25351;&#26631;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#20302;&#20110;SOTA MT&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#25351;&#26631;&#19982;&#25105;&#20204;&#30340;NLI&#25351;&#26631;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#26082;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65288;15&#65285;-30&#65285;&#65289;&#65292;&#21448;&#33719;&#24471;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#26356;&#39640;&#30340;&#36136;&#37327;&#25351;&#26631;&#65288;+5&#65285;&#33267;30&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#29616;&#23454;&#30340;&#26080;&#38480;&#21046;&#29305;&#24449;&#34920;&#31034;&#21464;&#20307;&#65292;&#23427;&#32771;&#34385;&#20102;&#32593;&#32476;&#30340;&#26377;&#38480;&#34920;&#36798;&#24615;&#12290;&#32467;&#21512;&#35760;&#24518;&#21644; dropout &#30340;&#26032;&#27169;&#22411; MD-Dropout &#26377;&#25928;&#38450;&#27490;&#20102;&#33192;&#32960;&#31070;&#32463;&#23849;&#28291;&#65292;&#24182;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.05530</link><description>&lt;p&gt;
&#35760;&#24518;-&#33192;&#32960;&#65306;&#24314;&#27169;&#26631;&#31614;&#22122;&#22768;&#19979;&#31070;&#32463;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Memorization-Dilation: Modeling Neural Collapse Under Label Noise. (arXiv:2206.05530v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#29616;&#23454;&#30340;&#26080;&#38480;&#21046;&#29305;&#24449;&#34920;&#31034;&#21464;&#20307;&#65292;&#23427;&#32771;&#34385;&#20102;&#32593;&#32476;&#30340;&#26377;&#38480;&#34920;&#36798;&#24615;&#12290;&#32467;&#21512;&#35760;&#24518;&#21644; dropout &#30340;&#26032;&#27169;&#22411; MD-Dropout &#26377;&#25928;&#38450;&#27490;&#20102;&#33192;&#32960;&#31070;&#32463;&#23849;&#28291;&#65292;&#24182;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#26159;&#25351;&#22312;&#21508;&#31181;&#20856;&#22411;&#20998;&#31867;&#38382;&#39064;&#20013;&#32463;&#39564;&#35266;&#23519;&#21040;&#30340;&#22810;&#31181;&#32039;&#24613;&#29616;&#35937;&#12290;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32456;&#27490;&#38454;&#27573;&#65292;&#21516;&#19968;&#31867;&#21035;&#30340;&#25152;&#26377;&#31034;&#20363;&#30340;&#29305;&#24449;&#23884;&#20837; tend to collapse &#21040;&#21333;&#20010;&#34920;&#31034;&#65292;&#24182;&#19988;&#19981;&#21516;&#31867;&#21035;&#30340;&#29305;&#24449; tend to separate&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#29616;&#23454;&#30340;&#26080;&#38480;&#21046;&#29305;&#24449;&#34920;&#31034;&#21464;&#20307;&#65292;&#23427;&#32771;&#34385;&#20102;&#32593;&#32476;&#30340;&#26377;&#38480;&#34920;&#36798;&#24615;&#12290;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#35760;&#24518;&#22122;&#22768;&#25968;&#25454;&#28857;&#20250;&#23548;&#33268;&#31070;&#32463;&#23849;&#28291;&#30340;&#24694;&#21270;&#65288;&#33192;&#32960;&#65289;&#12290;&#20351;&#29992;&#35760;&#24518;&#33192;&#32960;&#65288;M-D&#65289;&#29616;&#35937;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#31614;&#22122;&#22768;&#22914;&#20309;&#23548;&#33268;&#31070;&#32463;&#23849;&#28291;&#30340;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35760;&#24518;&#21644; dropout &#30340;&#26032;&#27169;&#22411; MD-Dropout&#65292;&#20316;&#20026;&#19968;&#31181;&#27491;&#21017;&#21270;&#22120;&#26469;&#38450;&#27490;&#33192;&#32960;&#31070;&#32463;&#23849;&#28291;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MD-Dropout &#25552;&#39640;&#20102;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The notion of neural collapse refers to several emergent phenomena that have been empirically observed across various canonical classification problems. During the terminal phase of training a deep neural network, the feature embedding of all examples of the same class tend to collapse to a single representation, and the features of different classes tend to separate as much as possible. Neural collapse is often studied through a simplified model, called the unconstrained feature representation, in which the model is assumed to have "infinite expressivity" and can map each data point to any arbitrary representation. In this work, we propose a more realistic variant of the unconstrained feature representation that takes the limited expressivity of the network into account. Empirical evidence suggests that the memorization of noisy data points leads to a degradation (dilation) of the neural collapse. Using a model of the memorization-dilation (M-D) phenomenon, we show one mechanism by wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#21442;&#25968;&#27491;&#21017;&#21270;&#20197;&#36991;&#20813;&#37325;&#22797;&#35757;&#32451;&#65292;&#24182;&#22312;&#19981;&#20250;&#36864;&#21270;&#24050;&#23398;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.17269</link><description>&lt;p&gt;
&#12298;&#28145;&#20837;&#30740;&#31350;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.17269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#21442;&#25968;&#27491;&#21017;&#21270;&#20197;&#36991;&#20813;&#37325;&#22797;&#35757;&#32451;&#65292;&#24182;&#22312;&#19981;&#20250;&#36864;&#21270;&#24050;&#23398;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28176;&#36827;&#24335;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#19968;&#31181;&#29615;&#22659;&#65292;&#21516;&#26102;&#36991;&#20813;&#20197;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#20986;&#29616;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#29616;&#35937;&#12290;&#24403;&#21069;&#30340;&#21333;&#20219;&#21153;&#25193;&#23637;&#24615;&#28176;&#36827;&#24335;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#37325;&#22797;&#35757;&#32451;&#20197;&#36991;&#20813;&#30693;&#35782;&#36864;&#21270;&#65292;&#20294;&#37325;&#22797;&#35757;&#32451;&#20250;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#65292;&#24182;&#21487;&#33021;&#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#30693;&#35782;&#33976;&#39311;&#21644;&#21442;&#25968;&#27491;&#21017;&#21270;&#20197;&#26032;&#30340;&#26041;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#19981;&#36827;&#34892;&#37325;&#22797;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is a setting where machine learning models learn novel concepts from continuously shifting training data, while simultaneously avoiding degradation of knowledge on previously seen classes which may disappear from the training data for extended periods of time (a phenomenon known as the catastrophic forgetting problem). Current approaches for continual learning of a single expanding task (aka class-incremental continual learning) require extensive rehearsal of previously seen data to avoid this degradation of knowledge. Unfortunately, rehearsal comes at a cost to memory, and it may also violate data-privacy. Instead, we explore combining knowledge distillation and parameter regularization in new ways to achieve strong continual learning performance without rehearsal. Specifically, we take a deep dive into common continual learning techniques: prediction distillation, feature distillation, L2 parameter regularization, and EWC parameter regularization. We first disprove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#37325;&#35201;&#24615;&#37319;&#26679;&#21644;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#39033;&#30340;CAM&#25913;&#36827;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#20998;&#21106;&#30340;&#36718;&#24275;&#31934;&#24230;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.12459</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#20998;&#21106;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;CAM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Importance Sampling CAMs for Weakly-Supervised Segmentation. (arXiv:2203.12459v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#37325;&#35201;&#24615;&#37319;&#26679;&#21644;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#39033;&#30340;CAM&#25913;&#36827;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#20998;&#21106;&#30340;&#36718;&#24275;&#31934;&#24230;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31867;&#28608;&#27963;&#22270;&#65288;CAM&#65289;&#21487;&#20197;&#21033;&#29992;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#36827;&#34892;&#23450;&#20301;&#21644;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#20687;&#32032;&#32423;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#31867;&#32593;&#32476;&#24448;&#24448;&#21482;&#20851;&#27880;&#21306;&#20998;&#24615;&#36739;&#24378;&#30340;&#21306;&#22495;&#65292;&#24182;&#19988;&#20250;&#20135;&#29983;&#27169;&#31946;&#30340;CAM&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#39044;&#27979;&#36793;&#32536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;CAM&#26041;&#27861;&#30340;&#36129;&#29486;&#65292;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;CAM&#20135;&#29983;&#30340;&#31867;&#21035;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#65292;&#24341;&#20837;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#20135;&#29983;&#38543;&#26426;&#30340;&#22270;&#20687;&#32423;&#21035;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#20351;CAM&#35206;&#30422;&#26356;&#24191;&#27867;&#30340;&#29289;&#20307;&#21306;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#39033;&#65292;&#26088;&#22312;&#23558;&#39044;&#27979;&#36793;&#32536;&#19982;&#22270;&#20687;&#20013;&#30340;&#36793;&#32536;&#21305;&#37197;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#22312;PASCAL VOC 2012&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#36825;&#20123;&#25913;&#36827;&#26174;&#33879;&#25552;&#39640;&#20102;&#36718;&#24275;&#31934;&#24230;&#24615;&#33021;&#65292;&#21516;&#26102;&#19982;&#24403;&#21069;&#29366;&#24577;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification networks can be used to localize and segment objects in images by means of class activation maps (CAMs). However, without pixel-level annotations, classification networks are known to (1) mainly focus on discriminative regions, and (2) to produce diffuse CAMs without well-defined prediction contours. In this work, we approach both problems with two contributions for improving CAM learning. First, we incorporate importance sampling based on the class-wise probability mass function induced by the CAMs to produce stochastic image-level class predictions. This results in CAMs which activate over a larger extent of objects. Second, we formulate a feature similarity loss term which aims to match the prediction contours with edges in the image. As a third contribution, we conduct experiments on the PASCAL VOC 2012 benchmark dataset to demonstrate that these modifications significantly increase the performance in terms of contour accuracy, while being comparable to current state
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;&#30340;&#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#25442;&#24120;&#29992;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;&#22312;15&#20010;&#22823;&#22411;&#34920;&#26684;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2203.09410</link><description>&lt;p&gt;
&#28145;&#24230;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;&#30340;&#26694;&#26550;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Framework and Benchmark for Deep Batch Active Learning for Regression. (arXiv:2203.09410v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;&#30340;&#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#25442;&#24120;&#29992;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;&#22312;15&#20010;&#22823;&#22411;&#34920;&#26684;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#30340;&#33719;&#21462;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#36873;&#25321;&#26080;&#26631;&#31614;&#25968;&#25454;&#25209;&#27425;&#36827;&#34892;&#26631;&#27880;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#36825;&#26679;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;(&#32593;&#32476;&#30456;&#20851;&#30340;)&#22522;&#30784;&#26680;&#12289;&#26680;&#21464;&#25442;&#21644;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#35768;&#22810;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#20197;&#21450;&#38750;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#29992;&#25551;&#32472;&#26377;&#38480;&#23485;&#24230;&#31070;&#32463;&#27491;&#20999;&#26680;&#26367;&#25442;&#24120;&#29992;&#30340;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;15&#20010;&#22823;&#22411;&#34920;&#26684;&#22238;&#24402;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#65292;&#26080;&#38656;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#25110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The acquisition of labels for supervised learning can be expensive. In order to improve the sample-efficiency of neural network regression, we study active learning methods that adaptively select batches of unlabeled data for labeling. We present a framework for constructing such methods out of (network-dependent) base kernels, kernel transformations and selection methods. Our framework encompasses many existing Bayesian methods based on Gaussian Process approximations of neural networks as well as non-Bayesian methods. Additionally, we propose to replace the commonly used last-layer features with sketched finite-width Neural Tangent Kernels, and to combine them with a novel clustering method. To evaluate different methods, we introduce an open-source benchmark consisting of 15 large tabular regression data sets. Our proposed method outperforms the state-of-the-art on our benchmark, scales to large data sets, and works out-of-the-box without adjusting the network architecture or traini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#20165;&#20855;&#26377;&#19968;&#20010;&#38544;&#21547;&#23618;&#21644;&#19968;&#20010;&#36755;&#20986;&#32500;&#24230;&#20197;&#21450;&#20165;&#20855;&#26377;&#19968;&#20010;&#36127;&#12289;&#38646;&#21644;&#19968;&#20010;&#27491;&#26435;&#37325;&#25110;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#65292;&#23427;&#26159;NP&#38590;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.07941</link><description>&lt;p&gt;
&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reachability In Simple Neural Networks. (arXiv:2203.07941v3 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#20165;&#20855;&#26377;&#19968;&#20010;&#38544;&#21547;&#23618;&#21644;&#19968;&#20010;&#36755;&#20986;&#32500;&#24230;&#20197;&#21450;&#20165;&#20855;&#26377;&#19968;&#20010;&#36127;&#12289;&#38646;&#21644;&#19968;&#20010;&#27491;&#26435;&#37325;&#25110;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#65292;&#23427;&#26159;NP&#38590;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#65288;&#28145;&#24230;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65306;&#22312;&#32473;&#23450;&#19968;&#20123;&#26377;&#25928;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#26159;&#21542;&#35745;&#31639;&#20986;&#26377;&#25928;&#36755;&#20986;&#65311;&#26368;&#36817;&#26377;&#20154;&#22768;&#31216;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#30001;&#32447;&#24615;&#19981;&#31561;&#24335;&#30340;&#21512;&#21462;&#32452;&#25104;&#30340;&#36755;&#20837;/&#36755;&#20986;&#32500;&#24230;&#30340;&#35268;&#33539;&#65292;&#35813;&#38382;&#39064;&#26159;NP&#23436;&#20840;&#38382;&#39064;&#12290; &#25105;&#20204;&#24635;&#32467;&#20102;&#35777;&#26126;&#24182;&#20462;&#22797;&#20102;&#21407;&#22987;&#19978;&#30028;&#21644;&#19979;&#30028;&#35777;&#26126;&#20013;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;&#21463;&#21040;&#36890;&#29992;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NP&#38590;&#24230;&#24050;&#32463;&#36866;&#29992;&#20110;&#31616;&#21333;&#35268;&#33539;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#21463;&#38480;&#31867;&#12290;&#20801;&#35768;&#19968;&#20010;&#38544;&#34255;&#23618;&#21644;&#19968;&#20010;&#36755;&#20986;&#32500;&#25968;&#20197;&#21450;&#20165;&#20855;&#26377;&#19968;&#20010;&#36127;&#12289;&#38646;&#21644;&#19968;&#20010;&#27491;&#26435;&#37325;&#25110;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#23601;&#36275;&#20197;&#30830;&#20445;NP&#38590;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30740;&#31350;&#30340;&#36825;&#20010;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35752;&#35770;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the complexity of the reachability problem for (deep) neural networks: does it compute valid output given some valid input? It was recently claimed that the problem is NP-complete for general neural networks and specifications over the input/output dimension given by conjunctions of linear inequalities. We recapitulate the proof and repair some flaws in the original upper and lower bound proofs. Motivated by the general result, we show that NP-hardness already holds for restricted classes of simple specifications and neural networks. Allowing for a single hidden layer and an output dimension of one as well as neural networks with just one negative, zero and one positive weight or bias is sufficient to ensure NP-hardness. Additionally, we give a thorough discussion and outlook of possible extensions for this direction of research on neural network verification.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2203.05711</link><description>&lt;p&gt;
&#30005;&#24433;&#21465;&#36848;&#25688;&#35201;&#65306;&#19968;&#20010;&#29992;&#20110;&#25925;&#20107;&#29702;&#35299;&#30340;&#35270;&#39057;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05711
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;AI&#26377;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#25925;&#20107;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#20854;&#20013;&#21253;&#21547;5,193&#20010;&#27969;&#34892;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#35270;&#39057;&#25688;&#35201;&#12290;SYMON&#25429;&#25417;&#20102;&#30001;&#20154;&#31867;&#21019;&#20316;&#32773;&#21046;&#20316;&#30340;&#38754;&#21521;&#20154;&#31867;&#35266;&#20247;&#30340;&#33258;&#28982;&#25925;&#20107;&#21465;&#36848;&#35270;&#39057;&#12290;&#20316;&#20026;&#19968;&#20010;&#21407;&#22411;&#21644;&#33258;&#28982;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;SYMON&#20855;&#26377;&#39640;&#35206;&#30422;&#30340;&#22810;&#27169;&#24577;&#25925;&#20107;&#20107;&#20214;&#12289;&#20016;&#23500;&#30340;&#24515;&#29702;&#29366;&#24577;&#25551;&#36848;&#21644;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#22823;&#35821;&#20041;&#24046;&#36317;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#21644;&#30005;&#24433;&#25688;&#35201;&#35270;&#39057;&#30340;&#38646;&#26679;&#26412;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#22312;&#25925;&#20107;&#29702;&#35299;&#20013;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;SYMON&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#36827;&#23637;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328; Twitter &#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#24076;&#33098;&#12289;&#35199;&#29677;&#29273;&#21644;&#32852;&#21512;&#29579;&#22269;&#35758;&#20250;&#25104;&#21592;&#30340;&#25512;&#25991;&#65292;&#24182;&#21457;&#29616;&#28040;&#26497;&#24773;&#32490;&#26356;&#26131;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2202.00396</link><description>&lt;p&gt;
&#28040;&#26497;&#24773;&#32490;&#20256;&#25773;&#26356;&#24555;&#65306;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328; Twitter &#20998;&#26512;&#30340;&#24773;&#24863;&#22312;&#25919;&#27835;&#27807;&#36890;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication. (arXiv:2202.00396v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328; Twitter &#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#24076;&#33098;&#12289;&#35199;&#29677;&#29273;&#21644;&#32852;&#21512;&#29579;&#22269;&#35758;&#20250;&#25104;&#21592;&#30340;&#25512;&#25991;&#65292;&#24182;&#21457;&#29616;&#28040;&#26497;&#24773;&#32490;&#26356;&#26131;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24050;&#32463;&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#23545;&#25919;&#31574;&#21046;&#23450;&#20135;&#29983;&#20102;&#26497;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#35199;&#26041;&#19990;&#30028;&#20013;&#65292;Twitter &#31561;&#24179;&#21488;&#35753;&#29992;&#25143;&#33021;&#22815;&#20851;&#27880;&#25919;&#27835;&#23478;&#65292;&#20351;&#20844;&#27665;&#26356;&#22810;&#22320;&#21442;&#19982;&#25919;&#27835;&#35752;&#35770;&#12290;&#21516;&#26679;&#65292;&#25919;&#27835;&#23478;&#20063;&#21033;&#29992; Twitter &#34920;&#36798;&#33258;&#24049;&#30340;&#35266;&#28857;&#65292;&#22312;&#24403;&#21069;&#35805;&#39064;&#19978;&#19982;&#20182;&#20154;&#36777;&#35770;&#65292;&#24182;&#25512;&#21160;&#33258;&#24049;&#30340;&#25919;&#27835;&#35758;&#31243;&#65292;&#26088;&#22312;&#24433;&#21709;&#36873;&#27665;&#34892;&#20026;&#12290;&#26412;&#25991;&#35797;&#22270;&#20998;&#26512;&#19977;&#20010;&#27431;&#27954;&#22269;&#23478;&#25919;&#27835;&#23478;&#30340;&#25512;&#25991;&#65292;&#24182;&#25506;&#32034;&#23427;&#20204;&#30340;&#20256;&#25773;&#24230;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20256;&#36798;&#28040;&#26497;&#24773;&#32490;&#30340;&#25512;&#25991;&#24448;&#24448;&#20250;&#34987;&#26356;&#39057;&#32321;&#22320;&#36716;&#21457;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#24076;&#33098;&#12289;&#35199;&#29677;&#29273;&#21644;&#32852;&#21512;&#29579;&#22269;&#65288;&#21253;&#25324;&#20998;&#26435;&#25919;&#24220;&#65289;&#35758;&#20250;&#25104;&#21592;&#30340;&#25968;&#21313;&#19975;&#26465;&#25512;&#25991;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25506;&#31350;&#21644;&#20998;&#26512;&#24046;&#24322;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media has become extremely influential when it comes to policy making in modern societies, especially in the western world, where platforms such as Twitter allow users to follow politicians, thus making citizens more involved in political discussion. In the same vein, politicians use Twitter to express their opinions, debate among others on current topics and promote their political agendas aiming to influence voter behaviour. In this paper, we attempt to analyse tweets of politicians from three European countries and explore the virality of their tweets. Previous studies have shown that tweets conveying negative sentiment are likely to be retweeted more frequently. By utilising state-of-the-art pre-trained language models, we performed sentiment analysis on hundreds of thousands of tweets collected from members of parliament in Greece, Spain and the United Kingdom, including devolved administrations. We achieved this by systematically exploring and analysing the differences bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476; CentSmoothie&#65292;&#33021;&#22815;&#20805;&#20998;&#32771;&#34385;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2112.07837</link><description>&lt;p&gt;
&#20013;&#24515;&#24179;&#28369;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Central-Smoothing Hypergraph Neural Networks for Predicting Drug-Drug Interactions. (arXiv:2112.07837v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476; CentSmoothie&#65292;&#33021;&#22815;&#20805;&#20998;&#32771;&#34385;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#26159;&#20351;&#29992;&#33647;&#21697;&#20449;&#24687;&#21644;&#35768;&#22810;&#23545;&#24050;&#30693;&#26631;&#31614;&#30340;&#30456;&#20114;&#20316;&#29992;&#33647;&#21697;&#26469;&#39044;&#27979;&#33647;&#21697;&#23545;&#30340;&#21103;&#20316;&#29992;&#65288;&#19981;&#33391;&#21453;&#24212;&#65289;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#39044;&#27979; DDI &#22270;&#20013;&#27599;&#23545;&#33410;&#28857;&#30340;&#26631;&#31614;&#65288;&#21363;&#21103;&#20316;&#29992;&#65289;&#65292;&#20854;&#20013;&#33410;&#28857;&#26159;&#33647;&#21697;&#65292;&#36793;&#32536;&#26159;&#20855;&#26377;&#24050;&#30693;&#26631;&#31614;&#30340;&#30456;&#20114;&#20316;&#29992;&#33647;&#21697;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#23427;&#21033;&#29992;&#22270;&#20013;&#30340;&#37051;&#22495;&#20449;&#24687;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110; DDI&#65292;&#30001;&#20110;&#21103;&#20316;&#29992;&#30340;&#24615;&#36136;&#65292;&#23384;&#22312;&#35768;&#22810;&#24102;&#26377;&#22797;&#26434;&#20851;&#31995;&#30340;&#26631;&#31614;&#12290;&#36890;&#24120;&#30340; GNN &#32463;&#24120;&#23558;&#26631;&#31614;&#22266;&#23450;&#20026;&#29420;&#28909;&#21521;&#37327;&#65292;&#36825;&#19981;&#33021;&#21453;&#26144;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#32597;&#35265;&#26631;&#31614;&#30340;&#22256;&#38590;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558; DDI &#34920;&#31034;&#20026;&#19968;&#20010;&#36229;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#36229;&#36793;&#26159;&#19968;&#20010;&#19977;&#20803;&#32452;&#65306;&#20004;&#20010;&#33647;&#21697;&#33410;&#28857;&#21644;&#19968;&#20010;&#26631;&#31614;&#33410;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; CentSmoothie&#65292;&#19968;&#31181;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#30340;&#24179;&#28369;&#27744;&#21270;&#26469;&#34701;&#21512;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;CentSmoothie &#22312;&#20004;&#20010;&#22823;&#22411; DDI &#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting drug-drug interactions (DDI) is the problem of predicting side effects (unwanted outcomes) of a pair of drugs using drug information and known side effects of many pairs. This problem can be formulated as predicting labels (i.e. side effects) for each pair of nodes in a DDI graph, of which nodes are drugs and edges are interacting drugs with known labels. State-of-the-art methods for this problem are graph neural networks (GNNs), which leverage neighborhood information in the graph to learn node representations. For DDI, however, there are many labels with complicated relationships due to the nature of side effects. Usual GNNs often fix labels as one-hot vectors that do not reflect label relationships and potentially do not obtain the highest performance in the difficult cases of infrequent labels. In this paper, we formulate DDI as a hypergraph where each hyperedge is a triple: two nodes for drugs and one node for a label. We then present CentSmoothie, a hypergraph neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#20998;&#22359;&#31890;&#23376;&#28388;&#27874;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#32500;&#21442;&#25968;&#12290;&#35813;&#31639;&#27861;&#20811;&#26381;&#20102;&#32500;&#24230;&#28798;&#38590;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#20284;&#28982;&#26368;&#22823;&#21270;&#65292;&#25104;&#21151;&#22320;&#22312;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#31354;&#38388;&#26102;&#38388;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#21442;&#25968;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2110.10745</link><description>&lt;p&gt;
&#39640;&#32500;&#21442;&#25968;&#23398;&#20064;&#30340;&#36845;&#20195;&#20998;&#22359;&#31890;&#23376;&#28388;&#27874;&#31639;&#27861;&#65306;&#25670;&#33073;&#32500;&#24230;&#28798;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Iterated Block Particle Filter for High-dimensional Parameter Learning: Beating the Curse of Dimensionality. (arXiv:2110.10745v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#20998;&#22359;&#31890;&#23376;&#28388;&#27874;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#32500;&#21442;&#25968;&#12290;&#35813;&#31639;&#27861;&#20811;&#26381;&#20102;&#32500;&#24230;&#28798;&#38590;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#20284;&#28982;&#26368;&#22823;&#21270;&#65292;&#25104;&#21151;&#22320;&#22312;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#31354;&#38388;&#26102;&#38388;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#21442;&#25968;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#12289;&#37096;&#20998;&#35266;&#27979;&#21644;&#38750;&#32447;&#24615;&#38543;&#26426;&#36807;&#31243;&#30340;&#21442;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26041;&#27861;&#35770;&#19978;&#30340;&#25361;&#25112;&#12290;&#31354;&#38388;&#26102;&#38388;&#30142;&#30149;&#20256;&#25773;&#31995;&#32479;&#25552;&#20379;&#20102;&#36825;&#31181;&#20135;&#29983;&#24320;&#25918;&#25512;&#26029;&#38382;&#39064;&#30340;&#36807;&#31243;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#20998;&#22359;&#31890;&#23376;&#28388;&#27874;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#19968;&#33324;&#29366;&#24577;&#31354;&#38388;&#12289;&#27979;&#37327;&#12289;&#36716;&#31227;&#23494;&#24230;&#21644;&#22270;&#32467;&#26500;&#30340;&#22270;&#24418;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#39640;&#32500;&#21442;&#25968;&#12290;&#23545;&#20987;&#36133;&#32500;&#24230;&#28798;&#38590; (COD)&#12289;&#31639;&#27861;&#25910;&#25947;&#21644;&#20284;&#28982;&#26368;&#22823;&#21270;&#33719;&#24471;&#20102;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#23545;&#40635;&#30137;&#20256;&#25773;&#30340;&#39640;&#24230;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#31354;&#38388;&#26102;&#38388;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36845;&#20195;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#31639;&#27861; (Li et al. (2020)) &#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#36845;&#20195;&#28388;&#27874;&#31639;&#27861; (Ionides et al. (2015)) &#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#32780;&#25105;&#20204;&#30340;IBPF&#31639;&#27861;&#22312;&#21508;&#31181;&#20855;&#26377;&#19981;&#21516;&#24230;&#37327;&#30340;&#23454;&#39564;&#20013;&#22987;&#32456;&#20987;&#36133;&#20102;COD&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter learning for high-dimensional, partially observed, and nonlinear stochastic processes is a methodological challenge. Spatiotemporal disease transmission systems provide examples of such processes giving rise to open inference problems. We propose the iterated block particle filter (IBPF) algorithm for learning high-dimensional parameters over graphical state space models with general state spaces, measures, transition densities and graph structure. Theoretical performance guarantees are obtained on beating the curse of dimensionality (COD), algorithm convergence, and likelihood maximization. Experiments on a highly nonlinear and non-Gaussian spatiotemporal model for measles transmission reveal that the iterated ensemble Kalman filter algorithm (Li et al. (2020)) is ineffective and the iterated filtering algorithm (Ionides et al. (2015)) suffers from the COD, while our IBPF algorithm beats COD consistently across various experiments with different metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.04829</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive joint distribution learning. (arXiv:2110.04829v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04829
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#23884;&#20837;&#24352;&#37327;&#31215;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#23481;&#32435;&#19968;&#20010;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#22810;&#36798;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#28857;&#30340;&#26679;&#26412;&#22823;&#23567;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#20943;&#36731;&#20102;RKHS&#24314;&#27169;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#20135;&#29983;&#20102;&#23450;&#20041;&#33391;&#22909;&#30340;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#23884;&#20837;&#35745;&#31639;&#36895;&#24230;&#24555;&#19988;&#36866;&#29992;&#20110;&#20174;&#39044;&#27979;&#21040;&#20998;&#31867;&#30340;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#26377;&#30410;&#30340;&#25968;&#20540;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new framework for embedding joint probability distributions in tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized and positive model of a Radon-Nikodym derivative, which we estimate from sample sizes of up to several million data points, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. The embedding is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32447;&#24615;&#21644;&#28145;&#24230;&#24191;&#20041;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65292;&#36890;&#36807;&#21387;&#32553;&#26412;&#22320;&#25968;&#25454;&#32479;&#35745;&#20449;&#24687;&#21644;&#20351;&#29992;&#23376;&#31354;&#38388;&#20849;&#35782;&#31639;&#27861;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#20013;&#24515;&#21270;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.12400</link><description>&lt;p&gt;
&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#32447;&#24615;&#21644;&#28145;&#24230;&#24191;&#20041;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Linear and Deep Generalized Canonical Correlation Analysis. (arXiv:2109.12400v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32447;&#24615;&#21644;&#28145;&#24230;&#24191;&#20041;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65292;&#36890;&#36807;&#21387;&#32553;&#26412;&#22320;&#25968;&#25454;&#32479;&#35745;&#20449;&#24687;&#21644;&#20351;&#29992;&#23376;&#31354;&#38388;&#20849;&#35782;&#31639;&#27861;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#20013;&#24515;&#21270;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#21644;&#28145;&#24230;&#24191;&#20041;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;GCCA&#65289;&#31639;&#27861;&#20998;&#21035;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#21644;&#31070;&#32463;&#32593;&#32476;&#20174;&#22810;&#20010;&#8220;&#35270;&#22270;&#8221;&#65288;&#20363;&#22914;&#38899;&#39057;&#21644;&#22270;&#20687;&#65289;&#20013;&#23547;&#25214;&#20302;&#32500;&#30340;&#20849;&#21516;&#34920;&#31034;&#12290;&#24403;&#36825;&#20123;&#35270;&#22270;&#30001;&#19981;&#21516;&#30340;&#35745;&#31639;&#20195;&#29702;&#65288;&#20363;&#22914;&#32452;&#32455;&#21644;&#36793;&#32536;&#35774;&#22791;&#65289;&#36827;&#34892;&#33719;&#21462;&#21644;&#23384;&#20648;&#65292;&#24182;&#19988;&#30001;&#20110;&#38544;&#31169;&#25110;&#36890;&#20449;&#25104;&#26412;&#31561;&#32771;&#34385;&#19981;&#24076;&#26395;&#25968;&#25454;&#20849;&#20139;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;GCCA&#26159;&#21512;&#29702;&#30340;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36825;&#20123;&#35270;&#22270;&#22312;&#20195;&#29702;&#22788;&#20445;&#25345;&#26412;&#22320;&#65292;&#24182;&#19988;&#21482;&#20801;&#35768;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#23548;&#20986;&#30340;&#26377;&#38480;&#20449;&#24687;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#30340;GCCA&#31639;&#27861;&#24212;&#29992;&#21040;&#36825;&#20123;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#36807;&#39640;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26368;&#22823;&#26041;&#24046;&#65288;MAX-VAR&#65289;&#24418;&#24335;&#19979;&#30340;&#32447;&#24615;&#21644;&#28145;&#24230;GCCA&#12290;&#36890;&#36807;&#22312;&#20256;&#36755;&#21069;&#31215;&#26497;&#21387;&#32553;&#65288;&#36890;&#36807;&#37327;&#21270;&#32534;&#30721;&#65289;&#26412;&#22320;&#25968;&#25454;&#32479;&#35745;&#20449;&#24687;&#21644;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23376;&#31354;&#38388;&#20849;&#35782;&#31639;&#27861;&#26469;&#35299;&#20915;&#24320;&#38144;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#26497;&#22823;&#22320;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#20013;&#24515;&#21270;GCCA&#31639;&#27861;&#30456;&#24403;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classic and deep generalized canonical correlation analysis (GCCA) algorithms seek low-dimensional common representations of data entities from multiple ``views'' (e.g., audio and image) using linear transformations and neural networks, respectively. When the views are acquired and stored at different computing agents (e.g., organizations and edge devices) and data sharing is undesired due to privacy or communication cost considerations, federated learning-based GCCA is well-motivated. In federated learning, the views are kept locally at the agents and only derived, limited information exchange with a central server is allowed. However, applying existing GCCA algorithms onto such federated learning settings may incur prohibitively high communication overhead. This work puts forth a communication-efficient federated learning framework for both linear and deep GCCA under the maximum variance (MAX-VAR) formulation. The overhead issue is addressed by aggressively compressing (via quantizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#23884;&#20837;&#30340;&#39044;&#27979;&#26041;&#26696;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#34920;&#31034;&#22522;&#30784;&#21160;&#21147;&#31995;&#32479;&#30340;&#25972;&#20010;&#24038;&#26080;&#38480;&#36712;&#36947;&#25110;&#26469;&#33258;&#36825;&#26679;&#30340;&#36712;&#36947;&#30340;&#35266;&#27979;&#65292;&#35813;&#26041;&#26696;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#21487;&#35745;&#31639;&#24615;&#21644;&#35823;&#24046;&#23481;&#38169;&#24615;&#65292;&#21487;&#20197;&#22312;&#38271;&#26399;&#20445;&#35777;&#39044;&#27979;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.10759</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#23884;&#20837;&#30340;&#29289;&#29702;&#31995;&#32479;&#39044;&#27979;&#30340;&#36890;&#29992;&#21487;&#35266;&#27979;&#38598;
&lt;/p&gt;
&lt;p&gt;
Universal set of Observables for Forecasting Physical Systems through Causal Embedding. (arXiv:2105.10759v3 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#23884;&#20837;&#30340;&#39044;&#27979;&#26041;&#26696;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#34920;&#31034;&#22522;&#30784;&#21160;&#21147;&#31995;&#32479;&#30340;&#25972;&#20010;&#24038;&#26080;&#38480;&#36712;&#36947;&#25110;&#26469;&#33258;&#36825;&#26679;&#30340;&#36712;&#36947;&#30340;&#35266;&#27979;&#65292;&#35813;&#26041;&#26696;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#21487;&#35745;&#31639;&#24615;&#21644;&#35823;&#24046;&#23481;&#38169;&#24615;&#65292;&#21487;&#20197;&#22312;&#38271;&#26399;&#20445;&#35777;&#39044;&#27979;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#31354;&#38388;&#20013;&#30340;&#19968;&#23545;&#20803;&#32032;&#21807;&#19968;&#22320;&#34920;&#31034;&#22522;&#30784;&#21160;&#21147;&#31995;&#32479;&#30340;&#25972;&#20010;&#24038;&#26080;&#38480;&#36712;&#36947;&#25110;&#26469;&#33258;&#36825;&#26679;&#30340;&#24038;&#26080;&#38480;&#36712;&#36947;&#30340;&#35266;&#27979;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#22240;&#26524;&#23884;&#20837;&#8221;&#29616;&#35937;&#12290;&#36825;&#20123;&#25104;&#23545;&#30340;&#38598;&#21512;&#26159;&#20174;&#39537;&#21160;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#23548;&#20986;&#30340;&#65292;&#24182;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#19982;&#39537;&#21160;&#31995;&#32479;&#19968;&#36215;&#30830;&#23450;&#19968;&#20010;&#25299;&#25169;&#20849;&#36717;&#20110;&#22522;&#30784;&#31995;&#32479;&#30340;&#31995;&#32479;&#65292;&#20351;&#24471;&#39044;&#27979;&#22522;&#30784;&#31995;&#32479;&#30340;&#21160;&#24577;&#25104;&#20026;&#21487;&#33021;&#65292;&#22240;&#20026;&#36825;&#31181;&#20849;&#36717;&#26159;&#21487;&#35745;&#31639;&#19988;&#36890;&#29992;&#30340;&#65292;&#21363;&#23427;&#19981;&#20381;&#36182;&#20110;&#22522;&#30784;&#31995;&#32479;&#65292;&#21363;&#20351;&#22312;&#23398;&#20064;&#20989;&#25968;&#26102;&#23384;&#22312;&#35823;&#24046;&#65292;&#20063;&#33021;&#20445;&#35777;&#21560;&#24341;&#23376;&#21253;&#21547;&#22240;&#26524;&#23884;&#20837;&#29289;&#20307;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#23454;&#29616;&#36825;&#20123;&#65292;&#25105;&#20204;&#24341;&#39046;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#26041;&#26696;&#65292;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#20648;&#22791;&#35745;&#31639;&#26041;&#26696;&#65292;&#21518;&#32773;&#36890;&#24120;&#23548;&#33268;&#38271;&#26399;&#19981;&#19968;&#33268;&#24615;&#24456;&#24046;&#65292;&#22240;&#20026;&#26080;&#27861;&#20445;&#35777;&#23384;&#22312;&#21487;&#23398;&#20064;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate when and how an entire left-infinite orbit of an underlying dynamical system or observations from such left-infinite orbits can be uniquely represented by a pair of elements in a different space, a phenomenon which we call \textit{causal embedding}. The collection of such pairs is derived from a driven dynamical system and is used to learn a function which together with the driven system would: (i). determine a system that is topologically conjugate to the underlying system (ii). enable forecasting the underlying system's dynamics since the conjugacy is computable and universal, i.e., it does not depend on the underlying system (iii). guarantee an attractor containing the image of the causally embedded object even if there is an error made in learning the function. By accomplishing these we herald a new forecasting scheme that beats the existing reservoir computing schemes that often lead to poor long-term consistency as there is no guarantee of the existence of a learna
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#23558;&#19978;&#19979;&#25991;&#23884;&#20837;&#20316;&#20026;Transformer&#22240;&#23376;&#30340;&#32447;&#24615;&#21472;&#21152;&#26469;&#25171;&#24320;Transformer&#8220;&#40657;&#21283;&#23376;&#8221;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#20854;&#25429;&#33719;&#30340;&#23618;&#27425;&#21270;&#35821;&#20041;&#32467;&#26500;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;Transformer&#32593;&#32476;&#30340;&#24037;&#20316;&#26041;&#24335;&#24102;&#26469;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2103.15949</link><description>&lt;p&gt;
&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#23454;&#29616;Transformer&#21487;&#35270;&#21270;:&#23558;&#19978;&#19979;&#25991;&#23884;&#20837;&#20316;&#20026;Transformer&#22240;&#23376;&#30340;&#32447;&#24615;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. (arXiv:2103.15949v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.15949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#23558;&#19978;&#19979;&#25991;&#23884;&#20837;&#20316;&#20026;Transformer&#22240;&#23376;&#30340;&#32447;&#24615;&#21472;&#21152;&#26469;&#25171;&#24320;Transformer&#8220;&#40657;&#21283;&#23376;&#8221;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#20854;&#25429;&#33719;&#30340;&#23618;&#27425;&#21270;&#35821;&#20041;&#32467;&#26500;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;Transformer&#32593;&#32476;&#30340;&#24037;&#20316;&#26041;&#24335;&#24102;&#26469;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;Transformer&#32593;&#32476;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#34920;&#31034;&#23398;&#20064;&#20013;&#25472;&#36215;&#20102;&#19968;&#22330;&#38761;&#21629;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#35299;&#37322;Transformer&#32593;&#32476;&#20013;&#30340;&#34920;&#31034;&#65292;&#20294;&#24191;&#27867;&#35748;&#20026;&#25105;&#20204;&#30340;&#29702;&#35299;&#36824;&#19981;&#22815;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#36275;&#22815;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#36827;&#34892;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#23558;&#20854;&#20316;&#20026;Transformer&#22240;&#23376;&#30340;&#32447;&#24615;&#21472;&#21152;&#26469;&#25171;&#24320;&#36825;&#20123;&#8220;&#40657;&#21283;&#23376;&#8221;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34987;Transformer&#22240;&#23376;&#25429;&#33719;&#30340;&#23618;&#27425;&#21270;&#35821;&#20041;&#32467;&#26500;&#65292;&#20363;&#22914;&#35789;&#32423;&#22810;&#20041;&#28040;&#27495;&#12289;&#21477;&#23376;&#32423;&#27169;&#24335;&#24418;&#25104;&#21644;&#38271;&#36317;&#31163;&#20381;&#36182;&#12290;&#34429;&#28982;&#19968;&#20123;&#27169;&#24335;&#31526;&#21512;&#20256;&#32479;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#20294;&#20854;&#20313;&#30340;&#27169;&#24335;&#30456;&#23545;&#20986;&#20046;&#24847;&#26009;&#65292;&#21487;&#33021;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#33021;&#24102;&#26469;&#26356;&#22810;&#30340;&#30693;&#35782;&#21644;&#23545;Transformer&#32593;&#32476;&#24037;&#20316;&#26041;&#24335;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/z&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these "black boxes" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/z
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARS&#30340;&#26032;&#22411;&#39640;&#25928;&#26041;&#27861;&#65292;&#22312;&#19968;&#33324;&#30340;&#24352;&#37327;&#20998;&#35299;&#20013;&#33258;&#21160;&#36873;&#25321;&#31209;&#65292;&#23398;&#20064;&#20108;&#20540;&#25513;&#30721;&#26469;&#36873;&#25321;&#26368;&#20339;&#30340;&#24352;&#37327;&#32467;&#26500;&#65292;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2006.10859</link><description>&lt;p&gt;
MARS:&#24352;&#37327;&#20998;&#35299;&#20013;&#30340;&#33258;&#21160;&#25490;&#21517;&#36873;&#25321;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MARS: Masked Automatic Ranks Selection in Tensor Decompositions. (arXiv:2006.10859v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARS&#30340;&#26032;&#22411;&#39640;&#25928;&#26041;&#27861;&#65292;&#22312;&#19968;&#33324;&#30340;&#24352;&#37327;&#20998;&#35299;&#20013;&#33258;&#21160;&#36873;&#25321;&#31209;&#65292;&#23398;&#20064;&#20108;&#20540;&#25513;&#30721;&#26469;&#36873;&#25321;&#26368;&#20339;&#30340;&#24352;&#37327;&#32467;&#26500;&#65292;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#21644;&#21152;&#36895;&#12290;&#21516;&#26102;&#65292;&#30830;&#23450;&#26368;&#20248;&#20998;&#35299;&#31209;&#30340;&#38382;&#39064;&#20173;&#28982;&#24456;&#20005;&#23803;&#65292;&#22240;&#20026;&#23427;&#26159;&#25511;&#21046;&#21387;&#32553;-&#20934;&#30830;&#24615;&#24179;&#34913;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARS&#30340;&#26032;&#22411;&#39640;&#25928;&#26041;&#27861;&#65292;&#22312;&#19968;&#33324;&#30340;&#24352;&#37327;&#20998;&#35299;&#20013;&#33258;&#21160;&#36873;&#25321;&#31209;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20108;&#20540;&#25513;&#30721;&#65292;&#36825;&#20123;&#25513;&#30721;&#21487;&#20197;&#36873;&#25321;&#26368;&#20339;&#30340;&#24352;&#37327;&#32467;&#26500;&#12290;&#23398;&#20064;&#26159;&#36890;&#36807;&#29305;&#23450;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#30340;&#26494;&#24347;&#26368;&#22823;&#21518;&#39564;(MAP)&#20272;&#35745;&#26469;&#23436;&#25104;&#30340;&#65292;&#24182;&#21487;&#20197;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#26631;&#20934;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#21508;&#31181;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;MARS&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor decomposition methods have proven effective in various applications, including compression and acceleration of neural networks. At the same time, the problem of determining optimal decomposition ranks, which present the crucial parameter controlling the compression-accuracy trade-off, is still acute. In this paper, we introduce MARS -- a new efficient method for the automatic selection of ranks in general tensor decompositions. During training, the procedure learns binary masks over decomposition cores that "select" the optimal tensor structure. The learning is performed via relaxed maximum a posteriori (MAP) estimation in a specific Bayesian model and can be naturally embedded into the standard neural network training routine. Diverse experiments demonstrate that MARS achieves better results compared to previous works in various tasks.
&lt;/p&gt;</description></item></channel></rss>