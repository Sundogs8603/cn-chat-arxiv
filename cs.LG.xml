<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#29305;&#24449;&#23545;&#27604;&#25439;&#22833;&#23454;&#29616;&#25968;&#25454;&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15890</link><description>&lt;p&gt;
&#36328;&#29305;&#24449;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2310.15890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#29305;&#24449;&#23545;&#27604;&#25439;&#22833;&#23454;&#29616;&#25968;&#25454;&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#22823;&#22810;&#25968;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20998;&#25955;&#24335;&#25968;&#25454;&#38598;&#22312;&#20195;&#29702;&#20043;&#38388;&#21487;&#20197;&#20855;&#26377;&#26174;&#33879;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#29305;&#24449;&#19978;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#25439;&#22833;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#23545;&#20110;&#19968;&#23545;&#30456;&#37051;&#20195;&#29702;&#65292;&#36328;&#29305;&#24449;&#26159;&#20174;&#19968;&#20010;&#20195;&#29702;&#30340;&#25968;&#25454;&#33719;&#21462;&#30340;&#29305;&#24449;&#65288;&#21363;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;&#28608;&#27963;&#65289;&#20851;&#20110;&#21478;&#19968;&#20010;&#20195;&#29702;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#35814;&#23613;&#30340;&#23454;&#39564;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#12289;CIFAR-100&#12289;Fashion MNIST &#21644; ImageNet&#65289;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#32593;&#32476;&#25299;&#25169;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20613;&#37324;&#21494;&#21464;&#25442;&#39044;&#27979;&#29366;&#24577;&#24207;&#21015;&#30340;&#26041;&#27861;&#65288;SPF&#65289;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#30340;&#39057;&#22495;&#29305;&#24615;&#65292;&#39640;&#25928;&#22320;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38271;&#26399;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.15888</link><description>&lt;p&gt;
&#36890;&#36807;&#20613;&#37324;&#21494;&#21464;&#25442;&#39044;&#27979;&#29366;&#24577;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
State Sequences Prediction via Fourier Transform for Representation Learning. (arXiv:2310.15888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15888
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20613;&#37324;&#21494;&#21464;&#25442;&#39044;&#27979;&#29366;&#24577;&#24207;&#21015;&#30340;&#26041;&#27861;&#65288;SPF&#65289;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#30340;&#39057;&#22495;&#29305;&#24615;&#65292;&#39640;&#25928;&#22320;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38271;&#26399;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#20013;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#20986;&#33394;&#24615;&#33021;&#25152;&#38656;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#25968;&#25454;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#36890;&#36807;&#39044;&#27979;&#38271;&#26399;&#26410;&#26469;&#29366;&#24577;&#26469;&#23398;&#20064;&#39044;&#27979;&#24615;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#24207;&#21015;&#29366;&#24577;&#20449;&#21495;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#20250;&#25913;&#21892;&#38271;&#26399;&#20915;&#31574;&#30340;&#36136;&#37327;&#65292;&#20294;&#22312;&#26102;&#22495;&#38590;&#20197;&#20998;&#36776;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20613;&#37324;&#21494;&#21464;&#25442;&#39044;&#27979;&#29366;&#24577;&#24207;&#21015;&#65288;SPF&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#30340;&#39057;&#22495;&#26469;&#39640;&#25928;&#22320;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#29366;&#24577;&#24207;&#21015;&#20013;&#32467;&#26500;&#20449;&#24687;&#30340;&#23384;&#22312;&#65292;
&lt;/p&gt;
&lt;p&gt;
While deep reinforcement learning (RL) has been demonstrated effective in solving complex control tasks, sample efficiency remains a key challenge due to the large amounts of data required for remarkable performance. Existing research explores the application of representation learning for data-efficient RL, e.g., learning predictive representations by predicting long-term future states. However, many existing methods do not fully exploit the structural information inherent in sequential state signals, which can potentially improve the quality of long-term decision-making but is difficult to discern in the time domain. To tackle this problem, we propose State Sequences Prediction via Fourier Transform (SPF), a novel method that exploits the frequency domain of state sequences to extract the underlying patterns in time series data for learning expressive representations efficiently. Specifically, we theoretically analyze the existence of structural information in state sequences, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15872</link><description>&lt;p&gt;
KirchhoffNet&#65306;&#19968;&#31181;&#36830;&#25509;&#28040;&#24687;&#20256;&#36882;&#21644;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#30005;&#36335;&#26725;&#25509;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27169;&#25311;&#30005;&#36335;&#30340;&#22522;&#26412;&#21407;&#29702;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#65292;&#24341;&#20837;&#20102;&#19968;&#31867;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#12290;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#20102;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#20256;&#32479;&#23618;&#65288;&#22914;&#21367;&#31215;&#12289;&#27744;&#21270;&#25110;&#32447;&#24615;&#23618;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#35753;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#26356;&#21152;&#26377;&#36259;&#30340;&#26159;&#20854;&#22312;&#30828;&#20214;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#24403;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#37096;&#32626;&#22312;GPU&#19978;&#12290;&#30456;&#21453;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#30005;&#36335;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#22312;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20869;&#26377;&#22810;&#23569;&#21442;&#25968;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20854;&#20013;f&#34920;&#31034;&#30828;&#20214;&#30340;&#26102;&#38047;&#39057;&#29575;&#12290;&#36825;&#31181;&#29305;&#24615;&#34920;&#26126;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20855;&#26377;&#28508;&#21147;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22240;&#26524;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#38388;&#20013;&#24515;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;13&#20010;&#26102;&#38388;&#22270;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#20171;&#25968;&#21644;&#25509;&#36817;&#24230;&#20013;&#24515;&#24615;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15865</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21160;&#24577;&#22270;&#20013;&#39044;&#27979;&#26102;&#38388;&#20013;&#24515;&#24615;
&lt;/p&gt;
&lt;p&gt;
Using Causality-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs. (arXiv:2310.15865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22240;&#26524;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#38388;&#20013;&#24515;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;13&#20010;&#26102;&#38388;&#22270;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#20171;&#25968;&#21644;&#25509;&#36817;&#24230;&#20013;&#24515;&#24615;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#20013;&#24515;&#24615;&#22312;&#32593;&#32476;&#31185;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26102;&#38388;&#25968;&#25454;&#20013;&#65292;&#38745;&#24577;&#22522;&#20110;&#36335;&#24452;&#30340;&#20013;&#24515;&#24615;&#22914;&#25509;&#36817;&#24230;&#25110;&#20171;&#25968;&#21487;&#33021;&#20250;&#23545;&#33410;&#28857;&#22312;&#26102;&#38388;&#22270;&#20013;&#30340;&#30495;&#23454;&#37325;&#35201;&#24615;&#20135;&#29983;&#35823;&#23548;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#23450;&#20041;&#20102;&#22522;&#20110;&#33410;&#28857;&#23545;&#20043;&#38388;&#26368;&#30701;&#26102;&#38388;&#36335;&#24452;&#30340;&#26102;&#38388;&#19968;&#33324;&#21270;&#20171;&#25968;&#21644;&#25509;&#36817;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#19968;&#33324;&#21270;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#35745;&#31639;&#36825;&#26679;&#30340;&#36335;&#24452;&#30340;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;De Bruijn&#22270;&#31070;&#32463;&#32593;&#32476;(DBGNN)&#65292;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#39044;&#27979;&#22522;&#20110;&#36335;&#24452;&#30340;&#26102;&#38388;&#20013;&#24515;&#24615;&#12290;&#25105;&#20204;&#22312;13&#20010;&#29983;&#29289;&#21644;&#31038;&#20132;&#31995;&#32479;&#30340;&#26102;&#38388;&#22270;&#20013;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#23427;&#30456;&#27604;&#38745;&#24577;&#22270;&#21367;&#31215;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#20171;&#25968;&#21644;&#25509;&#36817;&#24230;&#20013;&#24515;&#24615;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node centralities play a pivotal role in network science, social network analysis, and recommender systems. In temporal data, static path-based centralities like closeness or betweenness can give misleading results about the true importance of nodes in a temporal graph. To address this issue, temporal generalizations of betweenness and closeness have been defined that are based on the shortest time-respecting paths between pairs of nodes. However, a major issue of those generalizations is that the calculation of such paths is computationally expensive. Addressing this issue, we study the application of De Bruijn Graph Neural Networks (DBGNN), a causality-aware graph neural network architecture, to predict temporal path-based centralities in time series data. We experimentally evaluate our approach in 13 temporal graphs from biological and social systems and show that it considerably improves the prediction of both betweenness and closeness centrality compared to a static Graph Convolut
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#23398;&#20064;&#21010;&#20998;&#20107;&#20214;&#26102;&#38388;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#20107;&#20214;&#26102;&#38388;&#39044;&#27979;&#65292;&#36991;&#20813;&#20102;&#23545;&#20107;&#20214;&#23494;&#24230;&#36827;&#34892;&#24378;&#21442;&#25968;&#20551;&#35774;&#65292;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#29992;&#25968;&#25454;&#26377;&#38480;&#30340;&#20020;&#24202;&#29615;&#22659;&#20013;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15853</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21010;&#20998;&#20107;&#20214;&#26102;&#38388;&#31354;&#38388;&#26469;&#25913;&#36827;&#20107;&#20214;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Event Time Prediction by Learning to Partition the Event Time Space. (arXiv:2310.15853v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15853
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#23398;&#20064;&#21010;&#20998;&#20107;&#20214;&#26102;&#38388;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#20107;&#20214;&#26102;&#38388;&#39044;&#27979;&#65292;&#36991;&#20813;&#20102;&#23545;&#20107;&#20214;&#23494;&#24230;&#36827;&#34892;&#24378;&#21442;&#25968;&#20551;&#35774;&#65292;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#29992;&#25968;&#25454;&#26377;&#38480;&#30340;&#20020;&#24202;&#29615;&#22659;&#20013;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#22312;&#19968;&#20123;&#39044;&#20808;&#25351;&#23450;&#30340;&#65288;&#31163;&#25955;&#30340;&#65289;&#26102;&#38388;&#38388;&#38548;&#20869;&#20107;&#20214;&#21457;&#29983;&#30340;&#27010;&#29575;&#26469;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;&#36890;&#36807;&#36991;&#20813;&#23545;&#20107;&#20214;&#23494;&#24230;&#26045;&#21152;&#24378;&#28872;&#30340;&#21442;&#25968;&#20551;&#35774;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#24448;&#24448;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#21487;&#29992;&#25968;&#25454;&#26377;&#38480;&#65292;&#36890;&#24120;&#26356;&#21916;&#27426;&#23558;&#20107;&#20214;&#26102;&#38388;&#31354;&#38388;&#21010;&#20998;&#20026;&#23569;&#37327;&#36866;&#21512;&#25163;&#22836;&#39044;&#27979;&#20219;&#21153;&#30340;&#26102;&#38388;&#38388;&#38548;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21010;&#20998;&#23450;&#20041;&#30340;&#20999;&#28857;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#20004;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#19982;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30456;&#21305;&#37197;&#30340;&#26102;&#38388;&#38388;&#38548;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#30340;&#35266;&#23519;&#24615;&#25968;&#25454;&#38598;&#20013;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22823;&#22411;&#30340;&#12289;&#26032;&#35843;&#21644;&#30340;&#20013;&#39118;&#39118;&#38505;&#39044;&#27979;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#20020;&#24202;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed survival analysis methods improve upon existing approaches by predicting the probability of event occurrence in each of a number pre-specified (discrete) time intervals. By avoiding placing strong parametric assumptions on the event density, this approach tends to improve prediction performance, particularly when data are plentiful. However, in clinical settings with limited available data, it is often preferable to judiciously partition the event time space into a limited number of intervals well suited to the prediction task at hand. In this work, we develop a method to learn from data a set of cut points defining such a partition. We show that in two simulated datasets, we are able to recover intervals that match the underlying generative model. We then demonstrate improved prediction performance on three real-world observational datasets, including a large, newly harmonized stroke risk prediction dataset. Finally, we argue that our approach facilitates clinical d
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.15848</link><description>&lt;p&gt;
&#20851;&#20110;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#20844;&#24179;&#24615;&#12289;&#38544;&#31169;&#21644;&#27861;&#35268;&#20934;&#21017;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15848
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#36827;&#20837;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#20102;&#24778;&#20154;&#30340;&#25913;&#36827;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21487;&#20449;&#24615;&#23384;&#22312;&#20005;&#37325;&#25285;&#24551;&#12290;&#31185;&#23398;&#30028;&#33268;&#21147;&#20110;&#24320;&#21457;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#20854;&#24320;&#21457;&#36807;&#31243;&#20013;&#20005;&#37325;&#20381;&#36182;&#20351;&#29992;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#34892;&#20026;&#30446;&#26631;&#12290;&#25968;&#25454;&#20013;&#30340;&#20219;&#20309;&#32570;&#38519;&#37117;&#26377;&#21487;&#33021;&#30452;&#25509;&#36716;&#21270;&#20026;&#31639;&#27861;&#30340;&#32570;&#38519;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23545;&#31639;&#27861;&#30340;&#21518;&#26399;&#35780;&#20272;&#20197;&#30830;&#20445;&#20854;&#21487;&#20449;&#24615;&#65292;&#32780;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21333;&#29420;&#32771;&#34385;&#25968;&#25454;&#32452;&#20214;&#20197;&#29702;&#35299;&#20854;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. In recent years, there have been severe concerns over the trustworthiness of AI technologies. The scientific community has focused on the development of trustworthy AI algorithms. However, machine and deep learning algorithms, popular in the AI community today, depend heavily on the data used during their development. These learning algorithms identify patterns in the data, learning the behavioral objective. Any flaws in the data have the potential to translate directly into algorithms. In this study, we discuss the importance of Responsible Machine Learning Datasets and propose a framework to evaluate the datasets through a responsible rubric. While existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand it
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Weighted Graph Framework (DWGF)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20805;&#20998;&#21644;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26032;&#24847;&#22270;&#21457;&#29616;&#20013;&#26080;&#27861;&#24179;&#34913;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15836</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26032;&#24847;&#22270;&#21457;&#29616;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Diffusion Weighted Graph Framework for New Intent Discovery. (arXiv:2310.15836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Weighted Graph Framework (DWGF)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20805;&#20998;&#21644;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26032;&#24847;&#22270;&#21457;&#29616;&#20013;&#26080;&#27861;&#24179;&#34913;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24847;&#22270;&#21457;&#29616;&#26088;&#22312;&#36890;&#36807;&#26377;&#38480;&#30340;&#24102;&#26377;&#24050;&#30693;&#24847;&#22270;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#24110;&#21161;&#65292;&#35782;&#21035;&#20986;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#26032;&#24847;&#22270;&#21644;&#24050;&#30693;&#24847;&#22270;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#26410;&#32771;&#34385;&#26679;&#26412;&#20043;&#38388;&#30340;&#32467;&#26500;&#20851;&#31995;&#65292;&#29983;&#25104;&#30340;&#22122;&#22768;&#30417;&#30563;&#20449;&#21495;&#26080;&#27861;&#22312;&#25968;&#37327;&#21644;&#36136;&#37327;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#38459;&#30861;&#20102;&#26032;&#24847;&#22270;&#32858;&#31867;&#30340;&#24418;&#25104;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#26377;&#25928;&#20256;&#36882;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#26694;&#26550;&#65288;DWGF&#65289;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20805;&#20998;&#21644;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#27599;&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#27839;&#30528;&#30001;&#26368;&#36817;&#37051;&#25351;&#23548;&#30340;&#35821;&#20041;&#36335;&#24452;&#25193;&#25955;&#37051;&#22495;&#20851;&#31995;&#65292;&#20197;&#37492;&#21035;&#22320;&#21051;&#30011;&#20854;&#23616;&#37096;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#23616;&#37096;&#32467;&#26500;&#23545;&#20854;&#27491;&#26679;&#26412;&#36827;&#34892;&#25277;&#26679;&#21644;&#21152;&#26435;&#65292;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
New Intent Discovery (NID) aims to recognize both new and known intents from unlabeled data with the aid of limited labeled data containing only known intents. Without considering structure relationships between samples, previous methods generate noisy supervisory signals which cannot strike a balance between quantity and quality, hindering the formation of new intent clusters and effective transfer of the pre-training knowledge. To mitigate this limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to capture both semantic similarities and structure relationships inherent in data, enabling more sufficient and reliable supervisory signals. Specifically, for each sample, we diffuse neighborhood relationships along semantic paths guided by the nearest neighbors for multiple hops to characterize its local structure discriminately. Then, we sample its positive keys and weigh them based on semantic similarities and local structures for contrastive learning. During inferen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#21387;&#21147;&#27979;&#37327;&#36827;&#34892;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#23567;&#28431;&#27934;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22240;&#27668;&#20505;&#21464;&#21270;&#23548;&#33268;&#30340;&#39278;&#29992;&#27700;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15830</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#28418;&#31227;&#35299;&#37322;&#26041;&#27861;&#23545;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#30340;&#23567;&#28431;&#27934;&#36827;&#34892;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Localization of Small Leakages in Water Distribution Networks using Concept Drift Explanation Methods. (arXiv:2310.15830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#21387;&#21147;&#27979;&#37327;&#36827;&#34892;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#23567;&#28431;&#27934;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22240;&#27668;&#20505;&#21464;&#21270;&#23548;&#33268;&#30340;&#39278;&#29992;&#27700;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#39278;&#29992;&#27700;&#30340;&#21487;&#29992;&#24615;&#23558;&#26469;&#20250;&#20943;&#23569;&#65292;&#20351;&#24471;&#39278;&#29992;&#27700;&#25104;&#20026;&#36234;&#26469;&#36234;&#31232;&#32570;&#30340;&#36164;&#28304;&#12290;&#22823;&#37327;&#30340;&#27700;&#36890;&#36807;&#27700;&#36816;&#36755;&#21644;&#37197;&#27700;&#32593;&#32476;&#20013;&#30340;&#28431;&#27934;&#27969;&#22833;&#12290;&#28431;&#27934;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;&#38656;&#27714;&#30340;&#21464;&#21270;&#12290;&#23588;&#20854;&#26159;&#23567;&#28431;&#27934;&#24456;&#38590;&#30830;&#23450;&#65292;&#20294;&#23427;&#20204;&#30340;&#23450;&#20301;&#23545;&#20110;&#36991;&#20813;&#38271;&#26102;&#38388;&#30340;&#27700;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#23384;&#22312;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28431;&#27934;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31995;&#32479;&#30340;&#21508;&#31181;&#20449;&#24687;&#65292;&#20363;&#22914;&#23454;&#26102;&#38656;&#27714;&#27979;&#37327;&#21644;&#31934;&#30830;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#36825;&#22312;&#35768;&#22810;&#30495;&#23454;&#29615;&#22659;&#20013;&#26159;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#20165;&#20351;&#29992;&#21387;&#21147;&#27979;&#37327;&#26469;&#36827;&#34892;&#28431;&#27934;&#23450;&#20301;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#24314;&#31435;&#20102;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#30340;&#28431;&#27934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Facing climate change the already limited availability of drinking water will decrease in the future rendering drinking water an increasingly scarce resource. Considerable amounts of it are lost through leakages in water transportation and distribution networks. Leakage detection and localization are challenging problems due to the complex interactions and changing demands in water distribution networks. Especially small leakages are hard to pinpoint yet their localization is vital to avoid water loss over long periods of time. While there exist different approaches to solving the tasks of leakage detection and localization, they are relying on various information about the system, e.g. real-time demand measurements and the precise network topology, which is an unrealistic assumption in many real-world scenarios. In contrast, this work attempts leakage localization using pressure measurements only. For this purpose, first, leakages in the water distribution network are modeled employin
&lt;/p&gt;</description></item><item><title>&#37325;&#24230;&#22686;&#24378;&#12289;&#39640;&#20998;&#36776;&#29575;3D ResUNet&#33258;&#21160;&#20027;&#21160;&#33033;&#20998;&#21106;&#22312;SEG.A&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#32489;&#65292;&#36798;&#21040;&#20102;&#25152;&#26377;&#27979;&#35797;&#26696;&#20363;0.9&#20197;&#19978;&#30340;Dice&#20998;&#25968;&#65292;&#24182;&#22312;&#31283;&#23450;&#24615;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#21442;&#19982;&#32773;&#12290;&#23427;&#22312;&#20020;&#24202;&#35780;&#20272;&#12289;&#23450;&#37327;&#32467;&#26524;&#21644;&#20307;&#31215;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#21517;&#21015;&#21069;&#33541;&#12290;</title><link>http://arxiv.org/abs/2310.15827</link><description>&lt;p&gt;
&#12298;&#37325;&#24230;&#22686;&#24378;&#12289;&#39640;&#20998;&#36776;&#29575;3D ResUNet&#33258;&#21160;&#20027;&#21160;&#33033;&#20998;&#21106;&#65306;&#23545;SEG.A&#25361;&#25112;&#30340;&#36129;&#29486;&#12299;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Automatic Aorta Segmentation with Heavily Augmented, High-Resolution 3-D ResUNet: Contribution to the SEG.A Challenge. (arXiv:2310.15827v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15827
&lt;/p&gt;
&lt;p&gt;
&#37325;&#24230;&#22686;&#24378;&#12289;&#39640;&#20998;&#36776;&#29575;3D ResUNet&#33258;&#21160;&#20027;&#21160;&#33033;&#20998;&#21106;&#22312;SEG.A&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#32489;&#65292;&#36798;&#21040;&#20102;&#25152;&#26377;&#27979;&#35797;&#26696;&#20363;0.9&#20197;&#19978;&#30340;Dice&#20998;&#25968;&#65292;&#24182;&#22312;&#31283;&#23450;&#24615;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#21442;&#19982;&#32773;&#12290;&#23427;&#22312;&#20020;&#24202;&#35780;&#20272;&#12289;&#23450;&#37327;&#32467;&#26524;&#21644;&#20307;&#31215;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#21517;&#21015;&#21069;&#33541;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;3D&#21307;&#23398;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#20027;&#21160;&#33033;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#35768;&#22810;&#22240;&#32032;&#20351;&#24471;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#27604;&#22914;&#20027;&#21160;&#33033;&#22841;&#23618;&#30340;&#21487;&#33021;&#24615;&#25110;&#32773;&#23545;&#23567;&#20998;&#25903;&#36827;&#34892;&#20998;&#21106;&#21644;&#27880;&#37322;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MedGIFT&#22242;&#38431;&#22312;MICCAI 2023&#20250;&#35758;&#26399;&#38388;&#32452;&#32455;&#30340;SEG.A&#25361;&#25112;&#20013;&#30340;&#19968;&#39033;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20840;&#33258;&#21160;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#22686;&#24378;&#27604;&#28145;&#24230;&#26550;&#26500;&#26356;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#20256;&#32479;&#30340;&#21367;&#31215;U-Net&#30340;&#21464;&#20307;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#25152;&#26377;&#27979;&#35797;&#26696;&#20363;&#20013;&#37117;&#23454;&#29616;&#20102;0.9&#20197;&#19978;&#30340;Dice&#20998;&#25968;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#20855;&#26377;&#26368;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;&#35780;&#20272;&#12289;&#23450;&#37327;&#32467;&#26524;&#21644;&#20307;&#31215;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#20998;&#21035;&#25490;&#21517;&#31532;&#19968;&#12289;&#31532;&#22235;&#21644;&#31532;&#19977;&#12290;&#25105;&#20204;&#20813;&#36153;&#20844;&#24320;&#28304;&#20195;&#30721;&#65292;
&lt;/p&gt;
&lt;p&gt;
Automatic aorta segmentation from 3-D medical volumes is an important yet difficult task. Several factors make the problem challenging, e.g. the possibility of aortic dissection or the difficulty with segmenting and annotating the small branches. This work presents a contribution by the MedGIFT team to the SEG.A challenge organized during the MICCAI 2023 conference. We propose a fully automated algorithm based on deep encoder-decoder architecture. The main assumption behind our work is that data preprocessing and augmentation are much more important than the deep architecture, especially in low data regimes. Therefore, the solution is based on a variant of traditional convolutional U-Net. The proposed solution achieved a Dice score above 0.9 for all testing cases with the highest stability among all participants. The method scored 1st, 4th, and 3rd in terms of the clinical evaluation, quantitative results, and volumetric meshing quality, respectively. We freely release the source code,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38750;&#30417;&#30563;&#25968;&#25454;&#27969;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#21644;&#31995;&#32479;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#23545;&#28418;&#31227;&#26816;&#27979;&#21644;&#28418;&#31227;&#23450;&#20301;&#30340;&#26368;&#26032;&#30740;&#31350;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#38382;&#39064;&#30340;&#20934;&#30830;&#25968;&#23398;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.15826</link><description>&lt;p&gt;
&#20851;&#20110;&#27010;&#24565;&#28418;&#31227;&#25105;&#20204;&#25152;&#30693;&#30340;&#19968;&#20004;&#20214;&#20107; - &#23545;&#30417;&#27979;&#28436;&#21464;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
One or Two Things We know about Concept Drift -- A Survey on Monitoring Evolving Environments. (arXiv:2310.15826v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38750;&#30417;&#30563;&#25968;&#25454;&#27969;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#21644;&#31995;&#32479;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#23545;&#28418;&#31227;&#26816;&#27979;&#21644;&#28418;&#31227;&#23450;&#20301;&#30340;&#26368;&#26032;&#30740;&#31350;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#38382;&#39064;&#30340;&#20934;&#30830;&#25968;&#23398;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21608;&#22260;&#30340;&#19990;&#30028;&#19981;&#26029;&#21464;&#21270;&#12290;&#36825;&#20123;&#21464;&#21270;&#32463;&#24120;&#34987;&#25551;&#36848;&#20026;&#27010;&#24565;&#28418;&#31227;&#65292;&#24433;&#21709;&#30528;&#35768;&#22810;&#24037;&#19994;&#21644;&#25216;&#26415;&#36807;&#31243;&#12290;&#30001;&#20110;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#25925;&#38556;&#21644;&#20854;&#20182;&#24322;&#24120;&#34892;&#20026;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#21487;&#33021;&#20855;&#26377;&#23433;&#20840;&#39118;&#38505;&#65292;&#22240;&#27492;&#26816;&#27979;&#21644;&#20998;&#26512;&#27010;&#24565;&#28418;&#31227;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20851;&#20110;&#38750;&#30417;&#30563;&#25968;&#25454;&#27969;&#20013;&#27010;&#24565;&#28418;&#31227;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#34429;&#28982;&#35768;&#22810;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#30417;&#30563;&#25968;&#25454;&#27969;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#38024;&#23545;&#38750;&#30417;&#30563;&#29615;&#22659;&#30340;&#24037;&#20316;&#36827;&#34892;&#32508;&#36848;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#32622;&#23545;&#20110;&#30417;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#29305;&#21035;&#37325;&#35201;&#65292;&#36825;&#20123;&#25216;&#26415;&#30452;&#25509;&#36866;&#29992;&#20110;&#24037;&#31243;&#20013;&#30340;&#35768;&#22810;&#20219;&#21153;&#21644;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#29616;&#26377;&#28418;&#31227;&#26816;&#27979;&#24037;&#20316;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#28085;&#30422;&#20102;&#28418;&#31227;&#23450;&#20301;&#30740;&#31350;&#30340;&#26368;&#26032;&#29366;&#24577;&#12290;&#38500;&#20102;&#25552;&#20379;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#23545;&#25152;&#28041;&#38382;&#39064;&#30340;&#31934;&#30830;&#23450;&#20041;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The world surrounding us is subject to constant change. These changes, frequently described as concept drift, influence many industrial and technical processes. As they can lead to malfunctions and other anomalous behavior, which may be safety-critical in many scenarios, detecting and analyzing concept drift is crucial. In this paper, we provide a literature review focusing on concept drift in unsupervised data streams. While many surveys focus on supervised data streams, so far, there is no work reviewing the unsupervised setting. However, this setting is of particular relevance for monitoring and anomaly detection which are directly applicable to many tasks and challenges in engineering. This survey provides a taxonomy of existing work on drift detection. Besides, it covers the current state of research on drift localization in a systematic way. In addition to providing a systematic literature review, this work provides precise mathematical definitions of the considered problems and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#26469;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#26469;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#24773;&#20917;&#12290;&#22312;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#20013;&#65292;&#21028;&#21035;&#22120;&#24341;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15817</link><description>&lt;p&gt;
&#21028;&#21035;&#22120;&#24341;&#23548;&#19979;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminator Guidance for Autoregressive Diffusion Models. (arXiv:2310.15817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#26469;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#26469;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#24773;&#20917;&#12290;&#22312;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#20013;&#65292;&#21028;&#21035;&#22120;&#24341;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#12290;&#22312;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#21028;&#21035;&#22120;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#20351;&#29992;&#36807;&#65292;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;&#31163;&#25955;&#24773;&#20917;&#19979;&#20351;&#29992;&#21028;&#21035;&#22120;&#21644;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#23558;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#33021;&#22815;&#20174;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#20013;&#31934;&#30830;&#37319;&#26679;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36845;&#20195;&#22320;&#23558;&#21028;&#21035;&#22120;&#30340;&#39044;&#27979;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#21028;&#21035;&#22120;&#30456;&#36739;&#20110;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discrimiator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#26500;&#24314;&#20998;&#24067;&#24335;&#21160;&#21147;&#31995;&#32479;&#30340;&#38477;&#38454;&#27169;&#22411;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#21487;&#20197;&#22238;&#36991;&#23545;&#31934;&#30830;&#25130;&#26029;&#25237;&#24433;&#21644;&#23553;&#38381;&#24335;&#20462;&#27491;&#30340;&#38656;&#27714;&#12290;&#33258;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#26144;&#23556;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#21644;&#27979;&#35797;&#21512;&#36866;&#30340;&#28508;&#21464;&#37327;&#38598;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#34920;&#36798;ROMs&#30340;&#19981;&#21516;&#22352;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.15816</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#38477;&#32500;&#30340;&#36807;&#21435;&#19982;&#29616;&#22312;&#65306;&#26426;&#22120;&#23398;&#20064;&#26102;&#20195;&#32791;&#25955;PDEs&#30340;AIMs
&lt;/p&gt;
&lt;p&gt;
Nonlinear dimensionality reduction then and now: AIMs for dissipative PDEs in the ML era. (arXiv:2310.15816v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#26500;&#24314;&#20998;&#24067;&#24335;&#21160;&#21147;&#31995;&#32479;&#30340;&#38477;&#38454;&#27169;&#22411;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#21487;&#20197;&#22238;&#36991;&#23545;&#31934;&#30830;&#25130;&#26029;&#25237;&#24433;&#21644;&#23553;&#38381;&#24335;&#20462;&#27491;&#30340;&#38656;&#27714;&#12290;&#33258;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#26144;&#23556;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#21644;&#27979;&#35797;&#21512;&#36866;&#30340;&#28508;&#21464;&#37327;&#38598;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#34920;&#36798;ROMs&#30340;&#19981;&#21516;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#26500;&#24314;&#20998;&#24067;&#24335;&#21160;&#21147;&#31995;&#32479;&#30340;&#38477;&#38454;&#27169;&#22411;&#65288;ROMs&#65289;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#21463;Approximate Inertial Manifolds&#65288;AIMs&#65289;&#29702;&#35770;&#21551;&#21457;&#24182;&#20197;&#20854;&#20026;&#27169;&#26495;&#30340;&#25968;&#25454;&#36741;&#21161;&#27169;&#22411;&#65307;&#20854;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#24212;&#29992;&#26159;Garcia-Archilla&#65292;Novo&#21644;Titi&#25552;&#20986;&#30340;&#21518;&#22788;&#29702;Galerkin&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#35268;&#36991;&#23545;&#31934;&#30830;&#25130;&#26029;Galerkin&#25237;&#24433;&#21644;&#27966;&#29983;&#23553;&#38381;&#24335;&#20462;&#27491;&#30340;&#38656;&#27714;&#12290;&#24403;&#27491;&#30830;&#30340;&#28508;&#21464;&#37327;&#20107;&#20808;&#19981;&#30693;&#36947;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#26144;&#23556;&#65288;&#19968;&#31181;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65289;&#26469;&#21457;&#29616;&#22909;&#30340;&#28508;&#21464;&#37327;&#38598;&#21512;&#24182;&#27979;&#35797;&#35299;&#37322;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#65288;a&#65289;&#29702;&#35770;&#65288;Fourier&#31995;&#25968;&#65289;&#65292;&#65288;b&#65289;&#32447;&#24615;&#25968;&#25454;&#39537;&#21160;&#65288;POD&#27169;&#24577;&#65289;&#21644;/&#25110;&#65288;c&#65289;&#38750;&#32447;&#24615;&#25968;&#25454;&#39537;&#21160;&#65288;&#25193;&#25955;&#26144;&#23556;&#65289;&#22352;&#26631;&#26469;&#34920;&#36798;ROMs&#12290;&#21478;&#22806;&#65292;Bla
&lt;/p&gt;
&lt;p&gt;
This study presents a collection of purely data-driven workflows for constructing reduced-order models (ROMs) for distributed dynamical systems. The ROMs we focus on, are data-assisted models inspired by, and templated upon, the theory of Approximate Inertial Manifolds (AIMs); the particular motivation is the so-called post-processing Galerkin method of Garcia-Archilla, Novo and Titi. Its applicability can be extended: the need for accurate truncated Galerkin projections and for deriving closed-formed corrections can be circumvented using machine learning tools. When the right latent variables are not a priori known, we illustrate how autoencoders as well as Diffusion Maps (a manifold learning scheme) can be used to discover good sets of latent variables and test their explainability. The proposed methodology can express the ROMs in terms of (a) theoretical (Fourier coefficients), (b) linear data-driven (POD modes) and/or (c) nonlinear data-driven (Diffusion Maps) coordinates. Both Bla
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#28608;&#21169;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;SMILE&#65289;&#65292;&#36890;&#36807;&#36807;&#28388;&#20302;&#20110;&#24403;&#21069;&#31574;&#30053;&#30340;&#28436;&#31034;&#26469;&#35299;&#20915;&#27169;&#20223;&#23398;&#20064;&#21463;&#21040;&#22024;&#26434;&#28436;&#31034;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#38468;&#21152;&#20449;&#24687;&#12290;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#36807;&#31243;&#26469;&#25552;&#21462;&#25193;&#25955;&#19987;&#19994;&#27700;&#24179;&#30340;&#22122;&#22768;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#39044;&#27979;&#24403;&#21069;&#31574;&#30053;&#21644;&#28436;&#31034;&#32773;&#20043;&#38388;&#30340;&#19987;&#19994;&#27700;&#24179;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.15815</link><description>&lt;p&gt;
&#22909;&#65292;&#26356;&#22909;&#65292;&#26368;&#22909;&#65306;&#33258;&#25105;&#28608;&#21169;&#30340;&#27169;&#20223;&#23398;&#20064;&#22312;&#22024;&#26434;&#28436;&#31034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Good Better Best: Self-Motivated Imitation Learning for noisy Demonstrations. (arXiv:2310.15815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#28608;&#21169;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;SMILE&#65289;&#65292;&#36890;&#36807;&#36807;&#28388;&#20302;&#20110;&#24403;&#21069;&#31574;&#30053;&#30340;&#28436;&#31034;&#26469;&#35299;&#20915;&#27169;&#20223;&#23398;&#20064;&#21463;&#21040;&#22024;&#26434;&#28436;&#31034;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#38468;&#21152;&#20449;&#24687;&#12290;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#36807;&#31243;&#26469;&#25552;&#21462;&#25193;&#25955;&#19987;&#19994;&#27700;&#24179;&#30340;&#22122;&#22768;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#39044;&#27979;&#24403;&#21069;&#31574;&#30053;&#21644;&#28436;&#31034;&#32773;&#20043;&#38388;&#30340;&#19987;&#19994;&#27700;&#24179;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#26234;&#33021;&#20307;&#34892;&#20026;&#19982;&#19987;&#23478;&#28436;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#21457;&#29616;&#19968;&#31181;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26469;&#33258;&#38750;&#19987;&#23478;&#34892;&#20026;&#30340;&#22024;&#26434;&#28436;&#31034;&#30340;&#38480;&#21046;&#65292;&#27169;&#20223;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#35780;&#20272;&#20182;&#20204;&#19987;&#19994;&#27700;&#24179;&#30340;&#38468;&#21152;&#20449;&#24687;&#32780;&#36896;&#25104;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#28608;&#21169;&#30340;&#27169;&#20223;&#23398;&#20064;(SMILE)&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36880;&#27493;&#36807;&#28388;&#25481;&#34987;&#35748;&#20026;&#20302;&#20110;&#24403;&#21069;&#31574;&#30053;&#30340;&#31574;&#30053;&#25152;&#25910;&#38598;&#30340;&#28436;&#31034;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#39069;&#22806;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#36807;&#31243;&#26469;&#27169;&#25311;&#28436;&#31034;&#19987;&#19994;&#27700;&#24179;&#20174;&#20302;&#21040;&#39640;&#21644;&#30456;&#21453;&#26041;&#21521;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#25552;&#21462;&#25193;&#25955;&#19987;&#19994;&#27700;&#24179;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#22122;&#22768;&#20449;&#24687;&#26469;&#39044;&#27979;&#24403;&#21069;&#31574;&#30053;&#21644;&#28436;&#31034;&#32773;&#20043;&#38388;&#30340;&#25193;&#25955;&#27493;&#39588;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#19982;&#20182;&#20204;&#30340;&#19987;&#19994;&#27700;&#24179;&#24046;&#36317;&#30340;&#31561;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) aims to discover a policy by minimizing the discrepancy between the agent's behavior and expert demonstrations. However, IL is susceptible to limitations imposed by noisy demonstrations from non-expert behaviors, presenting a significant challenge due to the lack of supplementary information to assess their expertise. In this paper, we introduce Self-Motivated Imitation LEarning (SMILE), a method capable of progressively filtering out demonstrations collected by policies deemed inferior to the current policy, eliminating the need for additional information. We utilize the forward and reverse processes of Diffusion Models to emulate the shift in demonstration expertise from low to high and vice versa, thereby extracting the noise information that diffuses expertise. Then, the noise information is leveraged to predict the diffusion steps between the current policy and demonstrators, which we theoretically demonstrate its equivalence to their expertise gap. We furt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#32452;&#21512;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#24403;&#21069;&#31574;&#30053;&#31867;&#20284;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#22240;&#20026;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#19979;&#65292;&#23454;&#20307;&#30721;&#26377;&#26356;&#39640;&#30340;&#29109;&#21644;&#30721;&#23383;&#32423;&#21035;&#30340;Jaccard&#36317;&#31163;&#65292;&#20351;&#24471;&#19981;&#21516;&#23454;&#20307;&#26356;&#23481;&#26131;&#21306;&#20998;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#12290;</title><link>http://arxiv.org/abs/2310.15797</link><description>&lt;p&gt;
&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#32452;&#21512;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation. (arXiv:2310.15797v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#32452;&#21512;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#24403;&#21069;&#31574;&#30053;&#31867;&#20284;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#22240;&#20026;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#19979;&#65292;&#23454;&#20307;&#30721;&#26377;&#26356;&#39640;&#30340;&#29109;&#21644;&#30721;&#23383;&#32423;&#21035;&#30340;Jaccard&#36317;&#31163;&#65292;&#20351;&#24471;&#19981;&#21516;&#23454;&#20307;&#26356;&#23481;&#26131;&#21306;&#20998;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20027;&#23548;&#26041;&#27861;KG&#23884;&#20837;&#65288;KGE&#65289;&#36890;&#36807;&#29420;&#31435;&#21521;&#37327;&#34920;&#31034;&#23454;&#20307;&#65292;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#25928;&#29575;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#39044;&#23450;&#20041;&#30340;&#23567;&#35268;&#27169;&#30721;&#20070;&#20013;&#21305;&#37197;&#23454;&#20307;&#23545;&#24212;&#30340;&#30721;&#23383;&#26469;&#34920;&#31034;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#33719;&#21462;&#27599;&#20010;&#23454;&#20307;&#23545;&#24212;&#30721;&#23383;&#30340;&#36807;&#31243;&#31216;&#20026;&#23454;&#20307;&#37327;&#21270;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#22797;&#26434;&#30340;&#31574;&#30053;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26412;&#25991;&#34920;&#26126;&#31616;&#21333;&#30340;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#21487;&#20197;&#23454;&#29616;&#19982;&#24403;&#21069;&#31574;&#30053;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#29616;&#35937;&#24182;&#25581;&#31034;&#20102;&#22312;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#19979;&#65292;&#34920;&#31034;&#23454;&#20307;&#30340;&#37327;&#21270;&#32467;&#26524;-&#23454;&#20307;&#30721;&#20855;&#26377;&#26356;&#39640;&#30340;&#29109;&#21644;&#30721;&#23383;&#32423;&#21035;&#30340;Jaccard&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#23454;&#20307;&#26356;&#23481;&#26131;&#21306;&#20998;&#65292;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;KG&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents entities by composing entity-corresponding codewords matched from predefined small-scale codebooks. We refer to the process of obtaining corresponding codewords of each entity as entity quantization, for which previous works have designed complicated strategies. Surprisingly, this paper shows that simple random entity quantization can achieve similar results to current strategies. We analyze this phenomenon and reveal that entity codes, the quantization outcomes for expressing entities, have higher entropy at the code level and Jaccard distance at the codeword level under random entity quantization. Therefore, different entities become more easily distinguished, facilitating effective KG represen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#21069;&#32512;&#23376;&#31354;&#38388;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25972;&#20010;&#21333;&#32431;&#24418;&#27169;&#22411;&#65292;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#24191;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15793</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21069;&#32512;&#23376;&#31354;&#38388;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving generalization in large language models by learning prefix subspaces. (arXiv:2310.15793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#21069;&#32512;&#23376;&#31354;&#38388;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25972;&#20010;&#21333;&#32431;&#24418;&#27169;&#22411;&#65292;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#24191;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#24494;&#35843;&#65288;&#20063;&#34987;&#31216;&#20026;&#8220;&#23569;&#26679;&#26412;&#8221;&#23398;&#20064;&#35774;&#32622;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#20248;&#21270;&#26041;&#27861;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24341;&#20837;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25972;&#20010;&#21333;&#32431;&#24418;&#27169;&#22411;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#35782;&#21035;&#26356;&#24191;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#36866;&#24212;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#21017;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#22823;&#37327;&#30340;&#21442;&#25968;&#20351;&#24471;&#32852;&#21512;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#65292;&#20854;&#27425;&#65292;&#23427;&#20204;&#30340;&#30830;&#23450;&#24615;&#21442;&#25968;&#21021;&#22987;&#21270;&#26041;&#26696;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#26368;&#21021;&#30340;&#23376;&#31354;&#38388;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#65292;&#8220;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#8221;&#65288;PEFT&#65289;&#26041;&#27861;&#19982;&#26368;&#21021;&#30340;&#26041;&#27861;&#23436;&#20840;&#20860;&#23481;&#65292;&#24182;&#25552;&#20986;&#23398;&#20064;&#25972;&#20010;&#36830;&#32493;&#21069;&#32512;&#30340;&#21333;&#32431;&#24418;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as the "few-shot" learning setting). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Its adaptation to massive, pretrained transformers, however, poses some challenges. First, their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT) methods, however, are perfectly compatible with this original approach, and propose to learn entire simplex of continuous prefixes. We test our method on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#23567;&#35268;&#27169;&#27010;&#29575;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35825;&#23548;&#36755;&#20837;&#26367;&#25442;&#20026;&#23454;&#38469;&#25968;&#25454;&#65292;&#36890;&#36807;&#35757;&#32451;&#25512;&#29702;&#32593;&#32476;&#26469;&#23454;&#29616;&#23545;&#20219;&#21153;&#29305;&#23450;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#20803;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.15786</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20998;&#25674;&#25512;&#29702;&#29992;&#20110;&#23567;&#35268;&#27169;&#27010;&#29575;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Amortised Inference in Neural Networks for Small-Scale Probabilistic Meta-Learning. (arXiv:2310.15786v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#23567;&#35268;&#27169;&#27010;&#29575;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35825;&#23548;&#36755;&#20837;&#26367;&#25442;&#20026;&#23454;&#38469;&#25968;&#25454;&#65292;&#36890;&#36807;&#35757;&#32451;&#25512;&#29702;&#32593;&#32476;&#26469;&#23454;&#29616;&#23545;&#20219;&#21153;&#29305;&#23450;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#20803;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20840;&#23616;&#35825;&#23548;&#28857;&#30340;&#21464;&#20998;&#36924;&#36817;&#26159;&#22522;&#20110;&#20351;&#29992;&#19968;&#32452;&#35825;&#23548;&#36755;&#20837;&#26469;&#26500;&#24314;&#19968;&#31995;&#21015;&#26465;&#20214;&#20998;&#24067;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#36924;&#36817;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36825;&#20123;&#35825;&#23548;&#36755;&#20837;&#21487;&#20197;&#34987;&#23454;&#38469;&#25968;&#25454;&#26367;&#20195;&#65292;&#20351;&#24471;&#21464;&#20998;&#20998;&#24067;&#30001;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#19968;&#32452;&#36817;&#20284;&#20284;&#28982;&#32452;&#25104;&#12290;&#36825;&#31181;&#32467;&#26500;&#36866;&#21512;&#20110;&#20998;&#25674;&#25512;&#29702;&#65292;&#20854;&#20013;&#27599;&#20010;&#36817;&#20284;&#20284;&#28982;&#30340;&#21442;&#25968;&#26159;&#36890;&#36807;&#23558;&#27599;&#20010;&#25968;&#25454;&#28857;&#36890;&#36807;&#31216;&#20026;&#25512;&#29702;&#32593;&#32476;&#30340;&#20803;&#27169;&#22411;&#36890;&#36807;&#33719;&#24471;&#30340;&#12290;&#36890;&#36807;&#22312;&#30456;&#20851;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#36825;&#20010;&#25512;&#29702;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#20803;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;BNN&#19978;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The global inducing point variational approximation for BNNs is based on using a set of inducing inputs to construct a series of conditional distributions that accurately approximate the conditionals of the true posterior distribution. Our key insight is that these inducing inputs can be replaced by the actual data, such that the variational distribution consists of a set of approximate likelihoods for each datapoint. This structure lends itself to amortised inference, in which the parameters of each approximate likelihood are obtained by passing each datapoint through a meta-model known as the inference network. By training this inference network across related datasets, we can meta-learn Bayesian inference over task-specific BNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26080;&#37197;&#23545;MRI&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#25552;&#39640;SR&#24615;&#33021;&#65292;&#25913;&#21892;MRI&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.15767</link><description>&lt;p&gt;
&#26080;&#37197;&#23545;MRI&#36229;&#20998;&#36776;&#29575;&#19982;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning. (arXiv:2310.15767v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26080;&#37197;&#23545;MRI&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#25552;&#39640;SR&#24615;&#33021;&#65292;&#25913;&#21892;MRI&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;MRI&#20998;&#36776;&#29575;&#30340;&#22266;&#26377;&#38480;&#21046;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#26041;&#27861;&#23637;&#29616;&#20102;&#25552;&#21319;MRI&#20998;&#36776;&#29575;&#30340;&#28508;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;HR MRI&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#21487;&#33021;&#38590;&#20197;&#33719;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#37197;&#23545;MRI SR&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19979;&#30340;SR&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#30495;&#23454;&#30340;HR&#22270;&#20687;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;SR&#22270;&#20687;&#26500;&#24314;&#27491;&#36127;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#20419;&#36827;&#36776;&#21035;&#24615;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#21576;&#29616;&#30340;&#23454;&#35777;&#32467;&#26524;&#31361;&#20986;&#20102;&#23792;&#20540;&#20449;&#22122;&#27604;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#21363;&#20351;&#32570;&#20047;HR&#22270;&#20687;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution (HR) magnetic resonance imaging (MRI) is crucial for enhancing diagnostic accuracy in clinical settings. Nonetheless, the inherent limitation of MRI resolution restricts its widespread applicability. Deep learning-based image super-resolution (SR) methods exhibit promise in improving MRI resolution without additional cost. However, these methods frequently require a substantial number of HR MRI images for training, which can be challenging to acquire. In this paper, we propose an unpaired MRI SR approach that employs self-supervised contrastive learning to enhance SR performance with limited training data. Our approach leverages both authentic HR images and synthetically generated SR images to construct positive and negative sample pairs, thus facilitating the learning of discriminative features. Empirical results presented in this study underscore significant enhancements in the peak signal-to-noise ratio and structural similarity index, even when a paucity of HR image
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPA&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21307;&#30103;&#25968;&#25454;&#20013;&#19981;&#31283;&#23450;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21453;&#22240;&#26524;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#24739;&#30149;&#29575;&#26469;&#22788;&#29702;&#28151;&#28102;&#21464;&#37327;&#21644;&#26631;&#31614;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#40065;&#26834;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.15766</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#24615;&#24739;&#30149;&#29575;&#35843;&#25972;&#23454;&#29616;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning via Conditional Prevalence Adjustment. (arXiv:2310.15766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPA&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21307;&#30103;&#25968;&#25454;&#20013;&#19981;&#31283;&#23450;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21453;&#22240;&#26524;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#24739;&#30149;&#29575;&#26469;&#22788;&#29702;&#28151;&#28102;&#21464;&#37327;&#21644;&#26631;&#31614;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#40065;&#26834;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#25968;&#25454;&#36890;&#24120;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#65292;&#20854;&#20013;&#28151;&#28102;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#33021;&#21464;&#21270;&#24456;&#22823;&#12290;&#22914;&#26524;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#36825;&#20123;&#19981;&#31283;&#23450;&#30340;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#21487;&#33021;&#22312;&#26410;&#30693;&#30340;&#26469;&#28304;&#20013;&#21457;&#29983;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#31283;&#23450;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#27599;&#31181;&#26041;&#27861;&#37117;&#26377;&#20854;&#23616;&#38480;&#24615;&#12290;&#20363;&#22914;&#65292;&#23545;&#25239;&#35757;&#32451;&#24378;&#21046;&#27169;&#22411;&#23436;&#20840;&#24573;&#30053;&#19981;&#31283;&#23450;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#36825;&#26679;&#20570;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#19981;&#20339;&#12290;&#20854;&#20182;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65289;&#35797;&#22270;&#23398;&#20064;&#20165;&#20381;&#36182;&#20110;&#31283;&#23450;&#20851;&#32852;&#30340;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#36890;&#36807;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#22240;&#26524;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65288;&#36755;&#20837;X&#23548;&#33268;&#31867;&#21035;&#26631;&#31614;Y&#65289;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#23545;&#21453;&#22240;&#26524;&#20219;&#21153;&#65288;Y&#23548;&#33268;X&#65289;&#21487;&#33021;&#19981;&#36215;&#20316;&#29992;&#65292;&#36825;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPA&#65288;&#26465;&#20214;&#24739;&#30149;&#29575;&#35843;&#25972;&#65289;&#30340;&#21453;&#22240;&#26524;&#20219;&#21153;&#26041;&#27861;&#12290;CoPA&#20551;&#35774;&#65288;1&#65289;&#29983;&#25104;&#26426;&#21046;&#31283;&#23450;&#65292;&#21363;&#26631;&#31614;Y&#21644;&#28151;&#28102;&#21464;&#37327;Z&#29983;&#25104;X&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare data often come from multiple sites in which the correlations between confounding variables can vary widely. If deep learning models exploit these unstable correlations, they might fail catastrophically in unseen sites. Although many methods have been proposed to tackle unstable correlations, each has its limitations. For example, adversarial training forces models to completely ignore unstable correlations, but doing so may lead to poor predictive performance. Other methods (e.g. Invariant risk minimization [4]) try to learn domain-invariant representations that rely only on stable associations by assuming a causal data-generating process (input X causes class label Y ). Thus, they may be ineffective for anti-causal tasks (Y causes X), which are common in computer vision. We propose a method called CoPA (Conditional Prevalence-Adjustment) for anti-causal tasks. CoPA assumes that (1) generation mechanism is stable, i.e. label Y and confounding variable(s) Z generate X, and (
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#25345;&#20037;&#24615;&#25299;&#25169;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#21363;TNMF&#21644;rTNMF&#65292;&#29992;&#20110;&#20998;&#26512;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TNMF&#21644;rTNMF&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;NMF&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#21487;&#35270;&#21270;UMAP&#21644;t-SNE&#12290;</title><link>http://arxiv.org/abs/2310.15744</link><description>&lt;p&gt;
&#20351;&#29992;&#25299;&#25169;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20998;&#26512;&#21333;&#32454;&#32990;RNA&#27979;&#24207;
&lt;/p&gt;
&lt;p&gt;
Analyzing Single Cell RNA Sequencing with Topological Nonnegative Matrix Factorization. (arXiv:2310.15744v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#25345;&#20037;&#24615;&#25299;&#25169;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#21363;TNMF&#21644;rTNMF&#65292;&#29992;&#20110;&#20998;&#26512;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TNMF&#21644;rTNMF&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;NMF&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#21487;&#35270;&#21270;UMAP&#21644;t-SNE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;RNA&#27979;&#24207;(scRNA-seq)&#26159;&#19968;&#39033;&#30456;&#23545;&#36739;&#26032;&#30340;&#25216;&#26415;&#65292;&#30001;&#20110;&#20854;&#39640;&#32500;&#24230;&#12289;&#22797;&#26434;&#24615;&#21644;&#22823;&#35268;&#27169;&#24615;&#65292;&#24341;&#36215;&#20102;&#32479;&#35745;&#23398;&#12289;&#25968;&#25454;&#31185;&#23398;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#39046;&#22495;&#30340;&#24040;&#22823;&#20852;&#36259;&#12290;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#22240;&#20854;&#23545;&#32467;&#26524;&#20302;&#32500;&#24230;&#20998;&#37327;&#30340;&#20803;&#22522;&#22240;&#35299;&#37322;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;NMF&#26041;&#27861;&#22312;&#32570;&#20047;&#22810;&#23610;&#24230;&#20998;&#26512;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#25345;&#20037;&#24615;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;NMF&#26041;&#27861;&#65292;&#21363;&#25299;&#25169;NMF(TNMF)&#21644;&#40065;&#26834;&#25299;&#25169;NMF(rTNMF)&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#35745;12&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;TNMF&#21644;rTNMF&#26174;&#33879;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;NMF&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;TNMF&#21644;rTNMF&#23545;&#27969;&#34892;&#30340;UMAP(Uniform Manifold Approximation and Projection)&#21644;t-SNE(t-distributed stochastic neighbor embedding)&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-cell RNA sequencing (scRNA-seq) is a relatively new technology that has stimulated enormous interest in statistics, data science, and computational biology due to the high dimensionality, complexity, and large scale associated with scRNA-seq data. Nonnegative matrix factorization (NMF) offers a unique approach due to its meta-gene interpretation of resulting low-dimensional components. However, NMF approaches suffer from the lack of multiscale analysis. This work introduces two persistent Laplacian regularized NMF methods, namely, topological NMF (TNMF) and robust topological NMF (rTNMF). By employing a total of 12 datasets, we demonstrate that the proposed TNMF and rTNMF significantly outperform all other NMF-based methods. We have also utilized TNMF and rTNMF for the visualization of popular Uniform Manifold Approximation and Projection (UMAP) and t-distributed stochastic neighbor embedding (t-SNE).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;PulseDiff&#65292;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#20808;&#39564;&#26469;&#25913;&#36827;&#24515;&#30005;&#22270;(ECG)&#30340;&#25554;&#20540;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#27169;&#22411;&#32771;&#34385;&#20102;&#20027;&#20307;&#38388;&#21464;&#21270;&#21644;&#24515;&#36339;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15742</link><description>&lt;p&gt;
&#29992;&#22686;&#24378;&#30340;&#27169;&#26495;&#20808;&#39564;&#25913;&#36827;&#24515;&#30005;&#22270;(ECG)&#25554;&#20540;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion Models for ECG Imputation with an Augmented Template Prior. (arXiv:2310.15742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;PulseDiff&#65292;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#20808;&#39564;&#26469;&#25913;&#36827;&#24515;&#30005;&#22270;(ECG)&#30340;&#25554;&#20540;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#27169;&#22411;&#32771;&#34385;&#20102;&#20027;&#20307;&#38388;&#21464;&#21270;&#21644;&#24515;&#36339;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#24120;&#35268;&#20020;&#24202;&#25252;&#29702;&#30340;&#19968;&#37096;&#20998;&#65292;&#22914;&#24515;&#30005;&#22270;(ECG)&#31561;&#33033;&#20914;&#20449;&#21495;&#34987;&#24191;&#27867;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31227;&#21160;&#20581;&#24247;&#31995;&#32479;&#25910;&#38598;&#30340;&#20449;&#21495;&#23384;&#22312;&#22122;&#38899;&#21644;&#36136;&#37327;&#24046;&#30340;&#24405;&#38899;&#65292;&#23548;&#33268;&#32570;&#22833;&#20540;&#30340;&#23384;&#22312;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#38477;&#20302;&#20102;&#20449;&#21495;&#30340;&#36136;&#37327;&#65292;&#24182;&#24433;&#21709;&#20102;&#33258;&#21160;&#21270;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#23545;ECG&#36827;&#34892;&#32570;&#22833;&#20540;&#25554;&#34917;&#12290;&#28982;&#32780;&#65292;&#19982;&#30830;&#23450;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#28982;&#26377;&#38480;&#65292;&#22240;&#20026;&#35757;&#32451;&#30446;&#26631;&#20013;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#20027;&#20307;&#38388;&#30340;&#21464;&#21270;&#21644;&#24515;&#36339;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#25913;&#21892;&#27010;&#29575;&#27169;&#22411;&#30340;ECG&#25554;&#20540;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#20449;&#24687;&#20808;&#39564;&#30340;&#27169;&#26495;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;PulseDiff&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;1)&#25105;&#20204;&#39318;&#20808;&#20174;&#35266;&#27979;&#20013;&#25552;&#21462;&#19968;&#20010;&#20027;&#20307;&#32423;&#21035;&#30340;&#33033;&#20914;&#27169;&#26495;&#20316;&#20026;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Pulsative signals such as the electrocardiogram (ECG) are extensively collected as part of routine clinical care. However, noisy and poor-quality recordings, leading to missing values, are a major issue for signals collected using mobile health systems, decreasing the signal quality and affecting the automated downstream tasks. Recent studies have explored imputation of missing values for ECG with probabilistic time-series models. Nevertheless, in comparison with the deterministic models, their performance is still limited, as the variations across subjects and heart-beat relationships are not explicitly considered in the training objective. In this work, to improve the ECG imputation and forecasting accuracy with probabilistic models, we present an template-guided denoising diffusion probabilistic model, PulseDiff, which is conditioned an informative prior for a range of health conditions. Specifically, 1) we first extract a subject-level pulsative template from the observation as an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;&#20316;&#20026;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;transformers&#22312;&#22788;&#29702;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#25512;&#26029;&#25104;&#26412;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15719</link><description>&lt;p&gt;
&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Linear Transformers. (arXiv:2310.15719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;&#20316;&#20026;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;transformers&#22312;&#22788;&#29702;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#25512;&#26029;&#25104;&#26412;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
transformer&#26550;&#26500;&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#33021;&#22815;&#25429;&#25417;&#38271;&#36317;&#31163;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20063;&#26159;&#20854;&#22312;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#26102;&#26377;&#25928;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#25104;&#21151;&#65292;transformers&#20173;&#28982;&#26377;&#20004;&#20010;&#37325;&#22823;&#32570;&#28857;&#65292;&#38480;&#21046;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65306;(1)&#20026;&#20102;&#35760;&#20303;&#36807;&#21435;&#30340;&#20449;&#24687;&#65292;&#33258;&#27880;&#24847;&#26426;&#21046;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#21382;&#21490;&#20449;&#24687;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;(2)transformers&#30340;&#25512;&#26029;&#25104;&#26412;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24490;&#29615;&#26367;&#20195;&#26041;&#26696;&#65292;&#20854;&#20855;&#26377;&#29420;&#31435;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#26029;&#25104;&#26412;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#19978;&#36848;&#35745;&#31639;&#38480;&#21046;&#20960;&#20046;&#20351;&#24471;transformers&#30340;&#24212;&#29992;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35786;&#26029;&#29615;&#22659;&#20013;&#37327;&#21270;&#20102;&#25105;&#20204;&#26550;&#26500;&#20013;&#19981;&#21516;&#37096;&#20998;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention mechanism in the transformer architecture is capable of capturing long-range dependencies and it is the main reason behind its effectiveness in processing sequential data. Nevertheless, despite their success, transformers have two significant drawbacks that still limit their broader applicability: (1) In order to remember past information, the self-attention mechanism requires access to the whole history to be provided as context. (2) The inference cost in transformers is expensive. In this paper we introduce recurrent alternatives to the transformer self-attention mechanism that offer a context-independent inference cost, leverage long-range dependencies effectively, and perform well in practice. We evaluate our approaches in reinforcement learning problems where the aforementioned computational limitations make the application of transformers nearly infeasible. We quantify the impact of the different components of our architecture in a diagnostic environment and as
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#21464;&#37327;&#20998;&#32452;&#30340;&#26032;&#22411;&#24369;&#32422;&#26463;&#30340;&#21487;&#36776;&#35782;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#26102;&#38388;&#32467;&#26500;&#12289;&#24178;&#39044;&#25110;&#30417;&#30563;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.15709</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#27979;&#21464;&#37327;&#30340;&#20998;&#32452;&#20351;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21487;&#36776;&#35782;&#21270;
&lt;/p&gt;
&lt;p&gt;
Causal Representation Learning Made Identifiable by Grouping of Observational Variables. (arXiv:2310.15709v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#21464;&#37327;&#20998;&#32452;&#30340;&#26032;&#22411;&#24369;&#32422;&#26463;&#30340;&#21487;&#36776;&#35782;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#26102;&#38388;&#32467;&#26500;&#12289;&#24178;&#39044;&#25110;&#30417;&#30563;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#24456;&#26377;&#24847;&#20041;&#30340;&#35805;&#39064;&#26159;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;Causal Representation Learning&#65292;&#31616;&#31216;CRL&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23398;&#20064;&#38544;&#34255;&#29305;&#24449;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;CRL&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36866;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#32467;&#21512;&#20102;&#34920;&#31034;&#23398;&#20064;&#21644;&#22240;&#26524;&#21457;&#29616;&#36825;&#20004;&#20010;&#23481;&#26131;&#19981;&#36866;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35201;&#25214;&#21040;&#33021;&#20445;&#35777;&#21807;&#19968;&#35299;&#30340;&#23454;&#38469;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#23545;&#20110;&#20854;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#22522;&#20110;&#23545;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#30340;&#20551;&#35774;&#65292;&#27604;&#22914;&#26102;&#38388;&#22240;&#26524;&#24615;&#12289;&#30417;&#30563;&#25110;&#24178;&#39044;&#30340;&#23384;&#22312;&#65307;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#36807;&#20110;&#38480;&#21046;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#35266;&#27979;&#28151;&#21512;&#34920;&#29616;&#20986;&#21512;&#36866;&#30340;&#21464;&#37327;&#20998;&#32452;&#30340;&#26032;&#22411;&#24369;&#32422;&#26463;&#65292;&#23637;&#31034;&#20102;&#21487;&#36776;&#35782;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#19968;&#33268;&#30340;&#26032;&#22411;&#33258;&#25105;&#30417;&#30563;&#20272;&#35745;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
A topic of great current interest is Causal Representation Learning (CRL), whose goal is to learn a causal model for hidden features in a data-driven manner. Unfortunately, CRL is severely ill-posed since it is a combination of the two notoriously ill-posed problems of representation learning and causal discovery. Yet, finding practical identifiability conditions that guarantee a unique solution is crucial for its practical applicability. Most approaches so far have been based on assumptions on the latent causal mechanisms, such as temporal causality, or existence of supervision or interventions; these can be too restrictive in actual applications. Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision. The approach is based assuming the observational mixing exhibits a suitable grouping of the observational variables. We also propose a novel self-supervised estimation framework consistent with the model, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#35843;&#24230;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.15706</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#21270;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning. (arXiv:2310.15706v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#35843;&#24230;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FJSSP&#65289;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#21551;&#21457;&#24335;&#12289;&#31934;&#30830;&#21644;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#23545;&#23454;&#26102;&#21709;&#24212;&#31361;&#21457;&#20107;&#20214;&#30340;&#38656;&#27714;&#20135;&#29983;&#20102;&#22312;&#20960;&#31186;&#20869;&#29983;&#25104;&#26032;&#35843;&#24230;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#35843;&#24230;&#35268;&#21017;&#65288;DRs&#65289;&#33021;&#22815;&#22312;&#32422;&#26463;&#19979;&#29983;&#25104;&#35843;&#24230;&#65292;&#23613;&#31649;&#20854;&#36136;&#37327;&#21487;&#20197;&#24471;&#21040;&#25913;&#36827;&#12290;&#20026;&#20102;&#25913;&#21892;&#32467;&#26524;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;FJSSP&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#19968;&#20010;&#31574;&#30053;&#65292;&#23558;&#25805;&#20316;&#20998;&#37197;&#21040;&#26426;&#22120;&#19978;&#29983;&#25104;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22411;&#30340;FJSSP&#23454;&#20363;&#20013;&#20173;&#28982;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#31283;&#20581;&#35299;&#20915;&#22823;&#22411;FJSSP&#23454;&#20363;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Flexible Job Shop Scheduling Problem (FJSSP) has been extensively studied in the literature, and multiple approaches have been proposed within the heuristic, exact, and metaheuristic methods. However, the industry's demand to be able to respond in real-time to disruptive events has generated the necessity to be able to generate new schedules within a few seconds. Among these methods, under this constraint, only dispatching rules (DRs) are capable of generating schedules, even though their quality can be improved. To improve the results, recent methods have been proposed for modeling the FJSSP as a Markov Decision Process (MDP) and employing reinforcement learning to create a policy that generates an optimal solution assigning operations to machines. Nonetheless, there is still room for improvement, particularly in the larger FJSSP instances which are common in real-world scenarios. Therefore, the objective of this paper is to propose a method capable of robustly solving large insta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.15694</link><description>&lt;p&gt;
COPF: &#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#25216;&#26415;&#26159;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;RLHF&#30340;LM&#22312;&#24341;&#20837;&#26032;&#30340;&#26597;&#35810;&#25110;&#21453;&#39304;&#26102;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20154;&#31867;&#20559;&#22909;&#22312;&#19981;&#21516;&#39046;&#22495;&#25110;&#20219;&#21153;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#30001;&#20110;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#37325;&#26032;&#35757;&#32451;LM&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#23454;&#38469;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#65288;COPF&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#27861;&#20272;&#35745;&#19968;&#31995;&#21015;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#20989;&#25968;&#27491;&#21017;&#21270;&#19981;&#26029;&#25311;&#21512;&#31574;&#30053;&#24207;&#21015;&#12290;COPF&#21253;&#21547;&#19968;&#20010;&#21333;&#19968;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#27531;&#24046;&#20803;&#32032;&#20013;&#28155;&#21152;&#24130;&#27425;&#39033;&#25552;&#21319;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#24212;&#29992;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38750;&#24179;&#28369;&#20989;&#25968;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#35813;&#32593;&#32476;&#32467;&#26500;&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15690</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#36827;&#34892;&#25554;&#20540;&#21644;&#21453;&#38382;&#39064;&#30340;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems. (arXiv:2310.15690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#27531;&#24046;&#20803;&#32032;&#20013;&#28155;&#21152;&#24130;&#27425;&#39033;&#25552;&#21319;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#24212;&#29992;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38750;&#24179;&#28369;&#20989;&#25968;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#35813;&#32593;&#32476;&#32467;&#26500;&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#31216;&#20026;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#21892;2D&#21644;3D&#29615;&#22659;&#19979;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20989;&#25968;&#30340;&#25554;&#20540;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#27531;&#24046;&#20803;&#32032;&#20013;&#28155;&#21152;&#24130;&#27425;&#39033;&#65292;&#35813;&#32593;&#32476;&#32467;&#26500;&#22686;&#24378;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;&#32593;&#32476;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#32593;&#32476;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#22312;&#38750;&#24179;&#28369;&#20989;&#25968;&#26041;&#38754;&#20855;&#26377;&#24322;&#24120;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#38469;&#31034;&#20363;&#20063;&#35777;&#23454;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#12289;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30456;&#23545;&#20110;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#26356;&#28145;&#23618;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;&#32593;&#32476;&#32467;&#26500;&#36824;&#24212;&#29992;&#20110;&#35299;&#20915;&#21453;Burgers&#26041;&#31243;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#24635;&#20043;&#65292;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26126;&#26174;&#25552;&#21319;&#20102;&#25554;&#20540;&#21644;&#21453;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel neural network structure called the Power-Enhancing residual network, designed to improve interpolation capabilities for both smooth and non-smooth functions in 2D and 3D settings. By adding power terms to residual elements, the architecture boosts the network's expressive power. The study explores network depth, width, and optimization methods, showing the architecture's adaptability and performance advantages. Consistently, the results emphasize the exceptional accuracy of the proposed Power-Enhancing residual network, particularly for non-smooth functions. Real-world examples also confirm its superiority over plain neural network in terms of accuracy, convergence, and efficiency. The study also looks at the impact of deeper network. Moreover, the proposed architecture is also applied to solving the inverse Burgers' equation, demonstrating superior performance. In conclusion, the Power-Enhancing residual network offers a versatile solution that significa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#39044;&#31639;&#19979;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#23454;&#25968;&#32452;&#21512;&#32431;&#25506;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CSA&#31639;&#27861;&#65292;&#21487;&#22312;&#33218;&#25968;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20339;&#21160;&#20316;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Minimax-CombSAR&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#21160;&#20316;&#31867;&#22823;&#23567;&#20026;&#22810;&#39033;&#24335;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30340;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#23454;&#39564;&#20013;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.15681</link><description>&lt;p&gt;
&#22266;&#23450;&#39044;&#31639;&#19979;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#23454;&#25968;&#32452;&#21512;&#32431;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit. (arXiv:2310.15681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#39044;&#31639;&#19979;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#23454;&#25968;&#32452;&#21512;&#32431;&#25506;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CSA&#31639;&#27861;&#65292;&#21487;&#22312;&#33218;&#25968;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20339;&#21160;&#20316;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Minimax-CombSAR&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#21160;&#20316;&#31867;&#22823;&#23567;&#20026;&#22810;&#39033;&#24335;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30340;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#23454;&#39564;&#20013;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22266;&#23450;&#39044;&#31639;&#35774;&#32622;&#19979;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#23454;&#25968;&#32452;&#21512;&#32431;&#25506;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#32452;&#21512;&#25104;&#21151;&#20998;&#37197;&#65288;CSA&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#21160;&#20316;&#31867;&#22823;&#23567;&#30456;&#23545;&#20110;&#33218;&#25968;&#21576;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26368;&#20339;&#21160;&#20316;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CSA&#31639;&#27861;&#30340;&#35823;&#24046;&#27010;&#29575;&#19978;&#30028;&#19982;&#19979;&#30028;&#22312;&#25351;&#25968;&#30340;&#23545;&#25968;&#22240;&#23376;&#20869;&#30456;&#21305;&#37197;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21478;&#19968;&#20010;&#31639;&#27861;&#65292;&#21517;&#20026;&#26497;&#23567;&#21270;&#32452;&#21512;&#36830;&#32493;&#25509;&#21463;&#19982;&#25298;&#32477;&#65288;Minimax-CombSAR&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21160;&#20316;&#31867;&#22823;&#23567;&#20026;&#22810;&#39033;&#24335;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#26368;&#20248;&#30340;&#65292;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#36825;&#20123;&#31639;&#27861;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the real-valued combinatorial pure exploration of the multi-armed bandit in the fixed-budget setting. We first introduce the Combinatorial Successive Asign (CSA) algorithm, which is the first algorithm that can identify the best action even when the size of the action class is exponentially large with respect to the number of arms. We show that the upper bound of the probability of error of the CSA algorithm matches a lower bound up to a logarithmic factor in the exponent. Then, we introduce another algorithm named the Minimax Combinatorial Successive Accepts and Rejects (Minimax-CombSAR) algorithm for the case where the size of the action class is polynomial, and show that it is optimal, which matches a lower bound. Finally, we experimentally compare the algorithms with previous methods and show that our algorithm performs better.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65292;&#26082;&#21487;&#35299;&#37322;&#24615;&#21448;&#33021;&#34701;&#20837;&#30005;&#21147;&#34892;&#19994;&#30340;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#21644;&#39640;&#25928;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#21644;&#30005;&#21147;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15662</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#21450;&#20854;&#22312;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting. (arXiv:2310.15662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65292;&#26082;&#21487;&#35299;&#37322;&#24615;&#21448;&#33021;&#34701;&#20837;&#30005;&#21147;&#34892;&#19994;&#30340;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#21644;&#39640;&#25928;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#21644;&#30005;&#21147;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#26159;&#30005;&#21147;&#31995;&#32479;&#35268;&#21010;&#21644;&#31649;&#29702;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#19981;&#20934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#20572;&#30005;&#23041;&#32961;&#25110;&#33021;&#28304;&#28010;&#36153;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#29978;&#33267;&#27809;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#33410;&#20551;&#26085;&#36127;&#33655;&#39044;&#27979;&#25110;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#36127;&#33655;&#39044;&#27979;&#65292;&#20934;&#30830;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#36127;&#33655;&#39044;&#27979;&#20043;&#21518;&#24448;&#24448;&#20250;&#20570;&#20986;&#37325;&#22823;&#20915;&#31574;,&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65292;&#26082;&#21487;&#35299;&#37322;&#24615;&#21448;&#33021;&#34701;&#20837;&#30005;&#21147;&#34892;&#19994;&#30340;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#31181;&#22522;&#20110;&#25552;&#21319;&#30340;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#21033;&#29992;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#20844;&#20849;&#22522;&#20934;&#21644;&#30005;&#21147;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#20132;&#20114;&#24335;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electric load forecasting is an indispensable component of electric power system planning and management. Inaccurate load forecasting may lead to the threat of outages or a waste of energy. Accurate electric load forecasting is challenging when there is limited data or even no data, such as load forecasting in holiday, or under extreme weather conditions. As high-stakes decision-making usually follows after load forecasting, model interpretability is crucial for the adoption of forecasting models. In this paper, we propose an interactive GAM which is not only interpretable but also can incorporate specific domain knowledge in electric power industry for improved performance. This boosting-based GAM leverages piecewise linear functions and can be learned through our efficient algorithm. In both public benchmark and electricity datasets, our interactive GAM outperforms current state-of-the-art methods and demonstrates good generalization ability in the cases of extreme weather events. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#21160;&#37327;&#26799;&#24230;&#30340;HGNNs&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#38750;&#30446;&#26631;&#25915;&#20987;&#12290;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#29305;&#24449;&#20462;&#25913;&#65292;&#35813;&#27169;&#22411;&#22312;HGNNs&#35757;&#32451;&#20043;&#21069;&#23454;&#26045;&#25915;&#20987;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23545;HGNNs&#23545;&#25239;&#25915;&#20987;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.15656</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#37327;&#26799;&#24230;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#38750;&#30446;&#26631;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks. (arXiv:2310.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#21160;&#37327;&#26799;&#24230;&#30340;HGNNs&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#38750;&#30446;&#26631;&#25915;&#20987;&#12290;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#29305;&#24449;&#20462;&#25913;&#65292;&#35813;&#27169;&#22411;&#22312;HGNNs&#35757;&#32451;&#20043;&#21069;&#23454;&#26045;&#25915;&#20987;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23545;HGNNs&#23545;&#25239;&#25915;&#20987;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#39640;&#38454;&#34920;&#31034;&#33021;&#21147;&#65292;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#36229;&#22270;&#30456;&#20851;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#22270;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#19978;&#65292;&#23545;HGNNs&#30340;&#23545;&#25239;&#25915;&#20987;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#35797;&#22270;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;HGNNs&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#38750;&#30446;&#26631;&#25915;&#20987;&#65292;&#21363;MGHGA&#65292;&#37325;&#28857;&#26159;&#20462;&#25913;&#33410;&#28857;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;HGNNs&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#26367;&#20195;&#27169;&#22411;&#22312;&#36229;&#22270;&#24314;&#27169;&#20043;&#21069;&#23454;&#26045;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MGHGA&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#29305;&#24449;&#36873;&#25321;&#21644;&#29305;&#24449;&#20462;&#25913;&#12290;&#25105;&#20204;&#22312;&#29305;&#24449;&#36873;&#25321;&#27169;&#22359;&#20013;&#20351;&#29992;&#21160;&#37327;&#26799;&#24230;&#26426;&#21046;&#36873;&#25321;&#25915;&#20987;&#33410;&#28857;&#29305;&#24449;&#12290;&#22312;&#29305;&#24449;&#20462;&#25913;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#29305;&#24449;&#29983;&#25104;&#26041;&#27861;&#65288;&#30452;&#25509;&#20462;&#25913;&#21644;&#31526;&#21495;&#26799;&#24230;&#65289;&#26469;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Neural Networks (HGNNs) have been successfully applied in various hypergraph-related tasks due to their excellent higher-order representation capabilities. Recent works have shown that deep learning models are vulnerable to adversarial attacks. Most studies on graph adversarial attacks have focused on Graph Neural Networks (GNNs), and the study of adversarial attacks on HGNNs remains largely unexplored. In this paper, we try to reduce this gap. We design a new HGNNs attack model for the untargeted attack, namely MGHGA, which focuses on modifying node features. We consider the process of HGNNs training and use a surrogate model to implement the attack before hypergraph modeling. Specifically, MGHGA consists of two parts: feature selection and feature modification. We use a momentum gradient mechanism to choose the attack node features in the feature selection module. In the feature modification module, we use two feature generation approaches (direct modification and sign gra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#30340;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#31574;&#30053;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20513;&#37319;&#29992;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#24378;&#35843;&#20351;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#19981;&#21516;&#25915;&#20987;&#12290;&#36825;&#26159;&#39318;&#20010;&#32508;&#21512;&#35843;&#26597;LLMs&#26102;&#20195;&#26816;&#27979;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.15654</link><description>&lt;p&gt;
LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Detection of LLMs-Generated Content. (arXiv:2310.15654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15654
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#30340;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#31574;&#30053;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20513;&#37319;&#29992;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#24378;&#35843;&#20351;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#19981;&#21516;&#25915;&#20987;&#12290;&#36825;&#26159;&#39318;&#20010;&#32508;&#21512;&#35843;&#26597;LLMs&#26102;&#20195;&#26816;&#27979;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23548;&#33268;&#21512;&#25104;&#20869;&#23481;&#29983;&#25104;&#19981;&#26029;&#22686;&#21152;&#65292;&#28041;&#21450;&#23186;&#20307;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#20844;&#20849;&#35805;&#35821;&#21644;&#25945;&#32946;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;LLMs&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#29616;&#26377;&#26816;&#27979;&#31574;&#30053;&#21644;&#22522;&#20934;&#30340;&#35814;&#32454;&#27010;&#36848;&#65292;&#23457;&#26597;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#24182;&#30830;&#23450;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#21069;&#26223;&#65292;&#25552;&#20513;&#37319;&#29992;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#20027;&#24352;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#24212;&#23545;&#19981;&#21516;&#25915;&#20987;&#65292;&#20197;&#25269;&#24481;LLMs&#19981;&#26029;&#21457;&#23637;&#30340;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;LLMs&#26102;&#20195;&#26816;&#27979;&#30340;&#39318;&#20010;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#24076;&#26395;&#23427;&#33021;&#22815;&#25552;&#20379;&#23545;LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#24403;&#21069;&#24773;&#20917;&#30340;&#24191;&#27867;&#29702;&#35299;&#65292;&#24182;&#20026;&#30740;&#31350;&#35813;&#39046;&#22495;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22270;&#36827;&#34892;&#27450;&#39575;&#24615;&#20844;&#24179;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;FATE&#65292;&#22312;&#20445;&#25345;&#19979;&#28216;&#20219;&#21153;&#25928;&#29992;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25918;&#22823;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#26080;&#35770;&#26159;&#21542;&#32771;&#34385;&#20844;&#24179;&#24615;&#12290;&#35813;&#30740;&#31350;&#21487;&#20197;&#20026;&#35774;&#35745;&#40065;&#26834;&#21644;&#20844;&#24179;&#30340;&#22270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.15653</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#22270;&#36827;&#34892;&#27450;&#39575;&#24615;&#20844;&#24179;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Deceptive Fairness Attacks on Graphs via Meta Learning. (arXiv:2310.15653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22270;&#36827;&#34892;&#27450;&#39575;&#24615;&#20844;&#24179;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;FATE&#65292;&#22312;&#20445;&#25345;&#19979;&#28216;&#20219;&#21153;&#25928;&#29992;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25918;&#22823;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#26080;&#35770;&#26159;&#21542;&#32771;&#34385;&#20844;&#24179;&#24615;&#12290;&#35813;&#30740;&#31350;&#21487;&#20197;&#20026;&#35774;&#35745;&#40065;&#26834;&#21644;&#20844;&#24179;&#30340;&#22270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#22270;&#36827;&#34892;&#27450;&#39575;&#24615;&#20844;&#24179;&#25915;&#20987;&#65292;&#20197;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#22914;&#20309;&#36890;&#36807;&#27745;&#26579;&#25915;&#20987;&#22270;&#23398;&#20064;&#27169;&#22411;&#26469;&#27450;&#39575;&#24615;&#22320;&#21152;&#21095;&#20559;&#35265;&#65311;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;FATE&#12290;FATE&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20844;&#24179;&#23450;&#20041;&#21644;&#22270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#20219;&#24847;&#36873;&#25321;&#30340;&#25805;&#20316;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;FATE&#23454;&#20363;&#21270;&#20026;&#25915;&#20987;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#32479;&#35745;&#24179;&#34913;&#21644;&#20010;&#20307;&#20844;&#24179;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FATE&#33021;&#22815;&#22312;&#32500;&#25345;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#29992;&#30340;&#21516;&#26102;&#25918;&#22823;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#26080;&#35770;&#26159;&#21542;&#32771;&#34385;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#23545;&#20844;&#24179;&#22270;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#25552;&#20379;&#19968;&#20123;&#35265;&#35299;&#65292;&#24182;&#33021;&#20026;&#35774;&#35745;&#40065;&#26834;&#21644;&#20844;&#24179;&#30340;&#22270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity and individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24050;&#26377;&#30340;Transformer&#21040;CNN&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;CNN&#22359;&#65292;&#25552;&#21319;&#20102;&#39640;&#25928;CNN&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#39640;&#25928;CNN&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15648</link><description>&lt;p&gt;
&#21160;&#24577;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio Models. (arXiv:2310.15648v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24050;&#26377;&#30340;Transformer&#21040;CNN&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;CNN&#22359;&#65292;&#25552;&#21319;&#20102;&#39640;&#25928;CNN&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#39640;&#25928;CNN&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;&#22914;AudioSet&#65289;&#30340;&#24341;&#20837;&#20026;Transformers&#22312;&#38899;&#39057;&#39046;&#22495;&#30340;&#20351;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#21462;&#20195;&#20102;CNN&#20316;&#20026;&#35768;&#22810;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#38899;&#39057;&#39057;&#35889;Transformers&#22312;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#33391;&#30340;&#24615;&#33021;&#65292;&#21019;&#36896;&#20986;&#20102;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#20013;&#36229;&#36807;&#20102;CNN&#12290;&#28982;&#32780;&#65292;&#19982;CNN&#30456;&#27604;&#65292;&#30446;&#21069;&#27969;&#34892;&#30340;&#38899;&#39057;&#39057;&#35889;Transformers&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;Transformer&#21040;CNN&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#23637;&#31034;&#20102;&#39640;&#25928;CNN&#21487;&#20197;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36861;&#36214;&#29978;&#33267;&#36229;&#36807;Transformer&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24310;&#20280;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#30001;&#21160;&#24577;&#38750;&#32447;&#24615;&#12289;&#21160;&#24577;&#21367;&#31215;&#21644;&#27880;&#24847;&#26426;&#21046;&#26500;&#25104;&#30340;&#21160;&#24577;CNN&#22359;&#26469;&#25552;&#21319;&#39640;&#25928;CNN&#30340;&#23481;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#24577;CNN&#22312;&#24615;&#33021;&#21644;&#22797;&#26434;&#24230;&#30340;&#26435;&#34913;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#39640;&#25928;CNN&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of large-scale audio datasets, such as AudioSet, paved the way for Transformers to conquer the audio domain and replace CNNs as the state-of-the-art neural network architecture for many tasks. Audio Spectrogram Transformers are excellent at exploiting large datasets, creating powerful pre-trained models that surpass CNNs when fine-tuned on downstream tasks. However, current popular Audio Spectrogram Transformers are demanding in terms of computational complexity compared to CNNs. Recently, we have shown that, by employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch up with and even outperform Transformers on large datasets. In this work, we extend this line of research and increase the capacity of efficient CNNs by introducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic convolutions and attention mechanisms. We show that these dynamic CNNs outperform traditional efficient CNNs, in terms of the performance-complexity trade
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29305;&#23450;&#28151;&#28102;&#25216;&#26415;&#23545;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#28151;&#28102;&#25216;&#26415;&#23545;&#25152;&#26377;&#38745;&#24577;&#20998;&#26512;&#29305;&#24449;&#37117;&#20135;&#29983;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#30772;&#22351;&#20381;&#36182;&#36825;&#20123;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15645</link><description>&lt;p&gt;
&#28857;&#20142;&#37027;&#20010;&#26426;&#22120;&#20154;&#65281;&#20851;&#20110;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#29305;&#24449;&#23545;&#25239;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Light up that Droid! On the Effectiveness of Static Analysis Features against App Obfuscation for Android Malware Detection. (arXiv:2310.15645v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29305;&#23450;&#28151;&#28102;&#25216;&#26415;&#23545;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#28151;&#28102;&#25216;&#26415;&#23545;&#25152;&#26377;&#38745;&#24577;&#20998;&#26512;&#29305;&#24449;&#37117;&#20135;&#29983;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#30772;&#22351;&#20381;&#36182;&#36825;&#20123;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#20316;&#32773;&#23558;&#28151;&#28102;&#35270;&#20026;&#32469;&#36807;&#22522;&#20110;&#38745;&#24577;&#20998;&#26512;&#29305;&#24449;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#25163;&#27573;&#12290;&#23545;&#20110;&#23433;&#21331;&#31995;&#32479;&#65292;&#22810;&#39033;&#30740;&#31350;&#24050;&#30830;&#35748;&#35768;&#22810;&#21453;&#24694;&#24847;&#36719;&#20214;&#20135;&#21697;&#24456;&#23481;&#26131;&#36890;&#36807;&#31616;&#21333;&#30340;&#31243;&#24207;&#36716;&#25442;&#26469;&#35268;&#36991;&#12290;&#19982;&#36825;&#20123;&#30740;&#31350;&#30456;&#21453;&#65292;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#26041;&#27861;&#20063;&#34987;&#25552;&#20986;&#26469;&#23545;&#25239;&#28151;&#28102;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#30830;&#23450;&#29305;&#23450;&#28151;&#28102;&#31574;&#30053;&#25110;&#24037;&#20855;&#30340;&#20351;&#29992;&#23545;&#20110;&#22522;&#20110;&#38745;&#24577;&#20998;&#26512;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#26500;&#25104;&#20309;&#31181;&#39118;&#38505;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#19968;&#38382;&#39064;&#65292;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29305;&#23450;&#28151;&#28102;&#25216;&#26415;&#23545;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#25552;&#21462;&#30340;&#24120;&#35265;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#36825;&#20123;&#21464;&#21270;&#26159;&#21542;&#36275;&#22815;&#26174;&#33879;&#20197;&#30772;&#22351;&#20381;&#36182;&#36825;&#20123;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#28102;&#25216;&#26415;&#23545;&#25152;&#26377;&#38745;&#24577;&#20998;&#26512;&#29305;&#24449;&#37117;&#20135;&#29983;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware authors have seen obfuscation as the mean to bypass malware detectors based on static analysis features. For Android, several studies have confirmed that many anti-malware products are easily evaded with simple program transformations. As opposed to these works, ML detection proposals for Android leveraging static analysis features have also been proposed as obfuscation-resilient. Therefore, it needs to be determined to what extent the use of a specific obfuscation strategy or tool poses a risk for the validity of ML malware detectors for Android based on static analysis features. To shed some light in this regard, in this article we assess the impact of specific obfuscation techniques on common features extracted using static analysis and determine whether the changes are significant enough to undermine the effectiveness of ML malware detectors that rely on these features. The experimental results suggest that obfuscation techniques affect all static analysis features to varyi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;Conformal Prediction&#30340;Gaussian Process Regression&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27169;&#22411;&#23436;&#20840;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#35206;&#30422;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.15641</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#20445;&#35777;&#35206;&#30422;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Coverage Prediction Intervals with Gaussian Process Regression. (arXiv:2310.15641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;Conformal Prediction&#30340;Gaussian Process Regression&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27169;&#22411;&#23436;&#20840;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#35206;&#30422;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#19982;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19981;&#21516;&#65292;&#23427;&#25552;&#20379;&#20102;&#20854;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#22522;&#20110;&#27169;&#22411;&#20551;&#35774;&#27491;&#30830;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#30340;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25152;&#38656;&#30340;&#30693;&#35782;&#24456;&#23569;&#21487;&#29992;&#65292;&#36825;&#23548;&#33268;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#33021;&#38750;&#24120;&#35823;&#23548;&#20154;&#65292;&#20363;&#22914;&#23545;&#20110;95&#65285;&#32622;&#20449;&#27700;&#24179;&#20135;&#29983;&#30340;&#39044;&#27979;&#21306;&#38388;&#21487;&#33021;&#21482;&#35206;&#30422;&#20102;&#23569;&#20110;95&#65285;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;Conformal Prediction&#65288;CP&#65289;&#30340;GPR&#25193;&#23637;&#12290;&#36825;&#31181;&#25193;&#23637;&#21487;&#20197;&#22312;&#27169;&#22411;&#23436;&#20840;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#35206;&#30422;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;GPR&#30340;&#20248;&#21183;&#21644;CP&#30340;&#26377;&#25928;&#35206;&#30422;&#20445;&#35777;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Regression (GPR) is a popular regression method, which unlike most Machine Learning techniques, provides estimates of uncertainty for its predictions. These uncertainty estimates however, are based on the assumption that the model is well-specified, an assumption that is violated in most practical applications, since the required knowledge is rarely available. As a result, the produced uncertainty estimates can become very misleading; for example the prediction intervals (PIs) produced for the 95\% confidence level may cover much less than 95\% of the true labels. To address this issue, this paper introduces an extension of GPR based on a Machine Learning framework called, Conformal Prediction (CP). This extension guarantees the production of PIs with the required coverage even when the model is completely misspecified. The proposed approach combines the advantages of GPR with the valid coverage guarantee of CP, while the performed experimental results demonstrate its 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23450;&#21521;&#26080;&#29615;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23558;&#19978;&#19979;&#25991;&#29305;&#24449;&#26144;&#23556;&#21040;DAG&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#22270;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#25237;&#24433;&#23618;&#28385;&#36275;&#26080;&#29615;&#24615;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#22270;&#12290;</title><link>http://arxiv.org/abs/2310.15627</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#23450;&#21521;&#26080;&#29615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Contextual directed acyclic graphs. (arXiv:2310.15627v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23450;&#21521;&#26080;&#29615;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23558;&#19978;&#19979;&#25991;&#29305;&#24449;&#26144;&#23556;&#21040;DAG&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#22270;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#25237;&#24433;&#23618;&#28385;&#36275;&#26080;&#29615;&#24615;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#23450;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#32467;&#26500;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#20026;&#25972;&#20010;&#20154;&#21475;&#23398;&#20064;&#21333;&#20010;DAG&#19978;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#22270;&#32467;&#26500;&#22522;&#20110;&#21487;&#29992;&#30340;&#8220;&#19978;&#19979;&#25991;&#8221;&#29305;&#24449;&#32780;&#22240;&#20154;&#32780;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#23558;&#19978;&#19979;&#25991;&#29305;&#24449;&#26144;&#23556;&#21040;DAG&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#19978;&#19979;&#25991;DAG&#38382;&#39064;&#65292;DAG&#20197;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#12290;&#31070;&#32463;&#32593;&#32476;&#37197;&#22791;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25237;&#24433;&#23618;&#65292;&#30830;&#20445;&#36755;&#20986;&#30697;&#38453;&#26159;&#31232;&#30095;&#30340;&#65292;&#24182;&#28385;&#36275;&#26368;&#36817;&#21457;&#23637;&#30340;&#26080;&#29615;&#24615;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35745;&#31639;&#26694;&#26550;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;DAG&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#21644;&#36890;&#36807;&#25237;&#24433;&#23618;&#21453;&#21521;&#20256;&#25773;&#30340;&#20998;&#26512;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#22270;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#21017;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the structure of directed acyclic graphs (DAGs) from observational data remains a significant challenge in machine learning. Most research in this area concentrates on learning a single DAG for the entire population. This paper considers an alternative setting where the graph structure varies across individuals based on available "contextual" features. We tackle this contextual DAG problem via a neural network that maps the contextual features to a DAG, represented as a weighted adjacency matrix. The neural network is equipped with a novel projection layer that ensures the output matrices are sparse and satisfy a recently developed characterization of acyclicity. We devise a scalable computational framework for learning contextual DAGs and provide a convergence guarantee and an analytical gradient for backpropagating through the projection layer. Our experiments suggest that the new approach can recover the true context-specific graph where existing approaches fail.
&lt;/p&gt;</description></item><item><title>GUPNet++&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#27010;&#29575;&#26041;&#24335;&#24314;&#27169;&#20960;&#20309;&#25237;&#24433;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#28145;&#24230;&#39044;&#27979;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.15624</link><description>&lt;p&gt;
GUPNet++&#65306;&#29992;&#20110;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection. (arXiv:2310.15624v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15624
&lt;/p&gt;
&lt;p&gt;
GUPNet++&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#27010;&#29575;&#26041;&#24335;&#24314;&#27169;&#20960;&#20309;&#25237;&#24433;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#28145;&#24230;&#39044;&#27979;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#22312;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#29289;&#20307;&#30340;&#29289;&#29702;&#23610;&#23544;&#19982;&#22270;&#20687;&#24179;&#38754;&#20013;&#30340;&#20108;&#32500;&#25237;&#24433;&#20043;&#38388;&#30340;&#36879;&#35270;&#25237;&#24433;&#26469;&#20272;&#35745;&#29289;&#20307;&#30340;&#28145;&#24230;&#65292;&#36825;&#21487;&#20197;&#23558;&#25968;&#23398;&#20808;&#39564;&#24341;&#20837;&#28145;&#24230;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25237;&#24433;&#36807;&#31243;&#20063;&#20250;&#24341;&#20837;&#35823;&#24046;&#25918;&#22823;&#65292;&#20272;&#35745;&#39640;&#24230;&#30340;&#35823;&#24046;&#20250;&#34987;&#25918;&#22823;&#24182;&#21453;&#26144;&#21040;&#25237;&#24433;&#30340;&#28145;&#24230;&#20013;&#12290;&#36825;&#23548;&#33268;&#28145;&#24230;&#25512;&#26029;&#19981;&#21487;&#38752;&#65292;&#24182;&#19988;&#24433;&#21709;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;(GUPNet++)&#65292;&#36890;&#36807;&#20197;&#27010;&#29575;&#26041;&#24335;&#24314;&#27169;&#20960;&#20309;&#25237;&#24433;&#12290;&#36825;&#30830;&#20445;&#20102;&#28145;&#24230;&#39044;&#27979;&#26159;&#26377;&#30028;&#30340;&#65292;&#24182;&#19982;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#32852;&#12290;&#24341;&#20837;&#36825;&#31181;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#30340;&#24847;&#20041;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(1)&#12290;&#23427;&#27169;&#25311;&#20102;&#20960;&#20309;&#25237;&#24433;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#27169;&#22411;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometry plays a significant role in monocular 3D object detection. It can be used to estimate object depth by using the perspective projection between object's physical size and 2D projection in the image plane, which can introduce mathematical priors into deep models. However, this projection process also introduces error amplification, where the error of the estimated height is amplified and reflected into the projected depth. It leads to unreliable depth inferences and also impairs training stability. To tackle this problem, we propose a novel Geometry Uncertainty Propagation Network (GUPNet++) by modeling geometry projection in a probabilistic manner. This ensures depth predictions are well-bounded and associated with a reasonable uncertainty. The significance of introducing such geometric uncertainty is two-fold: (1). It models the uncertainty propagation relationship of the geometry projection during training, improving the stability and efficiency of the end-to-end model learni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15612</link><description>&lt;p&gt;
Nko&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#65306;&#24037;&#20855;&#12289;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23612;&#31185;&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#27809;&#26377;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20294;&#23427;&#22312;&#25991;&#21270;&#21644;&#25945;&#32946;&#20215;&#20540;&#19978;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#26088;&#22312;&#24320;&#21457;&#21487;&#29992;&#30340;&#23612;&#31185;&#35821;&#21644;&#20854;&#20182;&#24403;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#24179;&#34892;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20855;&#20307;&#21253;&#25324;&#65306;(1) Friallel&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#65292;&#36890;&#36807;&#22522;&#20110;&#21103;&#26412;&#32534;&#36753;&#30340;&#24037;&#20316;&#27969;&#31243;&#23454;&#29616;&#36136;&#37327;&#25511;&#21046;&#12290;(2) &#25193;&#23637;&#20102;FLoRes-200&#21644;NLLB-Seed&#35821;&#26009;&#24211;&#65292;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#19982;&#23612;&#31185;&#35821;&#24179;&#34892;&#32763;&#35793;&#20102;2,009&#21644;6,193&#20010;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;(3) nicolingua-0005&#65306;&#21253;&#21547;130,850&#20010;&#24179;&#34892;&#29255;&#27573;&#30340;&#19977;&#35821;&#21644;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#36229;&#36807;3&#30334;&#19975;&#23612;&#31185;&#35821;&#21333;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;(4) &#22522;&#32447;&#21452;&#35821;&#21644;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#19982;b...
&lt;/p&gt;
&lt;p&gt;
Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
&lt;/p&gt;</description></item><item><title>Slisemap&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#27969;&#24418;&#21487;&#35270;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22312;&#29289;&#29702;&#25968;&#25454;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.15610</link><description>&lt;p&gt;
&#20351;&#29992;Slisemap&#35299;&#37322;&#29289;&#29702;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Using Slisemap to interpret physical data. (arXiv:2310.15610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15610
&lt;/p&gt;
&lt;p&gt;
Slisemap&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#27969;&#24418;&#21487;&#35270;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22312;&#29289;&#29702;&#25968;&#25454;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#21487;&#35270;&#21270;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#22312;&#29289;&#29702;&#31185;&#23398;&#20013;&#21487;&#35270;&#21270;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#23558;&#19968;&#31181;&#26368;&#36817;&#20171;&#32461;&#30340;&#27969;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;Slise&#24212;&#29992;&#20110;&#29289;&#29702;&#21644;&#21270;&#23398;&#25968;&#25454;&#38598;&#12290;Slisemap&#23558;&#27969;&#24418;&#21487;&#35270;&#21270;&#19982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#30740;&#31350;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22797;&#26434;&#27169;&#25311;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#36807;Slisemap&#65292;&#25105;&#20204;&#25214;&#21040;&#19968;&#31181;&#23884;&#20837;&#65292;&#20351;&#24471;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#35299;&#37322;&#30340;&#25968;&#25454;&#39033;&#34987;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;Slisemap&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#40657;&#30418;&#27169;&#22411;&#19981;&#21516;&#34892;&#20026;&#30340;&#27010;&#35272;&#12290;&#36825;&#20351;&#24471;Slisemap&#25104;&#20026;&#19968;&#31181;&#26377;&#30417;&#30563;&#30340;&#27969;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#23884;&#20837;&#30340;&#27169;&#24335;&#21453;&#26144;&#20102;&#30446;&#26631;&#23646;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#29289;&#29702;&#25968;&#25454;&#19978;&#20351;&#29992;&#21644;&#35780;&#20272;Slisemap&#65292;&#24182;&#35777;&#26126;Slisemap&#22312;&#25214;&#21040;&#20998;&#31867;&#21644;&#22238;&#24402;&#27169;&#22411;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#26041;&#38754;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifold visualisation techniques are commonly used to visualise high-dimensional datasets in physical sciences. In this paper we apply a recently introduced manifold visualisation method, called Slise, on datasets from physics and chemistry. Slisemap combines manifold visualisation with explainable artificial intelligence. Explainable artificial intelligence is used to investigate the decision processes of black box machine learning models and complex simulators. With Slisemap we find an embedding such that data items with similar local explanations are grouped together. Hence, Slisemap gives us an overview of the different behaviours of a black box model. This makes Slisemap into a supervised manifold visualisation method, where the patterns in the embedding reflect a target property. In this paper we show how Slisemap can be used and evaluated on physical data and that Slisemap is helpful in finding meaningful information on classification and regression models trained on these data
&lt;/p&gt;</description></item><item><title>tagE&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25552;&#21462;&#20219;&#21153;&#30340;&#20855;&#36523;&#20195;&#29702;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#26102;&#30340;&#27495;&#20041;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15605</link><description>&lt;p&gt;
tagE: &#35753;&#20855;&#36523;&#20195;&#29702;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
tagE: Enabling an Embodied Agent to Understand Human Instructions. (arXiv:2310.15605v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15605
&lt;/p&gt;
&lt;p&gt;
tagE&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25552;&#21462;&#20219;&#21153;&#30340;&#20855;&#36523;&#20195;&#29702;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#26102;&#30340;&#27495;&#20041;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#20855;&#26377;&#29289;&#29702;&#23384;&#22312;&#30340;&#26234;&#33021;&#20195;&#29702;&#19982;&#20154;&#31867;&#20132;&#27969;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#30340;&#30740;&#31350;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24847;&#22270;&#39044;&#27979;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25688;&#35201;&#65292;&#20294;&#38754;&#21521;&#20855;&#36523;&#20195;&#29702;&#38656;&#35201;&#23454;&#38469;&#34892;&#21160;&#30340;&#24773;&#22659;&#30340;NLU&#30340;&#33539;&#22260;&#20173;&#28982;&#26377;&#38480;&#12290;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#27495;&#20041;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#32473;&#26234;&#33021;&#20195;&#29702;&#35299;&#35835;&#20154;&#31867;&#24847;&#22270;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#31216;&#20026;&#20855;&#36523;&#20195;&#29702;&#30340;&#20219;&#21153;&#21644;&#21442;&#25968;&#22522;&#30784;&#65288;tagE&#65289;&#12290;&#22312;&#26680;&#24515;&#37096;&#20998;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20197;&#20174;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22797;&#26434;&#20219;&#21153;&#25351;&#20196;&#20013;&#25552;&#21462;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23884;&#22871;&#35299;&#30721;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language serves as the primary mode of communication when an intelligent agent with a physical presence engages with human beings. While a plethora of research focuses on natural language understanding (NLU), encompassing endeavors such as sentiment analysis, intent prediction, question answering, and summarization, the scope of NLU directed at situations necessitating tangible actions by an embodied agent remains limited. The inherent ambiguity and incompleteness inherent in natural language present challenges for intelligent agents striving to decipher human intention. To tackle this predicament head-on, we introduce a novel system known as task and argument grounding for Embodied agents (tagE). At its core, our system employs an inventive neural network model designed to extract a series of tasks from complex task instructions expressed in natural language. Our proposed model adopts an encoder-decoder framework enriched with nested decoding to effectively extract tasks and t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#24320;&#25918;&#28023;&#22495;&#28023;&#19978;&#30417;&#35270;&#20013;&#30340;&#26377;&#24847;AIS&#20851;&#38381;&#12290;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#39044;&#27979;&#32467;&#26524;&#26469;&#25253;&#21578;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.15586</link><description>&lt;p&gt;
&#26816;&#27979;&#24320;&#25918;&#28023;&#22495;&#28023;&#19978;&#30417;&#35270;&#20013;&#30340;&#26377;&#24847;AIS&#20851;&#38381;&#20351;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning. (arXiv:2310.15586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#24320;&#25918;&#28023;&#22495;&#28023;&#19978;&#30417;&#35270;&#20013;&#30340;&#26377;&#24847;AIS&#20851;&#38381;&#12290;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#39044;&#27979;&#32467;&#26524;&#26469;&#25253;&#21578;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28023;&#19978;&#20132;&#36890;&#30417;&#35270;&#20013;&#65292;&#26816;&#27979;&#38750;&#27861;&#27963;&#21160;&#65292;&#22914;&#38750;&#27861;&#25429;&#40060;&#25110;&#38750;&#27861;&#36135;&#29289;&#36716;&#33337;&#26159;&#27839;&#28023;&#31649;&#29702;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#24320;&#25918;&#28023;&#22495;&#20013;&#65292;&#20154;&#20204;&#24517;&#39035;&#20381;&#36182;&#33337;&#19978;&#30340;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#65288;AIS&#65289;&#21457;&#20986;&#30340;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#34987;&#30417;&#35270;&#21355;&#26143;&#25429;&#33719;&#12290;&#28982;&#32780;&#65292;&#19981;&#35802;&#23454;&#30340;&#33337;&#21482;&#36890;&#24120;&#20250;&#26377;&#24847;&#20851;&#38381;&#20854;AIS&#21457;&#23556;&#26426;&#65292;&#20197;&#38544;&#34255;&#38750;&#27861;&#27963;&#21160;&#12290;&#22312;&#24320;&#25918;&#28023;&#19978;&#65292;&#24456;&#38590;&#23558;&#26377;&#24847;&#30340;AIS&#20851;&#38381;&#19982;&#30001;&#20110;&#21327;&#35758;&#38480;&#21046;&#65292;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#25110;&#38480;&#21046;&#21355;&#26143;&#20301;&#32622;&#32780;&#23548;&#33268;&#30340;&#25509;&#25910;&#32570;&#22833;&#21306;&#20998;&#24320;&#26469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#24322;&#24120;AIS&#25509;&#25910;&#20002;&#22833;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#39044;&#27979;&#26410;&#26469;&#19968;&#20998;&#38047;&#26159;&#21542;&#24212;&#25509;&#25910;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#39044;&#27979;&#32467;&#26524;&#26469;&#25253;&#21578;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In maritime traffic surveillance, detecting illegal activities, such as illegal fishing or transshipment of illicit products is a crucial task of the coastal administration. In the open sea, one has to rely on Automatic Identification System (AIS) message transmitted by on-board transponders, which are captured by surveillance satellites. However, insincere vessels often intentionally shut down their AIS transponders to hide illegal activities. In the open sea, it is very challenging to differentiate intentional AIS shutdowns from missing reception due to protocol limitations, bad weather conditions or restricting satellite positions. This paper presents a novel approach for the detection of abnormal AIS missing reception based on self-supervised deep learning techniques and transformer models. Using historical data, the trained model predicts if a message should be received in the upcoming minute or not. Afterwards, the model reports on detected anomalies by comparing the prediction w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#26469;&#25913;&#36827;&#31070;&#32463;&#27169;&#22359;&#32593;&#32476;(NMN)&#65292;&#24182;&#24341;&#20837;&#20102;&#35745;&#21010;&#30340;&#25945;&#24072;&#24341;&#23548;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#35823;&#24046;&#31215;&#32047;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15585</link><description>&lt;p&gt;
&#25945;&#24072;&#24341;&#23548;&#30340;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Multimodal Representations for Teacher-Guided Compositional Visual Reasoning. (arXiv:2310.15585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#26469;&#25913;&#36827;&#31070;&#32463;&#27169;&#22359;&#32593;&#32476;(NMN)&#65292;&#24182;&#24341;&#20837;&#20102;&#35745;&#21010;&#30340;&#25945;&#24072;&#24341;&#23548;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#35823;&#24046;&#31215;&#32047;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27169;&#22359;&#32593;&#32476;&#65288;NMN&#65289;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#32763;&#35793;&#20026;&#30001;&#19968;&#31995;&#21015;&#25512;&#29702;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#31243;&#24207;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#25353;&#39034;&#24207;&#22312;&#22270;&#20687;&#19978;&#25191;&#34892;&#20197;&#20135;&#29983;&#31572;&#26696;&#12290;&#19982;&#38598;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;NMN&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#24213;&#23618;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#25552;&#39640;NMN&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#35268;&#27169;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#33719;&#24471;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;NMN&#35757;&#32451;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#27169;&#22359;&#36755;&#20986;&#20256;&#25773;&#21040;&#21518;&#32493;&#27169;&#22359;&#65292;&#23548;&#33268;&#39044;&#27979;&#35823;&#24046;&#32047;&#31215;&#21644;&#35823;&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;NMN&#23398;&#20064;&#31574;&#30053;&#65292;&#28041;&#21450;&#35745;&#21010;&#30340;&#25945;&#24072;&#24341;&#23548;&#12290;&#26368;&#21021;&#65292;&#27169;&#22411;&#23436;&#20840;&#30001;&#22320;&#38754;&#30495;&#23454;&#30340;&#20013;&#38388;&#36755;&#20986;&#24341;&#23548;&#65292;&#20294;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#36880;&#28176;&#36807;&#28193;&#21040;&#33258;&#20027;&#34892;&#20026;&#12290;&#36825;&#20943;&#23569;&#20102;&#35823;&#24046;&#31215;&#32047;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Module Networks (NMN) are a compelling method for visual question answering, enabling the translation of a question into a program consisting of a series of reasoning sub-tasks that are sequentially executed on the image to produce an answer. NMNs provide enhanced explainability compared to integrated models, allowing for a better understanding of the underlying reasoning process. To improve the effectiveness of NMNs we propose to exploit features obtained by a large-scale cross-modal encoder. Also, the current training approach of NMNs relies on the propagation of module outputs to subsequent modules, leading to the accumulation of prediction errors and the generation of false answers. To mitigate this, we introduce an NMN learning strategy involving scheduled teacher guidance. Initially, the model is fully guided by the ground-truth intermediate outputs, but gradually transitions to an autonomous behavior as training progresses. This reduces error accumulation, thus improving 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#19978;&#30340;&#20998;&#35010;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#22312;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#19978;&#20998;&#21035;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#25110;&#20849;&#21516;&#25512;&#29702;&#65292;&#20197;&#35299;&#20915;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38590;&#39064;&#12290;&#36890;&#36807;&#36873;&#25321;&#20998;&#21106;&#28857;&#21644;&#24102;&#23485;&#20998;&#37197;&#20197;&#26368;&#23567;&#21270;&#31995;&#32479;&#24310;&#36831;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.15584</link><description>&lt;p&gt;
&#21152;&#36895;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#19978;&#30340;&#20998;&#35010;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Split Federated Learning over Wireless Communication Networks. (arXiv:2310.15584v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#19978;&#30340;&#20998;&#35010;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#22312;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#19978;&#20998;&#21035;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#25110;&#20849;&#21516;&#25512;&#29702;&#65292;&#20197;&#35299;&#20915;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38590;&#39064;&#12290;&#36890;&#36807;&#36873;&#25321;&#20998;&#21106;&#28857;&#21644;&#24102;&#23485;&#20998;&#37197;&#20197;&#26368;&#23567;&#21270;&#31995;&#32479;&#24310;&#36831;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20026;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#21442;&#25968;&#37327;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#24471;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#37096;&#32626;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#21464;&#24471;&#22256;&#38590;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#27169;&#22411;&#20998;&#21106;/&#20998;&#35010;&#65292;&#21363;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#20998;&#21035;&#37096;&#32626;&#22312;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#25110;&#20849;&#21516;&#25512;&#29702;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24182;&#34892;&#27169;&#22411;&#35757;&#32451;&#26426;&#21046;&#21644;&#20998;&#35010;&#23398;&#20064;&#30340;&#27169;&#22411;&#20998;&#21106;&#32467;&#26500;&#30340;&#20998;&#35010;&#32852;&#21512;&#23398;&#20064;&#65288;SFL&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#20010;&#20307;&#20998;&#21106;&#28857;&#30340;&#24322;&#26500;&#35774;&#22791;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32852;&#21512;&#38382;&#39064;&#65292;&#21363;&#36873;&#25321;&#20998;&#21106;&#28857;&#21644;&#24102;&#23485;&#20998;&#37197;&#20197;&#26368;&#23567;&#21270;&#31995;&#32479;&#24310;&#36831;&#12290;&#36890;&#36807;&#20351;&#29992;&#20132;&#26367;&#20248;&#21270;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of artificial intelligence (AI) provides opportunities for the promotion of deep neural network (DNN)-based applications. However, the large amount of parameters and computational complexity of DNN makes it difficult to deploy it on edge devices which are resource-constrained. An efficient method to address this challenge is model partition/splitting, in which DNN is divided into two parts which are deployed on device and server respectively for co-training or co-inference. In this paper, we consider a split federated learning (SFL) framework that combines the parallel model training mechanism of federated learning (FL) and the model splitting structure of split learning (SL). We consider a practical scenario of heterogeneous devices with individual split points of DNN. We formulate a joint problem of split point selection and bandwidth allocation to minimize the system latency. By using alternating optimization, we decompose the problem into two sub-problems and solve 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#20174;&#32447;&#24615;&#39640;&#26031;&#27169;&#22411;&#36716;&#21270;&#20026;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#37096;&#20998;&#21442;&#25968;&#20445;&#25345;&#19981;&#21464;&#26102;&#30340;&#37096;&#20998;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15580</link><description>&lt;p&gt;
&#36879;&#36807;&#21464;&#21270;&#30340;&#35270;&#35282;&#65292;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#22810;&#39033;&#24335;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Identifiable Latent Polynomial Causal Models Through the Lens of Change. (arXiv:2310.15580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#20174;&#32447;&#24615;&#39640;&#26031;&#27169;&#22411;&#36716;&#21270;&#20026;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#37096;&#20998;&#21442;&#25968;&#20445;&#25345;&#19981;&#21464;&#26102;&#30340;&#37096;&#20998;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#20302;&#32423;&#25968;&#25454;&#20013;&#25581;&#31034;&#28508;&#22312;&#30340;&#39640;&#32423;&#22240;&#26524;&#34920;&#31034;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20219;&#21153;&#26159;&#25552;&#20379;&#21487;&#38752;&#30340;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35782;&#21035;&#20986;&#36825;&#20123;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#21363;&#21487;&#35782;&#21035;&#24615;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#31361;&#30772;&#24615;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#20043;&#38388;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#30340;&#22240;&#26524;&#24433;&#21709;&#30340;&#21464;&#21270;&#26469;&#25506;&#32034;&#21487;&#35782;&#21035;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24314;&#31435;&#22312;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#20005;&#26684;&#36981;&#24490;&#32447;&#24615;&#39640;&#26031;&#27169;&#22411;&#30340;&#20551;&#35774;&#22522;&#30784;&#19978;&#12290;&#26412;&#25991;&#23558;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#33539;&#22260;&#25193;&#23637;&#21040;&#28041;&#21450;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#20851;&#31995;&#30001;&#22810;&#39033;&#24335;&#27169;&#22411;&#34920;&#31034;&#65292;&#24182;&#19988;&#22122;&#22768;&#20998;&#24067;&#31526;&#21512;&#25351;&#25968;&#20998;&#24067;&#26063;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#25152;&#26377;&#22240;&#26524;&#21442;&#25968;&#26045;&#21152;&#21464;&#21270;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#22312;&#37096;&#20998;&#21442;&#25968;&#20445;&#25345;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#39564;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal representation learning aims to unveil latent high-level causal representations from observed low-level data. One of its primary tasks is to provide reliable assurance of identifying these latent causal models, known as identifiability. A recent breakthrough explores identifiability by leveraging the change of causal influences among latent causal variables across multiple environments \citep{liu2022identifying}. However, this progress rests on the assumption that the causal relationships among latent causal variables adhere strictly to linear Gaussian models. In this paper, we extend the scope of latent causal models to involve nonlinear causal relationships, represented by polynomial models, and general noise distributions conforming to the exponential family. Additionally, we investigate the necessity of imposing changes on all causal parameters and present partial identifiability results when part of them remains unchanged. Further, we propose a novel empirical estimation me
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;</title><link>http://arxiv.org/abs/2310.15578</link><description>&lt;p&gt;
&#22312;PyTorch&#19978;&#37325;&#26032;&#23454;&#29616;&#30340;VMAF&#65306;&#19968;&#20123;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
VMAF Re-implementation on PyTorch: Some Experimental Results. (arXiv:2310.15578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15578
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26631;&#20934;&#30340;VMAF&#23454;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;PyTorch&#26694;&#26550;&#23454;&#29616;VMAF&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20010;&#23454;&#29616;&#65292;&#19982;&#26631;&#20934;&#30340;(libvmaf)&#36827;&#34892;&#27604;&#36739;&#65292;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the standard VMAF implementation we propose an implementation of VMAF using PyTorch framework. For this implementation comparisons with the standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We investigate gradients computation when using VMAF as an objective function and demonstrate that training using this function does not result in ill-behaving gradients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#24403;Oja&#31639;&#27861;&#24212;&#29992;&#20110;&#20849;&#20139;&#29305;&#24449;&#21521;&#37327;&#30340;&#23545;&#31216;&#30697;&#38453;&#26102;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#26041;&#27861;&#30340;&#36951;&#25022;&#26469;&#30028;&#23450;&#20854;&#36951;&#25022;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#21333;&#20301;&#29699;&#19978;&#30340;&#20108;&#27425;&#24418;&#24335;&#20248;&#21270;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.15559</link><description>&lt;p&gt;
&#20174;Oja&#31639;&#27861;&#21040;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Oja's Algorithm to the Multiplicative Weights Update Method with Applications. (arXiv:2310.15559v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#24403;Oja&#31639;&#27861;&#24212;&#29992;&#20110;&#20849;&#20139;&#29305;&#24449;&#21521;&#37327;&#30340;&#23545;&#31216;&#30697;&#38453;&#26102;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#26041;&#27861;&#30340;&#36951;&#25022;&#26469;&#30028;&#23450;&#20854;&#36951;&#25022;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#21333;&#20301;&#29699;&#19978;&#30340;&#20108;&#27425;&#24418;&#24335;&#20248;&#21270;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Oja&#31639;&#27861;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#20027;&#35201;&#22312;&#38543;&#26426;&#20027;&#25104;&#20998;&#26512;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35266;&#23519;&#65292;&#28982;&#32780;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21457;&#29616;&#65292;&#21363;&#24403;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#65288;&#19981;&#19968;&#23450;&#26159;&#38543;&#26426;&#30340;&#65289;&#20849;&#20139;&#20844;&#20849;&#29305;&#24449;&#21521;&#37327;&#30340;&#23545;&#31216;&#30697;&#38453;&#26102;&#65292;Oja&#31639;&#27861;&#30340;&#36951;&#25022;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#39044;&#27979;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#20013;&#30340;&#24191;&#20026;&#20154;&#30693;&#30340;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#26041;&#27861;&#30340;&#36951;&#25022;&#26469;&#30028;&#23450;&#12290;&#35752;&#35770;&#20102;&#22312;&#21333;&#20301;&#29699;&#19978;&#30340;&#20108;&#27425;&#24418;&#24335;&#20248;&#21270;&#30340;&#20960;&#20010;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oja's algorithm is a well known online algorithm studied mainly in the context of stochastic principal component analysis. We make a simple observation, yet to the best of our knowledge a novel one, that when applied to a any (not necessarily stochastic) sequence of symmetric matrices which share common eigenvectors, the regret of Oja's algorithm could be directly bounded in terms of the regret of the well known multiplicative weights update method for the problem of prediction with expert advice. Several applications to optimization with quadratic forms over the unit sphere in $\reals^n$ are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#27431;&#27954;&#22269;&#23478;&#30340;&#30005;&#21147;&#38656;&#27714;&#26102;&#38388;&#24207;&#21015;&#20026;&#20363;&#65292;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#12290;&#30740;&#31350;&#37319;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#20998;&#26512;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15555</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#21069;&#19968;&#22825;&#36127;&#33655;&#39044;&#27979;&#65306;&#20197;&#27431;&#27954;&#22269;&#23478;&#30005;&#21147;&#38656;&#27714;&#26102;&#38388;&#24207;&#21015;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transfer learning for day-ahead load forecasting: a case study on European national electricity demand time series. (arXiv:2310.15555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#27431;&#27954;&#22269;&#23478;&#30340;&#30005;&#21147;&#38656;&#27714;&#26102;&#38388;&#24207;&#21015;&#20026;&#20363;&#65292;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#12290;&#30740;&#31350;&#37319;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#20998;&#26512;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;(STLF)&#23545;&#30005;&#21147;&#32593;&#32476;&#30340;&#26085;&#24120;&#36816;&#33829;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30005;&#21147;&#38656;&#27714;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#38543;&#26426;&#24615;&#20351;&#24471;STLF&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#25913;&#36827;STLF&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#21487;&#33021;&#19981;&#21253;&#25324;&#30446;&#26631;&#24207;&#21015;&#30340;&#22810;&#20010;&#30005;&#21147;&#38656;&#27714;&#24207;&#21015;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;(NN)&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#32452;&#34920;&#31034;&#20856;&#22411;&#27431;&#27954;&#22269;&#23478;&#26410;&#26469;&#19968;&#22825;&#30005;&#21147;&#38656;&#27714;&#30340;27&#20010;&#26102;&#38388;&#24207;&#21015;&#65292;&#26469;&#30740;&#31350;&#36825;&#31181;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;STLF&#65292;&#21363;&#36716;&#31227;&#23398;&#20064;(TL)&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#27969;&#34892;&#19988;&#26131;&#20110;&#23454;&#26045;&#30340;NN&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#31995;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#27169;&#24335;&#24182;&#36741;&#21161;TL&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#32534;&#21046;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;TL&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#24102;&#26377;&#32858;&#31867;&#27493;&#39588;&#30340;&#65292;&#19968;&#31181;&#26159;&#27809;&#26377;&#30340;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20856;&#22411;&#30340;NN&#35757;&#32451;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term load forecasting (STLF) is crucial for the daily operation of power grids. However, the non-linearity, non-stationarity, and randomness characterizing electricity demand time series renders STLF a challenging task. Various forecasting approaches have been proposed for improving STLF, including neural network (NN) models which are trained using data from multiple electricity demand series that may not necessary include the target series. In the present study, we investigate the performance of this special case of STLF, called transfer learning (TL), by considering a set of 27 time series that represent the national day-ahead electricity demand of indicative European countries. We employ a popular and easy-to-implement NN model and perform a clustering analysis to identify similar patterns among the series and assist TL. In this context, two different TL approaches, with and without the clustering step, are compiled and compared against each other as well as a typical NN train
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#33258;&#36866;&#24212;&#27531;&#24046;&#20272;&#35745;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;PET&#22270;&#20687;&#30340;&#21512;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#32441;&#29702;&#21644;&#32467;&#26500;&#24046;&#24322;&#38382;&#39064;&#65292;&#21516;&#26102;&#23545;&#20302;&#21058;&#37327;PET&#21644;&#26631;&#20934;PET&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.15550</link><description>&lt;p&gt;
PET&#21512;&#25104;&#65306;&#33258;&#25105;&#30417;&#30563;&#33258;&#36866;&#24212;&#27531;&#24046;&#20272;&#35745;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PET Synthesis via Self-supervised Adaptive Residual Estimation Generative Adversarial Network. (arXiv:2310.15550v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#33258;&#36866;&#24212;&#27531;&#24046;&#20272;&#35745;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;PET&#22270;&#20687;&#30340;&#21512;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#32441;&#29702;&#21644;&#32467;&#26500;&#24046;&#24322;&#38382;&#39064;&#65292;&#21516;&#26102;&#23545;&#20302;&#21058;&#37327;PET&#21644;&#26631;&#20934;PET&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#26159;&#19968;&#31181;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#39640;&#28789;&#25935;&#24230;&#30340;&#20998;&#23376;&#25104;&#20687;&#25216;&#26415;&#12290;&#20943;&#23569;PET&#30340;&#36752;&#23556;&#26292;&#38706;&#65292;&#21516;&#26102;&#20445;&#25345;&#36275;&#22815;&#30340;&#22270;&#20687;&#36136;&#37327;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20174;&#20302;&#21058;&#37327;PET&#29983;&#25104;&#21512;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26041;&#27861;&#34987;&#25253;&#36947;&#20026;&#20302;&#21040;&#39640;&#22270;&#20687;&#24674;&#22797;&#25216;&#26415;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26131;&#20110;&#23637;&#31034;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#32441;&#29702;&#21644;&#32467;&#26500;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#20302;&#21058;&#37327;PET&#21644;&#26631;&#20934;PET&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#23578;&#26410;&#23436;&#20840;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#33258;&#36866;&#24212;&#27531;&#24046;&#20272;&#35745;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SS-AEGAN&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#65288;1&#65289;&#33258;&#36866;&#24212;&#27531;&#24046;&#20272;&#35745;&#26144;&#23556;&#26426;&#21046;AE-Net&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#37319;&#29992;&#20302;&#21058;&#37327;PET&#21644;&#21512;&#25104;PET&#20043;&#38388;&#30340;&#27531;&#24046;&#22270;&#21160;&#24577;&#20462;&#27491;&#21021;&#27493;&#21512;&#25104;&#30340;PET&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positron emission tomography (PET) is a widely used, highly sensitive molecular imaging in clinical diagnosis. There is interest in reducing the radiation exposure from PET but also maintaining adequate image quality. Recent methods using convolutional neural networks (CNNs) to generate synthesized high-quality PET images from low-dose counterparts have been reported to be state-of-the-art for low-to-high image recovery methods. However, these methods are prone to exhibiting discrepancies in texture and structure between synthesized and real images. Furthermore, the distribution shift between low-dose PET and standard PET has not been fully investigated. To address these issues, we developed a self-supervised adaptive residual estimation generative adversarial network (SS-AEGAN). We introduce (1) An adaptive residual estimation mapping mechanism, AE-Net, designed to dynamically rectify the preliminary synthesized PET images by taking the residual map between the low-dose PET and synthe
&lt;/p&gt;</description></item><item><title>&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23545;&#31216;&#12289;&#31209;1&#24352;&#37327;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#26102;&#65292;&#23558;&#26799;&#24230;&#19979;&#38477;&#24212;&#29992;&#20110;&#21319;&#32500;&#38382;&#39064;&#21487;&#20197;&#24471;&#21040;&#36817;&#20284;&#30340;&#31209;1&#24352;&#37327;&#21644;&#20855;&#26377;&#36867;&#36920;&#26041;&#21521;&#30340;&#20020;&#30028;&#28857;&#65292;&#36825;&#32467;&#26524;&#24378;&#35843;&#20102;&#24352;&#37327;&#21442;&#25968;&#21270;&#21644;&#19968;&#38454;&#26041;&#27861;&#22312;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15549</link><description>&lt;p&gt;
&#24352;&#37327;&#20248;&#21270;&#20013;&#30340;&#31639;&#27861;&#27491;&#21017;&#21270;: &#36808;&#21521;&#30697;&#38453;&#24863;&#30693;&#20013;&#30340;&#21319;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing. (arXiv:2310.15549v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15549
&lt;/p&gt;
&lt;p&gt;
&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23545;&#31216;&#12289;&#31209;1&#24352;&#37327;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#26102;&#65292;&#23558;&#26799;&#24230;&#19979;&#38477;&#24212;&#29992;&#20110;&#21319;&#32500;&#38382;&#39064;&#21487;&#20197;&#24471;&#21040;&#36817;&#20284;&#30340;&#31209;1&#24352;&#37327;&#21644;&#20855;&#26377;&#36867;&#36920;&#26041;&#21521;&#30340;&#20020;&#30028;&#28857;&#65292;&#36825;&#32467;&#26524;&#24378;&#35843;&#20102;&#24352;&#37327;&#21442;&#25968;&#21270;&#21644;&#19968;&#38454;&#26041;&#27861;&#22312;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23545;&#20110;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#24341;&#20837;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#20419;&#36827;&#20102;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;GD&#22312;&#24352;&#37327;&#20248;&#21270;&#20013;&#24341;&#23548;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#21319;&#32500;&#30697;&#38453;&#24863;&#30693;&#26694;&#26550;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#23558;&#23545;&#31216;&#30340;&#12289;&#31209;1&#30340;&#24352;&#37327;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#25104;&#20005;&#26684;&#38797;&#28857;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#38750;&#20984;&#30340;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#23610;&#24230;&#19979;&#65292;&#23558;GD&#24212;&#29992;&#20110;&#36825;&#20010;&#21319;&#32500;&#38382;&#39064;&#65292;&#21487;&#20197;&#24471;&#21040;&#36817;&#20284;&#31209;1&#30340;&#24352;&#37327;&#21644;&#20855;&#26377;&#36867;&#36920;&#26041;&#21521;&#30340;&#20020;&#30028;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#24352;&#37327;&#21442;&#25968;&#21270;&#19982;&#19968;&#38454;&#26041;&#27861;&#22312;&#36825;&#31867;&#38382;&#39064;&#20013;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient descent (GD) is crucial for generalization in machine learning models, as it induces implicit regularization, promoting compact representations. In this work, we examine the role of GD in inducing implicit regularization for tensor optimization, particularly within the context of the lifted matrix sensing framework. This framework has been recently proposed to address the non-convex matrix sensing problem by transforming spurious solutions into strict saddles when optimizing over symmetric, rank-1 tensors. We show that, with sufficiently small initialization scale, GD applied to this lifted problem results in approximate rank-1 tensors and critical points with escape directions. Our findings underscore the significance of the tensor parametrization of matrix sensing, in combination with first-order methods, in achieving global optimality in such problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#30340;&#36335;&#30001;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#22270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23616;&#37096;&#25110;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15543</link><description>&lt;p&gt;
&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#36335;&#30001;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Symmetry-preserving graph attention network to solve routing problems at multiple resolutions. (arXiv:2310.15543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#30340;&#36335;&#30001;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#22270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23616;&#37096;&#25110;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#21512;&#29702;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#37117;&#27809;&#26377;&#23436;&#20840;&#23562;&#37325;TSP&#21644;VRP&#20013;&#20135;&#29983;&#30340;&#23545;&#31216;&#24615;&#65292;&#21253;&#25324;&#26059;&#36716;&#12289;&#24179;&#31227;&#12289;&#25490;&#21015;&#21644;&#32553;&#25918;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#22823;&#22411;&#21644;&#38271;&#36317;&#31163;&#22270;&#30340;&#24773;&#20917;&#65292;&#25429;&#25417;&#36755;&#20837;&#22270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#65288;&#21363;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#20449;&#24687;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#25552;&#21462;&#23616;&#37096;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#23616;&#37096;&#25110;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20998;&#36776;&#29575;&#26041;&#26696;&#19982;&#31561;&#21464;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;mEGAT&#65289;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22522;&#20110;&#20302;&#32423;&#21035;&#21644;&#39640;&#32423;&#21035;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26368;&#20339;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Travelling Salesperson Problems (TSPs) and Vehicle Routing Problems (VRPs) have achieved reasonable improvement in accuracy and computation time with the adaptation of Machine Learning (ML) methods. However, none of the previous works completely respects the symmetries arising from TSPs and VRPs including rotation, translation, permutation, and scaling. In this work, we introduce the first-ever completely equivariant model and training to solve combinatorial problems. Furthermore, it is essential to capture the multiscale structure (i.e. from local to global information) of the input graph, especially for the cases of large and long-range graphs, while previous methods are limited to extracting only local information that can lead to a local or sub-optimal solution. To tackle the above limitation, we propose a Multiresolution scheme in combination with Equivariant Graph Attention network (mEGAT) architecture, which can learn the optimal route based on low-level and high-level graph res
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#25277;&#26679;&#20998;&#26512;&#30697;&#38453;&#26426;&#21046;&#30340;&#38544;&#31169;&#25918;&#22823;&#31639;&#27861;MMCC&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#26465;&#20214;&#24615;&#32452;&#21512;&#23450;&#29702;&#21487;&#20197;&#23558;&#30456;&#20851;&#36755;&#20986;&#35270;&#20026;&#29420;&#31435;&#36755;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;DP-FTRL&#31639;&#27861;&#26102;&#21487;&#20197;&#28176;&#36817;&#22320;&#19982;DP-SGD&#31639;&#27861;&#20013;&#25918;&#22823;&#21518;&#28155;&#21152;&#30340;&#22122;&#22768;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2310.15526</link><description>&lt;p&gt;
&#30697;&#38453;&#26426;&#21046;&#30340;&#38544;&#31169;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Privacy Amplification for Matrix Mechanisms. (arXiv:2310.15526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15526
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#25277;&#26679;&#20998;&#26512;&#30697;&#38453;&#26426;&#21046;&#30340;&#38544;&#31169;&#25918;&#22823;&#31639;&#27861;MMCC&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#26465;&#20214;&#24615;&#32452;&#21512;&#23450;&#29702;&#21487;&#20197;&#23558;&#30456;&#20851;&#36755;&#20986;&#35270;&#20026;&#29420;&#31435;&#36755;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;DP-FTRL&#31639;&#27861;&#26102;&#21487;&#20197;&#28176;&#36817;&#22320;&#19982;DP-SGD&#31639;&#27861;&#20013;&#25918;&#22823;&#21518;&#28155;&#21152;&#30340;&#22122;&#22768;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25918;&#22823;&#21033;&#29992;&#25968;&#25454;&#36873;&#25321;&#20013;&#30340;&#38543;&#26426;&#24615;&#26469;&#25552;&#20379;&#26356;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#36825;&#31181;&#20998;&#26512;&#23545;&#20110;DP-SGD&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#24182;&#19981;&#36866;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#65292;&#21363;DP-FTRL&#65292;&#20351;&#29992;&#30697;&#38453;&#26426;&#21046;&#26469;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#32780;&#19981;&#26159;&#20687;DP-SGD&#37027;&#26679;&#28155;&#21152;&#29420;&#31435;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MMCC&#8221;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#36890;&#36807;&#25277;&#26679;&#26469;&#20998;&#26512;&#20219;&#20309;&#36890;&#29992;&#30697;&#38453;&#26426;&#21046;&#30340;&#38544;&#31169;&#25918;&#22823;&#30340;&#39318;&#20010;&#31639;&#27861;&#12290;MMCC&#25509;&#36817;&#19968;&#20010;&#19979;&#30028;&#65292;&#24403;&#949;&#8594;0&#26102;&#65292;&#23427;&#36235;&#36817;&#20110;&#35813;&#19979;&#30028;&#12290;&#20026;&#20102;&#20998;&#26512;MMCC&#20013;&#30340;&#30456;&#20851;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#23558;&#20854;&#35270;&#20026;&#29420;&#31435;&#36755;&#20986;&#65292;&#36890;&#36807;&#23545;&#20808;&#21069;&#36755;&#20986;&#36827;&#34892;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#8220;&#26465;&#20214;&#24615;&#32452;&#21512;&#23450;&#29702;&#8221;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#29992;&#24615;&#65306;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#26174;&#31034;&#28155;&#21152;&#21040;&#20108;&#21449;&#26641;DP-FTRL&#30340;&#22122;&#22768;&#21487;&#20197;&#28176;&#36817;&#22320;&#19982;&#36890;&#36807;&#25918;&#22823;&#28155;&#21152;&#21040;DP-SGD&#20013;&#30340;&#22122;&#22768;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#25918;&#22823;&#31639;&#27861;&#20063;&#20855;&#26377;&#23454;&#38469;&#30340;&#32463;&#39564;&#24615;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy amplification exploits randomness in data selection to provide tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's success in machine learning, but, is not readily applicable to the newer state-of-the-art algorithms. This is because these algorithms, known as DP-FTRL, use the matrix mechanism to add correlated noise instead of independent noise as in DP-SGD.  In this paper, we propose "MMCC", the first algorithm to analyze privacy amplification via sampling for any generic matrix mechanism. MMCC is nearly tight in that it approaches a lower bound as $\epsilon\to0$. To analyze correlated outputs in MMCC, we prove that they can be analyzed as if they were independent, by conditioning them on prior outputs. Our "conditional composition theorem" has broad utility: we use it to show that the noise added to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with amplification. Our amplification algorithm also has practical empirical util
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#38706;&#30340;&#27934;&#23519;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#20013;&#38544;&#31169;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.15524</link><description>&lt;p&gt;
&#20851;&#20110;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20869;&#22312;&#38544;&#31169;&#23646;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Inherent Privacy Properties of Discrete Denoising Diffusion Models. (arXiv:2310.15524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#38706;&#30340;&#27934;&#23519;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#20013;&#38544;&#31169;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#38382;&#39064;&#23548;&#33268;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#28608;&#22686;&#65292;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#65292;&#20294;&#22312;&#25552;&#20379;&#25968;&#23398;&#29305;&#24449;&#21270;&#20854;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#20869;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#24320;&#21019;&#24615;&#29702;&#35770;&#30740;&#31350;&#65292;&#29992;&#20110;&#31163;&#25955;&#25968;&#25454;&#38598;&#29983;&#25104;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#27599;&#20010;&#23454;&#20363;&#24046;&#24322;&#38544;&#31169;&#65288;pDP&#65289;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#38416;&#26126;&#20102;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#28508;&#22312;&#38544;&#31169;&#27844;&#38706;&#65292;&#20174;&#32780;&#20026;&#36890;&#36807;DDMs&#38477;&#20302;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#38544;&#31169;&#39118;&#38505;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#36824;&#34920;&#26126;&#65292;&#20351;&#29992;$s$&#20010;&#22823;&#23567;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#20174;$(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP&#21040;$(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP&#30340;&#28608;&#22686;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy concerns have led to a surge in the creation of synthetic datasets, with diffusion models emerging as a promising avenue. Although prior studies have performed empirical evaluations on these models, there has been a gap in providing a mathematical characterization of their privacy-preserving capabilities. To address this, we present the pioneering theoretical exploration of the privacy preservation inherent in discrete diffusion models (DDMs) for discrete dataset generation. Focusing on per-instance differential privacy (pDP), our framework elucidates the potential privacy leakage for each data point in a given training dataset, offering insights into data preprocessing to reduce privacy risks of the synthetic dataset generation via DDMs. Our bounds also show that training with $s$-sized data points leads to a surge in privacy leakage from $(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pu
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#21644;&#23545;&#27604;&#24335;&#33539;&#24335;&#22312;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#26159;&#20114;&#34917;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;GCMAE&#65289;&#26694;&#26550;&#26469;&#32479;&#19968;&#23427;&#20204;&#65292;GCMAE&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#24357;&#34917;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#22312;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.15523</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#21644;&#23545;&#27604;&#24335;&#33539;&#24335;&#22312;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#20114;&#34917;
&lt;/p&gt;
&lt;p&gt;
Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning. (arXiv:2310.15523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15523
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#21644;&#23545;&#27604;&#24335;&#33539;&#24335;&#22312;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#26159;&#20114;&#34917;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;GCMAE&#65289;&#26694;&#26550;&#26469;&#32479;&#19968;&#23427;&#20204;&#65292;GCMAE&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#24357;&#34917;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#22312;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;GSSL&#65289;&#65292;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#36981;&#24490;&#29983;&#25104;&#24335;&#33539;&#24335;&#65292;&#24182;&#23398;&#20064;&#37325;&#26500;&#25513;&#30721;&#22270;&#30340;&#36793;&#32536;&#25110;&#33410;&#28857;&#29305;&#24449;&#12290;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#36890;&#36807;&#26368;&#22823;&#21270;&#21516;&#19968;&#22270;&#30340;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#24191;&#27867;&#29992;&#20110;GSSL&#12290;&#28982;&#32780;&#65292;MAE&#21644;CL&#22312;&#29616;&#26377;&#30340;GSSL&#24037;&#20316;&#20013;&#34987;&#21333;&#29420;&#32771;&#34385;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;MAE&#21644;CL&#30340;&#33539;&#24335;&#26159;&#20114;&#34917;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;GCMAE&#65289;&#26694;&#26550;&#26469;&#32479;&#19968;&#23427;&#20204;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#23616;&#37096;&#36793;&#32536;&#25110;&#33410;&#28857;&#29305;&#24449;&#65292;MAE&#19981;&#33021;&#25429;&#25417;&#21040;&#22270;&#30340;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#23545;&#29305;&#23450;&#30340;&#36793;&#32536;&#21644;&#29305;&#24449;&#25935;&#24863;&#12290;&#30456;&#21453;&#65292;&#22312;&#25552;&#21462;&#20840;&#23616;&#20449;&#24687;&#26041;&#38754;&#65292;CL&#34920;&#29616;&#20986;&#33394;&#65292;&#22240;&#20026;&#23427;&#32771;&#34385;&#20102;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;GCMAE&#35013;&#22791;&#20102;&#19968;&#20010;MAE&#20998;&#25903;&#21644;&#19968;&#20010;CL&#20998;&#25903;&#65292;&#24182;&#19988;&#36825;&#20004;&#20010;&#20998;&#25903;&#20849;&#20139;&#19968;&#20010;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#65292;&#36825;&#20351;&#24471;MAE&#20998;&#25903;&#33021;&#22815;&#21033;&#29992;CL&#20998;&#25903;&#25552;&#21462;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#24378;&#21046;GCMAE&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;...
&lt;/p&gt;
&lt;p&gt;
For graph self-supervised learning (GSSL), masked autoencoder (MAE) follows the generative paradigm and learns to reconstruct masked graph edges or node features. Contrastive Learning (CL) maximizes the similarity between augmented views of the same graph and is widely used for GSSL. However, MAE and CL are considered separately in existing works for GSSL. We observe that the MAE and CL paradigms are complementary and propose the graph contrastive masked autoencoder (GCMAE) framework to unify them. Specifically, by focusing on local edges or node features, MAE cannot capture global information of the graph and is sensitive to particular edges and features. On the contrary, CL excels in extracting global information because it considers the relation between graphs. As such, we equip GCMAE with an MAE branch and a CL branch, and the two branches share a common encoder, which allows the MAE branch to exploit the global information extracted by the CL branch. To force GCMAE to capture glob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15516</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs. (arXiv:2310.15516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#27169;&#22411;&#22312;&#35299;&#20915;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DRL&#27714;&#35299;&#22120;&#36890;&#24120;&#26159;&#29992;&#26469;&#35299;&#20915;&#33410;&#28857;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20363;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;&#24212;&#29992;&#31070;&#32463;&#26041;&#27861;&#26469;&#35299;&#20915;&#24359;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20363;&#22914;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#65288;CPP&#65289;&#65292;&#30340;&#30740;&#31350;&#21364;&#21313;&#20998;&#26377;&#38480;&#65292;&#22240;&#20026;&#19982;TSP&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#35299;&#31354;&#38388;&#36890;&#24120;&#26356;&#21152;&#19981;&#35268;&#21017;&#21644;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;DRL&#26694;&#26550;&#65292;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#65288;CPP-LC&#65289;&#30340;CPP&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36127;&#36733;&#32422;&#26463;&#30340;&#22797;&#26434;&#24359;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21019;&#26032;&#28857;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;CPP-LC&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#39034;&#24207;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;DRL&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21363;Arc-DRL&#27169;&#22411;&#65292;&#23427;&#30001;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;CPP-LC&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#20351;&#24471;DRL&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Deep reinforcement learning (DRL) models have shown promising results in solving routing problems. However, most DRL solvers are commonly proposed to solve node routing problems, such as the Traveling Salesman Problem (TSP). Meanwhile, there has been limited research on applying neural methods to arc routing problems, such as the Chinese Postman Problem (CPP), since they often feature irregular and complex solution spaces compared to TSP. To fill these gaps, this paper proposes a novel DRL framework to address the CPP with load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc routing problem with load constraints. The novelty of our method is two-fold. First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential model. Subsequently, we introduce an autoregressive model based on DRL, namely Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge effectively. Such a framework allows the DRL model to work efficiently 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#22238;&#31572;&#32422;&#26463;&#28385;&#36275;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;KITAB&#26469;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#28385;&#36275;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15511</link><description>&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#35780;&#20272;&#22522;&#20110;&#32422;&#26463;&#28385;&#36275;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval. (arXiv:2310.15511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#22238;&#31572;&#32422;&#26463;&#28385;&#36275;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;KITAB&#26469;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#28385;&#36275;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#22238;&#31572;&#32422;&#26463;&#28385;&#36275;&#26597;&#35810;&#65288;&#20363;&#22914;&#65292;&#8220;&#22307;&#22320;&#20122;&#21733;&#30340;&#20912;&#28103;&#28107;&#24215;&#21015;&#34920;&#8221;&#65289;&#30340;&#33021;&#21147;&#12290;&#36807;&#21435;&#65292;&#36825;&#26679;&#30340;&#26597;&#35810;&#34987;&#35748;&#20026;&#21482;&#33021;&#36890;&#36807;&#32593;&#32476;&#25628;&#32034;&#25110;&#30693;&#35782;&#24211;&#26469;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21021;&#27493;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#30340;&#26816;&#32034;&#22522;&#20934;&#35201;&#20040;&#24050;&#39281;&#21644;&#65292;&#35201;&#20040;&#19981;&#33021;&#34913;&#37327;&#32422;&#26463;&#28385;&#36275;&#12290;&#21463;&#21040;&#23545;LLMs&#20107;&#23454;&#19981;&#27491;&#30830;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#26085;&#30410;&#20851;&#27880;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KITAB&#65292;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#32422;&#26463;&#28385;&#36275;&#33021;&#21147;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;KITAB&#21253;&#21547;600&#22810;&#20301;&#20316;&#32773;&#21644;13,000&#20010;&#26597;&#35810;&#30340;&#19982;&#20070;&#31821;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#32852;&#30340;&#21160;&#24577;&#25968;&#25454;&#25910;&#38598;&#21644;&#32422;&#26463;&#39564;&#35777;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20854;&#20182;&#20316;&#32773;&#30340;&#31867;&#20284;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;GPT4&#21644;GPT3.5&#36827;&#34892;&#20102;&#25193;&#23637;&#23454;&#39564;&#65292;&#23545;&#24120;&#35265;&#30340;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#20102;&#34920;&#24449;&#21644;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., 'a list of ice cream shops in San Diego'). In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes acros
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;AutoDiff&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#21644;&#29305;&#24449;&#38388;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#32479;&#35745;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.15479</link><description>&lt;p&gt;
AutoDiff:&#32467;&#21512;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing. (arXiv:2310.15479v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15479
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;AutoDiff&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#21644;&#29305;&#24449;&#38388;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#32479;&#35745;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#35768;&#22810;&#23376;&#39046;&#22495;&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#20027;&#35201;&#33539;&#24335;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#35328;&#27169;&#22411;&#25110;&#35821;&#38899;&#21512;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#21147;&#37327;&#26469;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#19968;&#30452;&#26159;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#21512;&#25104;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#34920;&#26684;&#22312;&#32479;&#35745;&#19978;&#19982;&#30495;&#23454;&#25968;&#25454;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#22312;15&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#28789;&#27963;&#22320;&#25429;&#25417;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22914;&#33509;&#25509;&#32435;&#20102;&#35770;&#25991;&#65292;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#26681;&#25454;&#35201;&#27714;&#25552;&#20379;&#65292;&#24182;&#19988;&#23558;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model has become a main paradigm for synthetic data generation in many subfields of modern machine learning, including computer vision, language model, or speech synthesis. In this paper, we leverage the power of diffusion model for generating synthetic tabular data. The heterogeneous features in tabular data have been main obstacles in tabular data synthesis, and we tackle this problem by employing the auto-encoder architecture. When compared with the state-of-the-art tabular synthesizers, the resulting synthetic tables from our model show nice statistical fidelities to the real data, and perform well in downstream tasks for machine learning utilities. We conducted the experiments over 15 publicly available datasets. Notably, our model adeptly captures the correlations among features, which has been a long-standing challenge in tabular data synthesis. Our code is available upon request and will be publicly released if paper is accepted.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#29983;&#23384;&#20998;&#26512;&#27969;&#31243;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#29983;&#23384;&#22534;&#21472;&#25216;&#26415;&#23558;&#29983;&#23384;&#20998;&#26512;&#38382;&#39064;&#36716;&#21270;&#20026;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#24378;&#30340;&#22686;&#24378;&#26426;&#22120;&#36827;&#34892;&#24515;&#34928;&#39118;&#38505;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.15472</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24515;&#34928;&#39118;&#38505;&#39044;&#27979;&#30340;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Interpretable Survival Analysis for Heart Failure Risk Prediction. (arXiv:2310.15472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#29983;&#23384;&#20998;&#26512;&#27969;&#31243;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#29983;&#23384;&#22534;&#21472;&#25216;&#26415;&#23558;&#29983;&#23384;&#20998;&#26512;&#38382;&#39064;&#36716;&#21270;&#20026;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#24378;&#30340;&#22686;&#24378;&#26426;&#22120;&#36827;&#34892;&#24515;&#34928;&#39118;&#38505;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#65292;&#25110;&#26102;&#38388;&#20107;&#20214;&#20998;&#26512;&#65292;&#22312;&#21307;&#30103;&#30740;&#31350;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#24191;&#27867;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#21307;&#30103;&#30740;&#31350;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;Cox&#27169;&#22411;&#36827;&#34892;&#29983;&#23384;&#20998;&#26512;&#65292;&#22240;&#20026;&#20854;&#31616;&#21333;&#19988;&#21487;&#35299;&#37322;&#24615;&#24378;&#12290;Cox&#27169;&#22411;&#20551;&#35774;&#19968;&#20010;&#23545;&#25968;&#32447;&#24615;&#39118;&#38505;&#20989;&#25968;&#20197;&#21450;&#26102;&#38388;&#19978;&#30340;&#27604;&#20363;&#39118;&#38505;&#65292;&#24403;&#36825;&#20123;&#20551;&#35774;&#22833;&#36133;&#26102;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#22411;&#29983;&#23384;&#27169;&#22411;&#36991;&#20813;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#26377;&#26102;&#20197;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#65292;&#32780;&#36825;&#23545;&#20110;&#20020;&#24202;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#23384;&#20998;&#26512;&#27969;&#31243;&#65292;&#26082;&#21487;&#35299;&#37322;&#24615;&#24378;&#65292;&#21448;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#29983;&#23384;&#27169;&#22411;&#31454;&#20105;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#25913;&#36827;&#30340;&#29983;&#23384;&#22534;&#21472;&#25216;&#26415;&#23558;&#29983;&#23384;&#20998;&#26512;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;ControlBurn&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22686;&#24378;&#26426;&#22120;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#27969;&#31243;&#65292;&#25105;&#20204;&#39044;&#27979;&#24515;&#34928;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis, or time-to-event analysis, is an important and widespread problem in healthcare research. Medical research has traditionally relied on Cox models for survival analysis, due to their simplicity and interpretability. Cox models assume a log-linear hazard function as well as proportional hazards over time, and can perform poorly when these assumptions fail. Newer survival models based on machine learning avoid these assumptions and offer improved accuracy, yet sometimes at the expense of model interpretability, which is vital for clinical use. We propose a novel survival analysis pipeline that is both interpretable and competitive with state-of-the-art survival models. Specifically, we use an improved version of survival stacking to transform a survival analysis problem to a classification problem, ControlBurn to perform feature selection, and Explainable Boosting Machines to generate interpretable predictions. To evaluate our pipeline, we predict risk of heart failure 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#35821;&#20041;&#28151;&#28102;&#20462;&#27491;&#30340;&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#27169;&#22411;&#65292;&#36890;&#36807;&#26631;&#27880;&#20266;&#26631;&#31614;&#21644;&#20256;&#36882;&#20851;&#38190;&#30693;&#35782;&#26469;&#32531;&#35299;&#20107;&#20214;&#31867;&#22411;&#35821;&#20041;&#28151;&#28102;&#24182;&#25552;&#39640;&#27169;&#22411;&#22312;&#38271;&#23614;&#20107;&#20214;&#31867;&#22411;&#30340;&#29702;&#35299;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15470</link><description>&lt;p&gt;
&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#19982;&#35821;&#20041;&#28151;&#28102;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Continual Event Extraction with Semantic Confusion Rectification. (arXiv:2310.15470v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#35821;&#20041;&#28151;&#28102;&#20462;&#27491;&#30340;&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#27169;&#22411;&#65292;&#36890;&#36807;&#26631;&#27880;&#20266;&#26631;&#31614;&#21644;&#20256;&#36882;&#20851;&#38190;&#30693;&#35782;&#26469;&#32531;&#35299;&#20107;&#20214;&#31867;&#22411;&#35821;&#20041;&#28151;&#28102;&#24182;&#25552;&#39640;&#27169;&#22411;&#22312;&#38271;&#23614;&#20107;&#20214;&#31867;&#22411;&#30340;&#29702;&#35299;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#65292;&#26088;&#22312;&#25552;&#21462;&#19981;&#26029;&#20986;&#29616;&#30340;&#20107;&#20214;&#20449;&#24687;&#21516;&#26102;&#36991;&#20813;&#36951;&#24536;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20107;&#20214;&#31867;&#22411;&#30340;&#35821;&#20041;&#28151;&#28102;&#28304;&#20110;&#21516;&#19968;&#25991;&#26412;&#30340;&#27880;&#37322;&#38543;&#26102;&#38388;&#26356;&#26032;&#12290;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#29978;&#33267;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#35821;&#20041;&#28151;&#28102;&#20462;&#27491;&#30340;&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#27169;&#22411;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#21477;&#23376;&#26631;&#27880;&#20266;&#26631;&#31614;&#20197;&#32531;&#35299;&#35821;&#20041;&#28151;&#28102;&#12290;&#25105;&#20204;&#22312;&#24403;&#21069;&#27169;&#22411;&#21644;&#20043;&#21069;&#27169;&#22411;&#20043;&#38388;&#20256;&#36882;&#20851;&#38190;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#23545;&#20107;&#20214;&#31867;&#22411;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#30456;&#20851;&#31867;&#22411;&#26469;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#38271;&#23614;&#20107;&#20214;&#31867;&#22411;&#30340;&#35821;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study continual event extraction, which aims to extract incessantly emerging event information while avoiding forgetting. We observe that the semantic confusion on event types stems from the annotations of the same text being updated over time. The imbalance between event types even aggravates this issue. This paper proposes a novel continual event extraction model with semantic confusion rectification. We mark pseudo labels for each sentence to alleviate semantic confusion. We transfer pivotal knowledge between current and previous models to enhance the understanding of event types. Moreover, we encourage the model to focus on the semantics of long-tailed event types by leveraging other associated types. Experimental results show that our model outperforms state-of-the-art baselines and is proficient in imbalanced datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#38598;&#20013;&#21040;&#20998;&#25955;&#30340;&#26041;&#27861;&#22312;&#30005;&#21147;&#34892;&#19994;&#20013;&#30340;&#36716;&#21464;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#36171;&#33021;&#21644;&#30005;&#32593;&#31649;&#29702;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20135;&#29983;&#21644;&#28040;&#32791;&#26041;&#38754;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23558;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#21487;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#26356;&#22909;&#22320;&#21709;&#24212;&#38656;&#27714;&#21644;&#26356;&#22909;&#22320;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#35299;&#20915;&#22788;&#29702;&#22823;&#25968;&#25454;&#37327;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#19987;&#19994;&#30693;&#35782;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15468</link><description>&lt;p&gt;
&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#31995;&#32479;&#21644;&#30005;&#32593;&#20248;&#21270;&#20013;&#36171;&#33021;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Empowering Distributed Solutions in Renewable Energy Systems and Grid Optimization. (arXiv:2310.15468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#38598;&#20013;&#21040;&#20998;&#25955;&#30340;&#26041;&#27861;&#22312;&#30005;&#21147;&#34892;&#19994;&#20013;&#30340;&#36716;&#21464;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#36171;&#33021;&#21644;&#30005;&#32593;&#31649;&#29702;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20135;&#29983;&#21644;&#28040;&#32791;&#26041;&#38754;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23558;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#21487;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#26356;&#22909;&#22320;&#21709;&#24212;&#38656;&#27714;&#21644;&#26356;&#22909;&#22320;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#35299;&#20915;&#22788;&#29702;&#22823;&#25968;&#25454;&#37327;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#19987;&#19994;&#30693;&#35782;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30005;&#21147;&#34892;&#19994;&#20174;&#38598;&#20013;&#24335;&#21521;&#20998;&#25955;&#24335;&#26041;&#27861;&#30340;&#36716;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#36827;&#23637;&#22312;&#36171;&#33021;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#25913;&#21892;&#30005;&#32593;&#31649;&#29702;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;ML&#27169;&#22411;&#22312;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20135;&#29983;&#21644;&#28040;&#32791;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20351;&#29992;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#20915;&#31574;&#26641;&#31561;&#21508;&#31181;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#22914;&#25968;&#25454;&#20998;&#21106;&#12289;&#24402;&#19968;&#21270;&#12289;&#20998;&#35299;&#21644;&#31163;&#25955;&#21270;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23558;&#22823;&#25968;&#25454;&#21644;ML&#34701;&#20837;&#26234;&#33021;&#30005;&#32593;&#20855;&#26377;&#22810;&#31181;&#20248;&#21183;&#65292;&#21253;&#25324;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#65292;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#38656;&#27714;&#65292;&#26356;&#22909;&#22320;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#28982;&#32780;&#65292;&#36824;&#38656;&#35201;&#35299;&#20915;&#22788;&#29702;&#22823;&#25968;&#25454;&#37327;&#12289;&#30830;&#20445;&#32593;&#32476;&#23433;&#20840;&#21644;&#33719;&#21462;&#19987;&#19994;&#30693;&#35782;&#31561;&#25361;&#25112;&#12290;&#30740;&#31350;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study delves into the shift from centralized to decentralized approaches in the electricity industry, with a particular focus on how machine learning (ML) advancements play a crucial role in empowering renewable energy sources and improving grid management. ML models have become increasingly important in predicting renewable energy generation and consumption, utilizing various techniques like artificial neural networks, support vector machines, and decision trees. Furthermore, data preprocessing methods, such as data splitting, normalization, decomposition, and discretization, are employed to enhance prediction accuracy.  The incorporation of big data and ML into smart grids offers several advantages, including heightened energy efficiency, more effective responses to demand, and better integration of renewable energy sources. Nevertheless, challenges like handling large data volumes, ensuring cybersecurity, and obtaining specialized expertise must be addressed. The research inves
&lt;/p&gt;</description></item><item><title>EKGNet&#26159;&#19968;&#31181;&#20840;&#27169;&#25311;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#24515;&#30005;&#22270;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#12290;&#23427;&#21033;&#29992;&#27169;&#25311;&#35745;&#31639;&#21644;&#20122;&#38408;&#20540;&#26230;&#20307;&#31649;&#30340;&#33021;&#37327;&#25928;&#29575;&#65292;&#28040;&#38500;&#20102;&#23545;&#27169;&#25311;&#21040;&#25968;&#23383;&#36716;&#25442;&#22120;&#21644;&#38745;&#24577;&#38543;&#26426;&#23384;&#20648;&#22120;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EKGNet&#22312;&#20869;&#24739;&#32773;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#21644;&#24515;&#32908;&#26775;&#27515;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15466</link><description>&lt;p&gt;
EKGNet&#65306;&#19968;&#31181;&#29992;&#20110;&#24739;&#32773;&#20869;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;10.96&#956;W&#20840;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EKGNet: A 10.96{\mu}W Fully Analog Neural Network for Intra-Patient Arrhythmia Classification. (arXiv:2310.15466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15466
&lt;/p&gt;
&lt;p&gt;
EKGNet&#26159;&#19968;&#31181;&#20840;&#27169;&#25311;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#24515;&#30005;&#22270;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#12290;&#23427;&#21033;&#29992;&#27169;&#25311;&#35745;&#31639;&#21644;&#20122;&#38408;&#20540;&#26230;&#20307;&#31649;&#30340;&#33021;&#37327;&#25928;&#29575;&#65292;&#28040;&#38500;&#20102;&#23545;&#27169;&#25311;&#21040;&#25968;&#23383;&#36716;&#25442;&#22120;&#21644;&#38745;&#24577;&#38543;&#26426;&#23384;&#20648;&#22120;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EKGNet&#22312;&#20869;&#24739;&#32773;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#21644;&#24515;&#32908;&#26775;&#27515;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27169;&#25311;&#35745;&#31639;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#24515;&#30005;&#22270;(ECG)&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EKGNet&#65292;&#36825;&#26159;&#19968;&#31181;&#30828;&#20214;&#39640;&#25928;&#19988;&#23436;&#20840;&#27169;&#25311;&#30340;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#20302;&#21151;&#32791;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#20102;&#20122;&#38408;&#20540;&#21306;&#22495;&#20013;&#30340;&#26230;&#20307;&#31649;&#30340;&#33021;&#37327;&#25928;&#29575;&#65292;&#28040;&#38500;&#20102;&#27169;&#25311;&#21040;&#25968;&#23383;&#36716;&#25442;&#22120;(ADC)&#21644;&#38745;&#24577;&#38543;&#26426;&#23384;&#20648;&#22120;(SRAM)&#30340;&#38656;&#27714;&#12290;&#31995;&#32479;&#35774;&#35745;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#25311;&#24207;&#21015;&#20056;&#31215;&#32047;&#21152;(MAC)&#30005;&#36335;&#65292;&#21487;&#20197;&#32531;&#35299;&#24037;&#33402;&#12289;&#20379;&#30005;&#30005;&#21387;&#21644;&#28201;&#24230;&#21464;&#21270;&#12290;&#23545;PhysioNet&#30340;MIT-BIH&#21644;PTB&#35786;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;95%&#21644;94.25%&#30340;&#24179;&#22343;&#22343;&#34913;&#20934;&#30830;&#24615;&#65292;&#29992;&#20110;&#24739;&#32773;&#20869;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#21644;&#24515;&#32908;&#26775;&#27515;(MI)&#20998;&#31867;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an integrated approach by combining analog computing and deep learning for electrocardiogram (ECG) arrhythmia classification. We propose EKGNet, a hardware-efficient and fully analog arrhythmia classification architecture that archives high accuracy with low power consumption. The proposed architecture leverages the energy efficiency of transistors operating in the subthreshold region, eliminating the need for analog-to-digital converters (ADC) and static random access memory (SRAM). The system design includes a novel analog sequential Multiply-Accumulate (MAC) circuit that mitigates process, supply voltage, and temperature variations. Experimental evaluations on PhysioNet's MIT-BIH and PTB Diagnostics datasets demonstrate the effectiveness of the proposed method, achieving average balanced accuracy of 95% and 94.25% for intra-patient arrhythmia classification and myocardial infarction (MI) classification, respectively. This innovative approach presents a promising avenue fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20855;&#26377;&#31169;&#26377;&#21644;&#20844;&#20849;&#29305;&#24449;&#30340;&#20010;&#20154;&#21270;&#23398;&#20064;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21482;&#20445;&#25252;&#29305;&#23450;&#30340;&#32479;&#35745;&#37327;&#26469;&#25552;&#39640;&#23454;&#29992;&#24615;&#65292;&#22312;&#20004;&#20010;&#26631;&#20934;&#31169;&#26377;&#25512;&#33616;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15454</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#29305;&#24449;&#30340;&#20010;&#20154;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Private Learning with Public Features. (arXiv:2310.15454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20855;&#26377;&#31169;&#26377;&#21644;&#20844;&#20849;&#29305;&#24449;&#30340;&#20010;&#20154;&#21270;&#23398;&#20064;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21482;&#20445;&#25252;&#29305;&#23450;&#30340;&#32479;&#35745;&#37327;&#26469;&#25552;&#39640;&#23454;&#29992;&#24615;&#65292;&#22312;&#20004;&#20010;&#26631;&#20934;&#31169;&#26377;&#25512;&#33616;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#31169;&#26377;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#26159;&#31169;&#26377;&#29305;&#24449;&#21644;&#20844;&#20849;&#29305;&#24449;&#30340;&#32852;&#32467;&#12290;&#36825;&#22312;&#31169;&#20154;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#20363;&#22914;&#25512;&#33616;&#25110;&#24191;&#21578;&#39044;&#27979;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#19982;&#20010;&#20154;&#30456;&#20851;&#30340;&#29305;&#24449;&#26159;&#25935;&#24863;&#30340;&#65292;&#32780;&#19982;&#29289;&#21697;&#30456;&#20851;&#30340;&#29305;&#24449;&#65288;&#22914;&#25512;&#33616;&#30340;&#30005;&#24433;&#25110;&#27468;&#26354;&#65292;&#25110;&#32773;&#21521;&#29992;&#25143;&#23637;&#31034;&#30340;&#24191;&#21578;&#65289;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#24182;&#19988;&#19981;&#38656;&#35201;&#20445;&#25252;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#22312;&#20844;&#20849;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#31169;&#26377;&#31639;&#27861;&#26159;&#21542;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#23545;&#22810;&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#20010;&#32534;&#30721;&#22120;&#22788;&#29702;&#20844;&#20849;&#29305;&#24449;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#20998;&#31163;&#21482;&#20445;&#25252;&#26576;&#20123;&#20805;&#20998;&#32479;&#35745;&#37327;&#65288;&#32780;&#19981;&#26159;&#21521;&#26799;&#24230;&#28155;&#21152;&#22122;&#38899;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#20445;&#35777;&#20102;&#23454;&#29992;&#24615;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#26631;&#20934;&#31169;&#26377;&#25512;&#33616;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a class of private learning problems in which the data is a join of private and public features. This is often the case in private personalization tasks such as recommendation or ad prediction, in which features related to individuals are sensitive, while features related to items (the movies or songs to be recommended, or the ads to be shown to users) are publicly available and do not require protection. A natural question is whether private algorithms can achieve higher utility in the presence of public features. We give a positive answer for multi-encoder models where one of the encoders operates on public features. We develop new algorithms that take advantage of this separation by only protecting certain sufficient statistics (instead of adding noise to the gradient). This method has a guaranteed utility improvement for linear regression, and importantly, achieves the state of the art on two standard private recommendation benchmarks, demonstrating the importance of metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#36890;&#29992;&#38750;&#21442;&#25968;&#22240;&#26524;&#28508;&#21464;&#37327;&#27169;&#22411;&#21644;&#36890;&#29992;&#36716;&#25442;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#38750;&#32806;&#21512;&#24178;&#39044;&#24314;&#31435;&#20102;&#22240;&#26524;&#34920;&#36798;&#23398;&#20064;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#12290;&#22312;&#19981;&#30693;&#36947;&#20855;&#20307;&#24178;&#39044;&#23545;&#24212;&#30340;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20445;&#35777;&#20102;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#21464;&#37327;&#30340;&#23436;&#32654;&#24674;&#22797;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.15450</link><description>&lt;p&gt;
&#36890;&#29992;&#22240;&#26524;&#34920;&#36798;&#23398;&#20064;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#23454;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
General Identifiability and Achievability for Causal Representation Learning. (arXiv:2310.15450v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#36890;&#29992;&#38750;&#21442;&#25968;&#22240;&#26524;&#28508;&#21464;&#37327;&#27169;&#22411;&#21644;&#36890;&#29992;&#36716;&#25442;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#38750;&#32806;&#21512;&#24178;&#39044;&#24314;&#31435;&#20102;&#22240;&#26524;&#34920;&#36798;&#23398;&#20064;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#12290;&#22312;&#19981;&#30693;&#36947;&#20855;&#20307;&#24178;&#39044;&#23545;&#24212;&#30340;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20445;&#35777;&#20102;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#21464;&#37327;&#30340;&#23436;&#32654;&#24674;&#22797;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#36890;&#29992;&#38750;&#21442;&#25968;&#22240;&#26524;&#28508;&#21464;&#37327;&#27169;&#22411;&#21644;&#23558;&#28508;&#21464;&#37327;&#25968;&#25454;&#26144;&#23556;&#21040;&#35266;&#27979;&#25968;&#25454;&#30340;&#36890;&#29992;&#36716;&#25442;&#27169;&#22411;&#19979;&#30340;&#22240;&#26524;&#34920;&#36798;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;&#28508;&#22312;&#22240;&#26524;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#36827;&#34892;&#20004;&#20010;&#30828;&#24615;&#38750;&#32806;&#21512;&#24178;&#39044;&#26469;&#24314;&#31435;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20154;&#20204;&#19981;&#30693;&#36947;&#21738;&#20010;&#24178;&#39044;&#29615;&#22659;&#23545;&#24212;&#30340;&#33410;&#28857;&#26159;&#30456;&#21516;&#30340;&#65288;&#22240;&#27492;&#26159;&#38750;&#32806;&#21512;&#29615;&#22659;&#65289;&#12290;&#22312;&#21487;&#35782;&#21035;&#24615;&#26041;&#38754;&#65292;&#26412;&#25991;&#30830;&#20445;&#22312;&#38750;&#32806;&#21512;&#24178;&#39044;&#19979;&#33021;&#22815;&#23436;&#32654;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#21464;&#37327;&#12290;&#22312;&#21487;&#23454;&#29616;&#24615;&#26041;&#38754;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21033;&#29992;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35813;&#31639;&#27861;&#30340;&#21487;&#39564;&#35777;&#30340;&#20445;&#35777;&#65292;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#21464;&#37327;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#24471;&#20998;&#21464;&#21270;&#26469;&#20272;&#35745;&#36716;&#25442;&#22120;&#30340;&#36870;&#21644;&#38543;&#21518;&#30340;&#28508;&#21464;&#37327;&#12290;&#35813;&#20998;&#26512;&#36824;...
&lt;/p&gt;
&lt;p&gt;
This paper focuses on causal representation learning (CRL) under a general nonparametric causal latent model and a general transformation model that maps the latent data to the observational data. It establishes \textbf{identifiability} and \textbf{achievability} results using two hard \textbf{uncoupled} interventions per node in the latent causal graph. Notably, one does not know which pair of intervention environments have the same node intervened (hence, uncoupled environments). For identifiability, the paper establishes that perfect recovery of the latent causal model and variables is guaranteed under uncoupled interventions. For achievability, an algorithm is designed that uses observational and interventional data and recovers the latent causal model and variables with provable guarantees for the algorithm. This algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables. The analysis, addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#26368;&#20339;&#24050;&#30693;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.15448</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#36895;&#30340;&#19968;&#38454;&#27491;&#21017;&#21160;&#37327;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An accelerated first-order regularized momentum descent ascent algorithm for stochastic nonconvex-concave minimax problems. (arXiv:2310.15448v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#26368;&#20339;&#24050;&#30693;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#19968;&#38454;&#27491;&#21017;&#21160;&#37327;&#19979;&#38477;&#31639;&#27861;&#65288;FORMDA&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$&#20197;&#36798;&#21040;$\varepsilon$-&#31283;&#23450;&#28857;&#65292;&#36825;&#22312;&#30446;&#26631;&#20989;&#25968;&#31283;&#23450;&#24615;&#19979;&#23454;&#29616;&#20102;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26368;&#20339;&#24050;&#30693;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose an accelerated first-order regularized momentum descent ascent algorithm (FORMDA) for solving stochastic nonconvex-concave minimax problems. The iteration complexity of the algorithm is proved to be $\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$ to obtain an $\varepsilon$-stationary point, which achieves the best-known complexity bound for single-loop algorithms to solve the stochastic nonconvex-concave minimax problems under the stationarity of the objective function.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#32447;&#24615;VAE&#20013;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#36755;&#20837;&#32500;&#24230;&#30340;&#38480;&#21046;&#19979;&#65292;&#23398;&#20064;&#21160;&#24577;&#25910;&#25947;&#20026;&#30830;&#23450;&#24615;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#21518;&#39564;&#23849;&#22604;&#38408;&#20540;&#21644;KL&#36864;&#28779;&#21152;&#36895;&#31574;&#30053;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;VAE&#26368;&#21021;&#23398;&#20064;&#21040;&#32416;&#32544;&#34920;&#31034;&#65292;&#24182;&#36880;&#28176;&#33719;&#24471;&#19981;&#32416;&#32544;&#30340;&#34920;&#31034;&#12290;&#22312;&#30830;&#23450;&#24615;&#36807;&#31243;&#20013;&#65292;&#36229;fluous&#28508;&#22312;&#31354;&#38388;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15440</link><description>&lt;p&gt;
Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing. (arXiv:2310.15440v1 [stat.ML])&#30340;&#23398;&#20064;&#21160;&#24577;&#65306;&#21518;&#39564;&#23849;&#22604;&#38408;&#20540;&#65292;&#22810;&#20313;&#30340;&#28508;&#22312;&#31354;&#38388;&#38519;&#38449;&#20197;&#21450;KL&#36864;&#28779;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing. (arXiv:2310.15440v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#32447;&#24615;VAE&#20013;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#36755;&#20837;&#32500;&#24230;&#30340;&#38480;&#21046;&#19979;&#65292;&#23398;&#20064;&#21160;&#24577;&#25910;&#25947;&#20026;&#30830;&#23450;&#24615;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#21518;&#39564;&#23849;&#22604;&#38408;&#20540;&#21644;KL&#36864;&#28779;&#21152;&#36895;&#31574;&#30053;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;VAE&#26368;&#21021;&#23398;&#20064;&#21040;&#32416;&#32544;&#34920;&#31034;&#65292;&#24182;&#36880;&#28176;&#33719;&#24471;&#19981;&#32416;&#32544;&#30340;&#34920;&#31034;&#12290;&#22312;&#30830;&#23450;&#24615;&#36807;&#31243;&#20013;&#65292;&#36229;fluous&#28508;&#22312;&#31354;&#38388;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#23384;&#22312;&#19968;&#20010;&#33261;&#21517;&#26157;&#33879;&#30340;&#38382;&#39064;&#65292;&#21363;&#21464;&#20998;&#21518;&#39564;&#36890;&#24120;&#19982;&#20808;&#39564;&#38750;&#24120;&#25509;&#36817;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#21518;&#39564;&#23849;&#22604;&#65292;&#23427;&#38459;&#30861;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35843;&#30340;&#36229;&#21442;&#25968;&#946;&#20197;&#21450;&#19968;&#31181;&#31216;&#20026;KL&#36864;&#28779;&#30340;&#31574;&#30053;&#26469;&#35843;&#25972;&#35813;&#21442;&#25968;&#12290;&#26412;&#30740;&#31350;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;VAE&#20013;&#23545;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#32463;&#36807;&#20005;&#26684;&#35777;&#26126;&#65292;&#22312;&#36755;&#20837;&#32500;&#24230;&#36235;&#20110;&#26080;&#31351;&#22823;&#30340;&#26497;&#38480;&#19979;&#65292;&#21160;&#24577;&#25910;&#25947;&#20026;&#30830;&#23450;&#24615;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#35814;&#32454;&#21160;&#24577;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;VAE&#26368;&#21021;&#23398;&#20064;&#21040;&#32416;&#32544;&#34920;&#31034;&#65292;&#24182;&#36880;&#28176;&#33719;&#24471;&#19981;&#32416;&#32544;&#30340;&#34920;&#31034;&#12290;&#23545;&#30830;&#23450;&#24615;&#36807;&#31243;&#30340;&#22266;&#23450;&#28857;&#20998;&#26512;&#25581;&#31034;&#65292;&#24403;&#946;&#36229;&#36807;&#19968;&#23450;&#38408;&#20540;&#26102;&#65292;&#26080;&#35770;&#23398;&#20064;&#21608;&#26399;&#22810;&#38271;&#65292;&#21518;&#39564;&#23849;&#22604;&#37117;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36229;fluous&#28508;&#22312;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) face a notorious problem wherein the variational posterior often aligns closely with the prior, a phenomenon known as posterior collapse, which hinders the quality of representation learning. To mitigate this problem, an adjustable hyperparameter $\beta$ and a strategy for annealing this parameter, called KL annealing, are proposed. This study presents a theoretical analysis of the learning dynamics in a minimal VAE. It is rigorously proved that the dynamics converge to a deterministic process within the limit of large input dimensions, thereby enabling a detailed dynamical analysis of the generalization error. Furthermore, the analysis shows that the VAE initially learns entangled representations and gradually acquires disentangled representations. A fixed-point analysis of the deterministic process reveals that when $\beta$ exceeds a certain threshold, posterior collapse becomes inevitable regardless of the learning period. Additionally, the superfluou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31574;&#30053;&#21367;&#31215;&#65288;PC&#65289;&#30340;&#31163;&#31574;&#30053;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#20316;&#23884;&#20837;&#26469;&#35299;&#20915;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#20559;&#24046;&#21644;&#26041;&#24046;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;</title><link>http://arxiv.org/abs/2310.15433</link><description>&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#21367;&#31215;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#31163;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation for Large Action Spaces via Policy Convolution. (arXiv:2310.15433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31574;&#30053;&#21367;&#31215;&#65288;PC&#65289;&#30340;&#31163;&#31574;&#30053;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#20316;&#23884;&#20837;&#26469;&#35299;&#20915;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#20559;&#24046;&#21644;&#26041;&#24046;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#20934;&#30830;&#30340;&#31163;&#31574;&#30053;&#20272;&#35745;&#22120;&#23545;&#20110;&#35780;&#20272;&#21644;&#20248;&#21270;&#26032;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#31163;&#31574;&#30053;&#20272;&#35745;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#30340;&#35760;&#24405;&#31574;&#30053;&#21644;&#25105;&#20204;&#35201;&#35780;&#20272;&#30340;&#30446;&#26631;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#36890;&#24120;&#65292;&#32416;&#27491;&#20998;&#24067;&#36716;&#31227;&#30340;&#25216;&#26415;&#28041;&#21450;&#26576;&#31181;&#24418;&#24335;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#26080;&#20559;&#20540;&#20272;&#35745;&#65292;&#20294;&#24448;&#24448;&#20250;&#24102;&#26469;&#39640;&#26041;&#24046;&#30340;&#20195;&#20215;&#65292;&#21363;&#20351;&#22312;&#31616;&#21333;&#30340;&#19968;&#27493;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#37325;&#35201;&#24615;&#37319;&#26679;&#20381;&#36182;&#20110;&#20849;&#21516;&#25903;&#25345;&#20551;&#35774;&#65292;&#22312;&#21160;&#20316;&#31354;&#38388;&#24456;&#22823;&#26102;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31574;&#30053;&#21367;&#31215; (PC)&#23478;&#26063;&#30340;&#20272;&#35745;&#22120;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#36890;&#36807;&#21160;&#20316;&#23884;&#20837;&#25552;&#20379;&#30340;&#21160;&#20316;&#20869;&#37096;&#32467;&#26500;&#36827;&#34892;&#31574;&#30053;&#30340;&#31574;&#30053;&#21367;&#31215;&#12290;&#36825;&#31181;&#21367;&#31215;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#21487;&#20197;&#36827;&#34892;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Developing accurate off-policy estimators is crucial for both evaluating and optimizing for new policies. The main challenge in off-policy estimation is the distribution shift between the logging policy that generates data and the target policy that we aim to evaluate. Typically, techniques for correcting distribution shift involve some form of importance sampling. This approach results in unbiased value estimation but often comes with the trade-off of high variance, even in the simpler case of one-step contextual bandits. Furthermore, importance sampling relies on the common support assumption, which becomes impractical when the action space is large. To address these challenges, we introduce the Policy Convolution (PC) family of estimators. These methods leverage latent structure within actions -- made available through action embeddings -- to strategically convolve the logging and target policies. This convolution introduces a unique bias-variance trade-off, which can be controlled 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#25554;&#20540;&#30340;&#26032;&#22411;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21517;&#20026;Mason-Alberta&#38899;&#26631;&#20998;&#21106;&#22120;&#12290;&#35813;&#31995;&#32479;&#30340;&#21019;&#26032;&#28857;&#21253;&#25324;&#23558;&#22768;&#23398;&#27169;&#22411;&#35270;&#20026;&#26631;&#27880;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#20102;&#25554;&#20540;&#25216;&#26415;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20998;&#27573;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.15425</link><description>&lt;p&gt;
Mason-Alberta&#38899;&#26631;&#20998;&#21106;&#22120;: &#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#25554;&#20540;&#30340;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation. (arXiv:2310.15425v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#25554;&#20540;&#30340;&#26032;&#22411;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21517;&#20026;Mason-Alberta&#38899;&#26631;&#20998;&#21106;&#22120;&#12290;&#35813;&#31995;&#32479;&#30340;&#21019;&#26032;&#28857;&#21253;&#25324;&#23558;&#22768;&#23398;&#27169;&#22411;&#35270;&#20026;&#26631;&#27880;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#20102;&#25554;&#20540;&#25216;&#26415;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20998;&#27573;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#22312;&#32473;&#23450;&#27491;&#23383;&#27861;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#33258;&#21160;&#30830;&#23450;&#20998;&#27573;&#36793;&#30028;&#12290;&#36825;&#20123;&#24037;&#20855;&#22312;&#35821;&#38899;&#23398;&#20013;&#24456;&#24120;&#35265;&#65292;&#20197;&#20415;&#20351;&#29992;&#37027;&#20123;&#25163;&#21160;&#36716;&#24405;&#21644;&#20998;&#27573;&#38590;&#20197;&#23454;&#29616;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#65292;&#21363;Mason-Alberta&#38899;&#26631;&#20998;&#21106;&#22120;&#65288;MAPS&#65289;&#12290;MAPS&#23545;&#40784;&#22120;&#20316;&#20026;&#25105;&#20204;&#36861;&#27714;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#20004;&#20010;&#28508;&#22312;&#25913;&#36827;&#30340;&#35797;&#39564;&#24179;&#21488;&#12290;&#31532;&#19968;&#20010;&#26159;&#23558;&#24378;&#21046;&#23545;&#40784;&#22120;&#20013;&#30340;&#22768;&#23398;&#27169;&#22411;&#35270;&#20026;&#26631;&#27880;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#36825;&#26159;&#22522;&#20110;&#20154;&#20204;&#23545;&#35821;&#38899;&#20013;&#30340;&#27573;&#33853;&#24182;&#19981;&#26159;&#30495;&#27491;&#31163;&#25955;&#21644;&#24120;&#24120;&#37325;&#21472;&#30340;&#20849;&#21516;&#35748;&#35782;&#12290;&#31532;&#20108;&#20010;&#26159;&#25554;&#20540;&#25216;&#26415;&#65292;&#20351;&#24471;&#36793;&#30028;&#21487;&#20197;&#27604;&#29616;&#20195;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#24120;&#35265;&#30340;10&#27627;&#31186;&#38480;&#21046;&#26356;&#31934;&#30830;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#31995;&#32479;&#30340;&#37197;&#32622;&#19982;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;Montreal Forced Aligner&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forced alignment systems automatically determine boundaries between segments in speech data, given an orthographic transcription. These tools are commonplace in phonetics to facilitate the use of speech data that would be infeasible to manually transcribe and segment. In the present paper, we describe a new neural network-based forced alignment system, the Mason-Alberta Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two possible improvements we pursue for forced alignment systems. The first is treating the acoustic model in a forced aligner as a tagging task, rather than a classification task, motivated by the common understanding that segments in speech are not truly discrete and commonly overlap. The second is an interpolation technique to allow boundaries more precise than the common 10 ms limit in modern forced alignment systems. We compare configurations of our system to a state-of-the-art system, the Montreal Forced Aligner. The tagging approach did not gener
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25919;&#31574;&#20248;&#21270;&#36807;&#31243;&#20013;&#38750;&#24179;&#28369;&#25110;&#20998;&#24418;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22266;&#26377;&#38480;&#21046;&#30340;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#36935;&#21040;&#20998;&#24418;&#26223;&#35266;&#12290;</title><link>http://arxiv.org/abs/2310.15418</link><description>&lt;p&gt;
&#25919;&#31574;&#20248;&#21270;&#20013;&#30340;&#20998;&#24418;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
Fractal Landscapes in Policy Optimization. (arXiv:2310.15418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15418
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25919;&#31574;&#20248;&#21270;&#36807;&#31243;&#20013;&#38750;&#24179;&#28369;&#25110;&#20998;&#24418;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22266;&#26377;&#38480;&#21046;&#30340;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#36935;&#21040;&#20998;&#24418;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#26159;&#36830;&#32493;&#39046;&#22495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#23454;&#36341;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;&#36827;&#34892;RL&#35757;&#32451;&#21487;&#33021;&#22240;&#20026;&#22810;&#31181;&#21407;&#22240;&#32780;&#22833;&#36133;&#65292;&#29978;&#33267;&#22312;&#24050;&#30693;&#35299;&#30340;&#26631;&#20934;&#25511;&#21046;&#38382;&#39064;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29702;&#35299;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#19968;&#20010;&#22266;&#26377;&#38480;&#21046;&#65306;&#23545;&#20110;&#26576;&#20123;&#31867;&#21035;&#30340;MDPs&#65292;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#26223;&#35266;&#21487;&#20197;&#38750;&#24120;&#38750;&#24179;&#28369;&#25110;&#20998;&#24418;&#65292;&#20197;&#33267;&#20110;&#26681;&#26412;&#19981;&#23384;&#22312;&#38656;&#35201;&#20272;&#35745;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#20511;&#37492;&#28151;&#27788;&#29702;&#35770;&#21644;&#38750;&#24179;&#28369;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#31574;&#30053;&#20248;&#21270;&#30446;&#26631;&#30340;&#26368;&#22823;Lyapunov&#25351;&#25968;&#21644;H&#246;lder&#25351;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26679;&#26412;&#20013;&#20272;&#35745;&#30446;&#26631;&#20989;&#25968;&#30340;&#23616;&#37096;&#24179;&#28369;&#24615;&#65292;&#20197;&#20415;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#26159;&#21542;&#36935;&#21040;&#20998;&#24418;&#26223;&#35266;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#39564;&#26469;&#35828;&#26126;&#19968;&#20123;&#31574;&#30053;&#20248;&#21270;&#22833;&#36133;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient lies at the core of deep reinforcement learning (RL) in continuous domains. Despite much success, it is often observed in practice that RL training with policy gradient can fail for many reasons, even on standard control problems with known solutions. We propose a framework for understanding one inherent limitation of the policy gradient approach: the optimization landscape in the policy space can be extremely non-smooth or fractal for certain classes of MDPs, such that there does not exist gradient to be estimated in the first place. We draw on techniques from chaos theory and non-smooth analysis, and analyze the maximal Lyapunov exponents and H\"older exponents of the policy optimization objectives. Moreover, we develop a practical method that can estimate the local smoothness of objective function from samples to identify when the training process has encountered fractal landscapes. We show experiments to illustrate how some failure cases of policy optimization can b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#28857;/&#39034;&#24207;&#37325;&#26500;&#27169;&#22411;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#37325;&#26500;&#35823;&#24046;&#30340;&#32452;&#21512;&#20540;&#20043;&#27604;&#24471;&#21040;&#21629;&#21517;&#20998;&#25968;, &#36827;&#19968;&#27493;&#32467;&#21512;&#21629;&#21517;&#20998;&#25968;&#21644;&#24322;&#24120;&#20998;&#25968;&#23548;&#20986;&#24863;&#24212;&#24322;&#24120;&#20998;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#28857;&#24322;&#24120;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#30340;&#37327;&#21270;&#21644;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.15416</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;/&#39034;&#24207;&#37325;&#26500;&#30340;&#21629;&#21517;&#20998;&#25968;&#26465;&#20214;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Nominality Score Conditioned Time Series Anomaly Detection by Point/Sequential Reconstruction. (arXiv:2310.15416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#28857;/&#39034;&#24207;&#37325;&#26500;&#27169;&#22411;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#37325;&#26500;&#35823;&#24046;&#30340;&#32452;&#21512;&#20540;&#20043;&#27604;&#24471;&#21040;&#21629;&#21517;&#20998;&#25968;, &#36827;&#19968;&#27493;&#32467;&#21512;&#21629;&#21517;&#20998;&#25968;&#21644;&#24322;&#24120;&#20998;&#25968;&#23548;&#20986;&#24863;&#24212;&#24322;&#24120;&#20998;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#28857;&#24322;&#24120;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#30340;&#37327;&#21270;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#27169;&#24335;&#30340;&#23384;&#22312;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#22256;&#38590;&#22312;&#20110;&#24314;&#27169;&#26102;&#38388;&#30456;&#20851;&#30340;&#20851;&#31995;&#20197;&#23547;&#25214;&#19978;&#19979;&#25991;&#24322;&#24120;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#28857;&#24322;&#24120;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#28857;&#21644;&#24207;&#21015;&#30340;&#37325;&#26500;&#27169;&#22411;&#12290;&#22522;&#20110;&#28857;&#30340;&#27169;&#22411;&#35797;&#22270;&#37327;&#21270;&#28857;&#24322;&#24120;&#65292;&#32780;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#35797;&#22270;&#37327;&#21270;&#28857;&#24322;&#24120;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#12290;&#22312;&#20551;&#35774;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#28857;&#26159;&#20174;&#19968;&#31181;&#26631;&#20934;&#26102;&#38388;&#28857;&#24320;&#22987;&#30340;&#20004;&#20010;&#38454;&#27573;&#30340;&#20559;&#31163;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;&#37325;&#26500;&#35823;&#24046;&#30340;&#32452;&#21512;&#20540;&#20043;&#27604;&#35745;&#31639;&#24471;&#20986;&#30340;&#21629;&#21517;&#20998;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#19968;&#27493;&#25972;&#21512;&#21629;&#21517;&#20998;&#25968;&#21644;&#24322;&#24120;&#20998;&#25968;&#26469;&#23548;&#20986;&#24863;&#24212;&#24322;&#24120;&#20998;&#25968;&#65292;&#28982;&#21518;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#24863;&#24212;&#24322;&#24120;&#20998;&#25968;&#20248;&#20110;&#21407;&#22987;&#24322;&#24120;&#20998;&#25968;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is challenging due to the complexity and variety of patterns that can occur. One major difficulty arises from modeling time-dependent relationships to find contextual anomalies while maintaining detection accuracy for point anomalies. In this paper, we propose a framework for unsupervised time series anomaly detection that utilizes point-based and sequence-based reconstruction models. The point-based model attempts to quantify point anomalies, and the sequence-based model attempts to quantify both point and contextual anomalies. Under the formulation that the observed time point is a two-stage deviated value from a nominal time point, we introduce a nominality score calculated from the ratio of a combined value of the reconstruction errors. We derive an induced anomaly score by further integrating the nominality score and anomaly score, then theoretically prove the superiority of the induced anomaly score over the original anomaly score under certain condi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#22823;&#21270;&#33258;&#25105;&#23545;&#24328;&#30340;&#22870;&#21169;&#24182;&#26368;&#23567;&#21270;&#19982;&#20808;&#21069;&#21457;&#29616;&#30340;&#32422;&#23450;&#20132;&#20114;&#26102;&#30340;&#22870;&#21169;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#32422;&#23450;&#65292;&#30830;&#20445;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#20132;&#21449;&#23545;&#24328;&#30340;&#23545;&#25239;&#24615;&#20248;&#21270;&#36807;&#31243;&#20013;&#36981;&#23432;&#21892;&#24847;&#34892;&#20107;</title><link>http://arxiv.org/abs/2310.15414</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#22810;&#26679;&#21270;&#32422;&#23450;
&lt;/p&gt;
&lt;p&gt;
Diverse Conventions for Human-AI Collaboration. (arXiv:2310.15414v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#22823;&#21270;&#33258;&#25105;&#23545;&#24328;&#30340;&#22870;&#21169;&#24182;&#26368;&#23567;&#21270;&#19982;&#20808;&#21069;&#21457;&#29616;&#30340;&#32422;&#23450;&#20132;&#20114;&#26102;&#30340;&#22870;&#21169;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#32422;&#23450;&#65292;&#30830;&#20445;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#20132;&#21449;&#23545;&#24328;&#30340;&#23545;&#25239;&#24615;&#20248;&#21270;&#36807;&#31243;&#20013;&#36981;&#23432;&#21892;&#24847;&#34892;&#20107;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#20307;&#28216;&#25103;&#20013;&#65292;&#32422;&#23450;&#23545;&#20110;&#24378;&#22823;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#29609;&#23478;&#22312;&#27809;&#26377;&#26126;&#30830;&#20132;&#27969;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20849;&#21516;&#25112;&#30053;&#30340;&#21327;&#35843;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#33258;&#25105;&#23545;&#24328;&#65292;&#20250;&#25910;&#25947;&#21040;&#20219;&#24847;&#21644;&#38750;&#22810;&#26679;&#21270;&#30340;&#32422;&#23450;&#65292;&#23548;&#33268;&#22312;&#19982;&#26032;&#30340;&#21512;&#20316;&#20249;&#20276;&#20114;&#21160;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#33258;&#25105;&#23545;&#24328;&#36807;&#31243;&#20013;&#26368;&#22823;&#21270;&#20854;&#22870;&#21169;&#65292;&#24182;&#22312;&#19982;&#20808;&#21069;&#21457;&#29616;&#30340;&#32422;&#23450;&#36827;&#34892;&#20132;&#20114;&#26102;&#26368;&#23567;&#21270;&#20854;&#22870;&#21169;&#65288;&#20132;&#21449;&#23545;&#24328;&#65289;&#65292;&#20197;&#21050;&#28608;&#32422;&#23450;&#22312;&#35821;&#20041;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#32422;&#23450;&#12290;&#20026;&#20102;&#30830;&#20445;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#20132;&#21449;&#23545;&#24328;&#30340;&#23545;&#25239;&#24615;&#20248;&#21270;&#36807;&#31243;&#20013;&#22987;&#32456;&#36981;&#23432;&#21892;&#24847;&#34892;&#20107;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#21512;&#23545;&#24328;&#65288;mixed-play&#65289;&#30340;&#27010;&#24565;&#65292;&#21363;&#36890;&#36807;&#20174;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#30340;&#36716;&#25442;&#20013;&#38543;&#26426;&#29983;&#25104;&#21021;&#22987;&#29366;&#24577;&#65292;&#24182;&#23398;&#20064;&#22312;&#27492;&#21021;&#22987;&#29366;&#24577;&#19979;&#26368;&#22823;&#21270;&#33258;&#25105;&#23545;&#24328;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce \emph{mixed-play}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#23545;&#20110;&#20855;&#26377;Tsybakov&#22122;&#22768;&#30340;$d$&#32500;&#21322;&#31354;&#38388;&#65292;&#35745;&#31639;&#21644;&#26631;&#31614;&#30340;&#39640;&#25928;PAC&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#19968;&#23450;&#30340;&#22122;&#22768;&#21442;&#25968;&#33539;&#22260;&#20869;&#36798;&#21040;&#36739;&#20302;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.15411</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#24102;&#26377;Tsybakov&#22122;&#22768;&#30340;&#21322;&#31354;&#38388;&#20027;&#21160;&#23398;&#20064;&#65306;&#19968;&#31181;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Active Learning Halfspaces with Tsybakov Noise: A Non-convex Optimization Approach. (arXiv:2310.15411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15411
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#23545;&#20110;&#20855;&#26377;Tsybakov&#22122;&#22768;&#30340;$d$&#32500;&#21322;&#31354;&#38388;&#65292;&#35745;&#31639;&#21644;&#26631;&#31614;&#30340;&#39640;&#25928;PAC&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#19968;&#23450;&#30340;&#22122;&#22768;&#21442;&#25968;&#33539;&#22260;&#20869;&#36798;&#21040;&#36739;&#20302;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#23545;&#20110;&#20855;&#26377;Tsybakov&#22122;&#22768;&#30340;$d$&#32500;&#21322;&#31354;&#38388;&#65292;&#35745;&#31639;&#21644;&#26631;&#31614;&#30340;&#39640;&#25928;PAC&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#21463;&#21040;\cite{diakonikolas2020learning}&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24179;&#28369;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#20219;&#20309;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#37117;&#20250;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#20302;&#36807;&#37327;&#35823;&#24046;&#20445;&#35777;&#30340;&#21322;&#31354;&#38388;&#12290;&#26681;&#25454;&#19978;&#36848;&#32467;&#26500;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#20854;&#26631;&#31614;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d (\frac{1}{\epsilon})^{\frac{8-6\alpha}{3\alpha-1}})$&#65292;&#22312;Tsybakov&#22122;&#22768;&#21442;&#25968;$\alpha \in (\frac13, 1]$&#30340;&#20551;&#35774;&#19979;&#65292;&#36825;&#32553;&#23567;&#20102;&#20808;&#21069;&#24050;&#30693;&#30340;&#39640;&#25928;&#34987;&#21160;&#25110;&#20027;&#21160;&#31639;&#27861;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of computationally and label efficient PAC active learning $d$-dimensional halfspaces with Tsybakov Noise~\citep{tsybakov2004optimal} under structured unlabeled data distributions. Inspired by~\cite{diakonikolas2020learning}, we prove that any approximate first-order stationary point of a smooth nonconvex loss function yields a halfspace with a low excess error guarantee. In light of the above structural result, we design a nonconvex optimization-based algorithm with a label complexity of $\tilde{O}(d (\frac{1}{\epsilon})^{\frac{8-6\alpha}{3\alpha-1}})$\footnote{In the main body of this work, we use $\tilde{O}(\cdot), \tilde{\Theta}(\cdot)$ to hide factors of the form $\polylog(d, \frac{1}{\epsilon}, \frac{1}{\delta})$}, under the assumption that the Tsybakov noise parameter $\alpha \in (\frac13, 1]$, which narrows down the gap between the label complexities of the previously known efficient passive or active algorithms~\citep{diakonikolas2020polynomial,zhang2021im
&lt;/p&gt;</description></item><item><title>DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15393</link><description>&lt;p&gt;
DoGE: &#20351;&#29992;&#27867;&#21270;&#20272;&#35745;&#36827;&#34892;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15393
&lt;/p&gt;
&lt;p&gt;
DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#35821;&#26009;&#24211;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#32452;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30001;&#21508;&#31181;&#26469;&#28304;&#39046;&#22495;&#65288;&#22914;CommonCrawl&#12289;Wikipedia&#12289;Github&#31561;&#65289;&#25353;&#29031;&#29305;&#23450;&#30340;&#37319;&#26679;&#27010;&#29575;&#65288;&#39046;&#22495;&#26435;&#37325;&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#20248;&#21270;&#39046;&#22495;&#26435;&#37325;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DOmain reweighting with Generalization Estimation&#65288;DoGE&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#26032;&#35843;&#25972;&#20102;&#27599;&#20010;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#65292;&#26681;&#25454;&#23427;&#23545;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#27867;&#21270;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#35757;&#32451;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#33719;&#21462;&#37325;&#26032;&#21152;&#26435;&#30340;&#39046;&#22495;&#26435;&#37325;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#36890;&#36807;&#38236;&#20687;&#19979;&#38477;&#27861;&#26356;&#26032;&#39046;&#22495;&#26435;&#37325;&#20197;&#26368;&#22823;&#21270;&#25972;&#20307;&#30340;&#27867;&#21270;&#22686;&#30410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#33719;&#24471;&#30340;&#39046;&#22495;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#23436;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20803;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#21457;&#29983;&#21644;&#23646;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#20010;&#33322;&#22825;&#22120;&#25910;&#38598;&#30340;&#21407;&#20301;&#21644;&#36965;&#24863;&#35266;&#27979;&#25968;&#25454;&#65292;&#33021;&#22815;&#19982;&#20135;&#29983;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#29289;&#29702;&#36807;&#31243;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.15390</link><description>&lt;p&gt;
MEMPSEP III. &#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20803;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22810;&#20803;&#38598;&#25104;&#26041;&#27861;&#39044;&#27979;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#21457;&#29983;&#21644;&#23646;&#24615;&#65288;arXiv:2310.15390v1 [astro-ph.SR]&#65289;
&lt;/p&gt;
&lt;p&gt;
MEMPSEP III. A machine learning-oriented multivariate data set for forecasting the Occurrence and Properties of Solar Energetic Particle Events using a Multivariate Ensemble Approach. (arXiv:2310.15390v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15390
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20803;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#21457;&#29983;&#21644;&#23646;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#20010;&#33322;&#22825;&#22120;&#25910;&#38598;&#30340;&#21407;&#20301;&#21644;&#36965;&#24863;&#35266;&#27979;&#25968;&#25454;&#65292;&#33021;&#22815;&#19982;&#20135;&#29983;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#29289;&#29702;&#36807;&#31243;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#20803;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#22810;&#20010;&#33322;&#22825;&#22120;&#25910;&#38598;&#21407;&#20301;&#21644;&#36965;&#24863;&#35266;&#27979;&#30340;&#22826;&#38451;&#22280;&#23618;&#27979;&#37327;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#19982;&#20135;&#29983;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#29289;&#29702;&#36807;&#31243;&#30456;&#20851;&#12290;&#21033;&#29992;&#22320;&#29699;&#21516;&#27493;&#29615;&#22659;&#21355;&#26143;&#65288;GOES&#65289;&#22826;&#38451;&#31995;&#27963;&#21160;&#20107;&#20214;&#21015;&#34920;&#20174;&#22826;&#38451;&#27963;&#21160;&#21608;&#26399;&#65288;SC&#65289;23&#21644;SC 24&#30340;&#19968;&#37096;&#20998;&#65288;1998-2013&#65289;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;252&#20010;&#20135;&#29983;&#22826;&#38451;&#33021;&#31890;&#23376;&#65288;SEP&#65289;&#30340;&#22826;&#38451;&#20107;&#20214;&#65288;&#32768;&#26001;&#65289;&#21644;17,542&#20010;&#19981;&#20135;&#29983;SEP&#30340;&#20107;&#20214;&#12290;&#23545;&#20110;&#27599;&#20010;&#30830;&#23450;&#30340;&#20107;&#20214;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;1 au&#22788;&#30340;&#23616;&#37096;&#31561;&#31163;&#23376;&#20307;&#23646;&#24615;&#65292;&#22914;&#39640;&#33021;&#36136;&#23376;&#21644;&#30005;&#23376;&#25968;&#25454;&#65292;&#19978;&#28216;&#22826;&#38451;&#39118;&#26465;&#20214;&#20197;&#21450;&#22269;&#23478;&#22320;&#29702;&#23398;&#20250;&#21355;&#26143;&#65288;GOES&#65289;&#21644;&#39640;&#32423;&#32452;&#25104;&#25506;&#27979;&#22120;&#65288;ACE&#65289;&#33322;&#22825;&#22120;&#19978;&#30340;&#19981;&#21516;&#20202;&#22120;&#27979;&#24471;&#30340;&#26143;&#38469;&#30913;&#22330;&#30690;&#37327;&#37327;&#12290;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;&#26469;&#33258;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#12289;&#22826;&#38451;&#21644;&#26085;&#29699;&#21355;&#26143;&#65288;SoHO&#65289;&#20197;&#21450;Wind&#22826;&#38451;&#23556;&#30005;&#20202;&#30340;&#36965;&#24863;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#20801;&#35768;&#36827;&#34892;&#21508;&#31181;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
We introduce a new multivariate data set that utilizes multiple spacecraft collecting in-situ and remote sensing heliospheric measurements shown to be linked to physical processes responsible for generating solar energetic particles (SEPs). Using the Geostationary Operational Environmental Satellites (GOES) flare event list from Solar Cycle (SC) 23 and part of SC 24 (1998-2013), we identify 252 solar events (flares) that produce SEPs and 17,542 events that do not. For each identified event, we acquire the local plasma properties at 1 au, such as energetic proton and electron data, upstream solar wind conditions, and the interplanetary magnetic field vector quantities using various instruments onboard GOES and the Advanced Composition Explorer (ACE) spacecraft. We also collect remote sensing data from instruments onboard the Solar Dynamic Observatory (SDO), Solar and Heliospheric Observatory (SoHO), and the Wind solar radio instrument WAVES. The data set is designed to allow for variati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21487;&#32422;&#35838;&#31243;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#23398;&#20064;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#26679;&#26412;&#20002;&#22833;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#31639;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15389</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#19981;&#21487;&#32422;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Irreducible Curriculum for Language Model Pretraining. (arXiv:2310.15389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21487;&#32422;&#35838;&#31243;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#23398;&#20064;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#26679;&#26412;&#20002;&#22833;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#31639;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25968;&#25454;&#36873;&#25321;&#21644;&#35838;&#31243;&#35774;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21482;&#26377;&#23569;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#35757;&#32451;&#19978;&#26174;&#31034;&#20986;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26041;&#26696;&#26356;&#20851;&#27880;&#39046;&#22495;&#32423;&#21035;&#30340;&#36873;&#25321;&#65292;&#24573;&#35270;&#20102;&#27599;&#20010;&#21333;&#29420;&#35757;&#32451;&#28857;&#30340;&#26356;&#32454;&#31890;&#24230;&#30340;&#36129;&#29486;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24212;&#29992;&#20256;&#32479;&#30340;&#25968;&#25454;&#28857;&#36873;&#25321;&#26041;&#27861;&#24456;&#22256;&#38590;&#65306;&#22823;&#22810;&#25968;&#22312;&#32447;&#25209;&#36873;&#25321;&#26041;&#27861;&#25191;&#34892;&#20004;&#27425;&#21069;&#21521;&#25110;&#21518;&#21521;&#20256;&#36882;&#65292;&#36825;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#39069;&#22806;&#25104;&#26412;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21487;&#32422;&#35838;&#31243;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#35838;&#31243;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#23398;&#20064;&#33021;&#21147;&#30340;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#36991;&#20813;&#36807;&#39640;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#20351;&#29992;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#26679;&#26412;&#20002;&#22833;&#27839;&#20027;&#27169;&#22411;&#35757;&#32451;&#36712;&#36857;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;RedPajama-1B&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35838;&#31243;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on valida
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26234;&#33021;&#29615;&#22659;&#20013;&#36890;&#36807;&#35270;&#39057;&#36827;&#34892;&#36828;&#31243;&#24515;&#29575;&#30417;&#27979;&#21644;&#20272;&#35745;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15388</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#29615;&#22659;&#20013;&#36890;&#36807;&#35270;&#39057;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#36828;&#31243;&#24515;&#29575;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Remote Heart Rate Monitoring in Smart Environments from Videos with Self-supervised Pre-training. (arXiv:2310.15388v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26234;&#33021;&#29615;&#22659;&#20013;&#36890;&#36807;&#35270;&#39057;&#36827;&#34892;&#36828;&#31243;&#24515;&#29575;&#30417;&#27979;&#21644;&#20272;&#35745;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#20998;&#26512;&#35270;&#39057;&#36828;&#31243;&#20272;&#35745;&#24515;&#29575;&#22312;&#26234;&#33021;&#29615;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#26159;&#23427;&#20204;&#23545;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20005;&#37325;&#20381;&#36182;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#24212;&#36816;&#32780;&#29983;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#20272;&#35745;&#36828;&#31243;&#20809;&#35889;&#27979;&#37327;&#65288;PPG&#65289;&#21644;&#24515;&#29575;&#30417;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;3&#31181;&#31354;&#38388;&#21644;3&#31181;&#26102;&#38388;&#22686;&#24378;&#26041;&#27861;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#26694;&#26550;&#21033;&#29992;&#32534;&#30721;&#22120;&#30340;&#21518;&#26399;&#20013;&#38388;&#23884;&#20837;&#36827;&#34892;&#36828;&#31243;PPG&#21644;&#24515;&#29575;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#20197;&#21450;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have made it increasingly feasible to estimate heart rate remotely in smart environments by analyzing videos. However, a notable limitation of deep learning methods is their heavy reliance on extensive sets of labeled data for effective training. To address this issue, self-supervised learning has emerged as a promising avenue. Building on this, we introduce a solution that utilizes self-supervised contrastive learning for the estimation of remote photoplethysmography (PPG) and heart rate monitoring, thereby reducing the dependence on labeled data and enhancing performance. We propose the use of 3 spatial and 3 temporal augmentations for training an encoder through a contrastive framework, followed by utilizing the late-intermediate embeddings of the encoder for remote PPG and heart rate estimation. Our experiments on two publicly available datasets showcase the improvement of our proposed approach over several related works as well as supervised learni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;Talagrand&#19981;&#31561;&#24335;&#21644;Borel-Cantelli&#24341;&#29702;&#65292;&#24314;&#31435;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#35823;&#24046;&#32039;&#33268;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.15387</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Error analysis of generative adversarial network. (arXiv:2310.15387v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;Talagrand&#19981;&#31561;&#24335;&#21644;Borel-Cantelli&#24341;&#29702;&#65292;&#24314;&#31435;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#35823;&#24046;&#32039;&#33268;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#36817;&#24180;&#26469;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#23398;&#20064;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24613;&#38656;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#20854;&#35823;&#24046;&#25910;&#25947;&#36895;&#24230;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#30740;&#31350;&#22522;&#20110;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31867;&#21035;&#26500;&#24314;&#30340;GAN&#27169;&#22411;&#30340;&#35823;&#24046;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#25105;&#20204;&#30340;&#20551;&#35774;&#19979;&#65292;&#36825;&#20123;&#20989;&#25968;&#23646;&#20110;VC&#31867;&#22411;&#65292;&#24182;&#20855;&#26377;&#26377;&#30028;&#20449;&#23553;&#20989;&#25968;&#65292;&#21487;&#20197;&#24212;&#29992;Talagrand&#19981;&#31561;&#24335;&#12290;&#36890;&#36807;&#36816;&#29992;Talagrand&#19981;&#31561;&#24335;&#21644;Borel-Cantelli&#24341;&#29702;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;GAN&#35823;&#24046;&#30340;&#32039;&#33268;&#25910;&#25947;&#36895;&#24230;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;GAN&#35823;&#24046;&#20272;&#35745;&#65292;&#24182;&#33719;&#24471;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#25105;&#20204;&#30340;&#23450;&#20041;&#20013;&#65292;&#29992;&#31070;&#32463;&#32593;&#32476;&#36317;&#31163;&#23450;&#20041;&#30340;&#35823;&#24046;&#26159;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generative adversarial network (GAN) is an important model developed for high-dimensional distribution learning in recent years. However, there is a pressing need for a comprehensive method to understand its error convergence rate. In this research, we focus on studying the error convergence rate of the GAN model that is based on a class of functions encompassing the discriminator and generator neural networks. These functions are VC type with bounded envelope function under our assumptions, enabling the application of the Talagrand inequality. By employing the Talagrand inequality and Borel-Cantelli lemma, we establish a tight convergence rate for the error of GAN. This method can also be applied on existing error estimations of GAN and yields improved convergence rates. In particular, the error defined with the neural network distance is a special case error in our definition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20462;&#27491;&#20102;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.15386</link><description>&lt;p&gt;
&#20462;&#27491;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Course Correcting Koopman Representations. (arXiv:2310.15386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20462;&#27491;&#20102;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#34920;&#31034;&#26088;&#22312;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#31616;&#21270;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#24314;&#27169;&#21644;&#25511;&#21046;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27492;&#38382;&#39064;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#24314;&#27169;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#19981;&#21516;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#25512;&#29702;&#26102;&#38388;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#38271;&#26399;&#21160;&#24577;&#30340;&#20934;&#30830;&#25429;&#25417;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Koopman representations aim to learn features of nonlinear dynamical systems (NLDS) which lead to linear dynamics in the latent space. Theoretically, such features can be used to simplify many problems in modeling and control of NLDS. In this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons. We discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as Periodic Reencoding, for faithfully capturing long term dynamics. We justify this method both analytically and empirically via experiments in low and high dimensional NLDS.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#36817;&#29305;&#24449;&#32479;&#35745;&#22686;&#24378;&#30340;&#32852;&#37030;&#19977;&#32500;&#21307;&#23398;&#20307;&#31215;&#20998;&#21106;&#26041;&#26696;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;FL&#20998;&#21106;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#21644;&#25968;&#25454;&#20998;&#24067;&#24322;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15371</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#36817;&#29305;&#24449;&#32479;&#35745;&#22686;&#24378;&#30340;&#32852;&#37030;&#19977;&#32500;&#21307;&#23398;&#20307;&#31215;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Vicinal Feature Statistics Augmentation for Federated 3D Medical Volume Segmentation. (arXiv:2310.15371v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#36817;&#29305;&#24449;&#32479;&#35745;&#22686;&#24378;&#30340;&#32852;&#37030;&#19977;&#32500;&#21307;&#23398;&#20307;&#31215;&#20998;&#21106;&#26041;&#26696;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;FL&#20998;&#21106;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#21644;&#25968;&#25454;&#20998;&#24067;&#24322;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#22810;&#20010;&#21307;&#30103;&#26426;&#26500;&#33021;&#22815;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#21069;&#25552;&#19979;&#65292;&#20849;&#21516;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23567;&#22411;&#26426;&#26500;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#24322;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#65288;&#21363;&#38750;i.i.d.&#65289;&#21487;&#33021;&#38480;&#21046;FL&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#25968;&#25454;&#22686;&#24378;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#20256;&#32479;&#38598;&#20013;&#24335;DL&#27867;&#21270;&#33021;&#21147;&#30340;&#26377;&#25928;&#25216;&#26415;&#65292;&#20294;&#20854;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21463;&#21040;&#26114;&#36149;&#30340;&#26631;&#27880;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;3D&#21307;&#23398;&#20998;&#21106;&#36890;&#24120;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#37051;&#36817;&#29305;&#24449;&#23618;&#32423;&#25968;&#25454;&#22686;&#24378;&#65288;VFDA&#65289;&#26041;&#26696;&#65292;&#20197;&#26377;&#25928;&#20943;&#36731;&#23616;&#37096;&#29305;&#24449;&#28418;&#31227;&#65292;&#24182;&#20419;&#36827;&#38544;&#31169;&#24863;&#30693;&#30340;FL&#20998;&#21106;&#30340;&#21327;&#21516;&#35757;&#32451;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#36328;&#26426;&#26500;&#30340;&#24046;&#24322;&#65292;&#32780;&#26080;&#38656;&#36328;&#26426;&#26500;&#36716;&#31227;&#21407;&#22987;&#25968;&#25454;&#25110;&#28151;&#21512;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple client medical institutes collaboratively train a deep learning (DL) model with privacy protection. However, the performance of FL can be constrained by the limited availability of labeled data in small institutes and the heterogeneous (i.e., non-i.i.d.) data distribution across institutes. Though data augmentation has been a proven technique to boost the generalization capabilities of conventional centralized DL as a "free lunch", its application in FL is largely underexplored. Notably, constrained by costly labeling, 3D medical segmentation generally relies on data augmentation. In this work, we aim to develop a vicinal feature-level data augmentation (VFDA) scheme to efficiently alleviate the local feature shift and facilitate collaborative training for privacy-aware FL segmentation. We take both the inner- and inter-institute divergence into consideration, without the need for cross-institute transfer of raw data or their mixup. Specifically
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#39640;&#27010;&#29575;&#20844;&#24179;&#24615;&#20445;&#35777;&#30340;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;FRG&#65289;&#65292;&#36890;&#36807;&#29992;&#25143;&#23450;&#20041;&#30340;&#19978;&#30028;&#38480;&#21046;&#65292;&#22312;&#25152;&#26377;&#19979;&#28216;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;FRG&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15358</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#20445;&#35777;&#30340;&#20844;&#24179;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Fair Representations with High-Confidence Guarantees. (arXiv:2310.15358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#39640;&#27010;&#29575;&#20844;&#24179;&#24615;&#20445;&#35777;&#30340;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;FRG&#65289;&#65292;&#36890;&#36807;&#29992;&#25143;&#23450;&#20041;&#30340;&#19978;&#30028;&#38480;&#21046;&#65292;&#22312;&#25152;&#26377;&#19979;&#28216;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;FRG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#34920;&#31034;&#23398;&#20064;&#29983;&#25104;&#36328;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20855;&#26377;&#39044;&#27979;&#24615;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#33021;&#25552;&#20379;&#24378;&#26377;&#21147;&#20844;&#24179;&#20445;&#35777;&#30340;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#38450;&#27490;&#23545;&#24369;&#21183;&#32676;&#20307;&#22312;&#25152;&#26377;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;&#20026;&#20102;&#38450;&#27490;&#22312;&#25152;&#26377;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#19981;&#20844;&#24179;&#24453;&#36935;&#65292;&#25552;&#20379;&#25552;&#20379;&#20844;&#24179;&#20445;&#35777;&#30340;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24418;&#24335;&#21270;&#23450;&#20041;&#20102;&#23398;&#20064;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#30340;&#20844;&#24179;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#20445;&#35777;&#30340;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#65288;FRG&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#29992;&#25143;&#23450;&#20041;&#30340;&#19978;&#30028;&#20026;&#38480;&#21046;&#65292;&#22312;&#25152;&#26377;&#19979;&#28216;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#38477;&#20302;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#35777;&#26126;FRG&#33021;&#20197;&#39640;&#27010;&#29575;&#20445;&#35777;&#25152;&#26377;&#19979;&#28216;&#27169;&#22411;&#21644;&#20219;&#21153;&#30340;&#20844;&#24179;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FRG&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning is increasingly employed to generate representations that are predictive across multiple downstream tasks. The development of representation learning algorithms that provide strong fairness guarantees is thus important because it can prevent unfairness towards disadvantaged groups for all downstream prediction tasks. To prevent unfairness towards disadvantaged groups in all downstream tasks, it is crucial to provide representation learning algorithms that provide fairness guarantees. In this paper, we formally define the problem of learning representations that are fair with high confidence. We then introduce the Fair Representation learning with high-confidence Guarantees (FRG) framework, which provides high-confidence guarantees for limiting unfairness across all downstream models and tasks, with user-defined upper bounds. After proving that FRG ensures fairness for all downstream models and tasks with high probability, we present empirical evaluations that de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#20351;&#29992;&#38543;&#26426;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#35823;&#24046;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#38543;&#26426;&#25506;&#32034;&#36991;&#20813;&#20102;&#27599;&#27425;&#36845;&#20195;&#20013;&#38750;&#20984;&#33719;&#21462;&#20989;&#25968;&#30340;&#26114;&#36149;&#20248;&#21270;&#65292;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15351</link><description>&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#38543;&#26426;&#25506;&#32034;&#65306;&#26368;&#20339;&#36951;&#25022;&#21644;&#35745;&#31639;&#25928;&#29575;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency. (arXiv:2310.15351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#20351;&#29992;&#38543;&#26426;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#35823;&#24046;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#38543;&#26426;&#25506;&#32034;&#36991;&#20813;&#20102;&#27599;&#27425;&#36845;&#20195;&#20013;&#38750;&#20984;&#33719;&#21462;&#20989;&#25968;&#30340;&#26114;&#36149;&#20248;&#21270;&#65292;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20063;&#31216;&#20026;&#22522;&#20110;&#26680;&#30340;&#36172;&#21338;&#20248;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20351;&#29992;&#20174;&#20998;&#24067;&#20013;&#38543;&#26426;&#25277;&#26679;&#26469;&#25506;&#32034;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#38543;&#26426;&#25506;&#32034;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#35823;&#24046;&#29575;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#22312;&#26412;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26032;&#22411;&#38598;&#20013;&#36793;&#30028;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25506;&#32034;&#21644;&#39046;&#22495;&#32553;&#23567;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#29615;&#22659;&#19979;&#24314;&#31435;&#20854;&#26368;&#20339;&#36951;&#25022;&#20445;&#35777;&#12290;&#22312;&#26080;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#22635;&#34917;&#20102;&#22312;&#36951;&#25022;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;COLT&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#30001;&#20110;&#38543;&#26426;&#25506;&#32034;&#28040;&#38500;&#20102;&#27599;&#27425;&#36845;&#20195;&#20013;&#36873;&#25321;&#26597;&#35810;&#28857;&#30340;&#38750;&#20984;&#33719;&#21462;&#20989;&#25968;&#30340;&#26114;&#36149;&#20248;&#21270;&#65292;&#25152;&#20197;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20063;&#20855;&#26377;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider Bayesian optimization using Gaussian Process models, also referred to as kernel-based bandit optimization. We study the methodology of exploring the domain using random samples drawn from a distribution. We show that this random exploration approach achieves the optimal error rates. Our analysis is based on novel concentration bounds in an infinite dimensional Hilbert space established in this work, which may be of independent interest. We further develop an algorithm based on random exploration with domain shrinking and establish its order-optimal regret guarantees under both noise-free and noisy settings. In the noise-free setting, our analysis closes the existing gap in regret performance and thereby resolves a COLT open problem. The proposed algorithm also enjoys a computational advantage over prevailing methods due to the random exploration that obviates the expensive optimization of a non-convex acquisition function for choosing the query points at each iteration.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29289;&#29702;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#38544;&#24335;&#27431;&#25289;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24212;&#29992;&#20110;&#27714;&#35299;Burgers&#26041;&#31243;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#21069;&#19968;&#20010;&#32593;&#32476;&#21521;&#19979;&#19968;&#20010;&#32593;&#32476;&#20256;&#36882;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#38544;&#24335;&#27431;&#25289;&#36817;&#20284;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23398;&#20064;&#24403;&#21069;&#26102;&#38388;&#35299;&#12290;&#19982;&#36890;&#24120;&#30340;PINN&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#20934;&#30830;&#32467;&#26524;&#21644;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15343</link><description>&lt;p&gt;
&#29992;&#38544;&#24335;&#27431;&#25289;&#36801;&#31227;&#23398;&#20064;&#30340;PINNs&#27714;&#35299;Burgers&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Burgers' pinns with implicit euler transfer learning. (arXiv:2310.15343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29289;&#29702;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#38544;&#24335;&#27431;&#25289;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24212;&#29992;&#20110;&#27714;&#35299;Burgers&#26041;&#31243;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#21069;&#19968;&#20010;&#32593;&#32476;&#21521;&#19979;&#19968;&#20010;&#32593;&#32476;&#20256;&#36882;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#38544;&#24335;&#27431;&#25289;&#36817;&#20284;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23398;&#20064;&#24403;&#21069;&#26102;&#38388;&#35299;&#12290;&#19982;&#36890;&#24120;&#30340;PINN&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#20934;&#30830;&#32467;&#26524;&#21644;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Burgers&#26041;&#31243;&#26159;&#35745;&#31639;&#24314;&#27169;&#20013;&#22810;&#31181;&#29616;&#35937;&#65288;&#22914;&#27969;&#20307;&#21160;&#21147;&#23398;&#12289;&#27668;&#20307;&#21160;&#21147;&#23398;&#12289;&#20914;&#20987;&#29702;&#35770;&#12289;&#23431;&#23449;&#23398;&#31561;&#65289;&#30340;&#19968;&#20010;&#25104;&#29087;&#30340;&#27979;&#35797;&#26696;&#20363;&#12290;&#26412;&#25991;&#23558;&#29289;&#29702;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#38544;&#24335;&#27431;&#25289;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24212;&#29992;&#20110;&#27714;&#35299;Burgers&#26041;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#31995;&#21015;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#23547;&#27714;&#26102;&#38388;&#31163;&#25955;&#35299;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#65292;&#21069;&#19968;&#20010;ANN&#23558;&#20854;&#30693;&#35782;&#20256;&#36882;&#32473;&#19979;&#19968;&#20010;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;Burgers&#26041;&#31243;&#38544;&#24335;&#27431;&#25289;&#36817;&#20284;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23398;&#20064;&#24403;&#21069;&#26102;&#38388;&#35299;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#19968;&#20010;&#20855;&#26377;&#31934;&#30830;&#35299;&#65292;&#21478;&#19968;&#20010;&#20855;&#26377;&#26367;&#20195;&#35299;&#26512;&#35299;&#12290;&#19982;&#36890;&#24120;&#30340;PINN&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#20934;&#30830;&#32467;&#26524;&#21644;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Burgers equation is a well-established test case in the computational modeling of several phenomena such as fluid dynamics, gas dynamics, shock theory, cosmology, and others. In this work, we present the application of Physics-Informed Neural Networks (PINNs) with an implicit Euler transfer learning approach to solve the Burgers equation. The proposed approach consists in seeking a time-discrete solution by a sequence of Artificial Neural Networks (ANNs). At each time step, the previous ANN transfers its knowledge to the next network model, which learns the current time solution by minimizing a loss function based on the implicit Euler approximation of the Burgers equation. The approach is tested for two benchmark problems: the first with an exact solution and the other with an alternative analytical solution. In comparison to the usual PINN models, the proposed approach has the advantage of requiring smaller neural network architectures with similar accurate results and potentiall
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#31232;&#30095;&#32593;&#32476;&#30340;&#28151;&#21512;&#31890;&#24230;&#29305;&#24449;&#20132;&#20114;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#29305;&#24449;&#22495;&#21644;&#29305;&#24449;&#20540;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.15342</link><description>&lt;p&gt;
&#38754;&#21521;&#28145;&#24230;&#31232;&#30095;&#32593;&#32476;&#30340;&#28151;&#21512;&#31890;&#24230;&#29305;&#24449;&#20132;&#20114;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Hybrid-grained Feature Interaction Selection for Deep Sparse Network. (arXiv:2310.15342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#31232;&#30095;&#32593;&#32476;&#30340;&#28151;&#21512;&#31890;&#24230;&#29305;&#24449;&#20132;&#20114;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#29305;&#24449;&#22495;&#21644;&#29305;&#24449;&#20540;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31232;&#30095;&#32593;&#32476;&#34987;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#20855;&#26377;&#39640;&#32500;&#31232;&#30095;&#29305;&#24449;&#30340;&#39044;&#27979;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20854;&#20013;&#29305;&#24449;&#20132;&#20114;&#36873;&#25321;&#26159;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#22312;&#31895;&#31890;&#24230;&#31354;&#38388;&#20013;&#25628;&#32034;&#29305;&#24449;&#20132;&#20114;&#65292;&#23545;&#20110;&#26356;&#32454;&#31890;&#24230;&#30340;&#32454;&#33410;&#21017;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#31232;&#30095;&#32593;&#32476;&#30340;&#28151;&#21512;&#31890;&#24230;&#29305;&#24449;&#20132;&#20114;&#36873;&#25321;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#32771;&#34385;&#29305;&#24449;&#22495;&#21644;&#29305;&#24449;&#20540;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#26679;&#24191;&#38420;&#30340;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#26102;&#35745;&#31639;&#30340;&#20998;&#35299;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OptFeature&#30340;&#36873;&#25321;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#29305;&#24449;&#22495;&#21644;&#29305;&#24449;&#20540;&#21516;&#26102;&#36873;&#25321;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#19977;&#20010;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OptFeature&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#39069;&#22806;&#30340;&#30740;&#31350;&#25903;&#25345;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep sparse networks are widely investigated as a neural network architecture for prediction tasks with high-dimensional sparse features, with which feature interaction selection is a critical component. While previous methods primarily focus on how to search feature interaction in a coarse-grained space, less attention has been given to a finer granularity. In this work, we introduce a hybrid-grained feature interaction selection approach that targets both feature field and feature value for deep sparse networks. To explore such expansive space, we propose a decomposed space which is calculated on the fly. We then develop a selection algorithm called OptFeature, which efficiently selects the feature interaction from both the feature field and the feature value simultaneously. Results from experiments on three large real-world benchmark datasets demonstrate that OptFeature performs well in terms of accuracy and efficiency. Additional studies support the feasibility of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#24182;&#34892;&#30340;ADMM&#31639;&#27861;&#35299;&#20915;&#27531;&#24046;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#24182;&#34892;&#23454;&#29616;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15334</link><description>&lt;p&gt;
ADMM&#35757;&#32451;&#31639;&#27861;&#29992;&#20110;&#27531;&#24046;&#32593;&#32476;&#65306;&#25910;&#25947;&#24615;&#65292;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ADMM Training Algorithms for Residual Networks: Convergence, Complexity and Parallel Training. (arXiv:2310.15334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#24182;&#34892;&#30340;ADMM&#31639;&#27861;&#35299;&#20915;&#27531;&#24046;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#24182;&#34892;&#23454;&#29616;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#21464;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#24182;&#34892;&#30340;&#36817;&#31471;&#28857;&#65288;&#26799;&#24230;&#65289;ADMM&#31639;&#27861;&#26469;&#35299;&#20915;&#23436;&#20840;&#36830;&#25509;&#30340;&#27531;&#24046;&#32593;&#32476;&#65288;FCResNets&#65289;&#35757;&#32451;&#38382;&#39064;&#12290;&#36890;&#36807;&#22522;&#20110;Kurdyka-Lojasiewicz&#65288;KL&#65289;&#23646;&#24615;&#20998;&#26512;&#26694;&#26550;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36817;&#31471;&#28857;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;Kurdyka-Lojasiewicz&#65288;KL&#65289;&#25351;&#25968;&#33539;&#22260;&#20869;&#65292;&#23454;&#29616;&#23616;&#37096;R-&#32447;&#24615;&#25110;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#24182;&#34892;&#23454;&#29616;&#22312;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#65288;&#27599;&#20010;&#33410;&#28857;&#30340;&#65289;&#20869;&#23384;&#28040;&#32791;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#24212;&#29992;&#20110;FCResNets&#35757;&#32451;&#38382;&#39064;&#30340;ADMM&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#25910;&#25947;&#36895;&#24230;&#65292;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#65288;&#27599;&#20010;&#33410;&#28857;&#30340;&#65289;&#20869;&#23384;&#38656;&#27714;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#39640;&#36895;&#24230;&#65292;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#40065;&#26834;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design a series of serial and parallel proximal point (gradient) ADMMs for the fully connected residual networks (FCResNets) training problem by introducing auxiliary variables. Convergence of the proximal point version is proven based on a Kurdyka-Lojasiewicz (KL) property analysis framework, and we can ensure a locally R-linear or sublinear convergence rate depending on the different ranges of the Kurdyka-Lojasiewicz (KL) exponent, in which a necessary auxiliary function is constructed to realize our goal. Moreover, the advantages of the parallel implementation in terms of lower time complexity and less (per-node) memory consumption are analyzed theoretically. To the best of our knowledge, this is the first work analyzing the convergence, convergence rate, time complexity and (per-node) runtime memory requirement of the ADMM applied in the FCResNets training problem theoretically. Experiments are reported to show the high speed, better performance, robustness and potential in the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21305;&#37197;&#24739;&#32773;&#30340;&#21307;&#23398;&#21644;&#33647;&#29289;&#29305;&#24615;&#26469;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#30149;&#21490;&#21644;&#33647;&#29289;&#29305;&#24449;&#26469;&#21046;&#23450;&#65292;&#24182;&#21457;&#29616;&#20943;&#23569;&#33647;&#29289;&#21058;&#37327;&#21487;&#20197;&#20943;&#36731;&#30149;&#24773;&#32780;&#19981;&#20250;&#23545;&#27835;&#30103;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.15333</link><description>&lt;p&gt;
&#20272;&#35745;&#21487;&#20449;&#36182;&#21644;&#23433;&#20840;&#30340;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Estimating Trustworthy and Safe Optimal Treatment Regimes. (arXiv:2310.15333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15333
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21305;&#37197;&#24739;&#32773;&#30340;&#21307;&#23398;&#21644;&#33647;&#29289;&#29305;&#24615;&#26469;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#30149;&#21490;&#21644;&#33647;&#29289;&#29305;&#24449;&#26469;&#21046;&#23450;&#65292;&#24182;&#21457;&#29616;&#20943;&#23569;&#33647;&#29289;&#21058;&#37327;&#21487;&#20197;&#20943;&#36731;&#30149;&#24773;&#32780;&#19981;&#20250;&#23545;&#27835;&#30103;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32479;&#35745;&#23398;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26174;&#33879;&#25512;&#21160;&#20102;&#24739;&#32773;&#25252;&#29702;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#22833;&#25968;&#25454;&#12289;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#23545;&#35299;&#37322;&#24615;&#21644;&#24739;&#32773;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36816;&#29992;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#26469;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#23558;&#20855;&#26377;&#30456;&#20284;&#21307;&#23398;&#21644;&#33647;&#29289;&#29305;&#24449;&#30340;&#24739;&#32773;&#36827;&#34892;&#21305;&#37197;&#65292;&#20174;&#32780;&#36890;&#36807;&#25554;&#20540;&#26500;&#24314;&#26368;&#20339;&#25919;&#31574;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#35813;&#26694;&#26550;&#21363;&#20351;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20063;&#33021;&#22815;&#35782;&#21035;&#26368;&#20339;&#25919;&#31574;&#30340;&#33021;&#21147;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#30740;&#31350;&#23545;&#37325;&#30151;&#24739;&#32773;&#36827;&#34892;&#30315;&#30187;&#27835;&#30103;&#30340;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#28872;&#25903;&#25345;&#22522;&#20110;&#24739;&#32773;&#30340;&#21307;&#30103;&#21382;&#21490;&#21644;&#33647;&#29289;&#29305;&#24449;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20943;&#23569;&#33647;&#29289;&#21058;&#37327;&#21487;&#20197;&#20943;&#36731;&#30149;&#24773;&#32780;&#19981;&#20250;&#23545;&#27835;&#30103;&#30340;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent statistical and reinforcement learning methods have significantly advanced patient care strategies. However, these approaches face substantial challenges in high-stakes contexts, including missing data, inherent stochasticity, and the critical requirements for interpretability and patient safety. Our work operationalizes a safe and interpretable framework to identify optimal treatment regimes. This approach involves matching patients with similar medical and pharmacological characteristics, allowing us to construct an optimal policy via interpolation. We perform a comprehensive simulation study to demonstrate the framework's ability to identify optimal policies even in complex settings. Ultimately, we operationalize our approach to study regimes for treating seizures in critically ill patients. Our findings strongly support personalized treatment strategies based on a patient's medical history and pharmacological features. Notably, we identify that reducing medication doses for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#24322;&#26500;&#28151;&#21512;&#27604;&#20363;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#65292;&#22312;&#36866;&#29992;&#20110;&#26222;&#36890;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#38754;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#23545;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21644;&#28151;&#21512;&#22238;&#24402;&#65288;MoRs&#65289;&#36827;&#34892;&#20102;&#20855;&#20307;&#30340;&#20272;&#35745;&#35823;&#24046;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30456;&#20284;&#24615;&#12289;&#25269;&#25239;&#23545;&#23569;&#37096;&#20998;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#25915;&#20987;&#12289;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#31561;&#20851;&#38190;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15330</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#65306;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#24322;&#26500;&#28151;&#21512;&#27169;&#22411;&#30340;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Federated Learning: A Federated Gradient EM Algorithm for Heterogeneous Mixture Models with Robustness against Adversarial Attacks. (arXiv:2310.15330v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#24322;&#26500;&#28151;&#21512;&#27604;&#20363;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#65292;&#22312;&#36866;&#29992;&#20110;&#26222;&#36890;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#38754;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#23545;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21644;&#28151;&#21512;&#22238;&#24402;&#65288;MoRs&#65289;&#36827;&#34892;&#20102;&#20855;&#20307;&#30340;&#20272;&#35745;&#35823;&#24046;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30456;&#20284;&#24615;&#12289;&#25269;&#25239;&#23545;&#23569;&#37096;&#20998;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#25915;&#20987;&#12289;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#31561;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#24322;&#26500;&#28151;&#21512;&#27604;&#20363;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26222;&#36890;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#38754;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#65292;&#28982;&#21518;&#23558;&#36825;&#19968;&#36890;&#29992;&#29702;&#35770;&#24212;&#29992;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21644;&#28151;&#21512;&#22238;&#24402;&#65288;MoRs&#65289;&#20197;&#25551;&#36848;&#27169;&#22411;&#21442;&#25968;&#21644;&#28151;&#21512;&#27604;&#20363;&#30340;&#26174;&#24335;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30456;&#20284;&#24615;&#12289;&#23545;&#23569;&#37096;&#20998;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#24377;&#24615;&#12289;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While supervised federated learning approaches have enjoyed significant success, the domain of unsupervised federated learning remains relatively underexplored. In this paper, we introduce a novel federated gradient EM algorithm designed for the unsupervised learning of mixture models with heterogeneous mixture proportions across tasks. We begin with a comprehensive finite-sample theory that holds for general mixture models, then apply this general theory on Gaussian Mixture Models (GMMs) and Mixture of Regressions (MoRs) to characterize the explicit estimation error of model parameters and mixture proportions. Our proposed federated gradient EM algorithm demonstrates several key advantages: adaptability to unknown task similarity, resilience against adversarial attacks on a small fraction of data sources, protection of local data privacy, and computational and communication efficiency.
&lt;/p&gt;</description></item><item><title>flwr-serverless&#26159;&#19968;&#31181;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#23433;&#20840;&#21644;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.15329</link><description>&lt;p&gt;
&#20351;&#29992;flwr-serverless&#30340;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Serverless Federated Learning with flwr-serverless. (arXiv:2310.15329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15329
&lt;/p&gt;
&lt;p&gt;
flwr-serverless&#26159;&#19968;&#31181;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#23433;&#20840;&#21644;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#25910;&#38598;&#21644;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#23384;&#20648;&#37327;&#30340;&#28608;&#22686;&#65292;&#32852;&#37030;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#21644;&#21463;&#27426;&#36814;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19990;&#30028;&#21508;&#22269;&#25552;&#20986;&#20102;&#35768;&#22810;&#20851;&#20110;&#20026;&#20010;&#20154;&#25968;&#25454;&#25552;&#20379;&#26356;&#22810;&#20445;&#25252;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#25514;&#26045;&#30340;&#24378;&#21270;&#20852;&#36259;&#30340;&#24314;&#35758;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#26032;&#30340;&#21644;&#29616;&#26377;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#24320;&#21457;&#20687;&#32852;&#37030;&#23398;&#20064;&#36825;&#26679;&#33021;&#26377;&#25928;&#35757;&#32451;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#65288;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#30340;&#25968;&#25454;&#12289;&#21516;&#26102;&#19981;&#25439;&#23475;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#31574;&#30053;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#21517;&#20026;Flower&#65288;Flwr&#65289;&#30340;Python&#21253;&#65292;&#20026;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#12289;&#28789;&#27963;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;Flower&#21482;&#33021;&#36816;&#34892;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#36816;&#34892;&#25104;&#26412;&#39640;&#12289;&#32791;&#26102;&#38271;&#65292;&#22240;&#20026;&#36807;&#31243;&#21463;&#38480;&#20110;&#24930;&#36895;&#25110;&#19981;&#31283;&#23450;&#30340;&#23458;&#25143;&#31471;&#35757;&#32451;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is becoming increasingly relevant and popular as we witness a surge in data collection and storage of personally identifiable information. Alongside these developments there have been many proposals from governments around the world to provide more protections for individuals' data and a heightened interest in data privacy measures. As deep learning continues to become more relevant in new and existing domains, it is vital to develop strategies like federated learning that can effectively train data from different sources, such as edge devices, without compromising security and privacy. Recently, the Flower (\texttt{Flwr}) Python package was introduced to provide a scalable, flexible, and easy-to-use framework for implementing federated learning. However, to date, Flower is only able to run synchronous federated learning which can be costly and time-consuming to run because the process is bottlenecked by client-side training jobs that are slow or fragile. Here, we in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;DeepVox&#21644;SAVE-CT&#27169;&#22411;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20851;&#23545;&#27604;&#24230;&#21644;&#21058;&#37327;&#30340;&#19977;&#32500;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33016;&#20027;&#21160;&#33033;&#20998;&#21106;&#21644;&#30244;&#26679;&#21160;&#33033;&#25193;&#24352;&#39044;&#27979;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12290;&#26032;&#27169;&#22411;&#22312;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#36739;&#39640;&#30340;Dice&#31995;&#25968;&#65292;&#24182;&#19988;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2310.15328</link><description>&lt;p&gt;
DeepVox&#21644;SAVE-CT&#65306;&#19968;&#31181;&#26080;&#20851;&#23545;&#27604;&#24230;&#21644;&#21058;&#37327;&#30340;&#19977;&#32500;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33016;&#37096;&#20027;&#21160;&#33033;&#20998;&#21106;&#21644;&#30244;&#26679;&#21160;&#33033;&#25193;&#24352;&#39044;&#27979;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;
&lt;/p&gt;
&lt;p&gt;
DeepVox and SAVE-CT: a contrast- and dose-independent 3D deep learning approach for thoracic aorta segmentation and aneurysm prediction using computed tomography scans. (arXiv:2310.15328v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;DeepVox&#21644;SAVE-CT&#27169;&#22411;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20851;&#23545;&#27604;&#24230;&#21644;&#21058;&#37327;&#30340;&#19977;&#32500;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33016;&#20027;&#21160;&#33033;&#20998;&#21106;&#21644;&#30244;&#26679;&#21160;&#33033;&#25193;&#24352;&#39044;&#27979;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12290;&#26032;&#27169;&#22411;&#22312;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#36739;&#39640;&#30340;Dice&#31995;&#25968;&#65292;&#24182;&#19988;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#20027;&#21160;&#33033;&#25193;&#24352;&#26159;&#19968;&#31181;&#33268;&#21629;&#30340;&#30142;&#30149;&#65292;&#21487;&#33021;&#36890;&#36807;&#20027;&#21160;&#33033;&#36880;&#28176;&#25193;&#22823;&#32780;&#23548;&#33268;&#21093;&#31163;&#25110;&#30772;&#35010;&#12290;&#23427;&#36890;&#24120;&#26159;&#26080;&#30151;&#29366;&#30340;&#65292;&#31579;&#26597;&#24314;&#35758;&#26377;&#38480;&#12290;&#37329;&#26631;&#20934;&#35780;&#20272;&#26159;&#36890;&#36807;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#65288;CTA&#65289;&#21644;&#25918;&#23556;&#31185;&#21307;&#24072;&#32791;&#26102;&#30340;&#35780;&#20272;&#26469;&#23436;&#25104;&#30340;&#12290;&#20854;&#20182;&#36866;&#24212;&#30151;&#30340;&#25195;&#25551;&#21487;&#20197;&#24110;&#21161;&#36827;&#34892;&#31579;&#26597;&#65292;&#20294;&#22914;&#26524;&#27809;&#26377;&#23545;&#27604;&#22686;&#24378;&#25110;&#20302;&#21058;&#37327;&#21327;&#35758;&#36827;&#34892;&#33719;&#21462;&#65292;&#21487;&#33021;&#20250;&#20351;&#20020;&#24202;&#35780;&#20272;&#22256;&#38590;&#65292;&#21516;&#26102;&#22686;&#21152;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#25195;&#25551;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#36873;&#25321;&#20102;587&#20363;&#29420;&#29305;&#30340;CT&#25195;&#25551;&#65292;&#21253;&#25324;&#23545;&#29031;&#32452;&#21644;&#33016;&#20027;&#21160;&#33033;&#25193;&#24352;&#24739;&#32773;&#65292;&#20351;&#29992;&#20302;&#21058;&#37327;&#21644;&#26631;&#20934;&#21058;&#37327;&#21327;&#35758;&#36827;&#34892;&#33719;&#21462;&#65292;&#26377;&#26080;&#23545;&#27604;&#22686;&#24378;&#12290;&#19968;&#31181;&#26032;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;DeepVox&#65292;&#22312;&#24320;&#21457;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#23637;&#31034;&#20102;0.932&#21644;0.897&#30340;Dice&#31995;&#25968;&#65292;&#19982;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thoracic aortic aneurysm (TAA) is a fatal disease which potentially leads to dissection or rupture through progressive enlargement of the aorta. It is usually asymptomatic and screening recommendation are limited. The gold-standard evaluation is performed by computed tomography angiography (CTA) and radiologists time-consuming assessment. Scans for other indications could help on this screening, however if acquired without contrast enhancement or with low dose protocol, it can make the clinical evaluation difficult, besides increasing the scans quantity for the radiologists. In this study, it was selected 587 unique CT scans including control and TAA patients, acquired with low and standard dose protocols, with or without contrast enhancement. A novel segmentation model, DeepVox, exhibited dice score coefficients of 0.932 and 0.897 for development and test sets, respectively, with faster training speed in comparison to models reported in the literature. The novel TAA classification mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32452;&#21512;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#35780;&#20272;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#23545;LXMERT&#36827;&#34892;&#24494;&#35843;&#26102;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;LXMERT&#27169;&#22411;&#30340;&#21387;&#32553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#25439;&#22833;3%&#30340;&#31934;&#24230;&#19979;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36890;&#36807;&#21098;&#26525;&#26041;&#27861;&#23558;LXMERT&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;40%-60%&#12290;</title><link>http://arxiv.org/abs/2310.15325</link><description>&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;LXMERT&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LXMERT Model Compression for Visual Question Answering. (arXiv:2310.15325v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32452;&#21512;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#35780;&#20272;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#23545;LXMERT&#36827;&#34892;&#24494;&#35843;&#26102;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;LXMERT&#27169;&#22411;&#30340;&#21387;&#32553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#25439;&#22833;3%&#30340;&#31934;&#24230;&#19979;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36890;&#36807;&#21098;&#26525;&#26041;&#27861;&#23558;LXMERT&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;40%-60%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;LXMERT&#22312;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#21464;&#24471;&#27969;&#34892;&#12290;&#26681;&#25454;&#20013;&#24425;&#31080;&#20551;&#35828;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#21253;&#21547;&#21487;&#21333;&#29420;&#35757;&#32451;&#20197;&#36798;&#21040;&#23436;&#20840;&#24615;&#33021;&#30340;&#36739;&#23567;&#23376;&#32593;&#32476;&#12290;&#26412;&#25991;&#32467;&#21512;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#35780;&#20272;&#22312;VQA&#20219;&#21153;&#19978;&#23545;LXMERT&#36827;&#34892;&#24494;&#35843;&#26102;&#26159;&#21542;&#23384;&#22312;&#36825;&#26679;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#21487;&#20197;&#36827;&#34892;&#22810;&#23569;&#21098;&#26525;&#32780;&#19981;&#20250;&#36896;&#25104;&#26174;&#33879;&#30340;&#31934;&#24230;&#25439;&#22833;&#26469;&#36827;&#34892;&#27169;&#22411;&#22823;&#23567;&#25104;&#26412;&#25910;&#30410;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;LXMERT&#36827;&#34892;40%-60%&#30340;&#26377;&#25928;&#21098;&#26525;&#65292;&#20165;&#20250;&#25439;&#22833;3%&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pretrained models such as LXMERT are becoming popular for learning cross-modal representations on text-image pairs for vision-language tasks. According to the lottery ticket hypothesis, NLP and computer vision models contain smaller subnetworks capable of being trained in isolation to full performance. In this paper, we combine these observations to evaluate whether such trainable subnetworks exist in LXMERT when fine-tuned on the VQA task. In addition, we perform a model size cost-benefit analysis by investigating how much pruning can be done without significant loss in accuracy. Our experiment results demonstrate that LXMERT can be effectively pruned by 40%-60% in size with 3% loss in accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#25351;&#23548;&#20154;&#31867;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#35828;&#26126;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#24187;&#35273;&#21442;&#32771;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.15319</link><description>&lt;p&gt;
&#20026;&#22522;&#20110;&#25351;&#23548;&#24615;&#35828;&#26126;&#29983;&#25104;&#30340;&#24187;&#35273;&#26816;&#27979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hallucination Detection for Grounded Instruction Generation. (arXiv:2310.15319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15319
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#25351;&#23548;&#20154;&#31867;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#35828;&#26126;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#24187;&#35273;&#21442;&#32771;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27169;&#25311;&#30340;&#20303;&#23429;&#29615;&#22659;&#20013;&#29983;&#25104;&#25351;&#23548;&#20154;&#31867;&#23548;&#33322;&#30340;&#35828;&#26126;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#27169;&#22411;&#23384;&#22312;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#24187;&#35273;&#65306;&#23427;&#20204;&#29983;&#25104;&#19982;&#20154;&#31867;&#36319;&#38543;&#32773;&#22312;&#25551;&#36848;&#30340;&#36335;&#24452;&#19978;&#25191;&#34892;&#25110;&#36935;&#21040;&#30340;&#34892;&#20026;&#25110;&#29289;&#20307;&#19981;&#19968;&#33268;&#30340;&#21442;&#32771;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#22823;&#22411;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#65292;&#26816;&#27979;&#36825;&#20123;&#24187;&#35273;&#21442;&#32771;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#32988;&#36807;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;&#21253;&#25324;&#20351;&#29992;&#30001;&#35828;&#26126;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#30340;&#35789;&#27010;&#29575;&#20197;&#21450;&#22522;&#20110;LSTM&#21644;Transformer&#30340;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of generating instructions to guide humans to navigate in simulated residential environments. A major issue with current models is hallucination: they generate references to actions or objects that are inconsistent with what a human follower would perform or encounter along the described path. We develop a model that detects these hallucinated references by adopting a model pre-trained on a large corpus of image-text pairs, and fine-tuning it with a contrastive loss that separates correct instructions from instructions containing synthesized hallucinations. Our final model outperforms several baselines, including using word probability estimated by the instruction-generation model, and supervised models based on LSTM and Transformer.
&lt;/p&gt;</description></item><item><title>HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15318</link><description>&lt;p&gt;
HetGPT: &#21033;&#29992;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25552;&#31034;&#35843;&#25972;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15318
&lt;/p&gt;
&lt;p&gt;
HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20026;&#34920;&#31034;&#21644;&#20998;&#26512;Web&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20016;&#23500;&#20449;&#24687;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#20351;&#24471;&#22312;&#32447;&#39029;&#38754;&#20998;&#31867;&#21644;&#31038;&#20132;&#25512;&#33616;&#31561;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#8220;&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;&#8221;&#33539;&#24335;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#23384;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36825;&#31181;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#8220;&#36127;&#36716;&#31227;&#8221;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#20852;&#36215;&#34920;&#26126;&#20102;&#23558;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#8221;&#33539;&#24335;&#24212;&#29992;&#20110;&#22270;&#24418;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;&#25552;&#31034;&#25216;&#26415;&#38024;&#23545;&#30340;&#26159;&#21516;&#36136;&#22270;&#65292;&#24573;&#35270;&#20102;Web&#22270;&#30340;&#20869;&#22312;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HetGPT&#65292;
&lt;/p&gt;
&lt;p&gt;
Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#20110;SAM&#21644;CLIP&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;SAM-CLIP&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2310.15308</link><description>&lt;p&gt;
SAM-CLIP: &#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#35821;&#20041;&#21644;&#31354;&#38388;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#20110;SAM&#21644;CLIP&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;SAM-CLIP&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65288;VFMs&#65289;&#30340;&#39046;&#22495;&#65292;&#22914;CLIP&#21644;Segment Anything Model&#65288;SAM&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#25193;&#22823;&#12290;VFMs&#20855;&#26377;&#28304;&#33258;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#19981;&#21516;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;CLIP&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;SAM&#19987;&#27880;&#20110;&#20998;&#21106;&#30340;&#31354;&#38388;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;VFMs&#39640;&#25928;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21560;&#25910;&#23427;&#20204;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#12290;&#19982;&#20256;&#32479;&#30340;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#27604;&#65292;&#36825;&#31181;&#31574;&#30053;&#20855;&#26377;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#21482;&#38656;&#35201;&#26368;&#21021;&#29992;&#20110;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;SAM&#21644;CLIP&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;SAM-CLIP&#65306;&#23558;SAM&#21644;CLIP&#30340;&#20248;&#21183;&#34701;&#21512;&#20026;&#21333;&#19968;&#20027;&#24178;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise. Our proposed method integrates multi-task learning, continual learning techniques, and teacher-student distillation. This strategy entails significantly less computational cost compared to traditional multi-task training from scratch. Additionally, it only demands a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that amalgamates the strengths of SAM and CLIP into a single backbone, making it apt for ed
&lt;/p&gt;</description></item><item><title>ADMarker&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#33258;&#28982;&#29983;&#27963;&#29615;&#22659;&#20013;&#30417;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#23427;&#20855;&#26377;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#20986;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#20934;&#30830;&#29575;&#21644;&#26089;&#26399;AD&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15301</link><description>&lt;p&gt;
ADMarker: &#19968;&#31181;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#30417;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;
&lt;/p&gt;
&lt;p&gt;
ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer's Disease. (arXiv:2310.15301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15301
&lt;/p&gt;
&lt;p&gt;
ADMarker&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#33258;&#28982;&#29983;&#27963;&#29615;&#22659;&#20013;&#30417;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#23427;&#20855;&#26377;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#20986;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#20934;&#30830;&#29575;&#21644;&#26089;&#26399;AD&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#21450;&#30456;&#20851;&#30196;&#21574;&#30151;&#30001;&#20110;&#20154;&#21475;&#32769;&#40836;&#21270;&#32780;&#25104;&#20026;&#20840;&#29699;&#26085;&#30410;&#20005;&#37325;&#30340;&#20581;&#24247;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ADMarker&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#21644;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#33258;&#28982;&#29983;&#27963;&#29615;&#22659;&#20013;&#26816;&#27979;&#22810;&#32500;AD&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;ADMarker&#20855;&#26377;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#20934;&#30830;&#26816;&#27979;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20849;&#21516;&#35299;&#20915;&#20102;&#25968;&#25454;&#26631;&#31614;&#26377;&#38480;&#12289;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#31561;&#20960;&#20010;&#20027;&#35201;&#30340;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#22810;&#27169;&#24335;&#30828;&#20214;&#31995;&#32479;&#65292;&#24182;&#22312;&#19968;&#20010;&#20026;&#26399;&#22235;&#21608;&#30340;&#20020;&#24202;&#35797;&#39564;&#20013;&#23558;&#20854;&#37096;&#32626;&#22312;91&#21517;&#32769;&#24180;&#21442;&#19982;&#32773;&#36523;&#19978;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ADMarker&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#20986;&#20840;&#38754;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;93.8&#65285;&#65292;&#24182;&#20197;&#24179;&#22343;88.9&#65285;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26089;&#26399;AD&#12290;ADMarker&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#22312;&#30417;&#27979;AD&#24739;&#32773;&#26102;&#25552;&#20379;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease (AD) and related dementia are a growing global health challenge due to the aging population. In this paper, we present ADMarker, the first end-to-end system that integrates multi-modal sensors and new federated learning algorithms for detecting multidimensional AD digital biomarkers in natural living environments. ADMarker features a novel three-stage multi-modal federated learning architecture that can accurately detect digital biomarkers in a privacy-preserving manner. Our approach collectively addresses several major real-world challenges, such as limited data labels, data heterogeneity, and limited computing resources. We built a compact multi-modality hardware system and deployed it in a four-week clinical trial involving 91 elderly participants. The results indicate that ADMarker can accurately detect a comprehensive set of digital biomarkers with up to 93.8% accuracy and identify early AD with an average of 88.9% accuracy. ADMarker offers a new platform that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#23616;&#37096;&#25910;&#25947;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNLCI&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39640;&#20445;&#30495;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#26080;&#31896;&#36229;&#38899;&#36895;&#27969;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15299</link><description>&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#25910;&#25947;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNLCI&#65289;&#29992;&#20110;&#20855;&#26377;&#38750;&#32467;&#26500;&#21270;&#32593;&#26684;&#30340;&#36229;&#38899;&#36895;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Neural Network with Local Converging Input (NNLCI) for Supersonic Flow Problems with Unstructured Grids. (arXiv:2310.15299v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#23616;&#37096;&#25910;&#25947;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNLCI&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39640;&#20445;&#30495;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#26080;&#31896;&#36229;&#38899;&#36895;&#27969;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#19978;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#22788;&#29702;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20195;&#29702;&#27169;&#22411;&#30528;&#37325;&#20110;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#25554;&#20540;&#65292;&#22240;&#27492;&#38656;&#35201;&#36739;&#22823;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#30340;&#39640;&#20445;&#30495;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#23616;&#37096;&#25910;&#25947;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNLCI&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39640;&#20445;&#30495;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23616;&#37096;&#20381;&#36182;&#22495;&#21644;&#25910;&#25947;&#30340;&#31895;&#30053;&#35299;&#20316;&#20026;&#36755;&#20837;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;&#20316;&#20026;&#39564;&#35777;&#26696;&#20363;&#65292;NNLCI&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#30740;&#31350;&#20855;&#26377;&#20984;&#22359;&#30340;&#23548;&#27969;&#36947;&#20013;&#30340;&#26080;&#31896;&#36229;&#38899;&#36895;&#27969;&#12290;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#20984;&#22359;&#20960;&#20309;&#24418;&#29366;&#21644;&#20301;&#32622;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, surrogate models based on deep neural networks (DNN) have been widely used to solve partial differential equations, which were traditionally handled by means of numerical simulations. This kind of surrogate models, however, focuses on global interpolation of the training dataset, and thus requires a large network structure. The process is both time consuming and computationally costly, thereby restricting their use for high-fidelity prediction of complex physical problems. In the present study, we develop a neural network with local converging input (NNLCI) for high-fidelity prediction using unstructured data. The framework utilizes the local domain of dependence with converging coarse solutions as input, which greatly reduces computational resource and training time. As a validation case, the NNLCI method is applied to study inviscid supersonic flows in channels with bumps. Different bump geometries and locations are considered to benchmark the effectiveness and versa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.15290</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#24555;&#36895;&#21487;&#38752;&#22320;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Fast and Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#20016;&#23500;&#30340;&#24739;&#32773;&#32423;&#25968;&#25454;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#39564;&#23460;&#26816;&#39564;&#12289;&#33647;&#29289;&#21644;&#35786;&#26029;&#65292;&#20026;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#38544;&#31169;&#30340;&#25285;&#24551;&#24120;&#24120;&#38480;&#21046;&#20102;&#23545;EHR&#30340;&#35775;&#38382;&#65292;&#38459;&#30861;&#20102;&#19979;&#28216;&#20998;&#26512;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20445;&#25252;&#38544;&#31169;&#30340;EHR&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#19971;&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#26469;&#22686;&#24378;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHRs) are rich sources of patient-level data, including laboratory tests, medications, and diagnoses, offering valuable resources for medical data analysis. However, concerns about privacy often restrict access to EHRs, hindering downstream analysis. Researchers have explored various methods for generating privacy-preserving EHR data. In this study, we introduce a new method for generating diverse and realistic synthetic EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted experiments on six datasets, comparing our proposed method with seven existing methods. Our results demonstrate that our approach significantly outperforms all existing methods in terms of data utility while requiring less training effort. Our approach also enhances downstream medical data analysis by providing diverse and realistic synthetic EHR data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#27169;&#22411;&#20197;&#35299;&#20915;&#22810;&#25945;&#24072;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#39046;&#22495;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#38388;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15288</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Active teacher selection for reinforcement learning from human feedback. (arXiv:2310.15288v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#27169;&#22411;&#20197;&#35299;&#20915;&#22810;&#25945;&#24072;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#39046;&#22495;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#38388;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#26680;&#24515;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#25152;&#26377;&#21453;&#39304;&#37117;&#26469;&#33258;&#19968;&#20010;&#21333;&#19968;&#30340;&#20154;&#31867;&#25945;&#24072;&#65292;&#23613;&#31649;&#38656;&#35201;&#35810;&#38382;&#19981;&#21516;&#25945;&#24072;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"Hidden Utility Bandit"&#65288;HUB&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#25945;&#24072;&#22312;&#29702;&#24615;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24418;&#24335;&#21270;&#20102;&#20174;&#22810;&#20010;&#25945;&#24072;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#35299;&#20915;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#20004;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#65306;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;"Active Teacher Selection"&#65288;ATS&#65289;&#31639;&#27861;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#20309;&#26102;&#20197;&#21450;&#36873;&#25321;&#21738;&#20010;&#25945;&#24072;&#26469;&#26597;&#35810;&#65292;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;HUB&#26694;&#26550;&#21644;ATS&#31639;&#27861;&#23637;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#40065;&#26834;&#22870;&#21169;&#24314;&#27169;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) enables machine learning systems to learn objectives from human feedback. A core limitation of these systems is their assumption that all feedback comes from a single human teacher, despite querying a range of distinct teachers. We propose the Hidden Utility Bandit (HUB) framework to model differences in teacher rationality, expertise, and costliness, formalizing the problem of learning from multiple teachers. We develop a variety of solution algorithms and apply them to two real-world domains: paper recommendation systems and COVID-19 vaccine testing. We find that the Active Teacher Selection (ATS) algorithm outperforms baseline algorithms by actively selecting when and which teacher to query. The HUB framework and ATS algorithm demonstrate the importance of leveraging differences between teachers to learn accurate reward models, facilitating future research on active teacher selection for robust reward modeling.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#21644;&#26032;&#39062;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;&#31639;&#27861;&#23545;&#31232;&#30095;&#21442;&#25968;&#21644;&#26410;&#30693;&#31574;&#30053;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.15286</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#24378;&#21270;&#23398;&#20064;&#30340;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Doubly Robust Approach to Sparse Reinforcement Learning. (arXiv:2310.15286v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15286
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#21644;&#26032;&#39062;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;&#31639;&#27861;&#23545;&#31232;&#30095;&#21442;&#25968;&#21644;&#26410;&#30693;&#31574;&#30053;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#29366;&#24577;-&#36716;&#31227;&#20998;&#24067;&#20026;&#35266;&#27979;&#29305;&#24449;&#30340;&#32447;&#24615;&#20989;&#25968;&#30340;&#21608;&#26399;&#24615;&#31232;&#30095;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#12290;&#20043;&#21069;&#24050;&#30693;&#30340;SMDP&#31639;&#27861;&#38656;&#35201;&#30693;&#36947;&#31232;&#30095;&#21442;&#25968;&#24182;&#33021;&#22815;&#35775;&#38382;&#26410;&#30693;&#31574;&#30053;&#30340;oracle&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;\emph{&#25152;&#26377;}&#21160;&#20316;&#30340;&#29305;&#24449;&#21521;&#37327;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#26159;$\tilde{O}(\sigma^{-1}_{\min} s_{\star} H \sqrt{N})$&#65292;&#20854;&#20013;$\sigma_{\min}$&#34920;&#31034;&#29305;&#24449;&#21521;&#37327;&#30340;&#24179;&#22343;Gram&#30697;&#38453;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#65292;$s_\star$&#26159;&#31232;&#30095;&#21442;&#25968;&#65292;$H$&#26159;&#19968;&#20010;&#21608;&#26399;&#30340;&#38271;&#24230;&#65292;$N$&#26159;&#22238;&#21512;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21305;&#37197;&#19978;&#30028;&#30340;&#36951;&#25022;&#19979;&#30028;&#65292;&#36866;&#29992;&#20110;&#26032;&#35782;&#21035;&#20986;&#30340;SMDP&#23376;&#31867;&#65292;&#23545;&#25968;&#22240;&#23376;&#38500;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new regret minimization algorithm for episodic sparse linear Markov decision process (SMDP) where the state-transition distribution is a linear function of observed features. The only previously known algorithm for SMDP requires the knowledge of the sparsity parameter and oracle access to an unknown policy. We overcome these limitations by combining the doubly robust method that allows one to use feature vectors of \emph{all} actions with a novel analysis technique that enables the algorithm to use data from all periods in all episodes. The regret of the proposed algorithm is $\tilde{O}(\sigma^{-1}_{\min} s_{\star} H \sqrt{N})$, where $\sigma_{\min}$ denotes the restrictive the minimum eigenvalue of the average Gram matrix of feature vectors, $s_\star$ is the sparsity parameter, $H$ is the length of an episode, and $N$ is the number of rounds. We provide a lower regret bound that matches the upper bound up to logarithmic factors on a newly identified subclass of SMDPs. Our
&lt;/p&gt;</description></item><item><title>UncertaintyPlayground&#26159;&#19968;&#27454;&#22522;&#20110;PyTorch&#21644;GPyTorch&#30340;Python&#24211;&#65292;&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#21487;&#35270;&#21270;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#36895;&#24230;&#20248;&#21270;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.15281</link><description>&lt;p&gt;
UncertaintyPlayground: &#19968;&#27454;&#24555;&#36895;&#31616;&#21270;&#30340;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
UncertaintyPlayground: A Fast and Simplified Python Library for Uncertainty Estimation. (arXiv:2310.15281v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15281
&lt;/p&gt;
&lt;p&gt;
UncertaintyPlayground&#26159;&#19968;&#27454;&#22522;&#20110;PyTorch&#21644;GPyTorch&#30340;Python&#24211;&#65292;&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#21487;&#35270;&#21270;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#36895;&#24230;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UncertaintyPlayground&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#21644;GPyTorch&#26500;&#24314;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#24211;&#36890;&#36807;&#31232;&#30095;&#21644;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;SVGPR&#65289;&#29992;&#20110;&#27491;&#24577;&#20998;&#24067;&#32467;&#26524;&#21644;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65288;MDN&#65289;&#29992;&#20110;&#28151;&#21512;&#20998;&#24067;&#65292;&#25552;&#20379;&#20102;&#39640;&#26031;&#21644;&#22810;&#27169;&#24577;&#32467;&#26524;&#20998;&#24067;&#30340;&#24555;&#36895;&#35757;&#32451;&#12290;&#38500;&#20102;&#20351;&#29992;&#19981;&#21516;&#36229;&#21442;&#25968;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#22806;&#65292;UncertaintyPlayground&#36824;&#21487;&#20197;&#21487;&#35270;&#21270;&#19968;&#20010;&#25110;&#22810;&#20010;&#23454;&#20363;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#24352;&#37327;&#25805;&#20316;&#65292;&#35813;&#24211;&#21487;&#20197;&#22312;CPU&#21644;GPU&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;PyTorch&#29305;&#23450;&#30340;&#36895;&#24230;&#20248;&#21270;&#25216;&#26415;&#12290;&#35813;&#24211;&#21253;&#21547;&#27599;&#20010;&#27169;&#22359;&#30340;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;GitHub Workflows&#65288;&#22312;&#32447;&#38598;&#25104;&#65289;&#21644;Tox&#65288;&#26412;&#22320;&#38598;&#25104;&#65289;&#30830;&#20445;&#22810;&#24179;&#21488;&#25345;&#32493;&#38598;&#25104;&#12290;&#26368;&#21518;&#65292;&#20195;&#30721;&#20351;&#29992;&#20102;Google&#39118;&#26684;&#30340;&#25991;&#26723;&#23383;&#31526;&#20018;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;MkDocs&#21644;MkDocStrings&#21019;&#24314;&#30340;&#25991;&#26723;&#32593;&#31449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces UncertaintyPlayground, a Python library built on PyTorch and GPyTorch for uncertainty estimation in supervised learning tasks. The library offers fast training for Gaussian and multi-modal outcome distributions through Sparse and Variational Gaussian Process Regressions (SVGPRs) for normally distributed outcomes and Mixed Density Networks (MDN) for mixed distributions. In addition to model training with various hyperparameters, UncertaintyPlayground can visualize the prediction intervals of one or more instances. Due to using tensor operations, the library can be trained both on CPU and GPU and offers various PyTorch-specific techniques for speed optimization. The library contains unit tests for each module and ensures multi-platform continuous integration with GitHub Workflows (online integration) and Tox (local integration). Finally, the code is documented with Google-style docstrings and offers a documentation website created with MkDocs and MkDocStrings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19977;&#37325;&#21333;&#32431;&#24418;&#30697;&#38453;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#36153;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#39033;&#30446;&#19982;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#36153;&#29992;&#27169;&#24335;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#36153;&#29992;&#65292;&#21516;&#26102;&#28385;&#36275;&#39044;&#31639;&#32422;&#26463;&#24182;&#20445;&#35777;&#39044;&#27979;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15275</link><description>&lt;p&gt;
&#19977;&#37325;&#21333;&#32431;&#24418;&#30697;&#38453;&#23436;&#25104;&#29992;&#20110;&#36153;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Triple Simplex Matrix Completion for Expense Forecasting. (arXiv:2310.15275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19977;&#37325;&#21333;&#32431;&#24418;&#30697;&#38453;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#36153;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#39033;&#30446;&#19982;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#36153;&#29992;&#27169;&#24335;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#36153;&#29992;&#65292;&#21516;&#26102;&#28385;&#36275;&#39044;&#31639;&#32422;&#26463;&#24182;&#20445;&#35777;&#39044;&#27979;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#39033;&#30446;&#36153;&#29992;&#26159;&#20225;&#19994;&#36991;&#20813;&#39044;&#31639;&#36229;&#25903;&#21644;&#39033;&#30446;&#22833;&#36133;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#26159;&#30001;&#36130;&#21153;&#20998;&#26512;&#24072;&#25110;&#25968;&#25454;&#31185;&#23398;&#25216;&#26415;&#65288;&#22914;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65289;&#23436;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20135;&#29983;&#19982;&#35745;&#21010;&#39044;&#31639;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#39033;&#30446;&#24320;&#22987;&#26102;&#25968;&#25454;&#28857;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#32422;&#26463;&#30340;&#38750;&#36127;&#30697;&#38453;&#23436;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39033;&#30446;&#19982;&#28508;&#22312;&#31354;&#38388;&#20013;&#26576;&#20123;&#36153;&#29992;&#27169;&#24335;&#30340;&#30456;&#20851;&#24615;&#65292;&#39044;&#27979;&#36153;&#29992;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#27169;&#22411;&#22312;&#22240;&#23376;&#30697;&#38453;&#21644;&#32570;&#22833;&#26465;&#30446;&#19978;&#21463;&#21040;&#19977;&#20010;&#27010;&#29575;&#21333;&#32431;&#24418;&#30340;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#30340;&#36153;&#29992;&#20540;&#20445;&#35777;&#28385;&#36275;&#39044;&#31639;&#32422;&#26463;&#65292;&#26080;&#38656;&#21518;&#22788;&#29702;&#12290;&#19968;&#20010;&#38750;&#31934;&#30830;&#30340;&#20132;&#26367;&#20248;&#21270;&#31639;&#27861;&#34987;&#24320;&#21457;&#29992;&#20110;&#35299;&#20915;&#30456;&#20851;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting project expenses is a crucial step for businesses to avoid budget overruns and project failures. Traditionally, this has been done by financial analysts or data science techniques such as time-series analysis. However, these approaches can be uncertain and produce results that differ from the planned budget, especially at the start of a project with limited data points. This paper proposes a constrained non-negative matrix completion model that predicts expenses by learning the likelihood of the project correlating with certain expense patterns in the latent space. The model is constrained on three probability simplexes, two of which are on the factor matrices and the third on the missing entries. Additionally, the predicted expense values are guaranteed to meet the budget constraint without the need of post-processing. An inexact alternating optimization algorithm is developed to solve the associated optimization problem and is proven to converge to a stationary point. Res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#38754;&#20020;&#33021;&#28304;&#12289;&#23545;&#40784;&#21644;&#20174;&#29421;&#20041;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#19977;&#22823;&#25361;&#25112;&#30340;&#31995;&#32479;&#21270;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#33021;&#28304;&#28040;&#32791;&#12289;&#31995;&#32479;&#35774;&#35745;&#21644;&#23545;&#40784;&#38382;&#39064;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#31995;&#32479;&#35774;&#35745;&#22312;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.15274</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;AGI&#65306;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Systematic AI Approach for AGI: Addressing Alignment, Energy, and AGI Grand Challenges. (arXiv:2310.15274v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#38754;&#20020;&#33021;&#28304;&#12289;&#23545;&#40784;&#21644;&#20174;&#29421;&#20041;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#19977;&#22823;&#25361;&#25112;&#30340;&#31995;&#32479;&#21270;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#33021;&#28304;&#28040;&#32791;&#12289;&#31995;&#32479;&#35774;&#35745;&#21644;&#23545;&#40784;&#38382;&#39064;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#31995;&#32479;&#35774;&#35745;&#22312;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30528;&#19977;&#22823;&#25361;&#25112;&#65306;&#33021;&#28304;&#22721;&#22418;&#12289;&#23545;&#40784;&#38382;&#39064;&#21644;&#20174;&#29421;&#20041;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#39134;&#36291;&#12290;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#26085;&#24120;&#36816;&#34892;&#36807;&#31243;&#20013;&#28040;&#32791;&#30528;&#19981;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#33258;2020&#24180;&#20197;&#26469;&#65292;&#27599;&#20010;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#27599;&#20004;&#20010;&#26376;&#23601;&#32763;&#20493;&#65292;&#30452;&#25509;&#23548;&#33268;&#33021;&#28304;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;&#20174;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#39134;&#36291;&#38656;&#35201;&#22810;&#20010;&#21151;&#33021;&#23376;&#31995;&#32479;&#20197;&#24179;&#34913;&#30340;&#26041;&#24335;&#36816;&#20316;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#32570;&#20047;&#31995;&#32479;&#35774;&#35745;&#65307;&#21363;&#20351;&#31995;&#32479;&#29305;&#24449;&#22312;&#20154;&#33041;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#20174;&#23427;&#22788;&#29702;&#20449;&#24687;&#30340;&#26041;&#24335;&#21040;&#23427;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;&#21516;&#26679;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#21644;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#31995;&#32479;&#35774;&#35745;&#65292;&#28982;&#32780;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#33041;&#30340;&#31995;&#32479;&#26550;&#26500;&#22312;&#20581;&#24247;&#30340;&#36947;&#24503;&#20915;&#31574;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#31995;&#32479;&#35774;&#35745;&#22312;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI faces a trifecta of grand challenges the Energy Wall, the Alignment Problem and the Leap from Narrow AI to AGI. Contemporary AI solutions consume unsustainable amounts of energy during model training and daily operations.Making things worse, the amount of computation required to train each new AI model has been doubling every 2 months since 2020, directly translating to increases in energy consumption.The leap from AI to AGI requires multiple functional subsystems operating in a balanced manner, which requires a system architecture. However, the current approach to artificial intelligence lacks system design; even though system characteristics play a key role in the human brain from the way it processes information to how it makes decisions. Similarly, current alignment and AI ethics approaches largely ignore system design, yet studies show that the brains system architecture plays a critical role in healthy moral decisions.In this paper, we argue that system design is critically im
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#20998;&#32452;&#26041;&#27861;&#65292;&#21517;&#20026;GradSim&#12290;&#23427;&#36890;&#36807;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35821;&#35328;&#38598;&#21512;&#36827;&#34892;&#22810;&#35821;&#35328;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#36127;&#38754;&#24178;&#25200;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#20027;&#39064;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.15269</link><description>&lt;p&gt;
GradSim: &#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#20998;&#32452;&#26041;&#27861;&#29992;&#20110;&#26377;&#25928;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GradSim: Gradient-Based Language Grouping for Effective Multilingual Training. (arXiv:2310.15269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15269
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#20998;&#32452;&#26041;&#27861;&#65292;&#21517;&#20026;GradSim&#12290;&#23427;&#36890;&#36807;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35821;&#35328;&#38598;&#21512;&#36827;&#34892;&#22810;&#35821;&#35328;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#36127;&#38754;&#24178;&#25200;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#20027;&#39064;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#22823;&#22810;&#25968;&#35821;&#35328;&#37117;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#20302;&#36164;&#28304;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22810;&#35821;&#35328;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#26159;&#25152;&#26377;&#35821;&#35328;&#37117;&#33021;&#20114;&#30456;&#31215;&#26497;&#22320;&#24433;&#21709;&#65292;&#22914;&#20309;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35821;&#35328;&#38598;&#21512;&#36827;&#34892;&#22810;&#35821;&#35328;&#35757;&#32451;&#65292;&#24182;&#36991;&#20813;&#37027;&#20123;&#29305;&#24449;&#25110;&#25968;&#25454;&#20998;&#24067;&#19981;&#20860;&#23481;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36127;&#38754;&#24178;&#25200;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#20998;&#32452;&#26041;&#27861;&#65292;&#31216;&#20026;GradSim&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;GradSim&#21487;&#20197;&#24102;&#26469;&#26368;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19982;&#36328;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#22320;&#30456;&#20851;&#12290;&#32467;&#26524;&#26159;&#65292;&#25105;&#20204;&#22312; AfriSenti &#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#24191;&#27867;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#38500;&#35821;&#35328;&#29305;&#24449;&#22806;&#65292;&#25968;&#25454;&#38598;&#30340;&#20027;&#39064;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of languages for multilingual training and avoid negative interference among languages whose characteristics or data distributions are not compatible. In this paper, we propose GradSim, a language grouping method based on gradient similarity. Our experiments on three diverse multilingual benchmark datasets show that it leads to the largest performance gains compared to other similarity measures and it is better correlated with cross-lingual model performance. As a result, we set the new state of the art on AfriSenti, a benchmark dataset for sentiment analysis on low-resource African languages. In our extensive analysis, we further reveal that besides linguistic features, the topics of the datase
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#20449;&#24687;&#39537;&#21160;&#30340;&#29366;&#24577;&#20999;&#25442;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#21487;&#20197;&#25581;&#31034;&#21160;&#24577;&#21464;&#21270;&#30340;&#21151;&#33021;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#23545;&#28508;&#22312;&#30340;&#35299;&#21078;&#36830;&#36890;&#22270;&#26377;&#25152;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.15263</link><description>&lt;p&gt;
&#29992;&#20110;&#21457;&#29616;&#20999;&#25442;&#33041;&#29366;&#24577;&#30340;&#29420;&#28909;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
One-hot Generalized Linear Model for Switching Brain State Discovery. (arXiv:2310.15263v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#20449;&#24687;&#39537;&#21160;&#30340;&#29366;&#24577;&#20999;&#25442;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#21487;&#20197;&#25581;&#31034;&#21160;&#24577;&#21464;&#21270;&#30340;&#21151;&#33021;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#23545;&#28508;&#22312;&#30340;&#35299;&#21078;&#36830;&#36890;&#22270;&#26377;&#25152;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#22238;&#36335;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#31070;&#32463;&#20449;&#21495;&#25512;&#26029;&#20986;&#30340;&#31070;&#32463;&#30456;&#20114;&#20316;&#29992;&#20027;&#35201;&#21453;&#26144;&#21151;&#33021;&#24615;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#38271;&#26102;&#38388;&#30340;&#23454;&#39564;&#20013;&#65292;&#23454;&#39564;&#23545;&#35937;&#21160;&#29289;&#21487;&#33021;&#20250;&#32463;&#21382;&#30001;&#23454;&#39564;&#12289;&#21050;&#28608;&#25110;&#34892;&#20026;&#29366;&#24577;&#23450;&#20041;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#22240;&#27492;&#21151;&#33021;&#24615;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#20026;&#20102;&#24314;&#27169;&#21160;&#24577;&#21464;&#21270;&#30340;&#21151;&#33021;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#37319;&#29992;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#29366;&#24577;&#20999;&#25442;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;&#21363;HMM-GLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#29289;&#23398;&#21487;&#20449;&#24230;&#19978;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22240;&#20026;&#21151;&#33021;&#24615;&#30456;&#20114;&#20316;&#29992;&#26159;&#21463;&#21040;&#28508;&#22312;&#35299;&#21078;&#36830;&#36890;&#22270;&#30340;&#24433;&#21709;&#21644;&#38480;&#21046;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20808;&#39564;&#20449;&#24687;&#39537;&#21160;&#30340;&#29366;&#24577;&#20999;&#25442;GLM&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#29366;&#24577;&#19979;&#24341;&#20837;&#20102;&#39640;&#26031;&#20808;&#39564;&#21644;&#29420;&#28909;&#20808;&#39564;&#12290;&#36825;&#20123;&#20808;&#39564;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#24212;&#35813;&#25429;&#25417;&#21040;&#29366;&#24577;&#24658;&#23450;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25581;&#31034;&#28508;&#22312;&#30340;&#35299;&#21078;&#36830;&#36890;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional interactions. In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional interactions can change over time. To model dynamically changing functional interactions, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs). However, we argue they lack biological plausibility, as functional interactions are shaped and confined by the underlying anatomical connectome. Here, we propose a novel prior-informed state-switching GLM. We introduce both a Gaussian prior and a one-hot prior over the GLM in each state. The priors are learnable. We will show that the learned prior should capture the state-constant interaction, shedding light on the underlying anatomical connectome and rev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#22788;&#29702;&#27169;&#24577;&#20002;&#22833;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23558;&#38901;&#24459;&#29305;&#24449;&#19982;&#35821;&#35328;&#32447;&#32034;&#32467;&#21512;&#20351;&#29992;&#65292;&#25552;&#39640;&#20102;DDSD&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15261</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#27169;&#24577;&#20002;&#22833;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Modality Dropout for Multimodal Device Directed Speech Detection using Verbal and Non-Verbal Features. (arXiv:2310.15261v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#22788;&#29702;&#27169;&#24577;&#20002;&#22833;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23558;&#38901;&#24459;&#29305;&#24449;&#19982;&#35821;&#35328;&#32447;&#32034;&#32467;&#21512;&#20351;&#29992;&#65292;&#25552;&#39640;&#20102;DDSD&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#23548;&#21521;&#30340;&#35821;&#38899;&#35782;&#21035;&#65288;DDSD&#65289;&#26159;&#19968;&#31181;&#23558;&#38382;&#39064;&#21306;&#20998;&#20026;&#38024;&#23545;&#35821;&#38899;&#21161;&#25163;&#30340;&#26597;&#35810;&#21644;&#26049;&#30333;&#25110;&#32972;&#26223;&#35821;&#38899;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;DDSD&#31995;&#32479;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65288;&#22914;&#22768;&#23398;&#29305;&#24449;&#12289;&#25991;&#26412;&#21644;/&#25110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#29305;&#24449;&#65289;&#26469;&#23558;&#35821;&#38899;&#20998;&#31867;&#20026;&#35774;&#22791;&#23548;&#21521;&#25110;&#20854;&#20182;&#31867;&#22411;&#65292;&#24182;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#24448;&#24448;&#35201;&#22788;&#29702;&#20854;&#20013;&#19968;&#31181;&#25110;&#22810;&#31181;&#27169;&#24577;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#22815;&#23545;&#32570;&#22833;&#27169;&#24577;&#26356;&#21152;&#31283;&#20581;&#30340;DDSD&#31995;&#32479;&#30340;&#34701;&#21512;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;DDSD&#20013;&#23558;&#38750;&#35821;&#35328;&#32447;&#32034;&#65288;&#20855;&#20307;&#26159;&#38901;&#24459;&#29305;&#24449;&#65289;&#19982;&#35821;&#35328;&#32447;&#32034;&#32467;&#21512;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#38750;&#32447;&#24615;&#20013;&#38388;&#34701;&#21512;&#30340;&#26041;&#24335;&#65292;&#23558;&#38901;&#24459;&#29305;&#24449;&#30340;&#20998;&#25968;&#21644;&#23884;&#20837;&#19982;&#23545;&#24212;&#30340;&#35821;&#35328;&#32447;&#32034;&#32467;&#21512;&#36215;&#26469;&#65292;&#21457;&#29616;&#38901;&#24459;&#29305;&#24449;&#21487;&#20197;&#20351;DDSD&#24615;&#33021;&#22312;&#32473;&#23450;&#22266;&#23450;&#25805;&#20316;&#28857;&#19978;&#30340;&#35823;&#25509;&#21463;&#29575;&#65288;FA&#65289;&#25552;&#39640;&#22810;&#36798;8.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Device-directed speech detection (DDSD) is the binary classification task of distinguishing between queries directed at a voice assistant versus side conversation or background speech. State-of-the-art DDSD systems use verbal cues, e.g acoustic, text and/or automatic speech recognition system (ASR) features, to classify speech as device-directed or otherwise, and often have to contend with one or more of these modalities being unavailable when deployed in real-world settings. In this paper, we investigate fusion schemes for DDSD systems that can be made more robust to missing modalities. Concurrently, we study the use of non-verbal cues, specifically prosody features, in addition to verbal cues for DDSD. We present different approaches to combine scores and embeddings from prosody with the corresponding verbal cues, finding that prosody improves DDSD performance by upto 8.5% in terms of false acceptance rate (FA) at a given fixed operating point via non-linear intermediate fusion, whil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22122;&#22768;&#29615;&#22659;&#20013;&#38382;&#39064;&#32763;&#35793;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#28304;&#35821;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#32763;&#35793;&#38382;&#39064;&#30340;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.15259</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#39046;&#22495;&#33258;&#36866;&#24212;&#32763;&#35793;&#24102;&#26377;&#38382;&#39064;&#29305;&#23450;&#22870;&#21169;&#30340;&#22122;&#22768;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards. (arXiv:2310.15259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15259
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22122;&#22768;&#29615;&#22659;&#20013;&#38382;&#39064;&#32763;&#35793;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#28304;&#35821;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#32763;&#35793;&#38382;&#39064;&#30340;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;(CQA)&#24179;&#21488;&#26159;&#24110;&#21161;&#32452;&#32455;&#20869;&#29992;&#25143;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20351;&#23427;&#20204;&#23545;&#38750;&#33521;&#35821;&#29992;&#25143;&#21487;&#35775;&#38382;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#32763;&#35793;&#38382;&#39064;&#21487;&#20197;&#25299;&#23485;&#31038;&#21306;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#20351;&#26377;&#31867;&#20284;&#38382;&#39064;&#30340;&#20154;&#33021;&#22815;&#21463;&#30410;&#65292;&#20294;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#36827;&#34892;&#38382;&#39064;&#32763;&#35793;&#20250;&#38754;&#20020;&#26356;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#35821;&#27861;&#27491;&#30830;&#24615;&#27809;&#26377;&#21463;&#21040;&#30417;&#25511;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#34987;&#38750;&#27597;&#35821;&#29992;&#25143;&#20197;&#38472;&#36848;&#21477;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#20855;&#26377;&#19981;&#27491;&#30830;&#30340;&#20027;&#35859;&#35821;&#24207;&#65292;&#29978;&#33267;&#26377;&#26102;&#32570;&#23569;&#38382;&#21495;&#12290;&#30001;&#20110;&#25968;&#25454;&#23384;&#22312;&#22122;&#22768;&#65292;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#21019;&#24314;&#19968;&#20010;&#21512;&#25104;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20063;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#28304;&#35821;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;BERTScore&#21644;Masked Language Model (MLM) S&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24179;&#34913;&#20102;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question-Answering (CQA) portals serve as a valuable tool for helping users within an organization. However, making them accessible to non-English-speaking users continues to be a challenge. Translating questions can broaden the community's reach, benefiting individuals with similar inquiries in various languages. Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is not monitored. These questions may be phrased as statements by non-native speakers, with incorrect subject-verb order and sometimes even missing question marks. Creating a synthetic parallel corpus from such data is also difficult due to its noisy nature. To address this issue, we propose a training methodology that fine-tunes the NMT system only using source-side data. Our approach balances adequacy and fluency by utilizing a loss function that combines BERTScore and Masked Language Model (MLM) S
&lt;/p&gt;</description></item><item><title>SimBIG&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26143;&#31995;&#22242;&#38598;&#32676;&#30340;&#22330;&#26223;&#32423;&#20998;&#26512;&#65292;&#21033;&#29992;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#29305;&#24615;&#65292;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20005;&#26684;&#30340;&#20851;&#20110;&#23431;&#23449;&#21442;&#25968;&#21644;&#21704;&#21187;&#24120;&#37327;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.15256</link><description>&lt;p&gt;
SimBIG: &#22522;&#20110;&#22330;&#26223;&#30340;&#23431;&#23449;&#26143;&#31995;&#22242;&#38598;&#32676;&#27169;&#25311;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
SimBIG: Field-level Simulation-Based Inference of Galaxy Clustering. (arXiv:2310.15256v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15256
&lt;/p&gt;
&lt;p&gt;
SimBIG&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26143;&#31995;&#22242;&#38598;&#32676;&#30340;&#22330;&#26223;&#32423;&#20998;&#26512;&#65292;&#21033;&#29992;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#29305;&#24615;&#65292;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20005;&#26684;&#30340;&#20851;&#20110;&#23431;&#23449;&#21442;&#25968;&#21644;&#21704;&#21187;&#24120;&#37327;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#36890;&#36807;&#23545;&#26143;&#31995;&#22242;&#38598;&#32676;&#30340;&#22330;&#26223;&#32423;&#20998;&#26512;&#36827;&#34892;&#23431;&#23449;&#21442;&#25968;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#65288;SBI&#65289;&#26041;&#27861;&#12290;&#26631;&#20934;&#30340;&#26143;&#31995;&#22242;&#38598;&#32676;&#20998;&#26512;&#20381;&#36182;&#20110;&#20998;&#26512;&#27719;&#24635;&#32479;&#35745;&#37327;&#65292;&#22914;&#21151;&#29575;&#35889;$P_\ell$&#65292;&#24182;&#22522;&#20110;&#25668;&#21160;&#29702;&#35770;&#30340;&#35299;&#26512;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26143;&#31995;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;SimBIG&#21069;&#21521;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;SBI&#12290;&#25105;&#20204;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;&#21270;&#23545;BOSS CMASS&#26143;&#31995;&#26679;&#26412;&#30340;&#19968;&#20010;&#23376;&#38598;&#36827;&#34892;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21387;&#32553;&#12290;&#25105;&#20204;&#23545;$\Omega_m$&#21462;&#20540;&#32422;&#20026;0.267&#65288;&#19978;&#38480;+0.033&#65292;&#19979;&#38480;-0.029&#65289;&#65292;$\sigma_8$&#21462;&#20540;&#32422;&#20026;0.762&#65288;&#19978;&#38480;+0.036&#65292;&#19979;&#38480;-0.035&#65289;&#36827;&#34892;&#25512;&#26029;&#12290;&#34429;&#28982;&#25105;&#20204;&#23545;$\Omega_m$&#30340;&#32422;&#26463;&#19982;&#26631;&#20934;&#30340;$P_\ell$&#20998;&#26512;&#19968;&#33268;&#65292;&#20294;&#23545;$\sigma_8$&#30340;&#32422;&#26463;&#26356;&#20026;&#20005;&#26684;&#65292;&#32422;&#26463;&#33539;&#22260;&#20026;$2.65$&#20493;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#32473;&#20986;&#20102;&#21704;&#21187;&#24120;&#37327;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first simulation-based inference (SBI) of cosmological parameters from field-level analysis of galaxy clustering. Standard galaxy clustering analyses rely on analyzing summary statistics, such as the power spectrum, $P_\ell$, with analytic models based on perturbation theory. Consequently, they do not fully exploit the non-linear and non-Gaussian features of the galaxy distribution. To address these limitations, we use the {\sc SimBIG} forward modelling framework to perform SBI using normalizing flows. We apply SimBIG to a subset of the BOSS CMASS galaxy sample using a convolutional neural network with stochastic weight averaging to perform massive data compression of the galaxy field. We infer constraints on $\Omega_m = 0.267^{+0.033}_{-0.029}$ and $\sigma_8=0.762^{+0.036}_{-0.035}$. While our constraints on $\Omega_m$ are in-line with standard $P_\ell$ analyses, those on $\sigma_8$ are $2.65\times$ tighter. Our analysis also provides constraints on the Hubble constant 
&lt;/p&gt;</description></item><item><title>SyncFusion&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21516;&#27493;&#30340;&#38899;&#35270;&#39057;&#38669;&#21033;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#37325;&#22797;&#30340;&#21160;&#20316;&#36215;&#22987;&#28857;&#65292;&#24182;&#20351;&#29992;&#38899;&#39057;&#25110;&#25991;&#26412;&#23884;&#20837;&#26469;&#29983;&#25104;&#26032;&#30340;&#21516;&#27493;&#38899;&#25928;&#38899;&#36712;&#65292;&#20026;&#22768;&#38899;&#35774;&#35745;&#24072;&#25552;&#20379;&#20102;&#23436;&#20840;&#30340;&#21019;&#20316;&#25511;&#21046;&#26435;&#24182;&#31616;&#21270;&#20102;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.15247</link><description>&lt;p&gt;
SyncFusion:&#22810;&#27169;&#24577;&#21516;&#27493;&#38899;&#35270;&#39057;&#38669;&#21033;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis. (arXiv:2310.15247v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15247
&lt;/p&gt;
&lt;p&gt;
SyncFusion&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21516;&#27493;&#30340;&#38899;&#35270;&#39057;&#38669;&#21033;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#37325;&#22797;&#30340;&#21160;&#20316;&#36215;&#22987;&#28857;&#65292;&#24182;&#20351;&#29992;&#38899;&#39057;&#25110;&#25991;&#26412;&#23884;&#20837;&#26469;&#29983;&#25104;&#26032;&#30340;&#21516;&#27493;&#38899;&#25928;&#38899;&#36712;&#65292;&#20026;&#22768;&#38899;&#35774;&#35745;&#24072;&#25552;&#20379;&#20102;&#23436;&#20840;&#30340;&#21019;&#20316;&#25511;&#21046;&#26435;&#24182;&#31616;&#21270;&#20102;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#35774;&#35745;&#28041;&#21450;&#21040;&#20026;&#30005;&#24433;&#12289;&#35270;&#39057;&#28216;&#25103;&#21644;&#34394;&#25311;/&#22686;&#24378;&#29616;&#23454;&#31561;&#21508;&#31181;&#23186;&#20307;&#36873;&#25321;&#12289;&#24405;&#21046;&#21644;&#32534;&#36753;&#38899;&#25928;&#12290;&#22312;&#35774;&#35745;&#22768;&#38899;&#26102;&#65292;&#20854;&#20013;&#19968;&#20010;&#26368;&#32791;&#26102;&#30340;&#27493;&#39588;&#26159;&#23558;&#38899;&#39057;&#19982;&#35270;&#39057;&#21516;&#27493;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21033;&#29992;&#35270;&#39057;&#25293;&#25668;&#30340;&#29615;&#22659;&#24405;&#38899;&#26469;&#24110;&#21161;&#27492;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#39057;&#28216;&#25103;&#21644;&#21160;&#30011;&#20013;&#65292;&#27809;&#26377;&#21442;&#32771;&#38899;&#39057;&#23384;&#22312;&#65292;&#38656;&#35201;&#20174;&#35270;&#39057;&#20013;&#25163;&#21160;&#27880;&#37322;&#20107;&#20214;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#37325;&#22797;&#21160;&#20316;&#30340;&#36215;&#22987;&#28857;&#65292;&#28982;&#21518;&#19982;&#38899;&#39057;&#25110;&#25991;&#26412;&#23884;&#20837;&#19968;&#36215;&#65292;&#29992;&#20110;&#26465;&#20214;&#21270;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#21516;&#27493;&#38899;&#25928;&#38899;&#36712;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;&#23436;&#20840;&#30340;&#21019;&#20316;&#25511;&#21046;&#26435;&#20132;&#32473;&#20102;&#22768;&#38899;&#35774;&#35745;&#24072;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#19982;&#35270;&#39057;&#21516;&#27493;&#30340;&#36127;&#25285;&#12290;&#27492;&#22806;&#65292;&#32534;&#36753;&#36215;&#22987;&#36712;&#36947;&#25110;&#26356;&#25913;&#26465;&#20214;&#23884;&#20837;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#35201;&#27604;&#32534;&#36753;&#38899;&#39057;&#36712;&#36947;&#26412;&#36523;&#35201;&#23569;&#24471;&#22810;&#65292;&#31616;&#21270;&#20102;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound design involves creatively selecting, recording, and editing sound effects for various media like cinema, video games, and virtual/augmented reality. One of the most time-consuming steps when designing sound is synchronizing audio with video. In some cases, environmental recordings from video shoots are available, which can aid in the process. However, in video games and animations, no reference audio exists, requiring manual annotation of event timings from the video. We propose a system to extract repetitive actions onsets from a video, which are then used - in conjunction with audio or textual embeddings - to condition a diffusion model trained to generate a new synchronized sound effects audio track. In this way, we leave complete creative control to the sound designer while removing the burden of synchronization with video. Furthermore, editing the onset track or changing the conditioning embedding requires much less effort than editing the audio track itself, simplifying th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#25512;&#26029;&#20986;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#35266;&#27979;&#21463;&#21040;&#30340;&#31995;&#32479;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.15234</link><description>&lt;p&gt;
&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#65306;&#31995;&#32479;&#25928;&#24212;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Field-level simulation-based inference with galaxy catalogs: the impact of systematic effects. (arXiv:2310.15234v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#25512;&#26029;&#20986;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#35266;&#27979;&#21463;&#21040;&#30340;&#31995;&#32479;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#31181;&#20174;&#26143;&#31995;&#32418;&#31227;&#35843;&#26597;&#20013;&#38480;&#21046;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#26080;&#20284;&#28982;&#25512;&#26029;&#65292;&#32780;&#19981;&#23545;&#23610;&#24230;&#36827;&#34892;&#21098;&#20999;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24503;&#26705;&#33922;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24320;&#21457;&#20102;&#33021;&#22815;&#20934;&#30830;&#25512;&#26029;&#20986;&#36890;&#36807;&#20165;&#21253;&#21547;&#26143;&#31995;&#20301;&#32622;&#21644;&#24452;&#21521;&#36895;&#24230;&#30340;&#30446;&#24405;&#26469;&#30830;&#23450;$\Omega_{\rm m}$&#20540;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#23545;&#22825;&#20307;&#29289;&#29702;&#21644;&#20122;&#32593;&#26684;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#35266;&#27979;&#21463;&#21040;&#35768;&#22810;&#25928;&#24212;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;1&#65289;&#25513;&#34109;&#25928;&#24212;&#65292;2&#65289;&#29305;&#24322;&#36895;&#24230;&#21644;&#24452;&#21521;&#36317;&#31163;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;3&#65289;&#19981;&#21516;&#30340;&#26143;&#31995;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#35266;&#27979;&#21482;&#20801;&#35768;&#25105;&#20204;&#27979;&#37327;&#32418;&#31227;&#65292;&#32416;&#32544;&#20102;&#26143;&#31995;&#30340;&#24452;&#21521;&#20301;&#32622;&#21644;&#36895;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;CAMELS&#39033;&#30446;&#20013;&#19981;&#21516;&#20195;&#30721;&#36816;&#34892;&#30340;&#26368;&#26032;&#27700;&#21160;&#21147;&#23398;&#27169;&#25311;&#29983;&#25104;&#30340;&#26143;&#31995;&#30446;&#24405;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#25311;&#32771;&#34385;&#20102;&#36825;&#20123;&#35266;&#27979;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been recently shown that a powerful way to constrain cosmological parameters from galaxy redshift surveys is to train graph neural networks to perform field-level likelihood-free inference without imposing cuts on scale. In particular, de Santi et al. (2023) developed models that could accurately infer the value of $\Omega_{\rm m}$ from catalogs that only contain the positions and radial velocities of galaxies that are robust to uncertainties in astrophysics and subgrid models. However, observations are affected by many effects, including 1) masking, 2) uncertainties in peculiar velocities and radial distances, and 3) different galaxy selections. Moreover, observations only allow us to measure redshift, intertwining galaxies' radial positions and velocities. In this paper we train and test our models on galaxy catalogs, created from thousands of state-of-the-art hydrodynamic simulations run with different codes from the CAMELS project, that incorporate these observational effect
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#20013;&#21253;&#21547;&#39640;&#27425;&#35856;&#27874;&#27169;&#24335;&#65292;&#21033;&#29992;&#24341;&#21147;&#27874;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#65292;&#21487;&#20197;&#22823;&#24133;&#24230;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#25628;&#32034;&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#28789;&#25935;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.15233</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#26041;&#27861;&#29992;&#20110;&#24102;&#26377;&#26356;&#39640;&#27425;&#35856;&#27874;&#30340;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#65306;&#36890;&#36807;&#21313;&#20493;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude. (arXiv:2310.15233v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#20013;&#21253;&#21547;&#39640;&#27425;&#35856;&#27874;&#27169;&#24335;&#65292;&#21033;&#29992;&#24341;&#21147;&#27874;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#65292;&#21487;&#20197;&#22823;&#24133;&#24230;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#25628;&#32034;&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#28789;&#25935;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#25628;&#32034;&#20351;&#29992;&#20449;&#21495;&#27169;&#22411;&#25110;&#27169;&#26495;&#12290;&#30446;&#21069;&#22312;LIGO-Virgo-Kagra (LVK)&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#27169;&#26495;&#20165;&#27169;&#25311;&#20102;&#20449;&#21495;&#30340;&#20027;&#23548;&#22235;&#26497;&#27169;&#24335;$(\ell,m)=(2,2)$&#65292;&#24573;&#30053;&#20102;&#27425;&#35201;&#30340;&#39640;&#38454;&#27169;&#24335;(HM)&#20363;&#22914;$(\ell,m)=(3,3)$&#65292;$(4,4)$&#65292;&#36825;&#20123;&#27169;&#24335;&#26159;&#30001;&#24191;&#20041;&#30456;&#23545;&#35770;&#39044;&#27979;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25628;&#32034;&#21487;&#33021;&#20250;&#22312;&#21442;&#25968;&#31354;&#38388;&#30340;&#19968;&#20123;&#26377;&#36259;&#21306;&#22495;&#65292;&#22914;&#39640;&#36136;&#37327;&#21644;&#38750;&#23545;&#31216;&#36136;&#37327;&#27604;&#30340;&#31995;&#32479;&#20013;&#22833;&#21435;&#23545;&#40657;&#27934;&#21512;&#24182;&#30340;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#23558;HM&#21253;&#21547;&#22312;&#27169;&#26495;&#24211;&#20013;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#29275;&#39039;&#38468;&#21152;&#20844;&#24335;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#27169;&#25311;&#19982;&#32473;&#23450;$(2,2)$&#27874;&#24418;&#30456;&#23545;&#24212;&#30340;&#33258;&#26059;&#23545;&#40784;&#30340;$(3,3)$&#65292;$(4,4)$&#27874;&#24418;&#12290;&#21487;&#20197;&#23545;&#27599;&#20010;&#27169;&#24335;&#30340;&#25968;&#25454;&#36827;&#34892;&#21333;&#29420;&#28388;&#27874;&#65292;&#24471;&#21040;&#20449;&#22122;&#27604;(SNR)&#30340;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#65292;&#28982;&#21518;&#21487;&#20197;&#20197;&#30456;&#23545;&#24265;&#20215;&#30340;&#26041;&#24335;&#23558;&#20854;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searches for gravitational wave events use models, or templates, for the signals of interest. The templates used in current searches in the LIGO-Virgo-Kagra (LVK) data model the dominant quadrupole mode $(\ell,m)=(2,2)$ of the signals, and omit sub-dominant higher-order modes (HM) such as $(\ell,m)=(3,3)$, $(4,4)$, which are predicted by general relativity. Hence, these searches could lose sensitivity to black hole mergers in interesting parts of parameter space, such as systems with high-masses and asymmetric mass ratios. We develop a new strategy to include HM in template banks that exploits the natural connection between the modes. We use a combination of post-Newtonian formulae and machine learning tools to model aligned-spin $(3,3)$, $(4,4)$ waveforms corresponding to a given $(2,2)$ waveform. Each of these modes can be individually filtered against the data to yield separate timeseries of signal-to-noise ratios (SNR), which can be combined in a relatively inexpensive way to margi
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#21521;&#37327;&#12290;&#36825;&#20123;&#20989;&#25968;&#21521;&#37327;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36824;&#20855;&#26377;&#23558;&#35821;&#20041;&#21521;&#37327;&#36827;&#34892;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20989;&#25968;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15213
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#21521;&#37327;&#12290;&#36825;&#20123;&#20989;&#25968;&#21521;&#37327;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36824;&#20855;&#26377;&#23558;&#35821;&#20041;&#21521;&#37327;&#36827;&#34892;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#33258;&#22238;&#24402;&#21464;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#21521;&#37327;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20219;&#21153;&#19978;&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#23569;&#25968;&#27880;&#24847;&#21147;&#22836;&#20256;&#36755;&#20102;&#23637;&#31034;&#20219;&#21153;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20989;&#25968;&#21521;&#37327;&#65288;FV&#65289;&#12290;FV&#23545;&#19978;&#19979;&#25991;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21363;&#23427;&#20204;&#22312;&#19981;&#31867;&#20284;&#20110;&#20854;&#25910;&#38598;&#26102;&#30340;ICL&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#35302;&#21457;&#23545;&#36755;&#20837;&#30340;&#20219;&#21153;&#25191;&#34892;&#65292;&#20363;&#22914;&#38646;&#26679;&#26412;&#21644;&#33258;&#28982;&#25991;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#23618;&#19978;&#27979;&#35797;&#20102;FV&#65292;&#24182;&#22312;&#20013;&#23618;&#21457;&#29616;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;FV&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#24182;&#21457;&#29616;&#34429;&#28982;&#23427;&#20204;&#36890;&#24120;&#21253;&#21547;&#32534;&#30721;&#20989;&#25968;&#30340;&#36755;&#20986;&#31354;&#38388;&#30340;&#20449;&#24687;&#65292;&#20294;&#20165;&#27492;&#20449;&#24687;&#26080;&#27861;&#37325;&#26500;FV&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;FV&#20013;&#30340;&#35821;&#20041;&#21521;&#37327;&#32452;&#21512;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23384;&#22312;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some exte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65288;MPI&#65289;&#26469;&#26377;&#25928;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21457;&#29616;&#20505;&#36873;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2310.15211</link><description>&lt;p&gt;
&#27169;&#25311;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#36335;&#24452;&#37325;&#35201;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Path Importance for Effective Alzheimer's Disease Drug Repurposing. (arXiv:2310.15211v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65288;MPI&#65289;&#26469;&#26377;&#25928;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21457;&#29616;&#20505;&#36873;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33647;&#29289;&#37325;&#29992;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#21457;&#29616;&#33539;&#24335;&#24050;&#32463;&#23853;&#38706;&#22836;&#35282;&#12290;&#22312;&#21508;&#31181;&#33647;&#29289;&#37325;&#29992;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#32593;&#32476;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#21033;&#29992;&#22797;&#26434;&#32593;&#32476;&#65292;&#25972;&#21512;&#22810;&#31181;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65288;&#22914;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65289;&#65292;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#32593;&#32476;&#20013;&#30456;&#21516;&#38271;&#24230;&#30340;&#36335;&#24452;&#23545;&#20110;&#35782;&#21035;&#33647;&#29289;&#30340;&#27835;&#30103;&#25928;&#26524;&#20855;&#26377;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20182;&#39046;&#22495;&#21457;&#29616;&#65292;&#30456;&#21516;&#38271;&#24230;&#30340;&#36335;&#24452;&#24182;&#19981;&#19968;&#23450;&#20855;&#26377;&#30456;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#20381;&#36182;&#20110;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#23545;&#33647;&#29289;&#37325;&#29992;&#23581;&#35797;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPI&#65288;&#27169;&#25311;&#36335;&#24452;&#37325;&#35201;&#24615;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#26041;&#27861;&#12290;MPI&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#36825;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#32593;&#32476;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#33410;&#28857;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#33647;&#29289;&#37325;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, drug repurposing has emerged as an effective and resource-efficient paradigm for AD drug discovery. Among various methods for drug repurposing, network-based methods have shown promising results as they are capable of leveraging complex networks that integrate multiple interaction types, such as protein-protein interactions, to more effectively identify candidate drugs. However, existing approaches typically assume paths of the same length in the network have equal importance in identifying the therapeutic effect of drugs. Other domains have found that same length paths do not necessarily have the same importance. Thus, relying on this assumption may be deleterious to drug repurposing attempts. In this work, we propose MPI (Modeling Path Importance), a novel network-based method for AD drug repurposing. MPI is unique in that it prioritizes important paths via learned node embeddings, which can effectively capture a network's rich structural information. Thus, leveraging learn
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#27573;&#32447;&#24615;&#22238;&#24402;&#21644;&#31354;&#27934;&#22240;&#26524;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20013;&#38271;&#26399;&#26085;&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30005;&#21147;&#28040;&#32791;&#24207;&#21015;&#20998;&#35299;&#20026;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;&#27531;&#24046;&#19977;&#20010;&#37096;&#20998;&#65292;&#24182;&#37319;&#29992;&#20004;&#38454;&#27573;&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36807;&#28388;&#22120;&#21644;&#39044;&#27979;&#22120;&#30340;&#24314;&#27169;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15204</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#27573;&#32447;&#24615;&#22238;&#24402;&#21644;&#31354;&#27934;&#22240;&#26524;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20013;&#38271;&#26399;&#26085;&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mid-Long Term Daily Electricity Consumption Forecasting Based on Piecewise Linear Regression and Dilated Causal CNN. (arXiv:2310.15204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#27573;&#32447;&#24615;&#22238;&#24402;&#21644;&#31354;&#27934;&#22240;&#26524;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20013;&#38271;&#26399;&#26085;&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30005;&#21147;&#28040;&#32791;&#24207;&#21015;&#20998;&#35299;&#20026;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;&#27531;&#24046;&#19977;&#20010;&#37096;&#20998;&#65292;&#24182;&#37319;&#29992;&#20004;&#38454;&#27573;&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36807;&#28388;&#22120;&#21644;&#39044;&#27979;&#22120;&#30340;&#24314;&#27169;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#26159;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#39044;&#27979;&#31639;&#27861;&#22312;&#29305;&#27530;&#26085;&#26399;&#22914;&#33410;&#20551;&#26085;&#19978;&#23481;&#26131;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#23558;&#26085;&#30005;&#21147;&#28040;&#32791;&#24207;&#21015;&#20998;&#35299;&#20026;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;&#27531;&#24046;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#37319;&#29992;&#20998;&#27573;&#32447;&#24615;&#22238;&#24402;&#20316;&#20026;&#36807;&#28388;&#22120;&#21644;&#31354;&#27934;&#22240;&#26524;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39044;&#27979;&#22120;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#39044;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#27493;&#39588;&#21253;&#25324;&#22312;&#26102;&#38388;&#36724;&#19978;&#35774;&#32622;&#26029;&#28857;&#65292;&#20351;&#29992;&#26376;&#20221;&#12289;&#24037;&#20316;&#26085;&#21644;&#33410;&#20551;&#26085;&#30340;&#29420;&#28909;&#32534;&#30721;&#20449;&#24687;&#25311;&#21512;&#20998;&#27573;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#38024;&#23545;&#26149;&#33410;&#30340;&#25361;&#25112;&#24615;&#39044;&#27979;&#65292;&#24341;&#20837;&#20102;&#36317;&#31163;&#20316;&#20026;&#21464;&#37327;&#65292;&#37319;&#29992;&#19977;&#27425;&#22810;&#39033;&#24335;&#24418;&#24335;&#32435;&#20837;&#27169;&#22411;&#12290;&#21069;&#19968;&#27493;&#24471;&#21040;&#30340;&#27531;&#24046;&#24207;&#21015;&#20351;&#29992;&#31354;&#27934;&#22240;&#26524;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#65292;&#26368;&#32456;&#30340;&#26085;&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#26159;&#20004;&#38454;&#27573;&#39044;&#27979;&#30340;&#24635;&#21644;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Daily electricity consumption forecasting is a classical problem. Existing forecasting algorithms tend to have decreased accuracy on special dates like holidays. This study decomposes the daily electricity consumption series into three components: trend, seasonal, and residual, and constructs a two-stage prediction method using piecewise linear regression as a filter and Dilated Causal CNN as a predictor. The specific steps involve setting breakpoints on the time axis and fitting the piecewise linear regression model with one-hot encoded information such as month, weekday, and holidays. For the challenging prediction of the Spring Festival, distance is introduced as a variable using a third-degree polynomial form in the model. The residual sequence obtained in the previous step is modeled using Dilated Causal CNN, and the final prediction of daily electricity consumption is the sum of the two-stage predictions. Experimental results demonstrate that this method achieves higher accuracy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.15202</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;
&lt;/p&gt;
&lt;p&gt;
Predicting Transcription Factor Binding Sites using Transformer based Capsule Network. (arXiv:2310.15202v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#30340;&#32467;&#21512;&#20301;&#28857;&#23545;&#20110;&#29702;&#35299;&#23427;&#20204;&#22914;&#20309;&#35843;&#25511;&#22522;&#22240;&#34920;&#36798;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#27835;&#30103;&#25163;&#27573;&#36827;&#34892;&#35843;&#33410;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#24037;&#20316;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#21033;&#29992;ChIP-seq&#25968;&#25454;&#38598;&#25366;&#25496;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;DNABERT-Cap&#26159;&#19968;&#20010;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#32463;&#36807;&#22823;&#37327;&#22522;&#22240;&#32452;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33014;&#22218;&#23618;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#23545;&#21253;&#21547;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20116;&#20010;&#32454;&#32990;&#31995;&#30340;&#22522;&#20934;ChIP-seq&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;A54&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of binding sites for transcription factors is important to understand how they regulate gene expression and how this regulation can be modulated for therapeutic purposes. Although in the past few years there are significant works addressing this issue, there is still space for improvement. In this regard, a transformer based capsule network viz. DNABERT-Cap is proposed in this work to predict transcription factor binding sites mining ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with large number of genomic DNA sequences, empowered with a capsule layer responsible for the final prediction. The proposed model builds a predictor for transcription factor binding sites using the joint optimisation of features encompassing both bidirectional encoder and capsule layer, along with convolutional and bidirectional long-short term memory layers. To evaluate the efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of five cell lines viz. A54
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25552;&#20379;&#24378;&#32467;&#26500;&#32534;&#30721;&#26102;&#28040;&#24687;&#20256;&#36882;&#30340;&#36129;&#29486;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#21644;&#32467;&#26500;&#20449;&#24687;&#20043;&#38388;&#30340;&#24352;&#37327;&#20056;&#31215;&#20132;&#20114;&#26041;&#24335;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#36873;&#25321;&#65292;&#29978;&#33267;&#22312;&#23481;&#37327;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#23436;&#20840;&#31227;&#38500;&#20102;&#28040;&#24687;&#20256;&#36882;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2310.15197</link><description>&lt;p&gt;
&#24378;&#32467;&#26500;&#32534;&#30721;&#26159;&#21542;&#33021;&#20943;&#23569;&#28040;&#24687;&#20256;&#36882;&#30340;&#37325;&#35201;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can strong structural encoding reduce the importance of Message Passing?. (arXiv:2310.15197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25552;&#20379;&#24378;&#32467;&#26500;&#32534;&#30721;&#26102;&#28040;&#24687;&#20256;&#36882;&#30340;&#36129;&#29486;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#21644;&#32467;&#26500;&#20449;&#24687;&#20043;&#38388;&#30340;&#24352;&#37327;&#20056;&#31215;&#20132;&#20114;&#26041;&#24335;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#36873;&#25321;&#65292;&#29978;&#33267;&#22312;&#23481;&#37327;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#23436;&#20840;&#31227;&#38500;&#20102;&#28040;&#24687;&#20256;&#36882;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#19978;&#25805;&#20316;&#30340;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#26159;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;(MPNNs)&#65292;&#20854;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#36890;&#36807;&#22312;1-hop&#37051;&#22495;&#20013;&#32858;&#21512;&#20449;&#24687;&#36827;&#34892;&#36845;&#20195;&#26356;&#26032;&#12290;&#30001;&#20110;&#36825;&#31181;&#35745;&#31639;&#33410;&#28857;&#23884;&#20837;&#30340;&#33539;&#24335;&#21487;&#33021;&#20250;&#38459;&#27490;&#27169;&#22411;&#23398;&#20064;&#31895;&#31961;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#22240;&#27492;&#21021;&#22987;&#29305;&#24449;&#36890;&#24120;&#20250;&#36890;&#36807;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22686;&#24378;&#65292;&#36890;&#24120;&#20197;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#25110;&#38543;&#26426;&#28216;&#36208;&#36716;&#31227;&#27010;&#29575;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#24378;&#32467;&#26500;&#32534;&#30721;&#26102;&#28040;&#24687;&#20256;&#36882;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#29305;&#24449;&#21644;&#32467;&#26500;&#20449;&#24687;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#23427;&#20204;&#30340;&#24352;&#37327;&#20056;&#31215;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;&#36830;&#25509;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#24120;&#35265;&#24773;&#26223;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#23481;&#37327;&#20005;&#37325;&#38477;&#20302;&#29978;&#33267;&#26368;&#32456;&#34987;&#23436;&#20840;&#31227;&#38500;&#30340;&#35774;&#32622;&#19979;&#36873;&#25321;&#30340;&#30456;&#20114;&#20316;&#29992;&#26041;&#24335;&#12290;&#25105;&#20204;res
&lt;/p&gt;
&lt;p&gt;
The most prevalent class of neural networks operating on graphs are message passing neural networks (MPNNs), in which the representation of a node is updated iteratively by aggregating information in the 1-hop neighborhood. Since this paradigm for computing node embeddings may prevent the model from learning coarse topological structures, the initial features are often augmented with structural information of the graph, typically in the form of Laplacian eigenvectors or Random Walk transition probabilities. In this work, we explore the contribution of message passing when strong structural encodings are provided. We introduce a novel way of modeling the interaction between feature and structural information based on their tensor product rather than the standard concatenation. The choice of interaction is compared in common scenarios and in settings where the capacity of the message-passing layer is severely reduced and ultimately the message-passing phase is removed altogether. Our res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;EMNH&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20803;&#27169;&#22411;&#24182;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EMNH&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#35299;&#20915;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.15196</link><description>&lt;p&gt;
&#39640;&#25928;&#20803;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#29992;&#20110;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization. (arXiv:2310.15196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;EMNH&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20803;&#27169;&#22411;&#24182;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EMNH&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#35299;&#20915;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#35299;&#20915;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#35299;&#20915;&#36136;&#37327;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;EMNH&#65289;&#65292;&#20854;&#20013;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#20803;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#20960;&#20010;&#27493;&#39588;&#23545;&#24212;&#30340;&#21333;&#30446;&#26631;&#23376;&#38382;&#39064;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#65288;&#37096;&#20998;&#65289;&#26550;&#26500;&#20849;&#20139;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#23454;&#29616;&#20803;&#27169;&#22411;&#30340;&#24182;&#34892;&#23398;&#20064;&#65292;&#20197;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65307;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19982;&#26435;&#37325;&#21521;&#37327;&#30456;&#20851;&#30340;&#27604;&#20363;&#23545;&#31216;&#37319;&#26679;&#26041;&#27861;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#26469;&#31995;&#32479;&#22320;&#22788;&#29702;&#25152;&#26377;&#30340;&#23376;&#38382;&#39064;&#12290;&#22312;&#22810;&#30446;&#26631;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;MOTSP&#65289;&#12289;&#22810;&#30446;&#26631;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;MOVRPTW&#65289;&#21644;&#22810;&#30446;&#26631;&#32972;&#21253;&#38382;&#39064;&#65288;MOKP&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;EMNH&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#35299;&#20915;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural heuristics based on deep reinforcement learning have exhibited promise in solving multi-objective combinatorial optimization problems (MOCOPs). However, they are still struggling to achieve high learning efficiency and solution quality. To tackle this issue, we propose an efficient meta neural heuristic (EMNH), in which a meta-model is first trained and then fine-tuned with a few steps to solve corresponding single-objective subproblems. Specifically, for the training process, a (partial) architecture-shared multi-task model is leveraged to achieve parallel learning for the meta-model, so as to speed up the training; meanwhile, a scaled symmetric sampling method with respect to the weight vectors is designed to stabilize the training. For the fine-tuning process, an efficient hierarchical method is proposed to systematically tackle all the subproblems. Experimental results on the multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle ro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#26679;&#24615;&#22686;&#24378;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;NHDE&#65289;&#26469;&#35299;&#20915;&#31070;&#32463;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#65288;MOCO&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25351;&#31034;&#22120;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#37325;&#24085;&#32047;&#25176;&#26368;&#26377;&#31574;&#30053;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22810;&#19988;&#20855;&#26377;&#26356;&#39640;&#22810;&#26679;&#24615;&#30340;&#24085;&#32047;&#25176;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.15195</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#26679;&#24615;&#22686;&#24378;&#30340;&#31070;&#32463;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural Multi-Objective Combinatorial Optimization with Diversity Enhancement. (arXiv:2310.15195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15195
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#26679;&#24615;&#22686;&#24378;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;NHDE&#65289;&#26469;&#35299;&#20915;&#31070;&#32463;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#65288;MOCO&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25351;&#31034;&#22120;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#37325;&#24085;&#32047;&#25176;&#26368;&#26377;&#31574;&#30053;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22810;&#19988;&#20855;&#26377;&#26356;&#39640;&#22810;&#26679;&#24615;&#30340;&#24085;&#32047;&#25176;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#65288;MOCO&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#20998;&#35299;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#23376;&#38382;&#39064;&#30340;&#37325;&#22797;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#24471;&#21040;&#26377;&#38480;&#30340;&#24085;&#32047;&#25176;&#38598;&#12290;&#38500;&#20102;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#26679;&#24615;&#22686;&#24378;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;NHDE&#65289;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#20135;&#29983;&#26356;&#22810;&#24085;&#32047;&#25176;&#35299;&#12290;&#19968;&#26041;&#38754;&#65292;&#20026;&#20102;&#38459;&#27490;&#19981;&#21516;&#23376;&#38382;&#39064;&#30340;&#37325;&#22797;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#31034;&#22120;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#24322;&#26500;&#22270;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#23454;&#20363;&#22270;&#21644;&#24085;&#32047;&#25176;&#21069;&#27839;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20026;&#20102;&#22312;&#27599;&#20010;&#23376;&#38382;&#39064;&#30340;&#37051;&#22495;&#20013;&#25366;&#25496;&#26356;&#22810;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#37325;&#24085;&#32047;&#25176;&#26368;&#26377;&#31574;&#30053;&#26469;&#37319;&#26679;&#21644;&#20445;&#30041;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#32463;&#20856;&#30340; MOCO &#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340; NHDE &#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#22810;&#26679;&#24615;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20248;&#36234;&#30340;&#32508;&#21512;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Most of existing neural methods for multi-objective combinatorial optimization (MOCO) problems solely rely on decomposition, which often leads to repetitive solutions for the respective subproblems, thus a limited Pareto set. Beyond decomposition, we propose a novel neural heuristic with diversity enhancement (NHDE) to produce more Pareto solutions from two perspectives. On the one hand, to hinder duplicated solutions for different subproblems, we propose an indicator-enhanced deep reinforcement learning method to guide the model, and design a heterogeneous graph attention mechanism to capture the relations between the instance graph and the Pareto front graph. On the other hand, to excavate more solutions in the neighborhood of each subproblem, we present a multiple Pareto optima strategy to sample and preserve desirable solutions. Experimental results on classic MOCO problems show that our NHDE is able to generate a Pareto front with higher diversity, thereby achieving superior overa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#21021;&#22987;&#29468;&#27979;&#20197;&#21450;&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#30340;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15191</link><description>&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Application of deep and reinforcement learning to boundary control problems. (arXiv:2310.15191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#21021;&#22987;&#29468;&#27979;&#20197;&#21450;&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#30340;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#26159;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#38750;&#20984;&#20248;&#21270;&#21644;&#25511;&#21046;&#38382;&#39064;&#65292;&#21253;&#25324;&#27969;&#20307;&#21147;&#23398;&#12289;&#32467;&#26500;&#24037;&#31243;&#21644;&#28909;&#20256;&#36882;&#20248;&#21270;&#12290;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#20351;&#31526;&#21512;&#25511;&#21046;&#26041;&#31243;&#35201;&#27714;&#30340;&#23553;&#38381;&#22495;&#20869;&#30340;&#29366;&#24577;&#20540;&#36798;&#21040;&#26399;&#26395;&#20540;&#30340;&#26368;&#20248;&#36793;&#30028;&#20540;&#12290;&#20256;&#32479;&#19978;&#65292;&#20351;&#29992;&#38750;&#32447;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#20869;&#28857;&#27861;&#65288;IPM&#65289;&#26469;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36981;&#24490;&#36845;&#20195;&#20248;&#21270;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26377;&#21551;&#21457;&#24615;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#30340;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#12290;&#20351;&#29992;&#20174;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#38382;&#39064;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#39564;&#35777;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The boundary control problem is a non-convex optimization and control problem in many scientific domains, including fluid mechanics, structural engineering, and heat transfer optimization. The aim is to find the optimal values for the domain boundaries such that the enclosed domain adhering to the governing equations attains the desired state values. Traditionally, non-linear optimization methods, such as the Interior-Point method (IPM), are used to solve such problems.  This project explores the possibilities of using deep learning and reinforcement learning to solve boundary control problems. We adhere to the framework of iterative optimization strategies, employing a spatial neural network to construct well-informed initial guesses, and a spatio-temporal neural network learns the iterative optimization algorithm using policy gradients. Synthetic data, generated from the problems formulated in the literature, is used for training, testing and validation. The numerical experiments ind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22686;&#24378;&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20027;&#20307;&#26080;&#20851;&#30340;&#24773;&#24863;&#33041;&#26426;&#25509;&#21475;&#12290;&#35813;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#33041;&#30005;&#20449;&#21495;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15189</link><description>&lt;p&gt;
&#26397;&#30528;&#20027;&#20307;&#26080;&#20851;&#30340;&#24773;&#24863;&#35782;&#21035;&#65306;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Subject Agnostic Affective Emotion Recognition. (arXiv:2310.15189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22686;&#24378;&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20027;&#20307;&#26080;&#20851;&#30340;&#24773;&#24863;&#33041;&#26426;&#25509;&#21475;&#12290;&#35813;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#33041;&#30005;&#20449;&#21495;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#24773;&#24863;&#35782;&#21035;&#65292;&#26088;&#22312;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#22312;&#20027;&#20307;&#26080;&#20851;&#30340;&#24773;&#22659;&#19979;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#20027;&#20307;&#26080;&#20851;&#24773;&#24863;&#33041;&#26426;&#25509;&#21475;&#20013;&#65292;&#33041;&#30005;&#20449;&#21495;&#34920;&#29616;&#20986;&#20027;&#20307;&#19981;&#31283;&#23450;&#24615;&#65292;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#39046;&#22495;&#27867;&#21270;&#21644;&#39046;&#22495;&#36866;&#24212;&#31561;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#22522;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#22788;&#29702;&#26032;&#20027;&#20307;&#26102;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22686;&#24378;&#39046;&#22495;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#20027;&#20307;&#26080;&#20851;&#30340;&#24773;&#24863;&#33041;&#26426;&#25509;&#21475;&#12290;&#25105;&#20204;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#22686;&#24378;&#65292;&#20854;&#20013;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#20998;&#31867;&#22120;&#21644;&#22522;&#20110;&#21487;&#20998;&#35299;&#20989;&#25968;&#30340;&#20998;&#24067;&#20559;&#31227;&#25511;&#21046;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#35299;&#37322;&#21487;&#20998;&#35299;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#26377;&#25928;&#22320;&#20272;&#35745;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on affective emotion recognition, aiming to perform in the subject-agnostic paradigm based on EEG signals. However, EEG signals manifest subject instability in subject-agnostic affective Brain-computer interfaces (aBCIs), which led to the problem of distributional shift. Furthermore, this problem is alleviated by approaches such as domain generalisation and domain adaptation. Typically, methods based on domain adaptation confer comparatively better results than the domain generalisation methods but demand more computational resources given new subjects. We propose a novel framework, meta-learning based augmented domain adaptation for subject-agnostic aBCIs. Our domain adaptation approach is augmented through meta-learning, which consists of a recurrent neural network, a classifier, and a distributional shift controller based on a sum-decomposable function. Also, we present that a neural network explicating a sum-decomposable function can effectively estimate the dive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#24494;&#32467;&#26500;&#19982;&#21147;&#23398;&#24615;&#33021;&#24314;&#31435;&#26144;&#23556;&#20851;&#31995;&#65292;&#21152;&#24555;&#20102;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#26681;&#25454;&#26399;&#26395;&#24615;&#33021;&#29983;&#25104;&#24494;&#32467;&#26500;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.15188</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#31896;&#24377;&#32420;&#32500;&#22797;&#21512;&#26448;&#26009;&#30340;&#21160;&#24577;&#21147;&#23398;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Approaches for Dynamic Mechanical Analysis of Viscoelastic Fiber Composites. (arXiv:2310.15188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#24494;&#32467;&#26500;&#19982;&#21147;&#23398;&#24615;&#33021;&#24314;&#31435;&#26144;&#23556;&#20851;&#31995;&#65292;&#21152;&#24555;&#20102;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#26681;&#25454;&#26399;&#26395;&#24615;&#33021;&#29983;&#25104;&#24494;&#32467;&#26500;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#29983;&#24577;&#35774;&#35745;&#26631;&#20934;&#30340;&#25512;&#21160;&#65292;&#22686;&#24378;&#22411;&#32858;&#21512;&#29289;&#65288;RP&#65289;&#22797;&#21512;&#26448;&#26009;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#36825;&#35201;&#27714;&#22312;&#36731;&#37327;&#21270;&#12289;&#21018;&#24615;&#21644;&#26377;&#25928;&#30340;&#25391;&#21160;&#25511;&#21046;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#36825;&#20123;&#26448;&#26009;&#23545;&#20110;&#25552;&#39640;&#33298;&#36866;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#33021;&#28304;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#21160;&#24577;&#21147;&#23398;&#20998;&#26512;&#65288;DMA&#65289;&#21487;&#34920;&#24449;&#31896;&#24377;&#24615;&#34892;&#20026;&#65292;&#28982;&#32780;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21152;&#36895;&#24494;&#32467;&#26500;&#35774;&#35745;&#21644;&#29702;&#35299;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#24494;&#32467;&#26500;&#26144;&#23556;&#21040;&#20854;&#21147;&#23398;&#24615;&#33021;&#19978;&#65292;&#21152;&#24555;&#36807;&#31243;&#24182;&#23454;&#29616;&#20174;&#26399;&#26395;&#24615;&#33021;&#29983;&#25104;&#24494;&#32467;&#26500;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased adoption of reinforced polymer (RP) composite materials, driven by eco-design standards, calls for a fine balance between lightness, stiffness, and effective vibration control. These materials are integral to enhancing comfort, safety, and energy efficiency. Dynamic Mechanical Analysis (DMA) characterizes viscoelastic behavior, yet there's a growing interest in using Machine Learning (ML) to expedite the design and understanding of microstructures. In this paper we aim to map microstructures to their mechanical properties using deep neural networks, speeding up the process and allowing for the generation of microstructures from desired properties.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31354;&#38388;&#21487;&#21464;&#24615;&#21644;&#27169;&#22411;&#30456;&#20114;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#26410;&#26469;&#28023;&#24179;&#38754;&#19978;&#21319;&#65292;&#24182;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15179</link><description>&lt;p&gt;
&#20943;&#23569;&#28023;&#24179;&#38754;&#19978;&#21319;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#31181;&#31354;&#38388;&#21487;&#21464;&#24615;&#24863;&#30693;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reducing Uncertainty in Sea-level Rise Prediction: A Spatial-variability-aware Approach. (arXiv:2310.15179v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15179
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31354;&#38388;&#21487;&#21464;&#24615;&#21644;&#27169;&#22411;&#30456;&#20114;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#26410;&#26469;&#28023;&#24179;&#38754;&#19978;&#21319;&#65292;&#24182;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22810;&#27169;&#22411;&#38598;&#21512;&#27668;&#20505;&#39044;&#27979;&#65292;&#30446;&#26631;&#26159;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#26410;&#26469;&#28023;&#24179;&#38754;&#19978;&#21319;&#65292;&#21516;&#26102;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#12290;&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#23545;&#26497;&#22320;&#20912;&#30422;&#21644;&#28023;&#27915;&#30340;&#24433;&#21709;&#65292;&#28023;&#24179;&#38754;&#19978;&#21319;&#24433;&#21709;&#30528;&#27839;&#28023;&#31038;&#21306;&#21644;&#20854;&#20182;&#22320;&#21306;&#30340;&#25968;&#30334;&#19975;&#20154;&#21475;&#65292;&#22240;&#27492;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#30001;&#20110;&#31354;&#38388;&#21464;&#24322;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#22914;&#21487;&#33021;&#30340;&#20020;&#30028;&#28857;&#65288;&#20363;&#22914;&#65292;&#26684;&#38517;&#20848;&#25110;&#35199;&#37096;&#21335;&#26497;&#27954;&#20912;&#26550;&#30340;&#23849;&#22604;&#65289;&#12289;&#27668;&#20505;&#21453;&#39304;&#24490;&#29615;&#65288;&#20363;&#22914;&#65292;&#20113;&#12289;&#27704;&#20037;&#20923;&#22303;&#34701;&#21270;&#65289;&#12289;&#26410;&#26469;&#25919;&#31574;&#20915;&#31574;&#21644;&#20154;&#31867;&#34892;&#20026;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#27668;&#20505;&#24314;&#27169;&#26041;&#27861;&#22312;&#22238;&#24402;&#25110;&#28145;&#24230;&#23398;&#20064;&#20013;&#20840;&#23616;&#20351;&#29992;&#30456;&#21516;&#30340;&#26435;&#37325;&#26469;&#32452;&#21512;&#19981;&#21516;&#30340;&#27668;&#20505;&#39044;&#27979;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#20934;&#30830;&#21487;&#38752;&#30340;&#28023;&#24179;&#38754;&#19978;&#21319;&#39044;&#27979;&#38656;&#35201;&#19981;&#21516;&#30340;&#22320;&#21306;&#37319;&#29992;&#19981;&#21516;&#30340;&#21152;&#26435;&#26041;&#26696;&#26159;&#19981;&#22815;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#22238;&#24402;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#31354;&#38388;&#21487;&#21464;&#24615;&#21644;&#27169;&#22411;&#30456;&#20114;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given multi-model ensemble climate projections, the goal is to accurately and reliably predict future sea-level rise while lowering the uncertainty. This problem is important because sea-level rise affects millions of people in coastal communities and beyond due to climate change's impacts on polar ice sheets and the ocean. This problem is challenging due to spatial variability and unknowns such as possible tipping points (e.g., collapse of Greenland or West Antarctic ice-shelf), climate feedback loops (e.g., clouds, permafrost thawing), future policy decisions, and human actions. Most existing climate modeling approaches use the same set of weights globally, during either regression or deep learning to combine different climate projections. Such approaches are inadequate when different regions require different weighting schemes for accurate and reliable sea-level rise predictions. This paper proposes a zonal regression model which addresses spatial variability and model inter-depende
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>http://arxiv.org/abs/2310.15168</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ghost on the Shell: An Expressive Representation of General 3D Shapes. (arXiv:2310.15168v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15168
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
The creation of photorealistic virtual worlds requires the accurate modeling of 3D surface geometry for a wide range of objects. For this, meshes are appealing since they 1) enable fast physics-based rendering with realistic material and lighting, 2) support physical simulation, and 3) are memory-efficient for modern graphics pipelines. Recent work on reconstructing and statistically modeling 3D shape, however, has critiqued meshes as being topologically inflexible. To capture a wide range of object shapes, any 3D representation must be able to model solid, watertight, shapes as well as thin, open, surfaces. Recent work has focused on the former, and methods for reconstructing open surfaces do not support fast reconstruction with material and lighting or unconditional generative modelling. Inspired by the observation that open surfaces can be seen as islands floating on watertight surfaces, we parameterize open surfaces by defining a manifold signed distance field on watertight templat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.15047</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20803;-&#65288;&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#65289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta- (out-of-context) learning in neural networks. (arXiv:2310.15047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Brown&#31561;&#20154;&#65288;2020&#65289;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#23454;&#39564;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20351;LLMs&#26356;&#23481;&#26131;&#8220;&#20869;&#21270;&#8221;&#25991;&#26412;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#35813;&#25991;&#26412;&#24191;&#27867;&#36866;&#29992;&#65288;&#20363;&#22914;&#30495;&#23454;&#38472;&#36848;&#25110;&#26435;&#23041;&#26469;&#28304;&#30340;&#25991;&#26412;&#65289;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#21512;&#25104;&#35745;&#31639;&#26426;&#35270;&#35273;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20551;&#35774;&#65292;&#35299;&#37322;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#30340;&#20986;&#29616;&#65306;&#19968;&#31181;&#26159;&#20381;&#36182;&#20110;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#21478;&#19968;&#31181;&#26159;&#26263;&#31034;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#30340;&#38544;&#21547;&#26799;&#24230;&#23545;&#40784;&#20559;&#24046;&#21487;&#33021;&#36127;&#36131;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#33021;&#24847;&#21619;&#30528;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#24605;&#32771;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/krasheni&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks. Our code can be found at https://github.com/krasheni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25216;&#26415;(MLT)&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#26500;&#24314;&#30340;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#65292;&#22312;&#19981;&#24179;&#34913;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#38408;&#20540;&#31227;&#21160;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15019</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20803;&#23398;&#20064;: &#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Meta learning with language models: Challenges and opportunities in the classification of imbalanced text. (arXiv:2310.15019v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25216;&#26415;(MLT)&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#26500;&#24314;&#30340;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#65292;&#22312;&#19981;&#24179;&#34913;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#38408;&#20540;&#31227;&#21160;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#36829;&#35268;&#35328;&#35770;&#20869;&#23481;&#26159;&#37325;&#35201;&#20294;&#22256;&#38590;&#30340;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26159;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#38480;&#21046;&#20197;&#21450;&#36829;&#35268;&#23450;&#20041;&#21644;&#25968;&#25454;&#26631;&#27880;&#30340;&#19981;&#19968;&#33268;&#24615;&#31561;&#22240;&#32032;&#65292;&#38590;&#20197;&#31361;&#30772;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#26377;&#38480;&#36164;&#28304;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25216;&#26415;(MLT)&#65292;&#23427;&#23558;&#20351;&#29992;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#26500;&#24314;&#30340;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#35777;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#25216;&#26415;&#22312;&#25968;&#20540;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#20135;&#29983;&#21512;&#29702;&#30340;&#32452;&#21512;&#26435;&#37325;&#12290;&#25105;&#20204;&#23558;MLT&#19982;&#38408;&#20540;&#31227;&#21160;(TM)&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#32452;&#21512;&#39044;&#27979;&#22120;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#20998;&#24067;&#21644;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;MLT&#26041;&#27861;&#30340;&#32479;&#35745;&#20248;&#21183;&#12290;&#25152;&#26377;&#20316;&#32773;&#23545;&#36825;&#39033;&#24037;&#20316;&#36129;&#29486;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out of policy speech (OOPS) content is important but difficult. While machine learning is a powerful tool to tackle this challenging task, it is hard to break the performance ceiling due to factors like quantity and quality limitations on training data and inconsistencies in OOPS definition and data labeling. To realize the full potential of available limited resources, we propose a meta learning technique (MLT) that combines individual models built with different text representations. We analytically show that the resulting technique is numerically stable and produces reasonable combining weights. We combine the MLT with a threshold-moving (TM) technique to further improve the performance of the combined predictor on highly-imbalanced in-distribution and out-of-distribution datasets. We also provide computational results to show the statistically significant advantages of the proposed MLT approach.  All authors contributed equally to this work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#20960;&#20309;&#20307;&#20013;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#25968;&#20540;&#27714;&#35299;&#22120;&#20174;&#32780;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.14948</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;&#65306;&#38754;&#21521;&#22797;&#26434;&#20960;&#20309;&#30340;&#24191;&#20041;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries. (arXiv:2310.14948v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#20960;&#20309;&#20307;&#20013;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#25968;&#20540;&#27714;&#35299;&#22120;&#20174;&#32780;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;[9]&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20197;&#21450;&#20182;&#20204;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20043;&#21518;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#22797;&#26434;&#30340;&#19977;&#32500;&#20960;&#20309;&#20307;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#32463;&#20856;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20256;&#32479;&#25968;&#20540;&#25216;&#26415;&#20013;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32593;&#26684;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#35777;&#26126;&#20102;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#35777;&#26126;&#20102;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#35745;&#31639;PDE&#27531;&#24046;&#26102;&#23384;&#22312;&#38382;&#39064;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#32463;&#20856;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#19982;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#35813;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#22312;&#19968;&#20010;&#19981;&#35268;&#21017;&#20960;&#20309;&#20307;&#19978;&#30340;&#19977;&#32500;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the seminal work of [9] and their Physics-Informed neural networks (PINNs), many efforts have been conducted towards solving partial differential equations (PDEs) with Deep Learning models. However, some challenges remain, for instance the extension of such models to complex three-dimensional geometries, and a study on how such approaches could be combined to classical numerical solvers. In this work, we justify the use of graph neural networks for these problems, based on the similarity between these architectures and the meshes used in traditional numerical techniques for solving partial differential equations. After proving an issue with the Physics-Informed framework for complex geometries, during the computation of PDE residuals, an alternative procedure is proposed, by combining classical numerical solvers and the Physics-Informed framework. Finally, we propose an implementation of this approach, that we test on a three-dimensional problem on an irregular geometry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#20989;&#25968;&#23450;&#29702;&#23545;&#38750;&#20984;&#12289;&#21463;&#32422;&#26463;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#36712;&#36857;&#36827;&#34892;&#24494;&#20998;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32447;&#24615;&#32553;&#25918;&#22320;&#35745;&#31639;&#36712;&#36857;&#23548;&#25968;&#65292;&#21487;&#20197;&#23481;&#26131;&#22320;&#36827;&#34892;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#35268;&#27169;&#12289;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.14468</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38544;&#24335;&#24494;&#20998;&#20197;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#20013;&#30340;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Implicit Differentiation for Learning Problems in Optimal Control. (arXiv:2310.14468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#20989;&#25968;&#23450;&#29702;&#23545;&#38750;&#20984;&#12289;&#21463;&#32422;&#26463;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#36712;&#36857;&#36827;&#34892;&#24494;&#20998;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32447;&#24615;&#32553;&#25918;&#22320;&#35745;&#31639;&#36712;&#36857;&#23548;&#25968;&#65292;&#21487;&#20197;&#23481;&#26131;&#22320;&#36827;&#34892;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#35268;&#27169;&#12289;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#20989;&#25968;&#23450;&#29702;&#65292;&#23545;&#38750;&#20984;&#12289;&#21463;&#32422;&#26463;&#30340;&#31163;&#25955;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#65288;COC&#65289;&#38382;&#39064;&#20013;&#20135;&#29983;&#30340;&#26368;&#20248;&#36712;&#36857;&#36827;&#34892;&#24494;&#20998;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#19968;&#31181;&#24494;&#20998;&#21345;&#40065;&#20160;-&#24211;&#24681;-&#22612;&#20811;&#65288;KKT&#65289;&#31995;&#32479;&#26469;&#27714;&#35299;&#36712;&#36857;&#23548;&#25968;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#36741;&#21161;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#26469;&#23454;&#29616;&#39640;&#25928;&#35745;&#31639;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30452;&#25509;&#35780;&#20272;&#30001;&#22312;&#65288;&#24494;&#20998;&#65289;KKT&#31995;&#32479;&#20013;&#28040;&#38500;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#39033;&#20135;&#29983;&#30340;&#30697;&#38453;&#26041;&#31243;&#12290;&#36890;&#36807;&#36866;&#24403;&#32771;&#34385;&#32467;&#26524;&#26041;&#31243;&#20013;&#30340;&#39033;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#34920;&#26126;&#36712;&#36857;&#23548;&#25968;&#19982;&#26102;&#38388;&#27493;&#25968;&#21576;&#32447;&#24615;&#32553;&#25918;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#36731;&#26494;&#24182;&#34892;&#21270;&#65292;&#19982;&#27169;&#22411;&#22823;&#23567;&#26174;&#33879;&#25913;&#21892;&#21487;&#25193;&#23637;&#24615;&#65292;&#30452;&#25509;&#35745;&#31639;&#21521;&#37327; - &#38597;&#21487;&#27604;&#20056;&#31215;&#65292;&#24182;&#19988;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#20855;&#26377;&#25913;&#36827;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;&#20316;&#20026;&#39069;&#22806;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#32479;&#19968;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new method for differentiating through optimal trajectories arising from non-convex, constrained discrete-time optimal control (COC) problems using the implicit function theorem (IFT). Previous works solve a differential Karush-Kuhn-Tucker (KKT) system for the trajectory derivative, and achieve this efficiently by solving an auxiliary Linear Quadratic Regulator (LQR) problem. In contrast, we directly evaluate the matrix equations which arise from applying variable elimination on the Lagrange multiplier terms in the (differential) KKT system. By appropriately accounting for the structure of the terms within the resulting equations, we show that the trajectory derivatives scale linearly with the number of timesteps. Furthermore, our approach allows for easy parallelization, significantly improved scalability with model size, direct computation of vector-Jacobian products and improved numerical stability compared to prior works. As an additional contribution, we unif
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#38543;&#26426;&#26041;&#21521;&#65292;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.14168</link><description>&lt;p&gt;
&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms. (arXiv:2310.14168v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#38543;&#26426;&#26041;&#21521;&#65292;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#21033;&#29992;&#20102;&#33258;&#21160;&#24494;&#20998;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#21363;&#21453;&#21521;&#27169;&#24335;&#24494;&#20998;&#65292;&#25110;&#31216;&#20026;&#21521;&#37327;&#38597;&#21487;&#27604;&#20056;&#31215;(VJP)&#65292;&#25110;&#22312;&#24494;&#20998;&#20960;&#20309;&#30340;&#32972;&#26223;&#19979;&#34987;&#31216;&#20026;&#25289;&#22238;&#36807;&#31243;&#12290;&#26799;&#24230;&#30340;&#35745;&#31639;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#36807;&#27491;&#21521;&#27169;&#24335;AD&#25110;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;(JVP)&#39640;&#25928;&#35745;&#31639;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#36825;&#20123;JVP&#27839;&#30528;&#20174;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#65288;&#20363;&#22914;&#20271;&#21162;&#21033;&#12289;&#27491;&#24577;&#12289;&#32500;&#26684;&#32435;&#12289;&#25289;&#26222;&#25289;&#26031;&#21644;&#22343;&#21248;&#20998;&#24067;&#65289;&#37319;&#26679;&#30340;&#38543;&#26426;&#26041;&#21521;&#35745;&#31639;&#12290;&#26799;&#24230;&#30340;&#35745;&#31639;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#21253;&#25324;&#25910;&#25947;&#36895;&#24230;&#20197;&#21450;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation within neural networks leverages a fundamental element of automatic differentiation, which is referred to as the reverse mode differentiation, or vector Jacobian Product (VJP) or, in the context of differential geometry, known as the pull-back process. The computation of gradient is important as update of neural network parameters is performed using gradient descent method. In this study, we present a genric randomized method, which updates the parameters of neural networks by using directional derivatives of loss functions computed efficiently by using forward mode AD or Jacobian vector Product (JVP). These JVP are computed along the random directions sampled from different probability distributions e.g., Bernoulli, Normal, Wigner, Laplace and Uniform distributions. The computation of gradient is performed during the forward pass of the neural network. We also present a rigorous analysis of the presented methods providing the rate of convergence along with the computat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;OGD&#31639;&#27861;\textsf{AdaOGD}&#65292;&#22312;&#24378;&#20984;&#24615;&#19979;&#23454;&#29616;&#20102;$ O(\log^2(T)) $&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#20351;&#24471;&#32852;&#21512;&#34892;&#21160;&#26368;&#21518;&#19968;&#27425;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.14085</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#26799;&#24230;&#21453;&#39304;&#30340;&#24378;&#21333;&#35843;&#21644;&#25351;&#25968;&#20984;&#21338;&#24328;&#20013;&#30340;&#33258;&#36866;&#24212;&#12289;&#21452;&#37325;&#26368;&#20248;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback. (arXiv:2310.14085v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;OGD&#31639;&#27861;\textsf{AdaOGD}&#65292;&#22312;&#24378;&#20984;&#24615;&#19979;&#23454;&#29616;&#20102;$ O(\log^2(T)) $&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#20351;&#24471;&#32852;&#21512;&#34892;&#21160;&#26368;&#21518;&#19968;&#27425;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#20984;&#24615;&#25110;&#21333;&#35843;&#24615;&#20551;&#35774;&#19979;&#65292;&#32593;&#19978;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#21452;&#37325;&#26368;&#20248;&#30340;&#65306;&#65288;1&#65289;&#22312;&#21333;&#20010;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#24378;&#20984;&#25104;&#26412;&#20989;&#25968;&#65292;&#23427;&#23454;&#29616;&#20102;$ \Theta(\log T) $&#30340;&#26368;&#20248;&#21518;&#24724;&#65307;&#65288;2&#65289;&#22312;&#20855;&#26377;&#24378;&#21333;&#35843;&#24615;&#30340;&#22810;&#20195;&#29702;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#20195;&#29702;&#20351;&#29992;OGD&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20851;&#20110;&#32852;&#21512;&#34892;&#21160;&#30340;&#26368;&#21518;&#19968;&#27425;&#25910;&#25947;&#21040;&#21807;&#19968;&#32435;&#20160;&#22343;&#34913;&#30340;&#26368;&#20248;&#36895;&#29575;$ \Theta(\frac{1}{T}) $&#12290;&#23613;&#31649;&#36825;&#20123;&#26377;&#38480;&#26102;&#38388;&#30340;&#20445;&#35777;&#31361;&#20986;&#20102;&#20854;&#20248;&#28857;&#65292;&#20294;OGD&#30340;&#32570;&#28857;&#26159;&#38656;&#35201;&#30693;&#36947;&#24378;&#20984;&#24615;/&#21333;&#35843;&#24615;&#30340;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;OGD&#31639;&#27861;\textsf{AdaOGD}&#65292;&#23427;&#19981;&#38656;&#35201;&#20808;&#39564;&#30340;&#30693;&#35782;&#36825;&#20123;&#21442;&#25968;&#12290;&#22312;&#21333;&#20010;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24378;&#20984;&#24615;&#19979;&#23454;&#29616;&#20102;$ O(\log^2(T)) $&#30340;&#21518;&#24724;&#65292;&#36825;&#26159;&#26368;&#20248;&#30340;&#38500;&#20102;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#27599;&#20010;&#20195;&#29702;&#37117;&#20351;&#29992;\textsf{AdaOGD}&#65292;&#21017;&#32852;&#21512;&#34892;&#21160;&#25910;&#25947;&#21040;&#26368;&#21518;&#19968;&#20010;&#36845;&#20195;&#26102;&#30340;&#19968;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\Theta(\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMET&#30340;&#21019;&#26032;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;COMET&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14017</link><description>&lt;p&gt;
&#20840;&#38754;&#23545;&#27604;&#65306;&#38754;&#21521;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series. (arXiv:2310.14017v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMET&#30340;&#21019;&#26032;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;COMET&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20943;&#23569;&#20102;&#23545;&#21171;&#21160;&#23494;&#38598;&#12289;&#39046;&#22495;&#29305;&#23450;&#21644;&#31232;&#32570;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#25968;&#25454;&#23618;&#38754;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COMET&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#23618;&#27425;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20013;&#25152;&#26377;&#20869;&#22312;&#23618;&#32423;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22411;&#31995;&#32479;&#22320;&#25429;&#25417;&#20102;&#26469;&#33258;&#22235;&#20010;&#28508;&#22312;&#23618;&#32423;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#65306;&#35266;&#27979;&#12289;&#26679;&#26412;&#12289;&#35797;&#39564;&#21644;&#24739;&#32773;&#23618;&#32423;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#21040;&#20445;&#25345;&#20840;&#38754;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#23454;&#29616;&#33258;&#30417;&#30563;&#26041;&#27861;&#20013;&#30340;&#20449;&#24687;&#26368;&#22823;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#31435;&#24739;&#32773;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#23558;COMET&#19982;&#20845;&#20010;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive representation learning is crucial in medical time series analysis as it alleviates dependency on labor-intensive, domain-specific, and scarce expert annotations. However, existing contrastive learning methods primarily focus on one single data level, which fails to fully exploit the intricate nature of medical time series. To address this issue, we present COMET, an innovative hierarchical framework that leverages data consistencies at all inherent levels in medical time series. Our meticulously designed model systematically captures data consistency from four potential levels: observation, sample, trial, and patient levels. By developing contrastive loss at multiple levels, we can learn effective representations that preserve comprehensive data consistency, maximizing information utilization in a self-supervised manner. We conduct experiments in the challenging patient-independent setting. We compare COMET against six baselines using three diverse datasets, which include 
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#20551;&#35774;&#38169;&#35823;&#20197;&#21450;&#20248;&#21270;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13639</link><description>&lt;p&gt;
&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#65306;&#23398;&#20064;&#29992;&#25143;&#21453;&#39304;&#32780;&#26080;&#38656;RL&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contrastive Preference Learning: Learning from Human Feedback without RL. (arXiv:2310.13639v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13639
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#20551;&#35774;&#38169;&#35823;&#20197;&#21450;&#20248;&#21270;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#30340;RLHF&#31639;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20248;&#21270;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#23545;&#40784;&#27169;&#22411;&#12290;&#36825;&#31181;&#33539;&#24335;&#20551;&#35774;&#20154;&#31867;&#20559;&#22909;&#26159;&#26681;&#25454;&#22870;&#21169;&#20998;&#24067;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23454;&#38469;&#19978;&#23427;&#20204;&#36981;&#24490;&#29992;&#25143;&#26368;&#20339;&#31574;&#30053;&#19979;&#30340;&#36951;&#25022;&#12290;&#22240;&#27492;&#65292;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19981;&#20165;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#38169;&#35823;&#20551;&#35774;&#65292;&#36824;&#23548;&#33268;&#20102;&#30001;&#20110;&#31574;&#30053;&#26799;&#24230;&#25110;RL&#38454;&#27573;&#30340;&#33258;&#21161;&#27861;&#24341;&#36215;&#30340;&#26840;&#25163;&#30340;&#20248;&#21270;&#25361;&#25112;&#12290;&#30001;&#20110;&#36825;&#20123;&#20248;&#21270;&#25361;&#25112;&#65292;&#24403;&#20195;&#30340;RLHF&#26041;&#27861;&#38480;&#21046;&#33258;&#24049;&#21482;&#33021;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#25110;&#38480;&#21046;&#20102;&#35266;&#27979;&#32500;&#24230;&#65288;&#22914;&#22522;&#20110;&#29366;&#24577;&#30340;&#26426;&#22120;&#20154;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;"
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new famil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;CO2&#25490;&#25918;&#21644;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13129</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;CO2&#25490;&#25918;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions. (arXiv:2310.13129v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;CO2&#25490;&#25918;&#21644;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20132;&#36890;&#32593;&#32476;&#38754;&#20020;&#30528;&#27425;&#20248;&#25511;&#21046;&#31574;&#30053;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#23545;&#20154;&#31867;&#20581;&#24247;&#12289;&#29615;&#22659;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182; contribute to traffic congestion. &#30001;&#20110;&#20132;&#36890;&#25317;&#22581;&#23548;&#33268;&#30340;&#31354;&#27668;&#27745;&#26579;&#27700;&#24179;&#19978;&#21319;&#21644;&#36890;&#21220;&#26102;&#38388;&#24310;&#38271;&#65292;&#20132;&#21449;&#36335;&#21475;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#22120;&#25104;&#20026;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#26377;&#20960;&#20010;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#22120;&#65292;&#20294;&#23545;&#20854;&#27604;&#36739;&#24615;&#33021;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20108;&#27687;&#21270;&#30899;&#65288;CO2&#65289;&#25490;&#25918;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#25991;&#29486;&#23545;&#35813;&#39046;&#22495;&#20851;&#27880;&#19981;&#22815;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EcoLight&#65292;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#26696;&#65292;&#19981;&#20165;&#33021;&#20943;&#23569;CO2&#25490;&#25918;&#65292;&#36824;&#33021;&#22312;&#35832;&#22914;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#39542;&#26102;&#38388;&#12289;CO2&#25490;&#25918;&#12289;&#31561;&#25351;&#26631;&#27604;&#36739;&#20102;&#34920;&#26684;&#24335;Q-Learning&#12289;DQN&#12289;SARSA&#21644;A2C&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19994;&#21153;&#25968;&#25454;&#39044;&#27979;&#23458;&#36816;&#28193;&#36718;&#29123;&#26009;&#28040;&#36153;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20837;&#21464;&#37327;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#23454;&#38469;&#21487;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13123</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19994;&#21153;&#25968;&#25454;&#39044;&#27979;&#23458;&#36816;&#28193;&#36718;&#29123;&#26009;&#28040;&#36153;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fuel Consumption Prediction for a Passenger Ferry using Machine Learning and In-service Data: A Comparative Study. (arXiv:2310.13123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19994;&#21153;&#25968;&#25454;&#39044;&#27979;&#23458;&#36816;&#28193;&#36718;&#29123;&#26009;&#28040;&#36153;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20837;&#21464;&#37327;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#23454;&#38469;&#21487;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29615;&#20445;&#20132;&#36890;&#30340;&#37325;&#35201;&#24615;&#22686;&#21152;&#65292;&#20026;&#28023;&#36816;&#33337;&#21482;&#25552;&#20379;&#39640;&#25928;&#30340;&#25805;&#20316;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#32771;&#34385;&#21040;&#22825;&#27668;&#26465;&#20214;&#21644;&#20351;&#29992;&#33337;&#21482;&#19994;&#21153;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#29366;&#24577;&#30417;&#25511;&#26041;&#27861;&#38656;&#35201;&#20934;&#30830;&#23436;&#25972;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#33337;&#21482;&#30340;energy efficiency&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#33021;&#22815;&#23454;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#25152;&#26377;&#25805;&#20316;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20174;&#23458;&#36816;&#33337;&#21482;&#25910;&#38598;&#30340;&#19994;&#21153;&#25968;&#25454;&#26469;&#39044;&#27979;&#29123;&#26009;&#28040;&#32791;&#30340;&#27169;&#22411;&#12290;&#32479;&#35745;&#21644;&#39046;&#22495;&#30693;&#35782;&#26041;&#27861;&#34987;&#29992;&#26469;&#36873;&#25321;&#27169;&#22411;&#30340;&#36866;&#24403;&#36755;&#20837;&#21464;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#38450;&#27490;&#36807;&#25311;&#21512;&#12289;&#32570;&#22833;&#25968;&#25454;&#21644;&#22810;&#37325;&#20849;&#32447;&#24615;&#65292;&#24182;&#25552;&#20379;&#23454;&#38469;&#21487;&#24212;&#29992;&#24615;&#12290;&#30740;&#31350;&#20013;&#35843;&#26597;&#30340;&#39044;&#27979;&#27169;&#22411;&#21253;&#25324;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#65288;MLR&#65289;&#12289;&#20915;&#31574;&#26641;&#26041;&#27861;&#65288;DT&#65289;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21644;&#38598;&#25104;&#26041;&#27861;&#12290;&#26368;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#26469;&#33258;&#20110;&#20351;&#29992;ensemble&#26041;&#27861;&#24320;&#21457;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
As the importance of eco-friendly transportation increases, providing an efficient approach for marine vessel operation is essential. Methods for status monitoring with consideration to the weather condition and forecasting with the use of in-service data from ships requires accurate and complete models for predicting the energy efficiency of a ship. The models need to effectively process all the operational data in real-time. This paper presents models that can predict fuel consumption using in-service data collected from a passenger ship. Statistical and domain-knowledge methods were used to select the proper input variables for the models. These methods prevent over-fitting, missing data, and multicollinearity while providing practical applicability. Prediction models that were investigated include multiple linear regression (MLR), decision tree approach (DT), an artificial neural network (ANN), and ensemble methods. The best predictive performance was from a model developed using t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;Vendi&#20998;&#25968;&#26063;&#32676;&#65292;&#20026;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#20934;&#30830;&#27979;&#37327;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12952</link><description>&lt;p&gt;
&#12298;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;Vendi&#20998;&#25968;&#30340;&#36817;&#20146;&#65306;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#26063;&#32676;&#12299;
&lt;/p&gt;
&lt;p&gt;
Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning. (arXiv:2310.12952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;Vendi&#20998;&#25968;&#26063;&#32676;&#65292;&#20026;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#20934;&#30830;&#27979;&#37327;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#27979;&#37327;&#22810;&#26679;&#24615;&#23545;&#20110;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#12289;&#29983;&#24577;&#23398;&#21644;&#21270;&#23398;&#12290; Vendi&#20998;&#25968;&#26159;&#19968;&#31181;&#25193;&#23637;&#20102;q=1&#38454;Hill&#25968;&#30340;&#36890;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20511;&#37492;&#37327;&#23376;&#32479;&#35745;&#21147;&#23398;&#30340;&#24605;&#24819;&#12290;&#19982;&#29983;&#24577;&#23398;&#20013;&#30340;&#35768;&#22810;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#19981;&#21516;&#65292;Vendi&#20998;&#25968;&#32771;&#34385;&#20102;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20102;&#35299;&#38598;&#21512;&#20013;&#21508;&#20010;&#31867;&#21035;&#30340;&#26222;&#36941;&#24615;&#26469;&#35780;&#20272;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;Vendi&#20998;&#25968;&#23545;&#20110;&#32473;&#23450;&#38598;&#21512;&#20013;&#30340;&#27599;&#20010;&#39033;&#30446;&#37117;&#20197;&#19982;&#35813;&#39033;&#30446;&#30340;&#26222;&#36941;&#24615;&#25104;&#27604;&#20363;&#30340;&#25935;&#24863;&#24230;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#22312;&#39033;&#30446;&#26222;&#36941;&#24615;&#23384;&#22312;&#26174;&#33879;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#30456;&#20284;&#24615;&#25193;&#23637;&#20102;&#20854;&#20182;Hill&#25968;&#65292;&#20197;&#25552;&#20379;&#23545;&#31232;&#26377;&#25110;&#24120;&#35265;&#39033;&#30446;&#20998;&#37197;&#25935;&#24863;&#24230;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#26063;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#24230;&#27700;&#24179;&#30340;Vendi&#20998;&#25968;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring diversity accurately is important for many scientific fields, including machine learning (ML), ecology, and chemistry. The Vendi Score was introduced as a generic similarity-based diversity metric that extends the Hill number of order q=1 by leveraging ideas from quantum statistical mechanics. Contrary to many diversity metrics in ecology, the Vendi Score accounts for similarity and does not require knowledge of the prevalence of the categories in the collection to be evaluated for diversity. However, the Vendi Score treats each item in a given collection with a level of sensitivity proportional to the item's prevalence. This is undesirable in settings where there is a significant imbalance in item prevalence. In this paper, we extend the other Hill numbers using similarity to provide flexibility in allocating sensitivity to rare or common items. This leads to a family of diversity metrics -- Vendi scores with different levels of sensitivity -- that can be used in a variety o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22788;&#29702;&#25968;&#20540;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12046</link><description>&lt;p&gt;
&#20197;&#26426;&#22120;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#26041;&#27861;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems. (arXiv:2310.12046v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22788;&#29702;&#25968;&#20540;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#22312;&#22686;&#21152;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20026;&#31185;&#23398;&#38382;&#39064;&#25552;&#20379;&#25968;&#20540;&#35299;&#12290;&#36825;&#31181;&#25928;&#29575;&#22312;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#30340;&#25968;&#20540;&#25361;&#25112;&#38382;&#39064;&#25110;&#38656;&#35201;&#35780;&#20272;&#35768;&#22810;&#31867;&#20284;&#20998;&#26512;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20248;&#21183;&#12290;&#19968;&#20010;&#29305;&#23450;&#30340;&#31185;&#23398;&#20852;&#36259;&#39046;&#22495;&#26159;&#36870;&#38382;&#39064;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25105;&#20204;&#30693;&#36947;&#31995;&#32479;&#30340;&#27491;&#21521;&#21160;&#24577;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#65292;&#20219;&#21153;&#26159;&#26681;&#25454;&#36825;&#20123;&#21160;&#24577;&#30340;&#65288;&#28508;&#22312;&#26377;&#22122;&#22768;&#30340;&#65289;&#35266;&#27979;&#26469;&#25512;&#26029;&#31995;&#32479;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#32771;&#34385;&#25512;&#26029;&#32473;&#23450;2D&#22768;&#27874;&#26041;&#31243;&#30340;&#22024;&#26434;&#35299;&#30340;&#26041;&#22359;&#22495;&#20013;&#27874;&#28304;&#30340;&#20301;&#32622;&#30340;&#36870;&#38382;&#39064;&#12290;&#22312;&#20551;&#35774;&#20026;&#39640;&#26031;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26500;&#36896;&#28304;&#20301;&#32622;&#30340;&#20284;&#28982;&#20989;&#25968;&#65292;&#27599;&#20010;&#35780;&#20272;&#37117;&#38656;&#35201;&#23545;&#31995;&#32479;&#36827;&#34892;&#19968;&#27425;&#27491;&#21521;&#27169;&#25311;&#12290;&#20351;&#29992;&#26631;&#20934;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural networks have become a powerful tool as surrogate models to provide numerical solutions for scientific problems with increased computational efficiency. This efficiency can be advantageous for numerically challenging problems where time to solution is important or when evaluation of many similar analysis scenarios is required. One particular area of scientific interest is the setting of inverse problems, where one knows the forward dynamics of a system are described by a partial differential equation and the task is to infer properties of the system given (potentially noisy) observations of these dynamics. We consider the inverse problem of inferring the location of a wave source on a square domain, given a noisy solution to the 2-D acoustic wave equation. Under the assumption of Gaussian noise, a likelihood function for source location can be formulated, which requires one forward simulation of the system per evaluation. Using a standard neural network as a surrogate model make
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#32452;&#32553;&#25918;&#26041;&#27861;&#65288;BVS&#65289;&#30340;&#20960;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#30340;&#20998;&#32452;&#26041;&#26696;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11978</link><description>&lt;p&gt;
&#21487;&#20197;&#36890;&#36807;&#20998;&#32452;&#32553;&#25918;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?. (arXiv:2310.11978v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#32452;&#32553;&#25918;&#26041;&#27861;&#65288;BVS&#65289;&#30340;&#20960;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#30340;&#20998;&#32452;&#26041;&#26696;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#32452;&#26041;&#24046;&#32553;&#25918;&#65288;BVS&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#38382;&#39064;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#32479;&#19968;&#26041;&#24046;&#65288;&#25110;&#28201;&#24230;&#65289;&#32553;&#25918;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#26657;&#27491;&#12290;&#21407;&#22987;&#29256;&#26412;&#30340;BVS&#20351;&#29992;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20998;&#32452;&#65292;&#26088;&#22312;&#25552;&#39640;&#26465;&#20214;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#21363;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;BVS&#30340;&#20960;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#65288;X&#65289;&#30340;&#20998;&#32452;&#26041;&#26696;&#19978;&#36827;&#34892;&#25913;&#36827;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#65292;&#21363;&#22312;&#32473;&#23450;X&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#26657;&#20934;&#24615;&#12290;&#23558;BVS&#21450;&#20854;&#25913;&#36827;&#26041;&#26696;&#22312;&#39044;&#27979;&#21407;&#23376;&#21270;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#27979;&#35797;&#65292;&#24182;&#19982;&#20445;&#24207;&#22238;&#24402;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binwise Variance Scaling (BVS) has recently been proposed as a post hoc recalibration method for prediction uncertainties of machine learning regression problems that is able of more efficient corrections than uniform variance (or temperature) scaling. The original version of BVS uses uncertainty-based binning, which is aimed to improve calibration conditionally on uncertainty, i.e. consistency. I explore here several adaptations of BVS, in particular with alternative loss functions and a binning scheme based on an input-feature (X) in order to improve adaptivity, i.e. calibration conditional on X. The performances of BVS and its proposed variants are tested on a benchmark dataset for the prediction of atomization energies and compared to the results of isotonic regression.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#28201;&#24230;&#21442;&#25968;&#32435;&#20837;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#33268;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#65292;&#20197;&#25552;&#20379;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11479</link><description>&lt;p&gt;
&#20851;&#20110;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#33268;&#39044;&#27979;&#20013;&#30340;&#28201;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction. (arXiv:2310.11479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11479
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#28201;&#24230;&#21442;&#25968;&#32435;&#20837;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#33268;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#65292;&#20197;&#25552;&#20379;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#32463;&#24120;&#20351;&#29992;GNNs&#30340;&#24773;&#20917;&#19979;&#12290;&#19968;&#33268;&#39044;&#27979;(CP)&#20026;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;CP&#20445;&#35777;&#20102;&#19968;&#20010;&#39044;&#27979;&#38598;&#20197;&#25152;&#38656;&#30340;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#30340;&#24418;&#24335;&#30340;&#23448;&#26041;&#27010;&#29575;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;&#65292;&#21363;"&#20302;&#25928;&#29575;"&#65292;&#21463;&#21040;&#24213;&#23618;&#27169;&#22411;&#21644;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36125;&#21494;&#26031;&#23398;&#20064;&#36824;&#22522;&#20110;&#20272;&#35745;&#30340;&#21518;&#39564;&#20998;&#24067;&#25552;&#20379;&#19968;&#20010;&#21487;&#20449;&#21306;&#22495;&#65292;&#20294;&#21482;&#26377;&#22312;&#27169;&#22411;&#27491;&#30830;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#21306;&#22495;&#25165;&#26159;"&#33391;&#22909;&#26657;&#20934;"&#30340;&#12290;&#22312;&#19968;&#20010;&#26368;&#36817;&#30340;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35813;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#32553;&#25918;&#21442;&#25968;&#65292;&#29992;&#20110;&#20174;&#21518;&#39564;&#20272;&#35745;&#20013;&#26500;&#24314;&#26377;&#25928;&#30340;&#21487;&#20449;&#21306;&#22495;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;CP&#26694;&#26550;&#20013;&#23558;&#19968;&#20010;&#28201;&#24230;&#21442;&#25968;&#32435;&#20837;&#36125;&#21494;&#26031;GNNs&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing $\textit{valid}$ prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as $\textit{inefficiency}$, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is $\textit{well-calibrated}$ only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP fra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08644</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#21313;&#24180;&#26469;&#33268;&#21147;&#20110;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#28436;&#21270;&#30340;&#29289;&#29702;-&#27010;&#24565; (PC) &#27169;&#22411;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064; (ML) &#30340;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#29289;&#29702;&#29702;&#35299;&#30340;&#22256;&#38590;&#20351;&#24471;&#20854;&#22312;&#22686;&#24378;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#24212;&#29992;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#29289;&#29702;&#24615;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120; (MCP) &#20316;&#20026;&#24357;&#21512;PC&#27169;&#22411;&#21644;ML&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;MCP&#21033;&#29992;PC&#27169;&#22411;&#21644;GRNNs&#32972;&#21518;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#21516;&#26500;&#24615;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#29289;&#29702;&#36807;&#31243;&#30340;&#36136;&#37327;&#20445;&#25345;&#24615;&#36136;&#65292;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;&#29616;&#25104;&#30340;ML&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36825;&#31181;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#65288;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;LaSyn&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;</title><link>http://arxiv.org/abs/2310.05374</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#21512;&#25104;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis. (arXiv:2310.05374v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;LaSyn&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22521;&#35757;&#39640;&#24615;&#33021;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#25991;&#26412;&#25968;&#25454;&#30456;&#27604;&#65292;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#36890;&#24120;&#26356;&#21152;&#31232;&#32570;&#21644;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#26377;&#25928;&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#28508;&#21464;&#21512;&#25104;&#22120;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#12290;&#36825;&#20123;&#20266;&#22768;&#23398;&#34920;&#31034;&#29992;&#20110;&#22686;&#24378;&#27169;&#22411;&#35757;&#32451;&#30340;&#22768;&#23398;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20302;&#36164;&#28304;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;LaSyn&#12290;&#23545;&#20110;ASR&#65292;LaSyn&#25913;&#36827;&#20102;&#22312;LibriSpeech train-clean-100&#19978;&#35757;&#32451;&#30340;E2E&#22522;&#32447;&#65292;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#19978;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#12290;&#23545;&#20110;SLU&#65292;LaSyn&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;E2E&#22522;&#32447;&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accurac
&lt;/p&gt;</description></item><item><title>OpenPatch&#26159;&#19968;&#20010;3D&#25340;&#36148;&#65292;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#20013;&#38388;&#29305;&#24449;&#26469;&#25551;&#36848;&#27599;&#20010;&#24050;&#30693;&#31867;&#21035;&#30340;&#34917;&#19969;&#34920;&#31034;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.03388</link><description>&lt;p&gt;
OpenPatch:&#19968;&#20010;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;3D&#25340;&#36148;
&lt;/p&gt;
&lt;p&gt;
OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon. (arXiv:2310.03388v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03388
&lt;/p&gt;
&lt;p&gt;
OpenPatch&#26159;&#19968;&#20010;3D&#25340;&#36148;&#65292;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#20013;&#38388;&#29305;&#24449;&#26469;&#25551;&#36848;&#27599;&#20010;&#24050;&#30693;&#31867;&#21035;&#30340;&#34917;&#19969;&#34920;&#31034;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#23454;&#39564;&#23460;&#29615;&#22659;&#36801;&#31227;&#21040;&#24320;&#25918;&#19990;&#30028;&#65292;&#38656;&#35201;&#20351;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#26410;&#30693;&#26465;&#20214;&#12290;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#37096;&#32626;&#36807;&#31243;&#20013;&#20986;&#29616;&#26032;&#30340;&#31867;&#21035;&#21487;&#33021;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#26816;&#27979;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#33021;&#21147;&#24212;&#35813;&#22312;&#38656;&#35201;&#26102;&#20351;&#29992;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#20013;&#36827;&#34892;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#35745;&#31639;&#35757;&#32451;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;&#32477;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#22788;&#29702;2D&#22270;&#20687;&#65292;&#24573;&#35270;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22266;&#26377;&#30340;3D&#29305;&#24615;&#65292;&#24182;&#32463;&#24120;&#28151;&#28102;&#39046;&#22495;&#21644;&#35821;&#20041;&#30340;&#26032;&#39062;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21518;&#32773;&#65292;&#32771;&#34385;&#30001;3D&#28857;&#20113;&#25429;&#25417;&#30340;&#29289;&#20307;&#20960;&#20309;&#32467;&#26500;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#23450;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;OpenPatch&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#31616;&#21333;&#22320;&#25552;&#21462;&#20174;&#20854;&#20013;&#38388;&#29305;&#24449;&#20013;&#25551;&#36848;&#27599;&#20010;&#24050;&#30693;&#31867;&#21035;&#30340;&#19968;&#32452;&#34917;&#19969;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moving deep learning models from the laboratory setting to the open world entails preparing them to handle unforeseen conditions. In several applications the occurrence of novel classes during deployment poses a significant threat, thus it is crucial to effectively detect them. Ideally, this skill should be used when needed without requiring any further computational training effort at every new task. Out-of-distribution detection has attracted significant attention in the last years, however the majority of the studies deal with 2D images ignoring the inherent 3D nature of the real-world and often confusing between domain and semantic novelty. In this work, we focus on the latter, considering the objects geometric structure captured by 3D point clouds regardless of the specific domain. We advance the field by introducing OpenPatch that builds on a large pre-trained model and simply extracts from its intermediate features a set of patch representations that describe each known class. F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01320</link><description>&lt;p&gt;
Avalon&#30340;&#24605;&#32771;&#28216;&#25103;&#65306;&#36890;&#36807;&#36882;&#24402;&#24605;&#32771;&#23545;&#25239;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#22312;LLM&#20316;&#20026;&#26234;&#33021;&#20307;&#39046;&#22495;&#30340;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;LLM&#22788;&#29702;&#30340;&#20449;&#24687;&#22987;&#32456;&#26159;&#35802;&#23454;&#30340;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#31038;&#20250;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27450;&#39575;&#25110;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#36825;&#20010;&#30095;&#24573;&#20351;&#24471;LLM&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25805;&#32437;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21033;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#25506;&#32034;LLM&#22312;&#27450;&#39575;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#20805;&#28385;&#20102;&#38169;&#35823;&#20449;&#24687;&#65292;&#24182;&#38656;&#35201;&#22797;&#26434;&#30340;&#36923;&#36753;&#65292;&#34920;&#29616;&#20026;&#8220;&#24605;&#32771;&#30340;&#28216;&#25103;&#8221;&#12290;&#21463;&#21040;&#20154;&#31867;&#22312;Avalon&#28216;&#25103;&#20013;&#36882;&#24402;&#24605;&#32771;&#21644;&#36879;&#35270;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#65292;&#20197;&#22686;&#24378;LLM&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;ReCon&#32467;&#21512;&#20102;&#20844;&#24335;&#21270;&#24605;&#32771;&#21644;&#23436;&#21892;&#24605;&#32771;&#30340;&#36807;&#31243;&#65307;&#20844;&#24335;&#21270;&#24605;&#32771;&#20135;&#29983;&#21021;&#22987;&#24605;&#32771;&#65292;&#23436;&#21892;&#24605;&#32771;&#23545;&#21021;&#22987;&#24605;&#32771;&#36827;&#34892;&#35843;&#25972;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial tho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31958;&#23615;&#30149;&#32958;&#30149;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#34880;&#28165;&#20195;&#35874;&#29289;&#23545;&#30142;&#30149;&#30340;&#24433;&#21709;&#24182;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#26469;&#39044;&#27979;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#12290;&#26368;&#20248;&#27169;&#22411;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#31639;&#27861;&#65292;&#20854;&#22312;&#31579;&#36873;DN&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#20020;&#24202;&#25928;&#30410;&#21644;&#25311;&#21512;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.16730</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31958;&#23615;&#30149;&#32958;&#30149;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Explainable machine learning-based prediction model for diabetic nephropathy. (arXiv:2309.16730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31958;&#23615;&#30149;&#32958;&#30149;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#34880;&#28165;&#20195;&#35874;&#29289;&#23545;&#30142;&#30149;&#30340;&#24433;&#21709;&#24182;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#26469;&#39044;&#27979;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#12290;&#26368;&#20248;&#27169;&#22411;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#31639;&#27861;&#65292;&#20854;&#22312;&#31579;&#36873;DN&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#20020;&#24202;&#25928;&#30410;&#21644;&#25311;&#21512;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#34880;&#28165;&#20195;&#35874;&#29289;&#23545;&#31958;&#23615;&#30149;&#32958;&#30149;&#65288;DN&#65289;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;DN&#30340;&#24739;&#30149;&#29575;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;2018&#24180;4&#26376;&#33267;2019&#24180;4&#26376;&#22823;&#36830;&#21307;&#31185;&#22823;&#23398;&#38468;&#23646;&#31532;&#20108;&#21307;&#38498;&#30340;548&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#32477;&#23545;&#25910;&#32553;&#21644;&#36873;&#25321;&#31639;&#23376;&#65288;LASSO&#65289;&#22238;&#24402;&#27169;&#22411;&#21644;10&#25240;&#20132;&#21449;&#39564;&#35777;&#36873;&#25321;&#20102;&#26368;&#20248;&#30340;38&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#20915;&#31574;&#26641;&#21644;&#36923;&#36753;&#22238;&#24402;&#65292;&#24182;&#36890;&#36807;AUC-ROC&#26354;&#32447;&#12289;&#20915;&#31574;&#26354;&#32447;&#21644;&#26657;&#20934;&#26354;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;Shapley Additive exPlanations&#65288;SHAP&#65289;&#26041;&#27861;&#26469;&#37327;&#21270;&#26368;&#20248;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#20132;&#20114;&#25928;&#24212;&#12290;XGB&#27169;&#22411;&#22312;&#31579;&#36873;DN&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#65292;&#26368;&#39640;AUC&#20540;&#20026;0.966&#12290;XGB&#27169;&#22411;&#30340;&#20020;&#24202;&#20928;&#25928;&#30410;&#20063;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#19988;&#25311;&#21512;&#24230;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to analyze the effect of serum metabolites on diabetic nephropathy (DN) and predict the prevalence of DN through a machine learning approach. The dataset consists of 548 patients from April 2018 to April 2019 in Second Affiliated Hospital of Dalian Medical University (SAHDMU). We select the optimal 38 features through a Least absolute shrinkage and selection operator (LASSO) regression model and a 10-fold cross-validation. We compare four machine learning algorithms, including eXtreme Gradient Boosting (XGB), random forest, decision tree and logistic regression, by AUC-ROC curves, decision curves, calibration curves. We quantify feature importance and interaction effects in the optimal predictive model by Shapley Additive exPlanations (SHAP) method. The XGB model has the best performance to screen for DN with the highest AUC value of 0.966. The XGB model also gains more clinical net benefits than others and the fitting degree is better. In addition, there are s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#37329;&#34701;&#20132;&#26131;&#65292;&#24182;&#32771;&#34385;&#24773;&#32490;&#20449;&#24687;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#24120;&#35265;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#24212;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16679</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22312;&#32447;&#24773;&#32490;&#20998;&#26512;&#36827;&#34892;&#37329;&#34701;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Leveraging Deep Learning and Online Source Sentiment for Financial Portfolio Management. (arXiv:2309.16679v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#37329;&#34701;&#20132;&#26131;&#65292;&#24182;&#32771;&#34385;&#24773;&#32490;&#20449;&#24687;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#24120;&#35265;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#24212;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#26159;&#25351;&#22312;&#19968;&#31995;&#21015;&#37329;&#34701;&#36164;&#20135;&#65288;&#22914;&#32929;&#31080;&#12289;&#25351;&#25968;&#22522;&#37329;&#12289;&#22806;&#27719;&#25110;&#21152;&#23494;&#36135;&#24065;&#65289;&#19978;&#20998;&#37197;&#36164;&#37329;&#24182;&#36827;&#34892;&#20132;&#26131;&#25805;&#20316;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#21033;&#28070;&#21516;&#26102;&#26368;&#23567;&#21270;&#20132;&#26131;&#25805;&#20316;&#25152;&#36896;&#25104;&#30340;&#25439;&#22833;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19968;&#30452;&#20197;&#26469;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33258;&#21160;&#21270;&#37329;&#34701;&#20132;&#26131;&#23601;&#26159;&#20854;&#20013;&#26368;&#22797;&#26434;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#37329;&#34701;&#20132;&#26131;&#20013;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#35265;&#35299;&#65292;&#20998;&#21035;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#36827;&#34892;&#35752;&#35770;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#19982;&#20132;&#26131;&#36164;&#20135;&#30456;&#20851;&#30340;&#24773;&#32490;&#20449;&#24687;&#65292;&#25105;&#20204;&#36890;&#36807;&#30456;&#24212;&#30340;&#30740;&#31350;&#30740;&#31350;&#35770;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35757;&#32451;&#27492;&#31867;&#37329;&#34701;&#26234;&#33021;&#20307;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#35835;&#32773;&#25552;&#20379;&#24517;&#35201;&#30340;&#30693;&#35782;&#65292;&#20197;&#36991;&#20813;&#36825;&#20123;&#38382;&#39064;&#24182;&#23558;&#35752;&#35770;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#23454;&#36341;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial portfolio management describes the task of distributing funds and conducting trading operations on a set of financial assets, such as stocks, index funds, foreign exchange or cryptocurrencies, aiming to maximize the profit while minimizing the loss incurred by said operations. Deep Learning (DL) methods have been consistently excelling at various tasks and automated financial trading is one of the most complex one of those. This paper aims to provide insight into various DL methods for financial trading, under both the supervised and reinforcement learning schemes. At the same time, taking into consideration sentiment information regarding the traded assets, we discuss and demonstrate their usefulness through corresponding research studies. Finally, we discuss commonly found problems in training such financial agents and equip the reader with the necessary knowledge to avoid these problems and apply the discussed methods in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;&#65292;&#20197;&#23454;&#29616;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22909;&#25511;&#21046;&#65292;&#24182;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#20142;&#24230;&#20197;&#21450;&#26356;&#28385;&#36275;&#29305;&#23450;&#39118;&#26684;&#21644;&#39068;&#33394;&#35201;&#27714;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.15842</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Signal-Leak Bias in Diffusion Models. (arXiv:2309.15842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;&#65292;&#20197;&#23454;&#29616;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22909;&#25511;&#21046;&#65292;&#24182;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#20142;&#24230;&#20197;&#21450;&#26356;&#28385;&#36275;&#29305;&#23450;&#39118;&#26684;&#21644;&#39068;&#33394;&#35201;&#27714;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#23384;&#22312;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#26159;&#30001;&#20449;&#21495;&#27844;&#28431;&#24341;&#36215;&#30340;&#65292;&#20854;&#20998;&#24067;&#19982;&#22122;&#22768;&#20998;&#24067;&#19981;&#19968;&#33268;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27169;&#22411;&#38024;&#23545;&#29305;&#23450;&#39118;&#26684;&#36827;&#34892;&#35843;&#20248;&#26102;&#65292;&#36825;&#31181;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;&#29305;&#21035;&#26174;&#33879;&#65292;&#23548;&#33268;&#39118;&#26684;&#21305;&#37197;&#19981;&#22815;&#20248;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#20449;&#21495;&#27844;&#28431;&#12290;&#25105;&#20204;&#30456;&#21453;&#22320;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;&#65292;&#20197;&#23454;&#29616;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20142;&#24230;&#26356;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#20197;&#21450;&#26356;&#33021;&#21305;&#37197;&#25152;&#38656;&#39118;&#26684;&#25110;&#39068;&#33394;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#23545;&#31354;&#38388;&#39057;&#29575;&#21644;&#20687;&#32032;&#22495;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#22312;&#21021;&#22987;&#28508;&#21464;&#37327;&#20013;&#24341;&#20837;&#20449;&#21495;&#27844;&#28431;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#31526;&#21512;&#39044;&#26399;&#32467;&#26524;&#30340;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a bias in the inference pipeline of most diffusion models. This bias arises from a signal leak whose distribution deviates from the noise distribution, creating a discrepancy between training and inference processes. We demonstrate that this signal-leak bias is particularly significant when models are tuned to a specific style, causing sub-optimal style matching. Recent research tries to avoid the signal leakage during training. We instead show how we can exploit this signal-leak bias in existing diffusion models to allow more control over the generated images. This enables us to generate images with more varied brightness, and images that better match a desired style or color. By modeling the distribution of the signal leak in the spatial frequency and pixel domains, and including a signal leak in the initial latent, we generate images that better match expected results without any additional training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Detach-ROCKET&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#39034;&#24207;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#26680;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#29305;&#24449;&#20998;&#31163;&#26041;&#27861;&#21098;&#26525;&#38750;&#20027;&#35201;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14518</link><description>&lt;p&gt;
Detach-ROCKET: &#22522;&#20110;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#39034;&#24207;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels. (arXiv:2309.14518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Detach-ROCKET&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#39034;&#24207;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#26680;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#29305;&#24449;&#20998;&#31163;&#26041;&#27861;&#21098;&#26525;&#38750;&#20027;&#35201;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22914;&#21307;&#23398;&#12289;&#37329;&#34701;&#12289;&#29615;&#22659;&#31185;&#23398;&#21644;&#21046;&#36896;&#19994;&#65292;&#21487;&#20197;&#23454;&#29616;&#30142;&#30149;&#35786;&#26029;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#32929;&#20215;&#39044;&#27979;&#31561;&#20219;&#21153;&#12290;&#23613;&#31649;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;InceptionTime&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#38656;&#27714;&#30340;&#32321;&#37325;&#65292;&#21487;&#33021;&#20250;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;Rocket&#21450;&#20854;&#34893;&#29983;&#27169;&#22411;&#31561;&#38543;&#26426;&#21367;&#31215;&#26680;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#38543;&#26426;&#29983;&#25104;&#30340;&#22823;&#37327;&#29305;&#24449;&#65292;&#31616;&#21270;&#35757;&#32451;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#36136;&#65292;&#29983;&#25104;&#30340;&#22823;&#37096;&#20998;&#29305;&#24449;&#26159;&#20887;&#20313;&#25110;&#38750;&#20449;&#24687;&#24615;&#30340;&#65292;&#22686;&#21152;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#36127;&#36733;&#24182;&#25439;&#23475;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39034;&#24207;&#29305;&#24449;&#20998;&#31163;&#65288;SFD&#65289;&#20316;&#20026;&#19968;&#31181;&#35782;&#21035;&#21644;&#20462;&#21098;&#36825;&#20123;&#38750;&#20027;&#35201;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;SFD&#21033;&#29992;&#27169;&#22411;&#31995;&#25968;&#26469;&#20272;&#35745;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series classification is essential in many fields, such as medicine, finance, environmental science, and manufacturing, enabling tasks like disease diagnosis, anomaly detection, and stock price prediction. Machine learning models like Recurrent Neural Networks and InceptionTime, while successful in numerous applications, can face scalability limitations due to intensive training requirements. To address this, random convolutional kernel models such as Rocket and its derivatives have emerged, simplifying training and achieving state-of-the-art performance by utilizing a large number of randomly generated features from time series data. However, due to their random nature, most of the generated features are redundant or non-informative, adding unnecessary computational load and compromising generalization. Here, we introduce Sequential Feature Detachment (SFD) as a method to identify and prune these non-essential features. SFD uses model coefficients to estimate feature importance a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#32456;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#26597;&#35810;&#21644;&#23398;&#20064;&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#22312;&#21018;&#24615;&#29289;&#20307;&#25805;&#20316;&#26041;&#38754;&#23454;&#29616;&#25968;&#25454;&#21644;&#26102;&#38388;&#39640;&#25928;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#37325;&#22797;&#20351;&#29992;&#26032;&#33719;&#24471;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#24320;&#25918;&#19990;&#30028;&#21644;&#32456;&#36523;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14321</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#36741;&#21161;&#35821;&#35328;&#35268;&#21010;&#22120;&#30340;&#32456;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lifelong Robot Learning with Human Assisted Language Planners. (arXiv:2309.14321v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#32456;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#26597;&#35810;&#21644;&#23398;&#20064;&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#22312;&#21018;&#24615;&#29289;&#20307;&#25805;&#20316;&#26041;&#38754;&#23454;&#29616;&#25968;&#25454;&#21644;&#26102;&#38388;&#39640;&#25928;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#37325;&#22797;&#20351;&#29992;&#26032;&#33719;&#24471;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#24320;&#25918;&#19990;&#30028;&#21644;&#32456;&#36523;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20687;&#35268;&#21010;&#32773;&#19968;&#26679;&#23558;&#39640;&#32423;&#25351;&#20196;&#20998;&#35299;&#20026;&#21487;&#25191;&#34892;&#25351;&#20196;&#30340;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#22120;&#21482;&#33021;&#20351;&#29992;&#22266;&#23450;&#30340;&#25216;&#33021;&#38598;&#21512;&#36816;&#20316;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLM-based&#35268;&#21010;&#22120;&#26597;&#35810;&#26032;&#25216;&#33021;&#24182;&#20197;&#25968;&#25454;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#26041;&#24335;&#25945;&#25480;&#26426;&#22120;&#20154;&#36825;&#20123;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21018;&#24615;&#29289;&#20307;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#26032;&#33719;&#24471;&#30340;&#25216;&#33021;&#29992;&#20110;&#26410;&#26469;&#30340;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24320;&#25918;&#19990;&#30028;&#21644;&#32456;&#36523;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#23545;&#25552;&#20986;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#35270;&#39057;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#35266;&#30475;&#65306;https://sites.google.com/mit.edu/halp-robot-learning&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been shown to act like planners that can decompose high-level instructions into a sequence of executable instructions. However, current LLM-based planners are only able to operate with a fixed set of skills. We overcome this critical limitation and present a method for using LLM-based planners to query new skills and teach robots these skills in a data and time-efficient manner for rigid object manipulation. Our system can re-use newly acquired skills for future tasks, demonstrating the potential of open world and lifelong learning. We evaluate the proposed framework on multiple tasks in simulation and the real world. Videos are available at: https://sites.google.com/mit.edu/halp-robot-learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#32570;&#36135;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#22810;&#31181;&#20998;&#31867;&#25216;&#26415;&#21644;&#25104;&#26412;&#32771;&#34385;&#65292;&#36890;&#36807;&#25552;&#39640;&#26381;&#21153;&#27700;&#24179;&#26469;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#25972;&#20307;&#32452;&#32455;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.13837</link><description>&lt;p&gt;
&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#32570;&#36135;&#39044;&#27979;: &#20998;&#31867;&#25216;&#26415;&#21644;&#25104;&#26412;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations. (arXiv:2309.13837v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#32570;&#36135;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#22810;&#31181;&#20998;&#31867;&#25216;&#26415;&#21644;&#25104;&#26412;&#32771;&#34385;&#65292;&#36890;&#36807;&#25552;&#39640;&#26381;&#21153;&#27700;&#24179;&#26469;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#25972;&#20307;&#32452;&#32455;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#32570;&#36135;&#24773;&#20917;&#12290;&#32570;&#36135;&#26159;&#25351;&#30001;&#20110;&#24211;&#23384;&#32791;&#23613;&#32780;&#26080;&#27861;&#31435;&#21363;&#28385;&#36275;&#30340;&#35746;&#21333;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#31181;&#20998;&#31867;&#25216;&#26415;&#65292;&#21253;&#25324;&#24179;&#34913;&#35013;&#34955;&#20998;&#31867;&#22120;&#12289;&#27169;&#31946;&#36923;&#36753;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;-&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;ROC-AUC&#21644;PR-AUC&#31561;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#32771;&#34385;&#20102;&#21033;&#28070;&#20989;&#25968;&#21644;&#38169;&#20998;&#25104;&#26412;&#65292;&#32771;&#34385;&#20102;&#19982;&#24211;&#23384;&#31649;&#29702;&#21644;&#32570;&#36135;&#22788;&#29702;&#30456;&#20851;&#30340;&#36130;&#21153;&#24433;&#21709;&#21644;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#24211;&#23384;&#31995;&#32479;&#30340;&#26381;&#21153;&#27700;&#24179;&#65292;&#20174;&#32780;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#25972;&#20307;&#32452;&#32455;&#32489;&#25928;&#12290;&#22312;&#21830;&#19994;&#24212;&#29992;&#20013;&#32771;&#34385;&#21487;&#35299;&#37322;&#24615;&#26159;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#26412;&#30740;&#31350;&#36824;&#24212;&#29992;&#20102;&#25490;&#21015;&#37325;&#35201;&#24615;&#26041;&#27861;&#26469;&#36873;&#25321;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces an advanced analytical approach for predicting backorders in inventory management. Backorder refers to an order that cannot be immediately fulfilled due to stock depletion. Multiple classification techniques, including Balanced Bagging Classifiers, Fuzzy Logic, Variational Autoencoder - Generative Adversarial Networks, and Multi-layer Perceptron classifiers, are assessed in this work using performance evaluation metrics such as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit function and misclassification costs, considering the financial implications and costs associated with inventory management and backorder handling. The results demonstrate the effectiveness of the predictive model in enhancing inventory system service levels, which leads to customer satisfaction and overall organizational performance. Considering interpretability is a significant aspect of using AI in commercial applications, permutation importance is applied to the selected mo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13135</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#19981;&#33391;&#32467;&#26524;&#21644;&#24739;&#32773;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#22024;&#26434;&#21644;&#38388;&#27463;&#24615;&#65292;&#23454;&#38469;&#20013;&#39044;&#27979;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#36825;&#20123;&#25361;&#25112;&#36890;&#24120;&#36890;&#36807;&#22806;&#37096;&#22240;&#32032;&#35825;&#23548;&#30340;&#21464;&#21270;&#28857;&#65288;&#22914;&#33647;&#29289;&#20351;&#29992;&#65289;&#32780;&#21152;&#21095;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#25928;&#24212;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21463;&#27835;&#30103;&#24433;&#21709;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20351;&#29992;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#39044;&#27979;&#34880;&#31958;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#33647;&#21160;&#23398;&#32534;&#30721;&#22120;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36229;&#36807;&#22522;&#20934;&#32422;11&#65285;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36229;&#36807;8&#65285;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#20363;&#22914;&#21457;&#20986;&#20851;&#20110;&#24847;&#22806;&#27835;&#30103;&#21453;&#24212;&#30340;&#26089;&#26399;&#35686;&#21578;&#65292;&#25110;&#24110;&#21161;&#34920;&#24449;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#25552;&#20379;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.12671</link><description>&lt;p&gt;
&#22914;&#20309;&#24494;&#35843;&#27169;&#22411;&#65306;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization. (arXiv:2309.12671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#25552;&#20379;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#21644;&#25512;&#23548;&#20986;&#20855;&#26377;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#30340;&#26377;&#25928;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#31639;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#27169;&#22411;&#23398;&#20064;&#21644;&#31574;&#30053;&#20248;&#21270;&#20043;&#38388;&#30340;&#39640;&#32806;&#21512;&#24615;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#22238;&#25253;&#24046;&#24322;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#27169;&#22411;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#36807;&#22810;&#30340;&#27169;&#22411;&#26356;&#26032;&#32780;&#24615;&#33021;&#19979;&#38477;&#12290;&#20854;&#20182;&#26041;&#27861;&#20351;&#29992;&#24615;&#33021;&#24046;&#24322;&#36793;&#30028;&#26469;&#26126;&#30830;&#32771;&#34385;&#27169;&#22411;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#38408;&#20540;&#26469;&#38480;&#21046;&#27169;&#22411;&#20559;&#31227;&#65292;&#23548;&#33268;&#23545;&#38408;&#20540;&#30340;&#20005;&#37325;&#20381;&#36182;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#36866;&#24212;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#19968;&#20010;&#21487;&#20197;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#28982;&#21518;&#21046;&#23450;&#19968;&#20010;&#24494;&#35843;&#36807;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#33719;&#24471;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#65292;&#21516;&#26102;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30452;&#35266;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#27169;&#22411;&#30340;&#39640;&#25928;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#25216;&#26415;&#26469;&#32500;&#25345;&#26816;&#27979;&#22120;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#39057;&#29575;&#21644;&#25968;&#25454;&#20351;&#29992;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.09807</link><description>&lt;p&gt;
&#25209;&#37327;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#27169;&#22411;&#30340;&#39640;&#25928;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient Concept Drift Handling for Batch Android Malware Detection Models. (arXiv:2309.09807v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#27169;&#22411;&#30340;&#39640;&#25928;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#25216;&#26415;&#26469;&#32500;&#25345;&#26816;&#27979;&#22120;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#39057;&#29575;&#21644;&#25968;&#25454;&#20351;&#29992;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24555;&#36895;&#21457;&#23637;&#24615;&#23545;&#20110;&#38745;&#24577;&#25209;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#24555;&#23601;&#20250;&#36807;&#26102;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#29616;&#26377;&#25991;&#29486;&#23545;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#20851;&#27880;&#26377;&#38480;&#65292;&#35768;&#22810;&#20808;&#36827;&#30340;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#65288;&#22914;Drebin&#12289;DroidDet&#21644;MaMaDroid&#65289;&#20381;&#36182;&#20110;&#38745;&#24577;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37325;&#26032;&#35757;&#32451;&#25216;&#26415;&#22914;&#20309;&#33021;&#22815;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#20445;&#25345;&#26816;&#27979;&#22120;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#26041;&#38754;&#23545;&#26816;&#27979;&#22120;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65306;1&#65289;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#30340;&#39057;&#29575;&#65292;2&#65289;&#29992;&#20110;&#37325;&#26032;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#31532;&#19968;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#21608;&#26399;&#24615;&#37325;&#26032;&#35757;&#32451;&#19982;&#20165;&#22312;&#24517;&#35201;&#26102;&#35302;&#21457;&#37325;&#26032;&#35757;&#32451;&#30340;&#26356;&#20808;&#36827;&#30340;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#31532;&#20108;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#20943;&#23569;&#29992;&#20110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#37327;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22266;&#23450;si
&lt;/p&gt;
&lt;p&gt;
The rapidly evolving nature of Android apps poses a significant challenge to static batch machine learning algorithms employed in malware detection systems, as they quickly become obsolete. Despite this challenge, the existing literature pays limited attention to addressing this issue, with many advanced Android malware detection approaches, such as Drebin, DroidDet and MaMaDroid, relying on static models. In this work, we show how retraining techniques are able to maintain detector capabilities over time. Particularly, we analyze the effect of two aspects in the efficiency and performance of the detectors: 1) the frequency with which the models are retrained, and 2) the data used for retraining. In the first experiment, we compare periodic retraining with a more advanced concept drift detection method that triggers retraining only when necessary. In the second experiment, we analyze sampling methods to reduce the amount of data used to retrain models. Specifically, we compare fixed si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06782</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21315;&#20806;&#32423;&#25968;&#25454;&#38598;&#29992;&#20110;&#31890;&#23376;&#27969;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#22522;&#20110;&#39640;&#24230;&#31890;&#24230;&#25506;&#27979;&#22120;&#27169;&#25311;&#30340;&#23436;&#25972;&#20107;&#20214;&#37325;&#24314;&#65292;&#30740;&#31350;&#20102;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#31890;&#23376;&#27969;&#65288;PF&#65289;&#37325;&#24314;&#21487;&#36890;&#36807;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#22242;&#31751;&#25110;&#20987;&#20013;&#26469;&#26500;&#24314;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#20869;&#26680;&#30340;&#21464;&#25442;&#22120;&#65292;&#24182;&#35777;&#26126;&#20004;&#32773;&#37117;&#36991;&#20813;&#20102;&#20108;&#27425;&#20869;&#23384;&#20998;&#37197;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30495;&#23454;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#24471;&#27169;&#22411;&#22312;&#30828;&#20214;&#22788;&#29702;&#22120;&#19978;&#20855;&#26377;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#25903;&#25345;NVIDIA, AMD&#21644;&#33521;&#29305;&#23572; Habana&#21345;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#21487;&#20197;&#22312;&#30001;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#20987;&#20013;&#32452;&#25104;&#30340;&#39640;&#31890;&#24230;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33719;&#24471;&#19982;&#22522;&#20934;&#30456;&#31454;&#20105;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#26377;&#20851;&#22797;&#29616;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#27748;&#26222;&#26862;&#25277;&#26679;&#25506;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#21160;&#20316;&#38598;&#21512;&#22823;&#23567;&#20026;&#25351;&#25968;&#32423;&#21035;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.10238</link><description>&lt;p&gt;
Thompson Sampling&#29992;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit. (arXiv:2308.10238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10238
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#27748;&#26222;&#26862;&#25277;&#26679;&#25506;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#21160;&#20316;&#38598;&#21512;&#22823;&#23567;&#20026;&#25351;&#25968;&#32423;&#21035;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;&#65288;R-CPE-MAB&#65289;&#38382;&#39064;&#12290;&#22312;R-CPE-MAB&#20013;&#65292;&#29609;&#23478;&#20174;&#32473;&#23450;&#30340;d&#20010;&#38543;&#26426;&#33218;&#20013;&#36873;&#25321;&#19968;&#20010;&#65292;&#27599;&#20010;&#33218;s&#30340;&#22870;&#21169;&#36981;&#24490;&#26410;&#30693;&#20998;&#24067;&#65292;&#20854;&#24179;&#22343;&#20540;&#20026;&#956;s&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#65292;&#29609;&#23478;&#25289;&#21160;&#19968;&#20010;&#33218;&#24182;&#35266;&#23519;&#20854;&#22870;&#21169;&#12290;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#33218;&#25289;&#21160;&#27425;&#25968;&#26469;&#30830;&#23450;&#26368;&#20248;&#21160;&#20316;&#960;* = argmax&#960;&#8712;A &#956;T&#960;&#65292;&#20854;&#20013;A&#26159;&#26377;&#38480;&#22823;&#23567;&#30340;&#23454;&#20540;&#21160;&#20316;&#38598;&#21512;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20551;&#35774;&#21160;&#20316;&#38598;&#21512;A&#30340;&#22823;&#23567;&#22312;d&#30340;&#22810;&#39033;&#24335;&#32423;&#21035;&#19978;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#27748;&#26222;&#26862;&#25277;&#26679;&#25506;&#32034;&#65288;GenTS-Explore&#65289;&#31639;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#35299;&#20915;&#21160;&#20316;&#38598;&#21512;&#22823;&#23567;&#22312;d&#30340;&#25351;&#25968;&#32423;&#21035;&#19978;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#30456;&#20851;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem. In R-CPE-MAB, a player is given $d$ stochastic arms, and the reward of each arm $s\in\{1, \ldots, d\}$ follows an unknown distribution with mean $\mu_s$. In each time step, a player pulls a single arm and observes its reward. The player's goal is to identify the optimal \emph{action} $\boldsymbol{\pi}^{*} = \argmax_{\boldsymbol{\pi} \in \mathcal{A}} \boldsymbol{\mu}^{\top}\boldsymbol{\pi}$ from a finite-sized real-valued \emph{action set} $\mathcal{A}\subset \mathbb{R}^{d}$ with as few arm pulls as possible. Previous methods in the R-CPE-MAB assume that the size of the action set $\mathcal{A}$ is polynomial in $d$. We introduce an algorithm named the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large in $d$. We also introduce a novel problem-dependent sample complexity 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#30315;&#30187;&#39044;&#27979;&#20026;&#30446;&#26631;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21644;&#37327;&#21270;&#24739;&#32773;&#29305;&#23450;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#26368;&#26032;&#30340;&#36335;&#24452;&#31614;&#21517;&#31639;&#27861;&#65292;&#25506;&#32034;&#20854;&#22312;&#30315;&#30187;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20026;&#20010;&#24615;&#21270;&#30340;&#30315;&#30187;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.09312</link><description>&lt;p&gt;
&#30315;&#30187;&#39044;&#27979;&#20013;&#30340;&#36335;&#24452;&#31614;&#21517;
&lt;/p&gt;
&lt;p&gt;
Path Signatures for Seizure Forecasting. (arXiv:2308.09312v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#30315;&#30187;&#39044;&#27979;&#20026;&#30446;&#26631;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21644;&#37327;&#21270;&#24739;&#32773;&#29305;&#23450;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#26368;&#26032;&#30340;&#36335;&#24452;&#31614;&#21517;&#31639;&#27861;&#65292;&#25506;&#32034;&#20854;&#22312;&#30315;&#30187;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20026;&#20010;&#24615;&#21270;&#30340;&#30315;&#30187;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#39044;&#27979;&#31995;&#32479;&#29366;&#24577;&#26159;&#35768;&#22810;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#65289;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#20174;&#22823;&#33041;&#27979;&#37327;&#20013;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26082;&#27809;&#26377;&#23436;&#25972;&#30340;&#25551;&#36848;&#24213;&#23618;&#22823;&#33041;&#21160;&#24577;&#30340;&#27169;&#22411;&#65292;&#20063;&#27809;&#26377;&#21333;&#20010;&#24739;&#32773;&#34920;&#29616;&#20986;&#21333;&#19968;&#30340;&#30315;&#30187;&#21457;&#20316;&#27169;&#24335;&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#22797;&#26434;&#12290;&#22522;&#20110;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#21160;&#21457;&#29616;&#21644;&#37327;&#21270;&#21487;&#29992;&#20110;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#30315;&#30187;&#39044;&#27979;&#30340;&#32479;&#35745;&#29305;&#24449;&#65288;&#29983;&#29289;&#26631;&#24535;&#29289;&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#31639;&#27861;&#65292;&#23588;&#20854;&#26159;&#36335;&#24452;&#31614;&#21517;&#65292;&#21363;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#29305;&#21035;&#20540;&#24471;&#20851;&#27880;&#30340;&#26159;&#65292;&#19982;&#31616;&#21333;&#30340;&#32447;&#24615;&#29305;&#24449;&#30456;&#27604;&#65292;&#36825;&#32452;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#30340;&#25512;&#26029;&#22522;&#20110;&#32479;&#35745;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#24102;&#26377;&#20869;&#32622;&#30340;&#23376;&#38598;&#36873;&#25321;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting the state of a system from an observed time series is the subject of research in many domains, such as computational neuroscience. Here, the prediction of epileptic seizures from brain measurements is an unresolved problem. There are neither complete models describing underlying brain dynamics, nor do individual patients exhibit a single seizure onset pattern, which complicates the development of a `one-size-fits-all' solution. Based on a longitudinal patient data set, we address the automated discovery and quantification of statistical features (biomarkers) that can be used to forecast seizures in a patient-specific way. We use existing and novel feature extraction algorithms, in particular the path signature, a recent development in time series analysis. Of particular interest is how this set of complex, nonlinear features performs compared to simpler, linear features on this task. Our inference is based on statistical classification algorithms with in-built subset select
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;Bard&#21644;ChatGPT&#22312;&#21313;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#26041;&#35328;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#21830;&#19994;&#31995;&#32479;&#65292;&#20294;&#22312;&#21476;&#20856;&#38463;&#25289;&#20271;&#35821;&#21644;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#33853;&#21518;&#20110;&#35895;&#27468;&#32763;&#35793;&#31561;&#21830;&#19994;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.03051</link><description>&lt;p&gt;
&#12298;TARJAMAT: Bard&#21644;ChatGPT&#22312;&#21313;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#35780;&#20272;&#12299;
&lt;/p&gt;
&lt;p&gt;
TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties. (arXiv:2308.03051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03051
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;Bard&#21644;ChatGPT&#22312;&#21313;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#26041;&#35328;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#21830;&#19994;&#31995;&#32479;&#65292;&#20294;&#22312;&#21476;&#20856;&#38463;&#25289;&#20271;&#35821;&#21644;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#33853;&#21518;&#20110;&#35895;&#27468;&#32763;&#35793;&#31561;&#21830;&#19994;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;ChatGPT&#21644;Bard&#36825;&#26679;&#30340;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#35748;&#20026;&#22312;&#22810;&#35821;&#35328;&#19978;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#35328;&#21253;&#23481;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#23545;Bard&#21644;ChatGPT&#65288;&#21253;&#25324;GPT-3.5&#21644;GPT-4&#65289;&#22312;&#21313;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#21476;&#20856;&#38463;&#25289;&#20271;&#35821;&#65288;CA&#65289;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#65288;MSA&#65289;&#21644;&#20960;&#31181;&#22269;&#23478;&#26041;&#35328;&#21464;&#20307;&#31561;&#22810;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#22312;&#23384;&#22312;&#23569;&#37327;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#26041;&#35328;&#19978;&#21487;&#33021;&#20250;&#36935;&#21040;&#25361;&#25112;&#65292;&#20294;&#24179;&#22343;&#32780;&#35328;&#65292;&#23427;&#20204;&#27604;&#29616;&#26377;&#30340;&#21830;&#19994;&#31995;&#32479;&#26356;&#25797;&#38271;&#32763;&#35793;&#26041;&#35328;&#12290;&#28982;&#32780;&#65292;&#22312;CA&#21644;MSA&#26041;&#38754;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;LLM&#19982;&#35895;&#27468;&#32763;&#35793;&#31561;&#21830;&#19994;&#31995;&#32479;&#30456;&#27604;&#20173;&#28982;&#26377;&#25152;&#19981;&#36275;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#65292;&#20197;&#23457;&#26597;&#30456;&#23545;&#36739;&#26032;&#30340;&#27169;&#22411;Bard&#22312;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic (CA), Modern Standard Arabic (MSA), and several country-level dialectal variants. Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist, but on average are better translators of dialects than existing commercial systems. On CA and MSA, instruction-tuned LLMs, however, trail behind commercial systems such as Google Translate. Finally, we undertake a human-centric study to scrutinize the efficacy of the relatively recent model, Bard, in following human instructio
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15317</link><description>&lt;p&gt;
DiffKendall:&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation. (arXiv:2307.15317v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#23558;&#22312;&#22522;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#27169;&#22411;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#26032;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#26032;&#31867;&#21035;&#19978;&#36890;&#36947;&#19978;&#29305;&#24449;&#20540;&#30340;&#20998;&#24067;&#30456;&#23545;&#22343;&#21248;&#65292;&#38590;&#20197;&#30830;&#23450;&#26032;&#20219;&#21153;&#20013;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#12290;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36127;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#65292;&#26469;&#34913;&#37327;&#20004;&#20010;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#39640;&#20960;&#20309;&#30456;&#20284;&#24230;&#30340;&#29305;&#24449;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#20041;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning aims to adapt models trained on the base dataset to novel tasks where the categories are not seen by the model before. This often leads to a relatively uniform distribution of feature values across channels on novel classes, posing challenges in determining channel importance for novel tasks. Standard few-shot learning methods employ geometric similarity metrics such as cosine similarity and negative Euclidean distance to gauge the semantic relatedness between two features. However, features with high geometric similarities may carry distinct semantics, especially in the context of few-shot learning. In this paper, we demonstrate that the importance ranking of feature channels is a more reliable indicator for few-shot learning than geometric similarity metrics. We observe that replacing the geometric similarity metric with Kendall's rank correlation only during inference is able to improve the performance of few-shot learning across a wide range of datasets with diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.14642</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65306;&#25105;&#20204;&#24212;&#35813;&#22362;&#25345;&#21040;&#24213;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#65292;&#29305;&#21035;&#26159;&#30528;&#38470;&#31283;&#23450;&#65288;STL&#65289;&#20272;&#35745;&#22120;&#65292;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#25910;&#25947;&#20110;&#20960;&#20309;&#65288;&#20256;&#32479;&#19978;&#31216;&#20026;&#8220;&#32447;&#24615;&#8221;&#65289;&#36895;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;STL&#20272;&#35745;&#22120;&#30340;&#26799;&#24230;&#26041;&#24046;&#30340;&#20108;&#27425;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#21253;&#25324;&#20102;&#35823;&#25351;&#23450;&#30340;&#21464;&#20998;&#26063;&#12290;&#32467;&#21512;&#20808;&#21069;&#20851;&#20110;&#20108;&#27425;&#26041;&#24046;&#26465;&#20214;&#30340;&#24037;&#20316;&#65292;&#36825;&#30452;&#25509;&#26263;&#31034;&#20102;&#22312;&#20351;&#29992;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;BBVI&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#29616;&#26377;&#23545;&#20110;&#27491;&#24120;&#23553;&#38381;&#24418;&#24335;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#20998;&#26512;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#20854;&#19982;STL&#20272;&#35745;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20026;&#20004;&#32773;&#25552;&#20379;&#26126;&#30830;&#30340;&#38750;&#28176;&#36827;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;</title><link>http://arxiv.org/abs/2307.14619</link><description>&lt;p&gt;
&#27169;&#20223;&#22797;&#26434;&#36712;&#36857;&#65306;&#26725;&#25509;&#20302;&#23618;&#31283;&#23450;&#24615;&#19982;&#39640;&#23618;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#38543;&#26426;&#12289;&#38750;&#39532;&#23572;&#21487;&#22827;&#12289;&#28508;&#22312;&#22810;&#27169;&#24577;&#65288;&#21363;&#8220;&#22797;&#26434;&#8221;&#65289;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20302;&#23618;&#25511;&#21046;&#22120;&#65288;&#26080;&#35770;&#26159;&#23398;&#20064;&#30340;&#36824;&#26159;&#38544;&#21547;&#30340;&#65289;&#26469;&#31283;&#23450;&#22260;&#32469;&#19987;&#23478;&#28436;&#31034;&#30340;&#27169;&#20223;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#65288;a&#65289;&#21512;&#36866;&#30340;&#20302;&#23618;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#65288;b&#65289;&#23398;&#20064;&#31574;&#30053;&#30340;&#38543;&#26426;&#36830;&#32493;&#24615;&#23646;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24635;&#21464;&#24046;&#36830;&#32493;&#24615;&#8221;&#65289;&#65288;TVC&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#31934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#29366;&#24577;&#20998;&#24067;&#19978;&#30340;&#34892;&#21160;&#30340;&#27169;&#20223;&#32773;&#20250;&#19982;&#28436;&#31034;&#32773;&#23545;&#25972;&#20010;&#36712;&#36857;&#30340;&#20998;&#24067;&#30456;&#36817;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#23558;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#35268;&#21017;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#25216;&#24039;&#30456;&#32467;&#21512;&#65288;&#21363;&#22312;&#25191;&#34892;&#26102;&#28155;&#21152;&#22686;&#24378;&#22122;&#22768;&#65289;&#26469;&#30830;&#20445;TVC&#24182;&#19988;&#26368;&#23567;&#31243;&#24230;&#19978;&#38477;&#20302;&#31934;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20445;&#35777;&#23454;&#20363;&#21270;&#20026;&#30001;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#23398;&#20064;&#32773;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#28436;&#31034;&#32773;&#30340;&#20998;&#24067;&#65292;&#21017;&#26368;&#32456;&#23436;&#25104;&#36825;&#31181;&#23454;&#20363;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accuratel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13885</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#39640;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#23545;&#22122;&#22768;&#36755;&#20837;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22122;&#22768;&#65288;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#65289;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#26469;&#25429;&#25417;&#65292;&#21363;&#22312;&#36755;&#20837;&#21608;&#22260;&#30340;&#23616;&#37096;&#21306;&#22495;&#20869;&#27169;&#22411;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#30340;&#35745;&#31639;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#26420;&#32032;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#24320;&#21457;&#20102;&#39318;&#20010;&#20998;&#26512;&#20272;&#35745;&#22120;&#65292;&#20197;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#20272;&#35745;&#22120;&#30340;&#25512;&#23548;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#40065;&#26834;&#24615;&#19982;&#38543;&#26426;&#24179;&#28369;&#21644;softmax&#27010;&#29575;&#31561;&#27010;&#24565;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UGAT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#36716;&#31227;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12388</link><description>&lt;p&gt;
&#38754;&#21521;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22522;&#20110;&#23454;&#20363;&#30340;&#34892;&#21160;&#36716;&#25442;&#30340;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control. (arXiv:2307.12388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UGAT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#36716;&#31227;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#25968;&#30334;&#19975;&#20154;&#26085;&#24120;&#29983;&#27963;&#30340;&#22797;&#26434;&#32780;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#22522;&#20110;RL&#30340;TSC&#26041;&#27861;&#20027;&#35201;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#23384;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20043;&#38388;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21040;&#23454;&#38469;&#29615;&#22659;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;UGAT&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#20197;&#20943;&#36731;&#36716;&#31227;&#21160;&#24577;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#23558;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#23398;&#20064;&#31574;&#30053;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#20132;&#36890;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#36716;&#31227;&#21518;&#30340;RL&#31574;&#30053;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#29702;&#35299;&#20102;&#20851;&#31995;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#31181;&#32676;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#31995;&#30693;&#35782;&#33976;&#39311;&#33021;&#22815;&#23548;&#33268;&#20302;&#32858;&#31867;&#35823;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.11030</link><description>&lt;p&gt;
&#38598;&#32676;&#24863;&#30693;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65306;&#20851;&#31995;&#30693;&#35782;&#33976;&#39311;&#21487;&#35777;&#26126;&#30340;&#23398;&#20064;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering. (arXiv:2307.11030v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11030
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#29702;&#35299;&#20102;&#20851;&#31995;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#31181;&#32676;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#31995;&#30693;&#35782;&#33976;&#39311;&#33021;&#22815;&#23548;&#33268;&#20302;&#32858;&#31867;&#35823;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#20851;&#31995;&#30340;&#30693;&#35782;&#33976;&#39311;&#22312;&#21305;&#37197;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#20043;&#38388;&#30340;&#29305;&#24449;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#21644;&#23454;&#38469;&#24847;&#20041;&#65292;&#20294;&#23545;&#20110;&#21508;&#31181;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#20854;&#30456;&#24212;&#30340;&#29702;&#35770;&#35299;&#37322;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#29702;&#35299;&#20851;&#31995;&#30693;&#35782;&#33976;&#39311;&#65288;RKD&#65289;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#21322;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;RKD&#35270;&#20026;&#25945;&#24072;&#27169;&#22411;&#25581;&#31034;&#30340;&#30001;&#31181;&#32676;&#20135;&#29983;&#30340;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#12290;&#36890;&#36807;&#34913;&#37327;&#39044;&#27979;&#21644;&#22522;&#26412;&#20107;&#23454;&#32858;&#31867;&#20043;&#38388;&#24046;&#24322;&#30340;&#32858;&#31867;&#35823;&#24046;&#27010;&#24565;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#31181;&#32676;&#19978;&#30340;RKD&#21487;&#35777;&#26126;&#22320;&#23548;&#33268;&#20302;&#32858;&#31867;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#26377;&#38480;&#26080;&#26631;&#31614;&#26679;&#26412;&#30340;RKD&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#23545;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#38598;&#32676;&#24863;&#30693;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#23637;&#31034;&#20102;RKD&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#26377;&#27169;&#31946;&#22320;&#38754;&#30495;&#30456;&#30340;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32570;&#20047;&#26126;&#30830;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20302;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09302</link><description>&lt;p&gt;
&#21547;&#26377;&#19981;&#30830;&#23450;&#22320;&#38754;&#30495;&#30456;&#30340;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction under ambiguous ground truth. (arXiv:2307.09302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#26377;&#27169;&#31946;&#22320;&#38754;&#30495;&#30456;&#30340;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32570;&#20047;&#26126;&#30830;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20302;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31526;&#21512;&#39044;&#27979;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#32622;&#20449;&#21306;&#38388;&#26469;&#36827;&#34892;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#30495;&#27491;&#31867;&#21035;&#30340;&#29992;&#25143;&#25351;&#23450;&#30340;&#27010;&#29575;&#12290;&#36825;&#36890;&#24120;&#20551;&#35774;&#26377;&#19968;&#20010;&#29420;&#31435;&#30340;&#26657;&#20934;&#38598;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#35775;&#38382;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#26631;&#31614;&#24456;&#38590;&#33719;&#24471;&#65292;&#24182;&#19988;&#36890;&#24120;&#36890;&#36807;&#32858;&#21512;&#19987;&#23478;&#24847;&#35265;&#26469;&#36817;&#20284;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#36866;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;&#25968;&#25454;&#38598;&#22914;CIFAR&#21644;ImageNet&#12290;&#20351;&#29992;&#36825;&#26679;&#30340;&#26631;&#31614;&#24212;&#29992;&#31526;&#21512;&#39044;&#27979;&#20250;&#20302;&#20272;&#19981;&#30830;&#23450;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#24403;&#19987;&#23478;&#24847;&#35265;&#26080;&#27861;&#35299;&#20915;&#26102;&#65292;&#26631;&#31614;&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#27169;&#31946;&#24615;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#27809;&#26377;&#8220;&#28165;&#26224;&#8221;&#12289;&#26126;&#30830;&#30340;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#65292;&#32780;&#22312;&#26657;&#20934;&#36807;&#31243;&#20013;&#24212;&#35813;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#36825;&#31181;&#27169;&#31946;&#22320;&#38754;&#30495;&#30456;&#24773;&#26223;&#24320;&#21457;&#20102;&#19968;&#20010;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20381;&#36182;&#20110;&#23545;&#28508;&#22312;&#27169;&#31946;&#24615;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability. This generally assumes the availability of a held-out calibration set with access to ground truth labels. Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions. In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet. Applying conformal prediction using such labels underestimates uncertainty. Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels. That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration. In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlyi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03838</link><description>&lt;p&gt;
RADAR: &#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20197;&#21450;ChatGPT&#31867;&#24212;&#29992;&#30340;&#26222;&#21450;&#24050;&#32463;&#27169;&#31946;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#21644;&#31038;&#20250;&#39044;&#26399;&#30340;&#38761;&#21629;&#24615;&#21464;&#21270;&#22806;&#65292;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65288;AI&#25991;&#26412;&#65289;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22256;&#38590;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#28389;&#29992;&#21644;&#20844;&#24179;&#24615;&#25361;&#25112;&#65292;&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#29983;&#25104;&#65292;&#25220;&#34989;&#20197;&#21450;&#23545;&#26080;&#36764;&#20316;&#32773;&#30340;&#38169;&#35823;&#25351;&#25511;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#22522;&#20110;LLM&#30340;&#25913;&#20889;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;RADAR&#22522;&#20110;&#19968;&#20010;&#25913;&#20889;&#22120;&#21644;&#19968;&#20010;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#25913;&#20889;&#22120;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#36924;&#30495;&#30340;&#20869;&#23481;&#20197;&#35268;&#36991;AI&#25991;&#26412;&#26816;&#27979;&#12290;RADAR&#20351;&#29992;&#26469;&#33258;&#26816;&#27979;&#22120;&#30340;&#21453;&#39304;&#26469;&#26356;&#26032;&#25913;&#20889;&#22120;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice vers
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#25239;&#27169;&#22411;&#65288;QUAM&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;QUAM&#35782;&#21035;&#25972;&#20010;&#31215;&#20998;&#19979;&#20056;&#31215;&#36739;&#22823;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21518;&#39564;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QUAM&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#36817;&#20284;&#35823;&#24046;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.03217</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#27169;&#22411;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantification of Uncertainty with Adversarial Models. (arXiv:2307.03217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03217
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#25239;&#27169;&#22411;&#65288;QUAM&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;QUAM&#35782;&#21035;&#25972;&#20010;&#31215;&#20998;&#19979;&#20056;&#31215;&#36739;&#22823;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21518;&#39564;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QUAM&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#36817;&#20284;&#35823;&#24046;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#21487;&#25805;&#20316;&#30340;&#39044;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#25955;&#24230;&#20989;&#25968;&#21644;&#21518;&#39564;&#30340;&#20056;&#31215;&#30340;&#31215;&#20998;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22914;Deep Ensembles&#25110;MC dropout&#22312;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#32771;&#34385;&#21518;&#39564;&#22312;&#37319;&#26679;&#27169;&#22411;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#25239;&#27169;&#22411;&#65288;QUAM&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;QUAM&#35782;&#21035;&#25972;&#20010;&#31215;&#20998;&#19979;&#20056;&#31215;&#36739;&#22823;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21518;&#39564;&#12290;&#22240;&#27492;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QUAM&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#36817;&#20284;&#35823;&#24046;&#26356;&#23567;&#12290;&#20056;&#31215;&#36739;&#22823;&#30340;&#27169;&#22411;&#23545;&#24212;&#20110;&#23545;&#25239;&#27169;&#22411;&#65288;&#19981;&#26159;&#23545;&#25239;&#24615;&#31034;&#20363;&#65281;&#65289;&#12290;&#23545;&#25239;&#27169;&#22411;&#26082;&#26377;&#36739;&#39640;&#30340;&#21518;&#39564;&#65292;&#20063;&#26377;&#20854;&#39044;&#27979;&#19982;&#20854;&#20182;&#27169;&#22411;&#20043;&#38388;&#30340;&#36739;&#39640;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#22120;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16635</link><description>&lt;p&gt;
&#25913;&#36827;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness in Deepfake Detection. (arXiv:2306.16635v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#22120;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20986;&#26377;&#25928;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#65292;&#20294;&#26159;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24320;&#21457;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#26102;&#25152;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#31181;&#26063;&#21644;/&#25110;&#24615;&#21035;&#30340;&#20154;&#32676;&#30340;&#19981;&#20844;&#24179;&#34920;&#29616;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#32676;&#20307;&#21463;&#21040;&#19981;&#20844;&#24179;&#30340;&#23450;&#20301;&#25110;&#34987;&#25490;&#38500;&#22312;&#26816;&#27979;&#20043;&#22806;&#65292;&#20174;&#32780;&#35753;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#28145;&#24230;&#20266;&#36896;&#25805;&#32437;&#33286;&#35770;&#24182;&#30772;&#22351;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;&#34429;&#28982;&#36825;&#20123;&#30740;&#31350;&#30528;&#37325;&#20110;&#30830;&#23450;&#21644;&#35780;&#20272;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#24320;&#21457;&#20986;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31639;&#27861;&#23618;&#38754;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#22312;&#19981;&#32771;&#34385;&#25110;&#32771;&#34385;&#20154;&#21475;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20844;&#24179;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#12290;&#23545;&#22235;&#20010;&#28145;&#24230;&#20266;&#36896;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the development of effective deepfake detection models in recent years, several recent studies have demonstrated that biases in the training data utilized to develop deepfake detection models can lead to unfair performance for demographic groups of different races and/or genders. Such can result in these groups being unfairly targeted or excluded from detection, allowing misclassified deepfakes to manipulate public opinion and erode trust in the model. While these studies have focused on identifying and evaluating the unfairness in deepfake detection, no methods have been developed to address the fairness issue of deepfake detection at the algorithm level. In this work, we make the first attempt to improve deepfake detection fairness by proposing novel loss functions to train fair deepfake detection models in ways that are agnostic or aware of demographic factors. Extensive experiments on four deepfake datasets and five deepfake detectors demonstrate the effectiveness and flexi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ViNT&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;&#39046;&#22495;&#12290;ViNT&#36890;&#36807;&#36890;&#29992;&#30446;&#26631;&#36798;&#25104;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#28789;&#27963;&#30340;Transformer&#26550;&#26500;&#26469;&#23398;&#20064;&#23548;&#33322;&#21151;&#33021;&#21644;&#23454;&#29616;&#23545;&#21508;&#31181;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.14846</link><description>&lt;p&gt;
ViNT:&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ViNT: A Foundation Model for Visual Navigation. (arXiv:2306.14846v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ViNT&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;&#39046;&#22495;&#12290;ViNT&#36890;&#36807;&#36890;&#29992;&#30446;&#26631;&#36798;&#25104;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#28789;&#27963;&#30340;Transformer&#26550;&#26500;&#26469;&#23398;&#20064;&#23548;&#33322;&#21151;&#33021;&#21644;&#23454;&#29616;&#23545;&#21508;&#31181;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;"&#22522;&#30784;&#27169;&#22411;"&#65289;&#20351;&#20174;&#20107;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#20174;&#19994;&#32773;&#33021;&#22815;&#20351;&#29992;&#27604;&#23398;&#20064;&#33258;&#38646;&#24320;&#22987;&#25152;&#38656;&#25968;&#25454;&#38598;&#23567;&#24471;&#22810;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#21487;&#27867;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#22823;&#22411;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#24369;&#30417;&#30563;&#36827;&#34892;&#23398;&#20064;&#65292;&#28040;&#32791;&#27604;&#20219;&#20309;&#21333;&#20010;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#35201;&#22810;&#24471;&#22810;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Visual Navigation Transformer&#65288;ViNT&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;ViNT&#36890;&#36807;&#20351;&#29992;&#36866;&#29992;&#20110;&#20219;&#20309;&#23548;&#33322;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#30446;&#26631;&#36798;&#25104;&#30446;&#26631;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#28789;&#27963;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#26469;&#23398;&#20064;&#23548;&#33322;&#21151;&#33021;&#65292;&#24182;&#23454;&#29616;&#23545;&#21508;&#31181;&#19979;&#28216;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#36866;&#24212;&#12290;ViNT&#22312;&#22810;&#20010;&#29616;&#26377;&#30340;&#23548;&#33322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#28085;&#30422;&#25968;&#30334;&#23567;&#26102;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose pre-trained models ("foundation models") have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#32452;&#21512;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#27169;&#22359;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#20266;&#30417;&#30563;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#27169;&#24577;&#19978;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.12795</link><description>&lt;p&gt;
&#23398;&#20064;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Learning Unseen Modality Interaction. (arXiv:2306.12795v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#32452;&#21512;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#27169;&#22359;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#20266;&#30417;&#30563;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#27169;&#24577;&#19978;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#20551;&#23450;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#20351;&#29992;&#25152;&#26377;&#24863;&#20852;&#36259;&#30340;&#27169;&#24577;&#32452;&#21512;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#23545;&#24212;&#20851;&#31995;&#12290;&#26412;&#25991;&#23545;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#23436;&#25972;&#24615;&#20570;&#20986;&#20102;&#25361;&#25112;&#65292;&#32780;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21162;&#21147;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#32452;&#21512;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#21033;&#29992;&#19968;&#20010;&#27169;&#22359;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#22810;&#32500;&#29305;&#24449;&#25237;&#24433;&#21040;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#20449;&#24687;&#30340;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#27714;&#21644;&#25805;&#20316;&#23545;&#21487;&#29992;&#30340;&#27169;&#24577;&#36827;&#34892;&#20449;&#24687;&#32047;&#31215;&#12290;&#20026;&#20102;&#20943;&#23569;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36739;&#23569;&#36776;&#21035;&#21147;&#30340;&#27169;&#24577;&#32452;&#21512;&#30340;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#23398;&#20064;&#65292;&#36890;&#36807;&#20266;&#30417;&#30563;&#25351;&#31034;&#27169;&#24577;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#35270;&#39057;&#20998;&#31867;&#12289;&#26426;&#22120;&#20154;&#29366;&#24577;&#22238;&#24402;&#31561;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#24577;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences.In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a module that projects the multidimensional features of different modalities into a common space with rich information preserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to less discriminative modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality's prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.11768</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#21033;&#29992;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#20960;&#20309;&#32467;&#26500;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#29289;&#29702;&#21270;&#23398;&#24314;&#27169;&#21644;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#20256;&#32479;&#26041;&#27861;&#36153;&#26102;&#36153;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#25972;&#21512;&#19977;&#32500;&#20960;&#20309;&#25968;&#25454;&#65292;&#21152;&#19978;&#31867;&#20284;AlphaFold&#30340;&#24037;&#20855;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;&#19977;&#32500;&#32467;&#26500;&#39044;&#27979;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#20174;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#20027;&#27969;&#20219;&#21153;&#12289;&#24120;&#29992;&#30340;3D&#34507;&#30333;&#36136;&#34920;&#31034;&#21644;&#39044;&#27979;/&#29983;&#25104;&#27169;&#22411;&#20837;&#25163;&#65292;&#28982;&#21518;&#35814;&#32454;&#20171;&#32461;&#27599;&#20010;&#20219;&#21153;&#30340;&#22238;&#39038;&#65288;&#20363;&#22914;&#32467;&#21512;&#20301;&#28857;&#39044;&#27979;&#12289;&#32467;&#21512;&#26500;&#35937;&#29983;&#25104;&#12289;\emph{de novo} &#20998;&#23376;&#35774;&#35745;&#31561;&#65289;&#65292;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Seal&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;(VFMs)&#23545;&#27773;&#36710;&#28857;&#20113;&#24207;&#21015;&#36827;&#34892;&#20998;&#21106;&#12290;Seal&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09347</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#28860;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#23558;&#20219;&#24847;&#28857;&#20113;&#24207;&#21015;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Any Point Cloud Sequences by Distilling Vision Foundation Models. (arXiv:2306.09347v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Seal&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;(VFMs)&#23545;&#27773;&#36710;&#28857;&#20113;&#24207;&#21015;&#36827;&#34892;&#20998;&#21106;&#12290;Seal&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;(VFMs)&#30340;&#36827;&#23637;&#20026;&#22810;&#26679;&#21270;&#21644;&#39640;&#25928;&#29575;&#30340;&#35270;&#35273;&#24863;&#30693;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Seal&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;VFMs&#23545;&#21508;&#31181;&#27773;&#36710;&#28857;&#20113;&#24207;&#21015;&#36827;&#34892;&#20998;&#21106;&#12290;Seal&#20855;&#26377;&#19977;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#65306;i&#65289;&#21487;&#20280;&#32553;&#24615;&#65306;VFMs&#30452;&#25509;&#34987;&#25552;&#21462;&#21040;&#28857;&#20113;&#20013;&#65292;&#36991;&#20813;&#20102;&#39044;&#35757;&#32451;&#26399;&#38388;2D&#25110;3D&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;ii&#65289;&#19968;&#33268;&#24615;&#65306;&#22312;&#30456;&#26426;&#21040;&#28608;&#20809;&#38647;&#36798;&#21644;&#28857;&#21040;&#20998;&#21106;&#35268;&#21017;&#21270;&#38454;&#27573;&#37117;&#24378;&#21046;&#25191;&#34892;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20419;&#36827;&#20102;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;iii&#65289;&#27867;&#21270;&#33021;&#21147;&#65306;Seal&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#23454;&#29616;&#30693;&#35782;&#20256;&#36882;&#21040;&#28041;&#21450;&#22810;&#26679;&#21270;&#28857;&#20113;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#26469;&#33258;&#30495;&#23454;/&#21512;&#25104;&#12289;&#20302;/&#39640;&#20998;&#36776;&#29575;&#12289;&#22823;/&#23567;&#35268;&#27169;&#20197;&#21450;&#24178;&#20928;/&#30772;&#25439;&#25968;&#25454;&#38598;&#30340;&#28857;&#20113;&#12290;&#22312;&#23545;&#21313;&#19968;&#20010;&#19981;&#21516;&#28857;&#20113;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#20013;&#65292;&#23637;&#31034;&#20102;Seal&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, obviating the need for annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment regularization stages, facilitating cross-modal representation learning. iii) Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets. Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal. Notab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;Z&#20026;&#36830;&#32493;&#20540;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.06721</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Conditional Independence Testing. (arXiv:2306.06721v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;Z&#20026;&#36830;&#32493;&#20540;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#26816;&#39564;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#26159;&#35768;&#22810;&#22240;&#26524;&#22270;&#21457;&#29616;&#31639;&#27861;&#30340;&#26500;&#24314;&#22359;&#12290;CI&#27979;&#35797;&#26088;&#22312;&#25509;&#21463;&#25110;&#25298;&#32477;$X \perp \!\!\! \perp Y \mid Z$&#30340;&#38646;&#20551;&#35774;&#65292;&#20854;&#20013;$X \in \mathbb{R}&#65292;Y \in \mathbb{R}&#65292;Z \in \mathbb{R}^d$&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;Shah&#21644;Peters&#65288;2020&#65289;&#30340;&#19968;&#33324;&#21270;&#21327;&#26041;&#24046;&#27979;&#37327;&#21644;&#22522;&#20110;Cand\`es&#31561;&#20154;&#30340;&#26465;&#20214;&#38543;&#26426;&#21270;&#26816;&#39564;&#30340;&#20004;&#31181;&#31169;&#20154;CI&#27979;&#35797;&#36807;&#31243;&#65288;&#22312;&#27169;&#22411;-X&#20551;&#35774;&#19979;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#27979;&#35797;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#23427;&#20204;&#12290;&#36825;&#20123;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;Z&#20026;&#36830;&#32493;&#30340;&#19968;&#33324;&#24773;&#20917;&#30340;&#31169;&#20154;CI&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional independence (CI) tests are widely used in statistical data analysis, e.g., they are the building block of many algorithms for causal graph discovery. The goal of a CI test is to accept or reject the null hypothesis that $X \perp \!\!\! \perp Y \mid Z$, where $X \in \mathbb{R}, Y \in \mathbb{R}, Z \in \mathbb{R}^d$. In this work, we investigate conditional independence testing under the constraint of differential privacy. We design two private CI testing procedures: one based on the generalized covariance measure of Shah and Peters (2020) and another based on the conditional randomization test of Cand\`es et al. (2016) (under the model-X assumption). We provide theoretical guarantees on the performance of our tests and validate them empirically. These are the first private CI tests that work for the general case when $Z$ is continuous.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#32852;&#37030;&#21512;&#35268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#26631;&#31614;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#39044;&#27979;&#38598;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#30446;&#21069;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;</title><link>http://arxiv.org/abs/2306.05131</link><description>&lt;p&gt;
&#38754;&#21521;&#26631;&#31614;&#28418;&#31227;&#30340;&#32852;&#37030;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21512;&#35268;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction for Federated Uncertainty Quantification Under Label Shift. (arXiv:2306.05131v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#32852;&#37030;&#21512;&#35268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#26631;&#31614;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#39044;&#27979;&#38598;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#30446;&#21069;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#35768;&#22810;&#23458;&#25143;&#31471;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#20998;&#25955;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;FL&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20027;&#39064;&#65288;UQ&#65289;&#36827;&#34892;&#37096;&#20998;&#22788;&#29702;&#12290;&#22312;UQ&#26041;&#27861;&#20013;&#65292;&#21512;&#35268;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#26041;&#27861;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#25552;&#20379;&#26080;&#20998;&#24067;&#20445;&#35777;&#12290;&#25105;&#20204;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#21512;&#35268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#38544;&#31169;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#26631;&#31614;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#39044;&#27979;&#38598;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#30446;&#21069;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning framework where many clients collaboratively train models while keeping the training data decentralized. Despite recent advances in FL, the uncertainty quantification topic (UQ) remains partially addressed. Among UQ methods, conformal prediction (CP) approaches provides distribution-free guarantees under minimal assumptions. We develop a new federated conformal prediction method based on quantile regression and take into account privacy constraints. This method takes advantage of importance weighting to effectively address the label shift between agents and provides theoretical guarantees for both valid coverage of the prediction sets and differential privacy. Extensive experimental studies demonstrate that this method outperforms current competitors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21487;&#24494;&#20998;&#30340;&#24555;&#36895;&#36924;&#36817;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39640;&#20142;LHC&#25968;&#25454;&#30340;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25968;&#25454;&#20869;&#19982;&#31890;&#23376;&#25506;&#27979;&#22120;&#20013;&#30340;&#33021;&#37327;&#27785;&#31215;&#20998;&#24067;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.04712</link><description>&lt;p&gt;
&#21487;&#24494;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#22312;&#39640;&#20142;LHC&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentiable Earth Mover's Distance for Data Compression at the High-Luminosity LHC. (arXiv:2306.04712v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21487;&#24494;&#20998;&#30340;&#24555;&#36895;&#36924;&#36817;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39640;&#20142;LHC&#25968;&#25454;&#30340;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25968;&#25454;&#20869;&#19982;&#31890;&#23376;&#25506;&#27979;&#22120;&#20013;&#30340;&#33021;&#37327;&#27785;&#31215;&#20998;&#24067;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;(EMD)&#26159;&#22270;&#20687;&#35782;&#21035;&#21644;&#20998;&#31867;&#30340;&#26377;&#29992;&#25351;&#26631;&#65292;&#20294;&#20854;&#36890;&#24120;&#23454;&#29616;&#19981;&#21487;&#24494;&#20998;&#25110;&#36807;&#20110;&#32531;&#24930;&#65292;&#26080;&#27861;&#29992;&#20316;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20854;&#20182;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#26412;&#25991;&#35757;&#32451;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#23398;&#20064;&#20102;&#21487;&#24494;&#20998;&#30340;&#12289;&#24555;&#36895;&#30340;EMD&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#29992;&#20316;&#35745;&#31639;&#23494;&#38598;&#30340;EMD&#23454;&#29616;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#21487;&#24494;&#20998;&#30340;&#36924;&#36817;&#26041;&#27861;&#24212;&#29992;&#20110;&#29992;&#20110;&#25968;&#25454;&#21387;&#32553;&#30340;&#31867;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;(encoder NN)&#30340;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#26469;&#33258;&#27431;&#27954;&#26680;&#23376;&#30740;&#31350;&#32452;&#32455;&#30340;&#39640;&#20142;LHC&#12290;&#32534;&#30721;&#22120;NN&#30340;&#30446;&#26631;&#26159;&#22312;&#20445;&#30041;&#19982;&#31890;&#23376;&#25506;&#27979;&#22120;&#20013;&#30340;&#33021;&#37327;&#27785;&#31215;&#20998;&#24067;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#21516;&#26102;&#21387;&#32553;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#21487;&#24494;&#30340;EMD CNN&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;NN&#30340;&#24615;&#33021;&#36229;&#36234;&#22522;&#20110;&#24179;&#22343;&#24179;&#26041;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Earth mover's distance (EMD) is a useful metric for image recognition and classification, but its usual implementations are not differentiable or too slow to be used as a loss function for training other algorithms via gradient descent. In this paper, we train a convolutional neural network (CNN) to learn a differentiable, fast approximation of the EMD and demonstrate that it can be used as a substitute for computing-intensive EMD implementations. We apply this differentiable approximation in the training of an autoencoder-inspired neural network (encoder NN) for data compression at the high-luminosity LHC at CERN. The goal of this encoder NN is to compress the data while preserving the information related to the distribution of energy deposits in particle detectors. We demonstrate that the performance of our encoder NN trained using the differentiable EMD CNN surpasses that of training with loss functions based on mean squared error.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03552</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#21160;&#24577;&#20559;&#31227;&#30340;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#21463;&#21040;&#21160;&#24577;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#30340;&#29615;&#22659;&#21160;&#24577;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#29615;&#22659;&#21442;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26681;&#25454;&#20854;&#29615;&#22659;&#21442;&#25968;&#23558;&#24102;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20998;&#24320;&#20197;&#35757;&#32451;&#30456;&#24212;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#8220;&#29305;&#23450;&#22330;&#26223;&#8221;&#20351;&#29992;&#30340;&#65292;&#38024;&#23545;&#26576;&#20010;&#29615;&#22659;&#35757;&#32451;&#30340;&#31574;&#30053;&#19981;&#33021;&#20174;&#25910;&#38598;&#22312;&#20854;&#20182;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#25152;&#26377;&#20854;&#20182;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#21644;&#19981;&#21516;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#65292;&#24182;&#20174;&#20855;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#24577;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#37325;&#29992;&#12290;&#36825;&#31181;&#20998;&#24067;&#29992;&#20110;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23548;&#33268;&#20102; SRPO&#65288;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;&#65289;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SRPO &#22312;&#20855;&#26377;&#21160;&#24577;&#20559;&#31227;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#20998;&#31163;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17297</link><description>&lt;p&gt;
&#26080;&#29420;&#31435;&#24615;&#30340;&#27867;&#21270;&#35823;&#24046;&#65306;&#21435;&#22122;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning. (arXiv:2305.17297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#20998;&#31163;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#19968;&#20123;&#37325;&#35201;&#24037;&#20316;&#39564;&#35777;&#20102;&#29702;&#35770;&#24037;&#20316;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#30001;&#20110;&#25216;&#26415;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#20123;&#20551;&#35774;&#21253;&#25324;&#20855;&#26377;&#33391;&#22909;&#26465;&#20214;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#20197;&#21450;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#19968;&#20123;&#20851;&#20110;&#20998;&#24067;&#20559;&#31227;&#30340;&#24037;&#20316;&#36890;&#24120;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#25216;&#26415;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#26356;&#22909;&#22320;&#23545;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#36890;&#36807;&#20998;&#31163;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#20551;&#35774;&#26469;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#22312;&#36825;&#20123;&#26494;&#24347;&#30340;&#20551;&#35774;&#19979;&#65292;&#30740;&#31350;&#20102;&#21435;&#22122;&#38382;&#39064;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying the generalization abilities of linear models with real data is a central question in statistical learning. While there exist a limited number of prior important works (Loureiro et al. (2021A, 2021B), Wei et al. 2022) that do validate theoretical work with real data, these works have limitations due to technical assumptions. These assumptions include having a well-conditioned covariance matrix and having independent and identically distributed data. These assumptions are not necessarily valid for real data. Additionally, prior works that do address distributional shifts usually make technical assumptions on the joint distribution of the train and test data (Tripuraneni et al. 2021, Wu and Xu 2020), and do not test on real data.  In an attempt to address these issues and better model real data, we look at data that is not I.I.D. but has a low-rank structure. Further, we address distributional shift by decoupling assumptions on the training and test distribution. We provide anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110; MMD &#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.15901</link><description>&lt;p&gt;
&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;&#32463;&#39564;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Empirical Optimal Transport between Conditional Distributions. (arXiv:2305.15901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110; MMD &#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#20004;&#20010;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#20276;&#38543;&#26465;&#20214;&#20540;&#30340;&#36755;&#36816;&#25104;&#26412;&#65288;Wasserstein &#36317;&#31163;&#65289;&#65292;&#20197;&#21450;&#26465;&#20214;&#20998;&#24067;&#38388;&#30340;&#36755;&#36816;&#35745;&#21010;&#12290;&#30001;&#20110;&#21305;&#37197;&#26465;&#20214;&#20998;&#24067;&#26159;&#30417;&#30563;&#35757;&#32451;&#21028;&#21035;&#27169;&#22411;&#21644;&#65288;&#38544;&#24335;&#65289;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#24515;&#65292;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#20855;&#26377;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#38544;&#24335;&#29305;&#23450;&#20110;&#32852;&#21512;&#65288;&#26679;&#26412;&#65289;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#22240;&#27492;&#21046;&#23450;&#36825;&#20010;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#65288;i&#65289;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#65288;ii&#65289;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#29305;&#23450;&#30340;&#22522;&#20110; MMD&#65288;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65289;&#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given samples from two joint distributions, we consider the problem of Optimal Transportation (OT) between the corresponding distributions conditioned on a common variable. The objective of this work is to estimate the associated transport cost (Wasserstein distance) as well as the transport plan between the conditionals as a function of the conditioned value. Since matching conditional distributions is at the core of supervised training of discriminative models and (implicit) conditional-generative models, OT between conditionals has the potential to be employed in diverse machine learning applications. However, since the conditionals involved in OT are implicitly specified via the joint samples, it is challenging to formulate this problem, especially when (i) the variable conditioned on is continuous and (ii) the marginal of this variable in the two distributions is different. We overcome these challenges by employing a specific kernel MMD (Maximum Mean Discrepancy) based regularizer
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#32858;&#21512;&#38544;&#34255;&#29366;&#24577;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;MoE&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13999</link><description>&lt;p&gt;
&#36808;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model. (arXiv:2305.13999v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13999
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#32858;&#21512;&#38544;&#34255;&#29366;&#24577;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;MoE&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#19988;&#31232;&#30095;&#30340;&#21069;&#39304;&#23618;&#65288;S-FFN&#65289;&#65292;&#22914;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#25193;&#22823;Transformer&#27169;&#22411;&#35268;&#27169;&#20197;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#20165;&#28608;&#27963;&#37096;&#20998;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;FFN&#21442;&#25968;&#65292;S-FFN&#22312;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#65288;&#20197;FLOPs&#35745;&#31639;&#65289;&#19981;&#21464;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#31232;&#30095;&#31070;&#32463;&#35760;&#24518;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#19979;&#65292;&#20998;&#26512;&#20102;S-FFN&#30340;&#20004;&#20010;&#20027;&#35201;&#35774;&#35745;&#36873;&#25321;&#65306;&#20869;&#23384;&#22359;&#65288;&#21363;&#19987;&#23478;&#65289;&#22823;&#23567;&#21644;&#20869;&#23384;&#22359;&#36873;&#25321;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;S-FFN&#26550;&#26500;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#36873;&#25321;&#26041;&#27861; - Avg-K&#65292;&#36890;&#36807;&#22343;&#20540;&#32858;&#21512;&#30340;&#38544;&#34255;&#29366;&#24577;&#26469;&#36873;&#25321;&#22359;&#65292;&#30456;&#27604;&#21253;&#25324;Switch Transformer&#65288;Fedus&#31561;&#65292;2021&#65289;&#21644;HashLaye&#22312;&#20869;&#30340;&#29616;&#26377;MoE&#26550;&#26500;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for \textit{pretraining} large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method -\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLaye
&lt;/p&gt;</description></item><item><title>SMT 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;</title><link>http://arxiv.org/abs/2305.13998</link><description>&lt;p&gt;
SMT 2.0&#65306;&#19968;&#20010;&#20851;&#27880;&#23618;&#27425;&#21644;&#28151;&#21512;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes. (arXiv:2305.13998v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13998
&lt;/p&gt;
&lt;p&gt;
SMT 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Surrogate Modeling Toolbox (SMT)&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#12289;&#37319;&#26679;&#25216;&#26415;&#21644;&#19968;&#22871;&#31034;&#20363;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SMT 2.0&#65292;&#36825;&#26159;SMT&#30340;&#19968;&#20010;&#37325;&#35201;&#26032;&#29256;&#26412;&#65292;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#21319;&#32423;&#21644;&#26032;&#21151;&#33021;&#12290;&#36825;&#20010;&#29256;&#26412;&#22686;&#21152;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#20195;&#29702;&#27169;&#22411;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#22411;&#30340;&#21464;&#37327;&#22312;&#22810;&#20010;&#20195;&#29702;&#24314;&#27169;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;SMT 2.0&#36824;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;&#36825;&#20010;&#29256;&#26412;&#36824;&#21253;&#25324;&#20102;&#22788;&#29702;&#24102;&#22122;&#22768;&#21644;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#25968;&#25454;&#30340;&#26032;&#20989;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SMT 2.0&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#23618;&#27425;&#21644;&#28151;&#21512;&#36755;&#20837;&#30340;&#24320;&#28304;&#20195;&#29702;&#24211;&#12290;&#36825;&#20010;&#24320;&#28304;&#36719;&#20214;&#37319;&#29992;New BSD&#35768;&#21487;&#35777;&#36827;&#34892;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20174;Kohn-Sham&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#25552;&#20379;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#33021;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#26469;&#20419;&#36827;&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13316</link><description>&lt;p&gt;
KineticNet: &#28145;&#24230;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#30340;&#21160;&#33021;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
KineticNet: Deep learning a transferable kinetic energy functional for orbital-free density functional theory. (arXiv:2305.13316v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13316
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20174;Kohn-Sham&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#25552;&#20379;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#33021;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#26469;&#20419;&#36827;&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;OF-DFT&#65289;&#26377;&#26395;&#20197;&#26368;&#23567;&#20195;&#20215;&#35745;&#31639;&#22522;&#24577;&#20998;&#23376;&#24615;&#36136;&#65292;&#20294;&#30001;&#20110;&#25105;&#20204;&#26080;&#27861;&#23558;&#21160;&#33021;&#35745;&#31639;&#20026;&#30005;&#23376;&#23494;&#24230;&#30340;&#20989;&#25968;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20174;&#26356;&#26114;&#36149;&#30340;Kohn-Sham&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#25552;&#20379;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#33021;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#26469;&#20419;&#36827;&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#23454;&#38469;&#36816;&#29992;&#12290;KineticNet&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#27492;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orbital-free density functional theory (OF-DFT) holds the promise to compute ground state molecular properties at minimal cost. However, it has been held back by our inability to compute the kinetic energy as a functional of the electron density only. We here set out to learn the kinetic energy functional from ground truth provided by the more expensive Kohn-Sham density functional theory. Such learning is confronted with two key challenges: Giving the model sufficient expressivity and spatial context while limiting the memory footprint to afford computations on a GPU; and creating a sufficiently broad distribution of training data to enable iterative density optimization even when starting from a poor initial guess. In response, we introduce KineticNet, an equivariant deep neural network architecture based on point convolutions adapted to the prediction of quantities on molecular quadrature grids. Important contributions include convolution filters with sufficient spatial resolution i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#26469;&#35299;&#20915;MQA&#21487;&#33021;&#23548;&#33268;&#30340;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#21319;&#32423;&#21518;&#30340;GQA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36136;&#37327;&#65292;&#24182;&#20855;&#22791;&#19982;MQA&#30456;&#24403;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13245</link><description>&lt;p&gt;
GQA:&#20174;&#22810;&#22836;&#26816;&#26597;&#28857;&#35757;&#32451;&#24191;&#20041;&#22810;&#26597;&#35810;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. (arXiv:2305.13245v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#26469;&#35299;&#20915;MQA&#21487;&#33021;&#23548;&#33268;&#30340;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#21319;&#32423;&#21518;&#30340;GQA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36136;&#37327;&#65292;&#24182;&#20855;&#22791;&#19982;MQA&#30456;&#24403;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#20165;&#20351;&#29992;&#19968;&#20010;&#38190;&#20540;&#22836;&#65292;&#22823;&#22823;&#21152;&#24555;&#20102;&#35299;&#30721;&#22120;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;MQA&#21487;&#33021;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#65292;&#24182;&#19988;&#20026;&#20102;&#26356;&#24555;&#22320;&#25512;&#29702;&#32780;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;&#25105;&#20204;&#65288;1&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21033;&#29992;&#21407;&#22987;&#39044;&#35757;&#32451;&#35745;&#31639;&#37327;&#30340;5&#65285;&#65292;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;MQA&#30340;&#27169;&#22411;&#65292;&#24182;&#65288;2&#65289;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#65292;&#23427;&#26159;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#30340;&#24191;&#20041;&#24418;&#24335;&#65292;&#20351;&#29992;&#20013;&#38388;&#25968;&#37327;&#30340;&#38190;&#20540;&#22836;&#65288;&#22810;&#20110;&#19968;&#20010;&#65292;&#23569;&#20110;&#26597;&#35810;&#22836;&#30340;&#25968;&#37327;&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32463;&#36807;&#21319;&#32423;&#30340;GQA&#23454;&#29616;&#20102;&#19982;&#22810;&#22836;&#27880;&#24847;&#21147;&#30456;&#24403;&#30340;&#36895;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#25509;&#36817;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#32553;&#25918;&#23450;&#24459;&#26041;&#27861;&#25512;&#27979;&#20986;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#24418;&#29366;&#20248;&#21270;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#35745;&#31639;&#37327;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13035</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#20351;ViT&#25104;&#24418;&#65306;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#35774;&#35745;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (arXiv:2305.13035v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#32553;&#25918;&#23450;&#24459;&#26041;&#27861;&#25512;&#27979;&#20986;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#24418;&#29366;&#20248;&#21270;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#35745;&#31639;&#37327;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#32553;&#25918;&#23450;&#24459;&#34987;&#29992;&#26469;&#25512;&#23548;&#22312;&#32473;&#23450;&#35745;&#31639;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#65288;&#21442;&#25968;&#25968;&#37327;&#65289;&#12290;&#25105;&#20204;&#21457;&#23637;&#24182;&#25913;&#36827;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#20197;&#25512;&#27979;&#22914;&#23485;&#24230;&#21644;&#28145;&#24230;&#31561;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#24182;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#25104;&#21151;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#32463;&#36807;&#24418;&#29366;&#20248;&#21270;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#22312;&#20165;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#35745;&#31639;&#37327;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;SoViT-400m/14&#22312;ILSRCV2012&#19978;&#21462;&#24471;&#20102;90.3%&#30340;&#24494;&#35843;&#20934;&#30830;&#24230;&#65292;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;ViT-g/14&#65292;&#22312;&#30456;&#21516;&#35774;&#32622;&#19979;&#25509;&#36817;ViT-G/14&#65292;&#21516;&#26102;&#25512;&#26029;&#25104;&#26412;&#20063;&#19981;&#21040;&#19968;&#21322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#23383;&#24149;&#12289;VQA&#21644;&#38646;-shot&#36716;&#31227;&#65292;&#22312;&#24191;&#27867;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24182;&#30830;&#23450;&#20102;&#20854;&#38480;&#21046;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#25361;&#25112;&#20102;&#30450;&#30446;&#25193;&#22823;&#35270;&#35273;&#27169;&#22411;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReBRAC&#30340;&#26497;&#31616;&#31639;&#27861;&#65292;&#23427;&#22312;TD3+BC&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#20102;&#35774;&#35745;&#20803;&#32032;&#65292;&#36890;&#36807;&#23545;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19978;&#30340;&#39046;&#20808;&#22320;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.09836</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#31616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Minimalist Approach to Offline Reinforcement Learning. (arXiv:2305.09836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReBRAC&#30340;&#26497;&#31616;&#31639;&#27861;&#65292;&#23427;&#22312;TD3+BC&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#20102;&#35774;&#35745;&#20803;&#32032;&#65292;&#36890;&#36807;&#23545;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19978;&#30340;&#39046;&#20808;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;&#34429;&#28982;&#36825;&#20123;&#31639;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#24456;&#22810;&#31639;&#27861;&#21253;&#21547;&#20102;&#30475;&#20284;&#24494;&#19981;&#36275;&#36947;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#36825;&#20123;&#36873;&#25321;&#23545;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#36229;&#20986;&#20102;&#26680;&#24515;&#31639;&#27861;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#23545;&#20110;&#24050;&#26377;&#22522;&#32447;&#31639;&#27861;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23545;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ReBRAC&#30340;&#26497;&#31616;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;TD3+BC&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#20102;&#36825;&#20123;&#35774;&#35745;&#20803;&#32032;&#12290;&#25105;&#20204;&#20351;&#29992;D4RL&#21644;V-D4RL&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#20102;ReBRAC&#22312;51&#20010;&#20855;&#26377;&#33258;&#25105;&#24863;&#30693;&#21644;&#35270;&#35273;&#29366;&#24577;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#26041;&#27861;&#20013;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35828;&#26126;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#28040;&#34701;&#30740;&#31350;&#21644;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;ReBRAC&#30340;&#25104;&#21151;&#28304;&#20110;&#20854;&#22522;&#20110;&#31574;&#30053;&#25913;&#36827;&#21644;&#35780;&#35770;&#23478;&#27491;&#21017;&#21270;&#30340;&#21407;&#21017;&#24615;&#35774;&#35745;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant advancements in offline reinforcement learning (RL), resulting in the development of numerous algorithms with varying degrees of complexity. While these algorithms have led to noteworthy improvements, many incorporate seemingly minor design choices that impact their effectiveness beyond core algorithmic advances. However, the effect of these design choices on established baselines remains understudied. In this work, we aim to bridge this gap by conducting a retrospective analysis of recent works in offline RL and propose ReBRAC, a minimalistic algorithm that integrates such design elements built on top of the TD3+BC method. We evaluate ReBRAC on 51 datasets with both proprioceptive and visual state spaces using D4RL and V-D4RL benchmarks, demonstrating its state-of-the-art performance among ensemble-free methods. To further illustrate the efficacy of these design choices, we perform a large-scale ablation study and hyperparameter sensitivity anal
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.07644</link><description>&lt;p&gt;
&#35686;&#24789;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687; -- &#19982; GAN &#22312;&#35760;&#24518;&#33041;&#32959;&#30244;&#22270;&#20687;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images. (arXiv:2305.07644v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07644
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#32780;&#24320;&#21457;&#30340;&#65292;&#29616;&#22312;&#20063;&#34987;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#22312; GAN &#20043;&#21069;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20351;&#29992;&#20102;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#25351;&#26631;&#22914; FID &#21644; IS &#24182;&#19981;&#36866;&#21512;&#30830;&#23450;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#21482;&#26159;&#22797;&#21046;&#20102;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992; BRATS20 &#21644; BRATS21 &#25968;&#25454;&#38598;&#35757;&#32451; StyleGAN &#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#33041;&#32959;&#30244;&#22270;&#20687;&#65292;&#24182;&#27979;&#37327;&#21512;&#25104;&#22270;&#20687;&#19982;&#25152;&#26377;&#35757;&#32451;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#25968;&#25454;&#38598;&#12290;&#22914;&#26524;&#26368;&#32456;&#30446;&#26631;&#26159;&#20849;&#20139;&#21512;&#25104;&#30340;&#22270;&#20687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#25104;&#20687;&#26102;&#24212;&#35813;&#23567;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models were initially developed for text-to-image generation and are now being utilized to generate high quality synthetic images. Preceded by GANs, diffusion models have shown impressive results using various evaluation metrics. However, commonly used metrics such as FID and IS are not suitable for determining whether diffusion models are simply reproducing the training images. Here we train StyleGAN and diffusion models, using BRATS20 and BRATS21 datasets, to synthesize brain tumor images, and measure the correlation between the synthetic images and all training images. Our results show that diffusion models are much more likely to memorize the training images, especially for small datasets. Researchers should be careful when using diffusion models for medical imaging, if the final goal is to share the synthetic images.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13534</link><description>&lt;p&gt;
&#29992;&#22343;&#22330;&#21338;&#24328;&#20026;&#29983;&#25104;&#27169;&#22411;&#25645;&#24314;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
A mean-field games laboratory for generative modeling. (arXiv:2304.13534v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22343;&#22330;&#21338;&#24328; (MFGs) &#20316;&#20026;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#12289;&#22686;&#24378;&#21644;&#35774;&#35745;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102; MFGs &#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#31890;&#23376;&#21160;&#21147;&#23398;&#21644;&#20195;&#20215;&#20989;&#25968;&#25512;&#23548;&#20102;&#36825;&#19977;&#20010;&#31867;&#21035;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#8212;&#8212;&#19968;&#32452;&#32806;&#21512;&#30340;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#26469;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#23398;&#32467;&#26500;&#21644;&#29305;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#21512;&#25104;&#26679;&#26412;&#65292;&#21478;&#19968;&#20010;&#20195;&#29702;&#23545;&#26679;&#26412;&#36827;&#34892;&#35782;&#21035;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#22810;&#26679;&#19988;&#36924;&#30495;&#65292;&#21516;&#26102;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#31361;&#26174;&#20102; MFGs &#20316;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#39564;&#23460;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. There is a pervasive sense in the generative modeling community that the various flow and diffusion-based generative models have some foundational common structure and interrelationships. We establish connections between MFGs and major classes of flow and diffusion-based generative models including continuous-time normalizing flows, score-based models, and Wasserstein gradient flows. We derive these three classes of generative models through different choices of particle dynamics and cost functions. Furthermore, we study the mathematical structure and properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled nonlinear partial differential equations (PDEs). The theory of MFGs, therefore, enables the study of generative models through the theory of nonlinear PDEs. Throu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;GPS&#36712;&#36857;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#36712;&#36857;&#36880;&#28176;&#36716;&#25442;&#20026;&#22122;&#22768;&#65292;&#20877;&#20174;&#22122;&#22768;&#37325;&#26500;&#20266;&#36896;&#30340;&#36712;&#36857;&#65292;&#20197;&#36798;&#21040;&#29983;&#25104;&#38544;&#31169;&#20449;&#24687;&#20445;&#25252;&#30340;&#39640;&#36136;&#37327;&#36712;&#36857;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11582</link><description>&lt;p&gt;
GPS&#36712;&#36857;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model for GPS Trajectory Generation. (arXiv:2304.11582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;GPS&#36712;&#36857;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#36712;&#36857;&#36880;&#28176;&#36716;&#25442;&#20026;&#22122;&#22768;&#65292;&#20877;&#20174;&#22122;&#22768;&#37325;&#26500;&#20266;&#36896;&#30340;&#36712;&#36857;&#65292;&#20197;&#36798;&#21040;&#29983;&#25104;&#38544;&#31169;&#20449;&#24687;&#20445;&#25252;&#30340;&#39640;&#36136;&#37327;&#36712;&#36857;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPS&#35774;&#22791;&#21644;&#25968;&#25454;&#37319;&#38598;&#25216;&#26415;&#30340;&#37096;&#32626;&#65292;&#22823;&#37327;&#30340;GPS&#36712;&#36857;&#25968;&#25454;&#20026;&#25512;&#36827;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30740;&#31350;&#25552;&#20379;&#20102;&#26680;&#24515;&#25903;&#25345;&#12290;&#20294;&#26159;&#65292;GPS&#36712;&#36857;&#21253;&#25324;&#20010;&#20154;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#28041;&#21450;&#21040;&#38544;&#31169;&#38382;&#39064;&#12290;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#36712;&#36857;&#29983;&#25104;&#65292;&#29992;&#29983;&#25104;&#30340;&#26080;&#38544;&#31169;&#20449;&#24687;&#26367;&#25442;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#27963;&#21160;&#30340;&#22797;&#26434;&#21644;&#38543;&#26426;&#34892;&#20026;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36712;&#36857;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#29983;&#25104;&#65288;Diff-Traj&#65289;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;&#36712;&#36857;&#30340;&#26102;&#31354;&#29305;&#24449;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#21069;&#21521;&#36712;&#36857;&#22122;&#22768;&#22788;&#29702;&#36880;&#28176;&#23558;&#30495;&#23454;&#36712;&#36857;&#36716;&#25442;&#20026;&#22122;&#22768;&#12290;&#28982;&#21518;&#65292;Diff-Traj&#20174;&#22122;&#22768;&#37325;&#26500;&#20266;&#36896;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the deployment of GPS-enabled devices and data acquisition technology, the massively generated GPS trajectory data provide a core support for advancing spatial-temporal data mining research. Nonetheless, GPS trajectories comprise personal geo-location information, rendering inevitable privacy concerns on plain data. One promising solution to this problem is trajectory generation, replacing the original data with the generated privacy-free ones. However, owing to the complex and stochastic behavior of human activities, generating high-quality trajectories is still in its infancy. To achieve the objective, we propose a diffusion-based trajectory generation (Diff-Traj) framework, effectively integrating the generation capability of the diffusion model and learning from the spatial-temporal features of trajectories. Specifically, we gradually convert real trajectories to noise through a forward trajectory noising process. Then, Diff-Traj reconstructs forged trajectories from the noise
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#27880;&#37322;&#28145;&#24230;&#23398;&#20064;&#65288;MaDL&#65289;&#30340;&#27010;&#29575;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#30001;&#26131;&#38169;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#26377;&#22122;&#38899;&#31867;&#21035;&#26631;&#31614;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#22810;&#27880;&#37322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#34920;&#29616;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02539</link><description>&lt;p&gt;
&#22810;&#27880;&#37322;&#28145;&#24230;&#23398;&#20064;&#65306;&#20998;&#31867;&#38382;&#39064;&#30340;&#27010;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Multi-annotator Deep Learning: A Probabilistic Framework for Classification. (arXiv:2304.02539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02539
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#27880;&#37322;&#28145;&#24230;&#23398;&#20064;&#65288;MaDL&#65289;&#30340;&#27010;&#29575;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#30001;&#26131;&#38169;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#26377;&#22122;&#38899;&#31867;&#21035;&#26631;&#31614;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#22810;&#27880;&#37322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#34920;&#29616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#22797;&#26434;&#20998;&#31867;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#65292;&#28982;&#32780;&#30001;&#26131;&#38169;&#27880;&#37322;&#32773;&#65288;&#22914;&#20247;&#21253;&#24037;&#20154;&#65289;&#25552;&#20379;&#30340;&#23545;&#24212;&#31867;&#21035;&#26631;&#31614;&#26377;&#22122;&#38899;&#12290;&#22312;&#36825;&#26679;&#30340;&#22810;&#27880;&#37322;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#35757;&#32451;&#26631;&#20934;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#27425;&#20248;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#21517;&#20026;&#22810;&#27880;&#37322;&#28145;&#24230;&#23398;&#20064;&#65288;MaDL&#65289;&#30340;&#27010;&#29575;&#35757;&#32451;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#32852;&#21512;&#35757;&#32451;&#19968;&#20010;&#22320;&#38754;&#30495;&#30456;&#27169;&#22411;&#21644;&#19968;&#20010;&#27880;&#37322;&#32773;&#34920;&#29616;&#27169;&#22411;&#12290;&#22320;&#38754;&#30495;&#30456;&#27169;&#22411;&#23398;&#20064;&#39044;&#27979;&#23454;&#20363;&#30340;&#30495;&#23454;&#31867;&#21035;&#65292;&#32780;&#27880;&#37322;&#32773;&#34920;&#29616;&#27169;&#22411;&#25512;&#26029;&#27880;&#37322;&#32773;&#34920;&#29616;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;&#27169;&#22359;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#27880;&#37322;&#32773;&#30340;&#34920;&#29616;&#20570;&#20986;&#19981;&#21516;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#32771;&#34385;&#31867;&#21035;&#25110;&#23454;&#20363;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23398;&#20064;&#27880;&#37322;&#32773;&#23884;&#20837;&#20197;&#20272;&#35745;&#28508;&#22312;&#31354;&#38388;&#20013;&#27880;&#37322;&#32773;&#30340;&#23494;&#24230;&#20316;&#20026;&#36801;&#31227;&#23398;&#20064;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20351;&#25105;&#20204;&#33021;&#22815;&#20272;&#35745;&#30495;&#23454;&#30340;&#31867;&#21035;&#26631;&#31614;&#65292;&#32780;&#19988;&#33021;&#22815;&#20272;&#35745;&#25152;&#20998;&#37197;&#26631;&#31614;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#27880;&#37322;&#32773;&#24615;&#33021;&#20551;&#35774;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowd workers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of t
&lt;/p&gt;</description></item><item><title>DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00133</link><description>&lt;p&gt;
DeforestVis&#65306;&#20351;&#29992;&#20195;&#29702;&#20915;&#31574;&#26641;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00133
&lt;/p&gt;
&lt;p&gt;
DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#65288;&#21644;&#20851;&#38190;&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#22686;&#21152;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#26356;&#26131;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;ML&#12290;&#35299;&#37322;&#22797;&#26434;ML&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#19988;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65288;&#20363;&#22914;&#35268;&#21017;&#38598;&#21644;&#20915;&#31574;&#26641;&#65289;&#65292;&#20197;&#36275;&#22815;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#65292;&#20294;&#26356;&#31616;&#21333;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#35268;&#21017;&#38598;&#21487;&#20197;&#21464;&#24471;&#38750;&#24120;&#20887;&#38271;&#65292;&#21253;&#21547;&#35768;&#22810;if-else&#35821;&#21477;&#65292;&#32780;&#20915;&#31574;&#26641;&#30340;&#28145;&#24230;&#20250;&#38543;&#30528;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;ML&#27169;&#22411;&#32780;&#36805;&#36895;&#22686;&#21152;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#20854;&#26680;&#24515;&#30446;&#26631;&#65292;&#25552;&#20379;&#29992;&#25143;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;DeforestVis&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20351;&#29992;&#33258;&#36866;&#24212;&#22686;&#24378;&#65288;AdaBoost&#65289;&#25216;&#26415;&#29983;&#25104;&#30340;&#20195;&#29702;&#20915;&#31574;&#26641;&#65288;&#19968;&#32423;&#20915;&#31574;&#26641;&#65289;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#23545;&#22797;&#26434;ML&#27169;&#22411;&#34892;&#20026;&#30340;&#21451;&#22909;&#24635;&#32467;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity of machine learning (ML) models increases and the applications in different (and critical) domains grow, there is a strong demand for more interpretable and trustworthy ML. One straightforward and model-agnostic way to interpret complex ML models is to train surrogate models, such as rule sets and decision trees, that sufficiently approximate the original ones while being simpler and easier-to-explain. Yet, rule sets can become very lengthy, with many if-else statements, and decision tree depth grows rapidly when accurately emulating complex ML models. In such cases, both approaches can fail to meet their core goal, providing users with model interpretability. We tackle this by proposing DeforestVis, a visual analytics tool that offers user-friendly summarization of the behavior of complex ML models by providing surrogate decision stumps (one-level decision trees) generated with the adaptive boosting (AdaBoost) technique. Our solution helps users to explore the comple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25490;&#24207;&#26041;&#24335;&#65292;&#22312;&#22788;&#32602;&#30340;&#20010;&#20154;&#20013;&#28608;&#21169;&#38750;&#21512;&#20316;&#34892;&#20026;&#65292;&#23545;&#20110;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#24635;&#20184;&#27454;&#12290;</title><link>http://arxiv.org/abs/2303.17971</link><description>&lt;p&gt;
&#36890;&#36807;&#25490;&#24207;&#20419;&#36827;&#38750;&#21512;&#20316;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Promoting Non-Cooperation Through Ordering. (arXiv:2303.17971v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25490;&#24207;&#26041;&#24335;&#65292;&#22312;&#22788;&#32602;&#30340;&#20010;&#20154;&#20013;&#28608;&#21169;&#38750;&#21512;&#20316;&#34892;&#20026;&#65292;&#23545;&#20110;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#24635;&#20184;&#27454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24456;&#22810;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#22823;&#22478;&#24066;&#20013;&#30340;&#23567;&#20132;&#36890;&#36829;&#35268;&#20107;&#20214;&#65292;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#38656;&#35201;&#23450;&#26399;&#23545;&#22823;&#37327;&#20010;&#20154;&#36827;&#34892;&#24809;&#32602;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#32473;&#27599;&#20010;&#20010;&#20154;&#19968;&#20010;&#26426;&#20250;&#65292;&#25215;&#25285;&#19968;&#23567;&#31508;&#32602;&#27454;&#65292;&#24182;&#20445;&#35777;&#20813;&#38500;&#21487;&#33021;&#20250;&#38754;&#20020;&#30340;&#36739;&#22823;&#22788;&#32602;&#21644;&#27861;&#24459;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36829;&#27861;&#32773;&#25968;&#37327;&#20247;&#22810;&#65292;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#33021;&#21147;&#26377;&#38480;&#65292;&#20010;&#20154;&#38754;&#20020;&#30340;&#39118;&#38505;&#36890;&#24120;&#24456;&#23567;&#65292;&#29702;&#24615;&#30340;&#20010;&#20154;&#23558;&#36873;&#25321;&#19981;&#25903;&#20184;&#32602;&#27454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#65292;&#22914;&#26524;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#25353;&#29031;&#24050;&#30693;&#30340;&#20844;&#24320;&#39034;&#24207;&#22788;&#29702;&#36829;&#27861;&#32773;&#65292;&#23601;&#33021;&#36866;&#24403;&#22320;&#28608;&#21169;&#36829;&#27861;&#32773;&#25903;&#20184;&#32602;&#27454;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#20013;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#20419;&#36827;&#20102;&#38750;&#21512;&#20316;&#34892;&#20026;&#65292;&#28608;&#21169;&#20010;&#20154;&#25903;&#20184;&#32602;&#27454;&#12290;&#32780;&#19988;&#65292;&#23545;&#20110;&#20219;&#24847;&#32852;&#30431;&#20063;&#26159;&#36866;&#29992;&#30340;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#20013;&#22830;&#31649;&#29702;&#37096;&#38376;&#25910;&#21040;&#30340;&#39044;&#26399;&#24635;&#20184;&#27454;&#65292;&#24182;&#26174;&#31034;&#20854;&#26174;&#33879;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real world situations, like minor traffic offenses in big cities, a central authority is tasked with periodic administering punishments to a large number of individuals. Common practice is to give each individual a chance to suffer a smaller fine and be guaranteed to avoid the legal process with probable considerably larger punishment. However, thanks to the large number of offenders and a limited capacity of the central authority, the individual risk is typically small and a rational individual will not choose to pay the fine. Here we show that if the central authority processes the offenders in a publicly known order, it properly incentives the offenders to pay the fine. We show analytically and on realistic experiments that our mechanism promotes non-cooperation and incentives individuals to pay. Moreover, the same holds for an arbitrary coalition. We quantify the expected total payment the central authority receives, and show it increases considerably.
&lt;/p&gt;</description></item><item><title>CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09752</link><description>&lt;p&gt;
CoLT5: &#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;&#24555;&#36895;&#38271;&#36317;&#31163;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09752
&lt;/p&gt;
&lt;p&gt;
CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#20294;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#25991;&#26723;&#24456;&#26114;&#36149;&#8212;&#8212;&#36825;&#19981;&#20165;&#26159;&#22240;&#20026;&#20108;&#27425;&#27880;&#24847;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#23545;&#27599;&#20010;&#26631;&#35760;&#24212;&#29992;&#21069;&#39304;&#21644;&#25237;&#24433;&#23618;&#12290;&#28982;&#32780;&#65292;&#19981;&#26159;&#25152;&#26377;&#26631;&#35760;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CoLT5&#65292;&#19968;&#31181;&#38271;&#36755;&#20837;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#35745;&#31639;&#26469;&#21033;&#29992;&#27492;&#30452;&#35273;&#65292;&#22312;&#21069;&#39304;&#21644;&#27880;&#24847;&#23618;&#20013;&#20026;&#37325;&#35201;&#26631;&#35760;&#25552;&#20379;&#26356;&#22810;&#36164;&#28304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoLT5&#27604;LongT5&#34920;&#29616;&#26356;&#24378;&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26356;&#24555;&#65292;&#22312;&#38271;&#36755;&#20837;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;SOTA&#12290;&#27492;&#22806;&#65292;CoLT5&#33021;&#22815;&#26377;&#25928;&#19988;&#21487;&#25511;&#22320;&#21033;&#29992;&#26497;&#38271;&#30340;&#36755;&#20837;&#65292;&#23637;&#31034;&#20102;&#39640;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#30340;&#24378;&#22823;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
&lt;/p&gt;</description></item><item><title>CoSyn&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.03387</link><description>&lt;p&gt;
CoSyn&#65306;&#20351;&#29992;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#21452;&#26354;&#32447;&#32593;&#32476;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;
&lt;/p&gt;
&lt;p&gt;
CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network. (arXiv:2303.03387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03387
&lt;/p&gt;
&lt;p&gt;
CoSyn&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23545;&#35805;&#20013;&#38544;&#21547;&#30340;&#20167;&#24680;&#35328;&#35770;&#23545;&#26469;&#33258;&#21508;&#20010;&#32676;&#20307;&#30340;&#20154;&#20204;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#27492;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#12290;&#22823;&#37096;&#20998;&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#20110;&#26816;&#27979;&#26126;&#30830;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#36825;&#20123;&#35328;&#35770;&#26126;&#26174;&#19988;&#21033;&#29992;&#20102;&#20167;&#24680;&#30701;&#35821;&#65292;&#23545;&#20110;&#26816;&#27979;&#38544;&#21547;&#25110;&#36890;&#36807;&#38388;&#25509;&#25110;&#32534;&#30721;&#35821;&#35328;&#34920;&#36798;&#20986;&#30340;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoSyn&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26126;&#30830;&#22320;&#32467;&#21512;&#20102;&#29992;&#25143;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#26469;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;CoSyn&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32534;&#30721;&#36825;&#20123;&#22806;&#37096;&#19978;&#19979;&#25991;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#65292;&#28165;&#26224;&#22320;&#25429;&#25417;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#29420;&#31435;&#35780;&#20272;&#20102;&#20174;&#36825;&#20123;&#22024;&#26434;&#30340;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#30340;&#20449;&#24687;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25152;&#26377;&#36825;&#20123;&#25805;&#20316;&#65292;&#20197;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#30340;&#26080;&#26631;&#24230;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tremendous growth of social media users interacting in online conversations has led to significant growth in hate speech, affecting people from various demographics. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. In this paper, we present CoSyn, a context-synergized neural network that explicitly incorporates user- and conversational context for detecting implicit hate speech in online conversations. CoSyn introduces novel ways to encode these external contexts and employs a novel context interaction mechanism that clearly captures the interplay between them, making independent assessments of the amounts of information to be retrieved from these noisy contexts. Additionally, it carries out all these operations in the hyperbolic space to account for the scale-free dynamics of social media. We de
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26089;&#26399;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#30456;&#22270;&#65292;&#21457;&#29616;&#22235;&#31181;&#19981;&#21516;&#30340;&#29366;&#24577;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#8220;&#38497;&#23789;&#24230;&#20943;&#23567;&#8221;&#30456;&#20301;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.12250</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26089;&#26399;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#30456;&#22270;&#65306;&#23398;&#20064;&#29575;&#12289;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width. (arXiv:2302.12250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12250
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26089;&#26399;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#30456;&#22270;&#65292;&#21457;&#29616;&#22235;&#31181;&#19981;&#21516;&#30340;&#29366;&#24577;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#8220;&#38497;&#23789;&#24230;&#20943;&#23567;&#8221;&#30456;&#20301;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#20248;&#21270;&#21160;&#21147;&#23398;&#65292;&#24182;&#30740;&#31350;&#23398;&#20064;&#29575; $\eta$&#12289;&#28145;&#24230; $d$ &#21644;&#23485;&#24230; $w$ &#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#20540; $\lambda^H_t$&#65292;&#21363;&#25439;&#22833;&#20989;&#25968;&#26223;&#35266;&#30340;&#38497;&#23789;&#31243;&#24230;&#30340;&#34913;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#21160;&#21147;&#23398;&#21487;&#20197;&#23637;&#31034;&#20986;&#22235;&#31181;&#19981;&#21516;&#30340;&#29366;&#24577;&#65306;&#65288;i&#65289;&#26089;&#26399;&#20020;&#26102;&#29366;&#24577;&#65292;&#65288;ii&#65289;&#20013;&#38388;&#39281;&#21644;&#29366;&#24577;&#65292;&#65288;iii&#65289;&#36880;&#28176;&#38160;&#21270;&#29366;&#24577;&#21644;&#65288;iv&#65289;&#21518;&#26399;&#8220;&#31283;&#23450;&#36793;&#32536;&#8221;&#29366;&#24577;&#12290;&#26089;&#26399;&#21644;&#20013;&#38388;&#29366;&#24577;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#21576;&#29616;&#20986;&#20016;&#23500;&#30340;&#30456;&#22270;&#65292;&#21462;&#20915;&#20110; $\eta \equiv c / \lambda_0^H $&#12289;$d$ &#21644; $w$&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20020;&#30028;&#20540; $c$&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#25439;&#22833;&#21644;&#38497;&#23789;&#24230;&#30340;&#26089;&#26399;&#21160;&#21147;&#23398;&#20013;&#20998;&#38548;&#20986;&#19981;&#21516;&#30340;&#29616;&#35937;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#8220;&#38497;&#23789;&#24230;&#20943;&#23567;&#8221;&#30456;&#20301;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#38497;&#23789;&#24230;&#22312;&#26089;&#26399;&#26102;&#38388;&#19979;&#38477;&#65292;&#24403; $d$ &#21644; $1/w$ &#22686;&#21152;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We systematically analyze optimization dynamics in deep neural networks (DNNs) trained with stochastic gradient descent (SGD) and study the effect of learning rate $\eta$, depth $d$, and width $w$ of the neural network. By analyzing the maximum eigenvalue $\lambda^H_t$ of the Hessian of the loss, which is a measure of sharpness of the loss landscape, we find that the dynamics can show four distinct regimes: (i) an early time transient regime, (ii) an intermediate saturation regime, (iii) a progressive sharpening regime, and (iv) a late time ``edge of stability" regime. The early and intermediate regimes (i) and (ii) exhibit a rich phase diagram depending on $\eta \equiv c / \lambda_0^H $, $d$, and $w$. We identify several critical values of $c$, which separate qualitatively distinct phenomena in the early time dynamics of training loss and sharpness. Notably, we discover the opening up of a ``sharpness reduction" phase, where sharpness decreases at early times, as $d$ and $1/w$ are inc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;E-&#20540;&#26469;&#37327;&#21270;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#38543;&#26426;&#21270;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#32858;&#21512;&#30456;&#21516;&#25968;&#25454;&#30340;&#22810;&#27425;&#20998;&#26512;&#30340;&#35777;&#25454;&#65292;&#21516;&#26102;&#25511;&#21046;&#34394;&#20551;&#21457;&#29616;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.07294</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#19968;&#30340;E-&#20540;&#36890;&#36807;FDR&#25511;&#21046;&#21435;&#38543;&#26426;&#21270;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Derandomized Novelty Detection with FDR Control via Conformal E-values. (arXiv:2302.07294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07294
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;E-&#20540;&#26469;&#37327;&#21270;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#38543;&#26426;&#21270;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#32858;&#21512;&#30456;&#21516;&#25968;&#25454;&#30340;&#22810;&#27425;&#20998;&#26512;&#30340;&#35777;&#25454;&#65292;&#21516;&#26102;&#25511;&#21046;&#34394;&#20551;&#21457;&#29616;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26080;&#20998;&#24067;&#26041;&#27861;&#65292;&#29992;&#20110;&#20005;&#26684;&#26657;&#20934;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#36755;&#20986;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26377;&#24456;&#22810;&#20248;&#28857;&#65292;&#20294;&#23427;&#20063;&#26377;&#38543;&#26426;&#24615;&#30340;&#38480;&#21046;&#65292;&#21363;&#22312;&#20998;&#26512;&#30456;&#21516;&#25968;&#25454;&#26102;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#23545;&#20219;&#20309;&#21457;&#29616;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#36866;&#24403;&#30340;&#32479;&#19968;E-&#20540;&#32780;&#19981;&#26159;p-&#20540;&#26469;&#37327;&#21270;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#20351;&#32479;&#19968;&#25512;&#29702;&#26356;&#21152;&#31283;&#23450;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#20801;&#35768;&#26377;&#25928;&#22320;&#27719;&#24635;&#23545;&#30456;&#21516;&#25968;&#25454;&#22810;&#27425;&#20998;&#26512;&#30340;&#35777;&#25454;&#65292;&#21516;&#26102;&#21487;&#38752;&#22320;&#25511;&#21046;&#34394;&#20551;&#21457;&#29616;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19982;&#26631;&#20934;&#32479;&#19968;&#25512;&#29702;&#30456;&#27604;&#65292;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#38543;&#26426;&#24615;&#32780;&#19981;&#20250;&#25439;&#22833;&#22826;&#22810;&#21151;&#29575;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22522;&#20110;&#20174;&#30456;&#21516;&#25968;&#25454;&#20013;&#31934;&#24515;&#25552;&#21462;&#30340;&#38468;&#21152;&#36741;&#21161;&#20449;&#24687;&#26469;&#21152;&#26435;&#32479;&#19968;E-&#20540;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Conformal inference provides a general distribution-free method to rigorously calibrate the output of any machine learning algorithm for novelty detection. While this approach has many strengths, it has the limitation of being randomized, in the sense that it may lead to different results when analyzing twice the same data, and this can hinder the interpretation of any findings. We propose to make conformal inferences more stable by leveraging suitable conformal e-values instead of p-values to quantify statistical significance. This solution allows the evidence gathered from multiple analyses of the same data to be aggregated effectively while provably controlling the false discovery rate. Further, we show that the proposed method can reduce randomness without much loss of power compared to standard conformal inference, partly thanks to an innovative way of weighting conformal e-values based on additional side information carefully extracted from the same data. Simulations with synthet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Proximal Newton&#31639;&#27861;&#30340;&#39640;&#25928;&#22270;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#20984;minimax concave penalty&#65292;&#24182;&#21033;&#29992;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#21644;&#20960;&#20010;&#31639;&#27861;&#25216;&#24039;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.06434</link><description>&lt;p&gt;
Proximal Newton&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22270;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Graph Laplacian Estimation by Proximal Newton. (arXiv:2302.06434v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Proximal Newton&#31639;&#27861;&#30340;&#39640;&#25928;&#22270;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#20984;minimax concave penalty&#65292;&#24182;&#21033;&#29992;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#21644;&#20960;&#20010;&#31639;&#27861;&#25216;&#24039;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Laplacian&#32422;&#26463;&#30340;&#39640;&#26031;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;&#65288;LGMRF&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#22810;&#20803;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#25968;&#25454;&#20013;&#23398;&#20064;&#26435;&#37325;&#31232;&#30095;&#30340;&#20381;&#36182;&#22270;&#12290;&#36825;&#20010;&#22270;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30340;&#31934;&#24230;&#30697;&#38453;&#65292;&#21463;&#21040;Laplacian&#32467;&#26500;&#32422;&#26463;&#30340;&#38480;&#21046;&#65292;&#24182;&#24102;&#26377;&#31232;&#30095;&#24615;&#35825;&#23548;&#30340;&#24809;&#32602;&#39033;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;$\ell_1$-&#33539;&#25968;&#24809;&#32602;&#19981;&#36866;&#29992;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#23436;&#20840;&#22270;&#65292;&#25152;&#20197;&#25105;&#20204;&#37319;&#29992;&#20102;&#38750;&#20984;&#30340;MCP&#65288;minimax concave penalty&#65289;&#65292;&#23427;&#20419;&#36827;&#20855;&#26377;&#26356;&#20302;&#20272;&#35745;&#20559;&#24046;&#30340;&#31232;&#30095;&#35299;&#12290;&#20854;&#27425;&#65292;&#19982;&#29616;&#26377;&#30340;&#35813;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20108;&#38454;proximal Newton&#26041;&#27861;&#26469;&#33719;&#24471;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#20102;&#22810;&#31181;&#31639;&#27861;&#29305;&#24615;&#65292;&#22914;&#20351;&#29992;&#20849;&#36717;&#26799;&#24230;&#27861;&#12289;&#39044;&#26465;&#20214;&#21270;&#21644;&#20998;&#21106;&#21040;&#27963;&#21160;/&#33258;&#30001;&#38598;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Laplacian-constrained Gaussian Markov Random Field (LGMRF) is a common multivariate statistical model for learning a weighted sparse dependency graph from given data. This graph learning problem can be formulated as a maximum likelihood estimation (MLE) of the precision matrix, subject to Laplacian structural constraints, with a sparsity-inducing penalty term. This paper aims to solve this learning problem accurately and efficiently. First, since the commonly used $\ell_1$-norm penalty is inappropriate in this setting and may lead to a complete graph, we employ the nonconvex minimax concave penalty (MCP), which promotes sparse solutions with lower estimation bias. Second, as opposed to existing first-order methods for this problem, we develop a second-order proximal Newton approach to obtain an efficient solver, utilizing several algorithmic features, such as using Conjugate Gradients, preconditioning, and splitting to active/free sets. Numerical experiments demonstrate the advanta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#29305;&#24449;&#25193;&#25955;&#30697;&#38453;&#30340;&#26368;&#22823;&#22855;&#24322;&#20540;&#26469;&#32553;&#25918;&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;Hessians&#26469;&#34913;&#37327;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22122;&#22768;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#65292;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#23454;&#38469;&#22270;&#24418;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04451</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#65306;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;&#25913;&#36827;PAC-Bayesian&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion. (arXiv:2302.04451v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#29305;&#24449;&#25193;&#25955;&#30697;&#38453;&#30340;&#26368;&#22823;&#22855;&#24322;&#20540;&#26469;&#32553;&#25918;&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;Hessians&#26469;&#34913;&#37327;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22122;&#22768;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#65292;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#23454;&#38469;&#22270;&#24418;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22270;&#39044;&#27979;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;&#30001;&#20854;&#23454;&#35777;&#34920;&#29616;&#25152;&#39537;&#21160;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#23427;&#20204;&#26681;&#25454;&#26368;&#22823;&#24230;&#25968;&#22312;&#22270;&#32467;&#26500;&#26041;&#38754;&#36827;&#34892;&#32553;&#25918;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27867;&#21270;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#26681;&#25454;&#22270;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#25193;&#25955;&#30697;&#38453;&#30340;&#26368;&#22823;&#22855;&#24322;&#20540;&#36827;&#34892;&#32553;&#25918;&#12290;&#23545;&#20110;&#23454;&#38469;&#22270;&#24418;&#65292;&#36825;&#20123;&#30028;&#38480;&#30340;&#25968;&#20540;&#35201;&#27604;&#20808;&#21069;&#30340;&#30028;&#38480;&#23567;&#24471;&#22810;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#30456;&#31526;&#30340;&#27867;&#21270;&#24046;&#36317;&#19979;&#38480;&#65292;&#20854;&#28176;&#36817;&#22320;&#21305;&#37197;&#20102;&#25105;&#20204;&#30340;&#19978;&#38480;&#30028;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20808;&#21069;&#30340;&#35774;&#32622;&#65288;&#21363;&#21367;&#31215;&#21644;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65289;&#21644;&#26032;&#30340;&#35774;&#32622;&#65288;&#21363;&#22270;&#21516;&#26500;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;Hessians&#26469;&#34913;&#37327;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#22122;&#22768;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;Hessian&#30340;&#27979;&#37327;&#19982;&#35266;&#23519;&#21040;&#30340;&#27867;&#21270;&#24046;&#36317;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#22810;&#31181;&#24120;&#29992;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21450;&#20854;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#28857;&#65292;&#22312;&#22235;&#20010;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#29992;&#20363;&#19978;&#35299;&#20915;&#20102;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04032</link><description>&lt;p&gt;
&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#32593;&#32476;&#30340;&#31995;&#32479;&#24615;&#33021;&#20998;&#26512;&#65306;&#25171;&#30772;&#36801;&#31227;&#23398;&#20064;&#30340;&#32422;&#23450;
&lt;/p&gt;
&lt;p&gt;
A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions. (arXiv:2302.04032v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#22810;&#31181;&#24120;&#29992;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21450;&#20854;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#28857;&#65292;&#22312;&#22235;&#20010;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#29992;&#20363;&#19978;&#35299;&#20915;&#20102;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#26159;&#19968;&#31181;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#28145;&#24230;&#29305;&#24449;&#26469;&#27169;&#20223;&#20154;&#31867;&#24863;&#30693;&#12290;&#36817;&#24180;&#26469;&#65292;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#26377;&#36259;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#22270;&#20687;&#25110;&#31867;&#20284;&#22270;&#20687;&#36755;&#20986;&#30340;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#21512;&#25104;&#12289;&#20998;&#21106;&#12289;&#28145;&#24230;&#39044;&#27979;&#31561;&#12290;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#36890;&#24120;&#26159;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#25439;&#22833;&#35745;&#31639;&#12290;&#23613;&#31649;&#23545;&#35813;&#26041;&#27861;&#30340;&#20852;&#36259;&#21644;&#24191;&#27867;&#20351;&#29992;&#22686;&#21152;&#20102;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#30340;&#21162;&#21147;&#26469;&#25506;&#32034;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#30340;&#32593;&#32476;&#20197;&#21450;&#20174;&#21738;&#20123;&#23618;&#25552;&#21462;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#35780;&#20272;&#22810;&#31181;&#24120;&#29992;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#21450;&#38024;&#23545;&#22235;&#20010;&#29616;&#26377;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#29992;&#20363;&#30340;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#28857;&#26469;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep perceptual loss is a type of loss function in computer vision that aims to mimic human perception by using the deep features extracted from neural networks. In recent years, the method has been applied to great effect on a host of interesting computer vision tasks, especially for tasks with image or image-like outputs, such as image synthesis, segmentation, depth prediction, and more. Many applications of the method use pretrained networks, often convolutional networks, for loss calculation. Despite the increased interest and broader use, more effort is needed toward exploring which networks to use for calculating deep perceptual loss and from which layers to extract the features.  This work aims to rectify this by systematically evaluating a host of commonly used and readily available, pretrained networks for a number of different feature extraction points on four existing use cases of deep perceptual loss. The use cases of perceptual similarity, super-resolution, image segmentat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#36873;&#25321;&#26694;&#26550;&#65288;DSIR&#65289;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;KL&#20943;&#23569;&#20316;&#20026;&#25968;&#25454;&#24230;&#37327;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#22312;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#20197;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2302.03169</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03169
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#36873;&#25321;&#26694;&#26550;&#65288;DSIR&#65289;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;KL&#20943;&#23569;&#20316;&#20026;&#25968;&#25454;&#24230;&#37327;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#22312;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#20197;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#20110;&#36890;&#29992;&#39046;&#22495;&#65288;&#22914;GPT-3&#65289;&#21644;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;Codex&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20174;&#22823;&#22411;&#21407;&#22987;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#19968;&#20010;&#23376;&#38598;&#65292;&#20197;&#21305;&#37197;&#32473;&#23450;&#19968;&#20123;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#30340;&#25152;&#38656;&#30446;&#26631;&#20998;&#24067;&#12290;&#37492;&#20110;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#24230;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#19987;&#23478;&#25163;&#21160;&#31574;&#21010;&#25968;&#25454;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22312;LM&#25968;&#25454;&#36873;&#25321;&#20013;&#20351;&#29992;&#30340;&#32463;&#20856;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#20302;&#32500;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#25968;&#25454;&#36873;&#25321;&#19982;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#65288;DSIR&#65289;&#65292;&#23427;&#22312;&#19968;&#20010;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20197;&#20415;&#26681;&#25454;&#36825;&#20123;&#26435;&#37325;&#36827;&#34892;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#25968;&#25454;&#36873;&#25321;&#12290;&#20026;&#20102;&#30830;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;KL&#20943;&#23569;&#65292;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#34913;&#37327;&#25152;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#30446;&#26631;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#25968;&#25454;&#24230;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given some unlabeled target samples. Due to the large scale and dimensionality of the raw text data, existing methods use simple heuristics or use experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. To determine an appropriate feature space, we show that KL reduction, a data metric that measures the proximity between selected pretraining data and the target in a feature space, has high correlat
&lt;/p&gt;</description></item><item><title>NA-SODINN&#26159;&#19968;&#31181;&#22522;&#20110;&#27531;&#20313;&#22122;&#22768;&#27169;&#24335;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#22122;&#22768;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#22806;&#34892;&#26143;&#22270;&#20687;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02854</link><description>&lt;p&gt;
NA-SODINN:&#19968;&#31181;&#22522;&#20110;&#27531;&#20313;&#22122;&#22768;&#27169;&#24335;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#22806;&#34892;&#26143;&#22270;&#20687;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NA-SODINN: a deep learning algorithm for exoplanet image detection based on residual noise regimes. (arXiv:2302.02854v2 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02854
&lt;/p&gt;
&lt;p&gt;
NA-SODINN&#26159;&#19968;&#31181;&#22522;&#20110;&#27531;&#20313;&#22122;&#22768;&#27169;&#24335;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#22122;&#22768;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#22806;&#34892;&#26143;&#22270;&#20687;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;SODINN&#31639;&#27861;&#20171;&#32461;&#20102;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#22312;&#39640;&#23545;&#27604;&#24230;&#25104;&#20687;&#65288;HCI&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#27861;&#26159;&#20026;&#35282;&#24046;&#20998;&#25104;&#20687;&#65288;ADI&#65289;&#25968;&#25454;&#38598;&#20013;&#30340;&#22806;&#34892;&#26143;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#22806;&#34892;&#26143;&#25104;&#20687;&#25968;&#25454;&#25361;&#25112;&#65288;EIDC&#65289;&#20013;&#36827;&#34892;&#30340;HCI&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#65288;i&#65289;SODINN&#22312;&#26368;&#32456;&#26816;&#27979;&#22270;&#20013;&#21487;&#33021;&#20135;&#29983;&#22823;&#37327;&#30340;&#35823;&#25253;&#65292;&#65288;ii&#65289;&#20197;&#26356;&#23616;&#37096;&#30340;&#26041;&#24335;&#22788;&#29702;&#22270;&#20687;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23616;&#37096;&#22788;&#29702;&#26041;&#27861;&#24182;&#30456;&#24212;&#22320;&#35843;&#25972;&#23398;&#20064;&#36807;&#31243;&#26469;&#25552;&#39640;SODINN&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;NA-SODINN&#65292;&#36890;&#36807;&#35782;&#21035;&#22122;&#22768;&#27169;&#24335;&#26356;&#22909;&#22320;&#25429;&#25417;ADI&#22788;&#29702;&#24103;&#20013;&#30340;&#22270;&#20687;&#22122;&#22768;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#36890;&#36807;&#23616;&#37096;&#25509;&#25910;&#20989;&#25968;&#27979;&#35797;&#20102;&#20854;&#21069;&#20219;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;SODINN&#30340;&#20004;&#31181;&#28151;&#21512;&#27169;&#22411;&#21644;&#26356;&#26631;&#20934;&#30340;&#29615;&#24418;PCA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised deep learning was recently introduced in high-contrast imaging (HCI) through the SODINN algorithm, a convolutional neural network designed for exoplanet detection in angular differential imaging (ADI) datasets. The benchmarking of HCI algorithms within the Exoplanet Imaging Data Challenge (EIDC) showed that (i) SODINN can produce a high number of false positives in the final detection maps, and (ii) algorithms processing images in a more local manner perform better. This work aims to improve the SODINN detection performance by introducing new local processing approaches and adapting its learning process accordingly. We propose NA-SODINN, a new deep learning binary classifier based on a convolutional neural network (CNN) that better captures image noise correlations in ADI-processed frames by identifying noise regimes. Our new approach was tested against its predecessor, as well as two SODINN-based hybrid models and a more standard annular-PCA approach, through local receivin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.01735</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#26041;&#24046;&#32553;&#20943;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#25552;&#39640;&#35270;&#35273;&#34920;&#31034;&#36136;&#37327;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#65292;&#22312;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#37319;&#26679;&#20855;&#26377;&#30495;&#27491;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#30340;&#36127;&#26679;&#26412;&#65292;&#21017;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#26469;&#33258;&#30456;&#20284;&#30340;&#35299;&#21078;&#29305;&#24449;&#65292;&#27169;&#22411;&#21487;&#33021;&#38590;&#20197;&#21306;&#20998;&#23569;&#25968;&#23614;&#31867;&#26679;&#26412;&#65292;&#20351;&#24471;&#23614;&#31867;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#20248;&#21270;&#65292; &#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#65292;&#24182;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#23398;&#20064;&#39640;&#36136;&#37327;&#31070;&#32463;&#22330;&#12290;</title><link>http://arxiv.org/abs/2302.00617</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Learning Large-scale Neural Fields via Context Pruned Meta-Learning. (arXiv:2302.00617v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00617
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#20248;&#21270;&#65292; &#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#65292;&#24182;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#23398;&#20064;&#39640;&#36136;&#37327;&#31070;&#32463;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#22312;&#32447;&#19978;&#19979;&#25991;&#28857;&#36873;&#25321;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#39640;&#25928;&#20248;&#21270;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#23398;&#20064;&#27493;&#39588;&#38598;&#20013;&#22312;&#20855;&#26377;&#26368;&#39640;&#26399;&#26395;&#31435;&#21363;&#27169;&#22411;&#36136;&#37327;&#25913;&#36827;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#65292;&#23454;&#29616;&#20840;&#23616;&#32467;&#26500;&#30340;&#20960;&#20046;&#21363;&#26102;&#24314;&#27169;&#21644;&#39640;&#39057;&#32454;&#33410;&#30340;&#21518;&#32493;&#32454;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#26657;&#27491;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20803;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20943;&#23569;&#19978;&#19979;&#25991;&#38598;&#26102;&#24341;&#20837;&#30340;&#20219;&#20309;&#35823;&#24046;&#30340;&#26368;&#23567;&#21270;&#65292;&#24182;&#21516;&#26102;&#32531;&#35299;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#30701;&#35270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20803;&#27979;&#35797;&#26102;&#36827;&#34892;&#26799;&#24230;&#37325;&#26032;&#32553;&#25918;&#65292;&#20174;&#32780;&#22312;&#26174;&#33879;&#32553;&#30701;&#20248;&#21270;&#36807;&#31243;&#30340;&#21516;&#26102;&#23398;&#20064;&#26497;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#22330;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#30452;&#35266;&#26131;&#25026;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#37325;&#26500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an efficient optimization-based meta-learning technique for large-scale neural field training by realizing significant memory savings through automated online context point selection. This is achieved by focusing each learning step on the subset of data with the highest expected immediate improvement in model quality, resulting in the almost instantaneous modeling of global structure and subsequent refinement of high-frequency details. We further improve the quality of our meta-learned initialization by introducing a bootstrap correction resulting in the minimization of any error introduced by reduced context sets while simultaneously mitigating the well-known myopia of optimization-based meta-learning. Finally, we show how gradient re-scaling at meta-test time allows the learning of extremely high-quality neural fields in significantly shortened optimization procedures. Our framework is model-agnostic, intuitive, straightforward to implement, and shows significant reconst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#30740;&#31350;&#35270;&#35282;&#65292;&#22238;&#31572;&#20102;&#24403;&#22270;&#33410;&#28857;&#25968;&#37327;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;GNN&#30340;&#34892;&#20026;&#22914;&#20309;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35777;&#26126;&#19981;&#26029;&#22686;&#22823;&#30340;&#22270;&#26144;&#23556;&#21040;GNN&#20998;&#31867;&#22120;&#30340;&#29305;&#23450;&#36755;&#20986;&#30340;&#27010;&#29575;&#36235;&#20110;&#38646;&#25110;&#19968;&#65292;&#24314;&#31435;&#20102;&#36825;&#20123;GNN&#30340;&#38646;&#19968;&#23450;&#24459;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2301.13060</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#19968;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Zero-One Laws of Graph Neural Networks. (arXiv:2301.13060v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#30740;&#31350;&#35270;&#35282;&#65292;&#22238;&#31572;&#20102;&#24403;&#22270;&#33410;&#28857;&#25968;&#37327;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;GNN&#30340;&#34892;&#20026;&#22914;&#20309;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35777;&#26126;&#19981;&#26029;&#22686;&#22823;&#30340;&#22270;&#26144;&#23556;&#21040;GNN&#20998;&#31867;&#22120;&#30340;&#29305;&#23450;&#36755;&#20986;&#30340;&#27010;&#29575;&#36235;&#20110;&#38646;&#25110;&#19968;&#65292;&#24314;&#31435;&#20102;&#36825;&#20123;GNN&#30340;&#38646;&#19968;&#23450;&#24459;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#29992;&#20110;&#23545;&#22270;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26631;&#20934;&#20307;&#31995;&#32467;&#26500;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#24037;&#20316;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#30340;&#34920;&#31034;&#21644;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#35270;&#35282;&#65292;&#22238;&#31572;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#22270;&#33410;&#28857;&#30340;&#25968;&#37327;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;&#65292;GNNs&#30340;&#34892;&#20026;&#22914;&#20309;&#65311;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#25105;&#20204;&#20174;Erd&#337;s-R&#233;nyi&#27169;&#22411;&#20013;&#32472;&#21046;&#19981;&#26029;&#22686;&#22823;&#30340;&#22270;&#26102;&#65292;&#36825;&#20123;&#22270;&#26144;&#23556;&#21040;GNN&#20998;&#31867;&#22120;&#30340;&#29305;&#23450;&#36755;&#20986;&#30340;&#27010;&#29575;&#36235;&#20110;&#38646;&#25110;&#19968;&#12290;&#36825;&#20010;&#31867;&#21253;&#25324;&#27969;&#34892;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#12290;&#36825;&#20010;&#32467;&#26524;&#24314;&#31435;&#20102;&#36825;&#20123;GNN&#30340;&#38646;&#19968;&#23450;&#24459;&#65292;&#24182;&#19988;&#31867;&#27604;&#20110;&#20854;&#20182;&#25910;&#25947;&#23450;&#24459;&#65292;&#24102;&#26469;&#20102;&#23427;&#20204;&#22312;&#29702;&#35770;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#35266;&#23519;&#21040;&#29702;&#35770;&#19982;&#23454;&#36341;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erd\H{o}s-R\'enyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoret
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#34920;&#29616;&#21147;&#24378;&#19988;&#33021;&#22788;&#29702;&#38271;&#26399;&#32467;&#26500;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;Mo^usai&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22810;&#20998;&#38047;&#39640;&#36136;&#37327;&#38899;&#20048;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#26631;&#20934;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.11757</link><description>&lt;p&gt;
Mo^usai: &#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#28508;&#22312;&#25193;&#25955;&#36827;&#34892;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Mo\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#34920;&#29616;&#21147;&#24378;&#19988;&#33021;&#22788;&#29702;&#38271;&#26399;&#32467;&#26500;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;Mo^usai&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22810;&#20998;&#38047;&#39640;&#36136;&#37327;&#38899;&#20048;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#26631;&#20934;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#25991;&#26412;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#25991;&#26412;&#19982;&#21478;&#19968;&#31181;&#8220;&#35821;&#35328;&#8221;&#8212;&#8212;&#38899;&#20048;&#30340;&#20851;&#32852;&#20851;&#31995;&#65292;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#38899;&#20048;&#19982;&#25991;&#26412;&#19968;&#26679;&#65292;&#21487;&#20197;&#20256;&#36798;&#24773;&#24863;&#12289;&#25925;&#20107;&#21644;&#24605;&#24819;&#65292;&#20855;&#26377;&#33258;&#24049;&#29420;&#29305;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#12289;&#34920;&#29616;&#21147;&#24378;&#19988;&#33021;&#22815;&#22788;&#29702;&#38271;&#26399;&#32467;&#26500;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#19982;&#38899;&#20048;&#32852;&#31995;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Mo^usai&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#30340;&#20004;&#38454;&#27573;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22810;&#20998;&#38047;&#30340;&#39640;&#36136;&#37327;48kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#28040;&#36153;&#32423;GPU&#19978;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#65292;&#24182;&#20855;&#26377;&#21512;&#29702;&#30340;&#36895;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#22810;&#31181;&#26631;&#20934;&#19979;&#30456;&#23545;&#20110;&#29616;&#26377;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#25512;&#21160;&#24320;&#28304;&#25991;&#21270;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#24320;&#28304;&#30340;&#24037;&#20855;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another "language" of communication -- music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Mo\^usai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model's competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#32467;&#26500;&#20174;&#32780;&#23454;&#29616;&#23545;&#28508;&#22312;&#29305;&#24449;&#30340;&#24674;&#22797;&#65292;&#24182;&#34920;&#26126;GNN&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.10956</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20165;&#20165;&#20174;&#22270;&#32467;&#26500;&#20013;&#24674;&#22797;&#20986;&#38544;&#34255;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks can Recover the Hidden Features Solely from the Graph Structure. (arXiv:2301.10956v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#32467;&#26500;&#20174;&#32780;&#23454;&#29616;&#23545;&#28508;&#22312;&#29305;&#24449;&#30340;&#24674;&#22797;&#65292;&#24182;&#34920;&#26126;GNN&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#22788;&#29702;&#22270;&#23398;&#20064;&#38382;&#39064;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#23427;&#22312;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20854;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#38416;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#35282;&#24230;&#30740;&#31350;&#20102;GNN&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#32467;&#26500;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21463;&#38544;&#34255;&#65288;&#25110;&#28508;&#22312;&#65289;&#33410;&#28857;&#29305;&#24449;&#25511;&#21046;&#30340;&#22270;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#20123;&#29305;&#24449;&#21253;&#21547;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;&#36825;&#31181;&#26694;&#26550;&#30340;&#20856;&#22411;&#31034;&#20363;&#26159;&#20174;&#38544;&#34255;&#29305;&#24449;&#26500;&#24314;&#30340;kNN&#22270;&#12290;&#22312;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;GNN&#21487;&#20197;&#20165;&#20174;&#36755;&#20837;&#22270;&#20013;&#24674;&#22797;&#20986;&#38544;&#34255;&#33410;&#28857;&#29305;&#24449;&#65292;&#21363;&#20351;&#25152;&#26377;&#33410;&#28857;&#29305;&#24449;&#65292;&#21253;&#25324;&#38544;&#34255;&#29305;&#24449;&#26412;&#36523;&#21644;&#20219;&#20309;&#38388;&#25509;&#25552;&#31034;&#37117;&#19981;&#21487;&#29992;&#12290;GNN&#21487;&#20197;&#36827;&#19968;&#27493;&#21033;&#29992;&#24674;&#22797;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;GNN&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#22270;&#32467;&#26500;&#33258;&#36523;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are popular models for graph learning problems. GNNs show strong empirical performance in many practical tasks. However, the theoretical properties have not been completely elucidated. In this paper, we investigate whether GNNs can exploit the graph structure from the perspective of the expressive power of GNNs. In our analysis, we consider graph generation processes that are controlled by hidden (or latent) node features, which contain all information about the graph structure. A typical example of this framework is kNN graphs constructed from the hidden features. In our main results, we show that GNNs can recover the hidden node features from the input graph alone, even when all node features, including the hidden features themselves and any indirect hints, are unavailable. GNNs can further use the recovered node features for downstream tasks. These results show that GNNs can fully exploit the graph structure by themselves, and in effect, GNNs can use bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#31639;&#27861;&#65288;AdaFGDA&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#65292;&#22312;&#26799;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.07303</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#21450;&#20854;&#36739;&#20302;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Minimax Optimization with Lower complexities. (arXiv:2211.07303v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#31639;&#27861;&#65288;AdaFGDA&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#65292;&#22312;&#26799;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#65292;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#26799;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#31639;&#27861;&#19987;&#27880;&#20110;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#21152;&#36895;&#31639;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#24335;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#31639;&#27861;&#65288;&#21363;AdaFGDA&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#20998;&#24067;&#24335;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;AdaFGDA&#24314;&#31435;&#22312;&#22522;&#20110;&#21160;&#37327;&#30340;&#26041;&#24046;&#20943;&#23569;&#21644;&#23616;&#37096;SGD&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#33258;&#36866;&#24212;&#30697;&#38453;&#21487;&#20197;&#28789;&#27963;&#22320;&#32467;&#21512;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22362;&#23454;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#26694;&#26550;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a popular distributed and privacy-preserving machine learning paradigm. Meanwhile, minimax optimization, as an effective hierarchical optimization, is widely applied in machine learning. Recently, some federated optimization methods have been proposed to solve the distributed minimax problems. However, these federated minimax methods still suffer from high gradient and communication complexities. Meanwhile, few algorithm focuses on using adaptive learning rate to accelerate algorithms. To fill this gap, in the paper, we study a class of nonconvex minimax optimization, and propose an efficient adaptive federated minimax optimization algorithm (i.e., AdaFGDA) to solve these distributed minimax problems. Specifically, our AdaFGDA builds on the momentum-based variance reduced and local-SGD techniques, and it can flexibly incorporate various adaptive learning rates by using the unified adaptive matrix. Theoretically, we provide a solid convergence analysis framework fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#20809;&#20239;&#30005;&#27744;&#27169;&#22359;&#32454;&#32990;&#22270;&#20687;&#20013;THC&#32570;&#38519;&#20998;&#21106;&#36755;&#20986;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26126;&#30830;&#30340;&#36755;&#20986;&#19981;&#24179;&#34913;&#24230;&#37327;&#26041;&#27861;&#12289;&#22522;&#20110;&#20998;&#24067;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25512;&#24191;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20986;&#19981;&#24179;&#34913;&#65292;&#20197;&#21450;&#22797;&#21512;&#25439;&#22833;&#20989;&#25968;&#21644;&#33258;&#36866;&#24212;&#36229;&#21442;&#25968;&#36873;&#25321;&#31639;&#27861;&#30340;&#24341;&#20837;&#20197;&#23454;&#29616;&#36755;&#20986;&#19981;&#24179;&#34913;&#22312;&#26497;&#24230;&#19981;&#24179;&#34913;&#36755;&#20837;&#25968;&#25454;&#19978;&#30340;&#21327;&#35843;&#12290;</title><link>http://arxiv.org/abs/2211.05295</link><description>&lt;p&gt;
&#35299;&#20915;&#26497;&#24230;&#19981;&#24179;&#34913;&#20809;&#20239;&#30005;&#27744;&#27169;&#22359;&#32454;&#32990;&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#20998;&#21106;&#36755;&#20986;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Harmonizing output imbalance for defect segmentation on extremely-imbalanced photovoltaic module cells images. (arXiv:2211.05295v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#20809;&#20239;&#30005;&#27744;&#27169;&#22359;&#32454;&#32990;&#22270;&#20687;&#20013;THC&#32570;&#38519;&#20998;&#21106;&#36755;&#20986;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26126;&#30830;&#30340;&#36755;&#20986;&#19981;&#24179;&#34913;&#24230;&#37327;&#26041;&#27861;&#12289;&#22522;&#20110;&#20998;&#24067;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25512;&#24191;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20986;&#19981;&#24179;&#34913;&#65292;&#20197;&#21450;&#22797;&#21512;&#25439;&#22833;&#20989;&#25968;&#21644;&#33258;&#36866;&#24212;&#36229;&#21442;&#25968;&#36873;&#25321;&#31639;&#27861;&#30340;&#24341;&#20837;&#20197;&#23454;&#29616;&#36755;&#20986;&#19981;&#24179;&#34913;&#22312;&#26497;&#24230;&#19981;&#24179;&#34913;&#36755;&#20837;&#25968;&#25454;&#19978;&#30340;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20239;&#65288;PV&#65289;&#34892;&#19994;&#30340;&#25345;&#32493;&#21457;&#23637;&#23545;&#21333;&#26230;&#20809;&#20239;&#27169;&#22359;&#32454;&#32990;&#30340;&#36136;&#37327;&#25552;&#20986;&#20102;&#39640;&#35201;&#27714;&#12290;&#22312;&#23398;&#20064;&#20809;&#20239;&#27169;&#22359;&#32454;&#32990;&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#21306;&#22495;&#20998;&#21106;&#26102;&#65292;&#24494;&#23567;&#38544;&#34255;&#35010;&#32541;&#65288;THC&#65289;&#23548;&#33268;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#26679;&#26412;&#12290;&#32570;&#38519;&#20687;&#32032;&#19982;&#27491;&#24120;&#20687;&#32032;&#30340;&#27604;&#20363;&#21487;&#20197;&#20302;&#33267;1&#65306;2000&#12290;&#36825;&#31181;&#26497;&#31471;&#19981;&#24179;&#34913;&#20351;&#24471;&#38590;&#20197;&#20998;&#21106;&#20809;&#20239;&#27169;&#22359;&#32454;&#32990;&#30340;THC&#65292;&#20063;&#26159;&#35821;&#20041;&#20998;&#21106;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#22312;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;THC&#25968;&#25454;&#19978;&#36827;&#34892;&#32570;&#38519;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#19977;&#20010;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#65288;1&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#36755;&#20986;&#19981;&#24179;&#34913;&#24230;&#37327;&#26041;&#27861;&#65307;&#65288;2&#65289;&#25512;&#24191;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20986;&#19981;&#24179;&#34913;&#65307;&#65288;3&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#21512;&#25439;&#22833;&#20989;&#25968;&#21644;&#33258;&#36866;&#24212;&#36229;&#21442;&#25968;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#23454;&#29616;&#22312;&#26497;&#24230;&#19981;&#24179;&#34913;&#36755;&#20837;&#25968;&#25454;&#19978;&#30340;&#36755;&#20986;&#19981;&#24179;&#34913;&#30340;&#21327;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The continuous development of the photovoltaic (PV) industry has raised high requirements for the quality of monocrystalline of PV module cells. When learning to segment defect regions in PV module cell images, Tiny Hidden Cracks (THC) lead to extremely-imbalanced samples. The ratio of defect pixels to normal pixels can be as low as 1:2000. This extreme imbalance makes it difficult to segment the THC of PV module cells, which is also a challenge for semantic segmentation. To address the problem of segmenting defects on extremely-imbalanced THC data, the paper makes contributions from three aspects: (1) it proposes an explicit measure for output imbalance; (2) it generalizes a distribution-based loss that can handle different types of output imbalances; and (3) it introduces a compound loss with our adaptive hyperparameter selection algorithm that can keep the consistency of training and inference for harmonizing the output imbalance on extremelyimbalanced input data. The proposed metho
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#25628;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#36873;&#25321;&#29983;&#25104;&#19982;&#26597;&#35810;&#26368;&#30456;&#20284;&#20869;&#23481;&#27010;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#26597;&#35810;&#27169;&#24577;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#65288;&#32763;&#35793;&#20026;&#20013;&#25991;&#65289;</title><link>http://arxiv.org/abs/2210.03116</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Content-Based Search for Deep Generative Models. (arXiv:2210.03116v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03116
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#25628;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#36873;&#25321;&#29983;&#25104;&#19982;&#26597;&#35810;&#26368;&#30456;&#20284;&#20869;&#23481;&#27010;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#26597;&#35810;&#27169;&#24577;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#65288;&#32763;&#35793;&#20026;&#20013;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23450;&#20041;&#21644;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#29992;&#25143;&#19981;&#21487;&#33021;&#23436;&#20840;&#20102;&#35299;&#27599;&#20010;&#23384;&#22312;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#27169;&#22411;&#25628;&#32034;&#20219;&#21153;&#65306;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#21644;&#19968;&#32452;&#22823;&#35268;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25214;&#21040;&#19982;&#26597;&#35810;&#26368;&#21305;&#37197;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#19968;&#31995;&#21015;&#22270;&#20687;&#30340;&#20998;&#24067;&#65292;&#25105;&#20204;&#23558;&#25628;&#32034;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#36873;&#25321;&#29983;&#25104;&#19982;&#26597;&#35810;&#30456;&#20284;&#20869;&#23481;&#27010;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#27010;&#29575;&#30340;&#20844;&#24335;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#26597;&#35810;&#27169;&#24577;&#65288;&#20363;&#22914;&#22270;&#20687;&#12289;&#33609;&#22270;&#21644;&#25991;&#26412;&#65289;&#26469;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#27169;&#22411;&#26816;&#32034;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#26597;&#35810;&#27169;&#24577;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#27169;&#22411;&#21160;&#29289;&#22253;&#65288;Generative Model Zoo&#65289;&#19978;&#20248;&#20110;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing proliferation of customized and pretrained generative models has made it infeasible for a user to be fully cognizant of every model in existence. To address this need, we introduce the task of content-based model search: given a query and a large set of generative models, finding the models that best match the query. As each generative model produces a distribution of images, we formulate the search task as an optimization problem to select the model with the highest probability of generating similar content as the query. We introduce a formulation to approximate this probability given the query from different modalities, e.g., image, sketch, and text. Furthermore, we propose a contrastive learning framework for model retrieval, which learns to adapt features for various query modalities. We demonstrate that our method outperforms several baselines on Generative Model Zoo, a new benchmark we create for the model retrieval task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral2Spectral&#30340;&#28145;&#24230;&#37325;&#24314;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#22270;&#20687;-&#20809;&#35889;&#39046;&#22495;&#20869;&#30340;&#30456;&#20284;&#24615;&#20808;&#39564;&#36890;&#36807;&#26080;&#21442;&#32771;&#26041;&#24335;&#26469;&#36741;&#21161;&#20809;&#35889;CT&#30340;&#28145;&#24230;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2210.01125</link><description>&lt;p&gt;
Spectral2Spectral: &#26080;&#21442;&#32771;&#30340;&#22270;&#20687;&#20809;&#35889;&#30456;&#20284;&#24615;&#36741;&#21161;&#20809;&#35889;CT&#28145;&#24230;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep Reconstruction without Reference. (arXiv:2210.01125v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral2Spectral&#30340;&#28145;&#24230;&#37325;&#24314;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#22270;&#20687;-&#20809;&#35889;&#39046;&#22495;&#20869;&#30340;&#30456;&#20284;&#24615;&#20808;&#39564;&#36890;&#36807;&#26080;&#21442;&#32771;&#26041;&#24335;&#26469;&#36741;&#21161;&#20809;&#35889;CT&#30340;&#28145;&#24230;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#65288;PCD&#65289;&#30340;&#20809;&#35889;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#20026;&#29983;&#29289;&#21307;&#23398;&#26448;&#26009;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#29421;&#31364;&#33021;&#37327;&#21306;&#38388;&#20869;&#26377;&#38480;&#30340;&#20809;&#23376;&#25968;&#37327;&#23548;&#33268;&#22270;&#20687;&#32467;&#26524;&#30340;&#20449;&#22122;&#27604;&#36739;&#20302;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;CT&#37325;&#24314;&#30340;&#30417;&#30563;&#24335;&#28145;&#24230;&#37325;&#24314;&#32593;&#32476;&#24456;&#38590;&#35299;&#20915;&#65292;&#22240;&#20026;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#24102;&#26377;&#28165;&#26224;&#32467;&#26500;&#30340;&#26080;&#22122;&#22768;&#20020;&#24202;&#22270;&#20687;&#20316;&#20026;&#21442;&#32771;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#28145;&#24230;&#37325;&#24314;&#32593;&#32476;&#65292;&#23558;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#25968;&#25454;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#21629;&#21517;&#20026;Spectral2Spectral&#12290;&#25105;&#20204;&#30340;Spectral2Spectral&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20013;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#22270;&#20687;-&#20809;&#35889;&#39046;&#22495;&#20869;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#20808;&#39564;&#34987;&#25913;&#36827;&#20026;&#27491;&#21017;&#21270;&#39033;&#65292;&#36827;&#19968;&#27493;&#32422;&#26463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral computed tomography based on a photon-counting detector (PCD) attracts more and more attentions since it has the capability to provide more accurate identification and quantitative analysis for biomedical materials. The limited number of photons within narrow energy bins leads to imaging results of low signal-noise ratio. The existing supervised deep reconstruction networks for CT reconstruction are difficult to address these challenges because it is usually impossible to acquire noise-free clinical images with clear structures as references. In this paper, we propose an iterative deep reconstruction network to synergize unsupervised method and data priors into a unified framework, named as Spectral2Spectral. Our Spectral2Spectral employs an unsupervised deep training strategy to obtain high-quality images from noisy data in an end-to-end fashion. The structural similarity prior within image-spectral domain is refined as a regularization term to further constrain the network t
&lt;/p&gt;</description></item><item><title>&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#65288;Amortized Variational Inference&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#21270;&#20989;&#25968;&#23398;&#20064;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#23494;&#24230;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;VI&#31639;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#25512;&#26029;&#20986;&#30028;&#25968;&#25454;&#28857;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.10888</link><description>&lt;p&gt;
&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Amortized Variational Inference: A Systematic Review. (arXiv:2209.10888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10888
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#65288;Amortized Variational Inference&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#21270;&#20989;&#25968;&#23398;&#20064;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#23494;&#24230;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;VI&#31639;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#25512;&#26029;&#20986;&#30028;&#25968;&#25454;&#28857;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#30340;&#26680;&#24515;&#21407;&#21017;&#26159;&#23558;&#35745;&#31639;&#22797;&#26434;&#21518;&#39564;&#27010;&#29575;&#23494;&#24230;&#30340;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#22788;&#29702;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20010;&#29305;&#24615;&#20351;&#24471;VI&#27604;&#20960;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#25216;&#26415;&#26356;&#24555;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;VI&#31639;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#25512;&#26029;&#20986;&#30028;&#25968;&#25454;&#28857;&#32780;&#19981;&#37325;&#26032;&#36816;&#34892;&#20248;&#21270;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;&#38543;&#26426;&#12289;&#40657;&#30418;&#21644;&#20998;&#26399;VI&#31561;&#39046;&#22495;&#30340;&#21457;&#23637;&#24050;&#32463;&#24110;&#21161;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#29616;&#22312;&#65292;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#24191;&#27867;&#20351;&#29992;&#20998;&#26399;VI&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#21442;&#25968;&#21270;&#20989;&#25968;&#26469;&#23398;&#20064;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#23494;&#24230;&#21442;&#25968;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#21508;&#31181;VI&#25216;&#26415;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#20026;&#29702;&#35299;&#20998;&#26399;VI&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#35299;&#20915;&#20998;&#26399;VI&#33509;&#24178;&#38382;&#39064;&#30340;&#26368;&#26032;&#36235;&#21183;&#65292;&#22914;
&lt;/p&gt;
&lt;p&gt;
The core principle of Variational Inference (VI) is to convert the statistical inference problem of computing complex posterior probability densities into a tractable optimization problem. This property enables VI to be faster than several sampling-based techniques. However, the traditional VI algorithm is not scalable to large data sets and is unable to readily infer out-of-bounds data points without re-running the optimization process. Recent developments in the field, like stochastic-, black box-, and amortized-VI, have helped address these issues. Generative modeling tasks nowadays widely make use of amortized VI for its efficiency and scalability, as it utilizes a parameterized function to learn the approximate posterior density parameters. In this paper, we review the mathematical foundations of various VI techniques to form the basis for understanding amortized VI. Additionally, we provide an overview of the recent trends that address several issues of amortized VI, such as the 
&lt;/p&gt;</description></item><item><title>DenseShift&#32593;&#32476;&#26159;&#19968;&#31181;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Shift&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#24341;&#20837;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09708</link><description>&lt;p&gt;
DenseShift: &#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization. (arXiv:2208.09708v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09708
&lt;/p&gt;
&lt;p&gt;
DenseShift&#32593;&#32476;&#26159;&#19968;&#31181;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Shift&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#24341;&#20837;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20854;&#19981;&#26029;&#22686;&#21152;&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26080;&#20056;&#27861;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#24130;&#20056;&#27861;&#30340;&#37327;&#21270;&#65292;&#20063;&#34987;&#31216;&#20026;Shift&#32593;&#32476;&#65292;&#26088;&#22312;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#21644;&#31616;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20302;&#20301;Shift&#32593;&#32476;&#19981;&#22914;&#20840;&#31934;&#24230;&#32593;&#32476;&#20934;&#30830;&#65292;&#36890;&#24120;&#21463;&#21040;&#26377;&#38480;&#26435;&#37325;&#33539;&#22260;&#32534;&#30721;&#26041;&#26696;&#21644;&#37327;&#21270;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DenseShift&#32593;&#32476;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;Shift&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#35270;&#35273;&#21644;&#35821;&#38899;&#24212;&#29992;&#23454;&#29616;&#20102;&#19982;&#20840;&#31934;&#24230;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#30340;&#39640;&#25928;DenseShift&#32593;&#32476;&#37096;&#32626;&#26041;&#27861;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;1.6&#20493;&#21152;&#36895;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20302;&#20301;Shift&#32593;&#32476;&#20013;&#38646;&#26435;&#37325;&#20540;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural networks, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are not as accurate as their full-precision counterparts, typically suffering from limited weight range encoding schemes and quantization loss. In this paper, we propose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive performance to full-precision networks for vision and speech applications. In addition, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6X speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#35299;&#32544;&#35270;&#35282;&#22788;&#29702;&#35270;&#39057;&#39046;&#22495;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#32544;&#38745;&#24577;&#21644;&#21160;&#24577;&#20449;&#24687;&#24182;&#20351;&#29992;&#22810;&#31181;&#32422;&#26463;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#31354;&#38388;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#20943;&#23569;&#26102;&#38388;&#39046;&#22495;&#24046;&#24322;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07365</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#21160;&#20316;&#35782;&#21035;&#65306;&#19968;&#20010;&#35299;&#32544;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective. (arXiv:2208.07365v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35299;&#32544;&#35270;&#35282;&#22788;&#29702;&#35270;&#39057;&#39046;&#22495;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#32544;&#38745;&#24577;&#21644;&#21160;&#24577;&#20449;&#24687;&#24182;&#20351;&#29992;&#22810;&#31181;&#32422;&#26463;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#31354;&#38388;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#20943;&#23569;&#26102;&#38388;&#39046;&#22495;&#24046;&#24322;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#19968;&#39033;&#23454;&#36341;&#24615;&#32780;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#35299;&#32544;&#35270;&#35282;&#20837;&#25163;&#22788;&#29702;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#36890;&#36807;&#35299;&#32544;&#26469;&#20998;&#21035;&#22788;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#39046;&#22495;&#30340;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#21253;&#21547;&#38745;&#24577;&#20449;&#24687;&#30340;&#19968;&#32452;&#28508;&#22312;&#22240;&#32032;&#21644;&#21253;&#21547;&#21160;&#24577;&#20449;&#24687;&#30340;&#21478;&#19968;&#32452;&#28508;&#22312;&#22240;&#32032;&#20013;&#29983;&#25104;&#36328;&#39046;&#22495;&#35270;&#39057;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36716;&#31227;&#26102;&#24207;VAE&#65288;TranSVAE&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#36825;&#31181;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#32422;&#26463;&#28508;&#22312;&#22240;&#32032;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#20123;&#32422;&#26463;&#65292;&#38745;&#24577;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#35299;&#32544;&#21487;&#20197;&#36731;&#26494;&#31227;&#38500;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20174;&#24103;&#21644;&#35270;&#39057;&#23618;&#38754;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#26102;&#38388;&#24046;&#24322;&#12290;&#22312;UCF-HMDB&#12289;Jester&#21644;Epic-Kitchens&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;TranSVAE&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to handle the spatial and temporal domain divergence separately through disentanglement. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static information and another encoding the dynamic information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we propose several objectives to constrain the latent factors. With these constraints, the spatial divergence can be readily removed by disentangling the static domain-specific information out, and the temporal divergence is further reduced from both frame- and video-levels through adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#30340;&#26032;&#39062;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#33039;&#21608;&#26399;&#24615;&#29305;&#24615;&#65292;&#22312;&#23156;&#20799;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19977;&#31181;&#21464;&#20998;&#28508;&#22312;&#36712;&#36857;&#27169;&#22411;&#65292;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#20005;&#37325;&#30340;&#20808;&#22825;&#24615;&#24515;&#33039;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.15316</link><description>&lt;p&gt;
&#21160;&#24577;&#21464;&#20998;&#36712;&#36857;&#27169;&#22411;&#20013;&#30340;&#24515;&#33039;&#36229;&#22768;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Echocardiograms with Dynamic Variational Trajectory Models. (arXiv:2206.15316v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#30340;&#26032;&#39062;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#33039;&#21608;&#26399;&#24615;&#29305;&#24615;&#65292;&#22312;&#23156;&#20799;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19977;&#31181;&#21464;&#20998;&#28508;&#22312;&#36712;&#36857;&#27169;&#22411;&#65292;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#20005;&#37325;&#30340;&#20808;&#22825;&#24615;&#24515;&#33039;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#30340;&#26032;&#39062;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24515;&#33039;&#21608;&#26399;&#24615;&#30340;&#29305;&#24615;&#65292;&#23398;&#20064;&#19977;&#31181;&#21464;&#20998;&#28508;&#22312;&#36712;&#36857;&#27169;&#22411;&#65288;TVAE&#65289;&#30340;&#21464;&#20307;&#12290;&#20854;&#20013;&#21069;&#20004;&#31181;&#21464;&#20307;&#65288;TVAE-C&#21644;TVAE-R&#65289;&#27169;&#25311;&#24515;&#33039;&#30340;&#20005;&#26684;&#21608;&#26399;&#24615;&#36816;&#21160;&#65292;&#32780;&#31532;&#19977;&#31181;&#21464;&#20307;&#65288;TVAE-S&#65289;&#26356;&#20026;&#36890;&#29992;&#65292;&#20801;&#35768;&#35270;&#39057;&#20013;&#31354;&#38388;&#34920;&#31034;&#30340;&#31227;&#20301;&#12290;&#25152;&#26377;&#27169;&#22411;&#37117;&#22312;&#19968;&#20010;&#26032;&#30340;&#20869;&#37096;&#23156;&#20799;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#20581;&#24247;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#20581;&#24247;&#20154;&#32676;&#30340;&#35268;&#33539;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#65288;MAP&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26469;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#31163;&#32676;&#26679;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#20005;&#37325;&#30340;&#20808;&#22825;&#24615;&#24515;&#33039;&#32570;&#38519;&#65292;&#22914;&#22467;&#26222;&#26031;&#22374;&#24322;&#24120;&#25110;Shone&#32508;&#21512;&#24449;&#12290;&#27492;&#22806;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#26631;&#20934;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;MAP-based&#24322;&#24120;&#26816;&#27979;&#65292;&#23427;&#23454;&#29616;&#20102;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel anomaly detection method for echocardiogram videos. The introduced method takes advantage of the periodic nature of the heart cycle to learn three variants of a variational latent trajectory model (TVAE). While the first two variants (TVAE-C and TVAE-R) model strict periodic movements of the heart, the third (TVAE-S) is more general and allows shifts in the spatial representation throughout the video. All models are trained on the healthy samples of a novel in-house dataset of infant echocardiogram videos consisting of multiple chamber views to learn a normative prior of the healthy population. During inference, maximum a posteriori (MAP) based anomaly detection is performed to detect out-of-distribution samples in our dataset. The proposed method reliably identifies severe congenital heart defects, such as Ebstein's Anomaly or Shone-complex. Moreover, it achieves superior performance over MAP-based anomaly detection with standard variational autoencoders when detect
&lt;/p&gt;</description></item><item><title>ULF&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#21407;&#29702;&#30340;&#22122;&#22768;&#38477;&#20302;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2204.06863</link><description>&lt;p&gt;
ULF: &#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision. (arXiv:2204.06863v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06863
&lt;/p&gt;
&lt;p&gt;
ULF&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#21407;&#29702;&#30340;&#22122;&#22768;&#38477;&#20302;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#20195;&#26367;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#30340;&#32463;&#27982;&#26377;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#26631;&#31614;&#20989;&#25968;&#65288;LFs&#65289;&#23545;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#33258;&#21160;&#26631;&#27880;&#12290;&#26631;&#31614;&#20989;&#25968;&#26159;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20026;&#30456;&#20851;&#31867;&#21035;&#29983;&#25104;&#20154;&#24037;&#26631;&#31614;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;k&#25240;&#20132;&#21449;&#39564;&#35777;&#21407;&#29702;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#22122;&#22768;&#38477;&#20302;&#25216;&#26415;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ULF&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#25152;&#26377;&#26631;&#31614;&#20989;&#25968;&#20043;&#22806;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#32416;&#27491;&#29305;&#23450;&#20110;&#20445;&#30041;&#26631;&#31614;&#20989;&#25968;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#23545;&#24369;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ULF&#36890;&#36807;&#37325;&#26032;&#20272;&#35745;&#39640;&#21487;&#38752;&#24615;&#20132;&#21449;&#39564;&#35777;&#26679;&#26412;&#19978;&#30340;&#26631;&#31614;&#20989;&#25968;&#20998;&#37197;&#65292;&#26469;&#25913;&#36827;&#26631;&#31614;&#20989;&#25968;&#23545;&#31867;&#21035;&#30340;&#20998;&#37197;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#23454;&#20102;ULF&#22312;&#22686;&#24378;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rule-based mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise reduction techniques for WS based on the principle of k-fold cross-validation. We introduce a new algorithm ULF for Unsupervised Labeling Function correction, which denoises WS data by leveraging models trained on all but some LFs to identify and correct biases specific to the held-out LFs. Specifically, ULF refines the allocation of LFs to classes by re-estimating this assignment on highly reliable cross-validated samples. Evaluation on multiple datasets confirms ULF's effectiveness in enhancing WS learning without the need for manual labeling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23436;&#20840;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#32452;&#21512;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#24341;&#20837;&#38544;&#31169;&#36807;&#28388;&#22120;&#21644;&#38544;&#31169;&#37324;&#31243;&#34920;&#26469;&#35299;&#20915;&#29616;&#26377;&#32452;&#21512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.05481</link><description>&lt;p&gt;
&#23436;&#20840;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fully Adaptive Composition in Differential Privacy. (arXiv:2203.05481v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23436;&#20840;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#32452;&#21512;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#24341;&#20837;&#38544;&#31169;&#36807;&#28388;&#22120;&#21644;&#38544;&#31169;&#37324;&#31243;&#34920;&#26469;&#35299;&#20915;&#29616;&#26377;&#32452;&#21512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#26159;&#24046;&#20998;&#38544;&#31169;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#12290;&#33879;&#21517;&#30340;&#39640;&#32423;&#32452;&#21512;&#23450;&#29702;&#20801;&#35768;&#22312;&#22522;&#26412;&#38544;&#31169;&#32452;&#21512;&#20801;&#35768;&#30340;&#24773;&#20917;&#19979;&#65292;&#26597;&#35810;&#31169;&#26377;&#25968;&#25454;&#24211;&#30340;&#27425;&#25968;&#22686;&#21152;&#21040;&#21407;&#26469;&#30340;&#24179;&#26041;&#20493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#35201;&#27714;&#22312;&#19982;&#25968;&#25454;&#20132;&#20114;&#20043;&#21069;&#22266;&#23450;&#25152;&#26377;&#31639;&#27861;&#30340;&#38544;&#31169;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;Rogers&#31561;&#20154;&#24341;&#20837;&#20102;&#23436;&#20840;&#33258;&#36866;&#24212;&#32452;&#21512;&#65292;&#20854;&#20013;&#31639;&#27861;&#21644;&#20854;&#38544;&#31169;&#21442;&#25968;&#21487;&#20197;&#33258;&#36866;&#24212;&#36873;&#25321;&#12290;&#20182;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#27010;&#29575;&#23545;&#35937;&#26469;&#34913;&#37327;&#33258;&#36866;&#24212;&#32452;&#21512;&#20013;&#30340;&#38544;&#31169;&#65306;&#38544;&#31169;&#36807;&#28388;&#22120;&#65288;privacy filters&#65289;&#65292;&#29992;&#20110;&#25552;&#20379;&#32452;&#21512;&#20132;&#20114;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#20197;&#21450;&#38544;&#31169;&#37324;&#31243;&#34920;&#65288;privacy odometers&#65289;&#65292;&#23545;&#38544;&#31169;&#25439;&#22833;&#30340;&#26102;&#38388;&#22343;&#21248;&#30028;&#38480;&#12290;&#39640;&#32423;&#32452;&#21512;&#21644;&#29616;&#26377;&#30340;&#36807;&#28388;&#22120;&#21644;&#37324;&#31243;&#34920;&#20043;&#38388;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#30340;&#36807;&#28388;&#22120;&#23545;&#34987;&#32452;&#21512;&#30340;&#31639;&#27861;&#25552;&#20986;&#20102;&#26356;&#24378;&#30340;&#20551;&#35774;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#37324;&#31243;&#34920;&#21644;&#36807;&#28388;&#22120;&#23384;&#22312;&#36739;&#22823;&#30340;&#24120;&#25968;&#65292;&#20351;&#24471;&#23427;&#20204;&#19981;&#23454;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#33021;&#22815;&#22312;&#23436;&#20840;&#33258;&#36866;&#24212;&#32452;&#21512;&#20013;&#20351;&#29992;&#30340;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Composition is a key feature of differential privacy. Well-known advanced composition theorems allow one to query a private database quadratically more times than basic privacy composition would permit. However, these results require that the privacy parameters of all algorithms be fixed before interacting with the data. To address this, Rogers et al. introduced fully adaptive composition, wherein both algorithms and their privacy parameters can be selected adaptively. They defined two probabilistic objects to measure privacy in adaptive composition: privacy filters, which provide differential privacy guarantees for composed interactions, and privacy odometers, time-uniform bounds on privacy loss. There are substantial gaps between advanced composition and existing filters and odometers. First, existing filters place stronger assumptions on the algorithms being composed. Second, these odometers and filters suffer from large constants, making them impractical. We construct filters that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27719;&#32858;&#23618;&#65292;&#31216;&#20026;LAP&#65292;&#23427;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#33258;&#35299;&#37322;&#24615;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#35813;&#27169;&#22359;&#21487;&#20197;&#36731;&#26494;&#22320;&#25554;&#20837;&#20219;&#20309;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29978;&#33267;&#26159;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2201.11808</link><description>&lt;p&gt;
LAP: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22359;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#22522;&#20110;&#27010;&#24565;&#30340;&#33258;&#35299;&#37322;&#21644;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
LAP: An Attention-Based Module for Concept Based Self-Interpretation and Knowledge Injection in Convolutional Neural Networks. (arXiv:2201.11808v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27719;&#32858;&#23618;&#65292;&#31216;&#20026;LAP&#65292;&#23427;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#33258;&#35299;&#37322;&#24615;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#35813;&#27169;&#22359;&#21487;&#20197;&#36731;&#26494;&#22320;&#25554;&#20837;&#20219;&#20309;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29978;&#33267;&#26159;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#26410;&#30693;&#24773;&#20917;&#19979;&#23481;&#26131;&#21463;&#21040;&#20559;&#35265;&#21644;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#25512;&#29702;&#32972;&#21518;&#30340;&#22797;&#26434;&#35745;&#31639;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#24182;&#19981;&#21487;&#29702;&#35299;&#65292;&#38590;&#20197;&#24314;&#31435;&#20449;&#20219;&#12290;&#22806;&#37096;&#35299;&#37322;&#26041;&#27861;&#35797;&#22270;&#20197;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#35299;&#37322;&#32593;&#32476;&#20915;&#31574;&#65292;&#20294;&#30001;&#20110;&#20854;&#20551;&#35774;&#21644;&#31616;&#21270;&#32780;&#34987;&#25351;&#36131;&#23384;&#22312;&#35884;&#35823;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#27169;&#22411;&#30340;&#22266;&#26377;&#33258;&#35299;&#37322;&#24615;&#34429;&#28982;&#26356;&#25239;&#22362;&#25345;&#19978;&#36848;&#35884;&#35823;&#65292;&#20294;&#26080;&#27861;&#24212;&#29992;&#20110;&#24050;&#32463;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27719;&#32858;&#23618;&#65292;&#31216;&#20026;&#23616;&#37096;&#27880;&#24847;&#21147;&#27719;&#32858;&#65288;LAP&#65289;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#33258;&#35299;&#37322;&#24615;&#21644;&#21487;&#33021;&#24615;&#65292;&#32780;&#19981;&#20250;&#20007;&#22833;&#24615;&#33021;&#12290;&#35813;&#27169;&#22359;&#21487;&#20197;&#36731;&#26494;&#22320;&#25554;&#20837;&#20219;&#20309;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21253;&#25324;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#23398;&#20064;&#21306;&#20998;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the state-of-the-art performance of deep convolutional neural networks, they are susceptible to bias and malfunction in unseen situations. Moreover, the complex computation behind their reasoning is not human-understandable to develop trust. External explainer methods have tried to interpret network decisions in a human-understandable way, but they are accused of fallacies due to their assumptions and simplifications. On the other side, the inherent self-interpretability of models, while being more robust to the mentioned fallacies, cannot be applied to the already trained models. In this work, we propose a new attention-based pooling layer, called Local Attention Pooling (LAP), that accomplishes self-interpretability and the possibility for knowledge injection without performance loss. The module is easily pluggable into any convolutional neural network, even the already trained ones. We have defined a weakly supervised training scheme to learn the distinguishing features in d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27969;&#24418;&#29702;&#35770;&#30340;&#20005;&#26684;&#26041;&#27861;&#37325;&#26032;&#23457;&#35270;&#20102;&#23186;&#20307;&#36755;&#20837;&#21464;&#24322;&#20013;&#30340;&#24863;&#30693;&#22810;&#26679;&#24615;&#65288;DIV&#65289;&#21644;&#26377;&#25928;&#24615;&#65288;VAL&#65289;&#20004;&#20010;&#20851;&#38190;&#30446;&#26631;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#19981;&#21487;&#20998;&#21106;&#22320;&#30456;&#20114;&#32465;&#23450;&#65292;&#24182;&#35777;&#26126;&#20102;SOTA&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.01956</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#26377;&#25928;&#19988;&#22810;&#26679;&#21270;&#30340;DNN&#27979;&#35797;&#20013;&#30340;&#30495;&#23454;&#23186;&#20307;&#25968;&#25454;&#21464;&#24322;
&lt;/p&gt;
&lt;p&gt;
Provably Valid and Diverse Mutations of Real-World Media Data for DNN Testing. (arXiv:2112.01956v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27969;&#24418;&#29702;&#35770;&#30340;&#20005;&#26684;&#26041;&#27861;&#37325;&#26032;&#23457;&#35270;&#20102;&#23186;&#20307;&#36755;&#20837;&#21464;&#24322;&#20013;&#30340;&#24863;&#30693;&#22810;&#26679;&#24615;&#65288;DIV&#65289;&#21644;&#26377;&#25928;&#24615;&#65288;VAL&#65289;&#20004;&#20010;&#20851;&#38190;&#30446;&#26631;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#19981;&#21487;&#20998;&#21106;&#22320;&#30456;&#20114;&#32465;&#23450;&#65292;&#24182;&#35777;&#26126;&#20102;SOTA&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#24120;&#25509;&#21463;&#39640;&#32500;&#30340;&#23186;&#20307;&#25968;&#25454;&#65288;&#20363;&#22914;&#29031;&#29255;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#24182;&#29702;&#35299;&#23427;&#20204;&#30340;&#24863;&#30693;&#20869;&#23481;&#65288;&#20363;&#22914;&#19968;&#21482;&#29483;&#65289;&#12290;&#20026;&#20102;&#27979;&#35797;DNN&#65292;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#26469;&#35302;&#21457;&#38169;&#35823;&#39044;&#27979;&#12290;&#19968;&#20123;&#21021;&#27493;&#30340;&#24037;&#20316;&#20351;&#29992;&#23383;&#33410;&#32423;&#21464;&#24322;&#25110;&#39046;&#22495;&#29305;&#23450;&#30340;&#36807;&#28388;&#22120;&#65288;&#20363;&#22914;&#65292;&#27169;&#31946;&#65289;&#65292;&#20854;&#21551;&#29992;&#30340;&#21464;&#24322;&#21487;&#33021;&#26377;&#38480;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#20808;&#36827;&#30340;&#24037;&#20316;&#37319;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#65288;&#26080;&#38480;&#30340;&#65289;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#21464;&#24322;&#21518;&#30340;&#36755;&#20837;&#22312;&#24863;&#30693;&#19978;&#26377;&#25928;&#65288;&#20363;&#22914;&#65292;&#19968;&#21482;&#29483;&#22312;&#21464;&#24322;&#21518;&#20173;&#28982;&#26159;&#8220;&#29483;&#8221;&#65289;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#19981;&#31934;&#30830;&#19988;&#19981;&#21487;&#27867;&#21270;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20197;&#27969;&#24418;&#20026;&#22522;&#30784;&#65292;&#22312;&#19968;&#20010;&#20005;&#26684;&#30340;&#26041;&#24335;&#20013;&#37325;&#26032;&#23457;&#35270;&#23186;&#20307;&#36755;&#20837;&#21464;&#24322;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#30446;&#26631; - &#24863;&#30693;&#22810;&#26679;&#24615;&#65288;DIV&#65289;&#21644;&#26377;&#25928;&#24615;&#65288;VAL&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DIV&#21644;VAL&#19981;&#21487;&#20998;&#21106;&#22320;&#30456;&#20114;&#32465;&#23450;&#30340;&#37325;&#35201;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;SOTA&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) often accept high-dimensional media data (e.g., photos, text, and audio) and understand their perceptual content (e.g., a cat). To test DNNs, diverse inputs are needed to trigger mis-predictions. Some preliminary works use byte-level mutations or domain-specific filters (e.g., foggy), whose enabled mutations may be limited and likely error-prone. SOTA works employ deep generative models to generate (infinite) inputs. Also, to keep the mutated inputs perceptually valid (e.g., a cat remains a "cat" after mutation), existing efforts rely on imprecise and less generalizable heuristics.  This study revisits two key objectives in media input mutation - perception diversity (DIV) and validity (VAL) - in a rigorous manner based on manifold, a well-developed theory capturing perceptions of high-dimensional media data in a low-dimensional space. We show important results that DIV and VAL inextricably bound each other, and prove that SOTA generative model-based methods
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#31616;&#21333;&#30340;&#20915;&#31574;&#26641;&#25193;&#23637;&#65292;&#36890;&#36807;&#32487;&#32493;&#29983;&#38271;&#29616;&#26377;&#26641;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#24182;&#29992;&#26032;&#26641;&#26367;&#25442;&#19968;&#20123;&#26087;&#26641;&#26469;&#25511;&#21046;&#24635;&#26641;&#30340;&#25968;&#37327;&#12290;&#22312;72&#20010;&#20998;&#31867;&#38382;&#39064;&#30340;&#22522;&#20934;&#22871;&#20214;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2110.08483</link><description>&lt;p&gt;
&#26368;&#31616;&#26131;&#30340;&#27969;&#24335;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Simplest Streaming Trees. (arXiv:2110.08483v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08483
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#31616;&#21333;&#30340;&#20915;&#31574;&#26641;&#25193;&#23637;&#65292;&#36890;&#36807;&#32487;&#32493;&#29983;&#38271;&#29616;&#26377;&#26641;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#24182;&#29992;&#26032;&#26641;&#26367;&#25442;&#19968;&#20123;&#26087;&#26641;&#26469;&#25511;&#21046;&#24635;&#26641;&#30340;&#25968;&#37327;&#12290;&#22312;72&#20010;&#20998;&#31867;&#38382;&#39064;&#30340;&#22522;&#20934;&#22871;&#20214;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26862;&#26519;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#26641;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#25968;&#25454;&#38382;&#39064;&#19978;&#20173;&#28982;&#26159;&#20027;&#27969;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24403;&#21069;&#30340;&#23454;&#29616;&#21482;&#33021;&#20197;&#25209;&#22788;&#29702;&#27169;&#24335;&#36816;&#34892;&#65292;&#22240;&#27492;&#19981;&#33021;&#22312;&#26377;&#26356;&#22810;&#25968;&#25454;&#21040;&#36798;&#26102;&#36827;&#34892;&#22686;&#37327;&#26356;&#26032;&#12290;&#20043;&#21069;&#26377;&#20960;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#27969;&#24335;&#20915;&#31574;&#26641;&#21644;&#38598;&#25104;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26368;&#26032;&#31639;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#31934;&#24230;&#20302;&#21644;&#22312;&#20854;&#20182;&#38382;&#39064;&#19978;&#20869;&#23384;&#20351;&#29992;&#37327;&#22823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26368;&#31616;&#21333;&#30340;&#20915;&#31574;&#26641;&#25193;&#23637;&#65306;&#32473;&#23450;&#26032;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#32487;&#32493;&#29983;&#38271;&#29616;&#26377;&#26641;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#24182;&#29992;&#26032;&#26641;&#26367;&#25442;&#19968;&#20123;&#26087;&#26641;&#26469;&#25511;&#21046;&#24635;&#26641;&#30340;&#25968;&#37327;&#12290;&#22312;&#21253;&#21547;72&#20010;&#20998;&#31867;&#38382;&#39064;&#30340;&#22522;&#20934;&#22871;&#20214;&#65288;OpenML-CC18&#25968;&#25454;&#22871;&#20214;&#65289;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;Stream Decision Forest&#65288;SDF&#65289;&#26082;&#19981;&#36973;&#21463;&#19978;&#36848;&#38382;&#39064;&#30340;&#22256;&#25200;
&lt;/p&gt;
&lt;p&gt;
Decision forests, including random forests and gradient boosting trees, remain the leading machine learning methods for many real-world data problems, especially on tabular data. However, most of the current implementations only operate in batch mode, and therefore cannot incrementally update when more data arrive. Several previous works developed streaming trees and ensembles to overcome this limitation. Nonetheless, we found that those state-of-the-art algorithms suffer from a number of drawbacks, including low accuracy on some problems and high memory usage on others. We therefore developed the simplest possible extension of decision trees: given new data, simply update existing trees by continuing to grow them, and replace some old trees with new ones to control the total number of trees. In a benchmark suite containing 72 classification problems (the OpenML-CC18 data suite), we illustrate that our approach, Stream Decision Forest (SDF), does not suffer from either of the aforement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Bellman&#19968;&#33268;&#30340;&#24754;&#35266;&#35770;&#36848;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#36890;&#36807;&#22312;&#19982;Bellman&#26041;&#31243;&#19968;&#33268;&#30340;&#20989;&#25968;&#38598;&#21512;&#19978;&#23454;&#26045;&#21021;&#22987;&#29366;&#24577;&#30340;&#24754;&#35266;&#20027;&#20041;&#65292;&#25913;&#21892;&#20102;&#22522;&#20110;&#22870;&#21169;&#30340;&#24754;&#35266;&#20027;&#20041;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.06926</link><description>&lt;p&gt;
Bellman&#19968;&#33268;&#30340;&#24754;&#35266;&#35770;&#36848;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bellman-consistent Pessimism for Offline Reinforcement Learning. (arXiv:2106.06926v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Bellman&#19968;&#33268;&#30340;&#24754;&#35266;&#35770;&#36848;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#36890;&#36807;&#22312;&#19982;Bellman&#26041;&#31243;&#19968;&#33268;&#30340;&#20989;&#25968;&#38598;&#21512;&#19978;&#23454;&#26045;&#21021;&#22987;&#29366;&#24577;&#30340;&#24754;&#35266;&#20027;&#20041;&#65292;&#25913;&#21892;&#20102;&#22522;&#20110;&#22870;&#21169;&#30340;&#24754;&#35266;&#20027;&#20041;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25512;&#29702;&#25968;&#25454;&#38598;&#32570;&#20047;&#35814;&#23613;&#25506;&#32034;&#26102;&#65292;&#20351;&#29992;&#24754;&#35266;&#20027;&#20041;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#24754;&#35266;&#20027;&#20041;&#22686;&#21152;&#20102;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#36807;&#24230;&#24754;&#35266;&#30340;&#25512;&#29702;&#21516;&#26679;&#20250;&#38459;&#30861;&#21457;&#29616;&#33391;&#22909;&#31574;&#30053;&#65292;&#36825;&#23545;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22870;&#21169;&#30340;&#24754;&#35266;&#20027;&#20041;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bellman&#19968;&#33268;&#30340;&#24754;&#35266;&#20027;&#20041;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#65306;&#25105;&#20204;&#19981;&#26159;&#35745;&#31639;&#20540;&#20989;&#25968;&#30340;&#36880;&#28857;&#19979;&#30028;&#65292;&#32780;&#26159;&#22312;&#19982;Bellman&#26041;&#31243;&#19968;&#33268;&#30340;&#20989;&#25968;&#38598;&#21512;&#19978;&#23454;&#26045;&#21021;&#22987;&#29366;&#24577;&#19978;&#30340;&#24754;&#35266;&#20027;&#20041;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#20165;&#38656;&#35201;&#26631;&#20934;&#30340;Bellman&#23553;&#38381;&#24615;&#20316;&#20026;&#25506;&#32034;&#24615;&#35774;&#32622;&#20013;&#30340;&#35201;&#27714;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22870;&#21169;&#30340;&#24754;&#35266;&#20027;&#20041;&#26080;&#27861;&#25552;&#20379;&#20445;&#35777;&#12290;&#21363;&#20351;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#26356;&#24378;&#30340;&#34920;&#29616;&#21147;&#20551;&#35774;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#26679;&#26412;&#22797;&#26434;&#24615;&#19978;&#20248;&#20110;&#26368;&#36817;&#30340;&#22522;&#20110;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#22797;&#26434;&#24615;&#25913;&#21892;&#20102;&#927;(d)&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of pessimism, when reasoning about datasets lacking exhaustive exploration has recently gained prominence in offline reinforcement learning. Despite the robustness it adds to the algorithm, overly pessimistic reasoning can be equally damaging in precluding the discovery of good policies, which is an issue for the popular bonus-based pessimism. In this paper, we introduce the notion of Bellman-consistent pessimism for general function approximation: instead of calculating a point-wise lower bound for the value function, we implement pessimism at the initial state over the set of functions consistent with the Bellman equations. Our theoretical guarantees only require Bellman closedness as standard in the exploratory setting, in which case bonus-based pessimism fails to provide guarantees. Even in the special case of linear function approximation where stronger expressivity assumptions hold, our result improves upon a recent bonus-based approach by $\mathcal{O}(d)$ in its sample c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#19978;&#19979;&#25991;&#25512;&#26029;&#35774;&#32622;&#20013;&#65292;&#38024;&#23545;&#32047;&#35745;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26368;&#20248;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#28176;&#22686;&#31867;&#21035;&#22797;&#26434;&#24615;&#21644;&#36882;&#20943;&#36793;&#38469;&#25910;&#30410;&#26465;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#35823;&#37197;&#27979;&#35797;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#36873;&#25321;&#22312;&#22870;&#21169;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2106.06483</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#35768;&#22810;&#31867;&#21035;&#30340;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#36890;&#36807;&#31163;&#32447;&#31070;&#35861;&#36827;&#34892;&#26368;&#20248;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Optimal Model Selection in Contextual Bandits with Many Classes via Offline Oracles. (arXiv:2106.06483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#19978;&#19979;&#25991;&#25512;&#26029;&#35774;&#32622;&#20013;&#65292;&#38024;&#23545;&#32047;&#35745;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26368;&#20248;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#28176;&#22686;&#31867;&#21035;&#22797;&#26434;&#24615;&#21644;&#36882;&#20943;&#36793;&#38469;&#25910;&#30410;&#26465;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#35823;&#37197;&#27979;&#35797;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#36873;&#25321;&#22312;&#22870;&#21169;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#25104;&#26412;&#30340;&#20445;&#35777;&#65292;&#23601;&#22909;&#20687;&#26368;&#20248;&#24179;&#34913;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#27169;&#22411;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#19968;&#26679;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#19978;&#19979;&#25991;&#25512;&#26029;&#35774;&#32622;&#20013;&#23454;&#29616;&#31867;&#20284;&#20445;&#35777;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350; [Marinov and Zimmert, 2021] &#37492;&#21035;&#20986;&#27809;&#26377;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#26080;&#25104;&#26412;&#30340;&#36951;&#25022;&#30028;&#38480;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28176;&#22686;&#31867;&#21035;&#22797;&#26434;&#24615;&#21644;&#38543;&#30528;&#31867;&#21035;&#22797;&#26434;&#24615;&#22686;&#21152;&#26368;&#20339;&#31574;&#30053;&#20215;&#20540;&#36793;&#38469;&#25910;&#30410;&#36882;&#20943;&#30340;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#26080;&#25104;&#26412;&#27169;&#22411;&#36873;&#25321;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#35823;&#37197;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#23637;&#31034;&#20102;&#27169;&#22411;&#36873;&#25321;&#22312;&#22870;&#21169;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;&#19982;&#20808;&#21069;&#20851;&#20110;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#27169;&#22411;&#36873;&#25321;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25910;&#38598;&#26356;&#22810;&#25968;&#25454;&#26102;&#20250;&#20180;&#32454;&#22320;&#36866;&#24212;&#36880;&#28176;&#28436;&#21464;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#36229;&#36234;&#20102;&#36866;&#24212;&#26102;&#38388;&#22797;&#26434;&#24615;&#30340;&#33539;&#30068;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection in supervised learning provides costless guarantees as if the model that best balances bias and variance was known a priori. We study the feasibility of similar guarantees for cumulative regret minimization in the stochastic contextual bandit setting. Recent work [Marinov and Zimmert, 2021] identifies instances where no algorithm can guarantee costless regret bounds. Nevertheless, we identify benign conditions where costless model selection is feasible: gradually increasing class complexity, and diminishing marginal returns for best-in-class policy value with increasing class complexity. Our algorithm is based on a novel misspecification test, and our analysis demonstrates the benefits of using model selection for reward estimation. Unlike prior work on model selection in contextual bandits, our algorithm carefully adapts to the evolving bias-variance trade-off as more data is collected. In particular, our algorithm and analysis go beyond adapting to the complexity of t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#24930;-&#24555;&#38543;&#26426;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#40657;&#30418;&#27169;&#25311;&#22120;&#20272;&#35745;&#19981;&#21464;&#27969;&#24418;&#24182;&#35745;&#31639;&#26377;&#25928;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#36807;&#31243;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2104.02120</link><description>&lt;p&gt;
&#38754;&#21521;&#26410;&#30693;&#19981;&#21464;&#27969;&#24418;&#30340;&#24930;-&#24555;&#38543;&#26426;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonlinear model reduction for slow-fast stochastic systems near unknown invariant manifolds. (arXiv:2104.02120v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.02120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#24930;-&#24555;&#38543;&#26426;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#40657;&#30418;&#27169;&#25311;&#22120;&#20272;&#35745;&#19981;&#21464;&#27969;&#24418;&#24182;&#35745;&#31639;&#26377;&#25928;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#36807;&#31243;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#38543;&#26426;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20302;&#32500;&#19981;&#21464;&#26377;&#25928;&#27969;&#24418;&#21644;&#39640;&#32500;&#22823;&#24555;&#27169;&#30340;&#39640;&#32500;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#12290;&#36890;&#36807;&#20165;&#35775;&#38382;&#33021;&#22815;&#33719;&#24471;&#30701;&#26102;&#38388;&#27169;&#25311;&#30340;&#40657;&#30418;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36755;&#20986;&#19981;&#21464;&#27969;&#24418;&#30340;&#20272;&#35745;&#12289;&#22312;&#20854;&#19978;&#30340;&#26377;&#25928;&#38543;&#26426;&#21160;&#21147;&#36807;&#31243;&#65288;&#24179;&#22343;&#25481;&#20102;&#24555;&#27169;&#65289;&#65292;&#20197;&#21450;&#23545;&#24212;&#30340;&#27169;&#25311;&#22120;&#12290;&#36825;&#20010;&#27169;&#25311;&#22120;&#26159;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#20102;&#19981;&#21464;&#27969;&#24418;&#30340;&#20302;&#32500;&#29305;&#24615;&#65292;&#24182;&#19988;&#37319;&#29992;&#30340;&#26102;&#38388;&#27493;&#38271;&#22823;&#23567;&#21462;&#20915;&#20110;&#26377;&#25928;&#36807;&#31243;&#30340;&#27491;&#21017;&#24615;&#65292;&#22240;&#27492;&#36890;&#24120;&#27604;&#21407;&#27169;&#25311;&#22120;&#30340;&#26102;&#38388;&#27493;&#38271;&#22823;&#24471;&#22810;&#65292;&#21407;&#27169;&#25311;&#22120;&#38656;&#35201;&#35299;&#20915;&#24555;&#27169;&#12290;&#31639;&#27861;&#21644;&#20272;&#35745;&#21487;&#20197;&#23454;&#26102;&#36827;&#34892;&#65292;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#26377;&#25928;&#29366;&#24577;&#31354;&#38388;&#65292;&#32780;&#19981;&#20250;&#22833;&#21435;&#19982;&#22522;&#30784;&#21160;&#21147;&#23398;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a nonlinear stochastic model reduction technique for high-dimensional stochastic dynamical systems that have a low-dimensional invariant effective manifold with slow dynamics, and high-dimensional, large fast modes. Given only access to a black box simulator from which short bursts of simulation can be obtained, we design an algorithm that outputs an estimate of the invariant manifold, a process of the effective stochastic dynamics on it, which has averaged out the fast modes, and a simulator thereof. This simulator is efficient in that it exploits of the low dimension of the invariant manifold, and takes time steps of size dependent on the regularity of the effective process, and therefore typically much larger than that of the original simulator, which had to resolve the fast modes. The algorithm and the estimation can be performed on-the-fly, leading to efficient exploration of the effective state space, without losing consistency with the underlying dynamics. This cons
&lt;/p&gt;</description></item><item><title>ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2007.01777</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#20998;&#31867;&#36890;&#36807;&#21407;&#22411;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.01777
&lt;/p&gt;
&lt;p&gt;
ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;ProtoryNet&#65292;&#23427;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#26032;&#27010;&#24565;&#12290;&#21463;&#29616;&#20195;&#35821;&#35328;&#23398;&#20013;&#30340;&#21407;&#22411;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;ProtoryNet&#36890;&#36807;&#20026;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;&#25214;&#21040;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#65292;&#24182;&#23558;&#27599;&#20010;&#21477;&#23376;&#19982;&#30456;&#24212;&#30340;&#27963;&#21160;&#21407;&#22411;&#30340;&#25509;&#36817;&#31243;&#24230;&#36755;&#20837;&#21040;RNN&#20027;&#24178;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;RNN&#20027;&#24178;&#25429;&#25417;&#21040;&#21407;&#22411;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21407;&#22411;&#36712;&#36857;&#12290;&#21407;&#22411;&#36712;&#36857;&#33021;&#22815;&#30452;&#35266;&#32780;&#32454;&#33268;&#22320;&#35299;&#37322;RNN&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#20998;&#26512;&#25991;&#26412;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21407;&#22411;&#20462;&#21098;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#20351;&#29992;&#30340;&#21407;&#22411;&#24635;&#25968;&#65292;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ProtoryNet&#27604;&#22522;&#32447;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#20934;&#30830;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#30452;&#25509;&#23398;&#20064;&#32593;&#32476;&#30340;&#31232;&#30095;&#32467;&#26500;&#21644;&#26435;&#37325;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26377;&#25928;&#26550;&#26500;&#30830;&#23450;&#21644;&#32593;&#32476;&#23610;&#23544;&#32553;&#23567;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/1910.03201</link><description>&lt;p&gt;
&#21487;&#24494;&#31232;&#30095;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differentiable Sparsification for Deep Neural Networks. (arXiv:1910.03201v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#30452;&#25509;&#23398;&#20064;&#32593;&#32476;&#30340;&#31232;&#30095;&#32467;&#26500;&#21644;&#26435;&#37325;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26377;&#25928;&#26550;&#26500;&#30830;&#23450;&#21644;&#32593;&#32476;&#23610;&#23544;&#32553;&#23567;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26497;&#22823;&#22320;&#20943;&#36731;&#20102;&#29305;&#24449;&#24037;&#31243;&#30340;&#36127;&#25285;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#32593;&#32476;&#30340;&#26377;&#25928;&#26550;&#26500;&#30340;&#30830;&#23450;&#20063;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#21464;&#24471;&#36807;&#20110;&#24222;&#22823;&#65292;&#22823;&#37327;&#36164;&#28304;&#34987;&#25237;&#20837;&#21040;&#32553;&#23567;&#23427;&#20204;&#30340;&#22823;&#23567;&#12290;&#36825;&#20123;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#23545;&#36807;&#23436;&#22791;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#26469;&#26377;&#25928;&#35299;&#20915;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#24102;&#26377;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#20989;&#25968;&#23558;&#26080;&#20851;&#32039;&#35201;&#30340;&#21442;&#25968;&#32622;&#38646;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#32593;&#32476;&#30340;&#31232;&#30095;&#32467;&#26500;&#21644;&#26435;&#37325;&#12290;&#23427;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#21508;&#31181;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#20462;&#25913;&#35201;&#27714;&#24456;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#24494;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have significantly alleviated the burden of feature engineering, but comparable efforts are now required to determine effective architectures for these networks. Furthermore, as network sizes have become excessively large, a substantial amount of resources is invested in reducing their sizes. These challenges can be effectively addressed through the sparsification of over-complete models. In this study, we propose a fully differentiable sparsification method for deep neural networks, which can zero out unimportant parameters by directly optimizing a regularized objective function with stochastic gradient descent. Consequently, the proposed method can learn both the sparsified structure and weights of a network in an end-to-end manner. It can be directly applied to various modern deep neural networks and requires minimal modification to the training process. To the best of our knowledge, this is the first fully differentiable sparsification method.
&lt;/p&gt;</description></item></channel></rss>